[{"number": 51490, "title": "tf2.6 training speed can be ~1/10 of tf2.5 when embedding_column has medium bucket size(e.g. 4M)", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.7 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0 and 2.5.0\r\n- Python version: 3.6.11\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI run the same piece of code using tf 2.5.0 and 2.6.0, and the training is much slower on tf 2.6.0 (can be ~1/10 of tf 2.5.0)\r\nThe code is attached below. Using tf 2.5.0, the training speed on my server is ~58 step/s. Using tf 2.6.0, the training speed is ~ 0.9 step/s\r\n\r\n**Describe the expected behavior**\r\nThe training speed should be similar on tf 2.5.0 and 2.6.0.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```Python\r\nimport tensorflow as tf\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\nfrom tensorflow.feature_column import categorical_column_with_hash_bucket, embedding_column\r\ncat_column = categorical_column_with_hash_bucket('video_id', hash_bucket_size=4000000, dtype=tf.int64)\r\nemb_column = embedding_column(cat_column, dimension=64, combiner='sum')\r\ncls = tf.estimator.DNNClassifier([512,256], [emb_column], \"/tmp/model/\")\r\n\r\ndef input_fn():\r\n    ds = tf.data.Dataset.range(10000000)\r\n    ds = ds.map(lambda x: ({'video_id':x}, x % 2)).batch(1024)\r\n    return ds\r\n\r\ncls.train(input_fn)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sanatmpa1 Hi Sanat, have you been able to reproduce this problem? Just run the attached code in tf 2.5.0 and tf 2.6.0 env, and the training speed difference should be obvious.\r\n\r\nThis is a big problem for us, because we want to use the MutableHashTable in tf 2.6.0 (which doesn't exist in tf 2.x before 2.6), but got stuck in this issue. \r\nLooking forward to hearing from you :)", "@jvishnuvardhan,\r\n\r\nI am able to reproduce the issue and please take a look at the colab [gist here](https://colab.research.google.com/gist/sanatmpa1/7940b81b6e83a91371aa0302b4797201/51490.ipynb). As mentioned in issue description, the time taken for steps is around 10 times slower in `TF 2.6` compared to `TF 2.5`.", "@jvishnuvardhan Hi Vishnuvardhan,\r\nCould you help take a look at this issue at your earliest convenience? This is a real blocking issue for us. \r\nThanks!", "@jvishnuvardhan Hi Vishnuvardhan, are you currently looking into this issue?", "@ttang235 I can see lower performance with `TF2.6` but not sure what is the root-cause of the issue. \r\n\r\nI think `tf.estimator` and `tf.feature_columns` are old style `TF1.x` APIs. In the long run you can use `tf.keras` and `tf.distribute` to update `tf.estimator` based code.\r\n\r\nRegarding `feature_columns`, you can use tf.keras `preprocesing` layers. I think these layers provide better functionality and improved performance when compared to `tf.estimator` and `feature_column`.\r\n\r\n@rchao Can you please take a look at this issue? Thanks!\r\n\r\n", "We've been using estimator API for quite some time, and are not yet ready to migrate to keras. \r\n@rchao Hi Rick, could you help take a look? Thanks!", "Hello @ttang235, thanks for opening the issue. For a regression like this, running a bisect is usually the easiest way to figure out the culprit. ", "> Hello @ttang235, thanks for opening the issue. For a regression like this, running a bisect is usually the easiest way to figure out the culprit.\r\n\r\n@rchao Thanks for the tip! Is there a procedure/doc that I can follow to do this? ", "any updates on this issue?"]}, {"number": 51488, "title": "Support freezing ResourceGather ops in subgraphs", "body": "Support freezing ResourceGather ops in subgraphs.  Currently skipped.\r\nhttps://github.com/tensorflow/tensorflow/blob/00a29e825c5f1cd86fb2031f0c78326284637f7f/tensorflow/python/framework/convert_to_constants.py#L407\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNot implemented\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone freezing a graph with resource gather ops in the subgraph", "comments": []}, {"number": 51487, "title": "Add support/upgrade for flatbuffers 2.0", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6.0 / latest nightly\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe TensorFlow pip package currently pins the flatbuffers dependency to `~1.12`:\r\nhttps://github.com/tensorflow/tensorflow/blob/20b1e26b1df7b5dbe4548b9af8a6ff2348f620cc/tensorflow/tools/pip_package/setup.py#L86\r\n\r\nFlatbuffers 2.0 has been release in May and it would be great if TensorFlow would support the new release as well, or upgrade it's required Flatbuffers version to ~2.0. This might require some minor code changes similar to https://github.com/google/jax/pull/6710, but I don't think there should be any major blockers that would prevent an upgrade.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone using Flatbuffers 2.0 in their code or relying on packages that require 2.0.", "comments": ["This is on the long term roadmap", "This issue will be closed once [the pr](https://github.com/tensorflow/tensorflow/pull/51504) is merged.", "Moving this to closed status as pr is merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51487\">No</a>\n", "> Moving this to closed status as pr is merged.\r\n\r\n@Saduf2019 The PR you linked didn't fully solve the issue. I think this should be reopened until #51577 has been merged."]}, {"number": 51484, "title": "New TensorRT model occupying more GPU-Memory as compared to the older version", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.1/8.1.0\r\n- GPU model and memory: NVIDIA GeForce RTX 3080 10GB\r\n\r\n**Describe the current behavior**\r\nI am converting a tensorflow model (.h5 -> saved_model_format -> tensorrt model) to a tensorrt model using tensorflow 2.5.0 (attached tensorrt.py). The code is occupying almost **3.5GB** of GPU Memory. \r\n\r\nIf I load the same model in the below specified environment, then that code is occupying max **~1.1GB** of GPU memory:\r\n**TensorRT Version**: 5.1.2.2-1\r\n**GPU Type**: GeForce RTX 2080 Ti\r\n**Nvidia Driver Version**: 418.87.00\r\n**CUDA Version**:  10.1\r\n**CUDNN Version**: 7.6.2\r\n**Operating System + Version**:  Ubuntu 16.04.7 LTS\r\n**Python Version (if applicable)**: 3.6.13\r\n**TensorFlow Version (if applicable)**:  1.14.1\r\n\r\n**Describe the expected behavior**\r\nI want to figure out what is causing this huge memory usage difference. The base models (.h5) are same just the way of converting the models differs in TF1.14.1 & TF2.5. \r\n\r\nAlso I am getting this at inference time:\r\nE tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n\r\nI have attached codes & output for reference and reproducing this issue.\r\n\r\n**Standalone code to reproduce the issue**\r\n[attachment.zip](https://github.com/tensorflow/tensorflow/files/6981299/attachment.zip)\r\n- tensorrt.py - code to convert the dummy_model to a tensorrt_model\r\n- gpu_usage - to track the gpu usage\r\n- Code-Output - tensorrt_output.txt\r\n\r\n\r\n**Other info / logs**\r\n[tensorrt_output.txt](https://github.com/tensorflow/tensorflow/files/6981335/tensorrt_output.txt)\r\n[gpu_usage_dummy.txt](https://github.com/tensorflow/tensorflow/files/6981337/gpu_usage_dummy.txt)\r\n", "comments": ["@memr5 Could you please try using latest stable version of tensorflow  **`2.6.0`** and let us know if the issue still persists ?Thank you!", "> @memr5 Could you please try using latest stable version of tensorflow **`2.6.0`** and let us know if the issue still persists ?Thank you!\r\n\r\nHey @sushreebarsa, \r\nI will try the stable version. \r\nRecently, I also tried [NGC Tensorflow](https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow) but the memory occupied is way more than before. I am also going to try an older version of the NGC Container to see if the issue still persists. \r\n\r\nI will keep updating this thread.", "Hey @sushreebarsa,\r\n\r\nI tried with TF2.6, now I am not getting this error: \r\nE tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n\r\nbut still the memory usage is around ~3.5GB.", "@sanatmpa1 Was able to reproduce the issue in TF  [v2.5](https://colab.research.google.com/gist/sushreebarsa/d1881891194b308ec508a565414aff07/51484.ipynb), [2.6.0](https://colab.research.google.com/gist/sushreebarsa/a7a33f972fe7dd375beedb30bcd9ee97/untitled390.ipynb) , please find the gists attached .Thank you!", "In both cases are you using \"TensorRT 5.1.2.2-1\"?\r\n\r\nI will take a look tomorrow.\r\n\r\nFWIW, TRT 5 support is removed in master already.\r\n\r\n", "Talking about the NGC Tensorflow containers, I am using whatever I get in the container. \r\nFor 2080 container (19.10) it is TensorRT 5.1.5.\r\nAnd for 3080 container (21.07) it is TensorRT 8."]}, {"number": 51477, "title": "TFLite StaticHashTable always empty in Java", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): 2.5.0\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n\r\n### 2. Code\r\n\r\nHere is a simple model that looks up and returns the positions of string values:\r\n\r\n```\r\nclass Lookup(tf.keras.layers.Layer):\r\n    def build(self, input_shape):\r\n        names = tf.constant([\"a\", \"b\"])\r\n        numbers = tf.constant([1, 2], dtype=tf.int64)\r\n        \r\n        self.table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(names, numbers), -1)\r\n        self.built = True\r\n        \r\n    def call(self, names):\r\n        return self.table.lookup(tf.reshape(names, [-1]))\r\n\r\nwith tf.control_dependencies([tf.compat.v1.tables_initializer()]):  \r\n    names = tf.keras.Input(shape=(2,), dtype=tf.string, name='names')\r\n    model_outputs = Lookup()(names)\r\n    model = tf.keras.Model(\r\n        inputs=[names],\r\n        outputs=model_outputs,\r\n    )\r\n    \r\nmodel.save('./export')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./export')\r\ntflite_model = converter.convert()\r\nwith open('simple.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nWhen calling from python, we can get the indices we expect:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path='simple.tflite')\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], np.array([['a', 'b']]))\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint('output', output_data)\r\n```\r\n```\r\noutput [1 2]\r\n```\r\n\r\nBut when called from Java, the strings are not found:\r\n\r\n```\r\nprivate void runTFModel() throws IOException {\r\n    File file = new File(\"simple.tflite\")\r\n    FileInputStream inputStream = new FileInputStream(file);\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    MappedByteBuffer memoryMap =  fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size());\r\n\r\n    Interpreter interpreter = new Interpreter(memoryMap, new Interpreter.Options());\r\n\r\n    String[] names = new String[]{\"a\", \"b\"};\r\n    Object[] inputArray = new Object[]{names};\r\n\r\n    LongBuffer outputBuffer = ByteBuffer.allocateDirect(2 * 8).order(ByteOrder.nativeOrder()).asLongBuffer();\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, outputBuffer);\r\n\r\n    interpreter.runForMultipleInputsOutputs(inputArray, outputMap);\r\n\r\n    System.out.println(outputBuffer.get(0));\r\n    System.out.println(outputBuffer.get(1));\r\n}               \r\n```\r\n\r\nThis prints\r\n```\r\n-1\r\n-1\r\n```\r\n\r\n### 3. Failure after conversion\r\nThe static hash table on the java side always returns the default element.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 51467, "title": "\"ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors\" for a graph that doesn't appear to have dynamic-sized tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary (pip wheel)\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen using the Edge TPU compiler on a TFLite model (converted via `tf.compat.v1.TFLiteConverter.from_session` from a TensorFlow model), the compiler fails with the error message:\r\n`ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`\r\nI've examined the model in Netron and haven't been able to find a dynamic-sized tensor input.\r\n\r\n**Describe the expected behavior**\r\nThe Edge TPU compiler successfully compiles the model as it has static-sized tensors.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Sure, I would\r\n- Briefly describe your candidate solution(if contributing): Not a solution, but it would be great to understand which part(s) of the model the compiler thinks have dynamic sizes.\r\n\r\n**Standalone code to reproduce the issue**\r\n`edgetpu_compiler -s <path/to/model>`\r\nSee attached file [mobilenet_v2_small_from_session_quant_exp_conv_exp_quant_sel_ops_static.zip](https://github.com/tensorflow/tensorflow/files/6977427/mobilenet_v2_small_from_session_quant_exp_conv_exp_quant_sel_ops_static.zip)\r\nIt's converted from the `mobilenet_v2_small` model in this repository: https://github.com/gsethi2409/tf-pose-estimation\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nSnippet of code used for conversion to TFlite:\r\n```\r\ndef rep_data_gen():\r\n    for x in tf.data.Dataset.list_files(</path/to/training/images/):\r\n        image = tf.io.read_file(x.numpy())\r\n        image = tf.image.decode_jpeg(image)\r\n        image = tf.image.convert_image_dtype(image, tf.float32)\r\n        image = tf.image.resize(image, [320, 240])\r\n        image = image[np.newaxis, :, :, :]\r\n        # image = image / 255.0\r\n        yield [image, tf.constant([160, 120])]\r\n\r\n# Set input shape for non-placeholder, output tensors only look at name\r\nself.tensor_image.set_shape([1, 320, 240, 3])\r\nself.upsample_size.set_shape([2])\r\nsess_converter = tf.compat.v1.lite.TFLiteConverter.from_session(\r\n    self.persistent_sess,\r\n    input_tensors=[self.tensor_image, self.upsample_size],\r\n    output_tensors=[self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up]\r\n)\r\nsess_converter.experimental_new_converter = True\r\nmodel_name = \"mobilenet_v2_small_from_session_quant_exp_conv_exp_quant_sel_ops_static.tflite\"\r\nsess_converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nsess_converter.representative_dataset = rep_data_gen\r\nsess_converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS,\r\n                                            tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nsess_converter.inference_input_type = tf.int8  # or tf.uint8\r\nsess_converter.inference_output_type = tf.int8  # or tf.uint8\r\nsess_converter.experimental_new_quantizer = True\r\ntflite_model = sess_converter.convert()\r\n```\r\n", "comments": ["Checking in on this issue - is there any additional information I can provide to help move the investigation along?"]}, {"number": 51465, "title": "Output of tf.random.uniform can equal maxval, contrary to docstring", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Big Sur 11.2.3 (also observed on Mojave 10.14.6 and on Amazon Linux 2)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0 (also observed on 2.4.1)\r\n- Python version: 3.9.0 (also observed on 3.7 and 3.9.6)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A (observed on CPU)\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe function tf.random.uniform sometimes returns values equal to `maxval`, as in the code below.\r\n\r\n**Describe the expected behavior**\r\nAccording to the docstring for `tf.random.uniform`, \"The generated values follow a uniform distribution in the range [minval, maxval). The lower bound `minval` is included in the range, while the upper bound `maxval` is excluded.\"\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nPython 3.9.0 (default, Nov 27 2020, 21:03:12)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.19.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.random.set_seed(42)\r\n\r\nIn [3]: tf.random.uniform((10000000,), minval=5, maxval=6, seed=42)[1739846]\r\n2021-08-12 13:20:28.540136: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nOut[3]: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jvishnuvardhan,\r\n\r\nI am able to reproduce the issue in `TF 2.5.0, TF 2.6.0 and tf-nightly` which indicates the presence of upper bound `maxval` in generated values, and please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/a0a7ac88fcbfb016b3bcc2b6caea4912/51465.ipynb). Thanks!", "@NoahDStein Looks like this is a bug. \r\n\r\nWhen I tried with `numpy`, `maxval` is excluded from the distribution. But, `maxval` is included (as opposed to what mentioned in the docs) in TF. We will look into root-cause of the issue. Thanks!", "@NoahDStein @jvishnuvardhan This is the same behaviour as in numpy, see the comment in [the numpy docs there](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html): \r\n\r\n> The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample().\r\n\r\nHere the output of \r\n```Python\r\ntf.random.uniform((10000000,), seed=42)[1739846]\r\n```\r\nis `0.99999976`, and, as float precision drops away from zero,\r\n```\r\nnp.float32(5) + np.float32(0.99999976) == 6\r\n````\r\n\r\nIt might be helpful to add a similar comment in the tensorflow docs?"]}, {"number": 51463, "title": "Feature request: real and imag attributes for Tensors.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes, although I may need some help to point me where this code should be.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to have a `real` and `imag` property for Tensors in Tensorflow. These would simply call  to `tf.math.real` and `tf.math.imag` internally. This will make writing code with tensorflow more flexible and will bring more compatibility with numpy and python's `complex` type, which do have these attributes. For instance, I would like to do something like:\r\n```python\r\nout = operation(input)  # Function that returns an output type depending on the input type\r\nreturn out.real if isherm else out  # Different output depending on a variable (isherm) which I keep track of by other more means.\r\n```\r\n\r\n**Will this change the current api? How?**\r\nYes. It will add the `real` and `complex` attributes to Tensors/Variables. It will be backwards compatible as it only adds something new. \r\n\r\n**Who will benefit with this feature?**\r\nAnyone working with complex numbers as will bring easy and intuitive access to the real and imaginary parts of a complex tensor. I think this is specially useful for the experimental numpy behaviour.\r\n\r\n**Any Other info.**\r\nI am working with complex numbers using TensorFlow. In particular,  I am developing a python package, [qutip-tensorflow](https://github.com/qutip/qutip-tensorflow), that allows backing [QuTiP's Qobj](https://github.com/qutip/qutip) with Tensorflow's Tensors. The goal is to make QuTiP benefit from some of the very useful features in TensorFlow such as operating with a GPU or auto-differentiation. The feature suggested here will help with the development of qutip-tensorflow.", "comments": []}, {"number": 51457, "title": "Dereferencing null pointer in GPU delegate", "body": "The following code in GPU delegate constructor should probably use `options_` instead of `options` that is potentially a null pointer\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8b06339173d65b9f48236f4051d31e32a2e86191/tensorflow/lite/delegates/gpu/delegate.cc#L95-L102", "comments": ["@GoldFeniks \r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) for us to expedite the trouble-shooting process? Thanks!", "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Just using tflite as a cpp library\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Arch\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): commit hash b2b7933ffe56f140a34c452ab48a7738cf352b2b\r\n- Bazel version (if compiling from source): I used CMake 3.21.1\r\n- GCC/Compiler version (if compiling from source): 11.1.0\r\n\r\n**Describe the current behavior**\r\nCalling `TfLiteGpuDelegateV2Create(nullptr)` cases a `segmentation fault`\r\n\r\n**Describe the expected behavior**\r\nCode executes without an error \r\n\r\n**Standalone code to reproduce the issue**\r\n```c++\r\nauto tf_delegate = tflite::Interpreter::TfLiteDelegatePtr(TfLiteGpuDelegateV2Create(nullptr), TfLiteGpuDelegateV2Delete);\r\n```\r\n\r\n**Other info / logs** \r\nThe error is caused by this https://github.com/tensorflow/tensorflow/blob/8b06339173d65b9f48236f4051d31e32a2e86191/tensorflow/lite/delegates/gpu/delegate.cc#L95-L102\r\n\r\n`options_` should be used instead of `options` because the latter can be a null pointer"]}, {"number": 51454, "title": "Normalizations based on TFLite input type", "body": "Why there is a different kind of normalization technique used for each data type.\r\n\r\nCould you explain the reasoning for using these.\r\n\r\nUsing Tensorflow version 2.4.1.\r\nReference - https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/examples/label_image/bitmap_helpers_impl.h ,\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/examples/label_image/get_top_n_impl.h\r\n\r\nFor input normalizing .\r\n![Capturefgregtrgtr](https://user-images.githubusercontent.com/47776253/129198641-5f186ab7-7429-4f76-b7fa-29a4ee98b745.JPG)\r\nFor output .\r\n![Capturesssssss](https://user-images.githubusercontent.com/47776253/129198676-1b67f9a0-1375-425a-b612-29980532d631.JPG)\r\n\r\nAre these valid all kinds of TFLite Models?\r\n", "comments": ["Does this valid for all TFLite Models including RNN models ?\r\n\r\nCould you explain the reasoning for that ?"]}, {"number": 51453, "title": "unable to create tf.variables inside a function that is decorated with @tf.function", "body": "tf 2.5\r\n\r\n\r\n```\r\n@tf.function\r\ndef weight_fn():\r\n    w = tf.Variable(tf.truncated_normal())\r\n```\r\n\r\nI have a function like above that would be called about **50 times**, each time it should generate a new variable and return. But according to the [rule](https://tensorflow.google.cn/guide/function#creating_tfvariables) and the hint below,\r\n\r\n```\r\n    ValueError: A tf.Variable created inside your tf.function has been garbage-collected. Your code needs to keep Python references to variables created inside `tf.function`s.\r\n    \r\n    A common way to raise this error is to create and return a variable only referenced inside your function:\r\n    \r\n    @tf.function\r\n    def f():\r\n      v = tf.Variable(1.0)\r\n      return v\r\n    \r\n    v = f()  # Crashes with this error message!\r\n    \r\n    The reason this crashes is that @tf.function annotated function returns a **`tf.Tensor`** with the **value** of the variable when the function is called rather than the variable instance itself. As such there is no code holding a reference to the `v` created inside the function and Python garbage collects it.\r\n    \r\n    The simplest way to fix this issue is to create variables outside the function and capture them:\r\n    \r\n    v = tf.Variable(1.0)\r\n    \r\n    @tf.function\r\n    def f():\r\n      return v\r\n    \r\n    f()  # <tf.Tensor: numpy=1.>\r\n    v.assign_add(1.)\r\n    f()  # <tf.Tensor: numpy=2.>\r\n\r\n```\r\n\r\nI should define the weight variable outside the tf.function, which means I should manually define over 50 weight variables, each line with a weight variable.\r\n\r\n```\r\nw1 = tf.Variable(tf.truncated_normal())\r\nw2 = tf.Variable(tf.truncated_normal())\r\nw3 = tf.Variable(tf.truncated_normal())\r\n......\r\nw50 = tf.Variable(tf.truncated_normal())\r\n\r\n```\r\n\r\nUndoubtedly, this kind of behavior is really stupid, any solutions to this kind of unreasonable rule?\r\n\r\n", "comments": ["not sure if you saw https://github.com/tensorflow/tensorflow/pull/49310#issuecomment-900321706, but `ALLOW_DYNAMIC_VARIABLE_CREATION` may help: (untested)\r\n\r\n```python\r\nfrom tensorflow.python.eager import def_function  # def_function.function is the same as tf.function\r\nfrom tensorflow.python.ops import variables\r\n\r\n\r\ndef_function.ALLOW_DYNAMIC_VARIABLE_CREATION = True\r\n\r\nvars = {}\r\n\r\n@def_function.function\r\ndef weight_fn(val, key):\r\n    if key not in vars:\r\n      vars[key] = variables.Variable(val)\r\n\r\nweights = [weight_fn(tf.truncated_normal(), f\"w{ind+1}\") for ind in range(50)]\r\n```\r\n", "@sumanthratna Thanks for your remind.You are very kind. It seems this is the best solution we can get for now.", "/cc @mdanatg "]}, {"number": 51439, "title": "Vscode devcontainer", "body": "I know we closed https://github.com/tensorflow/tensorflow/pull/48679 but as Codespaces is GA now please keep this open so that in the meantime we have a PR where the user could test and bootstrap Github Codespaces and we could collect some feedback.", "comments": ["Keras has already merged its own https://github.com/keras-team/keras/commit/4c9cf436ca9b1b3a1e8bfd0c89dc96ec855f2233", "I don't know if Tensorflow team members have this setting enabled, if not it will be hard to test this in Codespaces for them (but It is still ok for Vsocde with remote extension):\n\nhttps://docs.github.com/en/codespaces/managing-codespaces-for-your-organization/enabling-codespaces-for-your-organization", "/cc @yarri-oss ", "Just a small reminder. \r\nCodespaces is in free trial until September 10, 2021 then we need to pay the VM instance to test this on the Github Codespace VM or with the Vscode browser. \r\n\r\nOf course it will be still free to test on our your HW resources with Vscode desktop and the remote extension.\r\n", "@angerson @mihaimaruseac I have minimized he Dockerfile and started to use your new `manylinux2014` (currently gpu-only) images.\r\n\r\nThis new setup is oriented to a Docker rootless installation. See\r\nhttps://github.com/microsoft/vscode-remote-release/issues/4646\r\nhttps://github.com/moby/moby/issues/41497\r\n\r\n\r\n", "@mihaimaruseac @angerson @yarri-oss You can test this clicking on the code button\r\n\r\n![immagine](https://user-images.githubusercontent.com/1710528/160184007-4f525ad8-d74c-438f-8299-ab1011b73bc8.png)\r\n", "Note: with the Codespace \"free tier\" available disk space (individual account) you will have a `no space left on device` cause with our manylinux/docker refactory effort we still have only a CUDA embedded image that is very large to run on the \"free tier\". \r\nSee more at https://github.com/tensorflow/build/pull/47\r\n"]}, {"number": 51438, "title": "SparseTensorToCSRSparseMatrix wastes so much time on cpu", "body": "tensorflow 2.5\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport datetime\r\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\r\n\r\na_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\r\na_values = np.array([1.0, 5.0, -1.0, -2.0], np.float32)\r\na_dense_shape = [4, 5]\r\n\r\nb_indices = np.array([[0, 0], [3, 0], [3, 1]])\r\nb_values = np.array([2.0, 7.0, 8.0], np.float32)\r\nb_dense_shape = [5, 3]\r\n\r\n\r\nstamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\nlogdir = 'logs/csrmatrix_test/%s' % stamp\r\ntf.profiler.experimental.start(logdir)\r\n\r\nwith tf.profiler.experimental.Trace(\"Train\", step_num=0):\r\n    # with tf.device('/gpu:0'):\r\n        # with tf.compat.v1.Session() as sess:\r\n            # Define (COO format) Sparse Tensors over Numpy arrays\r\n    a_st = tf.sparse.SparseTensor(a_indices, a_values, a_dense_shape)\r\n    b_st = tf.sparse.SparseTensor(b_indices, b_values, b_dense_shape)\r\n\r\n    # Convert SparseTensors to CSR SparseMatrix\r\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n        a_st.indices, a_st.values, a_st.dense_shape)\r\n    b_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n        b_st.indices, b_st.values, b_st.dense_shape)\r\n\r\n    # Compute the CSR SparseMatrix matrix multiplication\r\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\r\n        a=a_sm, b=b_sm, type=tf.float32)\r\n\r\n    # Convert the CSR SparseMatrix product to a dense Tensor\r\n    c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\r\n        c_sm, tf.float32)\r\n    # Evaluate the dense Tensor value\r\n    # c_sm_dense_value = sess.run(c_sm_dense)\r\n\r\n    print(c_sm_dense)\r\ntf.profiler.experimental.stop()\r\n```\r\n\r\n### trace\r\n![Screenshot from 2021-08-12 08-09-44](https://user-images.githubusercontent.com/12267324/129119243-edb879ee-6300-480d-b7ed-406f0277e116.png)\r\n\r\nI wonder why **sparse_tensor_to_csr_sparse_matrix** and **sparse_matrix_sparse_mat_mul** is calculated on both cpu and gpu because theoretically they should be calculated on gpu only for the sake of performance acceleration. Also, I've found the corresponding code [sparse_mat_mul_op](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/core/kernels/sparse/sparse_mat_mul_op.cc#L534) and [csr_sparse_matrix_to_dense_op](https://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/kernels/sparse/csr_sparse_matrix_to_dense_op.cc#L211) for gpu version. But unexpectedly, the calculation is placed on both the cpu and gpu for twice, so is it the duplicate calcualtion?", "comments": []}, {"number": 51433, "title": "How to add ARG_MAX operation in tensorflow lite android with GPU ?", "body": "I am working with tensorflow 2.5.0. I take a 4d tensor and perform argmax in a lambda function:\r\n\r\n```python\r\ndef activate_postprocess(tensor):\r\n  res = []\r\n  for i in range(k):\r\n    res.append(tf.argmax(tensor[i]))\r\n  return res\r\n\r\npost_processe_layer = tf.keras.layers.Lambda(activate_postprocess)\r\npost_processed_output = post_processed_layer(model.outputs)\r\n```\r\n\r\nthen I convert, adding `TFLITE_BUILTINS` and `SELECT_TF_OPS`:\r\n\r\n```python\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                             tf.lite.OpsSet.SELECT_TF_OPS\r\n                                             ]\r\n    tflite_model = converter.convert()\r\n```\r\n\r\nAnd then I run in the Android app, with GPU delegate\r\n\r\n```\r\nval options = Interpreter.Options()\r\nval compatList = CompatibilityList()\r\nval delegateOptions = compatList.bestOptionsForThisDevice\r\n                gpuDelegate = GpuDelegate(delegateOptions)\r\n                options.addDelegate(gpuDelegate)\r\n```\r\n\r\nAfter running the model, I receive the following error:\r\n```\r\nFailed for 'img.jpg' with error: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\n    ARG_MAX: Operation is not supported.\r\n```\r\nI also tried `options.setUseNNAPI(true) `before creating the interpreter, but it doesn't help.\r\nHow can I solve it? \r\n\r\nI see this: https://www.tensorflow.org/lite/guide/ops_custom\r\nBut I don't understand how to register the operator (https://www.tensorflow.org/lite/guide/ops_custom#create_and_register_the_operator) \r\n\r\nIs there an example with argmax? \r\n\r\nThanks\r\n\r\n\r\n\r\n", "comments": ["Yeah, I don't recall our team implementing an `ARG_MAX` op.  The closest we have is probably `MAX_POOL` with indices.  The shader code should be in the repository.\r\n\r\nAdding custom op to the delegates requires a LOT of plumbing.\r\n* First, you need to make tflite convert understand that custom op of yours (I have never worked in this domain and don't know how easy it is).\r\n* Then you need to create the op and register it, so that the op resolver understand the custom op.  You can skip the implementation of the op if you know you will fully work on the GPU side, but often it's beneficial to have a reference CPU implementation.\r\n* After that, you need to change the GPU delegate's model builder to recognize the new op.  This involves parsing the op and representing this in `GraphFloat32` which is the internal data structure the GPU delegate uses.  Might involve definition of attributes.\r\n* Last but not least, you need to implement the shader code.  If you're not proficient with compute shaders, this can be a major headache.", "@yonatanbitton NNAPI does support ArgMax. However, default output data type of [tf.argmax](https://www.tensorflow.org/api_docs/python/tf/math/argmax) is `int64`, which is not supported by [NNAPI's argmax](https://developer.android.com/ndk/reference/group/neural-networks). If `int32` is enough for your use, please use `tf.argmax(tensor[i], output_type=tf.dtypes.int32)` instead."]}, {"number": 51429, "title": "Problem with mean OpenGL since 2.5 version", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11 oct 5 2020\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2 XL\r\n- TensorFlow installed from (source or binary): Java modules. Also try to compile from sources and run using C++ API.\r\n- TensorFlow version (use command below): 2.5\r\n- GPU model and memory: Adreno 540 / 4GB RAM\r\n\r\n**Describe the current behavior**\r\n\r\nI have model, which makes something like chroma key. It returns 1 for pixel, where is person on image and 0 for other pixels. Starting with version 2.5, when I started recording to file, I have wrong output of model for some frames with OpenGL delegate. It looks like flashing. I attached video. I double check inputs, they are correct. Also for version 2.4 or 2.3 it works correct.\r\n\r\n**Describe the expected behavior**\r\n\r\nTFL 2.5 works the same as 2.4, without wrong output for my model.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nOn my device TFL uses OpenGL. I compared code version 2.5 and 2.3 and I found, what the reason of my problem is mean (/tensorflow/lite/delegates/gpu/gl/kernels/mean.cc). For version 2.5 mean uses GenerateTiledMean function for calculation, if I forced call GenerateTrivialMean instead of GenerateTiledMean, all works correct for me.\r\nAlso I have this problem only if I record output to file. For preview (without recording to file) it works perfect. I think there is problem with synchronisation or data race. I tryed to fix shader of GenerateTiledMean, but I didn't succeed.\r\nAs solution I can sugguest can disable GenerateTiledMean for Adreno, because many of other phones with Adreno use OpenCL.\r\n\r\n`   } else if (UseTiledImpl(ctx) && !ctx.gpu_info->IsAdreno()) {`\r\n\r\n\r\nI forgot one thing. If I force use SSBO instead of texture it fix my problem. But with SSBOs work slower, I think it is a reason, why TFL use texture for Adreno. \r\n\r\n**Other info / logs**\r\n\r\nI don't have any tfl errors in log.\r\n\r\nhttps://user-images.githubusercontent.com/9623833/129029414-0ca2adf7-25b4-43c4-983d-771d1754c3c2.mov\r\n\r\n", "comments": ["Do you think it is the same issue here? #51276 \r\n", "Maybe it is the same, but I am no sure. You may try to forced call GenerateTrivialMean for mean(/tensorflow/lite/delegates/gpu/gl/kernels/mean.cc), maybe it will help you. ", "It is already calling on GenerateTrivialMean ", "> It is already calling on GenerateTrivialMean\r\n\r\nI think, you have another problem, because force calling GenerateTrivialMean fix my issue.", "@srjoglekar246 Hi Sachin, since it's related with GPU, could you help take a look or triage to proper engineers? Thanks!", "@impjdi Can you take a look?", "@UnickSoft It would be helpful if you can attach a tflite model that can reproduce this.", "@impjdi Sorry I cannot provide tflite model. I tried to find open models with mean but I can't. If you provide me model with mean, I am able to test it."]}, {"number": 51428, "title": "GPU performance issue when calling slicing for tensors of types tf.int16, tf.int32 (op StridedSlice)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- binary wheel via PyPI\r\n- TensorFlow version (use command below):\r\n- v2.5.0-0-ga4dfb8d1a71 2.5.0\r\n- Python version:\r\n- 3.6\r\n- CUDA/cuDNN version:\r\n- CUDA 11.2\r\n- GPU model and memory:\r\n- V100 32 GB\r\n\r\n**Describe the current behavior**\r\nWhen slicing tensors of shape int16 or int32 it takes significant amount of time comparing to float16, float32\r\n*****************************************************\r\ntype array: <dtype: 'int16'>\r\ntook 35.05420684814453\r\n*****************************************************\r\ntype array: <dtype: 'int32'>\r\ntook 22.861242294311523\r\n*****************************************************\r\ntype array: <dtype: 'int64'>\r\ntook 5.330085754394531\r\n*****************************************************\r\ntype array: <dtype: 'float16'>\r\ntook 1.550912857055664\r\n*****************************************************\r\ntype array: <dtype: 'float32'>\r\ntook 2.5637149810791016\r\n*****************************************************\r\ntype array: <dtype: 'float64'>\r\ntook 5.917549133300781\r\n*****************************************************\r\ntype array: <dtype: 'bool'>\r\ntook 1.6639232635498047\r\n\r\n**Describe the expected behavior**\r\ntakes around the same time as the float versions\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\nwhen running the code using tf.debugging.set_log_device_placement(True) found that slicing int16 is not using gpu at all:\r\ntype array: <dtype: 'int16'>\r\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StridedSlice in device /job:localhost/replica:0/task:0/device:CPU:0\r\n\r\nfor int32 I don't understand the reason for such difference comparing to float32\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n# tf.debugging.set_log_device_placement(True)\r\nimport numpy as np\r\nimport time\r\n\r\ndef test_slicing():\r\n\r\n    with tf.device('/GPU:0'):\r\n      shape = (52000, 2, 15, 15)\r\n      np_array = np.ones(shape, dtype=np.uint16)\r\n\r\n      dtypes= [tf.uint16, tf.int16, tf.int64, tf.float16, tf.float32, tf.float64, tf.bool]\r\n      start = 0\r\n      end = 50000\r\n      for dtype in dtypes:\r\n        print(\"*****************************************************\")\r\n\r\n        tf_array = tf.constant(np_array, dtype=dtype) * 1 if dtype != tf.bool else tf.math.logical_and(tf.constant(np_array, dtype=dtype) , tf.constant(np_array, dtype=dtype)) \r\n        print(f'type array: {tf_array.dtype}')\r\n        \r\n        for i in range(1):\r\n            tf_array = tf.constant(np_array, dtype=dtype) * 1 if dtype != tf.bool else tf.math.logical_and(tf.constant(np_array, dtype=dtype) , tf.constant(np_array, dtype=dtype)) \r\n\r\n            tic = time.time()\r\n            a = tf_array[start:end]\r\n            a[0][0][0][0].numpy()\r\n            toc = time.time()\r\n            print(f'took {(toc - tic)*1000}')\r\n\r\nprint()          \r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\ntest_slicing()\r\n```\r\n", "comments": ["@farotem Could you please take a look at the [link](https://www.tensorflow.org/api_docs/python/tf/strided_slice) and let us know if it helps ? Thanks!", "Thank you for the fast response.\r\nI looked at it before and I'm not sure how it helps,\r\nI think the behavior for all types should work the same the link for the low level api is not helping if only for specific type there is a need for the low level api.\r\nany other ideas?\r\nIs it possible to add also add GPU tag to the issue?", "@sanatmpa1  Was able to replicate the issue on Colab with TF [v2.4](https://colab.research.google.com/gist/sushreebarsa/0ea29c048266b56cbd66deb01d0c835e/51428.ipynb#scrollTo=8-N00DpkFqHi), [v2.5](https://colab.research.google.com/gist/sushreebarsa/ec94cf63bfd9a465a27dd748f840cc48/51428.ipynb) , [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/8d86c84c3ecc09e3f0c4c166bd58db73/51428.ipynb#scrollTo=xWdtpONIGSwi) ,please find the gists attached.Thank you! ", "@farotem,\r\n\r\nPlease take a look at this [gist-2.6 ](https://colab.research.google.com/gist/sanatmpa1/d831e4f48f85388d413aa835433149ec/51428-tf-2-6.ipynb) from `TF 2.6.0` and [gist-nightly](https://colab.research.google.com/gist/sanatmpa1/40d1b9105daee4c7f8b3ffa5a708ed14/51428-nightly.ipynb) from `tf-nightly`, The time difference between int and float has reduced significantly in `TF 2.6` and you can notice that in `tf-nightly` the time taken for `int` is mostly close to what it takes for `float`, which indicates that its already fixed in nightly. Let me know if it addresses your issue. Thanks!", "Thank you for your response,\r\nI looked at both gists (2.6, nightly) and I see some concerned issues:\r\n\r\n- timing\r\n  1. float in tf 2.5: ~1.5ms - 2.5ms\r\n  2. float in tf 2.6, nightly ~14-16ms\r\n \r\n- runtime machine (enabling tf.debugging.set_log_device_placement(True))\r\n  1.  tf 2.5 looking the op StridedSlice for float is device:**GPU**:0\r\n  2.  tf 2.6, nightly looking the op StridedSlice for float is device:**CPU**:0\r\n\r\nso if I understand correctly the reason for the closing gap between int to float is that now the performance of float decrease because the op StridedSlice is no longer in GPU and only runs on CPU is that on purpose?\r\n\r\nI hoped the opposite direction would work and the int op StridedSlice that runs in tf 2.5 on CPU only will run on GPU in later releases just like float and the performance for int and float will be the same and increased.\r\n\r\n\r\n"]}, {"number": 51419, "title": "tf.reduce_sum is hard to use in a numerical stable way", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n### Edited to add:\r\n\r\nAs 1st comment clarifies, the described behavior is probably \"working as intended\" due to accumulation of floating point errors, which is mitigated in many common cases but not all cases, leading to surprising inconsistency.\r\n\r\nHowever, I still think at least 2 things could be improved:\r\n\r\n- `tf.reduce_sum` could include a note in its documentation explaining this behavior, like [`np.sum`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\r\n- `tf.reduce_sum` could take a `dtype` parameter to improve precision of the accumulator and output without needing to increase precision of the entire input tensor, also like [`np.sum`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\r\n\r\nOriginal report follows.\r\n\r\n---\r\n\r\n**System information**\r\nThis bug is reproducible on standard Colab, so I've only included the following as system information:\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7.11\r\n\r\n**Code to reproduce**\r\n\r\n```python\r\nwith tf.device(\"cpu:0\"):\r\n  print(tf.reduce_sum(tf.ones((20_000_000, 2)), axis=0))\r\n```\r\n\r\nFull reproduction in colab on a GPU-enabled VM: https://gist.github.com/nfelt/e6edc45736740ed899eea95a36bea359\r\n\r\n**Describe the current behavior**\r\n\r\nOn CPU, `tf.reduce_sum` on a rank-2 float32 tensor, along an axis containing more than 2^24 elements of value 1.0, emits an incorrect output that is truncated to 2^24 (the maximum integer precisely representable in float32).  I.e. the above code emits `tf.Tensor([16777216. 16777216.], shape=(2,), dtype=float32)`.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should emit the same answer that it does in pretty much every other context, which is the numerically correct answer up to floating point representation error, in this case `tf.Tensor([20000000. 20000000.], shape=(2,), dtype=float32)`.\r\n\r\nIn particular, we get the expected answer in all of the following situations:\r\n- Running `reduce_sum` on GPU instead of CPU\r\n- Using `float64` instead of `float32`\r\n- Summing a rank-1 tensor of more than 2^24 elements of value 1.0\r\n- Summing a rank-2 tensor along an axis with fewer than 2^24, but individually larger, elements, e.g. `tf.reduce_sum(tf.ones((10_000_000, 2)) * 2.0, axis=0)`\r\n  \r\n**Notes**\r\n\r\nIt seems like the problem of stopping at 2^24 is affecting the count of how many elements get summed, rather than the actual sum itself, based on these cases:\r\n\r\n```python\r\nwith tf.device(\"cpu:0\"):\r\n  print(tf.reduce_sum(tf.ones((20_000_000, 2)) * 0.5, axis=0))\r\n>>> tf.Tensor([8388608. 8388608.], shape=(2,), dtype=float32)\r\n```\r\n\r\nIn particular, it seems to only include the trailing 2^24 elements, based on this:\r\n\r\n```python\r\ntensor_with_one_big_value = tf.concat([[[10_000_000, 0]], tf.ones((20_000_000, 2))], axis=0)\r\nwith tf.device(\"cpu:0\"):\r\n  print(tf.reduce_sum(tensor_with_one_big_value, axis=0))\r\nwith tf.device(\"cpu:0\"):\r\n  print(tf.reduce_sum(tf.reverse(tensor_with_one_big_value, axis=[0]), axis=0))\r\n>>> tf.Tensor([16777216. 16777216.], shape=(2,), dtype=float32)\r\n>>> tf.Tensor([26777216. 16777216.], shape=(2,), dtype=float32)\r\n```\r\n\r\n", "comments": ["that's floating point arithmetic. adding 1 to 16777216 in float32 doesn't do anything, since 16777217 is not in float32. See [here](https://stackoverflow.com/questions/12596695/why-does-a-float-variable-stop-incrementing-at-16777216-in-c) ", "@HVoltBb thanks for pointing that out, I guess you're right that this explains the behavior.  I still think it's quite surprising that it's so inconsistent - between CPU and GPU, and between rank-1 and rank-2 `tf.reduce_sum()` invocations - but looking at the [numpy docs](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) I see it has a similar caveat that the choice of axis for the summation affects whether it uses a partial sum optimization to preserve better numerical accuracy in the overall results.  It would sure be nice if `tf.reduce_sum` documentation had an equivalent note, but fair enough that it's more a doc fix request than a bug fix then.\r\n\r\n", "@nfelt I assume when the job is offloaded to GPU, the underlying algorithm is breaking up the sum into multiple chunks, implicitly achieving a higher precision for this operation, and thus avoiding this issue. I haven't looked at rank-1 vs rank-2 difference, but will do it later.", "FWIW, one other thing that occurred to me is that `np.sum` supports a `dtype` parameter for the accumulator and result, while `tf.reduce_sum` does not, forcing a cast of the full input tensor to float64 to avoid this issue.  Unless there's some fancy lazy behavior happening under the hood or something to defer the cast, that would seem to require incurring a lot more memory overhead compared to numpy's approach.  So maybe this could also be reframed as an FR for adding that parameter.\r\n\r\n> I haven't looked at rank-1 vs rank-2 difference, but will do it later.\r\n\r\nBased on the note for `np.sum` about the partial sum optimization being axis-specific, I'm assuming the rank-1 vs rank-2 difference is actually a memory-aligned-axis difference, since from my testing, in the rank-2 case the effect only happens when reducing along axis 0, which I believe would be the non-memory-aligned direction.\r\n\r\n\r\n\r\n", "@nfelt  I think your explanation for the rank-1 vs rank-2 could be the case here.", "I came across this solution by accident today. The solution is the so called compensated sum, and it has been already implemented in tensorflow_probability. You can use the following code snippet to get the correct answer.\r\n```\r\nimport tensorflow_probability as tfp\r\n\r\nwith tf.device(\"cpu:0\"):\r\n  print(tfp.math.reduce_kahan_sum(tf.ones((20_000_000, 2)), axis=0))\r\n```\r\n"]}, {"number": 51417, "title": "Incorrect Results of MatMul on TFLite with GPU Delegate", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung S10e\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: RTX 2080 Ti 11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`: v1.12.1-59750-g417a4452453 2.7.0\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI create a very simple model with only one `reshape` operator and one `matmul` operator. It seems that the TFLite converter will remove the reshape operator but keep the shape before the reshape operator, which is not a problem on CPU but however will cause incorrect results on GPU delegate.\r\n\r\nWe can visualize the model using netron and one of the inputs for `FullyConnected` op is 3-dimensional, while this may not be the problem since reshaping tensor may just be another view of the data in the memory, it does produce incorrect results when using it with GPU delegate (see below).\r\n\r\n**Describe the expected behavior**\r\nOn GPU delegate, it should produce the correct results.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Yes\r\n- Briefly describe your candidate solution(if contributing): I haven't got a chance to view the codebase of FullyConnected op yet but I am willing to help :) I guess there are two possible solutions:\r\n  - Let TFLite converter keep the reshape operator\r\n  - Let TFLite interpreter able to handle such case\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\nThe python code to generate the model:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntfv1 = tf.compat.v1\r\n\r\n\r\ndef convert_to_tflite_model(path: str, graph: tf.Graph, input_tensors, output_tensors, use_fp16_optimization=False):\r\n  with graph.as_default(), tfv1.Session(graph=graph) as session:\r\n    session.run(tfv1.global_variables_initializer())\r\n    converter = tfv1.lite.TFLiteConverter.from_session(\r\n      session, input_tensors=input_tensors, output_tensors=output_tensors)\r\n    if use_fp16_optimization:\r\n      converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      converter.target_spec.supported_types = [tf.float16]\r\n\r\n    with open(path, \"wb\") as writer:\r\n      writer.write(converter.convert())\r\n\r\n\r\ndef generate_matmul_tflite_model(basename: str,\r\n                                 use_fp16_optimization=False):\r\n  with tf.Graph().as_default() as graph:\r\n    a = tfv1.placeholder(tf.float32, shape=[2, 8, 4], name=\"a\")\r\n    w = tfv1.get_variable(\"w\", shape=[16, 4], dtype=tf.float32)\r\n    s = tfv1.reshape(a, [2 * 8, -1])\r\n    c = tfv1.matmul(s, w, transpose_b=True)\r\n\r\n    convert_to_tflite_model(basename + '.tflite', graph, [a, w], [c],\r\n                            use_fp16_optimization=use_fp16_optimization)\r\n\r\n\r\ndef main():\r\n  generate_matmul_tflite_model(\"matmul\", use_fp16_optimization=False)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  main()\r\n``` \r\n\r\nWe can visualize it in [netron](https://netron.app):\r\n\r\n![Screenshot_20210810_174759](https://user-images.githubusercontent.com/23658877/128939552-6a8de325-0d4e-4cae-9077-55cd59e63cf4.png)\r\n\r\nAs you can see one of the inputs to the `FullyConnected` is 3-dimensional and the reshape op just get disappeared.\r\n\r\nThen we can use the tool `run_delegate_testing.sh` to test the model (should change the kMaxPrint to 100 since the first 16 elements are correct):\r\n```shell\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m matmul.tflite\r\n```\r\n\r\nThe patch to modify the `kMaxPrint`:\r\n```c++\r\ndiff --git a/tensorflow/lite/delegates/gpu/cl/testing/delegate_testing.cc b/tensorflow/lite/delegates/gpu/cl/testing/delegate_testing.cc\r\nindex eb924406ecd..d17b8a9c5bc 100644\r\n--- a/tensorflow/lite/delegates/gpu/cl/testing/delegate_testing.cc\r\n+++ b/tensorflow/lite/delegates/gpu/cl/testing/delegate_testing.cc\r\n@@ -69,7 +69,7 @@ void CompareCPUGPUResults(tflite::Interpreter* cpu, tflite::Interpreter* gpu,\r\n \r\n     std::cout << \"Output \" << tensor_ptr->name << \":\" << std::endl;\r\n \r\n-    const int kMaxPrint = 10;\r\n+    const int kMaxPrint = 100;\r\n     int printed = 0;\r\n     int total_different = 0;\r\n     for (int k = 0; k < tensor_elements_count; ++k) {\r\n```\r\n\r\nPlease refer to the log below.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Replacing 1 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nOutput MatMul:\r\nElement #0: CPU value - 1.55481, GPU value - 1.55469, abs diff - 0.000122547\r\nElement #1: CPU value - -0.968265, GPU value - -0.967773, abs diff - 0.000491381\r\nElement #3: CPU value - 1.34608, GPU value - 1.3457, abs diff - 0.000380635\r\nElement #4: CPU value - -1.47071, GPU value - -1.46973, abs diff - 0.00098145\r\nElement #5: CPU value - 0.576554, GPU value - 0.576172, abs diff - 0.000382423\r\nElement #6: CPU value - 0.716986, GPU value - 0.716309, abs diff - 0.000677407\r\nElement #7: CPU value - -1.51386, GPU value - -1.5127, abs diff - 0.00116575\r\nElement #8: CPU value - 1.26207, GPU value - 1.26074, abs diff - 0.00132298\r\nElement #9: CPU value - -0.136021, GPU value - -0.135742, abs diff - 0.000278458\r\nElement #10: CPU value - -1.08425, GPU value - -1.08398, abs diff - 0.000262737\r\nElement #11: CPU value - 1.55344, GPU value - 1.55273, abs diff - 0.000708699\r\nElement #12: CPU value - -0.946549, GPU value - -0.946289, abs diff - 0.000260055\r\nElement #13: CPU value - -0.316031, GPU value - -0.315918, abs diff - 0.000113398\r\nElement #14: CPU value - 1.35969, GPU value - 1.35938, abs diff - 0.00031805\r\nElement #15: CPU value - -1.46148, GPU value - -1.46094, abs diff - 0.000540376\r\nElement #16: CPU value - -0.968265, GPU value - 1.26074, abs diff - 2.22901\r\nElement #17: CPU value - 2.00199, GPU value - -1.80371, abs diff - 3.8057\r\nElement #18: CPU value - -1.64891, GPU value - 1.09766, abs diff - 2.74657\r\nElement #19: CPU value - 0.153611, GPU value - 0.369385, abs diff - 0.215774\r\nElement #20: CPU value - 1.4481, GPU value - -1.58105, abs diff - 3.02915\r\nElement #21: CPU value - -2.04669, GPU value - 1.69629, abs diff - 3.74298\r\nElement #22: CPU value - 1.22751, GPU value - -0.637695, abs diff - 1.86521\r\nElement #23: CPU value - 0.441976, GPU value - -0.863281, abs diff - 1.30526\r\nElement #24: CPU value - -1.80531, GPU value - 1.76562, abs diff - 3.57093\r\nElement #25: CPU value - 1.91808, GPU value - -1.44531, abs diff - 3.36339\r\nElement #26: CPU value - -0.702171, GPU value - 0.123291, abs diff - 0.825462\r\nElement #27: CPU value - -1.00014, GPU value - 1.2832, abs diff - 2.28334\r\nElement #28: CPU value - 2.00964, GPU value - -1.80273, abs diff - 3.81237\r\nElement #29: CPU value - -1.62704, GPU value - 1.07227, abs diff - 2.6993\r\nElement #30: CPU value - 0.117366, GPU value - 0.400391, abs diff - 0.283024\r\nElement #31: CPU value - 1.4736, GPU value - -1.59473, abs diff - 3.06833\r\nElement #32: CPU value - -0.28901, GPU value - -0.967773, abs diff - 0.678764\r\nElement #33: CPU value - -1.64891, GPU value - 2, abs diff - 3.64891\r\nElement #34: CPU value - 2.44461, GPU value - -1.64648, abs diff - 4.09109\r\nElement #35: CPU value - -1.5469, GPU value - 0.152832, abs diff - 1.69973\r\nElement #36: CPU value - -0.422372, GPU value - 1.44727, abs diff - 1.86964\r\nElement #37: CPU value - 2.09906, GPU value - -2.04492, abs diff - 4.14398\r\nElement #38: CPU value - -2.3217, GPU value - 1.22656, abs diff - 3.54826\r\nElement #39: CPU value - 0.936071, GPU value - 0.441895, abs diff - 0.494176\r\nElement #40: CPU value - 1.09799, GPU value - -1.80371, abs diff - 2.9017\r\nElement #41: CPU value - -2.37146, GPU value - 1.91602, abs diff - 4.28747\r\nElement #42: CPU value - 2.00219, GPU value - -0.701172, abs diff - 2.70336\r\nElement #43: CPU value - -0.245977, GPU value - -0.999023, abs diff - 0.753046\r\nElement #44: CPU value - -1.68062, GPU value - 2.00781, abs diff - 3.68844\r\nElement #45: CPU value - 2.44303, GPU value - -1.625, abs diff - 4.06803\r\nElement #46: CPU value - -1.51312, GPU value - 0.117432, abs diff - 1.63056\r\nElement #47: CPU value - -0.464946, GPU value - 1.47168, abs diff - 1.93663\r\nElement #48: CPU value - 1.34608, GPU value - -0.135742, abs diff - 1.48183\r\nElement #49: CPU value - 0.153611, GPU value - 1.91602, abs diff - 1.7624\r\nElement #50: CPU value - -1.5469, GPU value - -2.37109, abs diff - 0.824197\r\nElement #51: CPU value - 1.86863, GPU value - 1.18066, abs diff - 0.687964\r\nElement #52: CPU value - -0.895937, GPU value - 0.825195, abs diff - 1.72113\r\nElement #53: CPU value - -0.697382, GPU value - -2.26172, abs diff - 1.56434\r\nElement #54: CPU value - 1.80761, GPU value - 2.12891, abs diff - 0.321292\r\nElement #55: CPU value - -1.66569, GPU value - -0.523438, abs diff - 1.14225\r\nElement #56: CPU value - 0.369921, GPU value - -1.44531, abs diff - 1.81523\r\nElement #57: CPU value - 1.1821, GPU value - 2.41211, abs diff - 1.23001\r\nElement #58: CPU value - -1.91526, GPU value - -1.70898, abs diff - 0.206277\r\nElement #59: CPU value - 1.3217, GPU value - -0.178589, abs diff - 1.50029\r\nElement #60: CPU value - 0.187421, GPU value - 1.94238, abs diff - 1.75496\r\nElement #61: CPU value - -1.56671, GPU value - -2.35938, abs diff - 0.792663\r\nElement #62: CPU value - 1.86072, GPU value - 1.14355, abs diff - 0.717167\r\nElement #63: CPU value - -0.865786, GPU value - 0.865723, abs diff - 1.73151\r\nElement #64: CPU value - -1.47071, GPU value - -0.289062, abs diff - 1.18165\r\nElement #65: CPU value - 1.4481, GPU value - -1.64648, abs diff - 3.09458\r\nElement #66: CPU value - -0.422372, GPU value - 2.44336, abs diff - 2.86573\r\nElement #67: CPU value - -0.895937, GPU value - -1.54492, abs diff - 0.648985\r\nElement #68: CPU value - 1.59362, GPU value - -0.421875, abs diff - 2.01549\r\nElement #69: CPU value - -1.18738, GPU value - 2.09766, abs diff - 3.28504\r\nElement #70: CPU value - -0.0413711, GPU value - -2.32031, abs diff - 2.27894\r\nElement #71: CPU value - 1.24146, GPU value - 0.93457, abs diff - 0.306894\r\nElement #72: CPU value - -1.58158, GPU value - 1.09766, abs diff - 2.67924\r\nElement #73: CPU value - 0.826114, GPU value - -2.37109, abs diff - 3.19721\r\nElement #74: CPU value - 0.501611, GPU value - 2, abs diff - 1.49839\r\nElement #75: CPU value - -1.48186, GPU value - -0.245605, abs diff - 1.23626\r\nElement #76: CPU value - 1.43561, GPU value - -1.67969, abs diff - 3.1153\r\nElement #77: CPU value - -0.394892, GPU value - 2.44141, abs diff - 2.8363\r\nElement #78: CPU value - -0.919374, GPU value - -1.5127, abs diff - 0.593322\r\nElement #79: CPU value - 1.59678, GPU value - -0.464355, abs diff - 2.06113\r\nElement #80: CPU value - 0.576554, GPU value - -1.08398, abs diff - 1.66054\r\nElement #81: CPU value - -2.04669, GPU value - -0.701172, abs diff - 1.34552\r\nElement #82: CPU value - 2.09906, GPU value - 2, abs diff - 0.0990589\r\nElement #83: CPU value - -0.697382, GPU value - -1.91406, abs diff - 1.21668\r\nElement #84: CPU value - -1.18738, GPU value - 0.501953, abs diff - 1.68933\r\nElement #85: CPU value - 2.24963, GPU value - 1.25879, abs diff - 0.99084\r\nElement #86: CPU value - -1.75353, GPU value - -2.14648, abs diff - 0.392954\r\nElement #87: CPU value - 0.0427394, GPU value - 1.54785, abs diff - 1.50511\r\nElement #88: CPU value - 1.69766, GPU value - 0.123291, abs diff - 1.57437\r\nElement #89: CPU value - -2.26207, GPU value - -1.70898, abs diff - 0.553082\r\nElement #90: CPU value - 1.25951, GPU value - 2.11133, abs diff - 0.851816\r\nElement #91: CPU value - 0.615522, GPU value - -1.05078, abs diff - 1.6663\r\nElement #92: CPU value - -2.06418, GPU value - -0.737305, abs diff - 1.32687\r\nElement #93: CPU value - 2.08295, GPU value - 2.01562, abs diff - 0.0673242\r\nElement #94: CPU value - -0.658837, GPU value - -1.89648, abs diff - 1.23765\r\nElement #95: CPU value - -1.22166, GPU value - 0.464355, abs diff - 1.68602\r\nElement #96: CPU value - 0.716986, GPU value - 1.3457, abs diff - 0.628717\r\nElement #97: CPU value - 1.22751, GPU value - 0.152832, abs diff - 1.07468\r\nElement #98: CPU value - -2.3217, GPU value - -1.54492, abs diff - 0.776779\r\nElement #99: CPU value - 1.80761, GPU value - 1.86719, abs diff - 0.0595728\r\nElement #100: CPU value - -0.0413711, GPU value - -0.895996, abs diff - 0.854625\r\nPrinted 100 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 254 different elements, for output #0, threshhold - 0.0001\r\nCPU time - 0.013906ms\r\nGPU time(CPU->GPU->CPU) - 2.14057ms\r\n```", "comments": ["@impjdi could you take a look?", "Yeah, the GPU backends `MATMUL` (aka `FULLY_CONNECTED`) is not feature complete and may fail in various situations, including this one.  Unfortunately, everyone working on shaders are out, and it may take a long time to fix this.\r\n\r\nIn the short term, I would remove all the optional things, such as flattening and transposing.  If you look at your example, the tensor dimensions don't match up in a mathematical sense; it will only work if you flatten `a` to 16x4 and then transpose `w`.  You might get the correct answer when you make it explicitly 16x4 and 4x16 without fancy options that TFLite GPU doesn't handle.\r\n\r\nI do agree we should fail if things are not compatible, but this might be your short term solution if you want to be unblocked. ", "Yeah I totally understand that on GPU delegate does not support every option. But it will also fail even without the `transpose_b` option, like the model like as follows:\r\n\r\n```python\r\n  with tf.Graph().as_default() as graph:\r\n    a = tfv1.placeholder(tf.float32, shape=[2, 8, 4], name=\"a\")\r\n    w = tfv1.get_variable(\"w\", shape=[4, 16], dtype=tf.float32)\r\n    s = tfv1.reshape(a, [2 * 8, -1])   # this operator will disappear!\r\n    c = tfv1.matmul(s, w)\r\n```\r\n\r\nI think the problem is not lacking of implementation of those options in `MatMul` kernel. The problem is that the `tf.reshape` operator seems disappeared when exporting the TFLite model. I guess the TFLite converter \"assumes\" that the `MatMul` is able to handle 3-dim inputs (so we can remove the reshape) but it actually isn't on GPU delegate.\r\n\r\nBut if you use `tf.compat.v1.layers.flatten`, the reshape operator will be there:\r\n\r\n```python\r\n  with tf.Graph().as_default() as graph:\r\n    a = tfv1.placeholder(tf.float32, shape=[2, 8, 4], name=\"a\")\r\n    w = tfv1.get_variable(\"w\", shape=[32, 16], dtype=tf.float32)\r\n    s = tf.compat.v1.layers.flatten(a)\r\n    c = tfv1.matmul(s, w)\r\n```\r\n\r\nVisualization:\r\n![Screenshot_20210811_210209](https://user-images.githubusercontent.com/23658877/129122509-e80c81e4-091c-4e4b-b030-8c81e6323dfa.png)\r\n", "Thanks for digging deeper into this.  If you use `tf.compat.v1.layers.flatten`, is the output correct?", "Yes (assuming the abs diff threshold is `0.01`). The above model will have the testing log as follows:\r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nRESHAPE: Operation is not supported.\r\nTRANSPOSE: Operation is not supported.\r\n1 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\nINFO: Replacing 1 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nOutput MatMul:\r\nElement #0: CPU value - -0.0561157, GPU value - -0.0565186, abs diff - 0.000402838\r\nElement #1: CPU value - -0.235481, GPU value - -0.234985, abs diff - 0.00049594\r\nElement #2: CPU value - -0.198347, GPU value - -0.198975, abs diff - 0.000628054\r\nElement #3: CPU value - 0.0211472, GPU value - 0.0202637, abs diff - 0.000883516\r\nElement #4: CPU value - 0.221198, GPU value - 0.218872, abs diff - 0.00232619\r\nElement #5: CPU value - 0.217881, GPU value - 0.216797, abs diff - 0.00108384\r\nElement #6: CPU value - 0.0142447, GPU value - 0.0144043, abs diff - 0.000159643\r\nElement #7: CPU value - -0.202488, GPU value - -0.202759, abs diff - 0.000270754\r\nElement #8: CPU value - -0.233054, GPU value - -0.233398, abs diff - 0.000344336\r\nElement #10: CPU value - 0.179725, GPU value - 0.178711, abs diff - 0.00101388\r\nElement #11: CPU value - 0.243563, GPU value - 0.243896, abs diff - 0.000333682\r\nElement #12: CPU value - 0.0834703, GPU value - 0.0822754, abs diff - 0.00119487\r\nElement #13: CPU value - -0.153364, GPU value - -0.152344, abs diff - 0.0010207\r\nElement #14: CPU value - -0.249196, GPU value - -0.248047, abs diff - 0.00114955\r\nElement #15: CPU value - -0.115919, GPU value - -0.115723, abs diff - 0.000195846\r\nElement #16: CPU value - -0.168952, GPU value - -0.169067, abs diff - 0.000115559\r\nElement #17: CPU value - 0.23118, GPU value - 0.230225, abs diff - 0.000955477\r\nElement #18: CPU value - 0.418766, GPU value - 0.418457, abs diff - 0.00030908\r\nElement #20: CPU value - -0.179584, GPU value - -0.180298, abs diff - 0.000713363\r\nElement #21: CPU value - -0.4154, GPU value - -0.414551, abs diff - 0.000849575\r\nElement #22: CPU value - -0.269299, GPU value - -0.27002, abs diff - 0.000720531\r\nElement #23: CPU value - 0.124395, GPU value - 0.123169, abs diff - 0.00122568\r\nElement #25: CPU value - 0.311867, GPU value - 0.312744, abs diff - 0.000876725\r\nElement #26: CPU value - -0.0667149, GPU value - -0.0679321, abs diff - 0.00121719\r\nElement #27: CPU value - -0.38396, GPU value - -0.38208, abs diff - 0.00187981\r\nElement #28: CPU value - -0.348194, GPU value - -0.348877, abs diff - 0.00068295\r\nElement #29: CPU value - 0.00770001, GPU value - 0.00559235, abs diff - 0.00210766\r\nElement #30: CPU value - 0.356515, GPU value - 0.356689, abs diff - 0.00017485\r\nElement #31: CPU value - 0.377551, GPU value - 0.376953, abs diff - 0.000598162\r\nTotal 29 different elements, for output #0, threshhold - 0.0001\r\nCPU time - 0.014532ms\r\nGPU time(CPU->GPU->CPU) - 1.29234ms\r\n```", "I manually disabled the reshape operator in `model_builder.cc` but if I enabled it to run on GPU the results are still correct (see below).\r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nTRANSPOSE: Operation is not supported.\r\n1 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\nINFO: Replacing 1 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nOutput MatMul:\r\nElement #0: CPU value - -0.0561157, GPU value - -0.0555921, abs diff - 0.000523593\r\nElement #1: CPU value - -0.235481, GPU value - -0.234969, abs diff - 0.000512362\r\nElement #3: CPU value - 0.0211472, GPU value - 0.0206675, abs diff - 0.000479735\r\nElement #4: CPU value - 0.221198, GPU value - 0.22065, abs diff - 0.000548571\r\nElement #5: CPU value - 0.217881, GPU value - 0.217768, abs diff - 0.000113145\r\nElement #6: CPU value - 0.0142447, GPU value - 0.014671, abs diff - 0.000426342\r\nElement #7: CPU value - -0.202488, GPU value - -0.201914, abs diff - 0.000573993\r\nElement #8: CPU value - -0.233054, GPU value - -0.23286, abs diff - 0.000193775\r\nElement #9: CPU value - -0.0493513, GPU value - -0.0497158, abs diff - 0.000364546\r\nElement #10: CPU value - 0.179725, GPU value - 0.179137, abs diff - 0.000587821\r\nElement #11: CPU value - 0.243563, GPU value - 0.243292, abs diff - 0.000270531\r\nElement #12: CPU value - 0.0834703, GPU value - 0.0837656, abs diff - 0.000295311\r\nElement #13: CPU value - -0.153364, GPU value - -0.152775, abs diff - 0.000589684\r\nElement #14: CPU value - -0.249196, GPU value - -0.248855, abs diff - 0.000341728\r\nElement #15: CPU value - -0.115919, GPU value - -0.116139, abs diff - 0.000220343\r\nElement #16: CPU value - -0.168952, GPU value - -0.169855, abs diff - 0.0009031\r\nElement #17: CPU value - 0.23118, GPU value - 0.230849, abs diff - 0.000331476\r\nElement #18: CPU value - 0.418766, GPU value - 0.419311, abs diff - 0.000544816\r\nElement #19: CPU value - 0.22134, GPU value - 0.222261, abs diff - 0.000920296\r\nElement #20: CPU value - -0.179584, GPU value - -0.179135, abs diff - 0.000449657\r\nElement #21: CPU value - -0.4154, GPU value - -0.415835, abs diff - 0.000434399\r\nElement #22: CPU value - -0.269299, GPU value - -0.270218, abs diff - 0.000919074\r\nElement #23: CPU value - 0.124395, GPU value - 0.123836, abs diff - 0.000558779\r\nElement #24: CPU value - 0.40372, GPU value - 0.404036, abs diff - 0.000315309\r\nElement #25: CPU value - 0.311867, GPU value - 0.312767, abs diff - 0.000899494\r\nElement #26: CPU value - -0.0667149, GPU value - -0.0660583, abs diff - 0.000656664\r\nElement #27: CPU value - -0.38396, GPU value - -0.38415, abs diff - 0.000189781\r\nElement #28: CPU value - -0.348194, GPU value - -0.349056, abs diff - 0.000861824\r\nElement #29: CPU value - 0.00770001, GPU value - 0.00695852, abs diff - 0.00074149\r\nElement #31: CPU value - 0.377551, GPU value - 0.378358, abs diff - 0.000806987\r\nTotal 30 different elements, for output #0, threshhold - 0.0001\r\nCPU time - 0.034635ms\r\nGPU time(CPU->GPU->CPU) - 1.51333ms\r\n```", "@impjdi There is an optimization pass in the MLIR pass [`RemoveReshapeBeforeFullyConnected`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/optimize.cc#L1260-L1295) that causes the problem. So if I disable it by the patch below, the issue is resolved. \r\n\r\n```c++\r\ndiff --git a/tensorflow/compiler/mlir/lite/transforms/optimize.cc b/tensorflow/compiler/mlir/lite/transforms/optimize.cc\r\nindex 3553d0e5e9a..c2b6a958c2a 100644\r\n--- a/tensorflow/compiler/mlir/lite/transforms/optimize.cc\r\n+++ b/tensorflow/compiler/mlir/lite/transforms/optimize.cc\r\n@@ -1429,8 +1429,8 @@ void OptimizePass::runOnFunction() {\r\n   // Merge reshapes into fully connected ops before we start moving them past\r\n   // binary ops.\r\n   OwningRewritePatternList phase_0_patterns(&getContext());\r\n-  phase_0_patterns.insert<RemoveReshapeAfterFullyConnected,\r\n-                          RemoveReshapeBeforeFullyConnected>(ctx);\r\n+  phase_0_patterns.insert<RemoveReshapeAfterFullyConnected/*,\r\n+                          RemoveReshapeBeforeFullyConnected*/>(ctx);\r\n   (void)applyPatternsAndFoldGreedily(func, std::move(phase_0_patterns));\r\n \r\n   // Potentially the binary ops might be fused together, like hard_swish, thus\r\n@@ -1459,7 +1459,7 @@ void OptimizePass::runOnFunction() {\r\n       FuseBinaryOpToFollowingConv2D, FuseBinaryOpToFollowingDepthwiseConv2D,\r\n       FuseBinaryOpToFollowingFullyConnected, FuseConv2DAndMulWithQDQs,\r\n       FuseDepthwiseConv2DAndMulWithQDQs, ConvertTrivialTransposeOpToReshapeOp,\r\n-      RemoveReshapeAfterFullyConnected, RemoveReshapeBeforeFullyConnected,\r\n+      RemoveReshapeAfterFullyConnected, /*RemoveReshapeBeforeFullyConnected,*/\r\n       FuseUnpackAndConcatToReshape>(ctx);\r\n   if (enable_canonicalization_)\r\n     AddCanonicalizationPatterns(ctx, &phase_2_patterns);\r\n```\r\n\r\nSo the comment says:\r\n\r\n> Remove Reshape before FullyConnected when `keep_num_dims=false` and Reshape does not alter the last dimension as FullyConnected will collapse all other dimensions into a single dimension.\r\n\r\nHowever, this is not true (at least currently) for `FullyConnected` op on GPU delegate. But disabling the `RemoveReshapeBeforeFullyConnected` optimization pass is only a workaround. I guess this issue is really resolved only after the GPU delegate can handle such case.", "@Kipsora Thanks for digging deeper into this!  On behalf of TFLite & TFLite GPU, really appreciate it!\r\n\r\n@renjie-liu According to Jiacheng's findings, this optimization pass doesn't seem compatible with the GPU.  Can we drop this, or make this TAC specific, or find another solution?", "SG, I will file a bug to track this", "Meanwhile, can you try TAC: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/lite/experimental/tac\r\n\r\nthanks!"]}, {"number": 51404, "title": "Error to run the tflite model: self._interpreter.Invoke() RuntimeError: Input tensor 1283 lacks data", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.7.0-dev20210804\r\n\r\n### 2. Code\r\nThe code I use to convert the model and run the interpreter:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"mask_rcnn_coco_1204\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nfo = open(\r\n    \"mask_rcnn_inception_1024_int_quantized.tflite\", \"wb\")\r\nfo.write(tflite_model)\r\nfo.close\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"mask_rcnn_inception_1024_int_quantized.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\nHere is the [tflite model](https://drive.google.com/file/d/1yvvxl9iq2CpFqZ7_a4LMKZkpTppRJdGV/view?usp=sharing) that converted. \r\n### 3. Failure after conversion\r\nModel converting finished with no error, but the converted model couldn't run through the interpreter. \r\n\r\n### 4. Error traceback\r\n`Traceback (most recent call last):\r\n  File \"converter.py\", line 100, in <module>\r\n    interpreter.invoke()\r\n  File \"/home/dev/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 858, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Input tensor 1283 lacks data\r\n`\r\n\r\n_Originally posted by @JiashuGuo in https://github.com/tensorflow/tensorflow/issues/51209#issuecomment-895023730_", "comments": ["@JiashuGuo Thank you for the post! \r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) In order to expedite the trouble-shooting process of  the issue reported here. Thanks!", "> @JiashuGuo Thank you for the post!\r\n> Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) In order to expedite the trouble-shooting process of the issue reported here. Thanks!\r\n\r\nI updated the issue. ", "@abattery Do models with Flex ops work as-is in Python?", "Yes, it works", "> @abattery Do models with Flex ops work as-is in Python?\r\n\r\nHave you found any fixes for this? Is it because of mask_rcnn model has operations that don't support quantization? I found the mask_rcnn model doesn't work with quantization-aware training either.", "@JiashuGuo At a high level, may I know what is the output you desire from the MaskRCNN model? Is it just bounding boxes or segmentation masks as well?\r\n\r\nSome of the TF MaskRCNN versions aren't mobile friendly due to their source graph, which generates a lot of problems especially if you are considering quantization.", "I am expecting both bboxes and segment masks.\n\nOn Mon, Aug 23, 2021 at 9:06 AM Sachin Joglekar ***@***.***>\nwrote:\n\n> *External Email*\n>\n> @JiashuGuo <https://github.com/JiashuGuo> At a high level, may I know\n> what is the output you desire from the MaskRCNN model? Is it just bounding\n> boxes or segmentation masks as well?\n>\n> Some of the TF MaskRCNN versions aren't mobile friendly due to their\n> source graph, which generates a lot of problems especially if you are\n> considering quantization.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/51404#issuecomment-903910227>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AF72A22OHUQNG6TXSWAMPYTT6JWX7ANCNFSM5B3UTAXA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n", "I see. Where was this Mask RCNN model obtained from? I am not very familiar with the model structure and what kinds of I/O tensors it expects. Maybe try running a floating point model before trying quantized?", "> I see. Where was this Mask RCNN model obtained from? I am not very familiar with the model structure and what kinds of I/O tensors it expects. Maybe try running a floating point model before trying quantized?\r\n\r\nThe model is downloaded from [here](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1).  I also tried other versions in tf1 model zoo, but none works. So yeah, it seems now I can only run the float point model.", "To clarify: Does the floating point model work as expected?", "> To clarify: Does the floating point model work as expected?\r\n\r\nYes, it works.", "I see. That makese more sense. Adding @teijeong who might be able to dig deeper into why quantization isn't working for this model."]}, {"number": 51393, "title": "Slowdown of `tf.scan` operation in TF 2.5", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave Version 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nThere seems to be a considerable slowdown whenever a `tf.scan` operation is used inside the loss function of a model architecture and a learnable parameter (`self.transition_param` below) of the model architecture is part of the `tf.scan` op. Here's a code snippet to reproduce this:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\n\r\ndef gen_batches(num_batches, batch_size, units):\r\n\r\n    for _ in range(num_batches):\r\n        x = np.random.random((batch_size, 20, units))\r\n        y = np.random.randint(1, units, size=(batch_size, 20))\r\n        yield x, y\r\n\r\n\r\nclass MyModel(tf.keras.models.Model):\r\n\r\n    def __init__(self, units):\r\n        super().__init__()\r\n\r\n        self._tf_layers = {}\r\n\r\n        self.units = units\r\n\r\n        self.transition_param = self.add_weight(name=\"transition_param\", shape=(units, units))\r\n\r\n        self.optimizer = tf.keras.optimizers.Adam()\r\n        self._training = False\r\n\r\n    def _loss_fn_with_scan(self, inputs, transition_params):\r\n\r\n        first_input = tf.slice(inputs, [0, 0, 0], [-1, 1, -1])\r\n        first_input = tf.squeeze(first_input, [1])\r\n\r\n        rest_of_input = tf.slice(inputs, [0, 1, 0], [-1, -1, -1])\r\n\r\n        rest_of_input = tf.transpose(rest_of_input, [1, 0, 2])\r\n        transition_params = tf.expand_dims(transition_params, 0)\r\n\r\n        def _scan_fn(_state, _inputs):\r\n            _state = tf.expand_dims(_state, 2)\r\n            transition_scores = _state + transition_params\r\n            new_alphas = _inputs + tf.reduce_logsumexp(transition_scores, [1])\r\n            return new_alphas\r\n\r\n        all_alphas = tf.transpose(tf.scan(_scan_fn, rest_of_input, first_input), [1, 0, 2])\r\n        # add first state for sequences of length 1\r\n        all_alphas = tf.concat([tf.expand_dims(first_input, 1), all_alphas], 1)\r\n\r\n        return all_alphas\r\n\r\n    def _loss(self, x, y):\r\n\r\n        logits = tf.cast(x, dtype=tf.float32)\r\n\r\n        loss = self._loss_fn_with_scan(logits, self.transition_param)\r\n\r\n        return tf.reduce_mean(loss)\r\n\r\n    @tf.function\r\n    def train_on_batch(self, *args):\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            loss = self._loss(*args)\r\n        grads = tape.gradient(loss, self.trainable_weights)\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n        return loss\r\n\r\n    def train(self, epochs, batch_size, num_batches):\r\n\r\n        data_generator_iter = gen_batches(num_batches, batch_size, self.units)\r\n\r\n        sample_x, sample_y = next(data_generator_iter)\r\n\r\n        self.train_on_batch(sample_x, sample_y)\r\n\r\n        self._training = True\r\n\r\n        progress_bar = tqdm(range(epochs), desc=\"Epochs\")\r\n\r\n        for epoch in progress_bar:\r\n            for batch_x, batch_y in data_generator_iter:\r\n                loss = self.train_on_batch(batch_x, batch_y)\r\n\r\n            progress_bar.update(1)\r\n            progress_bar.set_postfix({\"loss\": f\"{loss.numpy():.3f}\"})\r\n\r\n\r\nnum_batches = 5000\r\nbatch_size = 32\r\nunits = 64\r\nepochs = 100\r\n\r\nmodel = MyModel(units)\r\nmodel.train(epochs, batch_size, num_batches)\r\n```\r\n\r\nThe same code takes  ~ 3 mins, 16 seconds on TF 2.3 . However, it takes ~ 4 mins and 1 second on TF 2.5 .\r\n\r\nThe above code is a stripped down version of a model architecture which uses transformers + CRF layer. The `_loss_fn_with_scan` is taken from [`crf_log_norm` function](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/text/crf.py#L157) of tensorflow-addons package but as the snippet shows, the problem is already visible without the CRF bits of the code and just using `tf.scan` as part of the loss function (`tf.scan` is part of tensorflow repo, hence the issue seems appropriate here). On large datasets, the slowdown is considerably large. For example, with TF 2.3 it took 1 hour 20 mins to complete the training and with TF 2.5 it takes close to 3 hours to complete the training which is a considerable increase and a big blocker for us to upgrade to TF 2.5.\r\n\r\n**Describe the expected behavior**\r\n\r\nTraining times should be comparable across TF 2.3 and TF 2.5 .\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): maybe\r\n- Briefly describe your candidate solution(if contributing): I am not familiar with the internals of `tf.scan` op. If there's some help available, we could give it a try.\r\n\r\n**Standalone code to reproduce the issue**\r\nA snippet is available above.", "comments": ["I am able to reproduce the issue in colab with CPU runtime, it took 2 minutes 12 seconds with `TF 2.3` and 2 minutes 56 seconds for `TF 2.5`\r\n\r\nPlease find the [gist here](https://colab.research.google.com/gist/sanatmpa1/54bf06a243d227c67188c77f1f867ca0/51393-cpu.ipynb). Thanks!", "Hi, \r\n\r\nThere's a lot going on in that example code. \r\n\r\nAre you able to minimize the code and isolate the problem this a little more?\r\n\r\nThat will make it easier for anyone investigating to track down what's going on. \r\n", "Hi @MarkDaoust the code snippet has the minimal stuff needed to demonstrate the problem:\r\n\r\n1. Dummy sample data\r\n2. Custom model with a custom training loop\r\n3. Custom loss function to use `tf.scan` op with a trainable param.\r\n\r\nAll of these are needed to demonstrate the problem.\r\n\r\nAnything that you suggest to get rid of?", "@sanatmpa1 @jvishnuvardhan Thanks for the repro. Could you please attach the full TensorFlow logs of the TF 2.3 run and TF 2.5 run, and also take the TensorFlow profile of a few training steps (https://www.tensorflow.org/guide/profiler#collect_performance_data)? This will help us isolate where the problem is (e.g., slow data processing, startup time, slow step time).", "@sanatmpa1 @jvishnuvardhan is there any update on this? Anything we can do to help? ", "Can you reproduce this on TF 2.6 or TF nightly.\r\nI've tried the Colab with TF 2.6 and It seems to freeze.", "@bhack,\r\n\r\nI've updated the [gist](https://colab.research.google.com/gist/sanatmpa1/54bf06a243d227c67188c77f1f867ca0/51393-cpu.ipynb#scrollTo=tgqM12gmpM5u) with results from TF 2.6 and TF nightly. Here's a comparison of results,\r\n\r\n2.3      - `0:02:12.134876`\r\n2.5      - `0:02:47.300976`\r\n2.6      - `0:01:37.283745`\r\nnightly -`0:02:46.074507`", "@sanatmpa1 are you able to respond to @zongweiz request for more profiling information in this gist? \r\n\r\nHere's an example of how you can add an [embedded TB profile run](https://colab.research.google.com/gist/yarri-oss/8d95ef510f6d1e88ef584d57617f57fb#scrollTo=TzD69-eA-K0R) to the 2.5 training. Thanks.", "@sanatmpa1 I suppose that these numbers, other then for 2.6, are wrong.\r\nCan you add this in your Colab to see if you have visible GPUs on all your tested versions?\r\n\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(\"Num GPUs:\", len(physical_devices))\r\n```\r\n\r\n", "@bhack @yarri-oss I can already see a small slowdown in a simple snippet with a single `tf.scan` operation.\r\n\r\nSnippet:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom time import time\r\nimport numpy as np\r\n\r\nstart = time()\r\n\r\nelems = np.random.random((20000))\r\nsum = tf.scan(lambda a, x: a+x, elems)\r\nend = time()\r\nprint(end - start)\r\n```\r\n\r\nThe snippet is taken from the [docs page](https://www.tensorflow.org/api_docs/python/tf/scan) of `tf.scan` op.\r\n\r\nRuntime for TF 2.3: ~1.18s\r\nRuntime for TF 2.5: ~1.3s\r\n\r\n**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave Version 10.14.5\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\nPython version: 3.7.5\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: -\r\nGPU model and memory: -\r\n\r\nThis is a 10% slowdown which is almost in line with our larger experiments where we see an average slowdown of 18% in prediction time of our models using a `tf.scan` op internally. So, this is definitely one of the contributing factors to the overall slowdown.\r\n\r\nHope this is useful for further debugging and helps in making progress on it.", "@dakshvar22 Can you test the same with TF nightly?", "@bhack `tf-nightly` is quite a lot slower! On the same system environment, the snippet runs in approximately 1.55s \ud83e\udd2f ", "Can you verify that you are placing the ops in the same device on all your tested version with this:\n\nhttps://www.tensorflow.org/api_docs/python/tf/debugging/set_log_device_placement", "I don't have a GPU device on the system, so it should be using just the CPU device for all tested versions?", "@bhack I verified this by adding this line to the top of the snippet\r\n\r\n```\r\ntf.debugging.set_log_device_placement(True)\r\n```\r\n\r\nI see identical logs like this (truncated) on all versions:\r\n\r\n```\r\n....\r\n2021-09-08 18:45:58.853542: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853578: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853653: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853689: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853751: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853789: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853851: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853886: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.853983: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LogicalAnd in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-08 18:45:58.870290: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op Pack in device /job:localhost/replica:0/task:0/device:CPU:0\r\n```", "Hi - I am seeing improvements in TF 2.6 per this [gist](https://gist.github.com/yarri-oss/c3cd93f3c45606654bfa60241fd8e514), I agree it would be nice to add a `tf.scan` performance test to monitor any future regressions.", "Hi @yarri-oss, I don't see this improvement, when I run all three versions 2.5 and 2.6 are slower than 2.3, see this [gist](https://gist.github.com/koernerfelicia/3c9a3b3ba6f23914afe9cec309a6d41b#file-notebook-ipynb)", "I've tested this on my local system with  TF-nighlty TF 2.6.0 TF 2.5.0:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom time import time\r\nimport numpy as np\r\nfrom pyinstrument import Profiler\r\n\r\nprint(\"version {}\".format(tf.__version__))\r\nnp.random.seed(0) \r\ninput_test = np.random.random((20000))\r\nfor i in range(30):\r\n    tf.scan(lambda a, x: a+x, input_test)\r\n```\r\nYou can see here some of the core operations:\r\n\r\n![immagine](https://user-images.githubusercontent.com/1710528/133339191-c35d5910-6f18-4ec4-a3fe-c0c42d00a739.png)\r\n\r\nI don't see too much difference between 2.5 and 2.6. I see a slower `add_v2`  in  `nightly` and probably a little bit of overhead with the new `error_handler` filtering machinery that we have introduced in master.\r\n\r\nP.s. This is in eager mode as your last two gist are in eager mode\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@bhack, have you compared this to 2.3? I also don't see much difference between 2.5 and 2.6, but a significant difference from 2.3 to 2.5", "I don't have the env ready for 2.3 now but you can post your graph with https://pyinstrument.readthedocs.io/en/latest/.\r\nYou need to use `-r html` for the interactive output so the you can take a screenshot of the critical section as I've done in the previous post.\r\n\r\n", "2.3:\r\n<img width=\"932\" alt=\"Screenshot 2021-09-29 at 09 02 49\" src=\"https://user-images.githubusercontent.com/45405119/135219257-67fdcd1d-f404-4133-bb25-3f277b2033e0.png\">\r\n\r\nhtml [here](https://gistpreview.github.io/?31109fd77a36bd4722e52e271d2998c5)\r\n\r\n2.5:\r\n<img width=\"938\" alt=\"2_5\" src=\"https://user-images.githubusercontent.com/45405119/135220112-98eacf4b-0be8-49ec-9b37-983ad7757921.png\">\r\n\r\nhtml [here](https://gistpreview.github.io/?6c8af2a23a0a6d56bad2e60e16d5b924)\r\n\r\n2.6:\r\n<img width=\"934\" alt=\"2_6\" src=\"https://user-images.githubusercontent.com/45405119/135220119-e6ff902d-242f-427f-be90-d112eb62f6a9.png\">\r\n\r\nhtml [here](https://gistpreview.github.io/?fb86e9e63ba8f55ed04b9b12e88b9a0c)\r\n\r\n", ">Hi @yarri-oss, I don't see this improvement, when I run all three versions 2.5 and 2.6 are slower than 2.3, see this gist\r\n\r\n@yarri-oss did you get a chance to look at the gist I shared above?", "Thanks, yes in eager mode it seems that `add_v2` is slower than `2.3`. Is this kernel using MLIR now?", "Hi, all!  Just wanted to let you know we're investigating -- no conclusions yet, but I have a couple leads I'm following.  I'll post back when I have more.  Thanks for the info so far!", "Do you still see a regression if you don't use `persistent=True` (doesn't seem like you are computing any higher order gradients so functionally your model should be equivalent)? Please ignore if you actually rely on a persistent tape in your original model.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Reopening as there is activity", "There is a bug in the bot. 16 days old stalled label was not removed with all these comments.", "Thanks Mihai!  Makes a convenient time to check in again, though -- so while I'm here:\r\n\r\nWe've identified the most likely cause of the slowdown is probably this one that appears in 2.5.0:\r\nhttps://github.com/tensorflow/tensorflow/commit/07b75ffa453b1ec9189371244d21b6684503340b\r\n\r\nThat lines up with Saurabh's intuition -- while that change was needed for correctness, it's only for higher order derivatives.  If you don't need persistence for another reason, you can turn it off 1st-order gradients.  We believe that should essentially avoid the associated slowdown and get performance back up in newer releases.\r\n\r\nHope that helps -- just let us know!\r\n", "Thanks @jsmeredith! We have started the performance tests to see if this helps in training time of our models. Will update here as soon as they end.\r\n\r\nWe do need `persistent=True` because we are using the gradient tape to compute the gradient of prediction loss and regularization loss w.r.t trainable variables separately in two different calls. This is needed because we nullify the regularization gradient for those trainable variables for which prediction gradient is zero (think of dropout, sparsity in weights matrices). We found in the past that it helped better train our model architectures which are highly sparse in nature.\r\n\r\nGiven that the above change is affecting the performance of certain TF Ops, is there a workaround (other than removing `persistent=True`) or a bugfix in plan?", "> There is a bug in the bot. 16 days old stalled label was not removed with all these comments.\r\n\r\nBot looks for only reply from issue author.", "> Bot looks for only reply from issue author.\r\n\r\nI think that it is a too narrow policy. Probably we could just exclude TF org members comments.\r\n", "@dakshvar22  Thanks for the info!  I'll do a little digging and see what ideas folks have.  \r\n\r\nAlso, if you have performance for the other versions in the 2.3-2.6 range, it sounded like might have been smaller -- but noticeable -- drops in versions other than with 2.5?  This high-order/persistence issue stood out when scanning, and it appears to line up with 2.5.0, but any other differences would be good to know in case a different/additional culprit is affecting you.\r\n", "Hi everybody, I work with @dakshvar22 and @koernerfelicia on the problem and have done some measurements on CPU:\r\n\r\n![image](https://user-images.githubusercontent.com/1809832/142467192-87ad7618-9441-4891-99f1-5b9f21140a92.png)\r\n\r\nHere are some explanations about this image:\r\n* The meaning of the **env**  column is mostly the tensorflow version:\r\n    * 2.3-env: TF 2.3\r\n    * 2.5-env: TF 2.5\r\n    * 2.6-env: TF 2.6\r\n    * 2.6-env-new: TF. 2.6 with the suggested gradient tape `persistent=False` change (https://github.com/RasaHQ/rasa/pull/9867)\r\n* `carbon-bot` and `financial-demo` are just 2 different internal dataset I measured on.\r\n* We measured on the different configs that might be affected by the slowdown\r\n* You can see the measured training time (columns: `total train`, `diet train`) in seconds\r\n* The relative speedups are just inferred values, they correspond to the respective row for `2.3-env`\r\n\r\nWhat we learn from this:\r\n1. The change brings training times back to what we had for TF 2.3 version\r\n2. However, this should not be considered a fix of the original root cause of the problem.\r\n3. Inference time is still taking a hit. This is most likely because of the `tf.scan` operation being slow\r\n\r\nHope this helps with your investigation @jvishnuvardhan @jsmeredith . We are also planning to do some measurements on GPU", "Hi @jsmeredith .. Is this still something you are looking into? Did you get a chance to look at the [above observations](https://github.com/tensorflow/tensorflow/issues/51393#issuecomment-973114987)?", "Greetings!  Sure, let me take another look -- I wasn't entirely sure if I was awaiting GPU data .  (I also admit I'm making heavy use of other folks' expertise on this one, and alas some of those people are now on different teams over the holidays.)  The timing is good though, I'm just wrapping up one project so I have a little time to see what we can do here.", "Hi @jsmeredith, do you have any updates on this issue? Would it be helpful to get measurements on GPU?"]}, {"number": 51354, "title": "Out-of-memory error in eager mode loop", "body": "Following code gives an out-of-memory error when running in tensorflow-2.5. The code itself does not do anything useful it is just meant to highlight what I believe is a problem.\r\n\r\nFor the small problem (i.e. ```b, a, c = 1, 1000, 5 # small problem```) the program completes. For the large problem (i.e. ```b, a, c = 1, 10000, 5 # large problem```) the program runs out of memory.\r\n\r\nMy expectation would have been that in eager mode allocated tensors have the lifetime of one iteration, while it seems more memory is being allocated with each iteration. Similar code with identical parameters runs in Pytorch.\r\n\r\nAm I wrong in my expectation or is this a bug? If I am wrong how could I do this in Tensorflow?\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.losses import Loss\r\n\r\nclass EagerOom(Loss):\r\n\r\n    def __init__(self):\r\n        super(EagerOom, self).__init__()\r\n\r\n    def __call__(self, targets, logits):\r\n        logits = tf.reshape(logits, [-1])\r\n        targets = tf.reshape(targets, [-1])\r\n        return self.inner(targets, logits)\r\n\r\n\r\n    @tf.custom_gradient\r\n    def inner(self, targets, logits):\r\n\r\n        g_logits = tf.zeros_like(logits)\r\n\r\n        for idx in range(len(targets)):\r\n            print(f\"idx: {idx}\")\r\n            g_logits_upd = logits - logits[idx]\r\n            g_logits += g_logits_upd\r\n        \r\n        loss = tf.reduce_mean(targets * logits)\r\n\r\n        def grad_fn(upstream):\r\n            g_targets = tf.zeros_like(targets)\r\n            return g_targets, g_logits\r\n\r\n        return loss, grad_fn\r\n\r\n\r\n\r\ndef main():\r\n    tf.random.set_seed(0)\r\n    # b, a, c = 1, 1000, 5 # small problem\r\n    b, a, c = 1, 10000, 5 # large problem\r\n    logits = tf.random.normal((b, a, c))\r\n    targets = tf.random.uniform((b, a), 0, c + 1, dtype=tf.int32)\r\n    targets = tf.one_hot(targets, depth=c, axis=-1)\r\n    loss_fn = EagerOom()\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        tape.watch(logits)\r\n        loss = loss_fn(targets, logits)\r\n    print(\"Done.\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n", "comments": ["Have you considered limiting gpu memory growth?\r\nMake sure you close all python sessions and exit python interpreter to free up memory and try running script as:\r\n\r\n```\r\n#On top of your script\r\nimport tensorflow as tf\r\ngpu = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory\r\n# Rest of your code\r\n```", "Thanks for your suggestion I tried it out. I don't see any change, it still runs out of memory.", "@Saduf2019 ,\r\nI was able to reproduce the issue in tf 2.4,v2.5 and tf-nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/002f2769f9e5f67886c67f0e29268e92/untitled51354.ipynb).", "@esghif \r\nCan you lower your [batch size](https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-405587015), also try  kill -9 PID and run again and let us know.", "Batch size is 1. OOM persists.", "@esghif \r\nCheck with nvidia-smi. [this is not a tf issue, can u close it here and create in discussion forum as there are more supporters there to suggest]\r\n\r\ncolab_00_introduction.ipynb - Colaboratory\r\nhttps://colab.research.google.com/github/drinkingkazu/2019-06-17-NeuralNets/blob/master/Preparation%20-%205%20Minutes%20Colaboratory.ipynb", "Is there any chance that someone will address the issue I am raising? The loop in the example should not lead to an OOM error irrespective of the number of iterations.", "@esghif Is this still an issue for you? \r\n\r\nI ran your code and didn't face any OOM error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/4bc239f61ac1c3654243072abf7086ec/untitled51354.ipynb). Thanks!", "Hi, jvishnuvardhan@,\r\n\r\nThanks for trying this out again. I think the principal issue still applies --\r\n\r\nAs surprising as it sounds, even if you do a custom_gradient(), the operations are still recorded on the tape by TensorFlow.\r\nThis is not documented in custom_gradient()'s docstring. A change is easy to implement (and I poked around with one or two weeks ago) -- but our current consensus is that we unlikely have to freedom to change the behavior of custom_gradient due to TF's backward compatibility promises (e.g. some may rely on automated high order gradients). \r\n\r\nAs a workaround, you can probably stop the recording by calling tf.stop_gtradient to all arguments of the function with custom_gradient:\r\n\r\n```\r\n    @tf.custom_gradient\r\n    def inner(self, targets, logits):\r\n        targets, logits = tf.stop_gradient([targets, logitcs])\r\n        ...\r\n        return loss, grad_fn\r\n```\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi *, I tried it out again with tf 2.6 and 2.8.0-dev20211104. Both led to an OOM error as previously. @jvishnuvardhan when b=1000 is the small problem that worked previously also. For b=10000 it still leads to an OOM error."]}, {"number": 51352, "title": "Is it possible to compile tflite-runtime python wheel on m1 mac (arm64)?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Big Sur version 11.4\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: v2.6.0, v2.5.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): 4.1.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 12.0.5 (clang-1205.0.22.11)\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory:\t\r\n```\r\n16 GB, Type:\tLPDDR4\r\nChipset Model:\tApple M1\r\n  Type:\tGPU\r\n  Bus:\tBuilt-In\r\n  Total Number of Cores:\t8\r\n  Vendor:\tApple (0x106b)\r\n  Metal Family:\tSupported, Metal GPUFamily Apple 7 \r\n```\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried several following ways of tflite-runtime compilation to python wheel:\r\n\r\n`sh build_pip_package_with_cmake.sh native`\r\n\r\ngetting:\r\n```\r\nCMake Error at /opt/homebrew/Cellar/cmake/3.21.1/share/cmake/Modules/CMakeTestCCompiler.cmake:69 (message):\r\n  The C compiler\r\n\r\n    \"/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc\"\r\n\r\n  is not able to compile a simple test program.\r\n\r\n  It fails with the following output:\r\n\r\n    Change Dir: /Users/koubadom/Projects/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/CMakeFiles/CMakeTmp\r\n\r\n    Run Build Command(s):/usr/bin/make -f Makefile cmTC_4563f/fast && /Applications/Xcode.app/Contents/Developer/usr/bin/make  -f CMakeFiles/cmTC_4563f.dir/build.make CMakeFiles/cmTC_4563f.dir/build\r\n    Building C object CMakeFiles/cmTC_4563f.dir/testCCompiler.c.o\r\n    /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc   -march=native -I/Users/koubadom/miniforge3/include/python3.8 -I/Users/koubadom/miniforge3/lib/python3.8/site-packages/pybind11/include  -arch arm64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.3.sdk -MD -MT CMakeFiles/cmTC_4563f.dir/testCCompiler.c.o -MF CMakeFiles/cmTC_4563f.dir/testCCompiler.c.o.d -o CMakeFiles/cmTC_4563f.dir/testCCompiler.c.o -c /Users/koubadom/Projects/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/CMakeFiles/CMakeTmp/testCCompiler.c\r\n    clang: error: the clang compiler does not support '-march=native'\r\n    make[1]: *** [CMakeFiles/cmTC_4563f.dir/testCCompiler.c.o] Error 1\r\n    make: *** [cmTC_4563f/fast] Error 2\r\n\r\n\r\n\r\n\r\n\r\n  CMake will not be able to correctly generate this project.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:40 (project)\r\n```\r\n\r\n`sh build_pip_package_with_cmake.sh arm64`\r\n\r\n```\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[3]: *** [_pywrap_tensorflow_interpreter_wrapper.dylib] Error 1\r\nmake[2]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2\r\nmake: *** [_pywrap_tensorflow_interpreter_wrapper] Error 2\r\n```\r\n`sh build_pip_package.sh`\r\n\r\n```\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [/Users/koubadom/Projects/tensorflow/tensorflow/lite/tools/make/gen/osx_arm64/bin/benchmark_model_performance_options] Error 1\r\nTraceback (most recent call last):\r\n```\r\n\r\n`sh build_pip_package_with_bazel.sh`\r\n\r\n```\r\n/private/var/tmp/_bazel_koubadom/4a39adbd2fff0dadf31cbeb460f8aca3/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Configuration Error: Invalid python library path: /usr/lib/python3/dist-packages\r\n```\r\n(I know this looks like some env var is badly set, but I tried everything what come to my mind.) I modified version of bazel in .bazelversion to match 4.1.0.\r\n\r\n`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON38 \\\r\n  tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh`\r\n\r\n```\r\n I got multiple errors especially with `ModuleNotFoundError: No module named 'pybind11'` and bazel version, I do know why, pybind11 is not there, I am sure it is on my local machine, how not in container.\r\n```\r\n\r\n\r\n\r\nAll attempts I did on git checkout v2.5.0 and everything is run under arm (not in Rosetta). Is here anybody who was successful doing this obscure task? I might made a lot of mistakes but I am trying hard... If somebody can give me a hint or compiled version for arm without any advice, I am ok with that :D. I would appreciate any version on my platform. \r\n\r\nThanks a lot. ", "comments": ["For Bazel build, you need to provide `--macos_cpus=arm64`\r\nSo the following command might work.\r\n```\r\n$ CUSTOM_BAZEL_FLAGS=--macos_cpus=arm64 build_pip_package_with_bazel.sh\r\n```\r\n\r\nFor CMake build, you need to use it as following.\r\n```\r\n$ cmake -Bbuild/xcode -DCMAKE_APPLE_SILICON_PROCESSOR=arm64 <TFLite_dir>\r\n```\r\nBut the current `tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh` doesn't have a way to provide a custom flag. You need to modify the script directly.\r\n", "Thanks a lot, this make the first part of the script run. On the other hand in this part:\r\n```\r\ncmake --build . \\\r\n     --verbose -j ${BUILD_NUM_JOBS} -t _pywrap_tensorflow_interpreter_wrapper\r\n```\r\nI am getting:\r\n\r\n```\r\n/Users/koubadom/Projects/tensorflow/tensorflow/lite/python/interpreter_wrapper/numpy.h:51:10: fatal error: 'numpy/arrayobject.h' file not found\r\n```\r\nnumpy is installed on my system", "If you're using `tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh` you'd better check if `${PYTHON_INCLUDE}` and `${PYBIND11_INCLUDE}` are defined properly.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh#L30\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh#L114", "Another step, thanks. However, I had to misconfigure something:\r\n```\r\n/Users/koubadom/miniforge3/lib/python3.8/site-packages/numpy/core/include/numpy/npy_common.h:388:9: error: unknown type name 'Py_hash_t'\r\ntypedef Py_hash_t npy_hash_t;\r\n```\r\nI am using miniforge3 on mac and I do not know if it is not the problem. I had to modify \"-I...\" parameters to include numpy, pybind and everything as you recommended (thx). But this error is very suspicious...\r\nAny idea? I cannot find where this type is defined...\r\n\r\nI am sorry for the questions related to my setup more than to TF, thank you for your time.", "Is anyone following up on this issue? And any plan to ship out-of-the-box arm64 wheel for macOS?\r\n\r\nThanks to the above workaround, I got past the first part of the script. I also passed the compilation stage when I ran `cmake --build . --verbose -j ${BUILD_NUM_JOBS} -t _pywrap_tensorflow_interpreter_wrapper`. But the same  `symbol(s) not found for architecture arm64` error occurred during the linking stage. Setting the environment variable `CMAKE_APPLE_SILICON_PROCESSOR=arm64` didn't seem to help.\r\n\r\n```\r\n...\r\n[100%] Linking CXX shared library _pywrap_tensorflow_interpreter_wrapper.dylib\r\n/opt/homebrew/Cellar/cmake/3.21.4/bin/cmake -E cmake_link_script CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/link.txt --verbose=1\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -I/Users/.../.pyenv/versions/3.9.7/include/python3.9 -I/Users/.../.pyenv/versions/3.9.7/lib/python3.9/site-packages/pybind11/include -O3 -DNDEBUG -arch arm64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.0.sdk -dynamiclib -Wl,-headerpad_max_install_names -o _pywrap_tensorflow_interpreter_wrapper.dylib -install_name @rpath/_pywrap_tensorflow_interpreter_wrapper.dylib CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/interpreter_wrapper.cc.o CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/interpreter_wrapper_pybind11.cc.o CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/numpy.cc.o CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/python_error_reporter.cc.o CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/python_utils.cc.o  libtensorflow-lite.a _deps/abseil-cpp-build/absl/flags/libabsl_flags.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_internal.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_marshalling.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_reflection.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_config.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_program_name.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_private_handle_accessor.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_commandlineflag.a _deps/abseil-cpp-build/absl/flags/libabsl_flags_commandlineflag_internal.a _deps/abseil-cpp-build/absl/container/libabsl_raw_hash_set.a _deps/abseil-cpp-build/absl/container/libabsl_hashtablez_sampler.a _deps/abseil-cpp-build/absl/base/libabsl_exponential_biased.a _deps/abseil-cpp-build/absl/hash/libabsl_hash.a _deps/abseil-cpp-build/absl/hash/libabsl_city.a _deps/abseil-cpp-build/absl/hash/libabsl_wyhash.a _deps/abseil-cpp-build/absl/status/libabsl_status.a _deps/abseil-cpp-build/absl/strings/libabsl_cord.a _deps/abseil-cpp-build/absl/types/libabsl_bad_optional_access.a _deps/abseil-cpp-build/absl/strings/libabsl_str_format_internal.a _deps/abseil-cpp-build/absl/synchronization/libabsl_synchronization.a _deps/abseil-cpp-build/absl/debugging/libabsl_stacktrace.a _deps/abseil-cpp-build/absl/debugging/libabsl_symbolize.a _deps/abseil-cpp-build/absl/debugging/libabsl_debugging_internal.a _deps/abseil-cpp-build/absl/debugging/libabsl_demangle_internal.a _deps/abseil-cpp-build/absl/synchronization/libabsl_graphcycles_internal.a _deps/abseil-cpp-build/absl/base/libabsl_malloc_internal.a _deps/abseil-cpp-build/absl/time/libabsl_time.a _deps/abseil-cpp-build/absl/strings/libabsl_strings.a _deps/abseil-cpp-build/absl/strings/libabsl_strings_internal.a _deps/abseil-cpp-build/absl/base/libabsl_throw_delegate.a _deps/abseil-cpp-build/absl/base/libabsl_base.a _deps/abseil-cpp-build/absl/base/libabsl_spinlock_wait.a _deps/abseil-cpp-build/absl/numeric/libabsl_int128.a _deps/abseil-cpp-build/absl/time/libabsl_civil_time.a _deps/abseil-cpp-build/absl/time/libabsl_time_zone.a -framework CoreFoundation _deps/abseil-cpp-build/absl/types/libabsl_bad_variant_access.a _deps/abseil-cpp-build/absl/base/libabsl_raw_logging_internal.a _deps/abseil-cpp-build/absl/base/libabsl_log_severity.a _deps/farmhash-build/libfarmhash.a _deps/fft2d-build/libfft2d_fftsg2d.a _deps/fft2d-build/libfft2d_fftsg.a _deps/flatbuffers-build/libflatbuffers.a _deps/ruy-build/libruy.a _deps/xnnpack-build/libXNNPACK.a /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.0.sdk/usr/lib/libm.tbd cpuinfo/libcpuinfo.a clog/libclog.a pthreadpool/libpthreadpool.a\r\nUndefined symbols for architecture arm64:\r\n  \"_PyBaseObject_Type\", referenced from:\r\n      pybind11::detail::make_object_base_type(_typeobject*) in interpreter_wrapper_pybind11.cc.o\r\n  \"_PyBuffer_Release\", referenced from:\r\n      pybind11::buffer_info::~buffer_info() in interpreter_wrapper_pybind11.cc.o\r\n  \"_PyBytes_AsString\", referenced from:\r\n      bool pybind11::detail::string_caster<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, false>::load_bytes<char>(std::__1::enable_if<std::is_same<char, char>::value, pybind11::handle>::type) in interpreter_wrapper_pybind11.cc.o\r\n...\r\n      bool pybind11::detail::argument_loader<tflite::interpreter_wrapper::InterpreterWrapper&, int, pybind11::handle&, bool, int>::load_impl_sequence<0ul, 1ul, 2ul, 3ul, 4ul>(pybind11::detail::function_call&, std::__1::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul>) in interpreter_wrapper_pybind11.cc.o\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[3]: *** [_pywrap_tensorflow_interpreter_wrapper.dylib] Error 1\r\nmake[2]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2\r\nmake: *** [_pywrap_tensorflow_interpreter_wrapper] Error 2\r\n```", "Are there any plans to release an arm64 wheel directly from tensorflow?  If so can I get the issue#?  I'd like to use it in Jupyters tensorflow-notebook (https://hub.docker.com/r/jupyter/tensorflow-notebook).  I believe it's the only remaining issue left and would allow creation of an (arm64) Jupyter tensorflow docker image.  \r\n\r\nFor details: https://github.com/jupyter/docker-stacks/pull/1446#discussion_r696068862 and https://github.com/jupyter/docker-stacks/pull/1446#issuecomment-905842644\r\n"]}, {"number": 51350, "title": "TensorArray Can't be used for tf.concat?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf2.3\r\n- Python version: python 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nI want to use model.fit but in my model definition, I use python list and for loop. So I want to change it by using TensorArray.\r\nbut I have tf.concat after for loop. So I test following code to see if it works:\r\n```python\r\na = tf.ones([2,3])\r\nb = [a] * 2\r\n\r\nta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\nta = ta.write(0, a)\r\nta = ta.write(1, a)\r\nta = ta.stack()\r\n\r\nd1 = tf.concat(b, axis=1)\r\nprint(\"d1:\", d1)\r\n\r\nd2 = tf.concat(ta, axis=1)\r\nprint(\"d1:\", d2)\r\n\r\nBut  I get these results:  \r\n------------------------------------------------\r\nd1: tf.Tensor(\r\n[[1. 1. 1. 1. 1. 1.]\r\n [1. 1. 1. 1. 1. 1.]], shape=(2, 6), dtype=float32)\r\n\r\nd1: tf.Tensor(\r\n[[[1. 1. 1.]\r\n  [1. 1. 1.]]\r\n\r\n [[1. 1. 1.]\r\n  [1. 1. 1.]]], shape=(2, 2, 3), dtype=float32)\r\n------------------------------------------------\r\n**Describe the expected behavior**\r\nI want TensorArray have the same shape as result of \"d1\", but nothing changed. It's a bug?\r\nd1: tf.Tensor(\r\n[[1. 1. 1. 1. 1. 1.]\r\n [1. 1. 1. 1. 1. 1.]], shape=(2, 6), dtype=float32)\r\n\r\nIF not bug, please tell me how to change TensorArray shape with tf.concat? Thanks\r\n```\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n", "comments": ["@ymzlygw In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @ymzlygw In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\n### There is no error messages but only wrong result. Code is simple as:\r\n\r\nimport tensorflow as tf\r\na = tf.ones([2,3])\r\nb = [a] * 2\r\n\r\nta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\nta = ta.write(0, a)\r\nta = ta.write(1, a)\r\nta = ta.stack()\r\n\r\nd1 = tf.concat(b, axis=1)\r\nprint(\"d1:\", d1)\r\n\r\nd2 = tf.concat(ta, axis=1)\r\nprint(\"d1:\", d2)\r\n\r\n\r\nAnd I find that I can use [TensorArray.concat() +  tf.reshape(ta, shape= [X,X])]  to achieve the correct the result, althong a little bad because you should caculate the resize_shape before. But I want TensorArray to do it simple , which is  my purpose.", "Hi @ymzlygw,\r\n\r\nYou're using `tf.concat` over a `TensorArray`, and `tf.concat` seems only to support lists (see [documentation](https://www.tensorflow.org/api_docs/python/tf/concat)). Instead, you can use [`TensorArray.concat()`](https://www.tensorflow.org/api_docs/python/tf/TensorArray#concat), i.e. call `ta.concat():`\r\n\r\n```import tensorflow as tf\r\na = tf.ones([2,3])\r\nb = [a] * 2\r\n\r\nta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\nta = ta.write(0, a)\r\nta = ta.write(1, a)\r\nta = ta.stack()\r\n\r\nd1 = tf.concat(b, axis=1)\r\nprint(\"d1:\", d1)\r\n\r\nd2 = ta.concat()\r\nprint(\"d1:\", d2)\r\n```\r\n\r\nLet me know if this resolves the issue, thanks.", "> Hi @ymzlygw,\r\n> \r\n> You're using `tf.concat` over a `TensorArray`, and `tf.concat` seems only to support lists (see [documentation](https://www.tensorflow.org/api_docs/python/tf/concat)). Instead, you can use [`TensorArray.concat()`](https://www.tensorflow.org/api_docs/python/tf/TensorArray#concat), i.e. call `ta.concat():`\r\n> \r\n> ```\r\n> a = tf.ones([2,3])\r\n> b = [a] * 2\r\n> \r\n> ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n> ta = ta.write(0, a)\r\n> ta = ta.write(1, a)\r\n> ta = ta.stack()\r\n> \r\n> d1 = tf.concat(b, axis=1)\r\n> print(\"d1:\", d1)\r\n> \r\n> d2 = ta.concat()\r\n> print(\"d1:\", d2)\r\n> ```\r\n> \r\n> Let me know if this resolves the issue, thanks.\r\n\r\nThanks for your reply , yeah,  as  I said above: \"\"\" I find that I can use [TensorArray.concat() + tf.reshape(ta, shape= [X,X])] to achieve the correct the result( as tf.concat(list) do ), but it's a little bad because you should caculate the resize_shape before. And in factly I want that TensorArray can do it itself ,  Maybe developer can develop it to support this?\"\"\"\r\n", "@Saduf2019  Was able to replicate the issue in TF v2.3,2.4 & 2.5 , please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/861bf61854b276b163b2f68b23cb5f78/untitled365.ipynb#scrollTo=HPDGc-XGqzHo) for reference.Thank you!"]}, {"number": 51348, "title": "[GPU DELEGATE] [2.5.0] Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 12x4", "body": "System information\r\n\r\nOS Platform and Distribution: Android all possible versions\r\nTensorFlow version : 2.5.0\r\nDescribe the current behavior\r\nGpu delegate failed to initialize with model\r\n\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: MUL: Expected a 3D tensor of shape HxWxC or a 4D tensor of shape 1xHxWxC but got 12x4 TfLiteGpuDelegate Prepare: delegate is not initialized Node number 243 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nimplementation(\"org.tensorflow:tensorflow-lite:2.5.0\")\r\nimplementation(\"org.tensorflow:tensorflow-lite-gpu:2.5.0\")\r\n", "comments": ["@MaybeAndroid In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "> @MaybeAndroid In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\n` \r\n    val options = Interpreter.Options().apply{\r\n        if(compatList.isDelegateSupportedOnThisDevice){\r\n            val delegateOptions = compatList.bestOptionsForThisDevice\r\n            this.addDelegate(GpuDelegate(delegateOptions))\r\n        } else {\r\n            this.setNumThreads(4)\r\n        }\r\n    }\r\n    val interpreter = Interpreter(model, options)\r\n`\r\n\r\nAll according this article - https://www.tensorflow.org/lite/performance/gpu \r\nIt's happens on Initialization of Interpreter\r\n\r\nAlso I found possible the same issue on 2.4.0 version as I have \r\nhttps://github.com/tensorflow/tensorflow/issues/45845", "@impjdi could you take a look?", "looks like the MUL is malformed with bad dimensions.  If you have A MUL B, then the dimensions of A and B must match.", "> looks like the MUL is malformed with bad dimensions. If you have A MUL B, then the dimensions of A and B must match.\r\n\r\nAccording to the advice in #45845 thread I checked 2.3.0 and 2.2.0 version of gpu delegate and they are working well with the model. I'm not familiar with internal structure of calculation, it looks like the 2.5.0 is corrupted."]}, {"number": 51346, "title": "tf.data.Iterator:  TypeError: object() takes no parameters", "body": "tf 2.5\r\n[tf.data.Iterator](https://tensorflow.google.cn/api_docs/python/tf/data/Iterator?hl=en)\r\n\r\n```\r\ndataset = tf.data.Dataset.range(10)\r\niterator = tf.data.Iterator(dataset)\r\nprint(iterator.get_next())\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test6.py\", line 32, in <module>\r\n    iterator = tf.data.Iterator(dataset)\r\nTypeError: object() takes no parameters\r\n```\r\n", "comments": ["@sjtusmartboy,\r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) to expedite the troubleshooting process. Thanks!", "@sanatmpa1 try the code and you find it", "I am able to reproduce the issue in colab, Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/9e778630d765279bed8c1a70b6ff289e/51346.ipynb).\r\n\r\n", "@sjtusmartboy,\r\n\r\nIf you want to iterate through `dataset`, Can you try using the python iterator protocol `iter()` and it works fine as added in the above gist.\r\n\r\nYou need to replace the second line, \r\n`iterator = tf.data.Iterator(dataset)` with `iterator = iter(dataset)` and let us know if it helps. Thanks!", "@sanatmpa1 thanks, it seems tf.data.Iterator is of no use at all because you are using the python iter. If not, please teach me how to use tf.data.Iterator."]}, {"number": 51344, "title": "Possible bug with session handle as feed dict input", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version:Python 3.8.5 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0 (nvcr.io/nvidia/tensorflow:21.03-tf2-py3)\r\n- GPU model and memory: TeslaT4\r\n\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2021-08-06 02:34:51.763077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nunknown 2.4.0\r\n```\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nBelow is the minimum reproduce code snippet derived from \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/v1_compat_tests/session_ops_test.py#L249\r\n\r\n```python\r\n\"\"\"Tests for tensorflow.ops.session_ops.\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import session_ops\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nimport tensorflow as tf\r\nimport sys\r\nimport time\r\n\r\ntf.debugging.set_log_device_placement(True)\r\n\r\ndef testFeedOneHandleDirectly():\r\n  config = tf.compat.v1.ConfigProto()\r\n  config.graph_options.optimizer_options.opt_level = -1\r\n  config.allow_soft_placement = True\r\n  config.graph_options.rewrite_options.constant_folding = (\r\n      rewriter_config_pb2.RewriterConfig.OFF)\r\n  config.graph_options.rewrite_options.pin_to_host_optimization = (\r\n      rewriter_config_pb2.RewriterConfig.OFF)\r\n  with tf.compat.v1.Session(config=config) as sess:\r\n\r\n    a = constant_op.constant(10.0)\r\n    b = constant_op.constant(5.0)\r\n    c = math_ops.multiply(a, b)\r\n    d = math_ops.multiply(c, c)\r\n\r\n    h_c = sess.run(session_ops.get_session_handle(c))\r\n    print(sess.run(d, feed_dict={c: h_c}))\r\n    #print(\"result = %f \" % d.eval())\r\n\r\n\r\ntestFeedOneHandleDirectly()\r\n\r\n```\r\n\r\nRun  with command \r\n```\r\npython handle_test.py\r\n```\r\nor\r\n```\r\nTF_CPP_MIN_VLOG_LEVEL=2 python handle_test.py 2>&1 | tee log.txt\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nd = c * c = (a * b) * (a * b) = (10 * 5) * (10 * 5) = 2500.00\r\nIf you run above script on cpu or gpu you'll get the correct result, but it seems there is something fishy underneath.\r\n\r\nWe are running tensorflow unit tests on our asic chip to verify our chip works properly on tensorflow and we failed on this test\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/v1_compat_tests/session_ops_test.py#L249\r\n\r\nThe error was related to memory copy between CPU and device. \r\nWe found a peculiar CopyCPUTensorToGPU call which invoked\r\n  Stream &ThenMemcpy(DeviceMemoryBase *gpu_dst, const void *host_src,  uint64 size);\r\nThe fishy part is that both gpu_dst and host_src are device pointers, which later triggered an error in our driver.\r\n\r\nThen we tried the same piece of code on GPU and surprisingly on GPU it has exactly the same issue. But the funny part is it seems nvidia cuda driver silently handled the wrong memory copy call. If you pass two device pointers to cuMemcpyHtoDAsync it just does a silent D2D copy and reports nothing. But on our platform it is illegal to call H2D memcpy interface with two device pointers.\r\n\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_util.cc:303] CopyCPUTensorToGPU                                                                                                                            \r\nI tensorflow/stream_executor/stream.cc:1543] [stream=0x2c48a6e0,impl=0x2c489490] Called Stream::ThenWaitFor(other=0x61ba9a0)\r\nI tensorflow/stream_executor/stream.cc:4655] [stream=0x2c48a6e0,impl=0x2c489490] Called Stream::ThenMemcpy(gpu_dst=0x7f6860000800, host_src=0x7f6860000700, size=4)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen call CopyCPUTensorToGPU it should have a device pointer as dest address and a host address as source address.\r\nIt seems to me the there is something flaky with the session handle mechanism.\r\nthe **h_c** got a session(resource) handle and corresponding resource is on devices. and when you pass h_c to feed dict it always assume the feed is on host and tries to send it to device again.\r\n```\r\nsess.run(d, feed_dict={c: h_c})\r\n```\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/Session\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No, I'm not sure how to fix it\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[log.txt.gz](https://github.com/tensorflow/tensorflow/files/6942630/log.txt.gz)", "comments": ["@Saduf2019  Was able to reproduce the issue in TF v2.4 , TF v2.5 , please find the [gist ](https://colab.research.google.com/gist/sushreebarsa/648145f54639cb34e614669fcafa4add/untitled357.ipynb)here.Thanks! ", "@kevint324 Can you please check with recent TF versions and let us know whether the issue persisting or not. Thanks!", "@jvishnuvardhan  The issue is valid on tf2.4 and tf2.5.  by recent you mean  tf2.6 or master? Is there a related bugfix or something?", "> by recent you mean tf2.6 or master?\r\n\r\nYes. `2.6` or `tf-nightly`. Thanks!", "on tf2.6 the problem remains the same.\r\n\r\nbtw, are those session handle ops  deprecated in tf2?"]}, {"number": 51276, "title": "TFLite GPU Delegate with OpenGL wrong output", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Dragonboard 845c (db845c)\r\n- TensorFlow installed from (source or binary): Yes\r\n- TensorFlow version (use command below): 2.4.2\r\n- Python version:\r\n- Bazel version (if compiling from source): 4.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nI am running a tflite ssd mobilenet face detection model on CPU and the results are correct but when I delegate to GPU using code below I get wrong results ( score is max): \r\n```\r\nconst TfLiteGpuDelegateOptionsV2 options = {\r\n            .is_precision_loss_allowed = 1, // FP16\r\n            .inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,\r\n            .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY,\r\n            .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n            .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n            .experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY,\r\n        };\r\n\r\n        TfLiteDelegate *delegate = TfLiteGpuDelegateV2Create(&options);\r\n        if (m_interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk)\r\n        {\r\n            std::cerr << \"Failed to modify graph with delegate\" << std::endl;\r\n            exit(0);\r\n        }\r\n``` \r\n\r\nI have tried more than 3 models and the results are the same, on CPU is working correctly but on gpu no detection or output is trash. \r\n\r\n> NOTE: I have Dragonboard 845 running latest Android 11 (flashed from master branch) and it currently supports only OpenGL.\r\n\r\nOutput when delegating: \r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nCUSTOM TFLite_Detection_PostProcess: TFLite_Detection_PostProcess\r\n98 operations will run on the GPU, and the remaining 1 operations will run on the CPU.\r\nINFO: Replacing 98 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 2 partitions.\r\nINFO: Initialized OpenGL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n```\r\n\r\n**Describe the expected behavior**\r\nModel should have same results when delegating on GPU using OpenGL only.\r\n\r\n> For reference I will upload tflite model also.\r\n[face_ssd_mobilenet_v2.tflite.zip](https://github.com/tensorflow/tensorflow/files/6940801/face_ssd_mobilenet_v2.tflite.zip)\r\n\r\n", "comments": ["@impjdi could you take a look?", "what happens when you set\r\n```\r\n            .is_precision_loss_allowed = 0\r\n```\r\n?\r\n", "Same results\r\n\r\n> what happens when you set\r\n> \r\n> ```\r\n>             .is_precision_loss_allowed = 0\r\n> ```\r\n> \r\n> ?\r\n\r\n", "[opengl_tflite_fail.txt](https://github.com/tensorflow/tensorflow/files/6944107/opengl_tflite_fail.txt)\r\n**adb logcat when running model with gpu delegate on OpenGL**", "**Extra info:**\r\nWhen delegating with XNNPACK: \r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Replacing 191 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 12 partitions.\r\n```\r\nresults are good, inference time is lower (I get around 8fps compared to running on CPU without delegate with 3fps)", "@impjdi any idea why this is happening? ", "If `.is_precision_loss_allowed = 0` and you're still seeing that, something is really broken.  Given that a lot of models use `CONV_2D` and `DEPTHWISE_CONV_2D`, I doubt there's bugs there.  I suspect reshapes and concats are not versatile as on the CPU and you might be hitting a corner case.", "I am building Tensorflow from latest master in bazel and my executable there, I really want to see if this has to do anything with the implemented OpenGL drivers from MESA-Freedreno as I don't have a device with proprietary Qualcomm driver implementation. I have tried one of this models and I didn't have any of these problems before.", "We have a decent amount of specialization logic (behave different if a certain GPU model, or have specialized code for 3x3 convolution, etc.), and the majority of the code path will be for Arm Mali & Qualcomm Adreno.  You might be hitting a corner case either hitting a specialization that isn't used anywhere, or MESA-Freedeno has a different behavior on undefined behavior; e.g. drivers behave differently when you access memory out of bounds.", "Do you have any suggestions on where should I file this issue or how to properly debug?", "This is the right component, but all the owners are out for an extended period of time (some until next January), and you won't get a response in a reasonable time frame.\r\n\r\nIf you want to debug and isolate the issue yourself, the best is bisecting the graph and isolating the problem to a single operation with specific input/output dimension.", "This is happening on latest master build, which is currently tensorflow-2.5.0"]}, {"number": 51244, "title": "tf.function block thread", "body": "I tried to run this code in multiprocess by `multiprocessing.Process`:\r\n\r\n```python\r\n@tf.function\r\ndef get_action(self, obs):\r\n    mu, sigma = self.Model(obs)\r\n\r\n    dist = tfp.distributions.Normal(mu, sigma)\r\n    action = tf.squeeze(dist.sample(), axis=0)\r\n    prob = tf.squeeze(dist.prob(action), axis=0)\r\n\r\n    return action, prob\r\n```\r\n\r\nthe tf.function would block the thread.", "comments": ["@yangtao121 In order to expedite the trouble-shooting process, please provide the complete code snippet to reproduce the issue reported here. please do let us know which version of tf you are using.    Thanks!", "My Tensorflow version is 2.5.0.\r\n\r\nThis code turns on multithreading(multiprocessing):\r\n\r\n```python\r\nthreads = [mp.Process(target=self.rolling, args=[env_name]) for _ in range(self.multi_worker_num)]\r\n```\r\n\r\n`self.rolling()`function as show below:\r\n\r\n```python\r\n def rolling(self, env_name):\r\n        env = gym.make(env_name).unwrapped\r\n        action_queue = Queue()\r\n        worker = Worker(\r\n            env=env,\r\n            env_args=self.env_args,\r\n            hyper_parameter=self.hyper_parameter,\r\n            get_data_queue=action_queue\r\n        )\r\n        policy = Gaussian_policy(output_queue=action_queue)\r\n        critic = Critic()\r\n\r\n        for _ in range(self.epochs):\r\n            policy.load_model('data/policy.h5')\r\n            critic.load_model('data/critic.h5')\r\n            self.ROLLING_EVENT.wait()\r\n            worker.update(policy, critic)\r\n            batch = worker.runner()\r\n            self.batches.put(batch)\r\n            if self.batches.qsize() < self.multi_worker_num - 1:\r\n                self.UPDATE_EVENT.wait()\r\n            # if self.roll_flag < self.multi_worker_num:\r\n\r\n            else:\r\n                self.roll_flag = 0\r\n                self.UPDATE_EVENT.set()\r\n                self.ROLLING_EVENT.clear()\r\n```\r\n\r\nThe `worker` is used for sampling data, the `policy` tell the `worker` how to do, `policy` give the action by:\r\n\r\n```\r\n    @tf.function\r\n    def get_action(self, obs):\r\n        # tf.print(obs)\r\n        # print(obs)\r\n        mu, sigma = self.Model(obs)\r\n\r\n        dist = tfp.distributions.Normal(mu, sigma)\r\n        action = tf.squeeze(dist.sample(), axis=0)\r\n        prob = tf.squeeze(dist.prob(action), axis=0)\r\n        # print(action)\r\n\r\n        return action, prob\r\n```\r\n\r\n`worker.runner()`:\r\n\r\n```python\r\n    def runner(self):\r\n        # print('start')\r\n        batches = []\r\n        for i in range(self.trajs):\r\n            collector = Collector(observation_dims=self.obs_dims, action_dims=self.act_dims,\r\n                                  episode_length=self.steps)\r\n            state = self.env.reset()\r\n            # print(i)\r\n\r\n            for t in range(self.steps):\r\n                state = state.reshape(1, -1)\r\n                print(state)\r\n                action, prob = self.policy.get_action(state)\r\n\r\n                action_ = action * 2\r\n\r\n                state_, reward, done, _ = self.env.step(action_)\r\n                collector.store(state, action, reward, prob)\r\n                state = state_\r\n\r\n                if (t + 1) % self.batch_size == 0 or t == self.steps - 1:\r\n                    observations, reward = collector.get_current_data()\r\n                    value_ = self.critic.get_value(state_.reshape(1, -1))\r\n                    values = self.critic.get_value(observations)\r\n\r\n                    gae, target = gae_target(self.gamma, self.lambada, reward, values, value_, done)\r\n\r\n                    collector.get_gae_target(gae, target)\r\n\r\n            batches.append(collector)\r\n\r\n        return batches\r\n```\r\n\r\nI found when I use multiprocessing, the thread will block at ` action, prob = self.policy.get_action(state)` in `worker.runner`  and had no error output. However, when I remove the `tf.function` above the `policy.get_action()` , it can work. But remove the `tf.function` would cause seriously memory leak.\r\n\r\nAnd the complete code can be found in my [github](https://github.com/yangtao121/RL).\r\n\r\nThanks"]}, {"number": 51241, "title": "Inconsistent eager/tf.function behavior for rank 0 shape in tf.reshape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\n\r\nThe eager version of `tf.reshape` takes a rank 0 tensor as a shape parameter while the jitted (`tf.function` decorated) does not.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be consistent, either fail in both or allow in both. To be consistent with other methods, I think it should fail in both.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): maybe\r\n- Briefly describe your candidate solution(if contributing): raise an error, such as done in other methods like `tf.random.uniform`\r\n\r\n**Standalone code to reproduce the issue**\r\n[executable example here](https://colab.research.google.com/drive/15wQJVDzEHc5puK6twrV9HzxcdWZWZPtC?usp=sharing)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef func():\r\n    return tf.reshape([[42]], 1)\r\n\r\n\r\nfunc_jit = tf.function(func=func)\r\n\r\nfunc()  # works\r\nfunc_jit()  # fails\r\n```", "comments": ["@Saduf2019  I am able to reproduce the error in [**`TF v2.5`**](https://colab.research.google.com/gist/kumariko/963e9b7da67ed97c33080f66a039a8e1/51241.ipynb) ,[**`TF nightly`**](https://colab.research.google.com/gist/kumariko/597f3aa9b8711421cc2d4cf715000395/51241.ipynb) and [**`TF v2.4`** ](https://colab.research.google.com/gist/kumariko/747438b98304e746222ff27bf89e39aa/51241.ipynb) Please find the gists for your reference here. Thanks!", "@mayou36 \r\nCan you please confirm the jit compilation is enabled.", "By \"jit\" I mean \"tf.function\" decorated (jitted), not XLA compiled. So it works in eager, but not inside a tf.function.\r\n\r\nIt seems to happen with and without `jit_compiled` on, both.", "@jonas-eschle \r\nPlease confirm if this is still an issue.", "It is. I've updated the gist and it is still broken, even with TF-nightly", "Thank you for raising this issue. We are discussing internally the available options, and we'll update with the result."]}, {"number": 51234, "title": "Tensorflow tf.map_fn over ragged tensor fails with object of type 'RaggedTensor' has no len", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (both RHEL 7 Power PC and Ubuntu 21.04 with Intel)\r\n\r\n- TensorFlow installed from (source or binary): conda install tensorflow\r\n- TensorFlow version (use command below): Tested on 2.4.1 and 2.5.0\r\n- Python version: 3.7.10\r\n- CUDA/cuDNN version: cudatoolkit 10.1.243 (Intel) and 10.2 (PowerPC)\r\n- GPU model and memory: 2080TI (Intel) and A100 (PowerPC)\r\n\r\n```\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n> \"\r\n2021-08-04 23:57:32.182116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\nunknown 2.4.1\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nhttps://stackoverflow.com/questions/68658093/tensorflow-tf-map-fn-over-ragged-tensor-fails-with-object-of-type-raggedtensor\r\nThis Tensorflow doc gives this example of using tf.map_fn on ragged tensors which works for Tensorflow 2.4.1 and above:\r\n\r\n```\r\ndigits = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\r\nprint(tf.map_fn(tf.math.square, digits))\r\n```\r\nHowever the following example results in error \"object of type 'RaggedTensor' has no len\" when run in Tensorflow 2.4.1 or Tensorflow 2.5:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nX=tf.ragged.constant([[1.,2.],[3.,4.,5.]], dtype=tf.float32)\r\n\r\n@tf.function\r\ndef powerX(i):\r\n    global X\r\n    return X**i\r\n\r\nY = tf.map_fn(powerX, tf.range(3, dtype=tf.float32))\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should return a ragged tensor.   \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nX=tf.ragged.constant([[1.,2.],[3.,4.,5.]], dtype=tf.float32)\r\n\r\n@tf.function\r\ndef powerX(i):\r\n    global X\r\n    return X**i\r\n\r\nY = tf.map_fn(powerX, tf.range(3, dtype=tf.float32))\r\n```\r\n", "comments": ["@jvishnuvardhan,\r\n\r\nI am able to reproduce the error in `TF 2.5` and `TF nightly`. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/fca7b3cb87d6bc407453600b4c784472/51234.ipynb). Thanks!", "Any luck on this?  I'm finding it extremely hard to parallelize any function that involves tf.where and tf.gather resulting in any intermediate step where there are unequal length tensors.  This makes Tensorflow super difficult and unsuitable for coding parallel algorithms as opposed to doing cookbook machine learning stuff where everything is the same size.", "When you use `tf.map_fn` with RaggedTensors, you currently need to specify the output signature of your function.  In this case, your function returns a 2D RaggedTensor, so either of the following will work:\r\n\r\n```\r\nY = tf.map_fn(powerX, tf.range(3, dtype=tf.float32), \r\n              fn_output_signature=tf.RaggedTensorSpec([None, None], tf.float32))\r\n```\r\n\r\nOr:\r\n\r\n```\r\nY = tf.map_fn(powerX, tf.range(3, dtype=tf.float32), \r\n              fn_output_signature=tf.RaggedTensorSpec([2, None], tf.float32))\r\n```\r\n", "(The reason the `tf.map_fn(tf.math.square, digits)` example works without specifying the output signature is that the output signature is identical to the input signature.  In that case, you don't need to specify it.)", "Thanks for the clarification. My experience on `tf.map_fn` is that it will not, in any event, result in parallel kernels running on the GPU.  It's about the same as a Python `for` loop.  The only way to get real parallelism is with `tf.vectorized_map`, with same-sized tensors. This requires some very tedious refactoring of methods and padding of vectors and clever tricks so that the algorithm for 1 problem instance of size M1 can be inside-outed for N problems  into a sequence of pure vector operations that operate on uniformly sized vectors of size max(M1,...MN).  I'm not sure if the experience would be the same if I tried to go down the `cupy` route.  In any event it's quite frustrating."]}]