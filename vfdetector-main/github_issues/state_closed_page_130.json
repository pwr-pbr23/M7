[{"number": 51048, "title": "Two python versions install tf in one system one succeeds while the other fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 (Core)\r\n- TensorFlow version:1.14.0\r\n- Python version: python3.6.8 and python3.7.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have python3.6 installed by yum earlier and install tensorflow through pip successfully. Then I compile a python3.7 and also install same version tensorflow by pip but it failed, the error as follows:\r\n```\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n```\r\nAny idea on why this happens?\r\n\r\n\r\n**Any other info / logs**\r\n```\r\n[root@localhost lib]# python3.6\r\nPython 3.6.8 (default, Nov 16 2020, 16:55:22)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n```\r\n\r\n\r\n```\r\n[root@localhost lib]# /root/python379tgz/Python-3.7.9-compiled/bin/python3.7\r\nPython 3.7.9 (default, Jul 30 2021, 12:33:12)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/root/python379tgz/Python-3.7.9-compiled/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /root/python379tgz/Python-3.7.9-compiled/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.```", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.5.0  | 11.2 |\n| 2.4.0  | 11.0 |\n| 2.1.0 - 2.3.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@st4rfall ,\r\n\r\nWe see that you are using tf version 1.14, 1.x is not actively supported, please update to 2.x and let us know if you are using same issue.Thanks!", "> @st4rfall ,\r\n> \r\n> We see that you are using tf version 1.14, 1.x is not actively supported, please update to 2.x and let us know if you are using same issue.Thanks!\r\n\r\nThanks for your reply. I try with 2.x version and it seems tensorflow could be imported.\r\n```\r\nPython 3.7.9 (default, Jul 30 2021, 12:33:12)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n2021-08-02 10:02:41.152225: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-08-02 10:02:41.152267: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\nHowever when I run my code  I meet AttributeError:\r\n```\r\nAttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n```\r\nI have tried [solution](https://github.com/theislab/scgen/issues/14) here but not working. So I guess I have to use older version tensorflow.\r\n", "update glibc, problem solved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51048\">No</a>\n", "@st4rfall ,\r\nGlad the suggestion worked for you.Thanks!"]}, {"number": 51047, "title": "Inception_v4 can not node in the pb file", "body": "HI,all\r\n\r\n* convert tensorflow model inceptionV4 to onnx format, ``Error`` in ``tensorflow_core/python/framework/graph_util_impl.py\"``\r\n* the model url (https://storage.googleapis.com/download.tensorflow.org/models/inception_v4_2016_09_09_frozen.pb.tar.gz)\r\n* I can find the node named ``graph/InceptionV4/Logits/Predictions`` by the scripts\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nimport os\r\n \r\nmodel_dir = '/home/yons/workspace/models/tf1/inception_v4'\r\nmodel_name = 'inception_v4_2016_09_09_frozen.pb'\r\n \r\n# \u8bfb\u53d6\u5e76\u521b\u5efa\u4e00\u4e2a\u56fegraph\u6765\u5b58\u653eGoogle\u8bad\u7ec3\u597d\u7684Inception_v4\u6a21\u578b\uff08\u51fd\u6570\uff09\r\ndef create_graph():\r\n    with gfile.GFile(os.path.join(\r\n            model_dir, model_name), 'rb') as f:\r\n        # \u4f7f\u7528tf.GraphDef()\u5b9a\u4e49\u4e00\u4e2a\u7a7a\u7684Graph\r\n        graph_def = tf.compat.v1.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        # Imports the graph from graph_def into the current default Graph.\r\n        tf.import_graph_def(graph_def, name='graph')\r\n \r\n# \u521b\u5efagraph\r\ncreate_graph()\r\n \r\ntensor_name_list = [tensor.name for tensor in tf.compat.v1.get_default_graph().as_graph_def().node]\r\nresult_file = os.path.join(model_dir, 'result.txt') \r\nwith open(result_file, 'w+') as f:\r\n    for tensor_name in tensor_name_list:\r\n        f.write(tensor_name+'\\n')\r\n\r\n```\r\n\r\n# Error\r\n```\r\n2021-07-30 16:41:47,830 - WARNING - From /home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tf2onnx/verbose_logging.py:76: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tf2onnx/convert.py\", line 605, in <module>\r\n    main()\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tf2onnx/convert.py\", line 213, in main\r\n    graph_def, inputs, outputs = tf_loader.from_graphdef(args.graphdef, args.inputs, args.outputs)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tf2onnx/tf_loader.py\", line 315, in from_graphdef\r\n    frozen_graph = freeze_session(sess, input_names=input_names, output_names=output_names)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tf2onnx/tf_loader.py\", line 262, in freeze_session\r\n    graph_def = convert_variables_to_constants(sess, graph_def, output_node_names)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 277, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 197, in extract_sub_graph\r\n    _assert_nodes_are_present(name_to_node, dest_nodes)\r\n  File \"/home/yons/workspace/anaconda3/envs/nx_cross_compilers/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 152, in _assert_nodes_are_present\r\n    assert d in name_to_node, \"%s is not in graph\" % d\r\nAssertionError: graph/InceptionV4/Logits/Predictions is not in graph\r\n```\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution ( Linux Ubuntu 18.04):\r\nUbuntu18.04\r\npython 3.7.10\r\ntensorflow-estimator 1.15.1 (binary)\r\ntensorflow-gpu 1.15.5(binary)\r\nonnx 1.6.0\r\nonnxruntime 1.3.0\r\ntf2onnx 1.9.1\r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51047\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51047\">No</a>\n"]}, {"number": 51046, "title": "Release candidate 2.6.0rc1 is not built with HDFS support anymore?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary from pip\r\n- TensorFlow version:\r\n2.6.0rc1\r\n- Python version:\r\n3.7.10\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nI installed the release candidate 2.6.0rc1 and tried to read files from HDFS and it failed complaining that hdfs is an unknown filesystem scheme. I have seen this problem before when working TensorFlow builds that were not built with HDFS support.\r\n\r\nAll previous TF 2.* versions including release candidates that I've tested is built with HDFS support and looking at the release notes I'm none the wiser as it does not mention that HDFS support is being dropped from the pip wheel.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nimport tensorflow as tf\r\ntf.io.gfile.glob(\"hdfs://localhost:8020/data\")\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-6-3127c92cb05e> in <module>\r\n----> 1 tf.io.gfile.glob(\"hdfs://localhost:8020/data\")\r\n\r\n/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py in get_matching_files_v2(pattern)\r\n    441         compat.as_str_any(matching_filename)\r\n    442         for matching_filename in _pywrap_file_io.GetMatchingFiles(\r\n--> 443             compat.as_bytes(pattern))\r\n    444     ]\r\n    445   else:\r\n\r\nUnimplementedError: File system scheme 'hdfs' not implemented (file: 'hdfs://localhost:8020/data')\r\n```\r\n", "comments": ["@robzor92 Could you please take a look at the  [tested build configuration](https://www.tensorflow.org/install/source) for reference and let us know if it helps ?Thank you!", "Hi @sushreebarsa ! The **Tested Build Configuration** does not mentioned particular build flags, like HDFS support for example.\r\n\r\nHowever I did some digging in the code and found out that HDFS and S3 support is not enabled by default anymore. https://github.com/tensorflow/tensorflow/pull/48522\r\n\r\nCan I also confirm with you that this is the case for the builds that are published on PyPi? So basically this is the question: _Should TensorFlow 2.6.0 installed through pip support HDFS or do I need to build TensorFlow myself in order for it to work_?\r\n", "@robzor92 Please see PR #51036 and PR #51037. HDFS is now supported through tensorflow-io PyPI package (in together with tensorflow 2.6).", "That's great @yongtang I will try it out! I think this information may be missing from the current release notes for the rc1.", "Or maybe not, I see it was already done. https://github.com/yongtang/tensorflow/commit/9fcaf0838677d1095a7573e36ebb4d17511af6ed\r\n\r\nThanks for the help! I will close this issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51046\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51046\">No</a>\n"]}, {"number": 51045, "title": "Update max version of bazel supported by tensorflow to 4.99.0", "body": "For fix the issue: https://github.com/tensorflow/tensorflow/issues/50966#issuecomment-888748570\r\n\r\nI'm a member of Microsoft VCPKG team, we recently upgraded the version of bazel used in vcpkg to 4.1.0, then tensorflow build failed with following error:\r\n```\r\nYou have bazel 4.1.0 installed.\r\nPlease downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n```\r\nThis issue is due to the fact that tensorflow has set the maximum supported version of bazel to 3.99.0 on line 53 of the https://github.com/tensorflow/tensorflow/blob/master/configure.py file.\r\n\r\nFor fixing this issue, update the max version of bazel supported by tensorflow to 4.99.0.", "comments": ["Let's make it 4.99 so we don't need to upgrade on every minor bump.\r\n\r\nThough, we know of at least one failure in TF when built with Bazel 4.0."]}, {"number": 51044, "title": "[MLIR][DISC] Bufferize mhlo::LogisticOp", "body": "", "comments": []}, {"number": 51043, "title": "bug in rsqrt", "body": "---------------------------------------------------------------------------part 1-----------------------------------------------------------------\r\n**my code:**\r\n````\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nnp.set_printoptions(threshold=np.inf)\r\n\r\ndef test():\r\n    with tf.device(\"/device:CPU:0\"):\r\n    # with tf.device('/gpu:1'):\r\n        a=tf.constant([1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38])\r\n        b = tf.rsqrt(a)\r\n    return b\r\n\r\nif __name__=='__main__':\r\n    with tf.Session() as sess:\r\n        print(sess.run(test()))\r\n````\r\n\r\n**expect the right result:**\r\n_[9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18, 9.223371843921341e+18]_\r\n\r\nin the env: **tensorflow== 1.14.0  python=3.6.8**\r\nget the wrong result:\r\n[1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19\r\n 1.383168e+19 1.383168e+19 1.383168e+19 9.223372e+18]\r\n\r\n\r\nin the env: **tensorflow== 1.14.0  python=3.6.8**\r\nget the wrong result:\r\n_[1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19\r\n 1.383168e+19 1.383168e+19 1.383168e+19 9.223372e+18]_\r\n\r\nin the env: **tensorflow== 1.14.0  python=2.7.18**\r\nget the wrong result:\r\n_[         inf          inf          inf          inf          inf\r\n          inf          inf          inf 9.223372e+18]_\r\n\r\nnote:if I use the gpu device, also get the wrong result\r\n----------------------------------------------------------------------part 2-----------------------------------------------------------------\r\n**my code2:**\r\n````\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nnp.set_printoptions(threshold=np.inf)\r\nimport math\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\ndef test():\r\n    a=tf.constant([1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38])\r\n    b = tf.math.rsqrt(a)\r\n    print(b)\r\n    return\r\n    \r\nif __name__=='__main__':\r\n    test()\r\n````\r\n\r\nin the env: **tensorflow== 2.4.0  python=3.8.5**\r\nget the wrong result:\r\n_[1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19\r\n 1.383168e+19 1.383168e+19 1.383168e+19 9.223372e+18]_\r\n\r\nin the same env **tensorflow== 2.4.0  python=3.8.5** if I use GPU, I mean os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\nget the right result:\r\n[9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18\r\n 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18]\r\n\r\n\r\n----------------------------------------------------------------------part 3-----------------------------------------------------------------\r\nnote:\r\n1\u3001the small shape tend to get the right result\r\n2\u3001similarity bug also in tf.floor\r\n\r\n", "comments": ["@BlueSkyyyyyy ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!", "> @BlueSkyyyyyy ,\r\n> \r\n> We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**no**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Linux Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **cpu**\r\n- TensorFlow installed from (source or binary):**binary**\r\n- TensorFlow version (use command below):**2.4.0**\r\n- Python version:**3.8.5**\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\n**Describe the current behavior**\r\nuse tf.math.rsqrt to cal:\r\n a=tf.constant([1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38])\r\n    b = tf.math.rsqrt(a)\r\n\r\nget the wrong results:\r\n[1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19\r\n1.383168e+19 1.383168e+19 1.383168e+19 9.223372e+18]\r\n\r\n**Describe the expected behavior**\r\n[9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18\r\n9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18]\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):no\r\n- Briefly describe your candidate solution(if contributing):no\r\n\r\n**Standalone code to reproduce the issue**\r\nmy code \r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nnp.set_printoptions(threshold=np.inf)\r\nimport math\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\ndef test():\r\n    a=tf.constant([1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38])\r\n    b = tf.math.rsqrt(a)\r\n    print(b)\r\n    return\r\n    \r\nif __name__=='__main__':\r\n    test()\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n**no**\r\n\r\n\r\n\r\n\r\n\r\n", "> @BlueSkyyyyyy ,\r\n> \r\n> We see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!\r\n@tilakrayal\r\nI have used the template to describe the problem as above. not only tf 2.4 but also other version has the similarity problem( I have trytensorflow== 1.14.0+ python=3.6.8  and tensorflow== 1.14.0 python=2.7.18 ).\r\n", "@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/f4397520f5bf766bf99160a9f4d6a996/51043.ipynb).", "@Saduf2019 ,@tensorflowbutler\uff0c \r\ndo you know why tensorflow has this bug and how to solve it ", "@BlueSkyyyyyy \r\nCan you try setting the dtype of the tf.constant to tf. float 64 bit floating point tensor)\r\n2)avoid using main, check if you can get better result by removing main.\r\nIf you really have to use main then try putting your code in py file in colab and try to execute using filename.py. [the loss of precision, due to the large number you have chosen causing the issue]", "@Saduf2019\r\n1\uff09I remove main ,but have the same problem;\r\n2)  if I use tf.float64, can get the right ans, but it is not a same question; float32 should support the precision. \r\n\r\nas i use float32 in numpy or python,  can get the right ans:\r\ncode:\r\n````\r\nimport numpy as np\r\nnp.set_printoptions(threshold=np.inf)\r\nimport math\r\n\r\nif __name__=='__main__':\r\n    a = [1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38,1.1754944e-38]\r\n\r\n    ans_py = []\r\n    for aa in a:\r\n        ans_py.append(float(1.0/math.sqrt(float(aa))))\r\n    print(\"ans_py:{}\".format(ans_py))\r\n\r\n    a_np = np.array(a,dtype=\"float32\")\r\n    ans_np = 1.0/np.sqrt(a_np)\r\n    print(\"ans_np:{}\".format(ans_np))\r\n````\r\nrun get the ans:\r\n```\r\nans_py:[9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18, 9.22337184392134e+18]\r\nans_np:[9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18\r\n 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18\r\n 9.223372e+18 9.223372e+18 9.223372e+18 9.223372e+18]\r\n```\r\nso i think float32 is enough for this case .\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@BlueSkyyyyyy \r\nThis is not a bug or performance issue, please move this to closed status and for any further queries open this at the tf discussion forum.", "@Saduf2019 \r\nWhy it is not a bug, I get obviously wrong result by tf .  Is there any knowledge or I have neglect lead to the wrong use ? thanks your patient", "I tested in colab both cpu and gpu implementation results match and I get\r\n```python\r\ntf.Tensor(\r\n[1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19\r\n 1.383168e+19 1.383168e+19 1.383168e+19 9.223372e+18], shape=(9,), dtype=float32)\r\n```\r\nI wonder why you are seeing different results. You may try test with google colab.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ymodak\r\n\r\n> tf.Tensor(\r\n[1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19 1.383168e+19\r\n 1.383168e+19 1.383168e+19 1.383168e+19 9.223372e+18], shape=(9,), dtype=float32)\r\n\r\nthis result is not right, that is the key point. \r\n1.383168e+19 is wrong ,9.223372e+18 is right.", "This is likely due to an Eigen approximation.  If `EIGEN_FAST_MATH = 1` (the default), then Eigen uses the fast reciprocal sqrt approximation, which is *much* faster, but less accurate (particularly as you deviate away from zero).  See [here](https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Core/arch/AVX/MathFunctions.h#L133).\r\n\r\nIf compiling TF from source, you can try explicitly disabling this (adding `-DEIGEN_FAST_MATH=0` to the set of compile flags).\r\n\r\nIf not building TF from source, the work-around is to use `tf.sqrt()` instead, and do divisions as necessary. ", "@cantonios \r\nthank you so much", "> @cantonios thank you so much\r\n\r\nIf your issue is resolved, could you please close the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51043\">No</a>\n"]}, {"number": 51042, "title": "Update max version of bazel supported by tensorflow to 4.1.0", "body": "For fix the issue: https://github.com/tensorflow/tensorflow/issues/50966#issuecomment-888748570\r\n\r\nI'm a member of Microsoft VCPKG team, we recently upgraded the version of bazel used in vcpkg to 4.1.0, then tensorflow build failed with following error:\r\n```\r\nYou have bazel 4.1.0 installed.\r\nPlease downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n```\r\nThis issue is due to the fact that tensorflow has set the maximum supported version of bazel to 3.99.0 on line 53 of the https://github.com/tensorflow/tensorflow/blob/master/configure.py file.\r\n\r\nFor fixing this issue, update the max version of bazel supported by tensorflow to 4.1.0.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51042) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51042) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51042) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51042) for more info**.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51042) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 51041, "title": "Does the \"p\" in \"pfor\" stand for \"print\" or something?", "body": "https://github.com/tensorflow/tensorflow/blob/bfb74e15879212c3e4573fb83cd87358e964b4be/tensorflow/python/ops/parallel_for/pfor.py#L924", "comments": ["`p` in `pfor` stands for parallel. See https://github.com/tensorflow/tensorflow/blob/72552c4a73650962abe5c110ab4e4068e741cef0/tensorflow/python/ops/parallel_for/pfor.py#L15", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51041\">No</a>\n"]}, {"number": 51039, "title": "Fix missing preprocess guards in CUDNN wrapper (Fixes build breakage when CUDNN Frontend not used or CUDNN < 8.1)", "body": "Fixes a block of code added by by a [previous PR](https://github.com/tensorflow/tensorflow/pull/50569/commits/07ed03809cbf142229c99ab1b3e6a0dc8641f59b). It is improperly gated by preprocessor macros. It uses `json` symbol which isn't defined unless the header at the top is included, which is gated by macros:\r\n\r\n```\r\n#if CUDNN_VERSION >= 8100 && TF_ENABLE_CUDNN_FRONTEND\r\n#include \"third_party/cudnn_frontend/include/cudnn_frontend.h\"\r\n#endif  // CUDNN_VERSION >= 8100 && TF_ENABLE_CUDNN_FRONTEND\r\n```\r\nThe header indirectly includes `nlohmann/json`, so if this is not present, and your CuDNN version is < 8100, then the build will break.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51039) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51039) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51039) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "LGTM"]}, {"number": 51038, "title": "Autograph set_verbosity doc example shows using integer values for env vars, but only strings can be used", "body": "## URL(s) with the issue:    \r\nhttps://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation usage description for setting verbosity through environment variables uses an integer value, however this is not possible as environment variables must be strings. In the current state the following code example is provided:\r\n\r\n```py\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['AUTOGRAPH_VERBOSITY'] = 5\r\n# Verbosity is now 5\r\n\r\ntf.autograph.set_verbosity(0)\r\n# Verbosity is now 0\r\n\r\nos.environ['AUTOGRAPH_VERBOSITY'] = 1\r\n# No effect, because set_verbosity was already called.\r\n```\r\n\r\nHowever it should actually be:\r\n\r\n```py\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['AUTOGRAPH_VERBOSITY'] = '5'\r\n# Verbosity is now 5\r\n\r\ntf.autograph.set_verbosity(0)\r\n# Verbosity is now 0\r\n\r\nos.environ['AUTOGRAPH_VERBOSITY'] = '1'\r\n# No effect, because set_verbosity was already called.\r\n```", "comments": ["@Saduf2019  ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/de7a8dd4b12cdfbe135adb82ca1af253/untitle51038.ipynb).", "This issue is TF version independent as this issue is python specific and purely a documentation error. There probably more instances in the docs of setting env vars to something other than strings, so I'll try and have a look to find more doc errors like this, but it might take a while to find more if they exist.", "The above commit fixes the issue. Thanks!\r\nFixed in nightly docs https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity?version=nightly"]}, {"number": 51037, "title": "[r2.6] Update RELEASE.md to add entry for modular file system support (s3 and hdfs)", "body": "**NOTE: This PR is for r2.6 branch**\r\n\r\nThis PR update RELEASE.md to add entry for modular file system support (s3 and hdfs).\r\n\r\n/cc @mihaimaruseac Don't know if this is still ok though I think it might make sense to add the entry to 2.6 branch of RELEASE.md\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["We won't cherrypick the code removal on the 2.6 branch, so I think we only need to update the notes on master."]}, {"number": 51036, "title": "Update RELEASE.md to add entry for modular file system support (s3 and hdfs)", "body": "This PR update RELEASE.md to add entry for modular file system support (s3 and hdfs).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51035, "title": "fix: miss typo codespelling", "body": "hello!, i have fixing some miss typo grammar on ```tensorflow```, and fixed with reference words from [Merriam webster dictionary](https://www.merriam-webster.com/)\r\n\r\nthanks !!!", "comments": []}, {"number": 51034, "title": "Update snappy library to 1.1.9", "body": "This PR updates snappy library from 1.1.8 to 1.1.9.\r\nThe previous version 1.1.8 was one and half year old. The latest one (1.1.9)\r\nwas released last month.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["On macOS build issue is caused by https://github.com/google/snappy/blob/1.1.9/snappy.cc#L1032 will investigate and resubmit."]}, {"number": 51033, "title": "Remove hdfs as support has been moved to modular file systems.", "body": "This PR removes hdfs as support has been moved to modular file systems.\r\n\r\nThis PR will help reduce build time of tensorflow core.\r\n\r\n/cc @kvignesh1420  @vnvo2409 @burgerkingeater FYI\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@kvignesh1420  Thanks for the reminder. PR #51036 has been created to update the RELEASE.md", "@yongtang Can you please resolve conflicts? Thanks!", "@gbaned The PR has been rebased and updated. Please take a look.", "Seems I need to manually import this", "Merged by 86b9b15c4b8c3dd75ad766bb14d0c2143327b0ff"]}, {"number": 51032, "title": "Remove AWS files as s3 support is now in modular file system", "body": "This PR remove AWS files, as s3 support is now in modular file system.\r\n\r\nThis PR will help reduce build time of tensorflow core.\r\n\r\n/cc @kvignesh1420  @vnvo2409 @burgerkingeater FYI\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@kvignesh1420  Thanks for the reminder. PR #51036 has been created to update the RELEASE.md", "@yongtang  Can you please resolve conflicts? Thanks!", "Thanks @gbaned . The PR has been updated with conflict resolved.", "@mihaimaruseac Can you please review this PR ? Thanks!", "@yongtang  Can you please resolve conflicts? Thanks!", " @mihaimaruseac @gbaned  The PR has been rebased to resolve the merge conflict. Can you take a look and see if the PR can be imported now (I noticed last time the internal import may have a failure)? ", "@yongtang  Can you please address Ubuntu Sanity errors? Thanks!", "@mihaimaruseac @gbaned The PR has been updated with the lint issue resolved. Please take a look and see if this fixes everything.", "Seems this needs manual import", "would you mind re-basing?", "Thanks @pkanwar23 @mihaimaruseac for the help. The PR has been rebased and updated. Please give it a try on import and see if the issue still persists.", "Most likely copybara won't merge this as it needed manual work, but it is now merged as of 7add88f2982331be6ba65ddc66f1c88053028fff\r\n\r\nSorry for the delay and thanks for sending all these PRs to TF"]}, {"number": 51031, "title": "Installation of tflite-model-maker 0.3.2. takes forever", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 Enterprise\r\n- TensorFlow installed from (source or binary): from PyPi\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.6.8 and 3.8.8\r\n- Installed using virtualenv? pip? conda?:  using pip in venv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 11.2, cuDNN 8.1.0\r\n- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design 8gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nInstallation of tflite-model-maker 0.3.2. takes forever.\r\nAttached log demonstrates situation for a 10+ minutes of installation. I will attach fuller log in the comments.\r\nIt downloads 17 versions of scipy package, 10+ versions of scikit-learn, 3 versions of tensorflow (although 2.5.0 version is already installed) and so on.\r\nI tried installation of tflite-model-maker 0.3.2 few times, but couldn't make it. \r\nInstalled nightly version, but I question its quality and stability - that's why I've decided to create an issue. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tflite-model-maker==0.3.2\r\n\r\n\r\n**Any other info / logs**\r\n[pip install tflite-model-maker.txt](https://github.com/tensorflow/tensorflow/files/6900929/pip.install.tflite-model-maker.txt)\r\n", "comments": ["@Extremesarova  I can able to install it properly and it took less than a minute. Please check the [colab](https://colab.research.google.com/gist/saikumarchalla/ca054239bda81200ac1c532a1c8cc253/untitled117.ipynb) gist. Thanks!\r\n\r\nFor more information, Please check this  [link](https://pypi.org/project/tflite-model-maker/).\r\n", "Thank you for fast response. I've tried installation under Ubuntu also right now - same as on Windows. \r\nAttaching updated logs from installation which started over 30 minutes ago and still continues (and on Ubuntu too).\r\n[pip install tflite-model-maker_updated.txt](https://github.com/tensorflow/tensorflow/files/6901076/pip.install.tflite-model-maker_updated.txt)\r\n\r\nCan you check on a local machine? \r\n", "The same situation is happening even on a freshly created venv", "In Colab python version is 3.7.11. Maybe it influences the installation process somehow. \r\nI tried installation with 3.6.8 and 3.8.8", "I'm having the same issue. It seems like pip is not able to resolve the dependency tree and downloads/checks many versions of most packages, including building a ton of wheels. I've tried this on Ubuntu and Mac OS. After about 18 hours I finally got an error on Ubuntu (Mac is still running). Traceback below:\r\n\r\n```bash\r\nERROR: Exception:\r\nTraceback (most recent call last):\r\n  File \"/home/[username]/.local/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 173, in _main\r\n    status = self.run(options, args)\r\n  File \"/home/[username]/.local/lib/python3.6/site-packages/pip/_internal/cli/req_command.py\", line 203, in wrapper\r\n    return func(self, options, args)\r\n  File \"/home/[username]/.local/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 316, in run\r\n    reqs, check_supported_wheels=not options.target_dir\r\n  File \"/home/[username]/.local/lib/python3.6/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\r\n    collected.requirements, max_rounds=try_to_avoid_resolution_too_deep\r\n  File \"/home/[username]/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 472, in resolve\r\n    state = resolution.resolve(requirements, max_rounds=max_rounds)\r\n  File \"/home/[username]/.local/lib/python3.6/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 383, in resolve\r\n    raise ResolutionTooDeep(max_rounds)\r\npip._vendor.resolvelib.resolvers.ResolutionTooDeep: 2000000\r\n```\r\n\r\nAn example of how it is trying to resolve dependencies is below. I've never seen pip do this before, but it's downloading every version that seems like it might work and trying them all, often building wheels, although in this example I think it downloads built modules. Note this is my second attempt, so most of these were cached already. It seems to do this with almost every dependency; I just picked one out part out of the output.\r\n\r\n```bash\r\nINFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\r\nCollecting scikit-learn!=0.19.0,>=0.14.0\r\n  Using cached scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\r\n  Downloading scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22.2 MB 2.2 MB/s\r\n  Using cached scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\r\n  Using cached scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\r\n  Using cached scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\r\n  Using cached scikit_learn-0.23.0-cp36-cp36m-manylinux1_x86_64.whl (7.3 MB)\r\n  Using cached scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\r\nINFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\r\n  Using cached scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\r\n  Using cached scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\r\n  Using cached scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\r\n  Using cached scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n  Using cached scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\r\n  Using cached scikit_learn-0.21.1-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n  Using cached scikit_learn-0.21.0-cp36-cp36m-manylinux1_x86_64.whl (6.6 MB)\r\n  Using cached scikit_learn-0.20.4-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\r\n  Using cached scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\r\n  Using cached scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\r\n  Using cached scikit_learn-0.20.1-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\r\n  Using cached scikit_learn-0.20.0-cp36-cp36m-manylinux1_x86_64.whl (5.3 MB)\r\n  Using cached scikit_learn-0.19.2-cp36-cp36m-manylinux1_x86_64.whl (4.9 MB)\r\n  Using cached scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\r\n  Using cached scikit_learn-0.18.2-cp36-cp36m-manylinux1_x86_64.whl (11.8 MB)\r\n  Using cached scikit_learn-0.18.1-cp36-cp36m-manylinux1_x86_64.whl (11.8 MB)\r\n  Using cached scikit-learn-0.18.tar.gz (8.9 MB)\r\n  Using cached scikit-learn-0.17.1.tar.gz (7.9 MB)\r\n  Using cached scikit-learn-0.17.tar.gz (7.8 MB)\r\n  Using cached scikit-learn-0.16.1.tar.gz (7.3 MB)\r\n  Using cached scikit-learn-0.16.0.tar.gz (7.3 MB)\r\n  Using cached scikit-learn-0.15.2.tar.gz (7.0 MB)\r\n  Using cached scikit-learn-0.15.1.tar.gz (7.0 MB)\r\n  Using cached scikit-learn-0.15.0.tar.gz (7.0 MB)\r\n  Using cached scikit-learn-0.14.1.tar.gz (6.8 MB)\r\n  Using cached scikit-learn-0.14.tar.gz (6.8 MB)\r\n```\r\n", "I was able to get it to work. Steps to fix:\r\n\r\n1. Copy the [requirements.txt](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/requirements.txt) file from the repo into my working directory and installing those requirements explicitly with\r\n\r\n    `$ pip install -r requirements.txt` \r\n\r\n2. Install tflite-model-maker with\r\n\r\n    `$ pip install tflite-model-maker`\r\n\r\nThere seems to be a bug right now where tflite-model-maker is not aware of its own requirements (or something funky with pip is happening) and it is trying to resolve them algorithmically instead of just reading the requirements file.", "Thanks for reporting and @lintian06 to take a look.", "Thank you @Extremesarova for reporting the issue, and @Engineero for the solution!\r\n\r\nThis is a mysterious issue! I think it might be some conflicts among (1) pre-installed pypi packages, and (2) dependencies required by Model Maker (may also related to platforms: OS + python version). Unfortunately, pip is not good enough to tell the information for us to find the root cause. :-(\r\n(Just succeeded to run `!pip install tflite-model-maker==0.3.2` in [colab](https://colab.research.google.com/))\r\n\r\nWe have automatic CI/CD pipelines to capture pip install failures, but cannot cover all cases. Happy to know if you find the root cause, so I can fix it. (Welcome pull requests too!)\r\n", "> it might be some conflicts among (1) pre-installed pypi packages\r\n\r\nI tried installing tflite-model-maker on a clean venv and the error was still there", "Hi @Extremesarova ! \r\nWe are checking to see whether you still need help in this issue . Have you checked these threads yet?  Ref [1](https://www.tensorflow.org/lite/guide/model_maker) , [2 ](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=sr3q-gvm3cI8), [3](https://colab.sandbox.google.com/gist/mohantym/7cee04a31addf6a5cd4caa8b5b354df2/model-maker-image-classification-tutorial.ipynb) .Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, @mohantym! I will try it and post an update here", "Ok @Extremesarova ! Did the issue get resolved ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51031\">No</a>\n"]}, {"number": 51030, "title": "TfLite python delegate support for RB5 robotics platform", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\nRobotics RB5 Platform\r\nLinux qrb5165-rb5 4.19.125 aarch64 GNU/Linux\r\nCPU: Qualcomm QRB5165\r\nRAM: 8GB\r\n- TensorFlow version (you are using):\r\n- Python 3.6.9\r\n- python3-tflite-runtime (2.5.0.post1)\r\n- hexagon_nn_skel_v1.20.0.1\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently it works on cpu cores, I would like to delegate it to libhexagon_nn_skel.so.\r\nWhen I run:\r\n`tflite.load_delegate('libhexagon_nn_skel.so')`\r\nIt returns:\r\n`OSError: libhexagon_nn_skel.so: wrong ELF class: ELFCLASS32`\r\nSame with v65 and v66\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nEveryone using RB5 platform with python, and likely oyher sililar Qualcomm products\r\n**Any Other info.**\r\n", "comments": ["> _Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template_\r\n> \r\n> **System information**\r\n> Robotics RB5 Platform\r\n> Linux qrb5165-rb5 4.19.125 aarch64 GNU/Linux\r\n> CPU: Qualcomm QRB5165\r\n> RAM: 8GB\r\n> \r\n> * TensorFlow version (you are using):\r\n> * Python 3.6.9\r\n> * python3-tflite-runtime (2.5.0.post1)\r\n> * hexagon_nn_skel_v1.20.0.1\r\n> * Are you willing to contribute it (Yes/No):\r\n>   Yes\r\n> \r\n> **Describe the feature and the current behavior/state.**\r\n> Currently it works on cpu cores, I would like to delegate it to libhexagon_nn_skel.so.\r\n> When I run:\r\n> `tflite.load_delegate('libhexagon_nn_skel.so')`\r\n\r\nAt first sight, judging from the delegate file name, I don't think you are loading the correct one.  Here's an example of using this API: [src code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/interpreter_test.py#L416-L429) and [build rule to compile the delegate .so](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/testdata/BUILD#L89-L97)\r\n@terryheo could provide more info about using delegate APIs in Python.\r\n\r\nAs for the TfLite hexagon delegate, have you built the TfLite Hexagon delegate .so file? @karimnosseir could provide more info about this delegate.\r\n\r\n> It returns:\r\n> `OSError: libhexagon_nn_skel.so: wrong ELF class: ELFCLASS32`\r\n> Same with v65 and v66\r\n> **Will this change the current api? How?**\r\n> No\r\n> **Who will benefit with this feature?**\r\n> Everyone using RB5 platform with python, and likely oyher sililar Qualcomm products\r\n> **Any Other info.**\r\n\r\n", "> At first sight, judging from the delegate file name, I don't think you are loading the correct one. Here's an example of using this API: [src code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/interpreter_test.py#L416-L429) and [build rule to compile the delegate .so](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/testdata/BUILD#L89-L97)\r\n> @terryheo could provide more info about using delegate APIs in Python.\r\n> \r\n> As for the TfLite hexagon delegate, have you built the TfLite Hexagon delegate .so file? @karimnosseir could provide more info about this delegate.\r\n\r\nI have downloaded the .so files from [TensorFlow Lite Hexagon delegate](https://www.tensorflow.org/lite/performance/hexagon_delegate) page. It's for Android not for ubuntu based linux I am using, my device has hexagon 698 (Snapdragon 865 should be supported). Mybe I need a native build, can someone help me build .so file as i can't find any guide for building it?\r\n\r\nEDIT:\r\nWhen I run `$ bazel build //tensorflow/lite/delegates/hexagon:hexagon_delegate` I get:\r\n\r\n- `bazel-bin/tensorflow/lite/delegates/hexagon/libhexagon_delegate.a`\r\n- `bazel-bin/tensorflow/lite/delegates/hexagon/libhexagon_delegate.pic.a`\r\n- `bazel-bin/tensorflow/lite/delegates/hexagon/libhexagon_delegate.so`\r\n\r\nIt's all run natively, built on master and r2.5. \r\n\r\npython3 console:\r\n`>>> import tflite_runtime.interpreter as tflite`\r\n`>>> tflite.load_delegate('libhexagon_delegate.so')`\r\nreturns error:\r\n`OSError: /usr/lib/libhexagon_delegate.so: undefined symbol: _ZTVN6tflite21HexagonDelegateKernelE`\r\n", "If you want to run on non-android then you will need to build\r\n* the hexagon delegate (libhexagon_delegate.so - or statically)\r\n* libhexagon_interface.so (You can reach to me directly and i can compile it and share it with you, the code to build it yourself is not fully in OSS yet, we are working on adding the remaining parts).\r\n\r\n", "Hi, @karimnosseir can you share a procedure or example to use tflite hexagon delegate for RB5 platform with python or c++?", "Hi @feefro Sorry this thread got missed.\r\nAll the code for building Hexagon delegate is already on Github under [tensorflow/lite/delegates/hexagon](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/hexagon).\r\nYou can build it using your own toolchain of desire.\r\nThe C/C++ API can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_delegate.h)", "maybe you can provide some CMake example for C++ code?\r\nwhich of those libraries should be linked ?\r\n libhexagon_delegate.so or libhexagon_interface.so  or libhexagon_nn_skel(_v65/_v66).so", "@karimnosseir can you help please?"]}, {"number": 51029, "title": "Query About tflite interpreter usage", "body": "Is it possible to provide input/output memory (pointer) for running inference..\r\n[](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter)\r\n\r\nFrom the snippet below\r\n```\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Fill input buffers\r\n  // TODO(user): Insert code to fill input tensors.\r\n  // Note: The buffer of the input tensor with index `i` of type T can\r\n  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`\r\n\r\n  // Run inference\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n```\r\n1) Does `AllocateTensors()` create memory for input & intermediate & output layers?\r\n2) How can I provide my buffer memory pointer (virtual mem) directly so to avoid setting the memory content.\r\n\r\n\r\nRegards,\r\nHarsh", "comments": ["@szihs \r\n1) Does AllocateTensors() create memory for input & intermediate & output layers?\r\nYes\r\n2)How can I provide my buffer memory pointer (virtual mem) directly so to avoid setting the memory content.\r\nIf there is no public api from lite, for work around its better to oepn his issue is tf discussion forum. as there is a larger community to respond there, this platform is for bugs and performance related issues", "@Saduf2019 may I know where is the channel/forum  link for the same?", "@szihs \r\nsure, please follow [this link](https://discuss.tensorflow.org/), and close this."]}, {"number": 51028, "title": "Add hlo to mhlo logistic translation", "body": "Add `hlo-to-mhlo` logistic op translation.\r\ncc @byronyi ", "comments": []}, {"number": 51027, "title": "Worse predict result from keras h5 model convert to tensorflow frozen pb model", "body": "Hi everyone,\r\nI have try to convert my keras h5 model to tensorflow frozen pb model.\r\nI want to use this pb model in Microsoft ML.net to classify images.\r\nI successfully convert the model to frozen pb, but I get the worse predict result than h5 model.\r\nSome classes can predict well like original model, some classes will get the bad result.\r\nAnyone try this before? Please help me, THX!!", "comments": ["I solve the problem."]}, {"number": 51026, "title": "TensorFlow Lite C API full example", "body": "Is there any example fully in C language using Tensorflow Lite C API? More specifically for image classification or object detection? I have not been able to find any TensorFlow example in C.", "comments": ["@barbaragabriella Could you please refer to this [link](https://www.tensorflow.org/lite/guide/android#use_tflite_c_api) and let us know if it helps ? Thanks !", "The instructions there say to download TensorFlow Lite AAR hosted at MavenCentral which leads to the started page https://search.maven.org/. Which project is it referring to exactly?", "That link is not proper for downloading AAR which is used in TFlite projects, could you please refer to these two already prepared Tensorflow lite AAR.\r\nhttps://search.maven.org/artifact/org.tensorflow/tensorflow-lite-task-text\r\nhttps://search.maven.org/artifact/org.tensorflow/tensorflow-lite-task-vision\r\nThis two mavens can be used in build file as following inside the  code\r\n\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite-task-vision:0.1.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-task-text:0.1.0'\r\n}\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51026\">No</a>\n"]}, {"number": 51025, "title": "Could provide the different platforms compiled release file, as widows lib and  in(cpu gpu) and linux .so?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@xinsuinizhuan ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n", "For more information on tensorflow version release,please take a look at this [link](https://github.com/tensorflow/tensorflow/releases).It provides the details regarding the releases.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51025\">No</a>\n"]}, {"number": 51024, "title": "Revert https://github.com/tensorflow/tensorflow/pull/50292", "body": "It causes errors like: https://github.com/tensorflow/tensorflow/pull/50292#issuecomment-888581796", "comments": ["@mihaimaruseac FYI"]}, {"number": 51023, "title": "[determinism] Add unimplemented exception to nearest-neighbor resizing", "body": "This current PR adds and tests determinism-unimplemented exception-throwing for `tf.image.resize` when `method=ResizeMethod.NEAREST` and when its GPU-implemented backprop code-path is executed.\r\n\r\nThis PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#nearest-neighbor-image-resizing).\r\n\r\nCC @reedwm, @sanjoy, @nluehr", "comments": []}, {"number": 51022, "title": "Disallow negative ngram_widths values in tf.raw_ops.StringNGrams", "body": "PiperOrigin-RevId: 387148179\nChange-Id: I641395a09a208be72ef9b3ceb128cf8a83a0775b", "comments": []}, {"number": 51021, "title": "Disallow negative ngram_widths values in tf.raw_ops.StringNGrams", "body": "PiperOrigin-RevId: 387148179\nChange-Id: I641395a09a208be72ef9b3ceb128cf8a83a0775b", "comments": []}, {"number": 51020, "title": "Disallow negative ngram_widths values in tf.raw_ops.StringNGrams", "body": "PiperOrigin-RevId: 387148179\nChange-Id: I641395a09a208be72ef9b3ceb128cf8a83a0775b", "comments": []}, {"number": 51019, "title": "Disallow negative ngram_widths values in tf.raw_ops.StringNGrams", "body": "PiperOrigin-RevId: 387148179\nChange-Id: I641395a09a208be72ef9b3ceb128cf8a83a0775b", "comments": []}, {"number": 51018, "title": "Prevent heap OOB read in TFLite's `gather.cc`.", "body": "Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8", "comments": []}]