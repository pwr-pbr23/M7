[{"number": 16171, "title": "Updating the docker login command. The email flag is deprecated.", "body": "PiperOrigin-RevId: 181769938", "comments": []}, {"number": 16170, "title": "tf.train.latest_checkpoint fails when paths have \"//\" ", "body": "```tf.train.latest_checkpoint``` returns the error \r\n```\r\nERROR:tensorflow:Couldn't match files for checkpoint\r\n``` \r\nwhen paths in the \"checkpoint\" file have \"//\" instead of \"/\". Usually, good practice of using ```os.path.join``` will help avoid this situation, but I believe TensorFlow should account for '//'s in paths as several developers do not use ```os.path.join```\r\n\r\n### To reproduce this error\r\n- Create directory ```/home/user/model```\r\n- Create file ```/home/user/model/checkpoint``` whose contents are \r\n\r\n```\r\nmodel_checkpoint_path: \"/home/user/model//model_1\"\r\nall_model_checkpoint_paths: \"/home/user/model//model_0\"\r\nall_model_checkpoint_paths: \"/home/user/model//model_1\"\r\n```\r\n- Create empty files ```model/model_1.data-00000-of-00001```, ```model/model_1.index```, ```model/model_1.meta```\r\n- Run ```tf.train.latest_checkpoint('/home/user/model')```. \r\nExpected output is ```u'/home/user/model//model_1'```, but TF returns an error ```ERROR:tensorflow:Couldn't match files for checkpoint /home/user/model//model_1```\r\n\r\n\r\n\r\n\r\n### System information\r\n- OS: Ubuntu 16.04\r\n- TF installed via ```pip install tensorflow-gpu```\r\n- TF version: 1.4.1\r\n- Python version: 2.7 \r\n- CUDA/cuDNN version: 8.0\r\n- GPU model and memory: GeForce GTX 1080, 8GB\r\n\r\n### Have I written custom code\r\nYes\r\n\r\n### OS Platform and Distribution\r\nUbuntu 16.04\r\n\r\n### TensorFlow installed from\r\nInstalled thru ```pip install tensorflow-gpu```\r\n\r\n### TensorFlow version\r\n1.4.1\r\n\r\n### Bazel version\r\nN/A\r\n\r\n### Exact command to reproduce\r\nN/A\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nExact command to reproduce", "I have noticed the same problem on Ubuntu 16.04.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 119 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 16169, "title": "Branch 182086883", "body": "", "comments": []}, {"number": 16168, "title": "Fix unicode string conversion issue in Python 2", "body": "This fix tries to address the issue raised in #16149 where the unicode string conversion with Python 2 does not match the behavior with Python 3.\r\n\r\nThe issue was that in Python 3, TensorFlow tries to do a unicode conversion in UTF8 while in Python 2\r\nthe default conversion was used.\r\n\r\nThis fix addresses the issue so that behaviors of TensorFlow with Python 2 and Python 3 match.\r\n\r\nThis fix fixes #16149.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16167, "title": "Documentation Method Templates Improvement", "body": "### System information\r\nN/A\r\n\r\n### Describe the problem\r\nThe method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.\r\n\r\nI.e. this is what we have at present (bad): \r\n<img width=\"399\" alt=\"screen shot 2018-01-16 at 2 23 08 pm\" src=\"https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png\">\r\n\r\nThis is a more practical and copy/paste-friendly version:\r\n<img width=\"426\" alt=\"screen shot 2018-01-16 at 2 22 49 pm\" src=\"https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png\">\r\n\r\nI'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.\r\n\r\n### Source code / logs\r\nN/A", "comments": ["@wolffg, could you consider this feature req. Thanks for the suggestion @aidangomez.", "This is an easy fix, and clear win for regular functions.\r\n\r\nIt's probably better not to change this for methods. Adding the path to the class, on each method signature  seems like unnecessary noise, since you rarely do a direct lookup of the method:\r\n\r\nget_collection(...)\r\ntf.Graph.get_collection(...)\r\n\r\nget_name_scope(...)\r\ntf.Graph.get_name_scope(...)\r\n\r\n\r\nWDYT?", "Agreed. That's an important distinction.", "Fix inflight.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 16166, "title": "import error cudnnSetRNNDescriptor_v6 in tensorflow", "body": "hi,\r\nI have installed tensorflow-gpu on ubuntu server.at the beginning, my tensorflow work well but recently it gives an error when I logging to the python console and import tensorflow as tf.(server has python 3.5)\r\n![tensor](https://user-images.githubusercontent.com/22906072/35007461-c60ec326-fb34-11e7-84f2-3ff8ca685895.jpg)\r\n", "comments": ["Seems your CUDA installation is not properly configured. Try run `ldd libtensorflow_framework.so` to see if there're missing dependencies.", "![image](https://user-images.githubusercontent.com/22906072/35027864-0e02c700-fb8e-11e7-96a1-9748b1164d68.png)\r\n", "For the full path to `libtensorflow_framework.so`, see line `ImportError` of error log.", "yes, framework available in the specific location.", "I mean `ldd FULL_PATH/libtensorflow_framework.so`. \r\nAnd it would be better if you fill out the issue template. This unlikely to be a tensorflow bug, make sure your environment is set up exactly like https://www.tensorflow.org/install/install_linux says.\r\n", "ok.i have updated tensorflow mention in site.i will try again for this and if anything there i will post here. Thanks for your valuable time. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "its solved"]}, {"number": 16165, "title": "Error when building from source Fedora 27 CUDA 9.1", "body": "### System information\r\n- OS Platform and Distribution: Fedora 27\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: r1.4\r\n- Python version: 3.6.3\r\n- Bazel version: 0.8.1\r\n- GCC/Compiler version: 7.2.1\r\n- CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5\r\n- **GPU model and memory**: NVidia Geforce GTX 960 4GB\r\n- Exact command to reproduce: bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nSo I'm attempting to build tensorflow from source on fedora with the version of CUDA and cuDNN I already had installed to avoid have to also install an older version. The build however errors with the following message:\r\n\r\n```\r\nERROR: .cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/xd009642/.cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory)\r\n<command-line>:0:15: warning: ISO C++11 requires whitespace after the macro name\r\n<command-line>:0:1: error: macro names must be identifiers\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.392s, Critical Path: 0.12s\r\nFAILED: Build did NOT complete successfully\r\n```\r\nI also tried the command `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package` from [here](http://www.python36.com/install-tensorflow141-gpu/) with the same end result.\r\n\r\nAny guidance is appreciated as this is my first time using bazel (and also trying to compile tensorflow).", "comments": [" I suspect you need the 1.5 release candidate or newer to compile with CUDA 9.1, but I'm not sure. I'd try that first. Also, you could try making sure you can compile with CPU only first (that's usually a lot easier and can help you isolate problem.) And typically we only support Ubuntu, so I have no Fedora experience, but others on the forum might.\r\n", "I tried again on 1.5, the build completed successfully for the CPU but for GPU I got this message (paths cut down to just filenames for brevity):\r\n\r\n```\r\n2 errors detected in the compilation of \".../tmpxft_00000ee6_00000000-6_lstm_ops_gpu.cu.cpp1.ii\".\r\nERROR: /home/xd009642/3rdparty/tensorflow/tensorflow/contrib/rnn/BUILD:218:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.pic.o' was not created\r\nERROR: tensorflow/tensorflow/contrib/rnn/BUILD:218:1: not all outputs were created or valid\r\n```\r\n\r\nI'm going to run again since I forgot --verbose_failures on the GPU build and see if that sheds anymore insight.", "I got this message:\r\n\r\n```\r\nusr/include/bits/floatn.h(61): error: invalid argument to attribute \"__mode__\"\r\nusr/include/bits/floatn.h(73): error: identifier \"__float128\" is undefined\r\n```\r\n\r\nWhich led me to https://github.com/caffe2/caffe2/issues/1194 I'm currently trying out the added host define recommended in the [arch link in that issue](https://git.archlinux.org/svntogit/community.git/commit/trunk?h=packages/cuda&id=ae90e4d243510e9565e66e9e8e08c509f5719fe0). If that solves it I'll close the issue and hopefully this will help out other users!", "So firstly apologies, this is a large template based error. It got a lot further in the build and failed in list_kernels.cu\r\n\r\nHere is the full error it seems to be complaining about misuse of an std::tuple, so I'm guessing this is a glibc/nvcc issue? EDIT [this](https://github.com/kokkos/kokkos/issues/1306) seems to be the same issue\r\n\r\n```\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>&; bool <anonymous> = true; _Elements = {tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]':\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:670:591:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ConstructibleTuple<_UElements ...>() && std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1 == 1)), tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_NonNestedTuple<const tuple<_Tps ...>&>()), bool>::type <anonymous> > constexpr std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::tuple(const std::tuple<_Tps ...>&) [with _UElements = {tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece}; _Dummy = void; typename std::enable_if<((std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ConstructibleTuple<_UElements ...>() && std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1 == 1)), tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_NonNestedTuple<const tuple<_Tps ...>&>()), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/stl_pair.h:198:8:   required from 'struct std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/ext/aligned_buffer.h:84:62:   required from 'struct __gnu_cxx::__aligned_buffer<std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> > >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable_policy.h:248:39:   required from 'struct std::__detail::_Hash_node_value_base<std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> > >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable_policy.h:279:8:   required from 'struct std::__detail::_Hash_node<std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> >, true>'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable_policy.h:2007:65:   required from 'struct std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> >, true> > >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable.h:173:7:   required from 'class std::_Hashtable<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> >, std::allocator<std::pair<const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece> >, tensorflow::UnaryVariantOpRegistry::TupleHash, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/unordered_map.h:104:12:   required from 'class std::unordered_map<std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, tensorflow::Variant*)>, tensorflow::UnaryVariantOpRegistry::TupleHash>'\r\n./tensorflow/core/framework/variant_op_registry.h:183:162:   required from here\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:497:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/type_traits:1547:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:504:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece>&; bool <anonymous> = true; _Elements = {tensorflow::VariantUnaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement\r\n     }\r\n ^\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>&; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]':\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:670:591:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ConstructibleTuple<_UElements ...>() && std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1 == 1)), tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_NonNestedTuple<const tuple<_Tps ...>&>()), bool>::type <anonymous> > constexpr std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::tuple(const std::tuple<_Tps ...>&) [with _UElements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}; _Dummy = void; typename std::enable_if<((std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ConstructibleTuple<_UElements ...>() && std::_TC<((1 == sizeof... (_UElements)) && (! std::is_same<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::tuple<_Tps ...> >::value)), tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1 == 1)), tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>::_NonNestedTuple<const tuple<_Tps ...>&>()), bool>::type <anonymous> = <missing>]'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/stl_pair.h:198:8:   required from 'struct std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/ext/aligned_buffer.h:84:62:   required from 'struct __gnu_cxx::__aligned_buffer<std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> > >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable_policy.h:248:39:   required from 'struct std::__detail::_Hash_node_value_base<std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> > >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable_policy.h:279:8:   required from 'struct std::__detail::_Hash_node<std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> >, true>'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable_policy.h:2007:65:   required from 'struct std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> >, true> > >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/hashtable.h:173:7:   required from 'class std::_Hashtable<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> >, std::allocator<std::pair<const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)> > >, std::__detail::_Select1st, std::equal_to<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece> >, tensorflow::UnaryVariantOpRegistry::TupleHash, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >'\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/unordered_map.h:104:12:   required from 'class std::unordered_map<std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>, std::function<tensorflow::Status(tensorflow::OpKernelContext*, const tensorflow::Variant&, const tensorflow::Variant&, tensorflow::Variant*)>, tensorflow::UnaryVariantOpRegistry::TupleHash>'\r\n./tensorflow/core/framework/variant_op_registry.h:186:180:   required from here\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:497:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/type_traits:1547:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:504:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>&; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement\r\n     }\r\n ^\r\nERROR: /home/xd009642/3rdparty/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created\r\nERROR: /home/xd009642/3rdparty/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid\r\n```", "I have not tried to build tensorflow on fedora yet. You can try by using latest bazel and linux kernal and gcc version according to https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html . Also have you checked if nvidia driver is loaded properly by running command \"nvidia-smi\" ?", "I'll try when I get home from work, however that page show you can build GCC 6.3, I have GCC 7.2 installed. I saw some stuff yesterday that suggests that cuda is broken with GCC >6.x I'll see if I can dig it out later\r\n\r\nEDIT: smi output to confirm the driver is installed properly.\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 387.34                 Driver Version: 387.34                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 960     Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   23C    P8     7W / 160W |    292MiB /  1993MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1314      G   /usr/libexec/Xorg                             15MiB |\r\n|    0      1492      G   /usr/bin/gnome-shell                          42MiB |\r\n|    0      1935      G   /usr/libexec/Xorg                             76MiB |\r\n|    0      2048      G   /usr/bin/gnome-shell                          78MiB |\r\n|    0      2981      G   ...-token=49AE7F2C125E2F5D8E867A8E61CBEB05    71MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "What is the status? Did you downgrade gcc to version 6 or 5.4?", "Status hasn't moved on much. I didn't want to downgrade and installing more than one gcc at once seemed a bit of a pain so I tried the nvidia docker approach. Which when installed via the distros package manager deletes things like the dockerd/daemon.json which took me down another rabbit hole...\r\n\r\nI'm going to give it a proper stab today and hopefully will have something to report back (hopefully success).", "So I've installed gcc 6.4 under the alias cuda-gcc (thanks to the negativo17 nvidia repo). Set that as the path to gcc in configure but it's still getting jammed on the template code. I think from looking at similar issues in other cuda based projects it's nvcc thinking it can compile it when it should instead forward it to gcc.\r\n\r\nWith this in mind I've removed the reference to the list_kernels gpu source and I'm going to see if that means it builds. I admit I'm not familiar with bazel so if you know a better way to force the fallback to gcc feel free to chip in :smile: ", "Right it worked! For other fedora 27 users in the same situation here's a summary of the steps taken:\r\n\r\n* Add the [negativo17 nvidia repo ](https://negativo17.org/cuda-9-0-cudnn-7-0-and-wayland-support-in-fedora-27/) \r\n* Install nvidia drivers, cuda, cdnn, cuda-gcc and cuda-gcc-c++\r\n* Patch cuda/include/host_defines.h and append `#define _BITS_FLOATN_H`\r\n* Run the configure script, point to cuda and cuda-gcc\r\n* Go into tensorflow/core/kernel/BUILD and remove gpu_srcs for `list_kernels` library\r\n* Build\r\n\r\nThere are naturally some restarts required for driver installs to take effect, but I think that's all the steps.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "### I had the same issue with multiple sources. Being a minimalist, Instead of using negativo's repositories (**nothing against them, I think they are great and convenient!**) Here is what I did on Fedora 27 after installing all the normal development packages included in epel-release and so forth:\r\n\r\n### 1. Compiled Video card drivers from source using **NVIDIA-Linux-x86_64-390.48.run** file provided by NVidia..\r\n### 2. Compiled the Cuda using cuda_**9.1.85_387.26_linux.run**. as well as declined to install driver for GPU. patched using the three run files, _cuda_9.1.85.1_linux.run cuda_9.1.85.2_linux.run_ _cuda_9.1.85.3_linux.run_.\r\n\r\n### 3. Created two files within the **/etc/profile.d/** directory:\r\n```\r\ntouch /etc/profile.d/cuda.sh\r\necho \"PATH=$PATH:/usr/local/cuda/bin\" >> /etc/profile.d/cuda.sh\r\necho \"export PATH\" >> /etc/profile.d/cuda.sh\r\ntouch /etc/profile.d/cuda.csh\r\necho \"set path = ( $path /usr/local/cuda/bin )\" >> /etc/profile.d/cuda.csh\r\n```\r\n### 4. Installed gcc63 from package - More than likely other versions will function as well.\r\n### 5. Created the following two symbolic links:\r\n```\r\n/usr/local/cuda/bin/gcc -> /usr/bin/gcc63\r\n/usr/local/cuda/bin/g++ -> /usr/bin/g++63\r\n```\r\n### 6. Took the advice from \"xd009642\"  Patch cuda/include/host_defines.h\r\n```\r\necho` \"#define _BITS_FLOATN_H\" >> /usr/local/cuda/include/host_defines.h\r\n```\r\n### This resolved all compiling issues with NVCC for me so far. I understand many gcc versions are not supported but it is likely that many will work as well.\r\n\r\n\r\n"]}, {"number": 16164, "title": "Accidentally cancelled inceptionV3 during install, now can't install at all", "body": "Hello,\r\ni was setting up tensorflow for image classification, and after i ran : \r\n\r\npython -m scripts.retrain \\\r\n  --bottleneck_dir=tf_files/bottlenecks \\\r\n  --model_dir=tf_files/models/\"${ARCHITECTURE}\" \\\r\n  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\r\n  --output_graph=tf_files/retrained_graph.pb \\\r\n  --output_labels=tf_files/retrained_labels.txt \\\r\n  --architecture=\"${ARCHITECTURE}\" \\\r\n  --image_dir=tf_files/flower_photos\r\n\r\nIt automatically started installing inception, i realized that i needed to change some options so i cancelled the install of inception.\r\nNow i believe that i have a half install that doesn't let me install the full package or use the half package.\r\n\r\nI may be wrong, but any suggestions would be appreciated.\r\nFYI: i've run :\r\npip install inception, to which i receive a \"python setup.py egg_info\" failed with error code 1 in {my local/temp dir}\r\n\r\nI also just tried running the scripts.retrain again, to which i receive a \"EOFError: compressed file ended before the end-of-stream marker was reached\"\r\n\r\nRunning on Windows 7", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: no\r\nOS Platform and Distribution: Windows 7\r\nTensorFlow installed from: github and pip install\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory: gtx 970 16gb\r\nExact command to reproduce: python -m scripts.retrain \r\n--bottleneck_dir=tf_files/bottlenecks \r\n--model_dir=tf_files/models/\"${ARCHITECTURE}\" \r\n--summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \r\n--output_graph=tf_files/retrained_graph.pb \r\n--output_labels=tf_files/retrained_labels.txt \r\n--architecture=\"${ARCHITECTURE}\" \r\n--image_dir=tf_files/flower_photos", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@shlens any idea?", "You should be able to recover from this error by deleting model_dir, in your case `tf_files/models/\"${ARCHITECTURE}\"`. Closing this for now, please reopen if that doesn't work!", "facing the same issue. Can you help me, how have you resloved it?"]}, {"number": 16163, "title": "Dataset.from_generator doesn't release memory after recreating the session", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0-rc0\r\n- **Python version**: Python 3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nAfter closing the session and creating new one an iterator creates the generator instance but doesn't free the memory of the previous one.\r\n\r\nEvery calling of the line `session.run(x)` (see below) increases memory consumption of the script:\r\n\r\n- 519 MiB after the first,\r\n- 600 MiB after the second,\r\n- 681 MiB after the third and so on.\r\n\r\nAs you can see the delta is equal to 80 MiB = N * sizeof(data.dtype). (data.dtype is float64 here)\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nN = 10 * 1024 * 1024\r\n\r\ndef generate():\r\n  data = np.random.rand(N)\r\n  for k in range(N):\r\n    yield data[k].copy()\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  x = tf.data.Dataset\\\r\n    .from_generator(generate, tf.float32)\\\r\n    .make_one_shot_iterator()\\\r\n    .get_next()\r\n\r\nwhile True:\r\n  session = tf.Session(graph=graph)\r\n  session.run(x) # <--- PUT A BREAKPOINT HERE!\r\n                 #  Be careful running the code without it!\r\n  session.close()\r\n```", "comments": ["Of course, if `session.run` consumes all the elements produced by the generator, it will be removed.", "@mrry, could you comment on this please?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yes, this is a bug. I have a fix in preparation.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The fix has been submitted internally, but not yet merged into the Git master branch. It should be coming in the next push!"]}, {"number": 16162, "title": "segmentation fault when calling help on GraphNodeProto", "body": "### System information\r\n- **Have I written custom code**:\r\nNothing beside the example code below.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux RBSylaptop 4.14.0-2-amd64 #1 SMP Debian 4.14.7-1 (2017-12-22) x86_64 GNU/Linux\r\n\r\n- **TensorFlow installed from**:\r\n`    $ pip3 install tensorflow-gpu`\r\n\r\n- **TensorFlow version**:\r\ntf.VERSION = 1.4.1\r\ntf.GIT_VERSION = v1.4.0-19-ga52c8d9\r\ntf.COMPILER_VERSION = v1.4.0-19-ga52c8d9\r\n\r\n- **Python version**:\r\nPython 3.6.4\r\n\r\n- **Bazel version**:\r\nN/A\r\n\r\n- **GCC/Compiler version**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX 1070\r\n8192 MB\r\n\r\n- **Exact command to reproduce**:\r\npython3 -c \"import tensorflow as tf; help(tf.profiler.profile(tf.get_default_graph()))\"\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\n### Problem description\r\nCalling help on the value returned by `tf.profiler.profile` generate a segmentation fault. However it might be related to the warning about the python version.\r\n\r\n### Source code / logs\r\nHere is the full python session:\r\n```\r\n$ python3                                                                                \r\nPython 3.6.4 (default, Jan  5 2018, 02:13:53) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n>>> help(tf.profiler.profile(tf.get_default_graph()))\r\nParsing Inputs...\r\n\r\n=========================Options=============================\r\n-max_depth                  10000\r\n-min_bytes                  0\r\n-min_peak_bytes             0\r\n-min_residual_bytes         0\r\n-min_output_bytes           0\r\n-min_micros                 0\r\n-min_accelerator_micros     0\r\n-min_cpu_micros             0\r\n-min_params                 0\r\n-min_float_ops              0\r\n-min_occurrence             0\r\n-step                       -1\r\n-order_by                   name\r\n-account_type_regexes       _trainable_variables\r\n-start_name_regexes         .*\r\n-trim_name_regexes          \r\n-show_name_regexes          .*\r\n-hide_name_regexes          \r\n-account_displayed_op_only  true\r\n-select                     params\r\n-output                     stdout:\r\n\r\n==================Model Analysis Report======================\r\nnode name | # parameters\r\n_TFProfRoot (--/0 params)\r\n\r\n======================End of Report==========================\r\nzsh: segmentation fault  python3\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "I edited my comment above, although I don't think these informations are relevant to this bug.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I recompiled from the sources, and it also crashes.\r\nIt crashes as well when I try to get the help of tf.RunMetadata().\r\nI suspect it would crash if I try to get the help of any protobuf class.", "It would help if you could try with python 3.5 to see if it's the py3 version problem.\r\n\r\nCC @av8ramit ", "@Celelibi is this still an issue with 1.5?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Celelibi is this still an issue with r1.5 or r1.6rcs?", "Nagging Assignee @av8ramit: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16161, "title": "tf.case raising IllegalArgumentError 'None of the conditions evaluated as True' when used with Dataset", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 64bit\r\n- **TensorFlow installed from (source or binary)**:\r\nvia pip\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.5\r\n\r\nWhen I use tf.case within a tf.data.Dataset, I get an IllegalArgumentError 'None of the conditions evaluated as True'. However, when I use the same code without the Dataset, it works fine. Furthermore, if I understand the error message correctly, it already tells me that one condition evaluated to true (see the end of the first line):\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): assertion failed: [None of the conditions evaluated as True. Conditions: (Equal_3:0, Equal_4:0, Equal_5:0), Values:] [1 0 0]\r\n\t [[Node: case/If_0/Assert_1/AssertGuard/Assert = Assert[T=[DT_STRING, DT_BOOL], summarize=3](case/If_0/Assert_1/AssertGuard/Assert/Switch, case/If_0/Assert_1/AssertGuard/Assert/data_0, case/If_0/Assert_1/AssertGuard/Assert/Switch_1)]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n```\r\n\r\nThe following code reproduces the issue:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef random_map(i):\r\n\trandom_int = tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)\r\n\trandom_int = tf.Print(random_int, [random_int, tf.equal(random_int, 0), tf.equal(random_int, 1), tf.equal(random_int, 2)], 'random_int')\r\n\r\n\tresult = tf.case([\r\n\t\t(tf.equal(random_int, 0), lambda: i * 10000),\r\n\t\t(tf.equal(random_int, 1), lambda: i * 20000),\r\n\t\t(tf.equal(random_int, 2), lambda: i * 30000)\r\n\t], exclusive=True)\r\n\r\n\treturn result\r\n\r\n\r\nprint('working =========================================================================')\r\nwith tf.Session() as sess:\r\n\tinput_pl = tf.placeholder(dtype=tf.int32)\r\n\tresult = random_map(input_pl)\r\n\tfor i in range(5):\r\n\t\tresult_value = sess.run(result, feed_dict={input_pl: i})\r\n\t\tprint(result_value)\r\n\r\nprint('not working =====================================================================')\r\nwith tf.Session() as sess:\r\n\tdataset = tf.data.Dataset.from_tensor_slices(tf.range(5))\r\n\tdataset = dataset.map(random_map)\r\n\titerator = dataset.make_one_shot_iterator()\r\n\tnext_result = iterator.get_next()\r\n\r\n\tfor i in range(5):\r\n\t\tresult_value = sess.run(next_result)\r\n\t\tprint(result_value)\r\n```\r\n\r\nI also found [this question]( http://www.programfaqs.com/faq/tensorflow-case-error-invalid-argument-assertion-failed-none-of-the-conditions-evaluated-as-true/), which seems to be the same problem.", "comments": ["@mrry, could you please take a look?", "It looks like the bug has already been fixed: I ran your code with TF 1.5.0rc1 and it works as expected. (I also confirmed that it failed with TF 1.4.) Feel free to reopen if you continue to have problems after upgrading.", "Can you suggest a work around for 1.4?"]}, {"number": 16160, "title": "tf.contrib.lookup.HashTable(kv_initializer) does not work in eager mode.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 7.3.0 (clang-703.0.31)\r\nTarget: x86_64-apple-darwin15.4.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.2)\r\nprotobuf (3.4.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:\r\n\r\n== nvidia-smi ===================================================\r\n/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 7.3.0 (clang-703.0.31)\r\nTarget: x86_64-apple-darwin15.4.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.2)\r\nprotobuf (3.4.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0-rc1\r\ntf.GIT_VERSION = v1.5.0-rc0-9-gf9472619f6\r\ntf.COMPILER_VERSION = v1.5.0-rc0-9-gf9472619f6\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:\r\n\r\n== nvidia-smi ===================================================\r\n/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.5.0-rc0-9-gf9472619f6 1.5.0-rc1\r\n\r\n### Describe the problem\r\nCan not use hashtable in eager mode.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nkeyfile = \"./key_dict\"\r\nkv_initializer = tf.contrib.lookup.TextFileInitializer(\r\n    keyfile, tf.string, 0, tf.int64, 1, delimiter=\"\\t\")\r\ntable = tf.contrib.lookup.HashTable(kv_initializer, 0)\r\ntable.init.run()\r\n\r\nfilenames = [\"./data1\"]\r\ndataset = tf.data.TextLineDataset(filenames)\r\n#dataset = dataset.map(lambda tkns:table.lookup(tkns))\r\nfor x in tfe.Iterator(dataset):\r\n    print(x)\r\n\r\nTraceback (most recent call last):\r\n  File \"torch/textline.py\", line 13, in <module>\r\n    table = tf.contrib.lookup.HashTable(kv_initializer, 0)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 282, in __init__\r\n    super(HashTable, self).__init__(table_ref, default_value, initializer)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 168, in __init__\r\n    self._init = initializer.initialize(self)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 531, in initialize\r\n    if constant_op.is_constant(filename):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 224, in is_constant\r\n    op = tensor_or_op.op\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 825, in op\r\n    raise AttributeError(\"op not supported for Eager Tensors.\")\r\nAttributeError: op not supported for Eager Tensors.\r\n```", "comments": ["@alextp, please comment/take a look.", "thanks for the fix. But now  \r\n\r\n```\r\nFile \"torch/textline.py\", line 16, in <module>\r\n    table.init.run()\r\nAttributeError: 'NoneType' object has no attribute 'run'\r\n```\r\nAnd if I comment this out, \r\n```\r\ndataset = dataset.map(lambda tkns:table.lookup(tkns))\r\n```\r\ngive this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1022, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 100, in _convert_to_graph_tensor\r\n    return constant_op.constant(value.numpy())\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 677, in numpy\r\n    raise ValueError(\"Resource handles are not convertible to numpy.\")\r\nValueError: Resource handles are not convertible to numpy.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 524, in _apply_op_helper\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1022, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 100, in _convert_to_graph_tensor\r\n    return constant_op.constant(value.numpy())\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 677, in numpy\r\n    raise ValueError(\"Resource handles are not convertible to numpy.\")\r\nValueError: Resource handles are not convertible to numpy.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"torch/textline.py\", line 39, in <module>\r\n    dataset = dataset.map(lambda tkns:table.lookup(tkns))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 780, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1591, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 486, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 321, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 338, in _create_definition_if_needed_impl\r\n    outputs = self._func(*inputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1556, in tf_map_func\r\n    ret = map_func(nested_args)\r\n  File \"torch/textline.py\", line 39, in <lambda>\r\n    dataset = dataset.map(lambda tkns:table.lookup(tkns))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py\", line 229, in lookup\r\n    self._table_ref, key_tensor, self._default_value, name=scope)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_lookup_ops.py\", line 457, in _lookup_table_find_v2\r\n    default_value=default_value, name=name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 528, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'table_handle' to a tensor and failed. Error: Resource handles are not convertible to numpy.\r\n```", "On Wed, Jan 17, 2018 at 11:45 PM, xiaoyun wu <notifications@github.com>\nwrote:\n\n> thanks for the fix. But now File \"torch/textline.py\", line 16, in\n> table.init.run()\n> AttributeError: 'NoneType' object has no attribute 'run'\n>\njust do table.init.run() (which uses a session) if you're in graph mode.\n\n\n> And if I comment this out,\n> dataset = dataset.map(lambda tkns:table.lookup(tkns))\n> give this:\n> Traceback (most recent call last):\n> File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/\n> python3.6/site-packages/tensorflow/python/framework/op_def_library.py\",\n> line 510, in _apply_op_helper\n> preferred_dtype=default_dtype)\n> File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/\n> python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1022,\n> in internal_convert_to_tensor\n> ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n> File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/\n> python3.6/site-packages/tensorflow/python/eager/function.py\", line 100,\n> in _convert_to_graph_tensor\n> return constant_op.constant(value.numpy())\n> File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/\n> python3.6/site-packages/tensorflow/python/framework/ops.py\", line 677, in\n> numpy\n> raise ValueError(\"Resource handles are not convertible to numpy.\")\n> ValueError: Resource handles are not convertible to numpy.\n>\n> You seem to have found a bug with the eager-dataset integration. For\ncleanliness, can you file another issue?\n\n-- \n - Alex\n", "it is the same code that still failing... I do not get the reason for file another issue.\r\n", "Ok. I have a fix which is getting sent through to github soon.\n\nOn Fri, Jan 19, 2018 at 2:46 AM, xiaoyun wu <notifications@github.com>\nwrote:\n\n> it is the same code that still failing... I do not get the reason for file\n> another issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16160#issuecomment-358930504>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXxWWFryD_dej-Bb0PbjrQAKBrQ0ks5tMHJ8gaJpZM4RfwC7>\n> .\n>\n\n\n\n-- \n - Alex\n"]}, {"number": 16159, "title": ":q", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16158, "title": "GAN model: move generated and real operations under discriminator namespace", "body": "Hi everybody,\r\n\r\n`gan_model` runs the discriminator on both the generated and real data. This PR changes/fixes the namespace of the generated graph.\r\n\r\n* Current: The network variables and operations on generated data are in the `Discriminator` namespace but the operations on real data are in the `Discriminator_1` namespace.\r\n* PR: The network variables stay in the `Discriminator` namespace. Operations on generated data are in `Discriminator/generated` and operations on real data are in `Discriminator/real`.\r\n\r\n`gan_model` only searches the `Discriminator` namespace for regularization. Presumably, if you were running activity regularization in your discriminator, only the part on generated data would be picked up. Plus, the graph looks much better visually this way and you can tell which discriminator is which.\r\n\r\nCheers", "comments": ["Can one of the admins verify this patch?", "Previous creates two separate namespaces, `Discriminator`, `Discriminator_1`.\r\n![image](https://user-images.githubusercontent.com/12462956/34987069-e4b9f762-fa87-11e7-9515-031c839e62af.png)\r\n\r\nPR creates `Discriminator/real` and `Discriminator/generated`, both pulling from variable scope `Discriminator`.\r\n![image](https://user-images.githubusercontent.com/12462956/34986975-8f51f9fa-fa87-11e7-8707-0a6e87622ea5.png)\r\n\r\n"]}, {"number": 16157, "title": "Update rules_closure to fix bazel version check", "body": "Related https://github.com/bazelbuild/bazel/issues/4425#issuecomment-357681237", "comments": ["test this please", "Also test on Windows with Bazel:\r\nhttp://ci.tensorflow.org/view/TF%20pull%20requests/job/tensorflow-pr-win-bazel/56/console"]}, {"number": 16156, "title": "fixed_address_empty", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16155, "title": "[Bug] slim.tfexample_decoder.TFExampleDecoder() crashes if RAW image with float type is used.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 'unkown', '1.4.0-rc0'\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0.3\r\n- **GPU model and memory**: Pascal Titan X\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nTo feed raw images with floating type saved in tfrecord, I am using slim.tfexample_decoder.TFExampleDecoder().\r\n\r\nThe problem is if I set the dtype of raw image to tf.float32, the function doesn't work with following error messages. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 475, in <module>\r\n    tf.app.run()\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 304, in main\r\n    common_queue_min=10 * FLAGS.batch_size)\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py\", line 97, in __init__\r\n    tensors = dataset.decoder.decode(data, items)\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 427, in decode\r\n    outputs.append(handler.tensors_to_item(keys_to_tensors))\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 324, in tensors_to_item\r\n    return self._decode(image_buffer, image_format)\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py\", line 353, in _decode\r\n    pred_fn_pairs, default=decode_image, exclusive=True)\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3262, in case\r\n    case_seq = _build_case()\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3257, in _build_case\r\n    strict=strict, name=\"If_%d\" % i)\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1902, in cond\r\n    (val_x.dtype.name, val_y.dtype.name))\r\nValueError: Outputs of true_fn and false_fn must have the same type: float32, uint8\r\n\r\n```\r\n\r\nTo see what the problem is, I checked the ``slim/data/tfexample_decoder.py``.\r\nI figured out that there is a ``case()`` function in `` _decode()`` of ``class Image``, and if I set dtype for the raw image as other than uint8, then, the returning tensors' dtypes from ``decode_image()`` and ``decode_raw()`` used in ``case()`` are always mis-matched. You can check the code in below.\r\n\r\n```\r\n def _decode(self, image_buffer, image_format):\r\n    \"\"\"Decodes the image buffer.\r\n    Args:\r\n      image_buffer: The tensor representing the encoded image tensor.\r\n      image_format: The image format for the image in `image_buffer`. If image\r\n        format is `raw`, all images are expected to be in this format, otherwise\r\n        this op can decode a mix of `jpg` and `png` formats.\r\n    Returns:\r\n      A tensor that represents decoded image of self._shape, or\r\n      (?, ?, self._channels) if self._shape is not specified.\r\n    \"\"\"\r\n    def decode_image():\r\n      \"\"\"Decodes a png or jpg based on the headers.\"\"\"\r\n      return image_ops.decode_image(image_buffer, self._channels)\r\n\r\n    def decode_raw():\r\n      \"\"\"Decodes a raw image.\"\"\"\r\n      return parsing_ops.decode_raw(image_buffer, out_type=self._dtype)\r\n\r\n    pred_fn_pairs = {\r\n        math_ops.logical_or(\r\n            math_ops.equal(image_format, 'raw'),\r\n            math_ops.equal(image_format, 'RAW')): decode_raw,\r\n    }\r\n    image = control_flow_ops.case(\r\n        pred_fn_pairs, default=decode_image, exclusive=True)\r\n\r\n    image.set_shape([None, None, self._channels])\r\n    if self._shape is not None:\r\n      image = array_ops.reshape(image, self._shape)\r\n\r\n    return image\r\n```\r\nThe returning tensor of `` return image_ops.decode_image(image_buffer, self._channels)`` only supports uint8 type of tensors.  \r\nTo fix this problem, I changed the previous code into the following code. \r\n``return math_ops.cast(image_ops.decode_image(image_buffer, self._channels), self._dtype)``\r\n\r\n### Source code / logs\r\nI am attaching my source code for feeding dataset from tfrecord using slim.\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\nfrom datasets import dataset_utils\r\n\r\nslim = tf.contrib.slim\r\n\r\n_FILE_PATTERN = '%s_*.tfrecord'\r\n\r\nSPLITS_TO_SIZES = {'train': 24523, 'validation': 6130} # lineGT 20180112\r\n_NUM_CLASSES = 10\r\n\r\n_ITEMS_TO_DESCRIPTIONS = {\r\n    'intensity': 'an intensity map',\r\n    'heightmap': 'an heightmap map',\r\n    'label': 'Ground truth segmentation mask',\r\n}\r\n\r\n\r\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\r\n  if split_name not in SPLITS_TO_SIZES:\r\n    raise ValueError('split name %s was not recognized.' % split_name)\r\n\r\n  if not file_pattern:\r\n    file_pattern = _FILE_PATTERN\r\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\r\n  print(file_pattern)\r\n  # Allowing None in the signature so that dataset_factory can use the\r\n  # default.\r\n  if reader is None:\r\n    reader = tf.TFRecordReader\r\n\r\n  keys_to_features = {\r\n      'image/intensity':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value=''),\r\n      'image/format':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value='raw'),\r\n      'image/height':\r\n          tf.FixedLenFeature(\r\n              (), tf.int64, default_value=0),\r\n      'image/width':\r\n          tf.FixedLenFeature(\r\n              (), tf.int64, default_value=0),\r\n      'image/mask':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value=''),\r\n      'image/mask/format':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value='raw'),\r\n      'image/heightmap':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value=''),\r\n      'image/heightmap/format':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value='raw'),\r\n      'image/filename':\r\n          tf.FixedLenFeature(\r\n              (), tf.string, default_value=''),\r\n  }\r\n\r\n  items_to_handlers = {\r\n      'intensity':\r\n          slim.tfexample_decoder.Image(\r\n            'image/intensity', 'image/format', channels=1, dtype=tf.float32),\r\n      'heightmap':\r\n          slim.tfexample_decoder.Image(\r\n            'image/heightmap', 'image/heightmap/format', channels=1, dtype=tf.float32),\r\n      'label':\r\n          slim.tfexample_decoder.Image(\r\n            'image/mask', 'image/mask/format', channels=1),\r\n      'height':\r\n          slim.tfexample_decoder.Tensor('image/height'),\r\n      'width':\r\n          slim.tfexample_decoder.Tensor('image/width'),\r\n      'fileid':\r\n          slim.tfexample_decoder.Tensor('image/filename'),\r\n  }\r\n\r\n  decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features,\r\n                                                    items_to_handlers)\r\n\r\n  labels_to_names = None\r\n\r\n  return slim.dataset.Dataset(\r\n      data_sources=file_pattern,\r\n      reader=reader,\r\n      decoder=decoder,\r\n      num_samples=SPLITS_TO_SIZES[split_name],\r\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\r\n      num_classes=_NUM_CLASSES,\r\n      labels_to_names=labels_to_names)\r\n```", "comments": ["@sguada, could you please take a look. It seems like if this is desirable it would be great if the original poster @Gabriel-Kang could contribute a fix. What do you think?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "If you save images as Floats then they are just Tensors, so try using the slim.tfexample_decoder.Tensor decoder instead.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This appears to also be an issue with other datatypes, e.g. uint16", "Please try using the slim.tfexample_decoder.Tensor decoder instead.", "I have a similar problem with JPEGs - if I set dtype to float32, I get this error", "jpegs cannot be float32", "Yeah, I figured that out, and now I'm using tf.convert_image_dtype() on them before feeding them to my network.\r\nI wonder if it'd make sense to add this \"intelligence\" to the decoder though."]}, {"number": 16154, "title": "Added save_checkpoint_steps attribute to MonitoredTrainingSession", "body": "fix #15900\r\n- Added `save_checkpoint_steps` attribute to `MonitoredTrainingSession`.\r\nIf both `save_checkpoint_steps` and `save_checkpoint_secs` are both `None` then default saver is disabled. Default is `save_checkpoint_secs=600`\r\n- Added `test_save_checkpoint_steps`", "comments": ["Can one of the admins verify this patch?", "[edit] making fixes to pylint fails", "Thanks for comments -- have made modifications and squashed. ", "@ispirmustafa another look?", "Awesome! happy to contribute, this scratches a real itch for me. ", "Approving for tf-api-owners.", "Looks like the test fails. Mind taking a look?", "There still seem to be issues with that test. Mind taking another look?", "@jhseu I clicked on the CI details and it guided me to invocation log. How should I be using this to find where the test failed?\r\n\r\n[EDIT] OK i think i fixed it. waiting on CI ....", "Waiting for #17051 to fix the linter issue.", "There are a number of failures that look related, @twairball can you take a look?", "Sorry about that! should be fixed now. ", "Can you update the API golden files according to the error message in the API compatibility test?", "That has to be run with Python2, and on linux or mac.", "I'm having trouble running the suggested command, environment is python 2.7 with tf 1.4.1\r\n\r\n````\r\n(py2) thorishere:tensorflow airball$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n>           --update_goldens True\r\nTraceback (most recent call last):\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 38, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: dlopen(/Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __Py_FalseStruct\r\n  Referenced from: /Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /Users/airball/python/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n(py2) thorishere:tensorflow airball$ pip freeze | grep tensor\r\ntensorflow==1.4.1\r\ntensorflow-tensorboard==0.4.0\r\n\r\n````", "If you built TF from source, you have to rerun ./configure to set it to python2. It remembers which Python it is built against and will only work against that.", "Hi @martinwicke  I didn't build TF from source, installed via `pip install` on mac osx, on python3. \r\nI have the same error running the bazel commands on python3. \r\n\r\nI see that it's complaining about running `import tensorflow` while in the tensorflow directory. I'm not very experienced with bazel, so I assumed I'm supposed to be running the bazel commands from the tensorflow directory. \r\n\r\nCould you give me some hint about this? would like to close this out ~\r\n", "Try this: In the directory into which you cloned TensorFlow, run ./configure, and select the python2 when asked.\r\n\r\nRun `bazel build //tensorflow/tools/api/tests/api_compatibility_test`\r\nRun `bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True`\r\n\r\nDoes that work for you?", "@twairball any luck getting the Python2 issues sorted out?", "Hi @rmlarsen  @martinwicke  - Thanks for all the help, I think I got the golden file updated now? waiting for CI. \r\n\r\nI ran into an issue, and didn't come back to solving this until today when I realized I had commented in a related issue. I want to document an important step here for future users who might run into similar situation, caused by Anaconda pathing. \r\n\r\nRelated issue: https://github.com/tensorflow/tensorflow/issues/8655\r\n\r\n1. Run `./configure` and set path to python2 -- remove the first `/` if anaconda path starts with `//` e.g.. `/anaconda/...` instead of `//anaconda/...`. The double-slash causes errors with bazel in `PYTHON_PATH`\r\n\r\n2.  `bazel build //tensorflow/tools/api/tests:api_compatibility_test`\r\n\r\n3.  `bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True`\r\n", "@twairball Great! Running the tests now.", "fixed lint", "Woohoo! Looks like we are all set. @twairball thanks for the contribution."]}, {"number": 16153, "title": "New features: tf.alphas and tf.alphas_like - Related to #16128", "body": "This PR is related to the issue: #16128\r\n\r\n#### I send my work here for peer reviewing and discussion. Please do not merge now.\r\n\r\n### A few interrogations before merging\r\n\r\n1. Are the names I have chosen fine with everyone or you would like it to be changed to something else ?\r\n2. Do my implementations seem fine ?\r\n3. What kind of tests should I implement ? Where shall I put them ?\r\n4. Is it a good idea to replace the function body of tf.ones/tf.zeros and tf.ones_like/tf.zeros_like by a function call to tf.alphas and tf.alphas_like ? Not doing it would lead to code duplication, however I would understand that you might be reluctant, these functions are at the core of the library.\r\n\r\n### Why I created these functions ?\r\n\r\nI oftenly need to create similar tensors with a non-zero/one value. A simple example would be cost functions in GANs with *label smoothing* applied. \r\n\r\nAs stated by @facaiy in #16128, I could use : \r\n```python\r\nb1 = tf.ones_like(a, dtype=tf.float32) * 0.9 # Tensor full of 0.9\r\nb2 = tf.ones_like(a, dtype=tf.int32) * 2 # Tensor full of 2\r\nb4 = tf.ones_like(a, dtype=tf.bool) # Tensor full of True\r\n```\r\n However, as shown in my later comments in the issue, the method implemented in this PR is almost twice as fast.\r\n\r\nIn a wider view, I think that using a single function more *generic* is always a good thing whenever it is possible.\r\n\r\n### How does the function API work ?\r\n\r\nIn a very similar manner than the existing ones: \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.constant([\r\n    [\r\n        [4, 5, 6],\r\n        [1, 2, 3]\r\n    ],\r\n    [\r\n        [4, 5, 6],\r\n        [1, 2, 3]\r\n    ]\r\n])\r\n\r\nb1 = tf.alphas_like(a, 0.5431)\r\nb2 = tf.alphas_like(a, 5)\r\nb3 = tf.alphas_like(a, -5)\r\nb4 = tf.alphas_like(a, True)\r\n\r\nwith tf.Session() as sess:\r\n    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])\r\n    \r\nprint(\"b1:\", _b1)\r\nprint(\"b2:\", _b2)\r\nprint(\"b3:\", _b3)\r\nprint(\"b4:\", _b4)\r\n\r\n############### OUTPUTS ###############\r\n\r\n>>> b1: [\r\n  [\r\n    [ 0.5431  0.5431  0.5431]\r\n    [ 0.5431  0.5431  0.5431]\r\n  ]\r\n  [\r\n    [ 0.5431  0.5431  0.5431]\r\n    [ 0.5431  0.5431  0.5431]\r\n  ]\r\n]\r\n\r\n>>> b2: [\r\n  [\r\n    [5 5 5]\r\n    [5 5 5]\r\n  ]\r\n  [\r\n    [5 5 5]\r\n    [5 5 5]\r\n  ]\r\n]\r\n\r\n>>> b3: [\r\n  [\r\n    [-5 -5 -5]\r\n    [-5 -5 -5]\r\n  ]\r\n  [\r\n    [-5 -5 -5]\r\n    [-5 -5 -5]\r\n  ]\r\n]\r\n\r\n>>> b4: [\r\n  [\r\n    [ True  True  True]\r\n    [ True  True  True]\r\n  ]\r\n  [\r\n    [ True  True  True]\r\n    [ True  True  True]\r\n  ]\r\n]\r\n```\r\n\r\n---------------------------\r\n\r\nI'm of course free for discussion over video-calls. It's the first time I try to make a change at the core of TF, and I'm quite afraid of breaking everything ;) Thanks for your help btw.\r\n\r\nAll the best,\r\n\r\nJonathan DEKHTIAR", "comments": ["Can one of the admins verify this patch?", "Hi everyone, \r\n\r\nI have added a few corrections to this PR. I also have implemented a few unittests in order to be sure that everything is executing fine.\r\n\r\n### Compiled Library for test (in whl format):\r\n\r\nI have compiled the library for test only: https://drive.google.com/open?id=1qHtPZpOzgJGvuWnaOPSIU1caLIpoEA8m\r\n\r\n* **OS**: Ubuntu 16.04\r\n* **GPU Support**: No\r\n* **Python**: Python 3.5\r\n* **Architecture:** x86_64\r\n* **Size:** 55MB\r\n\r\n### The PR can be tested with the following script:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nops_array = list()\r\n\r\ntarget_np_array = np.ones([2,3])\r\n\r\nfor dtype in [np.int32, np.float32, np.bool, np.uint8]:\r\n    for target_value in [0, 1, 0.7, -5.2]:\r\n        \r\n        op_val = np.array(target_value).astype(dtype)\r\n        \r\n        tmp_ops = tf.alphas_like(target_np_array, op_val)\r\n        ops_array.append(tmp_ops)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    result_array = sess.run(ops_array)\r\n\r\nfor _, result in enumerate(result_array):\r\n    if _:\r\n        print(\"################\")\r\n    print(result)\r\n```\r\n\r\n### Outputs:\r\n\r\n```shell\r\n[[0 0 0]\r\n [0 0 0]]\r\n################\r\n[[1 1 1]\r\n [1 1 1]]\r\n################\r\n[[0 0 0]\r\n [0 0 0]]\r\n################\r\n[[-5 -5 -5]\r\n [-5 -5 -5]]\r\n################\r\n[[0. 0. 0.]\r\n [0. 0. 0.]]\r\n################\r\n[[1. 1. 1.]\r\n [1. 1. 1.]]\r\n################\r\n[[0.7 0.7 0.7]\r\n [0.7 0.7 0.7]]\r\n################\r\n[[-5.2 -5.2 -5.2]\r\n [-5.2 -5.2 -5.2]]\r\n################\r\n[[False False False]\r\n [False False False]]\r\n################\r\n[[ True  True  True]\r\n [ True  True  True]]\r\n################\r\n[[ True  True  True]\r\n [ True  True  True]]\r\n################\r\n[[ True  True  True]\r\n [ True  True  True]]\r\n################\r\n[[0 0 0]\r\n [0 0 0]]\r\n################\r\n[[1 1 1]\r\n [1 1 1]]\r\n################\r\n[[0 0 0]\r\n [0 0 0]]\r\n################\r\n[[251 251 251]\r\n [251 251 251]]\r\n```\r\n", "@DEKHTIARJonathan Thanks for the contribution. I'm not sure it is necessary to add this API. Constant folding optimizations in TensorFlow will replace expressions like '5 * ones_like(x)' with a the same constant that alpha_as(x, 5) would have produced, if the shape of x can be deduced.\r\n\r\nMaybe API reviewers can comment?", "@DEKHTIARJonathan Your proposed alphas API is essentially the same as tf.constant: https://www.tensorflow.org/versions/master/api_docs/python/tf/constant", "https://www.tensorflow.org/api_docs/python/tf/fill ?", "@josh11b Indeed, even closer."]}, {"number": 16152, "title": "DeprecationWarning from `inspect.getargspec()`", "body": "`inspect.getargspec` is deprecated in Python 3\r\nhttps://docs.python.org/3/library/inspect.html#inspect.getargspec\r\n\r\nI solved the problem in keras like this:\r\nhttps://github.com/keras-team/keras/pull/7035\r\n\r\n### System information\r\n- Using tensorflow as a keras backend (keras 2.1.2)\r\n- Linux Ubuntu 16.04\r\n- installed from conda\r\n- version 1.3.0\r\n- python 3.6.4\r\n\r\n### Describe the problem\r\nWe recently switched from theano to tensorflow and this warning message is filling up my test output.\r\n\r\n### Source code / logs\r\n```\r\n/home/<name>/.conda/envs/<env>/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning:\r\n  \r\n  inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n```\r\n", "comments": ["@martinwicke, FYI\r\n@frexvahi, thanks for reporting this. Would you be willing to submit a PR to fix this, since you already didd this in Keras?\r\n", "I could submit a PR, but getting someone to sign the corporate CLA would be harder.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I fixed it on my end. I will try to submit a pull request today or tomorrow with the fix.", "Just a quick update: I submitted a pull request that should fix the issue.", "Can you say \"Fixes #16152\" in the description for your PR? That way this issue will be closed when your PR is submitted.", "A quick update. I believe the issue has been fixed in r1.7 since ever since I started using it I haven't had any of the warnings.", "Thanks!", "I'm still hitting this, Tensorflow 1.7.0 and 1.8.0-rc1. Python 3.6.5. Afraid I don't have an easy reproducer to hand, will update if I get one.", "Reproducer below tested on 1.8.0-rc1. Was hard to make a reproducer because the warning did not show up unless I had `warnings.filterwarnings('error')`, which I use to make it easier to find the sources of warnings.\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\nimport warnings\r\nwarnings.filterwarnings('error')\r\n\r\ntf.reduce_sum(tf.placeholder(tf.float64))\r\n```\r\n\r\nFull stack trace below.\r\n\r\n<details>\r\n\r\n```.pytb\r\n---------------------------------------------------------------------------\r\nDeprecationWarning                        Traceback (most recent call last)\r\n<ipython-input-1-1de048b23827> in <module>()\r\n      4 warnings.filterwarnings('error')\r\n      5 \r\n----> 6 tf.reduce_sum(tf.placeholder(tf.float64))\r\n\r\n~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    403       if is_in_graph_mode.IS_IN_GRAPH_MODE() and _PRINT_DEPRECATION_WARNINGS:\r\n    404         invalid_args = []\r\n--> 405         named_args = tf_inspect.getcallargs(func, *args, **kwargs)\r\n    406         for arg_name, spec in iter(deprecated_positions.items()):\r\n    407           if (spec.position < len(args) and\r\n\r\n~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/tf_inspect.py in getcallargs(func, *positional, **named)\r\n    107   argspec will be used.\r\n    108   \"\"\"\r\n--> 109   argspec = getargspec(func)\r\n    110   call_args = named.copy()\r\n    111   this = getattr(func, 'im_self', None) or getattr(func, '__self__', None)\r\n\r\n~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/tf_inspect.py in getargspec(object)\r\n     53   decorators, target = tf_decorator.unwrap(object)\r\n     54   return next((d.decorator_argspec for d in decorators\r\n---> 55                if d.decorator_argspec is not None), _inspect.getargspec(target))\r\n     56 \r\n     57 \r\n\r\n/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py in getargspec(func)\r\n   1069     warnings.warn(\"inspect.getargspec() is deprecated, \"\r\n   1070                   \"use inspect.signature() or inspect.getfullargspec()\",\r\n-> 1071                   DeprecationWarning, stacklevel=2)\r\n   1072     args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = \\\r\n   1073         getfullargspec(func)\r\n\r\nDeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n```\r\n\r\n</details>", "Tensorflow 1.9.0-dev20180427\r\n```\r\nimport tensorflow as tf\r\nminiconda3/envs/kaggle/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n```", "Added PR #19199 as an attempt for the fix.", "I am still hitting this warning with the following versions:\r\n * 1.8.0\r\n * 1.9.0\r\n * 1.10.0\r\n * 1.10.1\r\n * 1.11.0-rc2", "Had so many of these on 1.8 that i could not longer see my test output. Updated to 1.10 and now i only have a few dozen of these warnings left.", "Yeah, still seeing this in 1.11.0-rc0:\r\n\r\n```\r\n/Users/josh/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\n/Users/josh/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\n...\r\n```\r\n\r\nCan we reopen?", "any updates?", "I think the issue has been resolved in #22517 now.", "Still getting this issue with 1.11.0 and Python 3.6 ", "me too.", "me too", "I see it in 1.12 too on Python 3.6", "> I see it in 1.12 too on Python 3.6\r\n\r\nthe same to you", "Resolved in #22517, so I believe it will appear in the next release (TF 1.13 or 2.0)."]}, {"number": 16151, "title": "ValueError: Labels are incompatible with given information. ", "body": "Have I written custom code: yes\r\nOS: Windows 8.1\r\nTensorflow installed from: conda\r\nTensorflow version: 1.4\r\n\r\nI am having problems in adding validation monitors to `Estimator.fit`. With this code I have:\r\n\r\n```\r\ndef main(_):\r\n    image_paths, labels = dataset_utils.read_dataset_list('../test/dummy_labels_file.txt')\r\n    data_dir = \"../test/dummy_data/\"\r\n    images = dataset_utils.read_images(data_dir=data_dir, image_paths=image_paths, image_extension='png')\r\n    print('Done reading images')\r\n    images = dataset_utils.resize(images, (1596, 48))\r\n    images = dataset_utils.transpose(images)\r\n    labels = dataset_utils.encode(labels)\r\n    x_train, x_test, y_train, y_test = dataset_utils.split(features=images, test_size=0.5, labels=labels)\r\n    print(x_test)\r\n    x_train_seq_lens = dataset_utils.get_seq_lens(x_train)\r\n    x_test_seq_lens = dataset_utils.get_seq_lens(x_test)\r\n\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": np.array(x_train),\r\n           \"seq_lens\": np.array(x_train_seq_lens)},\r\n        y=np.array(y_train),\r\n        num_epochs=1,\r\n        shuffle=True,\r\n        batch_size=1\r\n    )\r\n\r\n    validation_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": np.array(x_test),\r\n           \"seq_lens\": np.array(x_test_seq_lens)},\r\n        y=np.array(y_test),\r\n        shuffle=True\r\n    )\r\n\r\n    validation_monitor = learn.monitors.ValidationMonitor(\r\n        input_fn=validation_input_fn,\r\n        every_n_steps=1\r\n    )\r\n\r\n    model = GridRNNModelFn(num_time_steps=1596, num_features=48, num_hidden_units=128, num_classes=80,\r\n                           learning_rate=0.001, optimizer=Optimizers.MOMENTUM)\r\n\r\n    classifier = learn.Estimator(model_fn=model.model_fn, params=model.params, model_dir=\"/tmp/grid_rnn_ocr_model\")\r\n    classifier.fit(input_fn=train_input_fn, monitors=[validation_monitor])\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run(main=main)\r\n```\r\n\r\nIt throws this error:\r\n\r\n`ValueError: Labels are incompatible with given information. Given labels: Tensor(\"random_shuffle_queue_DequeueUpTo:3\", shape=(?, 37), dtype=int32), required signatures: TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None), Dimension(33)]), is_sparse=False).`\r\n\r\nWhich leads me to think that the dynamic label lengths are not accepted. To reproduce this, simply clone this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr) and run the script specified in the readme.", "comments": ["Managed to solve it. All the labels just have to be padded such that they'll have the same length."]}, {"number": 16150, "title": "Tensorflow Debugger with Multithreading", "body": "\r\n## System information\r\n\r\n1. OS Platform: Ubuntu 14.04 \r\n2. TensorFlow installed from source : GPU-Version 1.14 branch \r\n3. cuDNN: 7.0\r\n4. Python version: 3.4\r\n\r\n--\r\n\r\nI'm using **tf.Estimator** together with **numpy_input_fn**. Moreover, I set all num_threads to 1. However, tfdbg somehow doesn't get along well with child threads auto-created by new estimator API.\r\n\r\nHere is how I defined Estimator:\r\n\r\n```\r\ndef do_train(tr_data, vl_data):\r\n\r\n    # Create Estimator\r\n    sess_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n    sess_conf.gpu_options.allow_growth = True\r\n    config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,  # CheckpointSaverHook\r\n                                    save_checkpoints_steps=100,  # CheckpointSaverHook\r\n                                    log_step_count_steps=10,  # SummarySaverHook\r\n                                    session_config=sess_conf)\r\n    music_classifier = tf.estimator.Estimator(model_fn,\r\n                                              config=config,\r\n                                              params={'max_seq': tr_data.max_seq})\r\n    # Prepare input data\r\n    tr_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        {'input_ph': tr_data.data, 'seq_len_ph': tr_data.seq_len},\r\n        batch_size=FLAGS.batch_size,\r\n        num_epochs=FLAGS.num_epochs,\r\n        num_threads=1,\r\n        shuffle=True\r\n    )\r\n    # Extra Hooks\r\n    logging_hook = tf.train.LoggingTensorHook(\r\n        tensors={'probabilities': 'probs_tensor'},\r\n        every_n_iter=10\r\n    )\r\n    debugging_hook = tf_debug.LocalCLIDebugHook(thread_name_filter=\"MainThread$\", dump_root=\"./dump\")\r\n    # Train\r\n    music_classifier.train(tr_input_fn, hooks=[debugging_hook])\r\n```\r\n\r\n\r\nAlthough the code runs fine; whenever I run it in debug mode, it fails at executing run command on tfdbg command line. Following is the exception I got:\r\n\r\n```\r\n2018-01-16 10:25:26.587936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-16 10:25:26.588715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.38GiB\r\n2018-01-16 10:25:26.588732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-01-16 10:25:41.373907: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434\r\nTraceback (most recent call last):\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434\r\n     [[Node: rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _class=[\"loc:@rnn/while/TensorArrayReadV3\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter, rnn/while/rnn/msarnn_cell/Identity/Enter/_259, rnn/while/TensorArrayReadV3, rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter_1, ^rnn/while/rnn/msarnn_cell/cond/Merge/_263)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 202, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 191, in main\r\n    do_train(tr_data, vl_data)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 169, in do_train\r\n    music_classifier.train(tr_input_fn, hooks=[debugging_hook])\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 783, in _train_model\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.4/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 827, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434\r\n     [[Node: rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _class=[\"loc:@rnn/while/TensorArrayReadV3\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter, rnn/while/rnn/msarnn_cell/Identity/Enter/_259, rnn/while/TensorArrayReadV3, rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter_1, ^rnn/while/rnn/msarnn_cell/cond/Merge/_263)]]\r\n\r\nCaused by op 'rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3', defined at:\r\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 202, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 191, in main\r\n    do_train(tr_data, vl_data)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 169, in do_train\r\n    music_classifier.train(tr_input_fn, hooks=[debugging_hook])\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py\", line 116, in model_fn\r\n    initializer=tf.glorot_normal_initializer())  # [BSxMTxOS], [BSxSS]\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py\", line 253, in dynamic_msa_rnn\r\n    dtype=tf.float32)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py\", line 614, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py\", line 777, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py\", line 760, in _time_step\r\n    skip_conditionals=True)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py\", line 236, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py\", line 748, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py\", line 204, in call\r\n    ms_analyzer = self._extract_features(inputs)\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py\", line 151, in _extract_features\r\n    sub_sig = self._gen_sub_sig(inputs)  # BSxTxIN\r\n  File \"/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py\", line 105, in _gen_sub_sig\r\n    self.__ta_signal = self.__ta_signal.write(self.__time, inputs)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 310, in write\r\n    name=name)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 5038, in _tensor_array_write_v3\r\n    flow_in=flow_in, name=name)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434\r\n     [[Node: rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _class=[\"loc:@rnn/while/TensorArrayReadV3\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter, rnn/while/rnn/msarnn_cell/Identity/Enter/_259, rnn/while/TensorArrayReadV3, rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter_1, ^rnn/while/rnn/msarnn_cell/cond/Merge/_263)]]\r\n```\r\n\r\nThank you for your support in advance.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "1. OS Platform and Distribution: Ubuntu 14.04 LTE\r\n2. TensorFlow version: 1.14\r\n3. Bazel version: 0.9.0\r\n4. CUDA/cuDNN version: 8.0/7.0.5\r\n5. GPU model and memory: GeForce GTX1060 - 6070MB\r\n6. Exact command to reproduce: python3.4 -m music_modeling --debug\r\n\r\nThank you for support again.", "Thanks for the detailed report!\r\n\r\n@caisq any idea what's going on?", "Is this still happening with the latest version?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16149, "title": "Dataset from string generator raises Exception with python 2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary cpu version from pypi\r\n- **TensorFlow version (use command below)**: ('v1.5.0-rc0-9-gf9472619f6', '1.5.0-rc1')\r\n- **Python version**: 2.7.14\r\n\r\n### Describe the problem\r\nI'm reading text data from file in generator. Encoding: UTF-8.\r\nAfter some preprocessing i return it in generator manner.\r\nNext, i'm trying to create Dataset from this generator.\r\n\r\nCode below produce exception in both Python 2&3 for TensorFlow 1.4.\r\nFor TF 1.5.rc1 & Python 3 there is no errors.\r\nFor TF 1.5.rc1 & Python 2 error exist.\r\n\r\n### Source code / logs\r\n```python\r\n# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef generator():\r\n    data = [\r\n        [u'\u041f\u0440\u043e\u0441\u0442\u043e\u0439', u'\u0442\u0435\u0441\u0442', u'\u044e\u043d\u0438\u043a\u043e\u0434\u0430'],\r\n        [u'\u043d\u0438\u043a\u043e\u0433\u0434\u0430', u'\u043d\u0435', u'\u0431\u044b\u0432\u0430\u0435\u0442', u'\u043f\u0440\u043e\u0441\u0442\u044b\u043c']\r\n    ]\r\n\r\n    for seq in data:\r\n        yield seq, [0, 1, 2, 3]\r\n\r\n\r\ndef dataset():\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator,\r\n        (tf.string, tf.int32),\r\n        (tf.TensorShape([None]), tf.TensorShape([None]))\r\n    )\r\n    dataset = dataset.padded_batch(2, padded_shapes=([None], [None]), padding_values=('', 0))\r\n\r\n    return dataset\r\n\r\n\r\niterator = dataset().make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    value = sess.run(next_element)\r\n    print(value)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "There might be some discrepancy between Python 2 and Python 3 with TensorFlow. In Python 3 the unicode is considered as UTF8 while in Python 2 a default (ascii) encoding is assumed.\r\n\r\nCreated a PR #16168 for a fix."]}, {"number": 16148, "title": "non_max_suppression is on CPU?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n      binary(By pip)\r\n- **TensorFlow version (use command below)**:\r\n      1.4.1\r\n- **Python version**: \r\n      3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n    8.0.61/6.0.21\r\n- **GPU model and memory**:\r\n    GTX 1080 Ti, 11172MiB\r\n- **Exact command to reproduce**:\r\n     python main.py\r\n\r\n### Describe the problem\r\n    \r\nI train my RFCN by tensorflow. My project need very high speed. So I use the profile and I find that non_max_suppression is on CPU? Is there a GPU version?I think if you calculate all pairs of boxes IOU first, then just for-loop once will ultimately boost speed, there have some trick in it, just see the source code in [https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms](https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms). I think cuda version of NMS is faster than CPU version.\r\n", "comments": ["Currently TensorFlow has only CPU version NMS, see #7511.", "closing as duplicate of #7511 "]}, {"number": 16147, "title": "Inference on V100 with TF1.5 is extremely slow. ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source and Virtual Env, problem doesn't change\r\n- **TensorFlow version (use command below)**: 1.5.0-rc1 (Makes no difference on 1.4)\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: CUDA 9/7.0.5\r\n- **GPU model and memory**: V100 - 16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n- Inference using the V100 is very slow. For example performing object detection with SSD Mobilenet is achieving a max frame rate of ~8, compared to ~45 on a GTX1080\r\n- Initialization with a warm up image is extremely slow - up to 2 mins for the first image. \r\n- I have tried model quantization using graph_transforms/transform_graph (in an attempt to use the FP16 mode) and various combinations of CUDA, cuDNN and Tensorflow versions with no difference. \r\n\r\nIs there some recommended environment setup for the V100?\r\nI am successfully running Darknet (https://pjreddie.com/darknet/) with a massive increase of speed. ", "comments": ["CC @zheng-xq @tfboyd @reedwm ", "If it is only the first image that is very slow, it is very likely that your TensorFlow was not compiled with the right compute capability. So JIT is taking that much time. If you build TensorFlow from source, please make sure to enable \"7.0\" for V100 support. Thanks!", "Hi Zheng,\r\n\r\nYes, it happens on the first image, but every image after that also experiences a much lower frame rate than my GTX1080.\r\n\r\nAnd I did compile Tensorflow with compute capability 7.0", "I have also attempted to use the pre-built wheels from here with no improvement:\r\nhttps://github.com/mind/wheels/releases/tag/tf1.4.1-gpu-cuda9", "@louisquinn   I work with XQ and want to reproduce your issue.  I read your comments but I am not 100% which example you are running.  I can run any inference and possibly reproduce the problem, but I would rather run exactly what you are doing so I can have a direct comparison.  Which TF script are you using, is an existing example or do you have code you can share publicly in github?  Thank you.", "@tfboyd Thanks for your response, and thanks in advance for your help!\r\n\r\nHere is a gist I worked up which will show the speeds. \r\nYou just need to change the graph_path and test_image_path\r\nhttps://gist.github.com/louisquinn/796736446c39875bbdbf27b605cba33f\r\n\r\nThe script runs a dummy image as a warm up then a single image. I am getting similar FPS when using a video. Interestingly the V100 performs better than the 1080 for Faster RCNN w/ Inception Resnet V2, although I expect the speed to be much higher?\r\n\r\nAttached is a screenshot of my nvidia-smi when the warm up image is running, it looks to be idle. When the test image is passed to the model, the GPU-Util jumps to > 80%\r\n![idle](https://user-images.githubusercontent.com/5274490/35126671-93719ef0-fd02-11e7-8347-b27e5ca3a0b5.png)\r\n", "@louisquinn  Thank you.  My Friday looks a bit weird but I will try to get to this and have some response (like an update :-) on progress and what I am finding and thinking) before EOD Monday 22-JAN.  A complexity is loading the graph vs. the code and the object detection code is not maintained by the TensorFlow team.  I will see if I can discern something from the data I collect.  I am looking to setup a decent inference test and while this may not be it, the outcome may be useful to you and others.  ", "@tfboyd no worries thanks for taking the time! Just some more information, I am getting similar speeds and wait times when running the TF label_image example with bazel ", "Seconded ... we're using an Azure NCv3 instance (V100) with CUDA 9.1 and bazel-compiled (with \"7.0\") and it's about the same speed as my GTX950m laptop for inference. Stink.\r\n\r\nWe're running [facenet](https://github.com/davidsandberg/facenet).", "Oh, [good find](https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/) @nathannz:\r\n\r\n> The reason for this disappointing performance is that the powerful Tensor Cores in the V100 are only used for matrix multiplications in half-precision (FP16) or mixed-precision mode.\r\n\r\nI think our model is `float32` ... I can't figure out an easy way to convert a pretrained model to `float16`.", "It would not be slower than a GTX950, something else is likely wrong.  If you are looking to utilize the TensorCores or just in general get the best inference on GPU you will want to look at TensorRT.  It can take a graph and optimize it.  Right now there is a script from NVIDIA that does it, but also a [PR](https://github.com/tensorflow/tensorflow/pull/16253) in progress to build it in to TensorFlow.  If I was trying to use it right now I would likely use the NVIDIA script that I have not seen or used but understand it exists.  I suspect it is not as easy to use as one would hope but some internal teams have tried it and the results were decent thus the desire to make it native to TF.  Progress will happen quickly as this is a priority and the initial focus is on FP16 in Q1 and then INT8.   But as with all things they take time.  \r\n\r\nI am looking for a good example to benchmark for inference.  I would like it to use tf.data rather than feed_dict and prefer to have the model in code vs. loading from a saved graph.  I do not doubt your problems but I spent an entire afternoon looking for a cpu regression related to inference and the end result was I proved 1.5 was faster than 1.4 and the user realized they made a mistake.  Not a big deal and I was glad to gather the info but the example was not something I wanted to add to the benchmark so that made the work kind of throw away.  ", "> something else is likely wrong ...\r\n\r\nThat's what we thought. We're running a standard benchmark suite (as we're actually evaluating hardware, and wanted to see what a V100 could do). That said, we've compiled from source, so it's possible something went wrong there. Anyway, it matches the description by the OP ... so maybe it is genuine (at least for out-of-the-box performance). See who else replies.\r\n\r\n> I am looking for a good example to benchmark for inference\r\n\r\nAren't there already benchmarks for TF (including ones that are rebuilt as part of CI workflow)? Might as well use them.", "@tfboyd \r\n\r\nThanks for your update. \r\nActually I am halfway through implementing TensorRT in Python, so I think I will hold off and wait for the official merge from the PR you mentioned. \r\n\r\nThanks so much for your help!", "Any update on this issue ? I have also tried to use the V100 on my own code (tensorflow 1.5.0 cuda 9.0 cudnn 7) and did not see any speed improvement with respect to GTX 1080 Ti. I tried either float32 and float 16 without any difference. (but I am not sure exactly if the underlying operations were done in float16 since it seems that operations might still be running float32 even though the graphs/input are defined as float16)\r\n\r\nI would be really interested to see any example of code that manages to use the Volta tensor cores power showing significant speed improvement with respect to previous generation of GPU (Pascal / Maxwell architecture)", "@Rov67777 \r\n\r\nI'm just following the progress of this [PR](https://github.com/tensorflow/tensorflow/pull/16253)\r\nTensorRT optimises Tensorflow frozen graphs for Volta architectures. I haven't altered any code to use TensorRT yet, but I may implement a simple model with the demo as a guide, although I don't have access to a V100 at the moment. ", "@Rov67777   ResNet50 FP32 on V100 is ~344 image/sec.  FP16 665 images/sec.  A GTX 1080 ti is likely similar to the original Titan X (Pascal) and I would guess ~200-210 images/sec  because a P100 is ~230 images/sec.  I do not believe the GTX 1080 has strong FP16 support even for inference and no mixed-mode so training would not so well at all based on my understanding.\r\n\r\nI do not know your use case but if you are looking for an FP16 example in the short term I have a link below.  For forward-pass only the results would be as dramatic but it would depend on how you are need to do inference. The benchmark script assumes a batch of images not data streaming in, which is a scenario I hope to have a better example for in the near future as I think it is common.  Meaning using tf.data where you provide some type of buffer to tf.data or something like that, sorry the idea is not complete, if you have a scenario please share and explain why it is interesting.  \r\n\r\nhttps://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks\r\n```\r\n# Real data\r\npython tf_cnn_benchmarks.py --data_format=NCHW --batch_size=128 --num_batches=100 --model=resnet50 --data_dir=/data/imagenet --optimizer=sgd --variable_update=parameter_server --all_reduce_spec='' --use_fp16=True --nodistortions --local_parameter_device=cpu --num_gpus=1 --display_every=10\r\n\r\n# Synthetic Data\r\npython tf_cnn_benchmarks.py --data_format=NCHW --batch_size=128 --num_batches=100 --model=resnet50 --optimizer=sgd --variable_update=parameter_server --all_reduce_spec='' --use_fp16=True --nodistortions --local_parameter_device=cpu --num_gpus=1 --display_every=10\r\n```\r\n\r\n\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Isn't this solved with TensorRT in 1.7?", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Has this been resolved? I am facing astronomically slow inference speeds 15s for a model with benchmark at 707ms @tfboyd @zheng-xq @tensorflowbutler @louisquinn \r\nLooking forward to a response. Thanks!"]}, {"number": 16146, "title": "Documentation for GridLSTMCell is lacking and does not match the paper", "body": "### Describe the problem\r\n\r\nThe documentation for tf.contrib.rnn.GridLSTMCell cites the paper \"Grid Long Short-Term Memory\", by Kalchbrenner et al.\r\nThe paper describes an architecture, called the 2D Grid LSTM, to replace a stack of LSTM cells. In a 2D Grid LSTM, 2 state components are passed from one layer to the next vertically.\r\n\r\nIn Tensorflow RNN parlance, one would expect both the state and the output of the cell to be an LSTMStateTuple, which would allow seamless integration with a MultiRNNCell.\r\nIn the current implementation, instead it appears that the vertical unrolling is done internally to the GridLSTMCell.\r\nI say it appears, because I can't quite make sense of the arguments and their documentation: specifically, there is a required \"num_frequency_block\" argument whose meaning is quite obscure.\r\nLooking at the implementation also did not help me understand what value is actually expected in that parameter, and the related parameters.\r\nNote that the above mentioned paper does not talk about frequencies anywhere.\r\n\r\nWould it be possible to expand on the documentation for the cell, as well as provide a code example on how to replicate the 2D Grid LSTM from the paper?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "None of that is relevant, because this is a documentation issue, but sure:\r\n\r\nHave I written custom code: n/a\r\nOS Platform and Distribution: n/a\r\nTensorFlow installed from: n/a\r\nTensorFlow version: 1.4 (but master has the same problem)\r\nBazel version: n/a\r\nCUDA/cuDNN version: n/a\r\nGPU model and memory:  n/a\r\nExact command to reproduce: n/a", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Sadly, we don't have an active maintainer for that code.\r\n\r\nMaybe @MarkDaoust has a suggestion for documentation, but as it stands, this part of the code in contrib comes \"as is\".", "I think I have an exampe use GridLSTMCell that belongs to contrib.grid_rnn and doesn't use the `frequence_blocks` argument:\r\n\r\n```\r\nclass GridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.static_rnn(self.cell, self.input_layer, dtype=tf.float32)\r\n\r\nclass BidirectionalGridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell_fw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n        self.cell_bw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_bidirectional_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.static_bidirectional_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)\r\n```", "Umm  so is anyone working on this? Can I work on it...?", "I don\u2019t think anyone is right now. You can take a shot at it.\n\nOn Mon, 4 Jun 2018 at 3:06 PM Jae Duk Seo <notifications@github.com> wrote:\n\n> Umm so is anyone working on this? Can I work on it...?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16146#issuecomment-394254199>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMp2w80CvfzIrievYhD6yOHak7YUgqXSks5t5NxkgaJpZM4RfUcV>\n> .\n>\n"]}, {"number": 16145, "title": "Decoding contents of BMP file on big endian", "body": "As the BMP file contents are encoded in little endian format, added byte swapping for reading the various header components correctly on big endian.", "comments": ["Can one of the admins verify this patch?", "@namrata-ibm thanks for the contribution!"]}, {"number": 16144, "title": "Is it possible to train CNN model by using tensorflow JAVA API?", "body": "Hello, TF.\r\nI have plane to train my CNN model by using tensorflow JAVA API.\r\nI got success on simple model( with a simple matmul operation between weights and bias)\r\nBUT I failed to train CNN model.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said:\r\n- Yes, it is possible to train from the Java API\r\n- However, it seems what you're asking about (based on the error message) is training from Android. For binary size reasons, we strip the number of operations in the pre-built Android packages. To include additional operations, you may need to use selective registration. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/selective_registration.h (unfortunately, selective registration is not yet well documented (#10299)", "@asimshankar \r\nI added  the sources(tensorflow/java/src/main/java) to my android project.\r\naccording to this article([](https://stackoverflow.com/questions/46030577/how-to-train-a-model-in-tensorflow-using-java)),\r\nTrain function of JAVA api is not sufficient.\r\n\r\nIs it right?\r\nTF Slim or other convolution layers are not supported I guess."]}, {"number": 16143, "title": "Undefined symbol \"_ZN3Aws8Security14SecureMemClearEPhj\"", "body": "compiled tensorflow r.15 from source , when import tensorflow in python got following error:\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/_pywrap_tensorflow_internal.so: Undefined symbol \"_ZN3Aws8Security14SecureMemClearEPhj\"\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\n\r\nthanks in advance !!!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I've found the problem is about Amazon platform , so I disable this feature from configuration as an workaround .  And now I can precede ."]}, {"number": 16142, "title": "fix typo", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}]