[{"number": 13351, "title": "TensorFlow variable initializers broken", "body": "Three symptoms observed with models after upgrading tensorflow:\r\n1. must feed placeholder error\r\nor\r\n2. things hang with 100% utilization inside python _build_initializer_expr\r\n3. things succeed but sess.run call takes 100x slower than before\r\n\r\nI believe this is due to this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/07adc2ea910de715d31e16a019fcbcccb575e931\r\n\r\nBecause the following work-around restores good behavior:\r\n```\r\nfrom tensorflow.python.ops import variables\r\ndef passthrough(obj, value): return value\r\ntry:\r\n  variables.Variable._build_initializer_expr=passthrough\r\nexcept: # older versions of TF don't have this\r\n  pass\r\n```\r\n\r\n\r\nHere's a self contained repro: https://github.com/yaroslavvb/stuff/blob/master/tf_initializer_bug_report.py\r\n\r\nIt works fine in tensorflow 1.2, or with the fix, in latest version is throws \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a va\r\nlue for placeholder tensor 'Wf_holder' with dtype float and shape [307328]\r\n         [[Node: Wf_holder = Placeholder[dtype=DT_FLOAT, shape=[307328], _device\r\n=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n```\r\n\r\nSorry I didn't make the repro smaller, I lost motivation after finding the quick fix :)", "comments": ["cc @benoitsteiner @alextp ", "@TimSalimans who is also affected ", "Just tested with tf nightly and it's still broken. I believe this bug is also responsible for deadlock during data-dependent init in tim's models.\r\n\r\nTo reproduce:\r\n```\r\npip install tf-nightly\r\nwget https://raw.githubusercontent.com/yaroslavvb/stuff/2c716e7abde25018ccfeeeff96c6397fef4f33ba/tf_initializer_bug_report.py\r\n\r\npython tf_initializer_bug_report.py\r\n...\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Wf_holder' with dtype float and shape [307328]\r\n\t [[Node: Wf_holder = Placeholder[dtype=DT_FLOAT, shape=[307328], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n```\r\n\r\nIf you comment out the top block, which essentially reverts https://github.com/tensorflow/tensorflow/commit/07adc2ea910de715d31e16a019fcbcccb575e931 then things work again and you see following output\r\n\r\n```\r\nRunning training.\r\nStep 0 loss 35.58, target decrease -7.771, actual decrease, -4.621 ratio 0.59 grad norm: 22.97 pregrad norm: 77563.16\r\nStep 3 loss 25.41, target decrease -0.841, actual decrease, -0.740 ratio 0.88 grad norm: 1.72 pregrad norm: 1541.23\r\nStep 6 loss 23.99, target decrease -0.204, actual decrease, -0.189 ratio 0.93 grad norm: 0.43 pregrad norm: 295.30\r\n\r\n```\r\n", "@asimshankar @josh11b It's this issue again. Apparently it broke recently. I didn't know that. ", "There is an internal bug for this which is being worked on", "I agree with your analysis that https://github.com/tensorflow/tensorflow/commit/07adc2ea910de715d31e16a019fcbcccb575e931 is the culprit and that introduced likely multiple bugs.  The description of that commit \"when a variable w is initialized with the value of another variable v, make sure that the initializer of v is run first.\" would have been fine, but the actual code change instead redirects references to v in w's initializer to v's initial value graph. This has at least a couple of problems:\r\n* there are some bugs in the code, e.g. if there is a while loop it can follow the cycle in the graph forever\r\n* if v was already initialized and has since been updated, w should get the new value not the original value\r\n\r\nIdeally, we would rewrite the logic to be less buggy:\r\n* scan w's initializer init_w for references to variables, being careful to avoid visiting nodes in a graph cycle more than once\r\n* replace w's initialize_op with: if v has not been initialized, run v's initialization op; once that is done run \"w = w's initialization expression\" without changes (in particular, without the rewrite to use v's initial value)\r\n", "I'm starting to push through changes to fix this. The first one is a refactoring to make subsequent changes easier as well as avoiding infinite recursion when the `initial_value` contains cycles (note that it also changes the name of `_build_initiazier_expr` to `_try_guard_against_uninitialized_depenencies` to clarify what it does). I'll now attempt to address more edge cases.\r\n\r\nHere's what I believe is a minimal reproduction of the bad behaviour in this case:\r\n\r\n```\r\np = tf.placeholder(dtype=tf.float32, shape=[])\r\nv0 = tf.Variable(p)\r\nv1 = tf.Variable(v0)\r\nwith tf.Session() as session:\r\n  session.run(v0.initializer, feed_dict={p: 0})\r\n  session.run(v1.initializer)  # InvalidArgumentError\r\n```\r\n\r\nThe transformation done by `_build_initializer_expr` makes `v1` depend on `v0.initialized_value()` which is a `cond` of the form:\r\n\r\n```\r\ncond(\r\n    is_variable_initialized(v0),\r\n    v0.read_value,\r\n    lambda: p)\r\n```\r\nBecause `p` isn't actually created inside `false_fn` it will always be evaluated when the `cond` is -- which is inconsistent with the intended behaviour. If you were to run that `session.run(v1.initializer)` statement and feed `p` it would work (but we shouldn't expect the user to do that).\r\n\r\nThis adds to the list of edge cases (that I'm aware of) where `_build_initializer_expr` behaves poorly:\r\n\r\n1) This issues of extraneous propagation of placeholders.\r\n\r\n2) Cyclic initializers (now \"fixed\" by having it ignore those graphs).\r\n\r\n3) When TF functions are involved. For example:\r\n\r\n```\r\n@function.Defun(tf.float32, shape_func=lambda op: [op.inputs[0].get_shape()])\r\ndef increment(x):\r\n  return x + tf.ones_like(x)\r\n\r\nv0 = tf.Variable(tf.zeros(shape=[]))\r\na = increment(v0)\r\nv = tf.Variable(a)  # NotFoundError: Op type not registered\r\n```\r\n\r\n4) With most stateful ops. For example:\r\n\r\n```\r\n# Graph rewriting results in wrong values when you have a variable initializer\r\n# which depends on a non-determinstic op which depends on a variable. Note that\r\n# we use random_uniform with integer types because the underlying op\r\n# \"RandomUniformInt\" takes in the minval and maxval as tensors unlike, for\r\n# example, random_normal with calls \"RandomStandardNormal\" and then transforms\r\n# it with the mean/stdev (so the non-determinsitic part doesn't actually)\r\n# depend on the variables and won't be rewritten.\r\nminval = tf.Variable(0, dtype=tf.int32)\r\nmaxval = tf.Variable(1000000, dtype=tf.int32)\r\nr = tf.random_uniform(shape=[], minval=minval, maxval=maxval, dtype=tf.int32)\r\nv0 = tf.Variable(r)\r\nv1 = tf.Variable(v0)  # Same results even if you use v0.initialized_value()\r\nwith tf.Session() as session:\r\n  session.run([minval.initializer, maxval.initializer])\r\n  session.run([v0.initializer, v1.initializer])\r\n  print(session.run([v0, v1])) # Different values.\r\n\r\n# _build_initializer_expr copies the random op so there will be two of them.\r\nprint(sum([op.type == 'RandomUniformInt'\r\n           for op in tf.get_default_graph().get_operations()]))\r\n```\r\n\r\n5) If you have an initializer where there is an intermediate op X which takes an argument that must be a reference type, and that argument is a variable. This algorithm will turn the reference type into a value type and cause an error. For example:\r\n\r\n```\r\nv0 = tf.Variable(tf.zeros(shape=[]))\r\none = tf.ones_like(v0)\r\nv0_plus_one = tf.assign(v0, v0.initialized_value() + one)\r\nv1 = tf.Variable(v0_plus_one)\r\nwith tf.Session() as session:\r\n  session.run(v1.initializer)  # InvalidArgumentError\r\n```\r\n\r\n6) Not a bug, as such, but the implementation will produce an exponentially large number of nodes given a diamond pattern dependency graph (now fixed by memoizing):\r\n\r\n```\r\n# Diamond pattern - exponential growth\r\n# N => N + 2^N - 1 Add ops\r\nN = 4\r\nv0 = tf.Variable(tf.zeros(shape=[]))\r\nx = v0\r\nfor i in range(N):\r\n  x = tf.add(x, x)\r\nv1 = tf.Variable(x)\r\nprint(sum([op.type == 'Add'\r\n           for op in tf.get_default_graph().get_operations()]))\r\nwith tf.Session() as session:\r\n  print(session.run(v1.initializer))\r\n  print(session.run(v1))\r\n```\r\n\r\n7) The implementation completely ignores control dependencies. Any new ops created simply don't have them.\r\n\r\n8) The implementation completely ignores control flow contexts. Any new ops created based on ops in a control flow context aren't added to that context.\r\n\r\n9) The implementation will only be able to find a Variable's initialized_value method if that variable is present in the `GLOBAL_VARIABLES` or `LOCAL_VARIABLES` collection. This is a weird inconsistency to the end user. It's even possible it could find the wrong Variable (although you'd have to be working really hard to get it to do that).\r\n\r\n10) The implementation relies on a hard-coded set of known variable ops. If new variable ops are added it won't recognize them without manual intervention.\r\n\r\n11) Even if the caller explicitly passes in a .initialized_value() to a Variable's constructor the `_build_initializer_expr` will probably create a bunch of new nodes in the graph anyway.\r\n\r\n12) It currently doesn't even do anything when the initial value depends on a ResourceVariable.\r\n\r\nThere are probably even more that I'm not currently aware of.\r\n\r\nI'll start working through these in the coming days. I'm working under the assumption that we can't just remove this transformation since there will already be a significant number of graphs which depend on it. That means most of the work will be to whitelist and/or blacklist the set of `initial_value` subgraphs for which we can safely apply the transformation.\r\n\r\n  ", "Working this feature into existing client code is complicated, I wonder if it would be much simpler to add it as new functionality. Since dependent initialization has never worked in TF, this can be a new op, like VariableV5, which can eventually replace Variable.\r\n\r\nThe example [here](https://gist.github.com/yaroslavvb/d592394c0cedd32513f8fbb87ca05938\r\n) worked for our code in OpenAI, and the idea there was to make variable reads have control dependency on \"safe-initializer\". \"Safe-initializer\" is an assign op that only runs when variable has not been initialized. \r\n\r\nPS, motivation for the breaking commit is https://github.com/tensorflow/tensorflow/issues/4920\r\n", "@djvisentin I think this is fixed, right?", "Are there any updates on this?\r\nI just tried depending variables, e.g.:\r\n\r\n```\r\nvar = tf.Variable(some_placeholder)\r\nvar2 = tf.Variable(var.initialized_value())\r\n```\r\nwhich did not work.\r\nAfter initialization, `sess.run(var)` gave me the correct values, but `sess.run(var.initialized_value())` raised an error because of missing `some_placeholder`.", "Well as a workaround I just convert the variable to a numpy array and initialize using the numpy array, it feels as inelegant as it looks but it's the simplest way I've found @Hoeze", "That's not pretty, but simple.\r\nNevertheless, imho this should be solved. A dependency tree initializing variables would be much more pretty.", "Nagging Assignee @djvisentin: It has been 152 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "If you use tf.function in tf v2 this will not be a problem, so this is how we plan on fully resolving this issue."]}, {"number": 13350, "title": "Compilation fails with --config=sycl due to missing comma in tensorflow/core/kernels/training_ops.cc", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nmaster from github\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\nbazel release 0.5.4\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:\r\nAMD Radeon (TM) R9 380 Series\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt --config=sycl //tensorflow/tools/pip_package:build_pip_package\r\n### Describe the problem\r\nBug in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc\r\nin line 2551 resulting from a missing comma. Instead of\r\n                            ctx, 0, use_exclusive_lock_, false & var));\r\nit should read\r\n                            ctx, 0, use_exclusive_lock_, false, &var));\r\n\r\n### Source code / logs\r\nWhen compiling with opencl enabled (--config=sycl) the bazel build command will result in an error.", "comments": []}, {"number": 13349, "title": "ImportError: DLL load failed: The specified module could not be found. While attempting to import TensorFlow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["i have windows 10\r\ni installed python 3.6.2 and anaconda 3 4.4\r\ni have GPU(GeForce GT 755M) and update the drivers\r\ni installed visual studio 2017\r\ni installed cuda 9  and cudnn7(cudnn64_7.dll)\r\ni installed the tensorflow according to the instruction in tutorial at tensorflow.org\r\ni also set the path variable for cudnn and cudo 9\r\n\r\nbut after all of this i having this error\r\n\r\n\r\n\r\nC:\\Users\\Admin>activate tensorflow\r\n\r\n(tensorflow) C:\\Users\\Admin>python\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "There is more than one set of instructions for install at tensorflow.org, and it's not clear which you followed.\r\n\r\nThere are two for windows, here: https://www.tensorflow.org/install/install_windows\r\n\r\nThe anaconda install is community supported, so it is more appropriate to ask questions regarding it on stackoverflow.  Including your exact sequence of installation actions may help someone to identify your problem.", "@mushtaqpatel5592 could you please try running [this script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c) and see what it gives you?\r\n\r\n@mrry do you think perhaps including a note for windows users and your script to the issue template may be helpful, or would clog it too much?", "@Carmezim We already include a link to common installations page (which links to the script) in the error message itself, so I'm not sure people would read it if it were in the issue template :).\r\n\r\nWe're working on better error messages for the Windows build, and hope to have something in 1.4 that should help cut down on these issues!", "@mrry Oh, I see. Didn't notice the link on common problems, my bad. That'll do :)\r\n\r\nOn Mon, Oct 2, 2017, 2:39 AM Derek Murray <notifications@github.com> wrote:\r\n\r\n> @Carmezim <https://github.com/carmezim> We already include a link to\r\n> common installations page (which links to the script) in the error message\r\n> itself, so I'm not sure people would read it if it were in the issue\r\n> template :).\r\n>\r\n> We're working on better error messages for the Windows build, and hope to\r\n> have something in 1.4 that should help cut down on these issues!\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/13349#issuecomment-333422695>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/APwNCjIDAKXv5EpmvOa_bB3rglGaB3cGks5soD7bgaJpZM4PmTg4>\r\n> .\r\n>\r\n", "I faced the same problem, so how to fix this?", "@yushinliu could you please describe what steps you followed with relevant info and which errors you're getting? Have you also tried following the suggested above?\r\n", "@Carmezim \r\nI have Win10 and I also installed cuda 9 and cudnn7. After input code ' import tensorflow '\r\n \r\n`>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n\r\nso I have no idea about that and I am now trying to back grade my CUDA from 9 to 8", "@yushinliu You're right about the CUDA versions. The only one supported as of now is v8.0 as you can see from the [docs](https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support). If you face any further issues after changing to the correct version please follow up.", "@Carmezim \r\nThanks, it works! so it was a version problem", "@yushinliu Cool. If you happen to face any difficulties please feel free to ask. Stackoverflow.com under TensorFlow tag is also a great resource for troubleshooting and support :)", "@yushinliu  so you solve this problem by using CUDA 8.0 and cuDNN of what version? I met exactly same errors and I'm encountering hardware warnings when installing CUDA 8.0. Thank you.", "@rainstaryl folks honestly need to check the docs before. You'll see the supported versions are 6.0/6.1.", "@Carmezim thank you and when do you expect CUDA9.0 could be supported normally?", "@rainstaryl Likely on TF 1.5 along with cuDNN 7", "@poxvoculi Should this issue be closed for inactivity from the author? This is also probably a duplicate. If he didn't manage to solve yet he could open again.", "@rainstaryl  yes , I solved it by installing the CUDA 8 and cuDNN v6.0, glad that you solved it as well", "Thanks for your help as always, @Carmezim! I think we can close this one now.\r\n\r\n(By the way, TF 1.4 should have more actionable error messages for DLL problems, so I'm hoping that the support load should shrink....)", "@rainstaryl try uninstalling previous versions of CUDA first before installing CUDA 8.0. Also, get cuDNN 5.1.\r\nIt worked in my system. Hope it helps :)"]}, {"number": 13348, "title": "Dataset API does not pass dimensionality information when constructing graph.  [using official ResNet]", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6.1 \r\n\r\n\r\n### Describe the problem\r\nWhen I try to combine the dataset API with the resnet architecture provided at tensorflow/models/official/resnet the graph cannot be constructed because the dimension of the input data is not passed to the model constructing function.  \r\n\r\n### Source code / logs\r\n\r\nSkeleton code:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    print(\"initialized\")\r\n\r\n    features_placeholder = tf.placeholder(prepared_x.dtype, prepared_x.shape)\r\n    labels_placeholder = tf.placeholder(dtype=tf.float32, shape=prepared_t.shape)\r\n\r\n    dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\r\n    dataset = dataset.shuffle(buffer_size=10000)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.repeat(num_epoch)\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    (next_x_test, next_t_test) = iterator.get_next()\r\n    next_x_test = tf.to_float(next_x_test, name='ToFloat')\r\n\r\n\r\n    sess.run(iterator.initializer, feed_dict={features_placeholder: prepared_x,\r\n                                              labels_placeholder: prepared_t})\r\n\r\n\r\n    print(next_x_test)\r\n    print(next_t_test)\r\n\r\n    model = resnet_v2(resnet_size=50, num_classes=num_bins)\r\n\r\n    output = model(next_x_test,is_training=True)\r\n```\r\n\r\nThe last line throws an error on compiling:\r\n> ValueError: The last dimension of the inputs to Dense should be defined. Found None.\r\n\r\nthis makes reference back to the resent_v2 definition where the final layer is a dense layer.\r\n\r\nIt appears that the dataset API is not passing dimension information hence the final dense layer does not know how to construct itself.  ", "comments": ["Actually this feels like a more general problem ... I just ran into this again when trying to reshape a tensor fed by a dataset iterator.  In this case the code is roughly: \r\n\r\n```\r\ntrain_dataset = Dataset.from_tensor_slices({\"image\": features_placeholder_train, \"label\": labels_placeholder_train})\r\n  iterator = train_dataset.make_initializable_iterator()\r\n  (next_x_test, next_y_test) = iterator.get_next()\r\ny_mod = tf.reshape(next_x_test,[-1,x_dim,y_dim,depth])\r\n\r\n```\r\n When running this, tensorflow throws\r\n \r\n> ValueError: Dimension size must be evenly divisible by ___  but is 1 for 'reshape/Reshape' with input shapes: [], [4] and with input tensors computed as partial shapes: input[1] = [?,28,28,1]. ", "Sorry this fell through the cracks. @mrry WDYT?", "@Noahyt What is the output of these `print()` statements?\r\n\r\n```python\r\n    print(next_x_test)\r\n    print(next_t_test)\r\n```", "@mrry  I am also facing a similar issue when applying a dense layer just after using tf.layer.Flatten() , is there any workaround where we can use dense with some layer that is of the shape [batch_size , None] (None as the dimensions of the second shape argument keeps changing ) . I did go through your answer on stack overflow regarding the same question https://stackoverflow.com/questions/39947793/dimentions-of-layers-in-conv-net-tensorflow-valueerror-shape-of-a-new-variable but that does not seems to work in tf r1.4", "@rishabh135 Can you provide enough information to reproduce the problem?", "@mrry  yeah , I have a network of following kind : \r\n```\r\n        h3 = conv1d(h2, s2, size=3, rate=2, scope=\"d_h3_conv\")\r\n        h3 = tf.layers.batch_normalization(h3)\r\n        h3 = tf.nn.leaky_relu(h3)\r\n        \r\n        h4 = tf.contrib.layers.flatten(h3)\r\n   \r\n        #h5 = tf.layers.dense(h4,1, activation=None, name=\"d_h5_lin\")\r\n        h5 = tf.layers.dense(h4,1)\r\n```\r\n\r\nand it throws up the above error (ValueError: The last dimension of the inputs to Dense should be defined. Found None.) But as I am working on audio dataset , hence I have a variable amount of dimension depending upon the number of timesteps , instead of using flatten I even tried using  tf.reshape along with tf.get_shape to make it work but to no avail.", "This is not an issue with the `tf.data` API: the `h4` tensor inherently has an unknown shape, because it depends on flattening a variable-size tensor. \r\n\r\nThe problem here is that `tf.layers.dense()` infers the number of input units in the layer from the shape of the input (`h4` in this case), and therefore it requires the input to have a static number of columns. However, in your program the number of columns depends on the sequence length, which is variable. You can't use `tf.layers.dense()` in this case; you must instead use a layer type that accepts variable-sized input, such as an RNN or a convolution.", "@mrry  thanks for that information , but I am wondering if I can't use the dynamic shape of the h4 tensor above using your answer from https://stackoverflow.com/questions/37096225/how-to-understand-static-shape-and-dynamic-shape-in-tensorflow  to run the tf.layers.dense() , isn't that possible ?", "No, because you cannot use a dynamic shape to define a variable. The dense layer is a wrapper [around two variables](https://github.com/tensorflow/tensorflow/blob/e210cb140a60a74d5e9ce3bf9ebedb21b4910f1c/tensorflow/python/layers/core.py#L131), whose shape depends on the number of columns in the input. Intuitively, the runtime has to allocate some memory for the variable that is shared across multiple steps, and so we require the shape of that memory to be the same across multiple steps. This requirement is enforced by requiring a static shape.", "I have experienced the same problem when using one_hot_column. I will try to extract a trivial working example out of my code but here is an excerpt:\r\n\r\n```python\r\n# columns are defined like this:\r\nvector = tf.contrib.layers.sparse_column_with_vocabulary_file(\r\n      column_name=\"vector\",\r\n      vocabulary_file=vocab,\r\n      vocab_size=vocab_size)\r\nvector_weighted = tf.contrib.layers.weighted_sparse_column(\r\n      sparse_id_column=vector,\r\n      weight_column_name=\"vector_weights\")\r\nvector_one_hot = tf.contrib.layers.one_hot_column(vector_weighted)\r\n\r\n# estimator is defined like this:\r\nestimator = tf.contrib.learn.DNNRegressor(\r\n    feature_columns=[vector_one_hot],\r\n    hidden_units=[1024],\r\n    optimizer=tf.train.AdagradOptimizer(learning_rate))\r\n```\r\n\r\nThis fails with \r\n```\r\nThe last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```", "@tilarids That sounds like it might have a different cause, since it looks like there's enough static information there to infer the appropriate shape. Please open a new issue with a complete set of details, and somebody who works on `tf.layers` will take a look.", "Was a new issue opened for this?  I'm receiving the same error following a call to Flatten().  If I run just the flatten through a session, I get a \"result.shape\" of (2, 4656).  No \"none\" is present.  But when I try to feed that same result into a Dense layer, I get the error (\"the last dimension of the inputs to `Dense` should be defined. Found `None`)."]}, {"number": 13347, "title": "Tensorflow 1.3.0  and Python 3.5.2  Issue in Redhat RH 6.7 x86_64 issue in runtime", "body": "** GLIBC version 2.12 ***\r\npython -c \"import tensorflow\"\r\n File \"/hdpapp/Anaconda3-4.2.0/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /hdpapp/Anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nUpgraded *** GLIB version to 2.14*****\r\npython -c \"import tensorflow\" \r\n File \"/hdpapp/Anaconda3-4.2.0/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /opt/glibc-2.14/lib/libc.so.6: version `GLIBC_2.17' not found (required by /hdpapp/Anaconda3-4.2.0/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nUpgraded *** GLIB version to 2.17*****\r\npython -c \"import tensorflow\"\r\nerror while loading shared libraries: __vdso_time: invalid mode for dlopen(): Invalid argument\r\n\r\nAny idea what configuration is not matching causing the runtime issue\r\n", "comments": ["We do not have official support for Redhat, or Anaconda. So I am marking this community support.\r\n\r\nHowever, the following is possible.\r\nIt is possible anaconda python and tensorflow are looking for different versions of glibc.\r\nCould you install pip and python through RPM and try again?", "`conda install tensorflow` works "]}, {"number": 13346, "title": "Diagnosis after running tensorflow_self_check.py", "body": "Hello\r\n\r\nAfter running tensorflow_self_check.py with Jupyter, I get the following message:\r\n\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.5.\r\n\r\n- TensorFlow is installed at: c:\\users\\gmagen\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\r\n\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: -1\r\n\r\n\r\nHow do I solve it|", "comments": ["This may be better for stackoverflow, there's not enough information here to address it as a bug on tesnorflow side (ie, filling out the template)", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13345, "title": "Exposing the TF Server interface through the C API", "body": "@asimshankar Would you be willing to expose the server interface through the C API? It consists of mainly 4 methods (`New`, `Start`, `Stop`, and `Join`) and it's the only functionality missing from the C API to allow easy setup for distributed training.", "comments": ["Resolved, as described in #13344."]}, {"number": 13344, "title": "Using CheckpointReader from another language", "body": "@asimshankar Is the checkpoint reader class (`checkpoint_reader.h`) currently exposed through any dynamic library?", "comments": ["I do not believe it is (and it isn't even C, it's C++ :).\r\n\r\nWhat exactly do you want to do? As in, we could consider a pure-C API for reading checkpoints, but the [`SaveV2`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/io_ops.cc#L60) and [`RestoreV2`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/io_ops.cc#L100) operations deal with checkpoints - so presumably those could be used without needing to expose a lower level API?", "@asimshankar Actually never mind. @allenlavoie pointed me to the `libtensorflow_framework.so` binary yesterday, and I was able to build the bindings I needed using that ([here](https://github.com/eaplatanios/tensorflow_scala/blob/master/jni/src/main/native/checkpoint_reader.cc) is a one example). It's actually great because with this binary I can now implement an interface to the server code for distributed training, add while loop backprop support, etc. :)\r\n\r\nThis also resolves #13345 and so I'll go ahead and close it."]}, {"number": 13343, "title": "R1.3", "body": "sadas", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@anony00 can you improve the description of the PR and what is this for ?", "r1.3 cherrypicks were already merged back into master."]}, {"number": 13342, "title": "slim.learning.train can't restore variables if new variable have created.", "body": "Look at my code:\r\n\r\n```python\r\ndef slim_train_init_fn_test():\r\n    '''\r\n        In the model_ckpt/slim_train_init_fn_test.ckpt,\r\n        {'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}\r\n    '''\r\n    x_y = slim.variable('x/y', initializer=4.0)\r\n    x_z = slim.variable('x/z', initializer=5.0)\r\n    y_z = slim.variable('y/z', initializer=6.0)\r\n\r\n    variables_to_restore = slim.get_variables_to_restore(include=['x'])\r\n\r\n    init_fn = tf.contrib.framework.assign_from_checkpoint_fn(\r\n        'model_ckpt/slim_train_init_fn_test.ckpt', variables_to_restore)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\r\n    loss = slim.variable('loss', initializer=10.0)\r\n    train_op = slim.learning.create_train_op(loss, optimizer)\r\n    slim.learning.train(train_op, 'log', init_fn=init_fn, number_of_steps=1)\r\n```\r\n\r\n In the `model_ckpt/slim_train_init_fn_test.ckpt`,  there is a map  `{'x/y': 1.0, 'x/z': 2.0, 'y/z': 3.0}`, and I use \r\n`get_variables_to_restore(include=['x'])` to restore variable `x_y` and  `x_z` which are in  `model_ckpt/slim_train_init_fn_test.ckpt`, but I create a new variable `loss` which isn't in `model_ckpt/slim_train_init_fn_test.ckpt`, the code raise an error:  Can't find key `loss` in check point file. \r\nSo I can't understand why check point file should have key `loss`,  does `init_fn` not pass `variables_to_restore` to `tf.train.Saver` ? Is it a bug?\r\n\r\n\r\nView the [assign_from_checkpoint_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L659), it indeed uses `assign_from_checkpoint_fn` to initialize `tf.train.Saver` .  And there is another parameter `saver`  in  [slim.learning.train](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L729), it is  just used to [save](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L774)  parameters of model, not `restore`. \r\n\r\nSo what's  wrong?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["After I view the source, I find there maybe  faulty design. [slim.learning.train](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L717) use `Supervisor`, and the [construct function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py#L200) of `Supervisor` use [_init_saver](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py#L438) to initialize `saver`, which means if global_variables() is not NULL, `saver` will be `tf.train.saver()`.\r\nAnd then it will create [SessionManager](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py#L342) and invoke [managed_session](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py#L889) and [prepare_or_wait_for_session](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py#L704),\r\nbut if the `saver` is not NULL, the `SessionManager` [won't run `init_op` and `init_fn`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/session_manager.py#L274), that's why it raise an error.\r\n\r\nIt means the `saver` is always not NULL, and `init_op` and `init_fn` will be never run.\r\n\r\nCan anyone give me response and how to fix it, I just want to restore some variables from checkpoint file and the other variables should invoke init_op to initialize.", "I have the same problem with you. I solve it by doing following two steps.\r\n\r\n### 1. pass the parameter `saver` to `slim.learning.train()`\r\n```python\r\nckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\r\nsaver = tf.train.Saver(var_list=optimistic_restore_vars(ckpt.model_checkpoint_path) if ckpt else None)\r\n```\r\nwhere function [optimistic_restore_vars](https://github.com/tensorflow/tensorflow/issues/312) is defined as\r\n```python\r\ndef optimistic_restore_vars(model_checkpoint_path):\r\n    reader = tf.train.NewCheckpointReader(model_checkpoint_path)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n                        if var.name.split(':')[0] in saved_shapes])\r\n    restore_vars = []\r\n    name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\r\n    with tf.variable_scope('', reuse=True):\r\n        for var_name, saved_var_name in var_names:\r\n            curr_var = name2var[saved_var_name]\r\n            var_shape = curr_var.get_shape().as_list()\r\n            if var_shape == saved_shapes[saved_var_name]:\r\n                restore_vars.append(curr_var)\r\n    return restore_vars\r\n```\r\n\r\n### 2. pass the parameter `local_init_op` to `slim.learning.train()` to initialize the added new variables\r\n```python  \r\nlocal_init_op = tf.global_variables_initializer()\r\n```\r\n\r\n### In last, the code should look like this\r\n```python\r\nckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\r\nsaver = tf.train.Saver(var_list=optimistic_restore_vars(ckpt.model_checkpoint_path) if ckpt else None)\r\nlocal_init_op = tf.global_variables_initializer()\r\n\r\n###########################\r\n# Kicks off the training. #\r\n###########################\r\nlearning.train(\r\n    train_tensor,\r\n    saver=saver,\r\n    local_init_op=local_init_op,\r\n    logdir=FLAGS.train_dir,\r\n    master=FLAGS.master,\r\n    is_chief=(FLAGS.task == 0),\r\n    init_fn=_get_init_fn(),\r\n    summary_op=summary_op,\r\n    number_of_steps=FLAGS.max_number_of_steps,\r\n    log_every_n_steps=FLAGS.log_every_n_steps,\r\n    save_summaries_secs=FLAGS.save_summaries_secs,\r\n    save_interval_secs=FLAGS.save_interval_secs,\r\n    sync_optimizer=optimizer if FLAGS.sync_replicas else None)\r\n```\r\n", "Don't put checkpoint file in `logdir`, so that the `saver` can't restore and  `init_fn` will work. So just put checkpoint file in another directory and pass to `tf.contrib.framework.assign_from_checkpoint_fn`.  I  read the source code and find the solution.\r\n\r\nOr you can use your own `saver` and pass `variables_to_restore` and make sure the checkpoint file is in your `logdir`. And you should first invoke `tf.global_variables_initializer()` to initialize other variables. Because in this case `slim` won't invoke `init_op`.", "Yeah, slim will not invoke init_op, but I found the parameter  `local_init_op` is done before `saver.restore`. Thus, I use `local_init_op=tf.global_variables_initializer()` and pass it into `slim.learning.train()`. Is this a proper way? Thank you.", "Ref in `tensorflow/python/training/Supervisor (about line 186):\r\n  ##### Custom model initialization\r\n\r\n  `managed_session()` only supports initializing the model by running an\r\n  `init_op` or restoring from the latest checkpoint.  If you have special\r\n  initialization needs, see how to specify a `local_init_op` when creating the\r\n  supervisor.  You can also use the `SessionManager` directly to create a\r\n  session and check if it could be initialized automatically.\r\n  \"\"\"\r\n", "I have the same problem, and solve it using session wrapper function.\r\n\r\n```Python\r\n  def _session_wrapper_fn(sess):\r\n    feed_init_fn(sess)\r\n    return sess\r\n\r\n  slim.learning.train(train_op, \r\n      logdir=FLAGS.train_log_dir,\r\n      graph=g,\r\n      master='',\r\n      is_chief=True,\r\n      number_of_steps=train_config.number_of_steps,\r\n      log_every_n_steps=train_config.log_every_n_steps,\r\n      save_interval_secs=train_config.save_interval_secs,\r\n      save_summaries_secs=train_config.save_summaries_secs,\r\n      session_config=session_config,\r\n      session_wrapper=_session_wrapper_fn,\r\n      init_fn=init_fn,\r\n      saver=saver)\r\n\r\n```", "@yekeren @gauss-clb \r\nI met the same problem but mine crashes when [a default saver is created](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L658)\r\n inside slim.learning.train for it requires all variables in the graph to be found in the checkpoint. If an explicit saver is defined using var_list, newly inserted variables won't be tracked and saved later on. May I know how did you work this out? Thanks!", "@ChanZou I noticed my case is slightly different. The data I fed in feed_init_fn() is the training data thus I do not have the need to save them later. For your case, I suggest you use the init_fn parameter of slim.learning.train. That is, as @gauss-clb said, ensure that there are no checkpoint files in logdir, then set init_fn to be the returned value of tf.contrib.framework.assign_from_checkpoint_fn.", "@yekeren Thank you for your reply! Yes I missed that important trick to fool the saver. Now it works. Thanks!", "@gauss-clb First of all, thanks for your sharing. \r\n\r\nAccording to your comment \"Don't put checkpoint file in logdir, so that the saver can't restore and init_fn will work.\" So in my concept, slim will still run init_op and init_fn when saver cannot restore successfully, which means all global variables are initialized then.\r\n\r\nHowever, in my case, I found that the additional variables (not included in checkpoint) are still not initialized in NON CHIEF worker, but the chief worker can train well (I think it means that all variables are initialized successfully).\r\nAny ideas? Thanks!", "@Amitayus I reviewed your solution and it comes up with a question. In your first step, you tell saver which variables should be restored according to ckpt file, and  set local_init_op as tf.global_variables_initialier().\r\n\r\nFrom source code [session_manager.py](https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/python/training/session_manager.py#L291), it shows that local_init_op will run after whatever saver successfully restored or restoring variables in init_fn. I'm wondering that it may cause that the initialized variables will be re-initialized due to local_init_op(tf.global_variables_initializer()) and leads to the failure of restoration from ckpt file. ", "Hello,\r\n\r\nRecently I faced the same problem and I solved with the following steps:\r\n\r\n1. As @Amitayus , I defined a `tf.saver()` (I get the variables to restore by using `optimistic_restore_vars` as well). This is done after defining the training op\r\n```\r\nvars2restore = optimistic_restore_vars(ckpt.model_checkpoint_path)\r\nsaver = tf.train.Saver(max_to_keep=3, keep_checkpoint_every_n_hours=2,\r\n                                       var_list=vars2restore if checkpoint_path else None)\r\n```\r\n2.  Define `init_fn` as:\r\n```\r\ninit_fn = tf.contrib.framework.assign_from_checkpoint_fn(checkpoint_path, vars2restore)\r\n```\r\n3. Pass both arguments to `slim.learning.train` as:\r\n```\r\nfinal_loss = slim.learning.train(\r\n                    train_op,\r\n                    log_dir,\r\n                    # session_config=tf.ConfigProto(allow_soft_placement=True),\r\n                    global_step=checkpoint_global_step_tensor,\r\n                    save_summaries_secs=save_summaries_secs,\r\n                    number_of_steps=training_schedule['max_iters'],\r\n                    # init_fn=InitAssignFn,\r\n                    # train_step_fn=train_step_fn,\r\n                    saver=saver,\r\n                    # local_init_op=local_init_op,\r\n                    init_fn=init_fn,\r\n                )\r\n```\r\nSo, in my case `local_init_op` did not work as it is *probably* re-initialized the restored variables.\r\nI am able to resume training with approximately the same loss reported before stopping/pausing it.\r\nNotice that I still need to check if the custom saver is actually used since `init_fn` ends up creating one with the `var_list` passed.\r\n\r\nAnyway, figure out I should publish this in case it helps someone!\r\n\r\nCheers,\r\nFerran."]}, {"number": 13341, "title": "LSTM RNN \"Variable rnn/basic_lstm_cell/kernel already exists, disallowed\" error ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I am trying to build a two layered stacked LSTM without using MultiRNNcell, as follows,\r\n    def initialize_lstm(self): \r\n        with tf.variable_scope(\"lstm_layer_1\") as scope:\r\n            self.lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(self.num_cells_1,reuse=tf.get_variable_scope().reuse)\r\n        with tf.variable_scope(\"lstm_layer_2\") as scope: \r\n            self.lstm_cell_2 = tf.nn.rnn_cell.BasicLSTMCell(self.num_cells_2)\r\n        self.lstm_c1 = tf.placeholder(tf.float32,[None,self.num_cells_1])\r\n        self.lstm_h1 = tf.placeholder(tf.float32,[None,self.num_cells_1])\r\n        self.lstm_c2 = tf.placeholder(tf.float32,[None,self.num_cells_2])\r\n        self.lstm_h2 = tf.placeholder(tf.float32,[None,self.num_cells_2])        \r\n        self.lstm_start_cell_1 = tf.nn.rnn_cell.LSTMStateTuple(self.lstm_c1,self.lstm_h1)\r\n        self.lstm_start_cell_2 = tf.nn.rnn_cell.LSTMStateTuple(self.lstm_c2,self.lstm_h2)\r\n        self.lstm_output_1,self.lstm_state_1 = \\\r\n        tf.nn.dynamic_rnn(self.lstm_cell_1,self.state_seq,initial_state = self.lstm_start_cell_1)\r\n        self.lstm_output_2,self.lstm_state_2 = \\\r\n        tf.nn.dynamic_rnn(self.lstm_cell_2,self.lstm_output_1,initial_state = self.lstm_start_cell_2)\r\n\r\nHowever, I get the following error, \"ValueError: Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\"...I am unable to figure out what is the issue?", "What if you replace line 3 with: \r\n`self.lstm_cell_1 = tf.nn.rnn_cell.BasicLSTMCell(self.num_cells_1)`", "I have run into same error .... any solution for this issue \"??????", "same error for  tf 1.4    i fixed it , \r\n\r\nit seem like   i  have  run  twice \"sess.run(tf.global_variables_initializer())\"  ,", "same error for tf 1.4", "I think you can close it now", "Did you guys find any fixes for this issue? I am using iphython and ran into this issue. Only kernel restart seems to fix the problem.", "use ' tf.reset_default_graph()'  could solve it", "ValueError: Variable rnn_outputs/rnn/lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:     how to fix it!!!", "yes!!!  'tf.reset_default_graph()' could solve it , that's right\r\n", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The build() / call() method on the LSTMcell actually creates the variables. So the variable_scope while creating the Cell doesn't matter as much as when its actually built (=called for the first time). My guess is that the building for both the cells is happening under the same variable scope and therefore it hits the same variable\r\n\r\nCould you produce a stack trace with your error? That could confirm this theory.", "Closing as this issue is resolved, feel free to reopen if problems persists or when you can provide a stack trace.", "But I have a question will 'tf.reset_default_graph()' reset the entire graph /network created ?"]}, {"number": 13340, "title": "Tensor Flow Installation issue: couldnot find a version that satisfies the requirement tensorflow", "body": "I am trying to install Tensorflow through anaconda. Following is the error when I try to install\r\n_\"couldnot find a version that satisfies the requirement tensorflow\"._\r\n\r\nI tried to create two sessions - with Python 3.5 and Python 3.6 - but both of them returns the error identically.\r\n\r\nCode used in the activated sessions: pip install tensorflow\r\n \r\nScreenshot1 - Python 3.6 (Environment name: tenflow)\r\n![image](https://user-images.githubusercontent.com/8396984/30917306-543b3d56-a3b9-11e7-91f8-aa705342e773.png)\r\n\r\nScreenshot2 - Python 3.6 (Environment name: tflow)\r\n![image](https://user-images.githubusercontent.com/8396984/30917325-603c716a-a3b9-11e7-9daf-c8e323d55bf2.png) ", "comments": ["Assuming your Python 3.5 distribution is 64-bit and not 32-bit, could you try `conda install tensorflow` rather than using `pip`?", "Thanks carmezium. My python is 32 bit. Now I realise there isn't tensorflow for 32 bit.", "Glad you found the culprit. If there isn't any other question this issue can be closed now."]}, {"number": 13339, "title": "Update protobuf to 3.4.1", "body": "This change is required for compatibility with the upcoming Bazel 0.7. There seem to be no issues in tensorflow itself but there are in the old version of the protobuf repository.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "There seems to be small failures in one of the tests:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/7006/consoleFull\r\nCould you take a look at the failure above and fix them?", "The culprit seems to be this commit for protobuf: https://github.com/google/protobuf/commit/dd19b876d4c9a604946f6c4e39cc4eac5f12cfb9\r\n\r\nThey raised the number of digits used for floats, I've made a similar change for tensorflow which should fix the failing test.", "Jenkins, test this please.", "Jenkins, test this please.\r\n", "The tests seem to be fixed, can it be merged please? I also need to fix compatibility for other Tensorflow-related repositories, such as Serving and Models, and currently they both fail because they depend on this main repository."]}, {"number": 13338, "title": "Feature request: add tf.layers.Group to group multiple layers under one name", "body": "Example usage (relevant for networks with skip connection i.e. u-net):\r\n\r\n```python\r\nencoder1 = tf.layers.Group([\r\n  tf.layers.Conv2d(...),\r\n  tf.layers.BatchNorm(),\r\n  ActivationLayer(),\r\n], name='encoder1')\r\n\r\nencoder2 = tf.layers.Group([\r\n  tf.layers.Conv2d(...),\r\n  tf.layers.BatchNorm(),\r\n  ActivationLayer(),\r\n], name='encoder1')\r\n\r\ninputs = outputs = tf.layers.Input(tensor=x)\r\n\r\nfor enc in [encoder1, encoder2]:\r\n  outputs = enc(outputs)\r\n\r\nencoders = tf.layers.Network(inputs, outputs)\r\n\r\n# same for decoders\r\nfor i, dec in enumerate(decoders):\r\n  outputs = dec(tf.concat([encoders.get_layer(f'encoder{i}').output, outputs], 3))\r\n```\r\n\r\nCurrently you could either\r\n* use `network.get_output_at()` however this requires to track node indices or parse `network.layers` if encoders are not symmetric (i.e. some encoder have no batch norm / dropout)\r\n* inherit from `tf.layers.Layer` however documentation is not clear on how to \"forward\" variables for example", "comments": ["In your example, you named your encoder2 'encoder1'\r\n", "@martinwicke WDYT?", "Sorry, I don't understand what this does -- is this a version of Sequential? ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since the interested parties seem to have moved on"]}, {"number": 13337, "title": "MonitoredTrainingSession: CreateSession still waiting", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-2195-gd86ee219e 1.4.0-dev\r\n- **Python version**: Python 3.6.1 :: Anaconda 4.4.0 (x86_64)\r\n- **Bazel version (if compiling from source)**: 0.5.2-homebrew\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: `./run.sh`\r\n\r\n### Describe the problem\r\n\r\nI've already raised this problem on [stackoverflow](https://stackoverflow.com/questions/46429766/distributed-tensorflow-createsession-still-waiting), but haven't got any feedback.\r\n\r\nI run the python script four times with the bash script. There are three `worker` tasks and one `master` task. Sometimes all the workers successfully exit. But often one or two of them hang and begin emitting \"CreateSession still waiting for some other task\" messages. Such waiting worker can wait for the chief (worker task 0) or some other task which is already completed.\r\n\r\nSo, the problem is that workers and even the chief do not wait for some lagging worker. Is it a bug? Chief is responsible for initialisation of variables. But if I put a variable on each worker nothing changes. The chief still do not wait.\r\n\r\nI've succeeded in synchronisation of these workers by adding `FIFOQueue` barriers to the beginning of each session.\r\n\r\n### Source code / logs\r\n\r\nTF script (`train.py`):\r\n\r\n```python\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--job', type=str)\r\nparser.add_argument('--task', type=int)\r\nargs = parser.parse_args()\r\nhosts = {\r\n    \"master\": [\r\n        \"localhost:2222\",\r\n    ],\r\n    \"worker\": [\r\n        \"localhost:2223\",\r\n        \"localhost:2224\",\r\n        \"localhost:2225\",\r\n    ]\r\n}\r\n\r\nnworkers = len(hosts['worker'])\r\ncluster = tf.train.ClusterSpec(hosts)\r\nserver = tf.train.Server(cluster, job_name=args.job, task_index=args.task)\r\n\r\nwith tf.device(f'/job:master/task:0'):\r\n    global_step = tf.train.get_or_create_global_step()\r\n    inc_global_step = tf.assign(global_step, global_step + 1)\r\n\r\nif args.job == 'worker':\r\n    hooks = [\r\n        tf.train.StopAtStepHook(last_step=4),\r\n    ]\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(args.task == 0),\r\n                                           hooks=hooks) as sess:\r\n        while not sess.should_stop():\r\n            print(args.task, sess.run(inc_global_step))\r\nelse:\r\n    server.join()\r\n```\r\nBash script (`run.sh`):\r\n```bash\r\n#!/bin/bash\r\npython train.py --job master --task 0 &\r\npython train.py --job worker --task 0 &\r\npython train.py --job worker --task 1 &\r\npython train.py --job worker --task 2 &\r\n```\r\nExample of message:\r\n```\r\n2017-09-27 11:52:48.973442: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n```", "comments": ["Got an answer on stackoverflow"]}, {"number": 13336, "title": "Cannot assign a device for operation 'save/ShardedFilename_1' when exporting custom Estimator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux ubuntu254 4.2.0-42-generic \\#49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.3.0\r\n- **Python version**: \r\n2.7.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nrelease 8.0, V8.0.44 / 6.0\r\n- **GPU model and memory**:\r\nGTX Titan Black 6 GB, GTX 1060 6 GB\r\n- **Exact command to reproduce**:\r\nrun estimator_CNN.py\r\n\r\n### Describe the problem\r\nContinued from [this discussion](https://groups.google.com/a/tensorflow.org/forum/#!searchin/discuss/export/discuss/XHABHQG5l2I/jZBvc0-NBgAJ). I want to export a custom Estimator (multiple CNN layer + CTC loss in multi GPU setting derived from cifar-10 multi GPU example) using `export_savemodel()`. But i encountered this error:\r\n`InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/ShardedFilename_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.`\r\nthis should not occured in 1.3.0 (please see discussion)\r\n\r\n### Source code / logs\r\nenvironment [tf_env.txt](https://github.com/tensorflow/tensorflow/files/1336111/tf_env.txt)\r\nfull error trace [error.txt](https://github.com/tensorflow/tensorflow/files/1336110/error.txt)\r\nsource code [TF_bug.zip](https://github.com/tensorflow/tensorflow/files/1336108/TF_bug.zip)\r\n", "comments": ["@ispirmustafa @isaprykin \r\n\r\nLooks like the sharded saver on GPU bug is back. \r\n\r\nSharded savers have a bunch of string processing ops and cannot live on GPUs. If they're forced onto GPUs by collocation, they break in this way. \r\n\r\nI haven't looked at all the code yet, so I'm not positive yet, but I think we have to fix Saver.", "**UPDATE**\r\nI have tried to export another custom Estimator (Recurrent CNN) in the same machine, trained with single GPU with latest tensorflow 1.4.0 and export it successfully.\r\nHowever if I add `with tf.device('/device:GPU:0')` to the model function, the same error occurs even if the training just use a single GPU.", "This sounds something that likely got fixed [here](https://github.com/tensorflow/tensorflow/commit/bf05a2eef97863fc78778bcde5987f93af8a7598).  \r\n\r\nI think this should go away if you try [the latest source version](https://www.tensorflow.org/install/install_sources) or wait for 1.5.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Sure, tensorflowbutler.  I'm pretty sure it's fixed in that commit I pointed to.", "Hello, it seems that my confirmation reply didn't posted correctly and i just realized that now, I'm sorry.\r\nas @isaprykin suggested, building from source was working. \r\nHowever, when I building latest tensorflow serving from source with GPU support, this exact error show up again. from what i know, tensorflow serving will pull latest tensorflow nighly, so this fix will have been merged, right?\r\nI use different machine when i try this, It only use a single GTX 1070  this time. is it because I load a 2 GPU model in a single GPU machine?\r\nI don't know if this is a tensorflow issue or serving issue. I will create new issue in a appropriate repo if needed.\r\nThank You.", "@dieka13 when do you encounter that error? When you run export_savedmodel? Or when you load it into tensorflow/serving?", "@martinwicke when I load it into tensorflow serving. \r\nno problem when running export_savedmodel now.\r\n\r\nI also run into another similar device assignment issue in #17149", "Then I believe this is best filed as an issue to tensorflow/serving.\n@nfiedel FYI.\n", "This is key the key log message: \"Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\". The binary you are trying to run most likely does not link in the GPU kernels. The supported build targets (tensorflow_model_server model_servers/BUILD) and pre-built binaries (installable via apt-get) currently support CPU with and without AVX, but not yet GPU. If you want to try adding a \"tensorflow_gpu_model_server\" that is probably the next step in sorting this out. You are also welcome to file a feature request under serving. Thanks!", "@nfiedel: but i used tensorflow/serving built with ```bazel build -c opt --config=cuda tensorflow_serving/...```, is that still the case?", "@dieka13 : Am not 100% certain, but pretty sure you need both the config as well as ops to be linked-in.", "@dieka13 : I also encounter the same error\r\n **Cannot assign a device for operation save/SaveV2: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.** \r\nIs there any solution as of now?"]}, {"number": 13335, "title": "Potential Bug in CUDA implementation of matrix_set_diag on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Windows 10 (64-bit)\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version (use command below)**: b'unknown' 1.3.0\r\n- **Python version**: 3.6.2\r\n- **CUDA/cuDNN version**: CUDA 8.0 /  cuDNN 6.0\r\n- **GPU model and memory**: Nvidia GTX 1060 6GB\r\n\r\n### Describe the problem\r\nI ran into this while trying to run the code at https://github.com/deepmind/dnc. It seems that tf.matrix_set_diag can cause tensorflow to crash when run on GPU under Windows. I am able to run other complex models on GPU so it seems as though my environment is configured correctly for CUDA.\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nbatch_size = 16\r\nnum_writes = 1\r\nmemory_size = 16\r\n\r\nwith tf.device('/gpu:0'):\r\n    link = tf.Variable(tf.ones([batch_size, num_writes, memory_size, memory_size], dtype=tf.float32))\r\n    test_op = tf.matrix_set_diag(link,tf.zeros([batch_size, num_writes, memory_size],dtype=tf.float32))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(test_op))\r\n```\r\nError log here for normal execution:\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/1336102/error_log.txt)\r\n\r\nThe log from running with cuda-memcheck shows what looks like an issue with a null pointer buried deep inside Eigen...\r\n[error_log_cuda_memcheck.txt](https://github.com/tensorflow/tensorflow/files/1336103/error_log_cuda_memcheck.txt)\r\n\r\n", "comments": ["@rmlarsen Possible bug in an eigen-compiled GPU op on windows: is this expected to work without a local compile from source?", "Happy to try building from source if you think it will help! It occurs to me that I updated my CUDA Toolkit to 8.0.61_win10 Patch 2 recently. Any chance that is the problem? I wouldn't expect the updated dlls to be incompatible with code linked against the old dlls, but the way MSVC's linker works may as well be voodoo to me...", "This is fixed at head now.", "Thanks! If anyone else has run into this, the changes were merged in #13677 which happened just after 1.4.0rc0 was released"]}, {"number": 13334, "title": "Make tf.pow work for integer inputs.", "body": "Currently tf.pow freezes if both inputs are integer tensors and y contains negative values. For example\r\nthe call\r\n`tf.pow(5, -2).eval()`\r\nnever finishes.\r\n\r\nThe cause is the implementation of pow in Eigen. To get around this issue I've changed the\r\nunderlying C++ functor to treat differently integer inputs. However, tf.pow now returns float tensors on\r\ninput integer tensors.\r\n\r\nIn order to pass the backwards compatibility test, I had to create a new op IntegralPow for the case of\r\ninteger inputs. This mirrors the implementation of abs using Abs and ComplexAbs. I've made this\r\nop hidden.\r\n\r\nI've also added tests for the pow function in python.\r\n\r\nThis should fix issue #9560.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@codrut3 can you look at the test failures.", "Jenkins, test this please.", "I did, and there were two types of failures:\r\n1. I was handling dtype incorrectly in math_ops.pow (I fixed this)\r\n2. There are use cases where it's natural to expect tf.pow to return an int32 tensor.\r\n    For example, one of the tests did x ** 2, for x a tf.int32 tensor.\r\n    Another computed tf.pow(2, layer), with layer a tf.constant.\r\n\r\nThese are reasonable use cases, and I don't see a way around them. I don't see how to fix the issue\r\nwithout returning float for all integer inputs.\r\n\r\nSo I was thinking of closing the pull request. I apologize for this.", "Thanks for the contribution, if you can come up with a work-around please re-open. "]}, {"number": 13333, "title": "TensorFlow buillt with -march option gives illegal instruction when imported", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: Build and Install TensorFlow wheel and import TensorFlow\r\n\r\n### Describe the problem\r\nI am trying to build TensorFlow by explicitly providing `\u2013march option on z13`.   \r\nIn the past I could build and install same version (v1.2.1) with default options(-march=native). \r\n\r\nThe issue is when I use below commands for configure and build respectively:\r\n* Configure -  `Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -march=z13   \r\n`   \r\n* Build - `bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package,\r\n`   \r\n\r\nthe build succeeds however the TensorFlow wheel when installed gives issue of python crash:\r\n```\r\nroot@8a268ac0eaea:~# python\r\n >>> import tensorflow as tf\r\nIllegal instruction (core dumped)\r\nroot@8a268ac0eaea:\r\n\r\n```\r\nI have verified the system arch to be z13. However when I use command `gcc \u2013Q \u2013help=target ` I could see `-march= zEC12`.\r\nI am now trying to debug why passing \u2018z13\u2019  causes above issue with wheel.\r\nAlso is there a way to find out what arch is exactly detected when \u2018native\u2019 is passed?\r\n\r\n", "comments": ["Closing this, seems like a problem with my environment. "]}, {"number": 13332, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13331, "title": "Passing CPU value along with GPU tensor", "body": "Hello, i'm trying to pass a CPU value or a block of values from one custom op into another along with a GPU tensor but the framework seems to be converting everything to GPU tensor. There doesn't seem to be a mechanism for passing mixed GPU+CPU op results right now.", "comments": ["Do you have a custom Op that produces both GPU-local and CPU-local tensors?  Can you be a bit more specific about what you're trying to do?\r\n", "Correct, it's a custom op that produces both CPU and GPU tensors but I can't seem to make the framework pass them both along through the graph pinned to their respective devices.", "Ok, let's get more specific.  You're trying to do something that probably wasn't anticipated in the early phases of design, but may be possible anyway.  An Op is supposed to allocate memory for its output tensors by calling OpKernelContext::allocate_output().  Two of the definitions take an AllocatorAttributes parameter, e.g. [this one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h#L808).  If the op is assigned to execute on a GPU (i.e. 'placed on' a GPU), then the allocator will by default allocate memory from GPU RAM, but AllocatorAttributes has an [on_host bit](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/allocator.h#L352) which if set should cause a GPU allocator to allocate from CPU RAM instead.\r\n\r\nAre you allocating memory from the OpKernelContext::allocate_output function or some other way?\r\n\r\nIf from allocate_output, are you setting the on_host attribute of AllocatorAttributes in the request?\r\n\r\nCan you confirm that you've actually allocated a CPU resident tensor within your Op prior to returning the value?\r\n\r\nIs it the case that you successfully allocate CPU RAM and set it, but that the framework automatically converts it to a GPU value, or just fails to return it?", "Thank you, I did try AllocatorAttributes but the next custom op that consumes the output of the previous op seemed to be getting a GPU address in a tensor that was allocated with on_host attribute in a previous op. So i assumed the framework transfered the tensor to GPU automatically. I didn't step through the framework code because I didn't have a debug build atm. I also tried to explicitly allocate a tensor in python as tf.Variable with a context with tf.device(\"/cpu:0\") but the same thing - GPU ops seem to get all tensors converted to their respective devices. I realize this might be a consequence of propagating a placement spec of an op to all it's I/Os or requiring a per-input/output device spec which can complicate things, just wanted to confirm that it is indeed the case or perhaps there's a workaround that I couldn't find.", "So, you're allocating from allocate_output() with the on_host bit, and you've confirmed that the tensor is allocated from CPU RAM when you access it in your Op.  Then the next Op in the flow graph (let's call it B) is also a GPU-placed Op, and when B reads the tensor via OpKernelContext::inputs() you find that it's allocated from GPU memory, correct?\r\n\r\nThe TF execution environment will copy a CPU tensor to GPU RAM when the consuming Op is placed on a GPU, unless the Op specifies that that particular input is expected to be in CPU RAM.  This specification is static, done when the Op is registered.   I don't have access to an example of how, at the moment, but the OpKernel definition will end of with a HOST_MEMORY MemoryType attribute for that input.  Maybe you can figure out by looking for how [this](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/framework/op_kernel.h?q=op_kernel.h&dr=CSs&l=276) gets set.\r\n\r\nSo the next question is, has op B declared that the corresponding input is HOST_MEMORY?", "TensorFlow GPU reductions use mixed inputs -- op is on GPU, input tensor is GPU, reduction indices are on CPU using host memory attribute\r\n\r\nfrom /core/kernels/reduction_ops_min.cc\r\n```\r\n#define REGISTER_GPU_KERNELS(type)          \\\r\n  REGISTER_KERNEL_BUILDER(                  \\\r\n      Name(\"Min\")                           \\\r\n          .Device(DEVICE_GPU)               \\\r\n          .TypeConstraint<type>(\"T\")        \\\r\n          .TypeConstraint<int32>(\"Tidx\")    \\\r\n          .HostMemory(\"reduction_indices\"), \\\r\n      ReductionOp<GPUDevice, type, Eigen::internal::MinReducer<type>>);\r\nREGISTER_GPU_KERNELS(float);\r\nREGISTER_GPU_KERNELS(double);\r\n\r\n```", "Ok, thank you. I missed the HostMemory bit in the custom ops doc, adding it fixed my crash."]}, {"number": 13330, "title": "Extracting out tf.bin_values_fixed_width from tf.histogram_fixed_width.", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "How can I visualize the jenkins logs remotely? All the tests pass locally (with both Python 3 and 2.7) so I need get access to the logs to determine why its failing via Jenkins.", "https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6999/console is an example of the link to the logs. I seem them in the review section below the comments on the PR.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/kernel_tests/histogram_ops_test.py\", line 117, in test_large_class_imbalance_still_ok\r\n    num_updates=1000)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/kernel_tests/histogram_ops_test.py\", line 162, in _check_auc\r\n    labels, scores, score_range, nbins=nbins)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/histogram_ops.py\", line 88, in auc_using_histogram\r\n    score_range, nbins)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/histogram_ops.py\", line 138, in _make_auc_histograms\r\n    name='hist_true')\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/histogram_ops.py\", line 138, in histogram_fixed_width\r\n    indices = bin_values_fixed_width(values, value_range, nbins, dtype, scope)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/histogram_ops.py\", line 96, in bin_values_fixed_width\r\n    return array_ops.reshape(indices, shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_array_ops.py\", line 3920, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/metrics/histogram_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py\", line 528, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'shape' to a tensor and failed. Error: Cannot convert a partially known TensorShape to a Tensor: (?,)\r\n```", "Jenkins, test this please.", "Jenkins, test this please", "The only test that appears to be failing is the goldens tensorflow/tools/api/tests:api_compatibility_test which passes locally. As per the jenkins stdout, I ran:\r\n\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\n\r\nbut this did not appear to alter any files.", "Any thoughts on how to get the compatibility tests to pass?", "The golden files should be updated as part of this pull request. The --update_goldens flag should update the goldens. @gunan might have some thoughts as to why it's not working.", "You need to use python2 to update goldens.\r\nSorry, I meant to document this for a long time now, but keep forgetting.", "Ran it locally using Python 2.7, no other files changed.", "That is weird. Could you share the stdout when you run the following:\r\n```\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\n```\r\nThen you should check what `git status` outputs.", "```\r\n(tf) Nathans-MacBook-Pro:tensorflow nathansilberman$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test           --update_goldens True\r\ns.\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.000s\r\n\r\nOK (skipped=1)\r\n(tf) Nathans-MacBook-Pro:tensorflow nathansilberman$ git status\r\nOn branch master\r\nYour branch is ahead of 'origin/master' by 1 commit.\r\n  (use \"git push\" to publish your local commits)\r\nChanges not staged for commit:\r\n  (use \"git add <file>...\" to update what will be committed)\r\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n\r\n\tmodified:   tensorflow/python/ops/histogram_ops_test.py\r\n\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n\t.cache/\r\n\ttf/\r\n\r\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n(tf) Nathans-MacBook-Pro:tensorflow nathansilberman$ python --version\r\nPython 2.7.10\r\n(tf) Nathans-MacBook-Pro:tensorflow nathansilberman$\r\n```\r\n\r\nNote that the tf directory contains my virtual env and the histogram_ops_test.py change is a minor formatting change.\r\n", "Is it possible in your local repository you have a commit not pushed to your remote branch?\r\nSorry for exploring possible issues on your end, I am also looking into why the test might be doing this.", "From api-review: I think I prefer the name \"tf.histogram_fixed_width_bins\" more than \"tf.bin_values_fixed_width\", and it would clearly indicate the connection to \"tf.histogram_fixed_width\".", "(otherwise fine for API review.)", "Updated the files, still seeing no differences when running the api_compatibility_test with --update_goldens=True. I'm not sure how to proceed here.", "I think the API test requires Python 2.", "@josh11b I ran the tests using Python2:\r\n\r\n```\r\n(tf2) Nathans-MacBook-Pro:tf nathansilberman$ python --version\r\nPython 2.7.10\r\n(tf2) Nathans-MacBook-Pro:tf nathansilberman$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test           --update_goldens True\r\ns.\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.000s\r\n\r\nOK (skipped=1)\r\n```", "There is our problem:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/api_compatibility_test.py#L194\r\n\r\nOnly runs on linux. I will check if we can enable this on other OSs.", "Any update on this?", "I did not have time to see if on macos we can run the api compatibility checks without any changes.\r\ncould you run and update the api goldens under an ubuntu docker image?\r\nYou should be able to use our devel docker images to make the update.\r\n\r\nOnce that is ready, we can get another final nod for api review and merge the change.", "@gunan, can we run compatibility in MacOS? If still unknown, is finding out as simple as running them to see whether there are differences in file handling (I thought you already fixed those anyway)?", "It should be as simple as that. Never quite found the time to look into it though.", "@nathansilberman sorry the code has since changed, could you please pull rebase and push again?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@nathansilberman this is ready to be merge, only pull rebase and push again. Thanks.", "Jenkins, test this please.", "Just double-checking @jhseu that the API change is fine.", "Yeah, this already went through API review. Good to merge.", "Thanks! @nathansilberman please update the goldens:\r\n\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```", "Ran the goldens but it still doesn't seem to update anything on osx...", "@drpngx this has been sitting for a while for the same reason. Mind merging and then just submitting a golden fix as a followup?", "@benoitsteiner ", "For some reason I can't merge.\r\n\r\n@nathansilberman could you try to modify the goldens by hand?\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/golden/tensorflow.pbtxt#L1160", "I see failures in MacOS Python2 but cannot replicate. I ran with:\r\n\r\n```\r\nbazel test -c opt --test_output=all --force_python=py2 tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\nas well as the default args and everything passes...", "Apparently, I needed to re-run the goldens from python 2.7.", "Jenkins, test this please.", "Woohoo!", "Phew, thanks for all your help guys!"]}, {"number": 13328, "title": "Why use tensorflow1.3.0GPU training to get the validation loss is much higher than the CPU training", "body": "when I use tf1.3.0 CPU vesion, the validation loss is 0.54; but get 0.74 using GPU instead. I can't handle it.\r\nCudnn version is 6.0\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. We test our code before releases, so this is possibly caused by your specific case. Open a new issue with a reproducible test case if you would like more help. Thank you."]}, {"number": 13327, "title": "v1.3.0 protobuf and sha256sum does not match", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI am compiling tensorflow_cc.so using bazel, and the head was at v1.3.0, it encountered an ERROR \r\n\r\ntensorflow/BUILD:446:1: error loading package 'tensorflow/c': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to .cache/bazel/_bazel_xiaochunlin/8e84b434cfbb634410b719fa2fe8ff20/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow:libtensorflow_cc.so'.\r\n\r\nI manually downloaded the protobuf with the given link in tensorflow/workspace.bzl , https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz , and the sha256sum indeed is e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d. So I guess the sha256sum and the given version protobuf don't match, can anyone verify this ??\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I had the same problem ; I swapped the order of the pull in tensorflow/workspace.bzl for the 3 instances of:\r\n  wget https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz\r\n wget http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz\r\n\r\nthe mirror,bazel.build had the correct sha256sum but was second in the list. so swapping the order worked for me.  \r\n", "oops I ignore the wget in the above comment.", "I have the same problem. I solved it by manually changing the sha265 sum in build directory. It's not the best solution, but it does the job, until the issue is resolved. ", "This looks like a dup of #12979 "]}, {"number": 13326, "title": "added scope filtering to summaries.merge_all", "body": "I found a use for filtering the ops returned by `merge_all by scope`, so this request implements that change. You could also pass the collection to `merge`, but I thought this was a bit nicer.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "You will have to rebuild the golden files, since this is an API change. See the failed logs for details.", "Jenkins, test this please.", "ping @jart ", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I fixed the conflict so we can move this along (or not, depending on @jart's verdict).", "@m-smith, can you make sure all commits in this PR have the correct email attached (the same one that was used in the CLA)? I need that to be able to merge this.", "I think that all my commits already use the same email that I used for the CLA!", "@m-smith the first commit 30350cdc5785df745436d65175fb136676dd2a40 on Sep 26 appears to have failed the CLA. Could you check again?", "I'm pretty much 100% sure that I used the same email for all my commits! The one causing an issue has the wrong user name I think. But I'd have to rewrite the whole history in order to fix this...should I just redo the changes or is there a way to force the CLA?", "Sorry, I did a merge for you. We can override the CLA, it's confused by this.", "@martinwicke @gunan Seems ready to merge. Please override the CLA.", "thanks!"]}, {"number": 13325, "title": "Give accumulate_n op a gradient (version 2)", "body": "This pull request is a restructuring of the changes in https://github.com/tensorflow/tensorflow/pull/13022 to address high-level comments. Two major changes to call out vis a vis the previous PR:\r\n* Instead of replacing `accumulate_n`, this PR now adds a new op, `accumulate_n_v2`. This new op is defined under `contrib/framework`.\r\n* Refactoring changes have been broken out into a separate PR, https://github.com/tensorflow/tensorflow/pull/13159\r\n\r\nOverall, this pull request addresses issue #10607 by adding a gradient to the existing `accumulate_n` operator. I followed the approach suggested by @alextp: rewrite `accumulate_n` as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.\r\n\r\n**Implementation Details**\r\nI have added a new C++ op, `AccumulateNV2`, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in `accumulate_n_optimizer.cc`, replaces this placeholder with a group of `AssignAdd` ops and some additional ops that create, initialize, and destroy temporary variables.\r\n\r\nI wrote a new Python wrapper function `accumulate_n_v2` that has the same signature as the original `accumulate_n` function. Unlike the original, `accumulate_n_v2` only validates its arguments and creates an instance of the `AccumulateNV2` placeholder op.\r\n\r\n**Testing**\r\nI added a more complete set of tests for `accumulate_n` in a previous pull request (https://github.com/tensorflow/tensorflow/pull/12196) to ensure that the op would still be correct after the changes in the current pull request. I copied all of the existing tests for `accumulate_n` into a test suite for the new `accumulate_n_v2` op (see tensorflow/contrib/framework/python/ops/accumulate_n_v2_test.py). I also added one additional test to verify that `accumulate_n_v2` now has a gradient. All the tests under `//tensorflow/python/...` currently pass on my MacOS and Linux test machines.\r\n\r\n**Things to Note**\r\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to `accumulate_n` to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the `shape` argument to the `accumulate_n` function.\r\n\r\nI had to put the graph rewrite code into the main build to get the rewrite to compile and run properly. It looks like some part of the API for adding an optimizer pass instantiates some static global data structures and can't be called from a dynamically-linked library.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Reassigning to Alex who has more context from reviewing the previous change.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13324, "title": "[Windows] Export nsync symbols from the Python extension DLL", "body": "Some extension modules (e.g. `_gru_ops.dll`) expect to be able to link against `nsync` symbols in the Python extension DLL. Any use of `tensorflow::mutex` in an extension module will rely on these symbols being exported. However, these are not currently exported as part of the Windows build.\r\n\r\nThis change modifies the Windows build to export symbols containing `nsync_` explicitly.", "comments": ["http://ci.tensorflow.org/view/Nightly/job/nightly-win/325/", "@tensorflow-jenkins test this please.", "@av8ramit Based on running equivalent tests on the CPU build (adding a `tensorflow::mutex` to one of the CPU-only contrib libraries) I think the most recent commit to this PR will get us past the nsync-related error in the GPU build.\r\n\r\nWould you mind restarting a GPU build to see if this does fix the problem (or to see what's the next breakage to fix :) ...)? ", "http://ci.tensorflow.org/view/Nightly/job/nightly-win/327/", "We are successfully running tests now! I'll approve and merge."]}, {"number": 13323, "title": "Branch 170099939", "body": "", "comments": ["Jenkins, test this please.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 13322, "title": "<DO NOT MERGE> experimental PR", "body": "", "comments": []}, {"number": 13321, "title": "Disable broken MaxPool<qint8> and MaxPoolV2<qint8> kernels on Windows.", "body": "This is intended to unbreak the Windows GPU build (#13065).", "comments": ["http://ci.tensorflow.org/view/Nightly/job/nightly-win/323/", "And now we wait....", "So it seems that this fix fixes the original issue, but now we have a similar one with a new error signature. Merging for now.", "Thanks! I've sent #13324, with a potential fix for the newly uncovered issue. "]}]