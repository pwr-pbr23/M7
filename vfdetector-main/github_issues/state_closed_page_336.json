[{"number": 44068, "title": "tf dataset sample_weight is not passed to custom metrics (or any metrics).", "body": "**System information**\r\n- v2.3.0-0-gb36436b087 2.3.0\r\n- code is tested on google colab\r\n\r\n**Describe the current behavior**\r\nsample_weight as defined in tf.data.Dataset (not passed explicitly in model.fit(...)) is not being called with \r\nany metrics' update_state(self, y_true, y_pred, sample_weight). \r\n\r\n**Describe the expected behavior**\r\nsample_weight should be passed to update_state of metrics.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab Notebook\r\nhttps://colab.research.google.com/drive/1eSVeRuUv0q_KbSHQ2C4AFSIep_HIt1mn?usp=sharing\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nx = np.random.randn(10, 1).astype('float32')\r\ny = np.random.randn(10, 1).astype('float32')\r\nw = np.random.randint(0, 2, (10, 1)).astype('float32')   # sample weight\r\n\r\nds = tf.data.Dataset.from_tensor_slices((x, y, w)).batch(2)\r\n\r\nmodel = Sequential()   # simple linear regression\r\nmodel.add(Dense(1))\r\n\r\n# Custom metrics, we will just sum up the sample_weight\r\n\r\nclass TestSampleWeight(tf.keras.metrics.Metric):\r\n  def __init__(self, name=\"test_sample_weight\", **kwargs):\r\n    super(TestSampleWeight, self).__init__(name=name, **kwargs)\r\n    self.sample_weight_sum = self.add_weight(name=\"sws\", initializer=\"zeros\")\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\r\n    values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\r\n    values = tf.cast(values, \"float32\")\r\n    \r\n    # self.sample_weight_sum.assign_add(1)    # Uncomment to simple sanity test for metrics, comment out next 2 lines.\r\n\r\n    sample_weight = tf.cast(sample_weight, \"float32\")\r\n    self.sample_weight_sum.assign_add(sample_weight)\r\n\r\n  def result(self):\r\n    return self.sample_weight_sum\r\n\r\n  def reset_states(self):\r\n    self.sample_weight_sum.assign(0.0)\r\n\r\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=TestSampleWeight())\r\n\r\nmodel.fit(ds)  # hit ValueError: None values not supported. sample_weight is None?? \r\n```\r\n\r\n**Other info / logs**\r\n\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f9624842a1aca1df03a7003f419381e2/44068.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/f9b3d4319ed0f5888d65849aadced529/44068-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@amahendrakar Did you attach any gist yet? I actually am also able to reproduce this without using tf.data.Dataset, just using plain good old np.array passed into .fit() as x, y, and sample_weight. I am somewhat surprised this isn\u2019t raised, unless it is a recent regression.", "This may not be a bug. I should have used \"weighted_metrics\" instead of \"metrics\" in compile"]}, {"number": 44067, "title": "Expensive decompression in JPEG GetImageInfo", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nExtractJpegShape is as expensive to evaluate as full JPEG decompression on non-baseline JPEGs.\r\n**Describe the expected behavior**\r\nExtractJpegShape is much cheaper than full JPEG decompression.\r\n\r\n**Fix and Code To Reproduce**\r\nhttps://github.com/tensorflow/tensorflow/pull/44066\r\n", "comments": ["Fix merged in #44066.\r\n\r\nClosing."]}, {"number": 44066, "title": "Fix expensive decompression in JPEG GetImageInfo", "body": "The current implementation of GetImageInfo starts a libjpeg decompression pass over input images. Certain images (e.g., progressive JPEG) trigger a full image decompression, resulting in a performance degradation. This commit switches to an equivalent but cheaper libjpeg call for evaluating image dimensions.\r\n\r\nThe end result is performance is currently slowed down on these images by over **50x** when using ExtractJpegShape.\r\n\r\nTo reproduce, download input image as 'test_img.jpg':\r\n![test_img](https://user-images.githubusercontent.com/12423239/96190717-ac53d280-0f10-11eb-9bb9-ee1c3301c49e.jpg)\r\n\r\nConvert it to progressive jpeg:\r\n`jpegtran -progressive test_img.jpg > test_img_progressive.jpg`\r\n\r\nAnd benchmark using test_img_progressive.jpg with `use_shape=False` and `use_shape=True`. On my machine, the first finishes in 0.12 seconds and the second in 8.85 seconds (roughly 70x performance degradation).\r\n\r\n```python3\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport timeit\r\n\r\n# Inputs\r\n#filenames = [\"test_img.jpg\"]\r\nfilenames = [\"test_img_progressive.jpg\"]\r\n#use_shape = False\r\nuse_shape = True\r\n# End Inputs\r\n\r\ndef read_path(file_path):\r\n    img = tf.io.read_file(file_path)\r\n    return img\r\n\r\ndef jpeg_to_shape(image_buffer):\r\n  shape = tf.image.extract_jpeg_shape(image_buffer)\r\n  return shape\r\n\r\ndef fake_jpeg_to_shape(image_buffer):\r\n  \"\"\"Pretend we read shape using extract jpeg shape\"\"\"\r\n  shape = np.array([1920, 1080, 3], dtype=np.int32)\r\n  return shape\r\n\r\ndef create_dataset():\r\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\r\n    dataset = dataset.map(read_path)\r\n    dataset = dataset.cache()\r\n    if use_shape:\r\n        dataset = dataset.map(jpeg_to_shape)\r\n    else:\r\n        dataset = dataset.map(fake_jpeg_to_shape)\r\n    return dataset\r\n\r\ndef noop():\r\n    return None\r\n\r\ndef run_loop(dataset):\r\n    for x in dataset:\r\n        noop()\r\n\r\ndef main():\r\n    dataset = create_dataset()\r\n    dataset = dataset.repeat(1000) # run 1e3 times\r\n    timeit_results = timeit.timeit(lambda: run_loop(dataset),\r\n                                   number=1\r\n                                   )\r\n    print(\"Elapsed time:\\n{}\".format(timeit_results))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nInspecting profiling results, the _data decoding_ part of the decompression pass is being performed on the JPEG. In other words, the JPEG is being decompressed at near the full cost of decompression rather than simply extracting the shape (a _metadata_ operation).\r\n![get_shape_profiler](https://user-images.githubusercontent.com/12423239/96191390-fc7f6480-0f11-11eb-9943-4fd9d7fe70a0.png)\r\n\r\nThis PR replaces the `jpeg_start_decompress` call with a call to `jpeg_calc_output_dimensions`, which fills out the required `cinfo` fields without decompressing the data.\r\n", "comments": []}, {"number": 44065, "title": "Input shape for convolution1D", "body": "I have sparse coefficients array of shape 1D. Each value in the 1D array corresponds to one class. For example, I just have an array \r\na = [1,2,3,4,4,5,6,78,8543,35,878,.............]\r\nclass_label = [1,1,1,2,2,3,3,1,2,......]\r\n\r\nHow should I input this to Convolution1D? It always gives me an error of input dimensions.", "comments": ["Could you please answer this?", "@gharshini Can you please share a standalone code to reproduce the issue? The example data you mentioned above is one dataset (x (featurers), y(labels))? Thanks!", "model = Sequential()\r\nmodel.add(Convolution1D(filters=32, kernel_size=1,strides = 3, input_shape=(1886,)))\r\n\r\nI gave an input shape as above. I got the below error:\r\n\r\nValueError: Input 0 of layer conv1d_1 is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: [None, 1886]\r\n\r\n", "The data that I have is a sparse coefficient data that I got from the sparse representation of EEG signals. My task is to classify the sparse coefficients. They are 1D data. Please go through the link below:\r\n\r\nhttps://www.frontiersin.org/files/Articles/567151/fnins-14-00808-HTML-r2/image_m/fnins-14-00808-g003.jpg\r\n\r\nMy architecture is similar to the link. I am stuck with the input shape of conv1d for 1D array.", "@gharshini From [Keras website](https://keras.io/api/layers/convolution_layers/convolution1d/),\r\n\r\n> When using this `Convolution1D` layer as the first layer in a model, provide an input_shape argument (tuple of integers or None, e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, or (None, 128) for variable-length sequences of 128-dimensional vectors.\r\n\r\nI had updated the input_shape as shown below and everything works as expected.\r\n`model.add(Convolution1D(filters=32, kernel_size=1,strides = 3, input_shape=(1,1886)))`\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3ce64184332aa2fbe503ba5f5c1e2f75/untitled.ipynb). Thanks!\r\n\r\nI am closing this issue as this is resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 44064, "title": "[Intel MKL] Fixing a bug in Convolution+Add fusion", "body": "This PR fixes a bug in conv+add fusion by correctly copying input tensor(summand) to output tensor for proper in-place computation. ", "comments": ["Thank you for the comments. I have fixed the issues."]}, {"number": 44063, "title": "Fix a TF Lite issue when loading a saved TF Lite model on platforms with different endianness", "body": "In the current TensorFlow Lite mechanism, the saved `.tflite` model will not have any information indicating the endianness of the serialized model, which might cause some op tensors not being read correctly if loading the model on a machine with different endianness. Here is a related [issue](https://github.com/tensorflow/tensorflow/issues/28807).\r\n\r\nThe issue is also causing two test cases failure for TensorFlow Serving: `//tensorflow_serving/servables/tensorflow:saved_model_bundle_factory_test` and `//tensorflow_serving/servables/tensorflow:tflite_session_test`. Because these two test cases will load a pre-generated `model.tflite` file (generated on little endian), so they fail when testing on big endian machines.\r\n\r\nA similar issue is resolved for a regular TensorFlow model, as the endianness information is written alongside the model itself, see [here](https://github.com/tensorflow/tensorflow/blob/1492da4bae85410a9ce5b896b871de2fb930ba06/tensorflow/core/util/tensor_bundle/tensor_bundle.cc#L546). And such endianness info will be read on loading, and byte-swapping will be conducted if the endianness of the model is different from the endianness of the host, see [here](https://github.com/tensorflow/tensorflow/blob/1492da4bae85410a9ce5b896b871de2fb930ba06/tensorflow/core/util/tensor_bundle/tensor_bundle.cc#L793).\r\n\r\nConsequently, I am using a similar approach to saved `.tflite` models. I make use of the `Metadata` field in the [schema](https://github.com/tensorflow/tensorflow/blob/1492da4bae85410a9ce5b896b871de2fb930ba06/tensorflow/lite/schema/schema.fbs#L1073) of saved TF Lite Model, saving a string type metadata indicating `little` or `big` of the machine endianness where the model is saved.\r\n\r\nThis piece of metadata will be read when the `FlatBufferModel` is passed into an `InterpreterBuilder`, and the byte-swapping will be conducted during `ParseTensors`. Right now I am only supporting the byte-swap of float type tensors, as they are the most common ones. It is also possible to add support for more data types if needed.\r\n\r\nPlease also note that this PR will not cause any test cases regression for TensorFlow, and it is also compatible with TF Lite models generated before because if the `Metadata` field is not found in the saved model, the loader will assume the model is generated with the same host endianness (which is basically what is happening without the changes). And after the changes, even the `model.tflite` file generated on a little-endian machine will make the TensorFlow Serving test case pass on a big-endian machine (vice versa).\r\n\r\nThis PR is also including a small fix for aws building target on s390x arch.", "comments": ["Thank you @Sidong-Wei for your contribution!\r\n\r\nThis is indeed a valid concern and the issue should be carefully resolved for extending TFLite use cases beyonds mobile platforms.\r\n\r\nThere are multiple designs to resolve handling the endian problem, for example,\r\n1) Adding model metadata in flatbuffer like this one,\r\n2) Writing always tensor data with the certain endianness, or\r\n3) Adding a flag for specifying the target endianness during conversion.\r\n\r\nI prefer a solution that is the mixture of the 1) + 3) options.\r\n\r\n@jdduke Any thoughts on this?", "I believe flatbuffers default to little-endian encoding, even on big-endian host machines that create the flatbuffer. See also [this documentation](https://github.com/dvidelabs/flatcc/blob/master/doc/binary-format.md#big-endian-flatbuffers). That said, having some safeguards in place to ensure that (when we create the flatbuffer), and handling big-endian parsing/loading on the inference side, seem like reasonable requests.\r\n\r\nSo, given the gravity of flatbuffers for little-endian encoding, my preference would be for (2), and to fix the loading code to handle the big-endian case. The unfortunate side effect is that this will eliminate the benefits of mmap'ing the model file on big-endian machines. If that's a problem, we can discuss, perhaps with a special build flag to opt in to this path (where you know that both host/device are big-endian, I think we want to avoid the proliferation of big-endian-encoded .tflite models more broadly).", "Thanks a lot for the feedback. And yes, I think always serializing the flatbuffer model in Little-Endian makes sense, as you mentioned, the faltbuffer compiler seems to use this method by default. I think the problem here is that for some non-native data types like `Tensor`, flatbuffer compiler may not be able to handle it correctly, so some extra efforts on both the saving side and loading side will need to be made (i.e. manually do the byte-swap on Big-Endian machine).\r\n\r\nMy colleague @skribm9, is actually working on such a method as you suggested. I think it should be able to address the endianness issue while keeping the saved model in Little-Endian format.", "Closing the PR as the discussion has moved to the new issue: https://github.com/tensorflow/tensorflow/issues/45009, will start new PR once the issue is fixed."]}, {"number": 44062, "title": "tflite model maker export error, attribute ModelMetadataT not found", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nFollowing exactly the stock example https://www.tensorflow.org/lite/tutorials/model_maker_image_classification\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.3.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nTrying to run the example, everything goes fine and the model trains successfully, however when trying to finally export\r\nthe model\r\nmodel.export(export_dir='.')\r\nI get this error\r\nAttributeError: module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-4c3d25dac6cc> in <module>\r\n----> 1 model.export(export_dir='.')\r\n\r\n~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\image_classifier.py in export(self, export_dir, tflite_filename, label_filename, saved_model_filename, export_format, **kwargs)\r\n    305       **kwargs: Other parameters like `quantized` for TFLITE model.\r\n    306     \"\"\"\r\n--> 307     super(ImageClassifier, self).export(\r\n    308         export_dir,\r\n    309         tflite_filename=tflite_filename,\r\n\r\n~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\custom_model.py in export(self, export_dir, tflite_filename, label_filename, vocab_filename, saved_model_filename, export_format, **kwargs)\r\n    161       with_metadata = kwargs.get('with_metadata', True)\r\n    162       tflite_filepath = os.path.join(export_dir, tflite_filename)\r\n--> 163       self._export_tflite(tflite_filepath, **kwargs)\r\n    164     else:\r\n    165       with_metadata = False\r\n\r\n~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\image_classifier.py in _export_tflite(self, tflite_filepath, quantization_config, with_metadata, export_metadata_json_file)\r\n    344         populator = metadata_writer.MetadataPopulatorForImageClassifier(\r\n    345             tflite_filepath, model_info, label_filepath)\r\n--> 346         populator.populate()\r\n    347 \r\n    348       # Validate the output model file by reading the metadata and produce\r\n\r\n~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\metadata_writer_for_image_classifier.py in populate(self)\r\n     87   def populate(self):\r\n     88     \"\"\"Creates metadata and then populates it for an image classifier.\"\"\"\r\n---> 89     self._create_metadata()\r\n     90     self._populate_metadata()\r\n     91 \r\n\r\n~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow_examples\\lite\\model_maker\\core\\task\\metadata_writer_for_image_classifier.py in _create_metadata(self)\r\n     94 \r\n     95     # Creates model info.\r\n---> 96     model_meta = _metadata_fb.ModelMetadataT()\r\n     97     model_meta.name = self.model_info.name\r\n     98     model_meta.description = (\"Identify the most prominent object in the \"\r\n\r\nAttributeError: module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'", "comments": ["@shilan,\r\nI was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5d01785804ac2082a277421e3e421184/44062.ipynb).\r\n\r\nCould you please try running the code in a virtual environment and check if you are facing the same issue? Thanks!", "Also, could you please let us know if you have installed Python from the Microsoft store or from [python.org](https://www.python.org/downloads/windows/)? Thanks!", "I have tested and It works for python3.8.5 using conda virtual environment. Could you please check whether the version of `tflite-support` pip package is `0.1.0rc3.dev2` by `pip list | grep tflite-support`? \r\n\r\nMeanwhile, as for [PR](https://github.com/tensorflow/tensorflow/issues/43986), we should have fixed it. \r\n\r\nBTW, if you didn't use metadata when deploying the tflite model, you could set `with_metadata=False` like:\r\n```\r\nmodel.export(export_dir='.', with_metadata=False)\r\n```\r\nThis could  also avoid this bug.", "Hi, could you please uninstall tflite-support and install `0.1.0rc3.dev3` instead? There was a bug in tflite-support on Windows for previous version.\r\n```\r\npip install tflite-support==0.1.0rc3.dev3\r\n```", "many thanks :) updating the tflite-support version worked like a charm :)\r\nHowever I don't understand now, where is my tflite file now? Why do I get warnings?\r\n\r\nWARNING:tensorflow:From C:\\Users\\Chingool\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From C:\\Users\\Chingool\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From C:\\Users\\Chingool\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From C:\\Users\\Chingool\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: C:\\Users\\Chingool\\AppData\\Local\\Temp\\tmpa90j264d\\assets\r\nINFO:tensorflow:Assets written to: C:\\Users\\Chingool\\AppData\\Local\\Temp\\tmpa90j264d\\assets\r\nINFO:tensorflow:Label file is inside the TFLite model with metadata.\r\nINFO:tensorflow:Label file is inside the TFLite model with metadata.\r\nINFO:tensorflow:Saving labels in C:\\Users\\Chingool\\AppData\\Local\\Temp\\tmpswhd3rra\\labels.txt.\r\nINFO:tensorflow:Saving labels in C:\\Users\\Chingool\\AppData\\Local\\Temp\\tmpswhd3rra\\labels.txt.", "I found the model here, pretty weird place though:\r\nC:\\Users\\Chingool\\_bazel_Chingool\\gmciwitk\\external\\org_tensorflow\\bazel-machine-learning\\tensorflow\r\nI assume I can give it a better directory if I change the argument of this method model.export(export_dir='.'), right?", "> Hi, could you please uninstall tflite-support and install `0.1.0rc3.dev3` instead? There was a bug in tflite-support on Windows for previous version.\r\n> \r\n> ```\r\n> pip install tflite-support==0.1.0rc3.dev3\r\n> ```\r\n\r\nI tried to update, but I got this error:\r\n\r\nERROR: Could not find a version that satisfies the requirement tflite-support==0.1.0rc3.dev3 (from versions: 0.1.0a0.dev3, 0.1.0a0.dev4, 0.1.0a0.dev5, 0.1.0a0, 0.1.0a1, 0.1.0rc3.dev2)\r\nERROR: No matching distribution found for tflite-support==0.1.0rc3.dev3", "@Fresh-Broccoli,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 44060, "title": "[TfLite] gpu gl delegate: reduce_mean([1,2]) returns average of first row only ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nGiven a simple model that computes the average intensity of a square, grayscale image:\r\n\r\n```python\r\ninput = Input(shape=(16,16), batch_size=1, dtype=tf.float32)\r\noutput = tf.math.reduce_mean(input, axis = [1, 2])\r\nmodel = Model(inputs=input, outputs=output, name='mean')\r\n```\r\n\r\nwhen no delegate is used, the average is correct, but when the GPU delegate is used with a SSBO,\r\nthe returned value is the average of the first row only, not that of the whole image as it should be. \r\nI have tested thoroughly that the SSBO values are correct by reading them with glMapBufferRange.\r\nThe SSBO array has exactly 16x16 values and the result is always the average of pixels [0, 16, 32, ...].\r\n\r\nI can't see issues with lite/delegates/gpu/gl/kernels/mean.h , so I think the wrong dimension is being picked for some reason.\r\n\r\n**Describe the expected behavior**\r\n\r\n`reduce_mean` should return the average along the two dimensions.\r\n\r\n**Other info / logs** \r\n\r\nTo enable SSBOs I am using a slightly modified version of TFLite 2.2.0 (https://github.com/natario1/tensorflow/commit/7401fbb4fa0c94004865c089d8c89bdd566ad747) and passing in this object def:\r\n\r\n```cpp\r\ntflite::gpu::ObjectDef get_gpu_object_def() {\r\n    tflite::gpu::ObjectDef object_def;\r\n    object_def.data_type = tflite::gpu::DataType::FLOAT32;\r\n    object_def.data_layout = tflite::gpu::DataLayout::BHWC;\r\n    object_def.object_type = tflite::gpu::ObjectType::OPENGL_SSBO;\r\n    object_def.user_provided = true;\r\n    return object_def;\r\n}\r\n```\r\n\r\nNote that I use this same logic to run much more complex models (like MediaPipe models) and it works well, so I'm tempted to say that this is an issue with reduce_mean or with my test model.\r\n\r\n[simple_model_16x16b.tflite.zip](https://github.com/tensorflow/tensorflow/files/5387188/simple_model_16x16b.tflite.zip)\r\n\r\n**More info after investigation**\r\n\r\nI found out that the issue is fixed if I explicitly add the channel dimension to the model, so 1x16x16x1 instead of 1x16x16.\r\nThis is not consistent with the CPU behavior (delegate off, no SSBOs, only CPU data), where a 1x16x16 model works just fine.\r\n\r\nAlso it's not clear why, if the last 1 is absent, the dimensions are misinterpreted in such a way that leads `reduce_mean` to only average the first row. I could understand this if we were *adding* an extra dimension equal to 1 in the wrong place (which could be taken as width or height), but in this case we are actually *removing* it.", "comments": ["@natario1 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.6.0 and let us know if the issue still persists? Please refer to the [link](https://www.tensorflow.org/lite/performance/delegates) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44060\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44060\">No</a>\n"]}, {"number": 44059, "title": "NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array.", "body": "**System information**\r\n- This is custom code\r\n- Running Google Colab on Mac\r\n- TensorFlow version 2.3.0\r\n- Python version 3.6.9\r\n- XLA_GPU hosted by Colab; memory_limit = 15695549568\r\n\r\n**Describe the current behavior**\r\nI have a custom augmentation function written in Colab that worked normally until today. The last time I ran the entire code through was 09/08/2020 and the augmentation functioned performed the operations normally. Now, I receive an error that pertains to symbolic tensors, which I have never seen before.\r\n\r\n**Describe the expected behavior**\r\nAugmentation function is meant to map over a batch of images and masks, taking in samples one at a time, converting them to NumPy arrays, performing some sort of augmentation, then returning them back as tensors.\r\n\r\n**Standalone code to reproduce the issue**\r\nA link to the Colab notebook: https://colab.research.google.com/drive/1ujShezPjcveG2kg019c7zqweLtu1ESuW?usp=sharing\r\nA link to the training data I use in the notebook: https://drive.google.com/drive/folders/1ZS1wKjo692Lg7Vuhtr3akz0sYSOmZBKl\r\n\r\nThe particular section of code where the error stems from:\r\n```\r\n# Function used to perform \"on-the-fly\" augmentation during training.\r\n# UPDATED ON 09/21/2020.\r\n\r\ndef augmentation(img, msk):\r\n\r\n  # Call in skimage package, which will be used for transformations.\r\n  from skimage.transform import rotate, AffineTransform, warp\r\n  \r\n  # Create some random floats, which will be used in augmentation steps.\r\n  tilt = tf.random.uniform(shape = [], minval = -90, maxval = 90, dtype = tf.float32)\r\n  dx = tf.random.uniform(shape = [], minval = -20, maxval = 20, dtype = tf.float32)\r\n  dy = tf.random.uniform(shape = [], minval = -20, maxval = 20, dtype = tf.float32)\r\n  \r\n  # Cast image and mask to numpy arrays.\r\n  img = np.array(img)\r\n  msk = np.array(msk)\r\n\r\n  # Use TensforFlow-style if conditionals, used to flip image and mask.\r\n  img = tf.cond(tilt > 0, lambda: np.fliplr(img), lambda: np.flipud(img))\r\n  msk = tf.cond(tilt > 0, lambda: np.fliplr(msk), lambda: np.flipud(msk))\r\n\r\n  # Rotate the image and mask to some degree.\r\n  img = rotate(img, angle = tilt, mode = 'reflect')\r\n  msk = rotate(msk, angle = tilt, mode = 'reflect')\r\n\r\n  # Write the conditions for an affine transformation.\r\n  transform = AffineTransform(translation = (dx,dy))\r\n\r\n  # Perform the affine transformation.\r\n  img = warp(img, inverse_map = transform, mode = 'reflect')\r\n  msk = warp(msk, inverse_map = transform, mode = 'reflect')\r\n \r\n  # Convert the inputs back into tensors, put back into a tuple.\r\n  finalTuple = (tf.convert_to_tensor(img), tf.convert_to_tensor(msk))\r\n\r\n  return finalTuple\r\n\r\n# Callback for data augmentation.\r\nclass aug(tf.keras.callbacks.Callback):\r\n  def on_training_batch_begin(self, batch, logs = None):\r\n    batch.map(augmentation, num_parallel_calls = 5)\r\n    batch.shuffle(10)\r\n    \r\n# Callback for CSV logger (used for charting).\r\ncsv = tf.keras.callbacks.CSVLogger(f'/content/gdrive/My Drive/{today}_metrics.csv', separator=',', append=False)\r\n\r\n# Callback for saving the model.\r\nsave_model_path = f'/content/gdrive/My Drive/{today}_wellpad_model_.h5'\r\ncp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path,\r\n                                        monitor='val_loss',\r\n                                        mode='min',\r\n                                        save_best_only=True)\r\n```\r\n```\r\n# Test the augmentation function on a batch of images.\r\ntest_batch = evaluation.take(1)\r\n\r\ntest_batch = test_batch.map(augmentation,num_parallel_calls=5) # ERROR OCCURS HERE.\r\n\r\n# For plotting below.\r\ntest_batch = [(image,mask) for image, mask in test_batch]\r\n\r\nplt.figure(figsize=(10,10))\r\nplt.subplot(1,2,1)\r\n# Show the original image.\r\nplt.imshow(test_batch[0][0][0])\r\n# Show the masked image.\r\nplt.subplot(1,2,2)\r\nplt.imshow(tf.squeeze(test_batch[0][1][0]))\r\n```\r\n\r\n**Other info / logs** \r\nHere is the full error I receive:\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-17-81c1086f76f0> in <module>()\r\n      2 test_batch = evaluation.take(1)\r\n      3 \r\n----> 4 test_batch = test_batch.map(augmentation,num_parallel_calls=5)\r\n      5 \r\n      6 test_batch = [(image,mask) for image, mask in test_batch]\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)\r\n   1700           num_parallel_calls,\r\n   1701           deterministic,\r\n-> 1702           preserve_cardinality=True)\r\n   1703 \r\n   1704   def flat_map(self, map_func):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\r\n   4082         self._transformation_name(),\r\n   4083         dataset=input_dataset,\r\n-> 4084         use_legacy_function=use_legacy_function)\r\n   4085     if deterministic is None:\r\n   4086       self._deterministic = \"default\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\r\n   3369       with tracking.resource_tracker_scope(resource_tracker):\r\n   3370         # TODO(b/141462134): Switch to using garbage collection.\r\n-> 3371         self._function = wrapper_fn.get_concrete_function()\r\n   3372         if add_to_graph:\r\n   3373           self._function.add_to_graph(ops.get_default_graph())\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)\r\n   2937     \"\"\"\r\n   2938     graph_function = self._get_concrete_function_garbage_collected(\r\n-> 2939         *args, **kwargs)\r\n   2940     graph_function._garbage_collector.release()  # pylint: disable=protected-access\r\n   2941     return graph_function\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n   2904       args, kwargs = None, None\r\n   2905     with self._lock:\r\n-> 2906       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2907       seen_names = set()\r\n   2908       captured = object_identity.ObjectIdentitySet(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_fn(*args)\r\n   3362           attributes=defun_kwargs)\r\n   3363       def wrapper_fn(*args):  # pylint: disable=missing-docstring\r\n-> 3364         ret = _wrapper_helper(*args)\r\n   3365         ret = structure.to_tensor_list(self._output_structure, ret)\r\n   3366         return [ops.convert_to_tensor(t) for t in ret]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in _wrapper_helper(*args)\r\n   3297         nested_args = (nested_args,)\r\n   3298 \r\n-> 3299       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n   3300       # If `func` returns a list of tensors, `nest.flatten()` and\r\n   3301       # `ops.convert_to_tensor()` would conspire to attempt to stack\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    256       except Exception as e:  # pylint:disable=broad-except\r\n    257         if hasattr(e, 'ag_error_metadata'):\r\n--> 258           raise e.ag_error_metadata.to_exception(e)\r\n    259         else:\r\n    260           raise\r\n\r\nNotImplementedError: in user code:\r\n\r\n    <ipython-input-16-3d72fc8870c2>:15 augmentation  *\r\n        img = np.array(img)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:848 __array__  **\r\n        \" a NumPy call, which is not supported\".format(self.name))\r\n\r\n    NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/99e132a15671c712f6e7592f7a608cb7/44059-tf-nightly.ipynb). Thanks!", "@bjarrell15 What version of tensorflow were you using before?", "@gowthamkpr I believe that it was 2.3.0 previously as well. ", "The functions passed to APIs like `Dataset.map` operate in graph mode (it's as if you decorated `augment` with `@tf.function`). In graph mode, you can typically only convert NumPy arrays to Tenors, not vice-versa.\r\n\r\nThere are a couple of ways to make this work:\r\n\r\n 1. if you can write `augmentation` entirely using TF ops, that would be a solution; [tf.image](https://www.tensorflow.org/api_docs/python/tf/image) and [tfa.image (TF Addons)](https://www.tensorflow.org/addons/api_docs/python/tfa/image) can probably be of help there, and I believe it has all the ops for standard augmentations\r\n 2. another option could be to use `tf.py_function`, and you can use NumPy inside that; it's not as fast as 1, and has certain limitation, so I only recommend it as last resort\r\n", "Side note - `Dataset.map` uses autograph, so you can write the conditionals like so:\r\n\r\n```\r\nif tilt > 0:\r\n  img = tf.image_flip_left_right(img)\r\n  msk = tf.image_flip_left_right(msk)\r\nelse:\r\n  img = tf.image_flip_up_down(img)\r\n  msk = tf.image_flip_up_down(msk)\r\n```\r\n", "@mdanatg Thank you for your suggestions! I have used tf.image and tfa.image augmentations in the past, but found them to be limiting. I have not used tf.py_function, so I will give that a look.\r\n\r\nThe fundamental issue is that this **did** work, but now there is (potentially) some sort of TF-side bug which is preventing the operation from working.", "@bjarrell15,\r\nCan you please elaborate the comment,\r\n\r\n> The fundamental issue is that this did work, but now there is (potentially) some sort of TF-side bug which is preventing the operation from working.\r\n\r\nDo you mean to say that it is working, or is it resulting in any other error? Thanks!", "@rmothukuru When I said \"this did\", I was referring to the original augmentation function. \r\n\r\nTo reiterate, here's what happened:\r\n1.) On September 8th of this year, I used this augmentation without issue to train a model.\r\n2.) On October 15th (the day I posted this issue), the same augmentation function does not work. Nothing has been changed about it. Instead, it passes me this error: `NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported`.\r\n\r\nDid something happen so that now I cannot use this function? Was Numpy support dropped at some point?", "The most likely cause for this is that previously, either the augmentation function was receiving numpy arrays somehow, or it was executed eagerly.\r\n\r\nThe latter is impossible if the function is called from a `Dataset.map`.\r\n\r\nThe former is possible, but usually leads to extremely slow performance (it would recompile the augment function every time it was called), which is why it's so surprising to hear it worked before.\r\n\r\nMixing symbolic tensors with numpy APIs was definitely never supported, hence the suspicion that somehow a previous inefficiency got suddenly exposed.\r\n\r\nIt would be super helpful to reproduce the previous behavior. I wanted to try running the colab code with TF 2.2 or TF 2.1 to look at the pre-September8 behavior, but haven't had the chance to do it. If someone could do that, it would be very useful.", "Hey @mdanatg, were you able to look at that? If not that's fine, just wanted to check in. \r\n\r\nI guess it would behoove me to give up on this function and try something else. It's just a shame that there isn't an easier way to rotate/transform an image tensor otf, similar to flip_left_right() and flip_up_down(). That's why I'm upset that this doesn't work: the skimage package makes this super easy. The fact that it used to work pretty effectively makes it sting all the more.", "Yes, I ran the colab with TF 2.0, 2.1, 2.2 and 2.3. They all raise errors, though with slightly different messages. Are you sure last time you tried it was this exact same code?\r\n\r\n", "@mdanatg Yes, 100% certain. I must have exposed something that was eventually patched. I don't know.", "Okay, so I've modified the function to remove any reference to numpy and it's now running successfully. I'm now using tensorflow_addons, but I strongly dislike the fact that it can't fill in empty values with reflected values when I rotate it. Nonetheless, it's functional:\r\n\r\n```\r\n!pip install -U tensorflow-addons\r\nimport tensorflow_addons as tfa\r\n\r\ndef augmentation(img, msk):\r\n\r\n  # tf.math package for graph ops.\r\n  import tensorflow.math as Math\r\n\r\n  # Create some random floats, which will be used in augmentation steps.\r\n  tilt = tf.random.uniform(shape = [], minval = -30, maxval = 30, dtype = tf.float32)\r\n  dx = tf.random.uniform(shape = [], minval = -5, maxval = 5, dtype = tf.float32)\r\n  dy = tf.random.uniform(shape = [], minval = -5, maxval = 5, dtype = tf.float32)\r\n\r\n  # Use TensforFlow-style if conditionals, used to flip image and mask.\r\n  img = tf.cond(tilt > 0, lambda: tf.image.flip_left_right(img), lambda: tf.image.flip_up_down(img))\r\n  msk = tf.cond(tilt > 0, lambda: tf.image.flip_left_right(msk), lambda: tf.image.flip_up_down(msk))\r\n\r\n  # Rotate the image and mask.\r\n\r\n  # Convert \"tilt\" to radians.\r\n  toRads = Math.multiply(Math.divide(tilt,180),tf.constant(math.pi))\r\n\r\n  img = tfa.image.rotate(img, toRads)\r\n  msk = tfa.image.rotate(msk, toRads)\r\n \r\n  # Affine transformation\r\n  img = tfa.image.translate(img, [dx,dy], 'BILINEAR')\r\n  msk = tfa.image.translate(msk, [dx,dy], 'BILINEAR')\r\n\r\n  # Put back into a tuple.\r\n  finalTuple = (img, msk)\r\n\r\n  return finalTuple\r\n```", "Thanks! Can you confirm whether you had TensorFlow stable installed both on Sep 8 and Oct 15? I tried to reproduce it with the [Sep 2 build of tf-nightly](https://pypi.org/project/tf-nightly/2.4.0.dev20200902/) just to be sure, and it still raises error. So far, we could not seem to reproduce the circumstances in which the skimage version worked. \r\n\r\nThe `tensorflow_addons` version that you drafted is something that is supported better for sure. For the rotate fill values I definitely recommend filing a [bug](https://github.com/tensorflow/addons/issues), I believe authors or contributors would be interested in improving the feature parity.", "@mdanatg The way I had tensorflow brought into my script was by using the colab magic cmd %tensorflow-version 2.x. On both days that brought in version 2.3.0. Does that answer your question?\r\n\r\nI will definitely heed your advice and create a bug for that. I'll spend some time formulating my request and post it in the next week or so.\r\n\r\nThank you for your help!", "@bjarrell15,\r\nAs you would be filing a separate bug next week, can you please confirm if we can close this issue. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44059\">No</a>\n"]}, {"number": 44057, "title": "Embedding metadata is not read or displayed in TensorBoard 2.3.0", "body": "Hello, I have trained a model (with TensorFlow 2.3.1) including embeddings via `keras.layers.Embedding` and prepared label files per embedding dictionary, which I point to via the `embeddings_metadata` parameter of `keras.callbacks.TensorBoard`.  TensorBoard executes and displays the various embedding vectors in the Projector without issue, but the embeddings are not labeled per the label files.  The label files are successfully created and stored in the log directory, and I can use them manually just fine via the `Load` button in the TensorBoard interface.\r\n\r\nI debugged the `projector_plugin.py` code and determined that `_get_metadata_file_for_tensor()` is never called (verified via `prints`, debugging with `pdb`, etc.).  This method looks to be the only place in the code used to retrieve the `metadata_path` from which the labels would be read, and is apparently only accessed via the `/metadata` HTTP route.\r\n\r\nIncidentally when I add the various `Embedding` layers in Keras, the first receives the `tensor_name` `layer_with_weights-0/embeddings/.ATTRIBUTES/VARIABLE_VALUE` but the subsequent ones are `layer_with_weights-1`, `2`, etc.  I see the `keras.callbacks.TensorBoard` hard-codes them to `layer_with_weights-0` but I think this is a separate issue as I have manually corrected the `projector_config.pbtxt` file and, as I mentioned above, the `_get_metadata_file_for_tensor()` method is never called anyway so they aren't even trying to be parsed and used to label tensors.\r\n\r\nAny thoughts?  Am I missing something?\r\n\r\nThanks!\r\n\r\n\r\nTodd", "comments": ["@toddwasson \r\n\r\nThis issue is more suitable for Tensorboard repo.Please post it on [Tensorboard repo](https://github.com/tensorflow/tensorboard/issues/new/choose) from here. Thanks!", "Oops, that's what I was trying to accomplish -- thanks for pointing me in the right direction!", "In case anyone comes looking in the future, I did create [a TensorBoard repo issue](https://github.com/tensorflow/tensorboard/issues/4246).", "@toddwasson \r\n\r\nCan we close the issue here and track the issue in Tensorboard repo issue you have created. Thanks!", "Yes, let's close it.  Thanks again for your help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44057\">No</a>\n"]}, {"number": 44056, "title": "Segmentation fault (core dumped) error when using representative_dataset_gen()", "body": "I used tf 2.3 and met segmentation fault (core dumped) error when using `representative_dataset_gen()`.\r\n**Code**\r\n```\r\n    def representative_dataset_gen():\r\n      for audio in validation_fingerprints:\r\n        yield [audio]\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(flags.train_dir + '/last_model')\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n    converter.allow_custom_ops = True\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    # converter.representative_dataset = representative_dataset_gen\r\n    quant_model = converter.convert()\r\n    with open(flags.train_dir + '/quant_last_model.tflite', 'wb') as w:\r\n      w.write(quant_model)\r\n```\r\nAbove code can run without error but if I use `converter.representative_dataset = representative_dataset_gen`, it fail.\r\n\r\nThe type and shape of data are at below. The input layer size is `[Batch, 16384]`\r\n```\r\ntype(validation_fingerprints): <class 'numpy.ndarray'>\r\nshape(validation_fingerprints): (3093, 16384)\r\n```\r\n\r\nAny advice? Thanks!", "comments": ["Any suggestion please? @amahendrakar ", "@tu1258,\r\nOn running the code, I am facing an error stating `NameError: name 'flags' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f265cceae1f37434f5d281850b53d799/44056.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "@amahendrakar \r\nThanks for the reply. The code is just a demonstration. To reproduce the result you would have to import some custom file and the full process may be too complex. The `flags.train_dir + '/last_model'` is the model I saved after training. Could you find or guess the solution by the code above? I attach the model_summary below.\r\n[model_summary.txt](https://github.com/tensorflow/tensorflow/files/5393551/model_summary.txt)\r\n\r\nEdit: I modified the `representative_dataset_gen()` function and now the error is now `Model resulted in Nan value during calibration. Please make sure model results in all real-values during inference with provided dataset.Node number 37 (CONV_2D) failed to invoke.`\r\n\r\nModified version\r\n```\r\ndef representative_dataset_gen():\r\n  for x in validation_fingerprints:\r\n    x = x[np.newaxis,:]\r\n    yield [x]\r\n```\r\n\r\nI assume the generate function can generate data now but the datatype may be wrong. `x` is an array with shape (1, 16384). Could you give some advice?", "@tu1258,\r\nWithout a reproducible code sample, it would be difficult for us to debug the issue. \r\n\r\n> To reproduce the result you would have to import some custom file and the full process may be too complex.\r\n\r\nIn this case, could you please provide a minimal code snippet to mimic the error so that we can reproduce the issue on our end. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44055, "title": "what is y_train in tensorflow", "body": "what is the meaning of y_train in this line of code?\r\n`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()`", "comments": [" #44055 @studentusinggithup,  y_train is the set of labels of all the data present in x_train\r\nsimilarly, y_test  is the set of labels of the data present in x_test", "This is a question for StackOverflow as it is not related to a bug in the code / feature request."]}, {"number": 44054, "title": "Add Sub gradients", "body": "@saxenasaurabh\r\n\r\nPart of #42668", "comments": []}, {"number": 44053, "title": "TFDS: deep_weeds down", "body": "This has been ongoing for several days:\r\n\r\n\r\ntfds.load('deep_weeds', shuffle_files=True, as_supervised=True, try_gcs=True)\r\n\r\nConnectionError: HTTPSConnectionPool(host='nextcloud.qriscloud.org.au', port=443): Max retries exceeded with url: /index.php/s/a3KxPawpqkiorST/download (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7ff54e566470>: Failed to establish a new connection: [Errno 111] Connection refused',))\r\n\r\n", "comments": ["@dcpatton \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know TF version you are using.\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "https://colab.research.google.com/drive/1sibA5g9okamy4UL4rdDF_k4yg9YD7Mq7?usp=sharing", "TFDS version 2.1.0", "I have tried in colab and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0773c687f8eeca8319d47936b08f0762/untitled464.ipynb).Thanks!", "The deep weeds dataset file hosting has been updated which is causing the issue.\r\ncommit [51e3fab](https://github.com/AlexOlsen/DeepWeeds/commit/da084f62bcd1a2b0afeb8b81f1a27be3186391a1#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5)", "Closing this issue since the associated PR is now merged. Feel free to comment if have further problems and we can revisit the issue if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44053\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44053\">No</a>\n", "Seems like this issue is back. [See gist\r\n](https://colab.research.google.com/gist/nikitamaia/de13336f4908173a86a413fa13afaa01/untitled3.ipynb)"]}, {"number": 44052, "title": "full integer quantization error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source):\r\nNightly\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nThe exact command\r\ntflite_model = converter.convert()\r\n\r\n**The output from the converter invocation**\r\nException has occurred: ValueError\r\nFailed to parse the model: pybind11::init(): factory function returned nullptr.\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\nruntime error\r\nFailed to parse the model: pybind11::init(): factory function returned nullptr.\r\n\r\nthe conversion works w/o quantization AND with dynamic range quantization. BUT, when I add representive dataset for converting full integer quantization with float fallback it fails with the above error.\r\n\r\nI also use these flags:\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nsince some of the operators are not yet implemented in tflite so maybe it's related to the root cause.\r\n\r\nWould be happy to get your advice.\r\n\r\n", "comments": ["@shlomi-amitai,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "Hi,\r\nI'm sorry but can't send the source code.\r\nI will try, however, to send a snippet.\r\n\r\nShlomi ", "[model4Quant.zip](https://github.com/tensorflow/tensorflow/files/5408084/model4Quant.zip)\r\nHi,\r\nprotobuffer (.pb) model and test code attached.\r\ntf nightly version : 2.4.0.dev20201002", "Was able to reproduce the issue. \r\n\r\nSession crashes on running the code with TF v2.3. Whereas, with TF-nightly, the error changes to `<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3221034a61bfcd53444ff2a277322845/44052.ipynb). Thanks!", "I have slicing and tensor indexing in the model. when I remove them, the problem solved.\r\nbut then there is another problem. the output values are constant or zeros.\r\nmy guess is that it's related to the tf ops that are not supported by tflite.\r\nattached the tflite model.\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5438984/model.zip)\r\n\r\n\r\n", "I use these flags:\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT] # dynamic range quantization\r\n        converter.representative_dataset = representative_data_gen  # + float fallback quantization\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n", "@shlomi-amitai It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.6.0 and let us know if the issue still persists? Please refer to the similar issue [link](https://stackoverflow.com/questions/64447009/very-high-error-after-full-integer-quantization-of-a-regression-network) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44052\">No</a>\n"]}, {"number": 44051, "title": "There is no defined version as defined in Tensorflow website", "body": "\r\n![Screenshot (130)_LI](https://user-images.githubusercontent.com/53373012/96131702-28e9af80-0f17-11eb-8791-d915f4c3d1c1.jpg)\r\n![Screenshot (131)](https://user-images.githubusercontent.com/53373012/96131712-2b4c0980-0f17-11eb-83ae-33c159b83228.png)\r\n\r\nOhk, So as defined in the tensorflow gpu documentation, the tensorflow 2.3 can have a cuda version of 10.1 and cuDNN version of 7.4. But there isn't any cuDNN v7.4 for CUDA 10.1, according to the nvidia cuDNN archives. So, please update the version of cuDNN in your documentation, to the specified CUDA and Tensorflow version.\r\n\r\n## URL(s) with the issue:\r\n\r\nLink to the tensorflow gpu section:\r\nhttps://www.tensorflow.org/install/source_windows#gpu\r\n\r\n", "comments": ["It should be 7.6, not 7.4. Will fix the typo", "Edit: Note that Linux is properly updated: https://www.tensorflow.org/install/source#ubuntu", "Should be fixed by above PR"]}, {"number": 44050, "title": "Training pipeline significantly slower for epoch > 1 with caching to disk", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nCustom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCloud AI platform\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nFrom PyPi\r\n- TensorFlow version (use command below):\r\n2.2\r\n- Python version:\r\n3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: \r\nNVIDIA P100 \r\n\r\n**Describe the current behavior**\r\nMy training pipeline is significantly slower for epoch > 1\r\n\r\n```\r\nEpoch 1/100\r\n3198/3198 - 2287s - loss: 2.7595 - binary_accuracy: 0.8080\r\nEpoch 2/100\r\n3198/3198 - 3974s - loss: 0.9275 - binary_accuracy: 0.8086\r\nEpoch 3/100\r\n3198/3198 - 3562s - loss: 0.5781 - binary_accuracy: 0.8124\r\nEpoch 4/100\r\n3198/3198 - 3572s - loss: 0.5227 - binary_accuracy: 0.8064\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe TFRecords originate in GCS. I am using caching to disk, so I expect epoch > 1 to be faster than epoch=1\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n  train_data = tf.data.TFRecordDataset(train_files)\r\n  train_data = train_data.cache(os.path.join(FLAGS.cache_dir, 'train'))\r\n  train_data = train_data.map(training_parser)\r\n  train_data = train_data.prefetch(4)\r\n\r\n  val_data = None\r\n  if val_files:\r\n    val_data = tf.data.TFRecordDataset(val_files)\r\n    val_data = val_data.cache(os.path.join(FLAGS.cache_dir, 'val'))\r\n    val_data = val_data.map(validation_parser)\r\n    val_data = val_data.prefetch(4)\r\n\r\n  model.fit(\r\n      x=train_data,\r\n      validation_data=val_data,\r\n      epochs=FLAGS.epochs,\r\n      callbacks=[tensorboard_callback,\r\n                 _HyperparmeterTuningCallback()],\r\n      verbose=2)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sadeel \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44050\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44050\">No</a>\n"]}, {"number": 44049, "title": "Keras EarlyStopping restore_best_weights docstring", "body": "The current description:\r\n```\r\nrestore_best_weights: Whether to restore model weights from\r\n        the epoch with the best value of the monitored quantity.\r\n        If False, the model weights obtained at the last step of\r\n        training are used.\r\n```\r\nIs ambiguous as `Whether to restore model weights from the epoch with the best value of the monitored quantity.` can mean either:\r\n- `Whether to restore model weights from the epoch, with the best value of the monitored quantity.`: Restore model weights at the end of training with the weights from the best epoch\r\n- `Whether to restore model weights, from the epoch with the best value of the monitored quantity.`: Restore model weights at the current epoch with the weights from the best epoch.\r\n\r\nWhat actually happens is that the weights are reset in the method `on_epoch_end`, not at the end of training.\r\n\r\nIn lieu of updating the code to change the behaviour, I propose clarifying the current behaviour in the documentation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44049) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 44048, "title": "Incompatible Layer Error in TF Sequential When Adding a Layer Stored in a Variable", "body": "**System information**\r\n- I write my own custom code \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): from pip\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.6.9\r\n\r\n**How to Reproduce the Error**\r\n>>> model = tf.keras.Sequential()\r\n>>> model.add(tf.keras.layers.InputLayer(input_shape=[7]))\r\n>>> layer = tf.keras.layers.Dense(units=20, activation='relu')\r\n>>> model.add(layer)\r\n>>> model.add(layer)\r\n\r\nAttempting the second \"model.add(layer)\" will yield an error as follows:\r\n``\r\nValueError: Input 0 of layer dense_7 is incompatible with the layer: expected axis -1 of input shape to have value 7 but received input with shape [None, 20]\r\n``\r\n\r\nIt looks like this is a bug. \r\n\r\nThe code will work if I do model.add(tf.keras.layers.Dense(units=20, activation='relu')) instead of model.add(layer). However, I can't do this since due to some project requirements, I need to assign tf.keras.layers.Dense(units=20, activation='relu') to a variable. \r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/ca1d87ab8843a70da58c69dffca99cb5/44048.ipynb). \r\n\r\nWith [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e68066ea1491ff966a99a328fc68186f/44048.ipynb#scrollTo=TDiTG9qgkM8O), the error changes to `ValueError: Dimensions must be equal, but are 20 and 7 for '{{node dense/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](Placeholder, dense/MatMul/ReadVariableOp)' with input shapes: [?,20], [7,20].`. Please find the attached gist. Thanks!", "A Sequential model is not appropriate when you need to do layer sharing.\r\nSee https://www.tensorflow.org/guide/keras/sequential_model#when_to_use_a_sequential_model\r\n\r\nFor this you can use Functional API which supports sharing layer instances that get reused multiple times in a same model.\r\nSee https://www.tensorflow.org/guide/keras/functional#shared_layers\r\n\r\nOr - you can create unique layer instance for using it in Sequential model.\r\n```python\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.InputLayer(input_shape=[7]))\r\nlayer = tf.keras.layers.Dense(units=20, activation='relu')\r\nlayer1 = tf.keras.layers.Dense(units=20, activation='relu')\r\nmodel.add(layer)\r\nmodel.add(layer1)\r\n```", "@ymodak : Thanks for sharing the links. I guess the right way to reuse the layer instance is through the Functional API.\r\n\r\nRegarding your proposed solution, to me, \"layer\" and \"layer1\" look redundant. Ideally, we don't need to make \"layer1\" which exactly similar to \"layer\".\r\nBut then, as we know, the error occurs if we don't make \"layer1\", and just do model.add(layer).", "Yes `Functional API` is the right approach to use here.\r\nUsage `layer1` and `layer` creates a unique layer instance even though they appear to be redundant.\r\nI will close this issue now. Feel free to reopen or create new issue if required.\r\nThanks!"]}, {"number": 44047, "title": "cuDNN in invalid state after OOM", "body": "**System information**\r\n- Have I written custom code\r\n- OS Platform and Distribution:  Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA 10.0.130-1, cuDNN 7.6.5\r\n- GPU model and memory: GeForce RTX 2070 super, 8 GiB\r\n\r\n**Describe the current behavior**\r\nAfter getting ResourceExhaustedError during an operation using cuDNN (for instance due to out of memory) it seems like cuDNN is left in a broken state and further calls using cuDNN fail even if they don't exceed the resources.\r\n\r\n**Describe the expected behavior**\r\nI would expect further calls to cuDNN to be ok. This is important since it seems that the only way to know if a computation will use too much resources (for instance when determining optimal batch size) is to actually try and fail.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.errors_impl import ResourceExhaustedError\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, None, None, None, 32])\r\nfil = tf.zeros((3, 3, 3, 32, 32), dtype=tf.float32)\r\nconv = tf.nn.conv3d(x, filter=fil, strides=(1, 1, 1, 1, 1), padding='VALID', name=None)\r\n\r\nwith tf.Session() as sess:\r\n    try:\r\n        c = np.zeros((1, 370, 370, 370, 32), np.float32)\r\n        sess.run(conv, feed_dict={x: c})\r\n        r1 = sess.run(conv)\r\n    except ResourceExhaustedError:\r\n        print('Resource Error')\r\n\r\nwith tf.Session() as sess:\r\n    c = np.zeros((1, 100, 100, 100, 32), np.float32)\r\n    sess.run(conv, feed_dict={x: c})\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-10-15 13:41:46.910748: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-10-15 13:41:47.076574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 13:41:47.077067: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1414860 executing computations on platform CUDA. Devices:\r\n2020-10-15 13:41:47.077083: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070 SUPER, Compute Capability 7.5\r\n2020-10-15 13:41:47.096784: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3498230000 Hz\r\n2020-10-15 13:41:47.097210: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x158f0a0 executing computations on platform Host. Devices:\r\n2020-10-15 13:41:47.097229: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-10-15 13:41:47.097351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.815\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.79GiB freeMemory: 7.31GiB\r\n2020-10-15 13:41:47.097368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-10-15 13:41:47.098194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-15 13:41:47.098208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-10-15 13:41:47.098215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-10-15 13:41:47.098286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7110 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-15 13:41:47.100978: W tensorflow/core/framework/allocator.cc:124] Allocation of 6483584000 exceeds 10% of system memory.\r\n2020-10-15 13:42:01.641512: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.94GiB.  Current allocation summary follows.\r\n2020-10-15 13:42:01.641594: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641616: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641638: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2020-10-15 13:42:01.641656: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641672: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641688: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641704: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641720: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641742: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): \tTotal Chunks: 1, Chunks in use: 1. 108.0KiB allocated for chunks. 108.0KiB in use in bin. 108.0KiB client-requested in use in bin.\r\n2020-10-15 13:42:01.641759: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641775: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641791: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641807: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641823: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641839: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641856: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641872: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641888: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641904: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641924: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 13:42:01.641942: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 6.94GiB allocated for chunks. 6.04GiB in use in bin. 6.04GiB client-requested in use in bin.\r\n2020-10-15 13:42:01.641960: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 5.94GiB was 256.00MiB, Chunk State: \r\n2020-10-15 13:42:01.641986: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 926.76MiB | Requested Size: 0B | in_use: 0, prev:   Size: 6.04GiB | Requested Size: 6.04GiB | in_use: 1\r\n2020-10-15 13:42:01.642004: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f86a6000000 of size 110592\r\n2020-10-15 13:42:01.642017: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f86a601b000 of size 1280\r\n2020-10-15 13:42:01.642031: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7f86a601b500 of size 6483584000\r\n2020-10-15 13:42:01.642043: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7f8828755900 of size 971780864\r\n2020-10-15 13:42:01.642055: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \r\n2020-10-15 13:42:01.642071: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB\r\n2020-10-15 13:42:01.642087: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 110592 totalling 108.0KiB\r\n2020-10-15 13:42:01.642101: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 6483584000 totalling 6.04GiB\r\n2020-10-15 13:42:01.642116: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 6.04GiB\r\n2020-10-15 13:42:01.642136: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \r\nLimit:                  7455476941\r\nInUse:                  6483695872\r\nMaxInUse:               6483695872\r\nNumAllocs:                       3\r\nMaxAllocSize:           6483584000\r\n\r\n2020-10-15 13:42:01.642153: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ***************************************************************************************_____________\r\n2020-10-15 13:42:01.642223: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at conv_ops_3d.cc:161 : Resource exhausted: OOM when allocating tensor with shape[1,368,368,368,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nResource Error\r\n2020-10-15 13:42:01.801362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-10-15 13:42:01.801391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-15 13:42:01.801396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-10-15 13:42:01.801400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-10-15 13:42:01.801462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7110 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-15 13:42:02.948064: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-15 13:42:02.967863: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node Conv3D}}]]\r\n\t [[{{node Conv3D}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py\", line 19, in <module>\r\n    sess.run(conv, feed_dict={x: c})\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]\r\n\t [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]\r\n\r\nCaused by op 'Conv3D', defined at:\r\n  File \"/home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py\", line 7, in <module>\r\n    conv = tf.nn.conv3d(x, filter=fil, strides=(1, 1, 1, 1, 1), padding='VALID', name=None)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1440, in conv3d\r\n    dilations=dilations, name=name)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]\r\n\t [[node Conv3D (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:7) ]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["I turned out that I reduced the minimal example above to much (it fails even if I remove the first run). Here is an update:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.errors_impl import ResourceExhaustedError\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, None, None, None, 1])\r\nfil = tf.zeros((3, 3, 3, 1, 32), dtype=tf.float32)\r\nconv = tf.nn.convolution(x, filter=fil, padding='VALID', name=None)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    try:\r\n        c = np.zeros((1, 370, 370, 370, 1), np.float32)\r\n        sess.run(conv, feed_dict={x: c})\r\n    except ResourceExhaustedError:\r\n        print('Resource Error')\r\n\r\nwith tf.Session(config=config) as sess:\r\n    c = np.zeros((1, 30, 30, 30, 1), np.float32)\r\n    sess.run(conv, feed_dict={x: c})\r\n```\r\nProduces\r\n```\r\n/home/dbergh/.virtualenvs/boneseg/bin/python3.6 /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-10-15 17:15:28.759710: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-10-15 17:15:28.917081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 17:15:28.917568: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0xf48540 executing computations on platform CUDA. Devices:\r\n2020-10-15 17:15:28.917584: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070 SUPER, Compute Capability 7.5\r\n2020-10-15 17:15:28.936804: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3498230000 Hz\r\n2020-10-15 17:15:28.937281: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x10c0cc0 executing computations on platform Host. Devices:\r\n2020-10-15 17:15:28.937311: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-10-15 17:15:28.937464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.815\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.79GiB freeMemory: 7.29GiB\r\n2020-10-15 17:15:28.937484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-10-15 17:15:28.938429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-15 17:15:28.938445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-10-15 17:15:28.938453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-10-15 17:15:28.938536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7096 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-15 17:15:39.476332: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.94GiB.  Current allocation summary follows.\r\n2020-10-15 17:15:39.476412: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476452: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476476: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2020-10-15 17:15:39.476496: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048): \tTotal Chunks: 2, Chunks in use: 2. 7.0KiB allocated for chunks. 7.0KiB in use in bin. 6.8KiB client-requested in use in bin.\r\n2020-10-15 17:15:39.476515: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476532: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476550: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476567: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476609: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476628: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476645: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476670: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 1015.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476709: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476764: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476802: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476835: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476865: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476883: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476900: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476923: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-15 17:15:39.476946: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 6.93GiB allocated for chunks. 6.19GiB in use in bin. 6.13GiB client-requested in use in bin.\r\n2020-10-15 17:15:39.476968: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 5.94GiB was 256.00MiB, Chunk State: \r\n2020-10-15 17:15:39.476999: I tensorflow/core/common_runtime/bfc_allocator.cc:619]   Size: 756.47MiB | Requested Size: 0B | in_use: 0, prev:   Size: 5.94GiB | Requested Size: 5.94GiB | in_use: 1\r\n2020-10-15 17:15:39.477020: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fafde000000 of size 6379012096\r\n2020-10-15 17:15:39.477036: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7fb15a380000 of size 793221376\r\n2020-10-15 17:15:39.477051: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fb18a000000 of size 268435456\r\n2020-10-15 17:15:39.477066: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fb21f400000 of size 3584\r\n2020-10-15 17:15:39.477081: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fb21f400e00 of size 1280\r\n2020-10-15 17:15:39.477095: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x7fb21f401300 of size 3584\r\n2020-10-15 17:15:39.477109: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x7fb21f402100 of size 1040128\r\n2020-10-15 17:15:39.477122: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: \r\n2020-10-15 17:15:39.477140: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 1280 totalling 1.2KiB\r\n2020-10-15 17:15:39.477157: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 3584 totalling 7.0KiB\r\n2020-10-15 17:15:39.477174: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 268435456 totalling 256.00MiB\r\n2020-10-15 17:15:39.477190: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 6379012096 totalling 5.94GiB\r\n2020-10-15 17:15:39.477205: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 6.19GiB\r\n2020-10-15 17:15:39.477227: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \r\nLimit:                  7441717658\r\nInUse:                  6647456000\r\nMaxInUse:               6647456000\r\nNumAllocs:                       5\r\nMaxAllocSize:           6379012096\r\n\r\n2020-10-15 17:15:39.477247: W tensorflow/core/common_runtime/bfc_allocator.cc:271] **************************************************************************************__________****\r\n2020-10-15 17:15:39.477328: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at conv_ops_3d.cc:399 : Resource exhausted: OOM when allocating tensor with shape[1,32,368,368,368] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nResource Error\r\n2020-10-15 17:15:39.497736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-10-15 17:15:39.497773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-15 17:15:39.497779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-10-15 17:15:39.497783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-10-15 17:15:39.497842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7096 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-15 17:15:40.247329: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-10-15 17:15:40.251851: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node convolution}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py\", line 21, in <module>\r\n    sess.run(conv, feed_dict={x: c})\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node convolution (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:10) ]]\r\n\r\nCaused by op 'convolution', defined at:\r\n  File \"/home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py\", line 10, in <module>\r\n    conv = tf.nn.convolution(x, filter=fil, padding='VALID', name=None)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 851, in convolution\r\n    return op(input, filter)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 966, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 591, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 208, in __call__\r\n    name=self.name)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1440, in conv3d\r\n    dilations=dilations, name=name)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node convolution (defined at /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py:10) ]]\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nwhile the code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.errors_impl import ResourceExhaustedError\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, None, None, None, 1])\r\nfil = tf.zeros((3, 3, 3, 1, 32), dtype=tf.float32)\r\nconv = tf.nn.convolution(x, filter=fil, padding='VALID', name=None)\r\n\r\n# with tf.Session(config=config) as sess:\r\n#     try:\r\n#         c = np.zeros((1, 370, 370, 370, 1), np.float32)\r\n#         sess.run(conv, feed_dict={x: c})\r\n#     except ResourceExhaustedError:\r\n#         print('Resource Error')\r\n\r\nwith tf.Session(config=config) as sess:\r\n    c = np.zeros((1, 30, 30, 30, 1), np.float32)\r\n    sess.run(conv, feed_dict={x: c})\r\n```\r\nproduces the output\r\n```\r\n/home/dbergh/.virtualenvs/boneseg/bin/python3.6 /home/dbergh/.PyCharm2019.3/config/scratches/scratch_59.py\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/dbergh/.virtualenvs/boneseg/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-10-15 17:16:25.418268: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-10-15 17:16:25.585102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 17:16:25.585564: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x237e670 executing computations on platform CUDA. Devices:\r\n2020-10-15 17:16:25.585579: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070 SUPER, Compute Capability 7.5\r\n2020-10-15 17:16:25.604750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3498230000 Hz\r\n2020-10-15 17:16:25.605156: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x24f72f0 executing computations on platform Host. Devices:\r\n2020-10-15 17:16:25.605177: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-10-15 17:16:25.605303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.815\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.79GiB freeMemory: 7.29GiB\r\n2020-10-15 17:16:25.605321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-10-15 17:16:25.606144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-15 17:16:25.606159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-10-15 17:16:25.606166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-10-15 17:16:25.606249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7096 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n\r\nProcess finished with exit code 0\r\n```\r\nindicating success.", "@\r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7ab0c0c2828b8aac1f4d1dce5879fe77/untitled440.ipynb).\r\nPlease note there is support for 2.x only, there is no support for 1.x, please try to upgrade and let us know if that helps.", "Ok. With 2.3.1 (over cuda 10.1) I get no error. The tensorflow 2 version of the code would be.\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.errors_impl import ResourceExhaustedError\r\ntf.disable_v2_behavior()\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, None, None, None, 1])\r\nfil = tf.zeros((3, 3, 3, 1, 32), dtype=tf.float32)\r\nconv = tf.nn.convolution(x, filter=fil, padding='VALID', name=None)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    try:\r\n        c = np.zeros((1, 370, 370, 370, 1), np.float32)\r\n        sess.run(conv, feed_dict={x: c})\r\n    except ResourceExhaustedError:\r\n        print('Resource Error')\r\n\r\nwith tf.Session(config=config) as sess:\r\n    c = np.zeros((1, 30, 30, 30, 1), np.float32)\r\n    sess.run(conv, feed_dict={x: c})\r\n```", "If someone else runs into this:\r\nThe error seems to subtly depend on the order of execution. The following code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.errors_impl import ResourceExhaustedError\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, None, None, None, 1])\r\nfil = tf.zeros((3, 3, 3, 1, 32), dtype=tf.float32)\r\nconv = tf.nn.convolution(x, filter=fil, padding='VALID', name=None)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    c = np.zeros((1, 30, 30, 30, 1), np.float32)\r\n    sess.run(conv, feed_dict={x: c})\r\n\r\nwith tf.Session(config=config) as sess:\r\n    try:\r\n        c = np.zeros((1, 370, 370, 370, 1), np.float32)\r\n        sess.run(conv, feed_dict={x: c})\r\n    except ResourceExhaustedError:\r\n        print('Resource Error')\r\n\r\nwith tf.Session(config=config) as sess:\r\n    c = np.zeros((1, 30, 30, 30, 1), np.float32)\r\n    sess.run(conv, feed_dict={x: c})\r\n```\r\n\r\nproduces no error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44047\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44047\">No</a>\n"]}, {"number": 44045, "title": "tflite_convert fails from keras model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.3\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, CONCATENATION, CONV_2D, ELU, FILL, FULLY_CONNECTED, GATHER, LOGISTIC, MAX_POOL_2D, PACK, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: StatelessWhile, TensorListFromTensor, TensorListReserve, TensorListStack.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nStandard text classification with Keras sequential model using embeddings, stacked CONV1D and stacked Bidirectional GRUs.\r\nSwitching to experimental converter does not help, --allow_custom_ops neither.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@tschul,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the Keras model you are using. \r\n\r\nAlso, please take a looks issues [#42912](https://github.com/tensorflow/tensorflow/issues/42912#issuecomment-686185561) and [#33490](https://github.com/tensorflow/tensorflow/issues/33490#issuecomment-544351257) with a similar error and let us know it it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44044, "title": "[tinyML Book] `hello_world` example not running on Ardunio Nano 33 BLE", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **masOS.Catalina.10.15.5**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- Tensorflow version (commit SHA if source):  **2.1.0-ALPHA-precompiled**\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): **Arduino Nano 33**\r\n\r\n**Describe the problem**\r\n\r\nAfter following the book and the instructions in the [video screencast](https://youtu.be/AfAyHheBk6Y?t=1671) I am able to compile the program and apparently load it onto the board, but nothing seems to happen.\r\n\r\nThen when I try to inspect the serial port logger, I am always faced with `Board at /dev/cu.usbmodem14301 is not available` and I am forced to reset the board and start again, but the same problem arises.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nFor details, see below.\r\n\r\nI face the same situation when I **only** use the code provided in the examples folder **and** when I modify it for my own model (following instructions in the book).\r\n\r\n1. Open the `Examples>Arduino_TensorFlowLite>hello_world` example and upload gives this output:\r\n```\r\nLibrary Arduino_TensorFlowLite has been declared precompiled:\r\nUsing precompiled library in /Users/tallamjr/Documents/Arduino/libraries/Arduino_TensorFlowLite/src/cortex-m4/fpv4-sp-d16-softfp\r\nSketch uses 231536 bytes (23%) of program storage space. Maximum is 983040 bytes.\r\nGlobal variables use 58272 bytes (22%) of dynamic memory, leaving 203872 bytes for local variables. Maximum is 262144 bytes.\r\nDevice       : nRF52840-QIAA\r\nVersion      : Arduino Bootloader (SAM-BA extended) 2.0 [Arduino:IKXYZ]\r\nAddress      : 0x0\r\nPages        : 256\r\nPage Size    : 4096 bytes\r\nTotal Size   : 1024KB\r\nPlanes       : 1\r\nLock Regions : 0\r\nLocked       : none\r\nSecurity     : false\r\nErase flash\r\n\r\nDone in 0.001 seconds\r\nWrite 231544 bytes to flash (57 pages)\r\n[==============================] 100% (57/57 pages)\r\nDone in 9.084 seconds\r\n```\r\nThis immediately seems fine, but I notice that the orange LED goes from slowly blinking (in a bootloader state) to a solid always on light.\r\n\r\nI have tried to change the `const int kInferencesPerCycle` as I initially thought that I am just not able to see the flicker, but this did not alter the solid light.\r\n\r\nI also get this same problem when running a modified model (using the steps in the book and this notebook: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb)\r\n\r\nEach time as well, when I try to inspect using the Serial Plotter, I get the following error in the console:\r\n```\r\nBoard at /dev/cu.usbmodem14301 is not available\r\n```\r\nAny help with this would be appreciated. I feel very stuck at the moment on what to try as it seems to compiling fine. Thanks", "comments": ["This seems to be resolved now, I changed the library from **2.1.0-ALPHA-precompiled** ---> **2.1.0-ALPHA**. I don't know why this specifically helps, but seems to have corrected the issue"]}, {"number": 44043, "title": "[TFLite] Built-in way to run the reference kernels with the Python API", "body": "Hello,\r\n\r\nWhen accelerating inference (via optimized software or hardware implementations) we often want to compare the result accuracy with the reference result defined by TFLite reference kernels (`kReference`) instead of the default kernels (which often are `kGenericOptimized`).\r\n\r\nWith the C++ API it is easy to pass the `tflite::ops::builtin::BuiltinRefOpResolver` instead of the `tflite::ops::builtin::BuiltinOpResolver` to the `tflite::InterpreterBuilder` constructor. Unfortunately with the Python API we need to recompile TensorFlow to expose the experimental `tf.lite.InterpreterWithCustomOps` API and manually export the symbol of a C function which adds the `tflite::ops::builtin::BuiltinRefOpResolver` to the current `tflite::MutableOpResolver`.\r\n\r\nAs manually compiling TensorFlow instead of using the public binaries is quite cumbersome for some of our users, we would like to see if it would be possible to add a built-in way to run a model with the reference kernels using the publicly distributed binaries.\r\n\r\n\r\nThanks\r\nThibaut", "comments": ["This seems like a reasonable request. We probably wouldn't expose this for other language bindings, like Java/Swift, but for Python it probably makes sense as that's \"close\" to the conversion code and could be useful for development, as you noted. \r\n\r\nAre you able to tell what the relative binary size difference would be if we linked in both the reference and optimized OpResolver into the binary?", "Comparing the full Linux TensorFlow 2.3 CPU I built with and without the reference kernels compiled into it, I have the following sizes.\r\n* Base: 127 157 306 bytes\r\n* With reference kernels: 127 276 271 bytes\r\n\r\nSo a difference of around 116 KiB (~0.09% relative increase, would be less for the GPU build).", "That doesn't seem too bad. Have you thought about what the API would look like when creating/using the Interpreter?", "I see a few ways:\r\n* Either adding an extra boolean or enum parameter to select if we want to use the optimized or reference kernels in `tf.lite.Interpreter` (default to optimized). Though it may make the interface more complex than needed for a feature that will not be used by many users.\r\n* Currently there is the experimental [`tf.lite.InterpreterWithCustomOps`](https://github.com/tensorflow/tensorflow/blob/547bd9c88b1a86f0543fff3460e2d4d1c8009cb4/tensorflow/lite/python/interpreter.py#L568). We could stabilize it and provide the reference kernels as a kind of custom ops (though it may be confusing as the reference kernels are not really custom ops). \r\n* Instead we could mimic more closely the C++ API and provide a `tf.lite.InterpreterWithCustomOpResolver` (or add an extra `op_resolver` parameter in `tf.lite.Interpreter`). We could then remove the experimental `tf.lite.InterpreterWithCustomOps` as we could easily register custom ops in the custom op resolver.\r\n\r\nI am personally more in favour of a `tf.lite.InterpreterWithCustomOpResolver` as it would not clutter the `tf.lite.Interpreter` while providing a good flexibility and be more general than `tf.lite.InterpreterWithCustomOps`. It would also be quite close to the [`InterpreterBuilder`](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter-builder) of the C++ API.", "There has also been an effort to make it easer to inject a delegate into the Python interpreter, and we could make a pretty straightforward delegate that maps to an OpResolver (in this case the reference OpResolver). Of course, we'd still need to expose and bundle that delegate somehow (or the OpResolver), so there's still a question of whether we'd need an extra API (or if we just implicitly link in the reference ops as a separate delegate .so that gets shipped).\r\n\r\n@dmitriykovalev FYI.", "Hi @Tessil , we have an internal change prepared for this, but we wanted to check with you first before landing. In our CL, we simply expose the option to use the Reference OpResolver, but we don't provide any finer-grained control (e.g., tweaking the actual registered kernels, or selecting their versions). Is that sufficient for your needs?", "Hi, \r\n\r\nYes it is perfectly sufficient for our needs. We don't need a per-operator fine-tuning of the registered kernels, our main goal is to have a way to run a model using the reference kernels with the Python API.", "> Hi,\r\n> \r\n> Yes it is perfectly sufficient for our needs. We don't need a per-operator fine-tuning of the registered kernels, our main goal is to have a way to run a model using the reference kernels with the Python API.\r\n\r\nGreat to know about this! Our CL is almost ready, and should be checked in by 3/12.", "> > Hi,\r\n> > Yes it is perfectly sufficient for our needs. We don't need a per-operator fine-tuning of the registered kernels, our main goal is to have a way to run a model using the reference kernels with the Python API.\r\n> \r\n> Great to know about this! Our CL is almost ready, and should be checked in by 3/12.\r\n\r\nHi @Tessil, the support of using reference op resolver in Python has been pushed. Pls refer to\r\nhttps://github.com/tensorflow/tensorflow/commit/6886228c611131432e550beefce79770d8538972 for details. In short, to use it, when creating the tflite.Interpreter, pass an additional arg as \"experimental_op_resolver=OpResolver.BUILTIN_REF\".\r\n\r\nCould you try it and see whether it works on your side? Thx!", "Hello @multiverse-tf ,\r\n\r\nThank you very much for the commit. \r\n\r\nSmall problem I think, though it may be the intended behaviour. \r\n\r\nI can use it with:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.lite.python.interpreter import OpResolver\r\n\r\ninterpreter = tf.lite.Interpreter(model, experimental_op_resolver=OpResolver.BUILTIN_REF)\r\n```\r\n\r\nbut not as I would have expected from the `@_tf_export('lite.experimental.OpResolver')`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ninterpreter = tf.lite.Interpreter(model, experimental_op_resolver=tf.lite.experimental.OpResolver.BUILTIN_REF)\r\n```\r\n\r\nI think that in https://github.com/tensorflow/tensorflow/blob/0a150957c4a23e7fd26b2df26e01af661f22a0b2/tensorflow/lite/python/lite.py#L52\r\na `from tensorflow.lite.python.interpreter import OpResolver  # pylint: disable=unused-import` line should be added, no?\r\n\r\nOtherwise everything seems to work well, thanks.", "> Hello @multiverse-tf ,\r\n> \r\n> Thank you very much for the commit.\r\n> \r\n> Small problem I think, though it may be the intended behaviour.\r\n> \r\n> I can use it with:\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> from tensorflow.lite.python.interpreter import OpResolver\r\n> \r\n> interpreter = tf.lite.Interpreter(model, experimental_op_resolver=OpResolver.BUILTIN_REF)\r\n> ```\r\n> \r\n> but not as I would have expected from the `@_tf_export('lite.experimental.OpResolver')`:\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> \r\n> interpreter = tf.lite.Interpreter(model, experimental_op_resolver=tf.lite.experimental.OpResolver.BUILTIN_REF)\r\n> ```\r\n> \r\n> I think that in\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/0a150957c4a23e7fd26b2df26e01af661f22a0b2/tensorflow/lite/python/lite.py#L52\r\n> \r\n> \r\n> a `from tensorflow.lite.python.interpreter import OpResolver # pylint: disable=unused-import` line should be added, no?\r\n> Otherwise everything seems to work well, thanks.\r\n\r\nThx for the verification! Regarding the small issue you mentioned, just checked in https://github.com/tensorflow/tensorflow/commit/fafba9e833f963bbb8f434f9e0f3f6f557d60d5d to fix it. Note that we've decided to change OpResolver to OpResolverType so that it's more clear semantically. Let me know if there're any other issues.", "Thank you, it's working perfectly well with `tf.lite.experimental.OpResolverType.BUILTIN_REF` now. I'll close the ticket then."]}, {"number": 44042, "title": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfa in position 3: invalid start byte", "body": "\r\nWhile converting my trained model from saved.pb and checkpoint file to frozen.pb getting the below error :\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 491, in <module>\r\n    run_main()\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 487, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/absl_py/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/absl_py/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 486, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 372, in main\r\n    freeze_graph(flags.input_graph, flags.input_saver, flags.input_binary,\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 338, in freeze_graph\r\n    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 253, in _parse_input_graph_proto\r\n    text_format.Merge(f.read(), input_graph_def)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 122, in read\r\n    return self._prepare_value(self._read_buf.read(length))\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 94, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 139, in as_str_any\r\n    return as_str(value)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 118, in as_str\r\n    return as_text(bytes_or_text, encoding)\r\n  File \"/home/micronagri/Desktop/tf2_configure/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/util/compat.py\", line 109, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xfa in position 3: invalid start byte\r\n\r\n\r\nBelow are my system details :\r\n\r\nOS: Ubuntu  - 20.04.1\r\nPython Version : 3.8.5\r\n\r\nI found couple of threads detailing the use of 'rb' instead of 'r', but I'm not sure of its working here. ", "comments": ["@vineethdas,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "The model was trained in the Windows System with the Tensorflow Version: 2.3.1\r\nI used the git repository - https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tools/freeze_graph.py\r\n\r\nThough I trained the model in Windows 10, I'm trying to extract the frozen-graph using ubuntu.\r\nThe input to the program are the saved.pb and chkpt-00 file.\r\nLet me know if that is possible, (using a windows trained model in linux environment)\r\n", "@vineethdas,\r\nCould you please provide the complete code to build the model and the saved model files as well, so that we can reproduce the issue on our end? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44041, "title": "AttributeError: module 'tensorflow' has no attribute 'matrix_determinant'", "body": "when i write in jupyter\r\n\r\n```\r\nimport tensorflow as tf\r\nmatrix_det = tf.matrix_determinant(matrix_3)\r\n```\r\n\r\nthe error will be:```\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-16-b52dc5929b23> in <module>\r\n----> 1 matrix_det = tf.matrix_determinant(matrix_3)\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'matrix_determinant'\r\n```", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4a15a2dad484fc852a66e8165d95b945/untitled441.ipynb).", "`tf.matrix_determinant` is deprecated and revived as `tf.linalg.det`.\r\nYou may try using [`tf.linalg.det`](https://www.tensorflow.org/api_docs/python/tf/linalg/det) function in TF 2.X\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44041\">No</a>\n"]}, {"number": 44040, "title": "not able to find libcudnn.so.7", "body": "**System information**\r\nOperating System: 20.04.4 LTS\r\nTensorFlow version: 2.3.1\r\nPython version: 3.7.4\r\nGCC/Compiler: 7.3.0\r\nCUDA: 10.1.243\r\ncuDNN: 8.0.4\r\nGPU model and memory: GeForce GTX 960m, 4 Gb\r\nNVIDIA-SMI: 450.80.02\r\n\r\n\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n2020-10-15 13:01:11.746116: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\n>>> import tensorflow as tf\r\n\r\n2020-10-15 12:44:04.961178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> \r\n>>> tf.config.list_physical_devices(\"GPU\")\r\n2020-10-15 12:44:11.883542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-15 12:44:11.915652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 12:44:11.916045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-10-15 12:44:11.916111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-15 12:44:11.918107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-15 12:44:11.919968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-15 12:44:11.920310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-15 12:44:11.922495: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-15 12:44:11.923927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n**2020-10-15 12:44:11.924102: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/nitin/catkin_ws/devel/lib:/usr/lib/cuda/include:/usr/lib/cuda/lib64\r\n2020-10-15 12:44:11.924120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries.** \r\nPlease make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\r\n\r\n\r\n\r\n", "comments": ["@nitin7999 \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#gpu).\r\nRefer the[ issue comment](https://github.com/tensorflow/tensorflow/issues/20271#issuecomment-622990424), [link](https://github.com/tensorflow/tensorflow/issues/38164#issuecomment-656767462) and see if it helps you. Thanks!", "ok how should i go about to change the cudnn installation alone. i guess cudnn 8.0.4 is not supported.\r\n\r\n", "What to do for this?\r\n\r\n>>> tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-10-15 22:07:08.142338: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-15 22:07:08.411314: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2599990000 Hz\r\n2020-10-15 22:07:08.412772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ebe611de0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-15 22:07:08.412847: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-15 22:07:08.663373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 22:07:08.664875: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ebe6a5fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-15 22:07:08.664954: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\n2020-10-15 22:07:08.683250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 22:07:08.684234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-10-15 22:07:08.684358: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-15 22:07:08.684445: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-15 22:07:08.684507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-15 22:07:08.684569: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-15 22:07:08.684626: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-15 22:07:08.684702: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-15 22:07:08.684780: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-15 22:07:08.685009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 22:07:08.685986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-15 22:07:08.686827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-15 22:07:08.700697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/nitin/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/nitin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py\", line 1563, in is_gpu_available\r\n    for local_device in device_lib.list_local_devices():\r\n  File \"/home/nitin/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/device_lib.py\", line 43, in list_local_devices\r\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\n**RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid**\r\n>>> \r\n\r\n\r\n", "@nitin7999 \r\n\r\ncuDNN 8 is not supported. Please try installing cuDNN 7.6 and check if you are facing the same issue? \r\nThanks!", "that is the result shown after installing cuDNN 7.6\r\n", "@nitin7999 Apologies for the delay in response. I see you are using GeForce GTX 960M which has 5.0 cuda compute capability.\r\nSee https://developer.nvidia.com/cuda-gpus\r\nAs of now we have TensorFlow pip pre built packages that support CUDA\u00ae architectures 3.5, 3.7, 5.2, 6.0, 6.1, 7.0 and higher than 7.0\r\nUnfortunately your gpu is not among the supported pip packages.\r\nSee https://www.tensorflow.org/install/gpu#hardware_requirements\r\n\r\nSolution here will be to build a pip package from source that supports your gpu architecture.\r\nThanks!\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44040\">No</a>\n"]}, {"number": 44039, "title": "Fix compilation of xtensa_hifi activations kernel", "body": "Update the activations kernel to latest reference implementation.\r\nUse the new TfLiteEvalTensor API.\r\nThis is an incremental change towards fixing Issue #43912.\r\n\r\nIssue #43912", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@pnikam-cad  Can you please resolve conflicts? Thanks!\r\n", "We are switched over to using the common xtensa directory and the xtensa_hifi directory was deleted with https://github.com/tensorflow/tensorflow/pull/47096.\r\n\r\nThis PR can be closed."]}, {"number": 44038, "title": "different result from tf.image.resize in tensorflow 2", "body": "I use tensorflow 2.2 and python 3.6.9\r\n\r\nI tried to convert tf 1.12 version model to tf 2.2 version model\r\n\r\nbut in tf 2.2\r\ntf.image.resize(image, [299, 299], method=\"bilinear\", antialias=True)\r\ntf.image.resize(image, [299, 299], method=\"bilinear\", antialias=False)\r\n\r\nare different results with tf.compat.v1.image.resize(image, [299,299], method='bilinear')\r\n\r\nfor the same image.\r\n", "comments": ["@kja815,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "@kja815 \r\nTF1 [resize](https://www.tensorflow.org/api_docs/python/tf/compat/v1/image/resize) has a different set of parameters than the TF2 [version](https://www.tensorflow.org/api_docs/python/tf/image/resize). Namely, `align_corner` is dropped, which will likely impact the image. Have you checked that the parameters are configured to be the same?", "@mkuchnik \r\n\r\nI know that parameter 'align_corner' is dropped.\r\nI tested various combination of parameter sets for tf.image.resize with bilinear method. but any combination can't get the same result with tf.compat.v1.image.resize(image, [299,299], method='bilinear').\r\nthere wasn't same resize method in tf 2 ? \r\nI want to get the same result without tf.compat.v1.image.resize because compat was deprecated soon.\r\n ", "> @kja815,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\n@kja815,\r\nAny updates regarding this? Could you please provide the script you are running and let us know the difference you have noticed. Thanks!", "@amahendrakar \r\nwith tf.io.gfile.GFile(image_path, 'rb') as input_file:\r\n    image = input_file.read()\r\nimage = tf.cast(tf.io.decode_image(image, channels=3, expand_animations=False), dtype=tf.float32)\r\ncompat_image = tf.compat.v1.image.resize(image, [299,299], method='bilinear', align_corners=False, preserve_aspect_ratio=False)\r\ntf2_image_true = tf.image.resize(image, [299, 299], method=\"bilinear\", antialias=True)\r\ntf2_image_false = tf.image.resize(image, [299, 299], method=\"bilinear\", antialias=False)\r\n\r\ncompat_image is different result from tf2_image_true and tf2_image_false", "Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c8295826ae5ccaa4908e1804a7ccfdc3/44038.ipynb). Thanks!", "This is because `half_pixel_centers` attribute was added in tf2 which assumes that pixels are 0.5, 0.5 for more reliable and correct resizes.\r\n\r\nCommit: https://github.com/tensorflow/tensorflow/commit/3ae2c6691b7c6e0986d97b150c9283e5cc52c15f\r\n\r\nTo me, this looks like an intended change (that breaks backwards compat) to fix known issues with resizes.\r\n\r\nAn option is to add `half_pixel_centers` attribute to `tf.image.resize` as well (currently it calls `resize_bilinear` with this option set to True) and give the option to users to set it to False which will revert the behavior back to <tf2.x. But, given that this was intended to fix / improve the behavior of old resizes, the new behavior should be used.", "@kja815,\r\nClosing this issue as the behavior is intended as per the above comment. Please feel free to reopen the issue if you disagree with that comment. Thanks!\r\n ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44038\">No</a>\n"]}, {"number": 44037, "title": "How to pass mutable tensors to a Custom Op in Python?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I've written custom ops and import it on Python side.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.5\r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying custom op with mutable inputs. Therefore I got my own custom op with:\r\n```\r\nREGISTER_OP(\"MyCustomOp\").Input(\"input1: Ref(float)\") ....\r\n```\r\n\r\nI was able to call this custom ops from my Python keras layer:\r\n```\r\nclass MyKerasLayer(Layer):\r\n...\r\n def build(self, input_shape):\r\n    ...\r\n    self.moving_mean = self.add_weight(shape=(5,), trainable=False, name=\"moving_mean\")\r\n\r\n def call(self, inputs):\r\n    return self.custom_op.MyCustomOp(input1=self.moving_mean, ...)\r\n```\r\n\r\nHowever, when running the custom layer in graph mode, I got errors like:\r\n```\r\nTypeError: 'MyCustomOp' Op requires that input 'input1' be a mutable tensor (e.g.: a tf.Variable)\r\n```\r\n\r\n**I'm trying to find a way you could feed a layer's weights into custom ops, in a way these inputs are mutable.**\r\n\r\n**Describe the expected behavior**\r\n\r\nBy passing `self.moving_mean` to my `MyCustomOp`, the Op accept it as a valid mutable tensor.\r\n\r\n\r\n", "comments": ["@golden0080 \r\nPlease share complete stand alone indented code or is possible share a colab gist with the error reported.", "I was able to work around this error, with the `getter` parameter to `Layer.add_weight`.\r\nI basically copied `base_layer_utils.make_variable` to create a RefVariable instead.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44037\">No</a>\n", "But I think to make this not an issue for anyone else, probably we should update TF doc to showcase how to use that `getter` parameter?", "@golden0080 Are you interested in updating the docs by raising a PR? Thanks!", "@jvishnuvardhan Yes, I think I could contribute to a doc update.\r\nCould you direct me to what I need to do for doc updates?", "\r\n> But I think to make this not an issue for anyone else, probably we should update TF doc to showcase how to use that `getter` parameter?\r\n\r\nHi golden0080, I also encountered this problem, may I know how you solved it? Thanks", "@Frank1993 Check out the code at: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L615\r\nIt basically means `getter` could be the variable builder, which the default one is only making normal `Variable`.\r\nBy providing a custom builder, it's totally possible to construct `RefVariable` instead of default ones.", "In addition, `RefVariable` class could be your friend: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L1556", "> @jvishnuvardhan Yes, I think I could contribute to a doc update.\r\n> Could you direct me to what I need to do for doc updates?\r\n\r\n@golden0080,\r\nSorry for the delayed response. Please refer the links mentioned below for Contributing to Tensorflow Documentation:\r\n\r\nContributor guide: https://www.tensorflow.org/community/contribute/docs,\r\nDocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\nDocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44036, "title": "Disable mkl inside eigen", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version:latest \r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):3.3.0\r\n- GCC/Compiler version (if compiling from source): gcc 8.3\r\n- I did **NOT** use the --config=mkl flag while building\r\n- The image is a call graph of 1024X1024 Matmul done 100 times, if you look carefully mkldnn_sgemm is called internally by Eigen, this is what I want to disable.\r\n \r\n![internal_eigen](https://user-images.githubusercontent.com/44555985/96214609-7444a200-0f99-11eb-988b-c4f9b32c6d2d.jpg)\r\n\r\n\r\nAfter some amount of reading I found out that MKL/mkl_dnn can be called internally by Eigen and after reading the Bazel documentation and seeing how TensorFlow is strucured.I want to disable mkl/mkl_dnn completely if eigen is to be used.\r\nThe Eigen[ build files loads the ifmkl symbol](https://github.com/tensorflow/tensorflow/blob/bd35e19edbf55bf3f6b5d10545725d41a99aa9ba/third_party/eigen3/BUILD#L5) from the mkl directory.\r\nMy next step was to look at the [if_mkl function inside the build_defs.bzl](https://github.com/tensorflow/tensorflow/blob/c63b59ce648b525c4bbb2d85f04f5b71f1849ff2/third_party/mkl/build_defs.bzl#L17)   \r\n\r\nI changed the [includes attribute in the cclibrary rule in the BUILD file of eigen](https://github.com/tensorflow/tensorflow/blob/44aace846df438bf55e88dad1e047b7c81f9efb9/third_party/eigen3/BUILD#L36) to [\"//conditions:default\"]\r\n\r\nwhich game me an error saying `ModuleNotFoundError: No module named 'portpicker' `\r\n\r\nSo pip installed portpicker `pip install portpicker`\r\nAnd the build completed sucessfully, but the profile literally shows no difference mkldnn_sgemm is still bieng called the same number of time by eigen internally\r\n\r\nNOTE: **The Ultimate aim is to disable mkldnn which is bieng called from inside eigen**", "comments": []}]