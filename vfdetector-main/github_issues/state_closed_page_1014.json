[{"number": 22930, "title": "absl not installed as it is included in stringpiece.h", "body": "https://github.com/tensorflow/tensorflow/blob/8201ea4dc1cdd9283121f58b48ba7237a05b220e/tensorflow/core/lib/core/stringpiece.h#L34", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nSource (Tried to build using cmake 3.11.1 but failed)\r\n- **TensorFlow version (use command below)**:\r\n1.10.0\r\n- **Python version**:\r\nN/A\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nVisual Studio 2015 - Visual C++ 14.0\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -Dtensorflow_BUILD_CC_EXAMPLE=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_BUILD_PYTHON_TESTS=OFF -Dtensorflow_ENABLE_GPU=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF\r\n### Describe the problem\r\nI am trying to build tensorflow as a standalone project and have been following this tutorial\r\n\r\nhttp://www.stefanseibert.com/2017/10/tensorflow-as-dll-into-your-windows-c-project-with-gpu-support-and-cmake-v1-3/\r\n\r\nbut alternatively with cpu support\r\n\r\nHere are the steps I used to generate the shared lib\r\n\r\n- Acquired source code from https://github.com/tensorflow/tensorflow.git\r\n- Have installed the dependencies since I do not use the python bindings, there is no need for SWIG, so I installed Git (version 2.15.1.windows.2) and cmake 3.11.1\r\n- I used the 64bit tools from Visual Studio 2015 since VS2015 is necessary to build the DLL. I should be able to open the \u201cVS2015 x64 Native Tools Command Prompt\u201d. This is needed so VS uses the 64 bit toolset.\r\n- Navigated in the commandline to the \u201ctensorflow/contrib/cmake\u201d subfolder of the source code and create a directory with \u201cmkdir build\u201d. Afterwards navigate to the fresh build folder with \u201ccd build\u201d.\r\n- Create a build solution: cmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -Dtensorflow_BUILD_CC_EXAMPLE=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_BUILD_PYTHON_TESTS=OFF -Dtensorflow_ENABLE_GPU=OFF -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF\r\n- Everything went fine till this. To build the tensorflow.dll, I issued the following command: MSBuild /p:Configuration=RelWithDebInfo tensorflow.vcxproj\r\n- This throws an error: D:\\work\\tensorflow\\tensorflow/core/lib/core/stringpiece.h(34): fatal error C1083: Cannot open include file: 'absl/strings/string_view.h': No such file or directory ( compiling source file D:\\work\\tensorflow\\tensorflow\\core\\lib\\core\\coding.cc) [D:\\work\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_lib.vcxproj].\r\n- I fixed the above error with this: https://github.com/tensorflow/tensorflow/issues/22007#issuecomment-424553600.\r\n- Doing the above I ended up with link error LNK2019: \r\n\r\nThis error seems simple but is frustrating me, and I would greatly appreciate all help in getting this done quickly, thanks!\r\n\r\n### Source code / logs\r\n- error LNK2019: unresolved external symbol \"void __cdecl absl::base_internal::ThrowStdOutOfRange(char const *)\" (?ThrowStdOutOfRange@base_internal@absl@@YA XPEBD@Z) referenced in function \"class std::basic_string,class std::allocator > __cdecl tensorflow::io::internal::JoinPathIm\r\n", "@Nagamani732 Please follow the [build compatibility guide](https://www.tensorflow.org/install/source_windows) here for TensorFlow installation on Windows. You may want to upgrade to Windows 10, post error messages here.", "@wt-huang Thanks for your reply. I followed the build compatibility guide and installed dependencies and tried to build tensorflow solution using cmake. But they didn't specify the minimum requirements for windows version. Will it be solved by installing windows 10 ?", "I also face this problem.  My OS is windows 10, cmake still can not find absl.", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Nagamani732  @yoobright   We no longer encourage users to use cmake for Windows. Please use Bazel instead. This should not be an issue with Bazel. Hence closing this. Feel free to provide more information/errors if you are facing with Bazel. We will reopen. Thanks !", "@Nagamani732 Did you found a way to fix this ? I also face the same issue."]}, {"number": 22929, "title": "remove support for TFOptimizer in optimizer.get", "body": "Fix #22780.\r\n\r\nI think we should forbid user to use TFOptimizer directly, which is an inner implementation for keras.", "comments": ["Perhaps we should better document it instead. Or rename `TFOptimizer` to `_TFOptimizer`."]}, {"number": 22928, "title": "Tensorflow setup", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I want to install tensflow and amazon server and os for that is oracle linux 5.11. which setup in can use. Will the ubuntu setup will work same for the oracle linux. ", "this is not the stackoverflow", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22927, "title": "Feature request: Import position of dense_to_sparse and sparse_to_dense.", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7 \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: No\r\n- **GCC/Compiler version (if compiling from source)**: No\r\n- **CUDA/cuDNN version**: cuda9.0/cuDNN7.0\r\n- **GPU model and memory**: 1080ti\r\n- **Exact command to reproduce**: No\r\n\r\n---- \r\n\r\n\r\nFirst of all, I have to appreciate your development. I have a trivial request related to python interface.\r\nCurrent implementation is the following,\r\n\r\n```python\r\ntf.sparse_to_dense\r\ntf.contrib.layers.dense_to_sparse\r\n```\r\n\r\nHowever, This implementation is counterintuitive. I think the following code is more intuitive.\r\n\r\n```python\r\ntf.sparse_to_dense\r\ntf.dense_to_sparse\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I found contrib will be moved to outside repository in tensorflow 2.0 road map. \r\n * https://www.tensorflow.org/community/roadmap\r\n\r\nI hope tensorflow API will be more user-friendly in tensorflow 2.0. So, this issue wil be solved in  tensorflow 2.0.\r\n\r\n\r\n"]}, {"number": 22925, "title": "\"#warning \"crt/link.stub is an internal header\" error when compiling TensorFlow w/ CUDA 10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 53faa313b7628cd8c9fbb836544cc6482cafb7a4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: CUDA 10.0, cudnn 7.3\r\n- **GPU model and memory**: V100\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nyes '' | TF_NEED_CUDA=1 TF_CUDA_VERSION=10.0 CUDNN_INSTALL_PATH=/usr/local/cudnn NCCL_INSTALL_PATH=/usr/local/nccl_2.3.4-1+cuda10.0_x86_64  ./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nStarting at commit 53faa313b7628cd8c9fbb836544cc6482cafb7a4, when trying to compile TensorFlow with CUDA 10.0, I get an error stating \"#warning \"crt/link.stub is an internal header file and must not be used directly\". The full error output is:\r\n\r\n```\r\nERROR: /home/reedwm/.cache/bazel/_bazel_reedwm/f53335a0bee8ade861f80a7955d79beb/external/nccl_archive/BUILD.bazel:139:1: C++ compilation of rule '@nccl_archive//:device_code' failed (Exit 1)\r\nbazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:19:2: warning: #warning \"crt/link.stub is an internal header file and must not be used directly.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [-Wcpp]\r\n #warning \"crt/link.stub is an internal header file and must not be used directly.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\r\n  ^~~~~~~\r\nbazel-out/k8-opt/bin/external/nccl_archive/device_code.cc: In function 'void __cudaRegisterLinkedBinary(const __fatBinC_Wrapper_t*, void (*)(void**), void*)':\r\nbazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:140:5: error: '__NV_EXTRA_INITIALIZATION' was not declared in this scope\r\n     __NV_EXTRA_INITIALIZATION\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~\r\nbazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:144:5: error: '__NV_EXTRA_FINALIZATION' was not declared in this scope\r\n     __NV_EXTRA_FINALIZATION\r\n     ^~~~~~~~~~~~~~~~~~~~~~~\r\nbazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:145:23: warning: statement has no effect [-Wunused-value]\r\n     for (__i = 0; __i < NUM_PRELINKED_OBJECTS; ++__i) {\r\nbazel-out/k8-opt/bin/external/nccl_archive/device_code.cc:145:53: error: expected ';' before ')' token\r\n     for (__i = 0; __i < NUM_PRELINKED_OBJECTS; ++__i) {\r\n                                                     ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n```\r\nThis does not occur before 53faa313b7628cd8c9fbb836544cc6482cafb7a4. Letting TensorFlow download and build NCCL does not resolve this issue.\r\n\r\n@chsigg, can you resolve this?", "comments": ["Same exact error occurs with Python 3.7 as well.", "same here. Highly appreciate fixing it!", "device_code.cc is actually a file from a template that ships with the CUDA SDK (cuda/bin/crt/link.stub). I will prepare a CL to replace that include along with the other substitutions. In the meantime, you can install NCCL and specify the version and install location during ./configure.", "Nagging Assignee @chsigg: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This was fixed by 8c072a519e2beed483adb361a9be934a47bee366."]}, {"number": 22924, "title": "Fix indentation in CRF", "body": "- Fixed the wrong indentation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @case540: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry @case540 what is status of this PR?\r\nCan I close it?", "Huh, the internal CL got reverted for some reason. No idea what happen. WIll start the copybara migration thing again."]}, {"number": 22923, "title": "r1.12-rc1 cherry-pick request: Update XlaSort to match the underlying HLO.", "body": "PiperOrigin-RevId: 215745043\r\nPiperOrigin-RevId: 215917470\r\n\r\nThis PR includes cherrypicks of the above two PiperOrigin CLs.  The cherrypick we really want is the second one (215917470), the first one (215745043) is necessary to avoid conflicts.\r\n\r\n------\r\ncl/215745043\r\nRemove CHECKs from HloInstruction constructors.\r\nMove these checks to RET_CHECKs in the HloVerifier. Added a new visitor class\r\nInstructionVerifier inside of hlo_verifier.cc for handling these random\r\nnon-result-shape verifications.\r\n\r\n------\r\ncl/215917470\r\nUpdate XlaSort to match the underlying HLO.", "comments": ["The `Ubuntu {CC,Python2,Python3,contrib}` and `Windows Bazel` tests are failing due to infrastructure issues, which should be fixed via #22904\r\n\r\nSubmitting this now, so we can get some early testing in."]}, {"number": 22922, "title": "r1.12-rc1 cherry-pick request: Make sure that all operands and outputs of Sort have the same layout.", "body": "PiperOrigin-RevId: 215324035\r\nPiperOrigin-RevId: 215515611\r\nPiperOrigin-RevId: 216726796\r\n\r\nThis PR includes cherrypicks of the above three PiperOrigin CLs.  All three are necessary to avoid conflicts.  The cherrypick we really want is the last one (216726796), the other two are coming along for the ride.\r\n\r\n------\r\ncl/215324035\r\n[XLA] Migrate from gtl::FlatSet to absl::flat_hash_set\r\n\r\n------\r\ncl/215515611\r\n[XLA] Modify the function that determines whether an instruction can change\r\nlayout so that it can be used by the HLO verifier.\r\n\r\nChange the function to a static member function of the LayoutAssignment class.\r\n\r\nAdd an std::function member to LayoutAssignment to store the function object\r\npassed down from the backend compiler class and use it to decide whether an\r\ninstruction can change layouts.\r\n\r\nFix affected test cases.\r\n\r\n------\r\ncl/216726796\r\n\r\nMake sure that all operands and outputs of Sort have the same layout.\r\n\r\nAlso fix the DotLayout test, it would pass even when commenting out the dot specific logic in GpuLayoutAssignment.\r\n", "comments": ["FYI the `Ubuntu {CC,Python2,Python3,contrib}` failures are due to infrastructure issues that are being fixed via #22904\r\n\r\nThis PR is pretty huge, but I'm merging it now, so that we can kick off a round of full tests to identify other issues."]}, {"number": 22921, "title": "r1.12-rc1 cherry-pick request: Changes NMS XLA implementation to match that of CPU.", "body": "PiperOrigin-RevId: 216796208", "comments": ["The failing `Ubuntu {CC,Python2,Python3,contrib}` and `MacOS Python2 and CC` tests are infrastructure failures that will be fixed by #22904.\r\n\r\nI'm merging this in now, so that we can get some early testing."]}, {"number": 22920, "title": "LocalMaster WaitForNotification cause master to hang", "body": "Could not understand why here we need to WaitForNotification after timeout?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/local_master.cc#L39\r\n\r\nThis wait for notification could cause master or worker could not exit normally, the later exit one will hang at this step, because nobody will give them a message.\r\n\r\nThe comment says that the call has borrowed pointers to request and response messages, I can not understand the call means what? which object and which class? and where it borrows its pointers?", "comments": ["@mrry ", "I am trying to remove the line, but the program cored. I set operation_timeout_in_ms in tf.ConfigProto to 20*1000.", "I use the estimator interface with 1 ps, 1 master and 1 worker to train an object detection task.\r\nI really appreciate your help @ymodak ", "@chengmengli06 I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Can you please provide a small repro describing your issue?\r\nAlso provide following information. Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n", "This is by design: if we do not wait for the notification, the callback may be called later and it will access memory (local variables captured in the callback) that has already been freed. The request and response objects are owned by the caller, which is the client in the `LocalMaster` case, but could be the gRPC runtime in other cases.", "Why do not mark this as cancelled, and remove the registered callback function? @mrry ", "That's not how the gRPC-style methods work: the callback `std::function` is no longer accessible to the caller, so the caller must wait for it to be called (and can use `call_options->StartCancel()` to make that happen sooner)."]}, {"number": 22919, "title": "r1.12-rc1 cherry-pick request: TPUEstimator: Initialize dataset iterators in parallel.", "body": "PiperOrigin-RevId: 216796462", "comments": []}, {"number": 22918, "title": "build failed, ubuntu 16.04 with python 3.7", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04.4 x64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.12.0rc0\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**: 0.18.0rc9\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 10.0 / 7.3\r\n- **GPU model and memory**: gtx1080Ti GDDR5X 11GB X 7\r\n- **Exact command to reproduce**: \r\nconfigure and build\r\n\r\n\r\n### Describe the problem\r\nconfigure and build\r\n\r\n### Source code / logs\r\nERROR: /home/wmind5/.cache/bazel/_bazel_wmind5/86e1c308a3d29b47174eb34b7539b75b/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc: In function \u00e2\u0080\u0098bool google::protobuf::python::descriptor::_GetItemByKey(google::protobuf::python::PyContainer*, PyObject*, const void**)\u00e2\u0080\u0099:\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from \u00e2\u0080\u0098const char*\u00e2\u0080\u0099 to \u00e2\u0080\u0098char*\u00e2\u0080\u0099 [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                                             ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: note: in expansion of macro \u00e2\u0080\u0098PyString_AsStringAndSize\u00e2\u0080\u0099\r\n         if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {\r\n             ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from \u00e2\u0080\u0098const char*\u00e2\u0080\u0099 to \u00e2\u0080\u0098char*\u00e2\u0080\u0099 [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                                             ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: note: in expansion of macro \u00e2\u0080\u0098PyString_AsStringAndSize\u00e2\u0080\u0099\r\n         if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {\r\n             ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \u00e2\u0080\u0098-Wno-writable-strings\u00e2\u0080\u0099\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/wmind5/repo/tensorflow/tensorflow/python/estimator/api/BUILD:12:1 C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nINFO: Elapsed time: 142.995s, Critical Path: 18.81s\r\nINFO: 482 processes: 482 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["@alanpurple You can follow the [instructions](https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/) here to install TensorFlow with CUDA 10 ", "@wt-huang \r\nis that link relate to any information about python 3.7 bug?\r\n\r\nI've always built tensorflow from source and no problem with python 3.6", "@alanpurple Officially TensorFlow doesn't support python 3.7 yet as shown in [installation guide](https://www.tensorflow.org/install/source_windows) due to protobuf incompatibility. Please use python 3.6 for now.", "@wt-huang \r\n\r\nthank you, so this is all because of protobuf", "Great, glad it worked out.", "Are all your questions answered?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22917, "title": "r1.12-rc1 cherry-pick request: Revert from 'global_steps/sec' back to 'global_step/sec'", "body": "PiperOrigin-RevId: 216598193", "comments": []}, {"number": 22916, "title": "Allows for new tf.cond to deal with functions that return nested outputs tensors", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "The email I used was not yet verified by my github account. It should be verified now. Please let me know if there are any other issues.", "Changed my account's primary email, please check again! "]}, {"number": 22915, "title": "r1.12-rc1 cherry-pick request: Query whether to enable XLA support on MacOS with no as a default", "body": "[This originated as a github PR]\r\n\r\nPiperOrigin-RevId: 216774289", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 22914, "title": "r1.12-rc1 cherry-pick request: Fix bug in MklSlice op when allocating output tensor.", "body": "Intel MKL bug fix cherry-pick. CopyFrom failures in MklToTf op and regression can happen (in the TF-MKL build) if not fixed.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I actually haven't contributed to this PR. Just to clear the CLA:\r\nI'm okay with my commit(s) being contributed to this project. \r\n\r\n@yiqianglee, could you please help confirm that you're fine with your commits being contributed here? Thank you!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@penpornk the CLA status occurs because you're doing the cherrypick, and you're a Googler, so the system gets confused.  I've just manually overridden it.", "I'm okay with my commit(s) being contributed to this project. Thanks!"]}, {"number": 22913, "title": "Create GitHub_Issues_Policy.md", "body": "", "comments": []}, {"number": 22912, "title": "_create_c_op: copy over device placement", "body": "Fixes the following problem \r\n```\r\nnode_def = tf._NodeDef(..., device='/device:GPU:0')\r\nop = tf.Operation(node_def, ...)\r\nop.device  # no device\r\n```", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Why do you need to call _NodeDef directly? It's a private API, so I'm wary of adding extra code to support this unintended use case.", "I don't need to. It's just there to create a NodeDef protobuf message\ndefined in\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/node_def.proto.\nI could have done it without _NodeDef.\n\nOn Thu, Oct 11, 2018, 4:52 PM Skye Wanderman-Milne <notifications@github.com>\nwrote:\n\n> Why do you need to call _NodeDef directly? It's a private API, so I'm wary\n> of adding extra code to support this unintended use case.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22912#issuecomment-429158688>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALPcHfrIb2TolErZC4VQSCfZqNwYSfn-ks5uj9nYgaJpZM4XYiNQ>\n> .\n>\n", "To give some background, we initially noticed issues with misplaced ops in multi-gpu training when we upgraded from 1.4 to 1.9. Some ops belonging to tower N, which used to run on GPU N, are now running on GPU 0, causing GPU 0 to run out of memory and a big slowdown.\r\n\r\nWe examined our code and verified that we called tf.device correctly when creating the ops. Further investigation revealed that ops created from [graph_editor.copy_with_input_replacements](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/graph_editor/transform.py#L630) didn't have devices assigned when the graph was constructed. `graph_editor.copy_with_input_replacements` replicates existing ops and then makes modifications. To replicate an op, it\r\n\r\n- [makes a copy of the original node's node_def](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/graph_editor/transform.py#L156), and\r\n- [calls tf.Operation() with it](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/contrib/graph_editor/transform.py#L176). \r\n\r\nThe resulting op had the original op's device (`op.device`) up until v1.7. What changed between 1.7 and 1.8 was that the default for `_USE_C_API` changed from False to True. It being True means `g._scoped_c_graph` is now [always set](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/python/framework/ops.py#L2908-L2913), and that activates [this branch](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/python/framework/ops.py#L1748-L1756), which sets `self._c_op` for the op using `_create_c_op`, which doesn't copy over the device assignment in `node_def`. Then, when we call `op.device`, we hit [this branch](https://github.com/tensorflow/tensorflow/blob/v1.9.0/tensorflow/python/framework/ops.py#L1894-L1895), which has no knowledge of the device in the original node_def that was used to create this op.\r\n\r\nWith this fix, we are no longer seeing misplaced ops, and training speed is back to what it was before.", "Ah, thanks for the explanation and for digging into this problem!\r\n\r\nCan you add a unit test to ops_test.py? You can just create an Operation from a NodeDef and check that Operation.device returns the right value.", "Unit test added.", "Thanks for contributing!"]}, {"number": 22911, "title": "1.12-rc1 cherry-pick request: Fix use-of-uninitialized-value bug for bfloat16 in MatrixBandPart.", "body": "MatrixBandPart ends up calling bfloat16() to create a zero value, but the implementation of bfloat16::bfloat16() returned an uninitialized value, not zero.\r\n\r\nPiperOrigin-RevId: 216761660", "comments": []}, {"number": 22910, "title": "1.12-rc1 cherry-pick request: Fix Keras support in Python 3.", "body": "PiperOrigin-RevId: 216770808", "comments": []}, {"number": 22909, "title": "[WZ] tmp dir for intermediate NVCC source files", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 22908, "title": "r1.5 tf_version_script.lds not found", "body": "### System information\r\n== cat /etc/issue ===============================================\r\nLinux sc-phy-370909L 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux sc-phy-370909L 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.15.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.6.0)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nA stock tensorflow r1.5 build fails with \"tf_version_script.lds not found.\"\r\n\r\n### Source code / logs\r\nERROR: /home/amcp011/Documents/tensorflow/tensorflow/python/BUILD:3059:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/amcp011/.cache/bazel/_bazel_amcp011/39f5672abd825ffe3e88afa17f90eb9e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/amcp011/local/bin:/home/amcp011/bin:/home/amcp011/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/amcp011/anaconda3/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /usr/bin/gcc -shared -o bazel-out/k8-py3-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow' -Lbazel-out/k8-py3-opt/bin/_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow -Wl,--version-script //tensorflow:tf_version_script.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/k8-py3-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)\r\ngcc: error: //tensorflow:tf_version_script.lds: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 97.480s, Critical Path: 10.96s\r\nINFO: 2 processes: 2 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "No: Have I written custom code\r\nUbuntu 16.04: OS Platform and Distribution\r\ngithub r1.5: TensorFlow installed from\r\nr1.5: TensorFlow version\r\n?: Bazel version\r\nN/A: CUDA/cuDNN version\r\nN/A: GPU model and memory\r\nProvided above: Exact command to reproduce\r\nN/A: Mobile device", "with tensorflow 1.5, you need a very old version of bazel to build.\r\nI would recommend checking workspace file, and using the minimum allowed version of bazel.\r\n\r\nAs this is a more than 1 year old branch, I will close this issue, we do not provide active support for old branches.", "I have been trying to compile tensorflow from github.  I thought r1.5 was the latest stable release.  Is this not so?  What tag should I use?  I need to be able to use kera.", "You should use `v1.11.0` tag. For full instructions, please see https://www.tensorflow.org/install/source"]}, {"number": 22907, "title": "1.12-rc1 cherry-pick request: TPUMirroredVariable device and init fixes.", "body": "Add 'device' property to TPUMirroredVariable, so tf.train.init_from_checkpoint can be supported.\r\n\r\nIn TPUMirroredVariable, when setting _initializer_op and _initial_value attributes, set the attributes of all the contained variables. This fixes a bug that tf.train.init_from_checkpoint doesn't overwrite the initialization values correctly for TPUMirroredVariable.\r\n\r\nPiperOrigin-RevId: 215843249\r\nPiperOrigin-RevId: 216429476", "comments": []}, {"number": 22906, "title": "remove .bazelrc from git tree", "body": "This PR removes .bazelrc from the git tree. Since it is modified during the configure, in essence it is a generated file and should not have been placed into the tree in the first place. Instead this PR adds a new file bazel_defaults and uses it to generate .bazelrc file. Also certain flags are such as -c opt and some environment variables that are not necessarily valid under all circumstances are disabled with this PR. Since these are mostly platform specific or override user command-line flags, these should be added with configuration prefixes rather than globally.\r\nWith this PR .bazelrc is created from template file and removed from git tree.", "comments": ["@gunan I remembered we enabled sandboxing for ourselves, but did we consciously turn it off for others? It does tend to cause trouble.\r\n\r\nIs this really a mistake or do we need it to avoid having to run configure?", "@perfinion @meteorcloudy @angersson as they were involved in moving bazelrc from under tools to root.\r\n\r\n@martinwicke I do not think we ever successfully enabled sandboxing across the board.\r\nI am not aware of any issues that can cause for running configure.\r\n\r\n", ".bazelrc was moved in https://github.com/tensorflow/tensorflow/pull/21374\r\nMy plan is to wait till 0.18.0 is released (should be monday) since it has try-import then bump the min version to 0.18.0 and add \"try-import %workspace%/.tf_configure.bazelrc\" and remove the bit in ./configure.py that edits the file.\r\n\r\nIf waiting till monday doesn't work tho, this still isn't the best way to fix it.\r\nIt looks like 0.18.0-rc9 temporarily added back parsing of tools/bazel.rc (https://github.com/bazelbuild/bazel/issues/6321). So we could move .bazelrc back to tools/ now and then when 0.18.0 is out we re-do this and use \"try-import .tf_configure.bazelrc\" and \"try-import .bazelrc.user\". This bazelrc cleanup is definitely good long term its unfortunately a bit of a mess during the transition. I'd rather just get it all over with asap tho."]}, {"number": 22905, "title": "Update issue templates", "body": "", "comments": []}, {"number": 22904, "title": "r1.12-rc1 cherry-pick request: Update to new RBE toolchain with Clang 8.0.0 r340178.", "body": "PiperOrigin-RevId: 216590129\r\n\r\nFix lstm_test&layer_norm_lstm_test w/ Clang 8.0.0\r\nPiperOrigin-RevId: 216475683\r\n\r\n\r\nFix lite/kernels:add_test for Clang 8.0.0\r\nPiperOrigin-RevId: 216455772\r\n\r\n\r\nFix mul_test with Clang 8.0.0\r\nPiperOrigin-RevId: 216570443\r\n\r\n\r\nThis is to bump the CI image so that Kokoro builds pass on release branch.\r\nAlso cherry-pick three tflite test fixes that failed in clang 8.0.0.", "comments": ["@MarkDaoust Hi Mark, do you know what might be causing the build_docs_test to fail here:\r\n======================================================================\r\nERROR: testBuildDocs (__main__.BuildDocsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/tools/docs/build_docs_test.py\", line 64, in testBuildDocs\r\n    raise RuntimeError(msg)\r\nRuntimeError:           Modules nested too deep:\r\ntf.contrib.estimator.python.estimator.head.array_ops.common_shapes.errors.c_api.ctypes.util\r\n\r\n\r\nThanks!", "Wow. It doesn't make any sense that this PR would cause this. This is a pure-python test.\r\n\r\nThis looks just like a bug @case540 and I hit with that estimator migration he's working on.\r\n\r\nLast time, we saw that a python module importing implementation details from estimator could cause this.\r\n\r\nsomething like:\r\n\r\n```\r\nfrom tensorflow.contrib.estimator.python import estimator \r\n```\r\n\r\nInjects the \"python\" module back into the `tf.contrib.estimator` namespace.\r\nBut I really don't understand why this would be happening now.\r\n\r\nTry adding `\"tf.contrib.estimator\":[\"python\"]` to [this dict](https://github.com/yifeif/tensorflow/blob/cherrypicks_IJGFA/tensorflow/tools/docs/generate_lib.py#L228).", "Thanks for taking a look @MarkDaoust!\r\nIt is a bit odd. There is no python version change with the image update in this PR either.", "FYI I just merged the recent changes to the branch (shown earlier in this comment thread), and now all presubmits have passed.  None of the changes that were merged should have affected the build_docs_test, but then again, this PR shouldn't have affected build_docs_test in the first place.  ;)\r\n\r\nI'm merging this in, since it fixes TF 1.12 presubmits.  Or at least it enables them to pass at least some of the time; we can investigate the weird build_docs_test separately."]}, {"number": 22903, "title": "tfcompile of a fitted keras model", "body": "Hello,\r\nI want to define a simple model in keras, fit it, then tfcompile it.\r\n\r\n    import tensorflow as tf\r\n    from tensorflow.python.keras.models import Sequential\r\n    from tensorflow.python.keras.layers import Dense\r\n    model = Sequential()\r\n    model.add(Dense(input_dim=X.shape[1], units=hidden_dim))\r\n    model.add(Dense(units=Y.shape[1], activation='linear'))\r\n    model.compile(loss='mse', optimizer='adam')\r\n    model.fit(X, Y, batch_size=100)\r\n\r\nCan you please give an example of how to tfcopile this fitted model, or at least give some steps (for example, should the graph be frozen before using tfcompile?)\r\n\r\n\r\n", "comments": ["Please take a look at [basic classification example](https://www.tensorflow.org/tutorials/keras/basic_classification) on TensorFlow's website to get started.", "Thanks for your answer. My problem is after the model is fit, I want to extract the graph, use tfcompile and produce a .so\r\nI am having these error messages \r\n\r\n    E tensorflow/compiler/tf2xla/graph_compiler.cc:116] Executor failed to create kernel. Not found: No registered 'VariableV2' OpKernel for XLA_CPU_JIT devices compatible with node dense_2/bias = VariableV2[_class=[\"loc:@dense_2/bias\"], container=\"\", dtype=DT_FLOAT, shape=[8], shared_name=\"\"]()\r\n        .  Registered:  device='CPU'\r\n\r\n         [[Node: dense_2/bias = VariableV2[_class=[\"loc:@dense_2/bias\"], container=\"\", \r\n    dtype=DT_FLOAT, shape=[8], shared_name=\"\"]()]]\r\n    2018-10-12 04:17:56.359405: F tensorflow/compiler/aot/tfcompile_main.cc:140] Non-OK-status: status status: Not found: No registered 'VariableV2' OpKernel for XLA_CPU_JIT devices compatible with node dense_2/bias = VariableV2[_class=[\"loc:@dense_2/bias\"], container=\"\", dtype=DT_FLOAT, shape=[8], shared_name=\"\"]()\r\n        .  Registered:  device='CPU'\r\n\r\n         [[Node: dense_2/bias = VariableV2[_class=[\"loc:@dense_2/bias\"], container=\"\", dtype=DT_FLOAT, shape=[8], shared_name=\"\"]()]]", "OK, I figured out that I needed to freeze the Keras graph before calling the tfcompile\r\nSo I do this:\r\n\r\n    session = tf.keras.backend.get_session()\r\n    output_node_names = [node.op.name for node in model.outputs]\r\n    graphdef = tf.graph_util.convert_variables_to_constants(session, session.graph_def, output_node_names)\r\n  \r\n I need indeed to transform variables to constants> Now how can I get a graph from the graphdef, and how can I get the input and output of that graph. Here is what I did, but does not seem to work\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graphdef)\r\n        inp = graph.get_tensor_by_name(graph.get_operations()[0].name+':0')\r\n        out = graph.get_tensor_by_name(graph.get_operations()[-1].name+':0')\r\n\r\n    ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22902, "title": "Linker error when compiling from head of master on MacOS", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave and High Sierra\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.17..2-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: \r\n```\r\nclang -v\r\nApple LLVM version 10.0.0 (clang-1000.11.45.2)\r\n```\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\nbazel test --test_output=all --nocache_test_results --config=opt tensorflow/compiler/tests:cpu_tests\r\n```\r\n(Configured with XLA support enabled)\r\n### Describe the problem\r\nWhen trying to run the above command I get:\r\n```\r\nld: can't open -exported_symbols_list file: -filelist\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\n### Source code / logs\r\nAfter applying this diff:\r\n```diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex df15914233..8de9c7cfdd 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -1636,8 +1636,7 @@ def tf_py_wrap_cc(\r\n     )\r\n     extra_linkopts = select({\r\n         \"@local_config_cuda//cuda:darwin\": [\r\n-            \"-Wl,-exported_symbols_list\",\r\n-            \"$(location %s.lds)\" % vscriptname,\r\n+            \"-Wl,-exported_symbols_list,$(location %s.lds)\" % vscriptname,\r\n         ],\r\n         clean_dep(\"//tensorflow:windows\"): [],\r\n         \"//conditions:default\": [\r\n```\r\n\r\nand running `bazel test --test_output=all --nocache_test_results --config=opt tensorflow/compiler/tests:cpu_tests`\r\n\r\nI get lots of warnings during linking:\r\n```\r\nld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::AWSSha256HMACOpenSSLImpl>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/platform/s3/libaws_crypto.lo(aws_crypto.o)\r\nld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::AWSLogSystem>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/platform/s3/libaws_logging.lo(aws_logging.o)\r\nld: warning: cannot export hidden symbol std::__1::shared_ptr<tensorflow::Notification>::__enable_weak_this(...) from bazel-out/darwin-opt/bin/tensorflow/core/grappler/libutils.a(utils.o)\r\n```\r\n\r\nAnd then when the tests are ran I get errors such as:\r\n\r\n```\r\n2018-10-11 13:02:47.136859: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n*** Received signal 10 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n0   libtensorflow_framework.so          0x00000001200870a7 _ZN10tensorflow7testingL17StacktraceHandlerEiP9__siginfoPv + 183\r\n1   libsystem_platform.dylib            0x00007fff66049b3d _sigtramp + 29\r\n2   ???                                 0x0000000000000000 0x0 + 0\r\n3   _pywrap_tensorflow_internal.so      0x00000001143583a9 _ZNSt3__110__function6__funcIZN3xla3cpu13CpuExecutable24ExecuteAsyncOnStreamImplEPKNS2_27ServiceExecutableRunOptionsEN4absl4SpanIKPKNS2_12ShapedBufferEEEPNS2_19HloExecutionProfileEE12AsyncRunTaskNS_9allocatorISH_EEFvvEEclEv + 73\r\n4   libtensorflow_framework.so          0x000000012032bd09 _ZNSt3__110__function6__funcIZN15stream_executor4host10HostStream11EnqueueTaskENS_8functionIFvvEEEE12NotifiedTaskNS_9allocatorIS8_EES6_EclEv + 25\r\n5   libtensorflow_framework.so          0x00000001200641da _ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi + 618\r\n6   libtensorflow_framework.so          0x0000000120063e6f _ZNSt3__110__function6__funcIZN10tensorflow6thread16EigenEnvironment12CreateThreadENS_8functionIFvvEEEEUlvE_NS_9allocatorIS8_EES6_EclEv + 47\r\n7   libtensorflow_framework.so          0x0000000120088a30 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEENS_8functionIFvvEEEEEEEEPvSB_ + 48\r\n8   libsystem_pthread.dylib             0x00007fff6605233d _pthread_body + 126\r\n9   libsystem_pthread.dylib             0x00007fff660552a7 _pthread_start + 70\r\n10  libsystem_pthread.dylib             0x00007fff66051425 thread_start + 13\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\ttensorflow::testing::StacktraceHandler(int, __siginfo*, void*)\r\n\t_sigtramp\r\n\r\n\tstd::__1::__function::__func<xla::cpu::CpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::Span<xla::ShapedBuffer const* const>, xla::HloExecutionProfile*)::AsyncRunTask, std::__1::allocator<xla::cpu::CpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::Span<xla::ShapedBuffer const* const>, xla::HloExecutionProfile*)::AsyncRunTask>, void ()>::operator()()\r\n\tstd::__1::__function::__func<stream_executor::host::HostStream::EnqueueTask(std::__1::function<void ()>)::NotifiedTask, std::__1::allocator<stream_executor::host::HostStream::EnqueueTask(std::__1::function<void ()>)::NotifiedTask>, void ()>::operator()()\r\n\tEigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\tstd::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()()\r\n\tvoid* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*)\r\n\t_pthread_body\r\n\t_pthread_start\r\n\tthread_start\r\n*** End stack trace ***\r\n```\r\n\r\nNote that `cc_wrapper.sh` calls `gcc` however I haven't changed it so it still points at Apple LLVM.\r\n\r\nNote that there is an issue #22759 however that one uses custom code, where as this is HEAD of master.", "comments": ["Is this still an issue?", "I'm seeing this now at 492cbeb448590084f975cba8896f2a85df23b036 with bazel 0.19.0.", "Still a problem (I've updated to the latest Xcode as well - still an issue)", "I think this is an issue with bazel 0.19. \r\nWe are not importing all the bazelrc files we need.\r\n\r\n@meteorcloudy has a fix for this in flight.", "I'm running 0.18\r\n```\r\n$ bazel version\r\nExtracting Bazel installation...\r\nBuild label: 0.18.1-homebrew```", "I am running into this issue as well while building with the master branch on MacOS High Sierra and Python 3.6.\r\n\r\nbazel version: `0.18.1-homebrew`\r\nclang version: `Apple LLVM version 10.0.0 (clang-1000.10.44.2)`\r\n\r\nRunning `bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package` gives the error\r\n\r\n```\r\nERROR: /Users/hpang/tensorflow/tensorflow/python/BUILD:3974:1: Couldn't build file tensorflow/python/_pywrap_tensorflow_internal.so: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)\r\nld: can't open -exported_symbols_list file: -filelist\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n", "ERROR: /Users/tianchuangxin1/tensorflow/tensorflow/python/BUILD:3960:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)\r\nld: can't open -exported_symbols_list file: -filelist\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.010s, Critical Path: 0.59s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully", "Same problem here but not only on master but also r1.12 and r1.11. At the moment I don't know any version I could compile. Bazel is 0.18.", "same problem with bazel 0.18.1\r\n\r\n/Users/sunpeng/build/tensorflow/tensorflow/python/BUILD:3865:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): cc_wrapper.sh failed: error executing command\r\n...\r\nld: can't open -exported_symbols_list file: -filelist\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)", "Ping @meteorcloudy. Sorry to be a pest, but it'd be really nice to be able to compile tensorflow on macOS. Even a workaround would be welcome.", "Sorry, I'm not very familiar with the clang compiler. @mhlopko Do you have any idea on this?", "I just tried again. Not sure what changed, but I can at least compile now. A bunch of tests are still failing, but I don't know whether that is expected--this is my first attempt to install in a few years.", "I'm having the same problem:\r\n```\r\nld: can't open -exported_symbols_list file: -filelist\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 37,565s, Critical Path: 10,44s\r\nINFO: 2 processes: 2 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nwhen using:\r\n**Tensorflow:** 1.9.0\r\n**Bazel:** 0.16.1\r\n**Clang:** Apple LLVM version 10.0.0 (clang-1000.10.44.4)\r\n**OS:** Mac OS X 10.14.2\r\n\r\nI've tried changing `tensorflow.bzl` as @georgepaw but it didn't work.\r\n\r\nDoes anyone know a fix for this?", "Hi, can I see the complete command line that fails? You can run bazel with `--subcommands` to get all command lines. Thanks!", "I'd changed to bazel 0.17.2 on my last attempts.\r\n\r\nI've attached the log from my last attempt with `--subcommands`:\r\n[tensorflow.log](https://github.com/tensorflow/tensorflow/files/2685728/tensorflow.log)\r\n", "I'd changed string in tensorflow/BUILD instead of tensorflow.bzl. And it works.", "> I'd changed string in tensorflow/BUILD instead of tensorflow.bzl. And it works.\r\n\r\nThat did the trick. Thank you @momiji7 ", "> I'd changed string in tensorflow/BUILD instead of tensorflow.bzl. And it works.\r\n\r\nhi @momiji7\uff0c I met the same error when I compile on my Mac. May I know your solution?", "I was getting the same error with bazel 0.22.0 on my Mac.  Full command was\r\n\r\n    bazel build --jobs=6 --verbose_failures -c opt --copt=-mavx --copt=-msse4.2 --define=grpc_no_ares=true --copt=-mavx2 --copt=-mfma //tensorflow:libtensorflow_cc.so\r\n\r\nI think what @momiji7 & @fgsalomon did was to comment out the lines in tensorflow/BUILD that had \"exported_symbols_list\" in them (and the lines immediately following).\r\n\r\nWhen I did that and re-ran bazel, I was able to get tensorflow to link without errors. Not sure if this has produced a *usable* version of the tf library, but we'll see. ", "For Tensorflow v1.9.0 what I did was change this part in `tensorflow/BUILD`:\r\n\r\n```\r\ntf_cc_shared_object(\r\n    name = \"libtensorflow.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n            \"$(location //tensorflow/c:exported_symbols.lds)\",\r\n            \"-Wl,-install_name,@rpath/libtensorflow.so\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"$(location //tensorflow/c:version_script.lds)\",\r\n        ],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/c:c_api\",\r\n        \"//tensorflow/c:c_api_experimental\",\r\n        \"//tensorflow/c:exported_symbols.lds\",\r\n        \"//tensorflow/c:version_script.lds\",\r\n        \"//tensorflow/c/eager:c_api\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n\r\ntf_cc_shared_object(\r\n    name = \"libtensorflow_cc.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list\",  # This line must be directly followed by the exported_symbols.lds file\r\n            \"$(location //tensorflow:tf_exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"$(location //tensorflow:tf_version_script.lds)\",\r\n]\r\n```\r\ninto:\r\n```\r\ntf_cc_shared_object(\r\n    name = \"libtensorflow.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/c:exported_symbols.lds)\",\r\n            \"-Wl,-install_name,@rpath/libtensorflow.so\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"$(location //tensorflow/c:version_script.lds)\",\r\n        ],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/c:c_api\",\r\n        \"//tensorflow/c:c_api_experimental\",\r\n        \"//tensorflow/c:exported_symbols.lds\",\r\n        \"//tensorflow/c:version_script.lds\",\r\n        \"//tensorflow/c/eager:c_api\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n\r\ntf_cc_shared_object(\r\n    name = \"libtensorflow_cc.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:darwin\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow:tf_exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//tensorflow:windows_msvc\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script\",  #  This line must be directly followed by the version_script.lds file\r\n            \"$(location //tensorflow:tf_version_script.lds)\",\r\n        ],\r\n    }),\r\n```", "Hi @georgepaw!\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF  and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22902\">No</a>\n"]}, {"number": 22901, "title": "We have removed this restriction. Please check out the code at head, or in the upcoming 1.4 RC. Please reopen if there are issues with it.", "body": "_Originally posted by @martinwicke in https://github.com/tensorflow/tensorflow/issues/12367#issuecomment-335320352_", "comments": ["It used to compile in tf1.8/tf1.11 with a dictionary for features in tf.estimator.export.SendInputReceiver but it is suddenly throwing the \"INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {dict}\" error again. Any idea why it might throw that error when you restart the console?\r\n\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']                                                                   \r\nINFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:                            \r\nINFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; \r\n\r\n# [START serving-function]\r\ndef json_serving_input_fn():\r\n  \"\"\"Build the serving inputs.\"\"\"\r\n  inputs = {'a': tf.placeholder(shape=[None], dtype='string'),\r\n            'b': tf.placeholder(shape=[None], dtype='string'), \r\n            'c': tf.placeholder(shape=[1,10], dtype='string')}\r\n  return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n# [END serving-function]", "@jg577 As mentioned in [#12367](https://github.com/tensorflow/tensorflow/issues/12367#issuecomment-335320352), you can use `SignatureDefs` to get around the restriction. `tf.estimator.export` will loop you back to the restriction.  Please refer to [SignatureDefs in SaveModel for TensorFlow Serving](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/signature_defs.md) for more details.", "Closing, feel free to reopen if problem persists."]}, {"number": 22900, "title": "RTX 2080 issue with Tensorflow", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NaN\r\n- **TensorFlow installed from (source or binary)**: Both \r\n- **TensorFlow version (use command below)**: 1.12.0-rc0-devel-gpu-py3 \r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: Latest\r\n- **GCC/Compiler version (if compiling from source)**: 5.x\r\n- **CUDA/cuDNN version**: 9.0 / 10.0 (Tried Both)    | cuDNN : 7.2.x / 7.3.1\r\n- **GPU model and memory**: RTX Geforce 2080\r\n- **Exact command to reproduce**:  Running the most basic mnist dataset using keras with Tf backend\r\n\r\nWhen I try running the code it starts and ends with a segmentation fault even without start of single epoch. The command nvidia-smi shows maximum usage of its volatile memory. I also restricted the gpu usage by a fraction of 0.5 still the problem persists.  I tried building the source files with cuda 9.0 / cudnn 7.2.1 in the docker environment yet the installed whl package produces the same error.", "comments": ["@tejavoo, Just to let you know, since 2080 is rather new, cuda 9.0 doesn't know anything about it. So try with cuda 10 installation and driver.", "@tejavoo Is this still an issue?", "@ymodak No its been solved and the link to how it was solved is [here](https://medium.com/@saitejadommeti/building-tensorflow-gpu-from-source-for-rtx-2080-96fed102fcca) in a medium article. Thanks to one and all", "Possibly related?  I have a system set up using Keras, TensorFlow, CUDA 9.0 on Ubuntu 18.0.  When running on 2080 Ti, I either get a validation score that is constant 5.0, or if I tweak a bit, inf.  Running the exact same code on a different machine with P100 works correctly (but slower, dammit).  The setup on the second machine is different, though, so could be a software version problem.\r\n\r\nUpdate: I upgraded to TF1.12 and CUDA 10, but unfortunately, the problem persisted.  So it is most likely something else in my case.  Sorry for the noise, but leaving this here in case others have the same problem."]}]