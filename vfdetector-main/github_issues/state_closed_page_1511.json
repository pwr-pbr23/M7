[{"number": 7600, "title": "[Discussion] model intellectual property protection", "body": "Usually for mobile devices (or any devices which are publicly accessible) model deployment, there is a requirement for IP protection. Obviously, the current protobuf format is not desirable.\r\n\r\nXLA can be a great help which will result in binary/so which is difficult for reverse engineering, but the current emphasis is mostly on performance improvement. For example, XLA does not cover the case when there are some customized op/kernels (executed either on standard CPU/GPU or customized accelerators) which could be common in mobile/IoT area.", "comments": ["cc @tatatodd ", "This is not really a feature request but rather a discussion topic. This is more appropriate for the tensorflow discuss google group. Thanks!"]}, {"number": 7599, "title": "cuda::Diagnostician::FindKernelDriverVersion tries to access /proc/driver/nvidia/version on Windows", "body": "Windows 10 64 bit\r\nCUDA 8.0\r\ncuDNN 5.1\r\ntensorflow_gpu 1.0.0\r\n\r\n### problem is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L345\r\n\r\n### the other place which opens the file is disabled on Windows:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L143\r\n\r\n### Logs or other output that would be helpful\r\n\r\n    E ...\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n    E ...\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n    E ...\\stream_executor\\cuda\\cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n    >>>    E ...\\stream_executor\\cuda\\cuda_dnn.cc:404] error retrieving driver version: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version\r\n    E ...\\stream_executor\\cuda\\cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n", "comments": ["Please take a look @mrry. Thanks!", "Yep, that looks like a bug in the error reporting path. Should be a simple fix, so I'm marking it Contributions Welcome.", "Here have a similar issue:\r\n\r\nTotal memory: 4.00GiB\r\nFree memory: 3.36GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 745, pci bus id: 0000:01:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:404] error retrieving driver version: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version", "@jhli973 This problem should be fixed at HEAD. Please try upgrading to a nightly build to see if that fixes the problem. "]}, {"number": 7598, "title": "make tf_upgrade.py more useful", "body": "When we upgrade `tf` code to `version 1.0.0` in a directory tree, some times we also want to copy all the other files from `intree` to the `outtree`\uff0cthis makes it easier to upgrade an entire project.\r\n\r\nSo I add an option named `--copyotherfiles` in `tf_upgrade.py`. Hopefully useful!\r\n\r\n```\r\n# just upgrade the .py files\r\ntf_upgrade.py --intree coolcode --outtree coolcode-upgraded\r\n# after upgrade the .py files, then copy all the other files to the outtree\r\ntf_upgrade.py --intree coolcode --outtree coolcode-upgraded --copyotherfiles True\r\n```", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I think I used the same email address in the git commits as was used to sign the CLA ~~~", "```\r\nFrom ec32f7548836da0cbbe747644c566c6716fd2e7d Mon Sep 17 00:00:00 2001\r\nFrom: tanggefu <tanggefu@mioji.com>\r\n```\r\n\r\nMaybe use git commit --amend to make sure it matches your github email account, which it looks like you signed the CLA with?", "I have confused my `github` account with `gitlab` account, I am sorry, I have pushed a new request with the right email at [here](https://github.com/tensorflow/tensorflow/pull/7601), and I will close this one, thank you very much."]}, {"number": 7597, "title": "Add --pull to docker build to automatically pull the latest image", "body": "Add --pull to docker build to automatically pull the latest image in FROM.", "comments": []}, {"number": 7596, "title": "OpenCL Improvements", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@pelmers can you sign the CLA with the email you used in your git commit?  Our bot surprisingly didn't flag this.\r\n\r\nI believe the others are repeat commiters so we don't need their explicit acknowldgement in every PR (so I've been told).", "also lots of failures, but I assume you're looking at that.", "@vrv submitted my CLA", "Cool, confirmed, thanks!", "Same for @krikru ", "I signed it.", "@lukeiwanski thanks -- you're a repeat contributor so no need to respond in the future :)\r\n\r\nsigncla bot is actually broken for this PR for some reason, i just manually inspected the emails of the others and found they weren't signed.", "@vrv sorry didn't notice your previous post :) ", "Still need @krikru to sign the CLA under the git commit email he used and then we can merge.", "Signed!", "Did you sign with \"krikr808@student.liu.se\" ? that's what you used in your git commit, and I don't see that email registered.", "PS: I've used this script with success to change email when my git commits had wrong email address -- https://help.github.com/articles/changing-author-info/", "@vrv Ah, no, I signed with another email address which I'm going to use on GitHub from here on.\r\n\r\nI created a new pull request from my fork of tensorflow-opencl into tensorflow-opencl in which I have changed the email of my commits to the one I signed the CLA with. Does that work?", "I suspect that unless that email shows up here via Benoit's push, it won't work. :(\r\n\r\nPerhaps you could tell Benoit to git commit --amend your commit with the correct email?", "Now I've sent @benoitsteiner an email.", "Validated the email addresses manually."]}, {"number": 7595, "title": "Shape inference issues after running dynamic_rnn", "body": "I am trying to use dynamic_rnn in the encoder part of seq2seq library. I am using a placeholder for encoder input with dimensions [None, None] to reflect dynamic batch size and time steps. \r\n\r\nThe problem is that the output that I get after the dynamic_rnn run has unknown shape along time dimension which causes issues when I try to use it along with attention. I have been able to overcome it by using the bucket length to set_shape but now that I am trying to ditch bucketing, it seems a little non-trivial. I was trying to use the maximum of sequence lengths but I have not been able to successfully use the value obtained from tf.reduce_max(sequence_length). Any help/suggestions would be much appreciated. Also, I am using version 0.11.", "comments": ["cc @ebrevdo who has substantial experience with it (he'll probably ask you to upgrade to TF 1.0 since there have been changes to sequential stuff)", "Sounds like your encoder inputs have unknown batch_size and time_steps,\nright?  But you expect dynamic_rnn to \"fill in\" the unknown time_steps?\nSince time_steps are not known at graph build time, you cannot fill it into\nthe static shape, even if you use reduce_max.  the result of reduce_max is\na Tensor, which you cannot use to set static shape.\n\nIt sounds like the attention mechanism wants a static number of time steps,\nand is unable to handle variable length outputs.  That or you're somehow\nnot using the attention correctly.\n\nThat said, we hope to have a better decoder + attention mechanism coming\nsoon (next week or two); in tf.contrib.seq2seq, on tensorflow's github\nmaster branch.\n\nOn Thu, Feb 16, 2017 at 3:17 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> cc @ebrevdo <https://github.com/ebrevdo> who has substantial experience\n> with it (he'll probably ask you to upgrade to TF 1.0 first)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280495558>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwOB6j2q_SNkIpu1No9nzufWbCiMks5rdNkGgaJpZM4MDp5g>\n> .\n>\n", "Thanks for such a quick response !\r\nYou are right that the dynamic_rnn can't fill in the unknown time steps but really it should stop at max of seq_len and can thus, fill in this dimension (am I expecting too much ?).\r\nYeah the attention mechanism currently requires the size of input to be known. Thanks for the heads up about the new version. But how are you guys planning on handling this. Is it a matter of upgrading Tensorflow or is there something that can be done in the current version itself ?", "Yup you're expecting too much :)  the dynamic_rnn will stop at the max of\nseq_len, but that max can change from one minibatch to the next!  So it's\nalso not known.\n\nThe new attentional mechanism should be able to handle an unknown sequence\nlength; we'll keep this in mind as we're designing it.\n\nOn Thu, Feb 16, 2017 at 3:47 PM, Shubham Toshniwal <notifications@github.com\n> wrote:\n\n> Thanks for such a quick response !\n> You are right that the dynamic_rnn can't fill in the unknown time steps\n> but really it should stop at max of seq_len and can thus, fill in this\n> dimension (am I expecting too much ?).\n> Yeah the attention mechanism currently requires the size of input to be\n> known. Thanks for the heads up about the new version. But how are you guys\n> planning on handling this. Is it a matter of upgrading Tensorflow or is\n> there something that can be done in the current version itself ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280502372>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw33Nx_vzLSn6SCCUiQiag-Ta7EQks5rdOAEgaJpZM4MDp5g>\n> .\n>\n", "As it turns out, it was just a matter of avoiding certain reshape operations. So I just replaced those by expand_dim operations and things are working fine now. \r\n\r\nUsing dynamic_rnn in encoder and raw_rnn in decoder, I was able to avoid the bucketing shenanigan. Again, thanks for the help and I will close this issue.   ", "Got it.  For posterity, can you tell us the original reshape operations and\nthe new expand_dim versions?  If you can expand_dim and preserve shape,\nthen presumably the reshape should work too.  This may be a shape inference\nbug on our end.\n\nOn Thu, Feb 16, 2017 at 10:54 PM, Shubham Toshniwal <\nnotifications@github.com> wrote:\n\n> As it turns out, it was just a matter of avoiding certain reshape\n> operations. So I just replaced those by expand_dim operations and things\n> are working fine now.\n>\n> Using dynamic_rnn in encoder and raw_rnn in decoder, I was able to avoid\n> the bucketing shenanigan. Again, thanks for the help and I will close this\n> issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280569737>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1oKwlgUsrN05Kveini0ssByb4kGks5rdUQpgaJpZM4MDp5g>\n> .\n>\n", "I will just describe the parts I modified and the code snippet for that piece.\r\n\r\n- attention_states has shape (batch_size, attn_length, attn_size) where attn_length is enc_inp_len\r\nThe first 2 dimensions are unknown and that will cause issue with reshaping ops trying to preserve the first 2 dimensions. However, the reshaping operation used in seq2seq below was easily replaceable by the next line. \r\n`hidden = array_ops.reshape( attention_states, [-1, attn_length, 1, attn_size])` \r\n`hidden = tf.expand_dims(attention_states, 2)`\r\n\r\n- Multiplying attention weights `alpha` with encoder hidden states, `hidden`, requires another reshape in the original code\r\nalpha has shape (batch_size, attn_length) and is calculated via\r\n`alpha = nn_ops.softmax(s)` \r\nThe original code uses the following line to get context vector **c**\r\n`c = math_ops.reduce_sum( \r\n                        array_ops.reshape(alpha, [-1, attn_length, 1, 1]) * hidden,                                       \r\n                        [1, 2])\r\n`\r\nI replaced it via\r\n`alpha = tf.expand_dims(alpha, 2)` \r\n`alpha = tf.expand_dims(alpha, 3)` \r\n`c = math_ops.reduce_sum(alpha * hidden, [1, 2])`\r\n\r\nHope this helps!\r\n\r\n\r\n\r\n", "By the way, since you guys are looking at improving the seq2seq library, there is one mathematical inaccuracy in current implementation. \r\n- The attention mechanism currently considers the whole padded input. \r\n- Even though the padded input won't contribute to context vector due to **0** output vector corresponding to it, it does reduce the probability mass of the actual inputs as it is part of the `softmax` calculation.\r\n- Ideally, there should be a mask applied to the `softmax` output and subsequent re-normalization.  Something along these lines:\r\n`attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)`\r\n`alpha = nn_ops.softmax(s) * attn_mask`\r\n`alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)`\r\n- However, I must say that things work out fine even without it but it's needed to match the math described in research papers.", "Thanks!  Forwarded these two notes (one on slicing to andrew; the other to\na collaborator working on attention).\n\nOn Fri, Feb 17, 2017 at 10:32 AM, Shubham Toshniwal <\nnotifications@github.com> wrote:\n\n> By the way, since you guys are looking at improving the seq2seq library,\n> there is one mathematical inaccuracy in current implementation.\n>\n>    - The attention mechanism currently considers the whole padded input.\n>    - Even though the padded input won't contribute to context vector due\n>    to *0* output vector corresponding to it, it does reduce the\n>    probability mass of the actual inputs as it is part of the softmax\n>    calculation.\n>    - Ideally, there should be a mask applied to the softmax output and\n>    subsequent re-normalization. Something along these lines:\n>    attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)\n>    alpha = nn_ops.softmax(s) * attn_mask\n>    alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)\n>    - However, I must say that things work out fine even without it but\n>    it's needed to match the math described in research papers.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280729677>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5ppHUp3GS1Cckrfwcj5g7ln1I3dks5rdefIgaJpZM4MDp5g>\n> .\n>\n"]}, {"number": 7594, "title": "Branch 147758266", "body": "", "comments": []}, {"number": 7593, "title": "Merge 1.0.0 back to master", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 7592, "title": "Branch 147741833", "body": "", "comments": ["This change breaks Windows build \r\nc:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\lib\\gtl\\flatmap.h(110): error C2061: syntax error: identifier 'difference_type' (compiling source file C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\resource_mgr.cc) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_core_framework.vcxproj]", "yeah, it's fixed internally and i'll push out the fix soon.", "@yifeif @gunan do you know why the windows build isn't part of our presubmit?"]}, {"number": 7591, "title": "Virtualenv setup returns ImportError", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/42274018/virtualenv-setup-returns-importerror?noredirect=1#comment71721085_42274018\r\n\r\n### Environment info\r\nOperating System: OS X El Capitan 10.11.5\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI am trying to install tensorflow with virtualenv, but I get the following error when I run \"virtualenv --system-site-packages tensorflow\". I am currently working this on macOS with python3.5.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"~/tensorflow/lib/python3.5/tempfile.py\", line 45, in <module>\r\n    from random import Random as _Random\r\nImportError: cannot import name 'Random'\r\n...Installing setuptools, pip, wheel...done.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/bin/virtualenv\", line 11, in <module>\r\nsys.exit(main())\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py\", line 713, in main\r\nsymlink=options.symlink)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py\", line 945, in create_environment\r\ndownload=download,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py\", line 901, in install_wheel\r\ncall_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/virtualenv.py\", line 797, in call_subprocess\r\n% (cmd_desc, proc.returncode))\r\nOSError: Command ~/tensorflow/bin/python3.5 - setuptools pip wheel failed with error code 1", "comments": ["Maybe try a different directory as your virtualenv dir i.e.\r\n```\r\nvirtualenv --system-site-packages ~/foo\r\n```\r\nmaybe you have some already existing tensorflow dir that is conflicting. However, this error has nothing to do with tensorflow yet. It means you don't have a working python installation which is a prerequisite for using tensorflow but not something we supprot or have control over. Good luck!"]}, {"number": 7590, "title": "Tutorials on Estimators, Canned Estimators", "body": "Hi, \r\nI checked out the tensorflow dev-summit yesterday and came to know about Estimators and Canned Estimators. Estimators and Canned Estimators are intended to bring simple model-data wrapping capability to tensorflow, I believe. However, I am not able to find a lot of tutorials/docs on the interface for an estimator or documentation on how to write Canned Estimators/Estimators. tf.contrib.learn has estimator modules only for simple ML methods like mlps, linear regressors. I would like to know if there are gists/github code for estimators/canned estimators for cnns/auto-encoders/LSTMS. ", "comments": ["Here are some pointers to get started:\r\n\r\nhttps://www.tensorflow.org/get_started/tflearn\r\nhttps://www.tensorflow.org/tutorials/wide\r\nhttps://www.tensorflow.org/tutorials/wide_and_deep\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn#existing-estimator-implementations\r\n\r\n", "I have checked these. I wanted more of a how-to for writing your own estimator ", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/estimators/index.md", "@aribornstein's answer seems to answer the question completely. If you have a specific implementation question about a specific algorithm, this is best asked on StackOverflow. Unless you think there is a bug in the documentation or the code. Thanks so much!", "@aribornstein Link is broken.", "@aselle link is broken -- this is the top hit for \"wrapping a model in an estimator\".  Might be worth fixin'"]}, {"number": 7589, "title": "test pull", "body": "test pull", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@thankshelen Can you test on a private repo please?"]}, {"number": 7588, "title": "Update Documentation for 1.0.0 Release", "body": "This PR just updates the installation and download links from from release candidate 2 (rc2) to the full 1.0.0 files.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "We are merging back the changes in 1.0 branch to master at #7593, including the binaries URLs. Closing this one. Thanks @seanmarcia for noticing and sending the PR!"]}, {"number": 7587, "title": "R1.0", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 7586, "title": "[CMake] Disable BUILD_TESTING for Eigen.", "body": "We don't actually run these tests, so building them is a waste of time. They can also cause surprising interactions with the environment (e.g. see issue #7374), in cases where some of the test dependencies are available.", "comments": ["Looks like the build is a bit hungover this morning....\r\n\r\n@tensorflow-jenkins test this please!", "@tensorflow-jenkins test this please."]}, {"number": 7585, "title": "Invalid argument error from tf.gather_nd after upgrade to r1.0", "body": "I just upgraded to r1.0 and ran into an issue which I find hard to dissect further. The issue did not occur before when I was using version r0.12. The error message is the following:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [1,4,1700], indices.shape [1,4,2], params_shape [1,73,1700]\r\n\t [[Node: gradients/GatherNd_5_grad/ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](GatherNd_5/indices/_77, gradients/AddN_9/_79, gradients/GatherNd_5_grad/Shape)]]\r\n```\r\n\r\nand the stack trace tells me it stems from the last of the following lines:\r\n\r\n```\r\ndef positional(visible):\r\n    \"\"\"\r\n    :param visible: a tensor of size (batch_size, input_dim, sequence_length) representing the sequence to be optimized\r\n    \"\"\"\r\n    FEET = np.array([4, 5, 8, 9])\r\n    feet_idx = np.array([range(i, i+3) for i in FEET*3])\r\n    batch_size = visible.get_shape()[0].value\r\n    dim = visible.get_shape()[1].value\r\n    seq_length = visible.get_shape()[2].value\r\n    idx_x, idx_y, idx_z = feet_idx[:, 0], feet_idx[:, 1], feet_idx[:, 2]\r\n    idx_x_t = [[[j, i] for i in idx_x] for j in range(batch_size)]\r\n    idx_y_t = [[[j, i] for i in idx_y] for j in range(batch_size)]\r\n    v_feet_x = tf.gather_nd(visible, idx_x_t)\r\n    v_feet_y = tf.gather_nd(visible, idx_y_t) #  stack trace points here\r\n    ...\r\n```\r\nThis function is called to calculate a cost function which is used during an optimization procedure. See below for a minimum working example.  The error, as far as I can tell from the stack trace, does not happen when the graph is built, but when the computation is executed, which is why I'm not sure how I can further narrow down the problem.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04, x64\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and cuDNN 5.1\r\n```\r\n-rw-r--r-- 1 root root   556000 Jan 27 00:48 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan 27 00:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan 27 00:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   415432 Jan 27 00:48 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Jan 27 00:48 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 30 11:39 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\nInstalled from source from revision `16485a3fb5ffcbaa244e55c388e43279d2770982` using bazel 0.4.4\r\n\r\n```\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n```\r\n\r\n### Minimum working example\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef get_costs(velos, seq_length):\r\n    velo_diff = tf.subtract(tf.slice(velos, [0, 0, 1], [-1, -1, seq_length-1]),\r\n                            tf.slice(velos, [0, 0, 0], [-1, -1, seq_length-1]))\r\n    velo_diff_sq = tf.multiply(velo_diff, velo_diff)\r\n    return tf.reduce_mean(velo_diff_sq)\r\n\r\n\r\ndef positional(visible):\r\n    \"\"\"\r\n    :param visible: a tensor of size (batch_size, input_dim, sequence_length) representing the sequence to be optimized\r\n    \"\"\"\r\n    FEET = np.array([4, 5, 8, 9])\r\n    feet_idx = np.array([range(i, i+3) for i in FEET*3])\r\n    batch_size = visible.get_shape()[0].value\r\n    dim = visible.get_shape()[1].value\r\n    seq_length = visible.get_shape()[2].value\r\n    idx_x, idx_y, idx_z = feet_idx[:, 0], feet_idx[:, 1], feet_idx[:, 2]\r\n    idx_x_t = [[[j, i] for i in idx_x] for j in range(batch_size)]\r\n    idx_y_t = [[[j, i] for i in idx_y] for j in range(batch_size)]\r\n    v_feet_x = tf.gather_nd(visible, idx_x_t)\r\n    v_feet_y = tf.gather_nd(visible, idx_y_t)\r\n    return get_costs(v_feet_y, seq_length)\r\n\r\n\r\nvisible = tf.Variable(np.reshape(np.arange(365), [1, 73, 5]), dtype=tf.float32)\r\ncost_op = positional(visible)\r\ntrain_op = tf.train.AdamOptimizer(0.01).minimize(cost_op)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run([train_op])\r\n```\r\n", "comments": ["I just tried your minimal test case in 0.12, and I got an error as well. Are you sure things were working in 0.12. Thanks!", "@aselle Thanks for your feedback. I tested it again in 0.12, too and also got an error, but this error is due to a bug in the minimum working example :) I'll edit this in just second. I then ran the fixed version on both 1.0 and 0.12, on 1.0 it does not pass, while on 0.12 it passes.\r\nEDIT: minimum working example is now fixed (was an issue with indexing)", "@kaufManu The error message in your original post comes from a ScatterNd op which is the gradient code for the gather_nd op in your example.  \r\n\r\nThe shape validation code for scatter_nd was changed recently.  @ebrevdo - could you take a look at this please? ", "Is there any update on this? Just encountered it as well. Feels indeed that the shape validation part of `scatter_nd` is buggy.\r\n\r\nIt checks for `updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:]` but as far as I understand something like `updates.shape = indices.shape[:-1] + params_shape[indices.shape[-1]:]` is what is expected.\r\n\r\nEDIT: by the way this test [scatter_nd_ops_test.py](https://github.com/tensorflow/tensorflow/blob/63a21e054007d86269ed1ad0145ebce04ee57a81/tensorflow/python/kernel_tests/scatter_nd_ops_test.py#L480) fails for me when I reproduce it :\r\n\r\n```python\r\nwith tf.Session():\r\n    indices = tf.zeros([1, 2, 3], dtype=tf.int32)\r\n    values = tf.zeros([1, 2, 6, 7, 8, 9])\r\n    shape = [3, 4, 5, 6, 7, 8, 9]\r\n    tf.scatter_nd(indices, values, shape).eval()\r\n```\r\n\r\nEDIT2: apaprently was corrected [here already](https://github.com/tensorflow/tensorflow/commit/3b7b39ac5dd2dceebe4b80b5e0b12316720a924b)", "Are you using tensorflow nightlies or master?", "@ebrevdo I am not using nightlies", "See if the bug has been fixed in the nightlies / on master.  We worked on\nthis code after TF 1.0 was released.\n\nOn Fri, Mar 17, 2017 at 2:53 AM, kaufManu <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I am not using nightlies\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7585#issuecomment-287311974>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzo8qqBI3iIjSYkJ59GHfn76RKu7ks5rmlgEgaJpZM4MDRDB>\n> .\n>\n", "> Shouldn't the automatically gradient scatter_nd code have the correct shape if the forward mode gather_nd is correct? I have code that works fine in forward mode but produces this error when I try to run training. I just built the master branch on my machine (commit c44601f5411408c324c81dd38bc8686cbd2568aa), but it doesn't make any difference.\r\n\r\nI just went back and checked again and saw that my install of the master branch didn't take because the version # hasn't changed. I forced the install and indeed this fixes the problem for me, so this should be in 1.0.2, whenever that comes out.\r\n", "Looks like a bug.\n\nOn Mar 24, 2017 5:22 PM, \"Christopher Barber\" <notifications@github.com>\nwrote:\n\n> Shouldn't the automatically gradient scatter_nd code have the correct\n> shape if the forward mode gather_nd is correct? I have code that works fine\n> in forward mode but produces this error when I try to run training. I just\n> built the master branch on my machine (commit c44601f\n> <https://github.com/tensorflow/tensorflow/commit/c44601f5411408c324c81dd38bc8686cbd2568aa>),\n> but it doesn't make any difference.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7585#issuecomment-289172834>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8IZr5rcEWKtbnkc5ABwZxtMk0d-ks5rpF4qgaJpZM4MDRDB>\n> .\n>\n", "What are the shapes of your Tensors?\n\nOn Mar 24, 2017 6:32 PM, \"Eugene Brevdo\" <ebrevdo@gmail.com> wrote:\n\n> Looks like a bug.\n>\n> On Mar 24, 2017 5:22 PM, \"Christopher Barber\" <notifications@github.com>\n> wrote:\n>\n>> Shouldn't the automatically gradient scatter_nd code have the correct\n>> shape if the forward mode gather_nd is correct? I have code that works fine\n>> in forward mode but produces this error when I try to run training. I just\n>> built the master branch on my machine (commit c44601f\n>> <https://github.com/tensorflow/tensorflow/commit/c44601f5411408c324c81dd38bc8686cbd2568aa>),\n>> but it doesn't make any difference.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/7585#issuecomment-289172834>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim8IZr5rcEWKtbnkc5ABwZxtMk0d-ks5rpF4qgaJpZM4MDRDB>\n>> .\n>>\n>\n", "Same problem here on r1.0 with different shapes, for example `tf.gather_nd(params, indces)` works for:\r\n- params.shape = [24, 1296, 1] \r\n- indices.shape = [24, 20, 2]\r\n\r\nwhears does not for:\r\n- params.shape = [24, 1296, 21] \r\n- indices.shape = [24, 20, 2]\r\n\r\nGetting same incorrect message.\r\n\r\n**Environment info**\r\nOperating System: Archlinux\r\nCUDA, cuDNN: 8.0 and cuDNN 5.1\r\n\r\n", "I don't think you saw that I edited my comment. The bug is NOT present on the current master branch, so it must have been fixed.", "Piotr, does the bug exist on master/the nightlies?\n\nOn Mar 25, 2017 3:12 PM, \"Christopher Barber\" <notifications@github.com>\nwrote:\n\n> I don't think you saw that I edited my comment. The bug is NOT present on\n> the current master branch, so it must have been fixed.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7585#issuecomment-289243050>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2tYjyzXT2QfaDBcYQP85SOiRhqQks5rpZFEgaJpZM4MDRDB>\n> .\n>\n", "FWIW the shape of my tensors for `output = tf.gather_nd(params, indices)` are:\r\n\r\nparams.shape == (16,8,8,8)\r\nindices.shape == (x,16,2)\r\noutput.shape == (x,16,8,8)\r\n\r\nwhere x is a batch size depending on data.\r\n", "Any updates on this?\r\n", "Notika, do you still get this error in the nightlies?\n\nOn Mar 27, 2017 11:01 AM, \"Nitika Verma\" <notifications@github.com> wrote:\n\n> Any updates on this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7585#issuecomment-289512842>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0Jk1Z-9DHIKUEAEJqiGFORX-nsMks5rp-g3gaJpZM4MDRDB>\n> .\n>\n", "For me similar error was gone after upgrading to v1.1", "Also fixed for me in 1.1.", "Thanks for confirming the fix everyone."]}, {"number": 7584, "title": "Fix: typo in README.md", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 7583, "title": "installed r1.0 -models directory is missing .", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Hi \r\nI have installed r1.0 -pip install , the models directory is missing .\r\nI know this has been an issue already but I cannot find a solution.]\r\nHow can I download the models.\r\nI tried git checkout r1.0 but get an error : not a git repository\r\n\r\nregards\r\nDenis"]}, {"number": 7582, "title": "Delete some files on Windows10 occured error!", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? No\r\n\r\n### Environment info\r\nOperating System: Win10\r\n\r\nInstalled version of CUDA and cuDNN: No\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide: Yes\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried? No\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nIn pycharm, the error log is :\r\n```\r\n\"D:\\Program Files (x86)\\Miniconda3\\python.exe\" G:/codes/tensorflow/models-master/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py\r\nExtracting MNIST_data\\train-images-idx3-ubyte.gz\r\nExtracting MNIST_data\\train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data\\t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data\\t10k-labels-idx1-ubyte.gz\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nTraceback (most recent call last):\r\n  File \"G:/codes/tensorflow/models-master/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py\", line 32, in <module>\r\n    scale = 0.01)\r\n  File \"G:\\codes\\tensorflow\\models-master\\autoencoder\\autoencoder_models\\DenoisingAutoencoder.py\", line 25, in __init__\r\n    self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))\r\nAttributeError: module 'tensorflow' has no attribute 'sub'\r\n\r\n```\r\nI clean some files on **c:**,   then i run the model zoo's example `models-master/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py` , unfortunately, it occurs the above error, what's happen?", "comments": ["Did you remove files related to tensorflow? Is your code correct? \r\nYou maybe wanted to use the method `tf. subtract` instead of `tf.sub`[https://www.tensorflow.org/api_docs/python/tf/subtract]( https://www.tensorflow.org/api_docs/python/tf/subtract)", "Release Note of V1.0.0:\r\n\r\ntf.mul, tf.sub and tf.neg are deprecated in favor of tf.multiply, tf.subtract and tf.negative.", "Also, if you have code that is failing like this, you can try to upgrade the code using the upgrade script we provide to change calls to the 1.0 versions\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/compatibility\r\nFix the python traceback first and then see if your other errors remain. Thanks!\r\n", "@Baschdl  @shiyemin Thanks! I replaced the `tf.sub` to `tf.subtract`, the error disappeared.", "@aselle However, I utilized your method to upgrade script, the bug still occurs:\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n```", "@mrry, is there a list of known ops that aren't available in windows?\r\n", "The [CMake readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) has a list of missing ops, but I don't know if people have been keeping it up-to-date as they add new ops  (though it looks promising that the [list of files excluded from the Windows build](https://github.com/tensorflow/tensorflow/blob/43c71a03380d8de18202cc399563814b2f438cd2/tensorflow/contrib/cmake/tf_core_kernels.cmake#L93) only includes quantization-related ops).\r\n\r\nContrib support is spotty, but all of the `tensor_forest` ops in the error message that [@linrio quoted](https://github.com/tensorflow/tensorflow/issues/7582#issuecomment-281032555) have been available at HEAD for some time now.", "@linrio, could you compile from source and see if that solves your issues. Otherwise, you will need to wait for the next release which resolves those ops being missing.", "@linrio Or use a nightly build, which should also have these ops built in.\r\n\r\nNote however that if you aren't using `tf.contrib.tensor_forest`, you can safely ignore these error messages.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7581, "title": "Status of Allocator API", "body": "Making this issue to track status of allocator C API, based on discussion at tfdev conference: cc @josh11b @vrv @skye @keveman @zheng-xq @keveman @yuanbyu \r\n\r\nCurrently allocator API is not public. This API, in `tensorflow/core/framework/allocator.h`, and more specifically, `bytes_in_use`, is currently the only practical way to implement user ops that\r\n\r\n1. Decide what to do based on available memory (like `foldr`, `map_fn`, with `swap_memory=True` option)\r\n2. Report available memory to user (https://github.com/tensorflow/tensorflow/issues/7537)\r\n\r\nThe three options are:\r\n\r\n1. Make this API public\r\n2. Keep this API non-public, but make it possible to create a user-op that uses this API.\r\n3. Keep this API non-public and do not provide a way to use it from user-op.\r\n\r\nCurrently it's somewhere between 2 and 3. API is not public and you can build user-op like [memory_probe_op](https://github.com/yaroslavvb/memory_probe_ops) with gcc, but not with Bazel. From an op-creator standpoint, 1 is preferable to 2 and 2 is preferable to 3. \r\n\r\nSimilar issue was https://github.com/tensorflow/tensorflow/issues/1419 where people tried to make a custom reader user op that uses methods from `reader_base.h`. The final solution was the opposite -- it was possible to make work with bazel (by commenting out `disallowed_deps` line), but not with `gcc`.", "comments": ["@zheng-xq , making you the owner of this issue.\r\n", "@josh11b Preferences on making the allocator API public?", "There is now a couple of official ops that let you query amount of memory used, this obviates the need for opening up the API.\r\n\r\nI think a bigger issue is that allocator API only gives correct results for GPU memory and not CPU memory. There used to be a work-around by parsing allocation/deallocation messages, but deallocation information stopped being correctly printed in version 1.1 https://github.com/tensorflow/tensorflow/issues/6716"]}, {"number": 7580, "title": "Upgrade docker images for distributed testing to unbutu:16.04", "body": "", "comments": ["https://ci.tensorflow.org/job/tensorflow-pull-requests-mac/3805/console shows an interestng new error I haven't seen before -- do you think it's related?", "Can't possibly be related since we don't even have Docker installed on the macs. Looking at the errors, it seems that someone changed the numpy version on our macs.", "@vrv. I don't think it is related. The files in tools/dist_test is not depended on by the failing tests. \r\n\r\nLooking closer at the log, I noticed that most of failures have the following error: \r\n```\r\nTypeError: norm() got an unexpected keyword argument 'keepdims'\r\n```\r\n\r\nas well as:\r\n```\r\nTypeError: linspace() got an unexpected keyword argument 'dtype'\r\n```\r\n\r\nThese seem to be some sort of numpy version inconsistency in the functions `np.linalg.norm` and `np.linspace` on that particular Mac machine (mac0)", "@yifeif another instance of inconsistency ...", "@yifeif  can you check the version of numpy and scipy on mac0? \r\nOn mac1, they are 1.11.0 and 0.15.1, respectively, consistent with our slave install script.", ">>> numpy.version.version\r\n'1.11.0'\r\n>>> scipy.version.version\r\n'0.15.1'\r\n\r\nbut looks like builds on mac0 is running with different Python library path compared to other macs(/Library/Python/2.7/site-packages vs /usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages)??\r\n\r\nI'll take it offline and investigate further."]}, {"number": 7579, "title": "Broken class hierarchy in sample code in getting started tutorial.", "body": "The last example in https://www.tensorflow.org/get_started/get_started does not work. When run it gives the following error message:\r\n\r\n```\r\n<ipython-input-65-6c74aa534fa7> in model(features, labels, mode, params)\r\n     18     # ModelFnOps connects subgraphs we built to the\r\n     19     # appropriate functionality.\r\n---> 20     return tf.contrib.learn.estimators.model_fn.ModelFnOps(\r\n     21         mode=mode, predictions=y,\r\n     22         loss= loss,\r\n\r\nAttributeError: module 'tensorflow.contrib.learn' has no attribute 'estimators'\r\n```\r\n\r\n\r\nThis comes from this line:\r\n\r\n```\r\nreturn tf.contrib.learn.estimators.model_fn.ModelFnOps(\r\n        mode=mode, predictions=y,\r\n        loss= loss,\r\n        train_op=train)\r\n```\r\n\r\nThere is a ModelFnOps in tf.contrib.learn and when using that the code at least runs. The result, though, seems to differ from what is mentioned in the doc.\r\n\r\ntf.VERSION is 1.0.0.", "comments": ["Thank you for reporting this. We apologize for that. We had tested the tutorial on RC2, where that worked, but in the final TensorFlow we need to replace `tf.contrib.learn.estimators.model_fn.ModelFnOps` to be`tf.contrib.learn.ModelFnOps`. numpy_input_fn also moved. So try the following block ...\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n# Declare list of features, we only have one real-valued feature\r\ndef model(features, labels, mode):\r\n  # Build a linear model and predict values\r\n  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\r\n  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\r\n  y = W*features['x'] + b\r\n  # Loss sub-graph\r\n  loss = tf.reduce_sum(tf.square(y - labels))\r\n  # Training sub-graph\r\n  global_step = tf.train.get_global_step()\r\n  optimizer = tf.train.GradientDescentOptimizer(0.01)\r\n  train = tf.group(optimizer.minimize(loss),\r\n                   tf.assign_add(global_step, 1))\r\n  # ModelFnOps connects subgraphs we built to the\r\n  # appropriate functionality.Try that and it should work fine (I verified it). I'll update the docs as well.\r\n  return tf.contrib.learn.ModelFnOps(\r\n      mode=mode, predictions=y,\r\n      loss= loss,\r\n      train_op=train)\r\n\r\nestimator = tf.contrib.learn.Estimator(model_fn=model)\r\n# define our data set\r\nx=np.array([1., 2., 3., 4.])\r\ny=np.array([0., -1., -2., -3.])\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs=1000)\r\n\r\n# train\r\nestimator.fit(input_fn=input_fn, steps=1000)\r\n# evaluate our model\r\nprint(estimator.evaluate(input_fn=input_fn, steps=10))\r\n```\r\n", "One more broken link in the example at https://www.tensorflow.org/get_started/get_started#tfcontriblearn.\r\n\r\nFollowing command:\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4,\r\n                                              num_epochs=1000)\r\n\r\nreturns following error:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/code.py\", line 91, in runcode\r\n    exec(code, self.locals)\r\n  File \"<input>\", line 1, in <module>\r\nAttributeError: module 'tensorflow.contrib.learn.python.learn.learn_io' has no attribute 'numpy_input_fn'\r\n\r\n", "@das007 I've the same problem at first. But it's solved after upgrade. Maybe you can have a try.", "@kenqrose  can you please explain how can I upgrade my tensorflow 0.12.1 to 1.0 on windows in one go ( I mean I don;t wanna do it for each file) ?", "I am using a cluster with CUDA 7.5 without root access. So, I am using tf 0.11.rc0. Getting the following error. Please help, I cannot upgrade to CUDA 8 and hence cannot upgrade to tf 1.0.\r\nAttributeError: module 'tensorflow.contrib.learn.python.learn.learn_io' has no attribute 'numpy_input_fn'", "Hi Guys i am very new at TensorFlow i am getting an error in the tutorial  getting started. Pleas can you tell me the reason. i copied the same program and tried to run but it gives an error\r\nHere is the error\r\n\r\n\r\n    estimator = tf.contrib.learn.Estimator(model_fn=model)\r\nAttributeError: 'module' object has no attribute 'learn'", "I have the same problem as das007. I use tf 1.0 and newest CUDA!", "Hi @aselle I tried your code but It can't work.\r\n\r\n`input_fn = tf.contrib.learn.io.numpy_input_fn({ \"x\": x}, y, batch_size = 4, num_epochs = 1000)`\r\n                                                            \r\n\r\n> AttributeError: 'module' object has no attribute 'numpy_input_fn'\r\n\r\n \r\nTensorflow's version: 0.12.0-rc1", "Hi @aselle I also tried your code but facing same error.\r\nI have Windows OS.\r\n\r\nI also tried (input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x}, y, batch_size=4,\r\nnum_epochs=1000)) from https://www.tensorflow.org/get_started/get_started#tfcontriblearn.\r\nbut facing same error **AttributeError: module 'tensorflow.contrib.learn.python.learn.learn_io' has no attribute 'numpy_input_fn'**", "For me, this issue has been resolved when tensorflow was updated.\r\n\r\nTo update to 1.0.0, when using conda (window),\r\n\r\n1. remove tensorflow\r\npip uninstall tensorflow\r\nor \r\nconda uninstall tensorflow\r\n\r\n2. install tensorflow\r\nconda install -c conda-forge tensorflow=1.0.0", "Hey, thanks for your answers. I didn\"t realise there were any and totally forgot about the issue ;) ", "@achbogga sorry for the late answer but I think you need to rename the method from \"tensorflow.contrib.learn.python.learn.learn_io.numpy_input_fn\" to \"tensorflow.contrib.learn.io.numpy_input_fn\"", "Just update both pip and then tensorflow your error will be gone\r\n"]}, {"number": 7578, "title": "cifar10_multi_gpu_train.py is mentioned in the tutorial, but doesn't exist in the source tree", "body": "cifar10_multi_gpu_train.py was removed in the 1.0, but still exists in the docs:\r\nhttps://www.tensorflow.org/tutorials/deep_cnn\r\n\r\nWhere can users find examples for multi-GPU/distributed training compatible with v1.0?", "comments": ["https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10", "Thanks. Sorry, my bad.\r\n\r\nIt is another issue, but it seems that is not 100% compatible with 1.0 guidelines though. It uses tf.get_variable_scope().reuse_variables() and the docs say (https://www.tensorflow.org/install/migration):\r\nConstructions like tf.get_variable_scope().reuse_variables() will likely not work.\r\n\r\n"]}, {"number": 7577, "title": "A suggestion for controlled dependency in batch normalization layer", "body": "In documents of `tf.contrib.layers.batch_norm`, it is suggested to bind update op of moving_mean and moving_variance with loss function, as follows:\r\n```\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \r\nif update_ops: \r\n    updates = tf.group(*update_ops)\r\n    total_loss = control_flow_ops.with_dependencies([updates], total_loss)\r\n```\r\nIs it better to bind `update_op` with `train_op`? Say, maybe one want to evaluate loss but do not update parameters?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7576, "title": "GrpcRemoteMaster was chosen instead of LocalMaster", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nnone.\r\n\r\n### Environment info\r\nOperating System:\r\nWindows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nnone\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n 9570f0f2804e857bc5593bed526eda7c1c915ed9\r\n2. The output of `bazel version`\r\n(compiled by cmake)\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import session\r\n\r\ndef main(_):\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"127.0.0.1:10000\"        \r\n    ],\r\n    \"ps\": [\r\n        \"127.0.0.1:10001\"\r\n    ]})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=\"worker\",\r\n                           task_index=0)\r\n  with tf.device(\"/job:ps/task:0\"):\r\n    weights = tf.Variable(tf.random_normal([10], stddev=0.35), name=\"weights\")  \r\n  init_op = tf.global_variables_initializer()\r\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\r\n    sess.run(init_op)\r\n    w  = sess.run(weights)\r\n    print(w)\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n### What other attempted solutions have you tried?\r\nchange \r\n```\r\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\r\n```\r\nto\r\n```\r\n  with tf.Session(\"grpc://localhost:10000\") as sess:\r\n```\r\n\r\nsolved it.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nThe reason is:\r\nIn tensorflow\\core\\distributed_runtime\\rpc\\grpc_server_lib.cc: GrpcServer::target() function, the host part of target is hardcoded to \"localhost\"\r\n```\r\nconst string GrpcServer::target() const {\r\n  return strings::StrCat(\"grpc://localhost:\", bound_port_);\r\n}\r\n```\r\nSo, if the session was created with another address string, e.g. 127.0.0.1, (localip), etc. GrpcRemoteMaster  will be used instead of LocalMaster. I suggest LocalMaster class should have a isLocalIP() function to deal with this.\r\n\r\n\r\n", "comments": ["@mrry ", "The only guaranteed way to get a `LocalMaster` is to pass `server.target` as the session target. Perhaps we could document this better, but I'd prefer to keep the rule for dispatching to the local master as simple as possible.\r\n\r\nIs there any reason why you prefer to construct the session target manually? ", "Hi @mrry, no particular seasons, my mistake. I copied the code from somewhere that I didn't understand fully. Thanks for your explanation."]}, {"number": 7575, "title": "Use environment invalidation for cuda_configure", "body": "This removes the needs for clean --expunge in configure :) but\r\nrequest the incoming bazel 0.4.5.\r\n\r\nThis should fix #4848", "comments": ["We will cut a new release soonish and that should include the necessary change to that, of course this make all the presubmit fails right now because it uses a non existent bazel feature", "Awesome! \\o/", "Nice, thanks.  this will have to wait for our CI to upgrade.  assigned gunan/yifei", "Thanks! Preparing a CL, will upgrade once 0.4.5 is out. ", "(Continues to be stalled while waiting on bazel 0.4.5)", "I don't think @gunan wanted to close that PR when merging his change, now this PR should be ready to merge.", "Sorry, yeah I think that was auto closed due to \"Fix ...\".\r\n\r\nNow that we are updated, Damien could you sync and resolve conflicts?", "Merged. I don't understand how I can make the windows CMake test fails :p", "win1 may be in a bad state.\r\nRerunning here:\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/1672/console", "Jenkins, test this please.", "Jenkins, test this please.", "This broke the ability to build for me, please advise... (See #8619)"]}, {"number": 7574, "title": "Apply gofmt to Go API", "body": "Applied gofmt to Go API all code. Used gofmt is 1.7.5.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 7573, "title": "Tensorflow freezes during execution of session.run()", "body": "We currently experience an issue with an implementation. During the execution of session.run, the process suddenly freezes. It does not crash but is irresponsive to ctrl+c. It isn't consuming any CPU anymore (and is not progressing either). This occurs on CPU, we haven't tested GPU. The issue seems to be highly related to #2788.\r\n\r\nWe ran the script on a machine running an up-to-date Ubuntu 16.04 with 128gb of ram, and 2 x Xeon CPU E5-2640 v4 processors. The issue occured with tensorflow 0.12.1 installed through anaconda. Then we reproduced the issue using a build of the master branch, without any CUDA support, using the system libraries rather than those shipped with anaconda.\r\n\r\nThe build of the master branch shows:\r\nprint(tensorflow.__version__)\r\n1.0.0-rc2\r\n\r\n$ git rev-parse HEAD\r\n1a0742f6a7a06ff54481385b5c51094b0fef8cf3\r\n\r\n$ bazel version\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\nAttached is the output of running  thread apply all bt 10 in gdb. All threads appear to be waiting. \r\nWe are running a model locally implemented using GPflow (not one of the default models of that library though), unfortunately this occurs on a confidential data set. If required we can look into providing a MWE but I can not guarantee we can reproduce this easily under different circumstances. \r\n[gdb.txt](https://github.com/tensorflow/tensorflow/files/780004/gdb.txt)\r\n", "comments": ["Without a minimal reproducible test case, we will be unlikely to be able to help you further. However, the most common cause of this is that your run() call is blocking on queues. \r\n\r\nSee e.g.\r\nhttps://github.com/tensorflow/tensorflow/issues/2788\r\n", "Unresponsive to CTRL+C is suspicious. Is it possible to kill it using `kill`? I've seen unkillable TF training caused by 1) stuck IO (ie, trying to write to NFS when net is down) 2) Nvidia driver (it gets stuck sometimes). Both can be troubleshooted by looking at the stuck call in `/proc/<pid>/stack`. ", "@javdrher and I can kill it, but no CTRL+C response, I'll look into your suggestions, thank you.", "Closing due to lack of activity.  Please reopen if necessary.", "Having this issue as well. I'm guessing it's some sort of issue with my code rather than TF, but it's still unusual. I've been running off of CPU, so Nvidia drivers shouldn't be the issue.\r\n\r\nCode is [here](https://github.com/AMLeng/TeamSnek2O/tree/saving), specifically `steer_eval.py`."]}, {"number": 7572, "title": "Bazel version required for TensorFlow 1.0 ?", "body": "Hi all,\r\n\r\nNeither the README.md nor https://www.tensorflow.org/install/install_sources mentions the minimal bazel version that is required for building tensorflow 1.0 from source.\r\n\r\nI have successfully built versions 0.9 and 0.10 from source using bazel 0.3.0 (first I had installed version 0.2.4, then while compiling tensorflow I got errors, opened an issue on github and was told to update to 0.3.0).\r\n\r\nNow I tried building tensorflow 1.0 and the same story repeats. I get error messages, google for them. Google points me to a github issue (in this case #6436) and there people are told to upgrade bazel to a newer version (0.4.2).\r\n\r\nPlease start specifying somewhere a minimal requirement for the bazel version used to build a particular tensorflow version.\r\n\r\nAnd if one needs to install a new bazel version each time one wants to install a newer tensorflow version, then please make bazel a part of tensorflow.\r\n\r\nI am building a lot of software from source code. If you for instance look at other build systems as for example CMake. Most software that I build nowadays with CMake still builds fine with version 2.8.12, which was released in 2013. For CMake requiring a newer version is the exception, whereas for bazel it seems to be the rule.\r\n\r\nBest regards\r\n\r\nSam", "comments": ["The interface of Bazel changes too frequently. Actually I'm wondering how big of the difference between Blaze and Bazel.", "0.4.2 is works for me:\r\nhttps://github.com/wangyum/Anaconda/blob/master/doc/installing-tensorflow-from-sources.md", "for macos version, bazel must not be 0.4.4 because of Bazel bug https://github.com/tensorflow/tensorflow/issues/7227", "Tried to install Tensorflow 1.0 with bazel 0.4.4 (under CentOS 6.8, using GCC 4.8.2, Python 3.6.0).\r\n\r\nConfiguration step went fine, but when trying to build the pip wheel with\r\n\r\n[sfux@e2174 tensorflow-1.0.0]$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nit failed:\r\n\r\n```\r\n ERROR: /cluster/scratch/sfux/cache/_bazel_sfux/9f758529667fd3bc1d9ed949725387b8/external/protobuf/BUILD:649:1: null failed: protoc failed: error executing command bazel-out/host/bin/external/protobuf/protoc '--python_out=bazel-out/local-py3-opt/genfiles/external/protobuf/python' -Iexternal/protobuf/python ... (remaining 13 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n bazel-out/host/bin/external/protobuf/protoc: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by bazel-out/host/bin/external/protobuf/protoc)\r\n Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n Use --verbose_failures to see the command lines of failed build steps.\r\n INFO: Elapsed time: 50.560s, Critical Path: 30.89s\r\n```\r\n\r\nCompiler is set correctly, GCC 4.8.2 library directory is added to $LD_LIBRARY_PATH:\r\n\r\n[sfux@euler06 ~]$ which gcc\r\n**/cluster/apps/gcc/4.8.2/bin/gcc**\r\n[sfux@euler06 ~]$ echo $LD_LIBRARY_PATH | grep /cluster/apps/gcc/4.8.2\r\n/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/lib:**/cluster/apps/gcc/4.8.2/lib64**\r\n[sfux@euler06 ~]$\r\n\r\nCROSSTOOL file contains path's to GCC 4.8.2:\r\n\r\n```\r\n[sfux@euler06 ~]$ grep /cluster/apps/gcc /cluster/apps/bazel/0.4.4/x86_64/tools/cpp/CROSSTOOL\r\n  tool_path { name: \"cpp\" path: \"/cluster/apps/gcc/4.8.2/bin/cpp\" }\r\n  tool_path { name: \"gcc\" path: \"/cluster/apps/gcc/4.8.2/bin/gcc\" }\r\n  cxx_builtin_include_directory: \"/cluster/apps/gcc/4.8.2/lib64\"\r\n  cxx_builtin_include_directory: \"/cluster/apps/gcc/4.8.2/include/c++/4.8.2\"\r\n  tool_path { name: \"cpp\" path: \"/cluster/apps/gcc/4.8.2/bin/cpp\" }\r\n[sfux@euler06 ~]$\r\n```\r\n\r\nWhy does the installation still fall back to /usr/lib64/listdc++.so ?\r\n\r\nAny help is appreciated\r\n\r\nBest regards\r\n\r\nSam", "@samfux84 Does this help https://github.com/bazelbuild/bazel/issues/1358?", "Hi Ilhe,\r\n\r\nbazelbuild/bazel#1358 did not help.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/forum/?hl=sr#!topic/discuss/VnpcZMToQ4A states that third_party/sycl/crosstool/CROSSTOOL.tpl can still contain path's to GCC in /usr. I have changed the path's in CROSSTOOL.tpl to the correct GCC installation. Now I get a different error:\r\n\r\n```\r\nERROR: /cluster/scratch/sfux/cache/_bazel_sfux/c3fb8682b40a7bc55991a3af7a35a071/external/jemalloc/BUILD:10:1: C++ compilation of rule '@jemalloc//:jemalloc' failed: gcc failed: error executing command \r\n  (cd /cluster/scratch/sfux/cache/_bazel_sfux/c3fb8682b40a7bc55991a3af7a35a071/execroot/tensorflow-1.0.0 && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/cluster/apps/java/1.8.0_91/x86_64/jre/lib/amd64/server:/cluster/apps/java/1.8.0_91/x86_64/jre/lib/amd64:/cluster/apps/java/1.8.0_91/x86_64/jre/../lib/amd64:/cluster/apps/python/3.6.0/x86_64/lib64:/cluster/apps/sqlite/3.15.0/x86_64/lib:/cluster/apps/openblas/0.2.13_seq/x86_64/gcc_4.8.2/lib:/cluster/apps/java/1.8.0_91/x86_64/jre/lib/amd64/server:/cluster/apps/java/1.8.0_91/x86_64/jre/lib/amd64:/cluster/apps/java/1.8.0_91/x86_64/jre/lib/amd64/xawt:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/lib:/cluster/apps/gcc/4.8.2/lib64 \\\r\n    PATH=/cluster/apps/python/3.6.0/x86_64/bin:/cluster/apps/sqlite/3.15.0/x86_64/bin:/cluster/apps/openblas/0.2.13_seq/x86_64/gcc_4.8.2/bin:/cluster/apps/swig/3.0.5/x86_64/bin:/cluster/apps/bazel/0.4.4/x86_64/output:/cluster/apps/java/1.8.0_91/x86_64/bin:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/etc:/cluster/apps/lsf/9.1/linux2.6-glibc2.3-x86_64/bin:/cluster/apps/modules/bin:/cluster/apps/gcc/4.8.2/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/cluster/home/sfux/bin:/cluster/apps/ansys/v150/fluent/license/lnamd64:/cluster/apps/adm:/cluster/home/sfux/shellscript:/cluster/home/sfux/prog/bash:/opt/ibutils/bin \\\r\n    TMPDIR=/scratch/38029933.tmpdir \\\r\n  /cluster/apps/gcc/4.8.2/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/cluster/apps/gcc/4.8.2/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/local-py3-opt/bin/external/jemalloc/_objs/jemalloc/external/jemalloc/src/pages.pic.d -fPIC -iquote external/jemalloc -iquote bazel-out/local-py3-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -isystem external/jemalloc/include -isystem bazel-out/local-py3-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -O3 -funroll-loops -D_GNU_SOURCE -D_REENTRANT -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/jemalloc/src/pages.c -o bazel-out/local-py3-opt/bin/external/jemalloc/_objs/jemalloc/external/jemalloc/src/pages.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/jemalloc/src/pages.c: In function 'je_pages_huge':\r\nexternal/jemalloc/src/pages.c:203:30: error: 'MADV_HUGEPAGE' undeclared (first use in this function)\r\n  return (madvise(addr, size, MADV_HUGEPAGE) != 0);\r\n```\r\n\r\nAny ideas ?\r\n\r\nBest regards\r\n\r\nSam", "@samfux84 \r\nTry to disable jemalloc:\r\n```\r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] n\r\njemalloc disabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] y\r\nHadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] N\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\n```", "I have disabled jemalloc. Even though I have adapted the paths in CROSSBUILD.tpl, the compilation still fails with:\r\n\r\n```\r\nERROR: /scratch/38110978.tmpdir/tensorflow-1.0.0/tensorflow/core/BUILD:1027:1: null failed: protoc failed: error executing command \r\n  (cd /cluster/scratch/sfux/cache/_bazel_sfux/26bed21e6a811013a8a37558ae385893/execroot/tensorflow-1.0.0 && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/protobuf/protoc '--cpp_out=bazel-out/local-py3-opt/genfiles/' -I. -I. -Iexternal/protobuf/src -Ibazel-out/local-py3-opt/genfiles/external/protobuf/src -Iexternal/protobuf/src -Ibazel-out/local-py3-opt/genfiles/external/protobuf/src tensorflow/core/protobuf/worker.proto): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/host/bin/external/protobuf/protoc: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by bazel-out/host/bin/external/protobuf/protoc)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 68.956s, Critical Path: 42.04s\r\n```\r\n", "Finally I could manage to compile Tensorflow 1.0. The problem was solved after applying the suggestion in https://github.com/tensorflow/tensorflow/issues/3261#issuecomment-234147379\r\n\r\nAfter setting the linker flag in the /tools/cpp/CROSSTOOL in bazel and the /thirdparty/sycl/crosstool/CROSSTOOL.tpl  in the tensorflow directory, the error was no longer showing up."]}, {"number": 7571, "title": "[fix] add estimators to allowed_symbols in contrib.learn", "body": "Fix \r\n```py\r\nimport tensorflow as tf\r\ntf.contrib.learn.estimators\r\n```\r\nresults in \r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'estimators'\r\n\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "signed", "CLAs look good, thanks!\n\n<!-- ok -->", "Which symbol do you need from estimators? Is it not accessible directly in `tf.contrib.learn`", "Seems like the code in [docs](https://www.tensorflow.org/get_started/get_started#a_custom_model) has been updated to use \r\n```py\r\ntf.contrib.learn.ModelFnOps\r\n```\r\ninstead of \r\n```py\r\ntf.contrib.learn.estimators.model_fn.ModelFnOps\r\n```\r\n\r\nI think this can be closed now.\r\n"]}]