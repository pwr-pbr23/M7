[{"number": 32464, "title": "[tflite] Support INT8 quantisation for SPLIT with TFLITE_BUILTINS_INT8 OpsSet", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14 and built from sources, master branch\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe new TFLiteConverter post-training quantisation flow, as described in https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations, does not support quantisation of SPLIT operation when only integer operations are requested in the output model. When this conversion is forced, the Runtime Error \"Quantization not yet supported for op: SPLIT\" is raised. The following code can be used to recreate the error\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef representative_dataset_gen():\r\n\tinput = np.array(np.random.random_sample([2,10]), dtype=np.float32)\r\n\tfor _ in range(10):\r\n\t\tyield [input]\r\n\r\n# tf Graph Input \r\nin_intact = tf.compat.v1.placeholder(\"float32\", [2, 10])\r\nout_split = tf.split(in_intact,2,axis=0)\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n\ttf.io.write_graph(tf.compat.v1.get_default_graph(), '.','split.pb', as_text=False)\r\n\r\ninput_name = [\"Placeholder\"]\r\noutput_name = [\"split\", \"split:1\"]\r\n\r\ntflite_model_name = \"int8_split.tflite\"\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\"split.pb\", input_name, output_name)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_model = converter.convert()\r\nopen(tflite_model_name, \"wb\").write(tflite_model)\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who attempts full 8-bit fixed point quantisation of models containinig SPLIT operation, present in some versions of Deepspeech.\r\n\r\n**Any Other info.**\r\nNote that there are separate TFLite implementations, SPLIT and SPLIT_V, for two different behaviors of Tensorflow's tf.split. This request is related only to SPLIT, although after implementing it, SPLIT_V can probably similarly be implemented.", "comments": ["This was resolved by commit [3c2be61e68acea533030b8f70fa0db3339b19f93](https://github.com/tensorflow/tensorflow/commit/3c2be61e68acea533030b8f70fa0db3339b19f93#diff-69178ece8d1001b2ef7f9b329e9a525b)", "Hi,\r\n\r\nI got into some problem there: My TF model has split.\r\n*) conversion/post-training quantization with TF 1.14 and TF 1.15 fails with error above\r\n*) conversion/post-training quantization with TF 2.2 works, but the edgetpu_compiler crashes with \"internal compiler error\". \r\nIt looks like I would need to use TF 1.X if I want to use edgetpu_compiler, but can't because the SPLIT isn't\r\nfixed on that.\r\nAny ideas would be helpful!\r\n\r\nThanks!", "Hi Johann, Sorry for the late reply.\r\n\r\nTo your first (and first half of the second) point, the SPLIT operator support was only incorporated into the main release starting TF version 2.1, so that is the expected behaviour. The second issue comes from the [edgetpu](https://github.com/google-coral/edgetpu) compiler, which is not related to this issue and the relevant commit. \r\n\r\nAccording to the [coral supported ops documentation](https://coral.ai/docs/edgetpu/models-intro/#supported-operations) SPLIT is supported except for the batch dimension, so that could be the problem. If you are not attempting to split along the batch dimension, consider raising an issue in the edgetpu repo (also worth debugging to make sure it is in fact caused by this particular operator in the first place; if not, raise a general issue on the problem, or preferably having isolated what exactly causes it).\r\n\r\nHope this helps!", "Hi,\n\nThanks for your response.\nI was able to narrow down the error, where the edgetpu_compiler crashes: it\nwas in the \"reshape\" operation (not in\nthe split). Following your advice, I posted an issue to the edgetpu repo\n(#173)\n\nThanks!\n-johann\n\nOn Mon, Jul 13, 2020 at 9:38 AM MohamedNourArm <notifications@github.com>\nwrote:\n\n> Hi Johann, Sorry for the late reply.\n>\n> To your first (and first half of the second) point, the SPLIT operator\n> support was only incorporated into the main release starting TF version\n> 2.1, so that is the expected behaviour. The second issue comes from the\n> edgetpu <https://github.com/google-coral/edgetpu> compiler, which is not\n> related to this issue and the relevant commit.\n>\n> According to the coral supported ops documentation\n> <https://coral.ai/docs/edgetpu/models-intro/#supported-operations> SPLIT\n> is supported except for the batch dimension, so that could be the problem.\n> If you are not attempting to split along the batch dimension, consider\n> raising an issue in the edgetpu repo (also worth debugging to make sure it\n> is in fact caused by this particular operator in the first place; if not,\n> raise a general issue on the problem, or preferably having isolated what\n> exactly causes it).\n>\n> Hope this helps!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32464#issuecomment-657665099>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHRRDY4LZP6CBKJZAIRNCLR3MZ6VANCNFSM4IWDVEQQ>\n> .\n>\n"]}, {"number": 32463, "title": "Handle the case where axis is a tuple.", "body": "Previously the code only handled ints and lists as axis. Gave an error `TypeError: 'tuple' object does not support item assignment` when giving tuple as axis argument. See this issue https://github.com/tensorflow/tensorflow/issues/32311", "comments": ["In response to your question on the issue, yes please do add a test. Thanks!", "@robieta Can you check if the test case is correct? I am not familiar with the testing framework.", "@robieta Sorry for the late response. I changed the input_shape to (2, 8, 8, 3).\r\nAlso `Allow edits from maintainers.` is on, so if anyone needs to make changes they can."]}, {"number": 32462, "title": ".h5 file to pb", "body": "is there any API to convert tf.keras.models.save() .h5  file to pb? ", "comments": ["@XuVision ,\r\nPlease refer this stackoverflow [link](https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb) for the solution. Thanks!", "great, thanks!", "Closing since the issue is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32462\">No</a>\n"]}, {"number": 32461, "title": "TensorShapeBase parameter list in .so differs from header, undefined reference ", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Mint 18.3 Xfce 64-bit, 4.15.0-45-generic #48~16.04.1-Ubuntu\r\n- TensorFlow installed from: source\r\n- TensorFlow version: r1.10 and r1.12\r\n- Python version: Python 2.7.12\r\n- Bazel version: 0.19.2\r\n- GCC/Compiler version: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- GPU model and memory: Integrated Intel HD inside a virtual machine\r\n\r\nI have compiled the tensorflow C++ api libraries from source for versions r1.10 and r1.12 on my Linux machine, thus I get a libtensorflow_cc.so (or libtensorflow.so) and libtensorflow_framework.so. I simply used `bazel build --config=opt //tensorflow:libtensorflow_cc.so` to do this.\r\n\r\nThe problem arises when I try to include the libraries in a project of mine. When compiling I get the following linker errors:\r\n`mydir/myfile.o: In function 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(std::initializer_list<long long>)':\r\n~/myproject/include/tensorflow/tensorflow/core/framework/tensor_shape.h:172: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'\r\n~/myproject/include/tensorflow/tensorflow/core/framework/tensor_shape.h:172: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'\r\n~/myproject/include/tensorflow/tensorflow/core/framework/tensor_shape.h:172: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(tensorflow::gtl::ArraySlice<long long>)'\r\ncollect2: error: ld returned 1 exit status`\r\n\r\nAnd it's true. When I do\r\n`nm -D libtensorflow_cc.so | c++filt | grep TensorShapeBase`\r\nI get\r\n`U tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)`\r\nand the same for `libtensorflow_framework.so`:\r\n`000000000049deb0 W tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)`.\r\n\r\nHowever, when looking in `tensorflow/core/framework/tensor_shape.h` it does say starting at line 171:\r\n`TensorShapeBase(std::initializer_list<int64> dim_sizes)\r\n: TensorShapeBase(gtl::ArraySlice<int64>(dim_sizes)) {}` \r\n\r\nHow can this issue be fixed? Why does it compile the library with different parameters than what is inside the header?\r\n\r\nCheers\r\n\r\nbtw: I also compiled a 800 MB libtensorflow.so with cmake but it doesn't work due to runtime problems concerning some estimator - from what I can see, though, it does have the correct parameter in the call.", "comments": ["I suspect tensorflow failed to build successfully. Can you use Bazel 0.15.0 to build TF and try again?\r\nhttps://www.tensorflow.org/install/source#tested_build_configurations", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32461\">No</a>\n"]}, {"number": 32460, "title": "Executor error message in GradientTape.jacobian", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.141-1-MANJARO-x86_64-with-arch-Manjaro-Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n**Describe the current behavior**\r\nI am calculating the Jacobian of a model with respect to its parameters using a gradient tape. The method `GradientTape.jacobian` outputs an error message about a missing function library. The program does not abort however and the computed Jacobian is correct.\r\n\r\n**Describe the expected behavior**\r\nThere should be no error message.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Dense(2, input_shape=(2,)),\r\n])\r\ninputs = tf.Variable([[1, 2]], dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as gtape:\r\n    outputs = model(inputs)\r\ngtape.jacobian(outputs, model.trainable_variables)\r\n```\r\n\r\n**Other info / logs**\r\nThe full output of the program shown above is:\r\n```\r\n2019-09-12 11:01:04.479226: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-12 11:01:04.498159: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304500000 Hz\r\n2019-09-12 11:01:04.498584: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556260300c60 executing computations on platform Host. Devices:\r\n2019-09-12 11:01:04.498609: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-09-12 11:01:04.630899: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Internal: No function library\r\n\t [[{{node loop_body/MatMul_1/pfor/cond}}]]\r\n```\r\nWith additional layers, more error messages like the one quoted above appear, one per matrix multiplication.\r\nThe problem persists when the Jacobian calculation is moved into a `tf.function` decorated function.", "comments": ["Was able to replicate the issue when tried in terminal for `TF version-2.0rc0`\r\n![32460](https://user-images.githubusercontent.com/52397990/64848842-be898000-d62f-11e9-9ba4-0a4863e98b55.png)\r\n", "It seems that I've found a solution. Try this:\r\n```python\r\ngtape.jacobian(outputs, model.trainable_variables, experimental_use_pfor = False)\r\n```\r\nIt has helped me", "@jl-wynen, This issue is fixed in Tf 2.0.\r\nPlease take a look at gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3cf63093471c1012746f5a77b75aa4be/untitled259.ipynb). Thanks!", "I still get the same error message on my machine.\r\nI installed TF into a new virtual environment using\r\n```\r\npip install tensorflow==\"2.0.0\"\r\n```", "Hi I think the provided gist by adagashwini is run on CPU. The issue happens when running on GPU.", "@jl-wynen  and @pawelc , I didn't see error message with Tf GPU as well.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3f3c64d01113dfc8ebcc8481ff652c14/untitled259.ipynb). Thanks!", "I had this same issue and while using `experimental_use_pfor=False` works for most of my cases, I found a slight difference in the behavior compared to `experimental_use_pfor=True`. When running with tensors of batch shape 0, the former fails due to a type error, whereas the latter one works fine. Not sure if this is expected or not?\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    variable = tf.Variable(1.0)\r\n    inputs = (\r\n        tf.constant(tf.random.uniform((0, 4))),\r\n        tf.constant(tf.random.uniform((0, 3))),\r\n    )\r\n\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        outputs = variable * tf.pow(tf.concat(inputs, axis=-1), 2.0)\r\n\r\n    jacobians_1 = tape.jacobian(\r\n        outputs,\r\n        variable,\r\n        experimental_use_pfor=True,\r\n    )\r\n    print(jacobians_1)\r\n    print(\"tape.jacobians(..., experimental_use_pfor=True) works!\")\r\n\r\n    try:\r\n        jacobians_2 = tape.jacobian(\r\n            outputs,\r\n            variable,\r\n            experimental_use_pfor=False,\r\n        )\r\n        print(jacobians_2)\r\n        print(\"tape.jacobians(..., experimental_use_pfor=False) works!\")\r\n    except TypeError:\r\n        print(\"tape.jacobians(..., experimental_use_pfor=False) doesn't work!\")\r\n        raise\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nThis results in:\r\n```\r\n$ python ./tests/test_jacobian_with_empty_inputs.py\r\n2020-01-09 12:41:21.118775: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-09 12:41:21.135469: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc7e6242a00 executing computations on platform Host. Devices:\r\n2020-01-09 12:41:21.135493: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\ntf.Tensor([], shape=(0, 7), dtype=float32)\r\ntape.jacobians(..., experimental_use_pfor=True) works!\r\ntape.jacobians(..., experimental_use_pfor=False) doesn't work!\r\nTraceback (most recent call last):\r\n  File \"./tests/test_jacobian_with_empty_inputs.py\", line 36, in <module>\r\n    main()\r\n  File \"./tests/test_jacobian_with_empty_inputs.py\", line 26, in main\r\n    experimental_use_pfor=False,\r\n  File \"/Users/hartikainen/conda/envs/softlearning-5/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1115, in jacobian\r\n    for i, out in enumerate(output):\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nI'm running this with the following versions:\r\n```bash\r\n$ pip freeze | grep \"tf\\|tensor\"\r\ntensorboard==2.0.2\r\ntensorflow==2.0.0\r\ntensorflow-estimator==2.0.1\r\ntensorflow-probability==0.8.0\r\ntfp-nightly==0.9.0.dev20191207\r\n$ python --version\r\nPython 3.7.5\r\n```", "The initial code snippet executes successfully in tf nightly on cpu as well gpu. Thanks!\r\n@hartikainen Can you please create a new issue thread for your repro code? Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32460\">No</a>\n"]}, {"number": 32459, "title": "TFRecordWriter doesn't work from TFRecordsDataset", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): barely\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: quadro 16gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWould like to write `TFRecordsDataset` to files using [`TFRecordWriter`](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/data/experimental/ops/writers.py#L30-L61), however it complains that it does not match the right format... which is odd as given the [tutorial](https://www.tensorflow.org/tutorials/load_data/tf_records) one would think this is possible\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIt \"just works\"\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfiles = ['file_with_n.tfrecords', 'aother_file_with.tfrecords', ...]\r\nn_records = # \r\n\r\nds = tf.data.TFRecordDataset(files)\r\n# pre-shuffle all data and then use smaller shuffle buffer size and shuffle files during training\r\nds = ds.shuffle(n_records)\r\nds = ds.batch(records_per_file)\r\nfor i, batch in enumerate(ds):\r\n    o_file = 'part_{}.tfrecord'.format(i)   \r\n    batch_ds = tf.data.Dataset.from_tensors(batch) # <--- batch is back as a DataSet\r\n    batch_ds = batch_ds.batch(1).map(lambda e: tf.reshape(e,(-1,)))\r\n    writer = tf.data.experimental.TFRecordWriter(o_file)\r\n    writer.write(batch_ds)\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please, help us to provide standalone code snippet to reproduce the issue in our environment.Thanks!", "@ravikyram not really needed as it I link to the tutorial and all I did was batch and iterator over the sequences....\r\n\r\nthat said, I found _my_ mistake. \r\n\r\nreplace `tf.data.Dataset.from_tesnors(batch)` with `tf.data.Dataset.from_tensor_slices(batch)` and it works. \r\n\r\nThis may want to be added to docs regarding records in regards to manipulating records.\r\ne.g. to \"pre-shuffle\" data with full `buffer_size` prior to training, shard the records into smaller files and at run time shuffle the subfiles as well as use a smaller shuffle buffer over the elements"]}, {"number": 32458, "title": "how to open tensorcore in TFS1.13", "body": "I have a language model, which has some layers as \"embedding+lstm+2dnn\", and I want to test the fp16 performance on TFS. \r\n\r\nBy the way,I have tested that it can speed up 2times In pytorch on 2080Ti.\r\n\r\nBut when I do test on 2080Ti in TFS1.13GPU(cuda version=10.0),  fp16 is slower than float32 when batch is small, the test result as followings:\r\n\r\nbatch | 1080Ti(tfsv1.5) | 2080Ti(tfsv1.5) | 2080Ti(tfsv1.13)\r\n    -  |float32 | fp16 | float32 | fp16 | float32 | fp16\r\n50   | 31ms | 56ms | 48ms | 60ms | 51ms | 60ms\r\n500 | 48ms | 42ms | 73ms | 62ms | 81ms | 61ms\r\n1000 | 71ms | 42ms | 129ms | 60ms | 123ms | 58ms\r\n1500 | 100ms | 47ms | 179ms | 76ms | 175ms | 60ms\r\n\r\nI have checked that the CUDNN math type is CUDNN_TENSOR_OP_MATH, but I donot know if it has opened tensorcores or how to open it?\r\n\r\nthanks a lot for any suggestions!", "comments": ["@wangyunxiaa, Did you try with latest Tf nightly version. \r\nAnd, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@gadagashwini I have tested it on 2080Ti with tf1.13 and tf1.14, not using latest Tf nightly version.\r\nmy test code as followings:\r\n`from tensorflow.python.platform import gfile\r\nimport time\r\nimport os\r\nimport datetime\r\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\r\n\r\nwith tf.Graph().as_default():\r\n    output_graph_def = tf.GraphDef()\r\n    with gfile.FastGFile(\"graph.pb\", \"rb\") as f:\r\n        output_graph_def.ParseFromString(f.read())\r\n        _ = tf.import_graph_def(output_graph_def, name=\"\")\r\n\r\n    graph = tf.get_default_graph()\r\n        with tf.Session() as sess:\r\n        input_x = sess.graph.get_tensor_by_name(\"ac_input:0\")\r\n        state_in = sess.graph.get_tensor_by_name(\"Inference/pre_state:0\")\r\n        state_out = sess.graph.get_tensor_by_name(\"Inference/cur_state:0\")\r\n        out = sess.graph.get_tensor_by_name(\"Inference/final_output/output:0\")`\r\nthe sess run and time collect as followings:\r\n`feed = {\r\n                 input_x: i2indataline50.reshape(2, -1),\r\n                 state_in: pre_state.reshape(-1, all_hidden_num*50),\r\n             }\r\n        pre_state, output = sess.run([state_out, out], feed)\r\n        print(\"=====loop 100 times=====\")\r\n        cur_time = time.time()\r\n        for i in range(100):\r\n            pre_state, output = sess.run([state_out, out], feed)\r\n        now = (time.time() - cur_time)*1000\r\n        print(\"t_use:\", now/100)`\r\nfloat32 and fp16 are two different graph.pb with the same model struct but different datetype(tf.float32 & tf.float16)\r\nI am confused that pytorch is faster than tf, and fp16 is also speed up 2 times, what should i do to use tensorcores for fp16?\r\nthanks!", "@wangyunxiaa, Will it possible to provide the graph.pb file to replicate the issue reported here. \r\nI tried with my own .pb file but I, end up in different error. Thanks!", "@gadagashwini , Hi,\r\nThe model and test code  in https://pan.baidu.com/s/1Q67Fvg8rlZappzjqIni_Ag", "Have you considered using [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision) for your use case.\r\nAlso see https://github.com/tensorflow/tensorflow/issues/30729#issuecomment-513132406", "Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have tried `export TF_ENABLE_AUTO_MIXED_PRECISION=1`\r\nand `os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'` \r\nthey are all no use", "@ymodak \r\nI am sorry for the latter reply,\r\nAnother question fused me ,\r\nwhen I use tf.contrib.rnn.LSTMBlockFusedCell, It is about 2\u00d7 times cost by tensorflow serving ( build from source 1.14 (GPU-2080TI cuda=10.1 version 1.13))than tensorflow(installed by pip install tensorflow-gpu==1.13.1)\r\nI am confused that whether I have not add some build options or the blockfused op just speed up in python api not in c++ api?\r\nthe options I used is `bazel build --color=yes --curses=yes --config=cuda --copt=\"-fPIC\" \\\r\n--copt=-O3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both \\\r\n${TF_SERVING_BAZEL_OPTIONS} \\\r\n--verbose_failures \\\r\n--output_filter=DONT_MATCH_ANYTHING \\\r\n${TF_SERVING_BUILD_OPTIONS} \\\r\ntensorflow_serving/model_servers:tensorflow_model_server`"]}, {"number": 32457, "title": "[TF 2.0] Feature request: let autograph accept collection inputs", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-dev20190901\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently it seems to me that there is no way to convert a function to graph if the input of the function is a variant length list of elements (except the simplest case when the elements themselves are all tensors with the same shape, for which we can use a TensorArray). \r\n\r\nAssume we have a function that runs well in graph mode for a certain type of input, say a tensor tuple containing 2 tensors with arbitrary shape:\r\n\r\n```\r\n@tf.function(input_signature=[(tf.TensorSpec(None), tf.TensorSpec(None))])\r\ndef do_something(input):\r\n    ....\r\n    return some_result\r\n```\r\n\r\nWe want to have another function that takes undetermined number of `input`s and merge the result, so we have:\r\n\r\n```\r\ndef merge(list_of_inputs):\r\n    return tf.reduce_sum(tf.stack([do_something(input) for input in list_of_inputs]))\r\n```\r\n\r\nThis function works well in eager mode, but we cannot add a `@tf.function` decorator to it, because `list_of_inputs` has no fixed length and the function will retrace for every calls, which is very slow. Even if we can pad the `list_of_inputs` to have the same length, it's still not working as the shapes of the elements might vary across different runs, so retracing will still be triggered.\r\n\r\nSo it would be great if we have something like `tf.Array`and `tf.ArraySpec`, which is defined as an array of objects that share the same specs. Then we can give the `merge` function a decorator like:\r\n\r\n```\r\n@tf.function(input_signature=[\r\n    tf.ArraySpec((tf.TensorSpec(None), tf.TensorSpec(None)))])\r\ndef merge(list_of_inputs):\r\n    return tf.reduce_sum(tf.stack([do_something(input) for input in list_of_inputs]))\r\n```\r\n and run it without retracing.\r\n\r\n**Will this change the current api? How?**\r\n\r\nAs described above, we propose to add `tf.Array`and `tf.ArraySpec`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone uses autograph will benefit. It would be much easier to write code that handles the case when a data example contains a set/list of variant number of objects, which is very common in NLP / vision / knowledge graph and so on.\r\n", "comments": ["To make sure I understand, you are suggesting adding support for heterogeneous dynamic lists, kind of like a `TensorArray` with elements of unknown shape? There are indeed primitive ops that should allow building such a type, although the gradients would probably become a bit more complicated.\r\n\r\nCould you elaborate a bit on the ArraySpec piece a bit - you defined it as an array of objects that share the same specs - in what way are the specs shared? In the example you gave, does merge take an arbitrary-length list of tuples, but the tuples all have the same shapes?", "\r\n> Could you elaborate a bit on the ArraySpec piece a bit - you defined it as an array of objects that share the same specs - in what way are the specs shared? In the example you gave, does merge take an arbitrary-length list of tuples, but the tuples all have the same shapes?\r\n\r\nYes, `merge` should take an arbitrary-length list of tuples, and the tuples all have the same signatures. I mention signatures rather than shapes here because signatures allow unknown dimension (i.e. the `None` value in `tf.shape()`), which is more flexible.\r\n", "Thanks! This makes sense. Essentially, this would be a more generic version of TensorArray that can hold structures, not just single Tensors.\r\n\r\nRight now, there are a few ways work with this kind of data (structures of TensorArray, Datasets), but neither as as practical as an actual Array class. So I think this is a good feature request.", "Just want to chime in here: This feature request is crucial to getting decent performance out of TF2 for Graph Neural Network implementations.\r\n\r\nIn the GNN setting, such a list of inputs are shape [None, 2] tensors of adjacency lists for different edge types. The number of edge types are fixed for a given instance of the GNN model, but of course not an inherent property of the model class. See https://github.com/microsoft/tf-gnn-samples/blob/master/gnns/rgcn.py for an example of a TF1 implementation of this, illustrating how the computation graph would look.\r\n\r\nThe performance difference between that example to a TF2 implementation without AutoGraph is roughly ~5x (!), which frankly makes TF2 unusable for serious uses of such models.\r\n\r\nThe solution proposed by @David-Mao would work well for this case: Practically, this would just mean that the number of elements in the input list (or tuple) would become part of the shape of inputs; and hence `foo(x, [v_0, ..., v_n], y)` should be handled no different than `foo(x, v_0, ..., v_n, y)` for implementation purposes, but with the advantage that the model author doesn't need to change `foo` every time `n` changes. Changing `n` should in most cases cause re-tracing (though interestingly enough, @David-Mao's example above wouldn't require that).\r\n[An extension of this idea would be to support dictionaries with a fixed keyset]", "Alright, we found \"fun\" workaround for this (left in the issue for anyone else encountering this problem).\r\n\r\nExisting code:\r\n```\r\nclass GNNInput(NamedTuple):\r\n    \"\"\"Input named tuple for the GNN.\"\"\"\r\n\r\n    node_features: tf.Tensor\r\n    adjacency_lists: Tuple[tf.Tensor, ...]\r\n\r\nclass GNN(tf.keras.layers.Layer):\r\n    [...]\r\n    def build(self, tensor_shapes: GNNInput):\r\n        [...]\r\n        super().build(tensor_shapes)\r\n\r\n    def call(self, inputs: GNNInput, training: bool = False):\r\n        [...]\r\n```\r\n\r\nNew `build`, enabling autograph with build-time constant shapes:\r\n```\r\n        call_input_spec = (\r\n            GNNInput(\r\n                node_features=tf.TensorSpec(\r\n                    shape=tf.TensorShape((None, initial_node_features_shape[1])), dtype=tf.float32\r\n                ),\r\n                adjacency_lists=tuple(\r\n                    tf.TensorSpec(shape=tf.TensorShape((None, 2)), dtype=tf.int32)\r\n                    for _ in range(len(adjacency_list_shapes))\r\n                ),\r\n            ),\r\n            tf.TensorSpec(shape=(), dtype=tf.bool),\r\n        )\r\n        setattr(self, \"call\", tf.function(func=self.call, input_signature=call_input_spec))\r\n```\r\n\r\nThis is clearly not pretty, but makes things go reasonably fast. Clearly, all this malarkey shouldn't be necessary, and a `@tf.function` should just magically work, but there you go.", "@mmjb thanks for sharing, it's a clever workaround!\r\n\r\nAs a side note, a few related APIs which generalize this notion to \"any structure of tensors of corelated shapes\" is [StructuredTensor](https://github.com/tensorflow/community/blob/master/rfcs/20190910-struct-tensor.md), (work-in-progress) and [RaggedTensor](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/ragged) (available, but not fully integrated with `tf.function` yet).\r\n\r\n@kkimdev FYI", "Thanks for pointing to the StructuredTensor RFC; that will clearly help for such models. We've been trying to use `NamedTuple`s for related things (see the sample above), but something that's better-integrated with the rest of tensorflow would be much appreciated...", "@mmjb To make sure I understand correctly, in your workaround `len(adjacency_list_shapes)` is fixed at the time of model building and cannot be changed for each different calls of `call()`, right? You didn't explicitly define `adjacency_list_shapes` in the code snippet so I don't know how that's passed...", "@David-Mao Yes, the list length is fixed at model-building time and is the same for all `call`s.", "@mmjb Thanks.\r\nIn my desired use case I would like to see the list length varies with calls, so this workaround doesn't work for me :(", "@David-Mao We could see you are using TF v2.0.0 which is not actively supported ,please refer this [comment](https://github.com/tensorflow/tensorflow/issues/52915#issuecomment-961288304). Could you please try to upgrade TF version to latest TF v2.7.0 and let us know ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32456, "title": "Add BuiltinOperatorName message in debug log", "body": "", "comments": ["Can you provide a sample of what the printed statement looks like (before and after)?", "> Can you provide a sample of what the printed statement looks like (before and after)?\r\n\r\nbefore : \r\n```\r\nNode   0 Operator Builtin Code  36\r\n  Inputs: 20 23\r\n  Outputs: 19\r\nNode   1 Operator Builtin Code  22\r\n  Inputs: 19 3\r\n  Outputs: 2\r\nNode   2 Operator Builtin Code   3\r\n  Inputs: 2 4 1\r\n  Outputs: 0\r\nNode   3 Operator Builtin Code  39\r\n  Inputs: 0 11\r\n  Outputs: 10\r\nNode   4 Operator Builtin Code  22\r\n  Inputs: 10 6\r\n  Outputs: 5\r\nNode   5 Operator Builtin Code  48\r\n  Inputs: 5 8\r\n  Outputs: 7 9\r\n....\r\n```\r\n\r\nafter : \r\n```\r\nNode   0 Operator Builtin Code  36 GATHER\r\n  Inputs: 20 23\r\n  Outputs: 19\r\nNode   1 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 19 3\r\n  Outputs: 2\r\nNode   2 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 2 4 1\r\n  Outputs: 0\r\nNode   3 Operator Builtin Code  39 TRANSPOSE\r\n  Inputs: 0 11\r\n  Outputs: 10\r\nNode   4 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 10 6\r\n  Outputs: 5\r\nNode   5 Operator Builtin Code  48 TOPK_V2\r\n  Inputs: 5 8\r\n  Outputs: 7 9\r\n...\r\n```"]}, {"number": 32455, "title": "Quantile Huber Loss to predict Specific Quantile Value", "body": "We know that we have huber loss function added in keras tf-2 already which can perform both kind of behaviour MSE and MAE depending on the scale of data. In most of the prediction and analysis models, we often do not need just median or mean predicted value, but we also need the specific quantile value of prediction. This is the loss function I am adding which is derived from **huber_loss** here\r\n[https://arxiv.org/pdf/1402.4624.pdf](https://arxiv.org/pdf/1402.4624.pdf). I name it **quantile_huber_loss** and the class which calls it is QuantileHuber. I have given great attention to the detail in implementing the function. function takes two extra arguements delta and quantile, where delta is common as the huber_loss and quantile is the float number between 0 and 1. Equation looks as below.\r\n![qhuber_form](https://user-images.githubusercontent.com/20843596/64757745-9a0aa680-d550-11e9-8568-e90471e73f56.png)\r\n", "comments": ["Please make the changes on master branch and then cherry-pick them to 2.0"]}, {"number": 32454, "title": "Calling tf.function from tf.py_function in dataset.map hangs.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS or Windows.\r\n- TensorFlow installed from (source or binary): binary.\r\n- TensorFlow version (use command below):2.0.0b1/rc0/rc1\r\n- Python version: 3.6.\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80\r\n\r\n**Describe the current behavior**\r\nCalling tf.function from tf.py_function in dataset.map hangs the program.\r\nBy removing tf.function decorator or enable run_functions_eagerly, program runs as expected.\r\n\r\n**Describe the expected behavior**\r\nCalling tf.function from tf.py_function does not hang the program.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef generate_feature(key):\r\n    if key > tf.constant(-1):\r\n        x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)\r\n        y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)\r\n    else:\r\n        x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)\r\n        y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)\r\n    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)\r\n    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)\r\n    return tf.stack([x, y])\r\n\r\ndef generate_feature_and_label(key):\r\n    feature = generate_feature(key)\r\n    if key > -1:\r\n        label = tf.constant(1)\r\n    else:\r\n        label = tf.constant(0)\r\n    return feature, label\r\n\r\ndef dataset_map(key):\r\n    feature, label = tf.py_function(func=generate_feature_and_label,\r\n                                    inp=[key],\r\n                                    Tout=[tf.float32, tf.int32])\r\n    feature = tf.ensure_shape(feature, [2])\r\n    label = tf.ensure_shape(label, [])\r\n    return feature, label\r\n\r\nif __name__ == '__main__':\r\n    print(tf.__version__)\r\n    # remove tf.function decorator or enable run_functions_eagerly to run successfully\r\n\r\n    # tf.config.experimental_run_functions_eagerly(True)\r\n    keys = list(range(-1000, 1000))\r\n    dataset = tf.data.Dataset.from_tensor_slices(keys)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.shuffle(buffer_size=len(keys))\r\n    dataset = dataset.map(map_func=dataset_map,\r\n                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.batch(4)\r\n\r\n    it = iter(dataset)\r\n    while True:\r\n        features, labels = next(it)\r\n        print(f'labels={labels}, features={features}')\r\n        input('press any key to continue...\\r\\n')\r\n```\r\n", "comments": ["Was able to reproduce the issue with Tensorflow 2.0.0.rc1. Please see the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/fdf0cddd3e7f02f433f573e782f905e1/untitled142.ipynb). Thanks!", "@alextp @jsimsa do you know of any technical limitations that might cause this pattern to hang?\r\n\r\n@makercob As a side note, the minimal repro shouldn't need `py_function` or `tf.function` because `Dataset.map` works in the same way as a `tf.function`. But I suspect your actual use case is a bit more complex. Anyway, the following code that has no `py_function` works:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef generate_feature(key):\r\n    if key > tf.constant(-1):\r\n        x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)\r\n        y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)\r\n    else:\r\n        x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)\r\n        y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)\r\n    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)\r\n    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)\r\n    return tf.stack([x, y])\r\n\r\ndef generate_feature_and_label(key):\r\n    feature = generate_feature(key)\r\n    if key > -1:\r\n        label = tf.constant(1)\r\n    else:\r\n        label = tf.constant(0)\r\n    return feature, label\r\n\r\ndef dataset_map(key):\r\n    feature, label = generate_feature_and_label(key)\r\n    feature = tf.ensure_shape(feature, [2])\r\n    label = tf.ensure_shape(label, [])\r\n    return feature, label\r\n\r\nif __name__ == '__main__':\r\n    print(tf.__version__)\r\n    # remove tf.function decorator or enable run_functions_eagerly to run successfully\r\n\r\n    # tf.config.experimental_run_functions_eagerly(True)\r\n    keys = list(range(-1000, 1000))\r\n    dataset = tf.data.Dataset.from_tensor_slices(keys)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.shuffle(buffer_size=len(keys))\r\n    dataset = dataset.map(map_func=dataset_map,\r\n                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.batch(4)\r\n\r\n    it = iter(dataset)\r\n    while True:\r\n        features, labels = next(it)\r\n        print(f'labels={labels}, features={features}')\r\n        input('press any key to continue...\\r\\n')\r\n```\r\n\r\n", "Indeed this is a TF runtime deadlock somewhere. The fact that it doesn't seem to respond to interrupts makes me think it's in the runtime and not in the python layer. @mhong can you triage this to someone who works on the runtime? Deadlocks are fairly serious...", "@mdanatg Yes, my actual data input pipeline involves tf.py_function in dataset map function, and tf.function is called in aforementioned tf.py_function, in hope of some performance gain.", "Sorry about the delay. TF runtime folks are looking into this bug. Will post updates here.", "The bad news:\r\n\r\nThe problem here is the use of `tf.function` inside of a `py_function` inside of a `tf.function` (introduced by tf.data around `dataset_map`). The current TensorFlow runtime will need two interop threadpool threads (one for the outer `tf.function` and another for the inner `tf.function`) to execute such function and if N (where N is the size of the interop threadpool thread) such functions are scheduled for execution concurrently (which can happen `tf.data.Dataset.map` is used with `num_parallel_calls`), then the entire execution can hang. \r\n\r\nThe good news:\r\n\r\nI don't see a reason why the user program (or any other program) should ever need to layer `tf.function` in a `py_function` in a `tf.function`. As per @mdanatg suggestion, with the exception of `if key > -1` (which can be replaced with `if key > tf.constant(-1`)), the code is already graph-compatible. In cases where the computation is not graph-compatible, my suggestion would be instead of \"nesting\" `py_function` and `tf.function` to \"compose\" `py_function` and `tf.function` using multiple `tf.data.Dataset.map` calls.", "@jsimsa Provided code above is just an example to replicate the issue.\r\nMy actual data input pipeline as follows:\r\nStep 1. read image and ROIs from database (in tf.py_function).\r\nStep 2. predicate and process ROIs (wished to done with tf.function in hope for performance gain).\r\nStep 3. augment the image and ROIs with OpenCV(in tf.py_function).\r\ntf.functions in step 2, are also used in scenarios other than data input pipeline.", "@makercob I think that is doable as @jsimsa suggested, without sacrificing the reusability of step 2, by breaking the py_function into two pieces:\r\n\r\n```\r\ndef read_from_database(key):  # step 1\r\n    # first py_function here\r\n    if key > tf.constant(-1):\r\n        x = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)\r\n        y = tf.random.uniform(shape=(), minval=0.0, maxval=20.0)\r\n    else:\r\n        x = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)\r\n        y = tf.random.uniform(shape=(), minval=40.0, maxval=100.0)\r\n    return x, y\r\n\r\ndef predicate_and_process(xy):  # step 2\r\n    x, y = xy\r\n    x = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(x)\r\n    y = tf.math.sign(tf.random.uniform(shape=(), minval=-1.0, maxval=1.0)) * tf.abs(y)\r\n    return x, y\r\n\r\ndef augment(xy): # step 3\r\n    # second py_function here\r\n\r\nds = ds.map(read_from_database)\r\nds = ds.map(predicate_and_process)\r\nds = ds.map(augment)\r\n```\r\n\r\nWDYT?", "@mdanatg No, not doable in my implementation, in which tf.function and py function usage are quite intertwined.", "+1 @mdanatg \r\n@makercob In your case: map_fn -> py_func -> tf.function, TF actually creates two graphs, one for map_fn and one for tf.function. It will introduce some extra overhead. For example ops in two graphs cannot be executed in parallel. @mdanatg's suggestion can maximize your performance gain.\r\n\r\nBut anyway, we are fixing this hanging issue by avoiding using threads from interop threadpool to execute py_func. Will update the thread when the fix is in.\r\n", "Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ea6fd93e618b4268a2fa29f54d92c71e/32454.ipynb). Thanks!", "I'm having a similar problem, though no intertwined use of py_function and tf_function. I suspect it's the same (or similar) problem. The first map() call uses a tf.numpy_function(), and the second map() uses a call to a tf.py_function(). Both those functions avoid any use of tensorflow functions.\r\n\r\nAs soon as I set # threads>1 in both calls to map() - or autotune - the pipeline deadlocks. If one of them is limited to 1 it works but is slow.\r\n\r\nAny recommendations on how to troubleshoot this? Tried tfnightly but no difference.\r\n", "> +1 @mdanatg\r\n> @makercob In your case: map_fn -> py_func -> tf.function, TF actually creates two graphs, one for map_fn and one for tf.function. It will introduce some extra overhead. For example ops in two graphs cannot be executed in parallel. @mdanatg's suggestion can maximize your performance gain.\r\n> \r\n> But anyway, we are fixing this hanging issue by avoiding using threads from interop threadpool to execute py_func. Will update the thread when the fix is in.\r\n\r\nI'm also seeing the same problem when using a `tf_py_function` wrapped in a `map()` during multi-GPU training. @qqfish any progress on this?", "I can able  to reprouduce the issue in TF 2.5 version and it is still hanging. Thanks!  ", "Hi @makercob , This issue is not replicating TF 2.6 version , Loop continued until key board interrupt happened . Providing [Gist ](https://colab.research.google.com/gist/mohantym/5b7c54efcbe3d630eb5a25397e2d8463/github_32454.ipynb)for reference", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32454\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32454\">No</a>\n"]}, {"number": 32453, "title": "Add tfrecord support to saved_model_cli", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n``` bash\r\nsaved_model_cli run --inputs=\"x=/tmp/example.[npy,npz]\" --dir=<model-dir> --tag_set=serve --signature_def=serving_default\r\n```\r\nsaved_model_cli does not support tfrecord as inputs \r\n**Will this change the current api? How?**\r\n``` bash\r\nsaved_model_cli run --inputs=/tmp/example.tfrecord --dir=<model-dir> --tag_set=serve --signature_def=serving_default\r\n```\r\n**Who will benefit with this feature?**\r\nDevelopers always use tfrecord to warm up serving, it will be more convenient for them to use tfrecord as saved_model_cli run inputs.\r\n**Any Other info.**\r\n", "comments": ["Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "I will use it to test saved model and troubleshoot model problems when serving is not available", "How can I verify the tfrecord file? Any info from file context.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32452, "title": "Unimplemented: this graph contains an operator of type Abs for which the quantized form is not yet implemented", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. Got Insightface trained model directly from https://drive.google.com/open?id=1Iw2Ckz_BnHZUi78USlaFreZXylJj7hnP\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.12.1-9365-gff401a6 1.15.0-dev20190821\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: Not installed\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 8.1.0-5ubuntu1~16.04) 8.1.0\r\n- **CUDA/cuDNN version**: CPU only\r\n- **GPU model and memory**: CPU only\r\n- **Exact command to reproduce**: tflite_convert \\\r\n--output_file=insightface_quant.tflite \\\r\n--graph_def_file=insightface.pb \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_dev_values=128 \\\r\n--input_arrays=img_inputs \\\r\n--output_arrays=resnet_v1_50/E_BN2/Identity \\\r\n--default_range_min=0 \\\r\n--default_range_max=127\r\n\r\n\r\n### Describe the problem\r\nI am trying to convert insightface pre-trained model so that I can run it on EdgeTPU. First, I am trying to convert the .pb model into tflite model with both weights and activations quantized to 8bit integer. That's when I get the error below. Is the support for operator Abs coming soon?\r\n2019-09-11 16:41:12.872535: W tensorflow/lite/toco/graph_transformations/quantize.cc:139] Constant array resnet_v1_50/conv1/W_conv2d lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2019-09-11 16:41:12.872690: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Abs for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\r\n\r\n\r\n### Source code / logs\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0911 16:39:50.219565 140227897026304 __init__.py:690] \r\n\r\n  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\r\n\r\n  Please upgrade your code to TensorFlow 2.0:\r\n    * https://www.tensorflow.org/beta/guide/migration_guide\r\n\r\n  Or install the latest stable TensorFlow 1.X release:\r\n    * `pip install -U \"tensorflow==1.*\"`\r\n\r\n  Otherwise your code may be broken by the change.\r\n\r\n  \r\n2019-09-11 16:39:50.225242: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-11 16:39:50.246574: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3504000000 Hz\r\n2019-09-11 16:39:50.246796: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x490a580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-09-11 16:39:50.246817: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/home/crossbar/.local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 515, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 511, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 199, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0911 16:39:52.657464 140689256613632 __init__.py:690] \r\n\r\n  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\r\n\r\n  Please upgrade your code to TensorFlow 2.0:\r\n    * https://www.tensorflow.org/beta/guide/migration_guide\r\n\r\n  Or install the latest stable TensorFlow 1.X release:\r\n    * `pip install -U \"tensorflow==1.*\"`\r\n\r\n  Otherwise your code may be broken by the change.\r\n\r\n  \r\n2019-09-11 16:39:53.139500: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1363 operators, 1866 arrays (0 quantized)\r\n2019-09-11 16:39:53.171158: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1363 operators, 1866 arrays (0 quantized)\r\n2019-09-11 16:39:53.395100: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"resnet_v1_50/E_Dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2019-09-11 16:39:53.398795: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 292 operators, 508 arrays (1 quantized)\r\n2019-09-11 16:39:53.536832: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"resnet_v1_50/E_Dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2019-09-11 16:39:53.545258: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 291 operators, 507 arrays (1 quantized)\r\n2019-09-11 16:39:53.549759: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"resnet_v1_50/E_Dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2019-09-11 16:39:53.561449: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 288 operators, 501 arrays (1 quantized)\r\n2019-09-11 16:39:53.562454: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"resnet_v1_50/E_Dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2019-09-11 16:39:53.567925: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 288 operators, 501 arrays (1 quantized)\r\n2019-09-11 16:39:53.570066: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 288 operators, 501 arrays (1 quantized)\r\n2019-09-11 16:39:53.573091: F tensorflow/lite/toco/tooling_util.cc:1728] Array resnet_v1_50/bn0/batchnorm/add_1, which is an input to the Abs operator producing the output array resnet_v1_50/prelu0/Abs, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007ff4c52a0700 (most recent call first):\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/home/crossbar/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/home/crossbar/.local/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n\r\n\r\n", "comments": ["Same error trying to convert MTCNN model for face detection on EdgeTPU.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32452\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32452\">No</a>\n"]}, {"number": 32451, "title": "Move AutoMixedPrecision ahead of GenericLayoutOptimizer", "body": "The layout optimizer grappler pass sets the data layout based on the dtype of the op.\r\nThe mixed precision optimizer needs to proceed the layout optimizer because it will\r\nchange dtypes from float32 to float16.", "comments": []}, {"number": 32450, "title": "node softmax_cross_entropy_with_logits", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n\r\n- TensorFlow version:1.14.0\r\n- Python version:3.6.8\r\n\r\n\r\n**Describe the problem**\r\n\r\n** I have created custom model for cifar 10 dataset. I m unable to use tf.nn.softmax_cross_entropy_with_logits_v2 gets   Invalid argument: logits and labels must be broadcastable: logits_size=[128,10] labels_size=[1,128]\r\n\r\nThe entire source code can be viewed at \r\nhttps://colab.research.google.com/drive/11tyipxzE9qkmlwkiPhs-A8I2OvT5Jebi", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "I have given shared permission you can check now", "@joyjeni \r\nCan you please help me with the files. It helps us to reproduce the issue in our environment.Thanks!\r\n", "It's fixed from my end. It was the behavior with calculating the loss on batches", "I am closing this issue since it looks to be fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32450\">No</a>\n", "> It's fixed from my end. It was the behavior with calculating the loss on batches\r\n\r\n@joyalbin \r\nHello, I have encountered the same error:\r\n\"Error recorded from training_loop: logits and labels must be broadcastable: logits_size=[4000,2] labels_size=[8,2]\"\r\n`  cross_entropy = tf.losses.softmax_cross_entropy(\r\n      logits=logits, \r\n      onehot_labels=labels,\r\n      label_smoothing=FLAGS.label_smoothing)`\r\n\r\nHow to fix this error?"]}, {"number": 32449, "title": "C API: Unexpected behaviour when multithreading calls of c_api's TF_SessionRun", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): [Official download URL from TF's website. ](https://www.tensorflow.org/install/lang_c),Which is then downloaded via nuget package.\r\n- TensorFlow version (use command below): Unable, we use c_api, the dll is downloaded from [here](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.14.0.zip)\r\n- Python version: -\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: We use CPU\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nI'm working with our team on [Tensorflow.NET](https://github.com/SciSharp/TensorFlow.NET/) and we wanted to exploit C#'s multithreading capabilities with Tensorflow. \r\nOur library is a port of close to 1-1 from python to C# with couple of changes to allow multithreading to work.\r\nWe use a different Session and Graph for every Thread so every thread is actually isolated from the other Threads.\r\nWhen calling `c_api.TF_SessionRun` from 10 parallel threads in an infinite loop for a minute or so - some sort of corruption occurs that causes Tensorflow to terminate with the following error code \r\n\r\n`Code -1073740791 (0xc0000409).` which is:\r\n> Stack buffer overflow / overrun. Error can indicate a bug in the executed software that causes > stack overflow, leading to abnormal termination of the software.\r\n\r\nThis does not occur every run, in-fact it might occur 1/10 and only after 1-5 minutes of running.\r\n\r\nIn most cases, the program will just terminate but even more rarely the following message is printed:\r\n> 2019-09-12 02:47:15.298642: F tensorflow/core/framework/tensor.cc:928] Check failed: buf_ null buf_ with non-zero shape size 15\r\n\r\nWe discussed the issue in our repository: https://github.com/SciSharp/TensorFlow.NET/issues/380\r\n\r\n**Describe the expected behavior**\r\nRun smoothly, `c_api.TF_SessionRun` expected to be thread-safe when used with a separate Session and Graph for every thread.\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/Nucs/TensorFlowNetMultithreading\r\nRequirements: Visual Studio 2017+\r\nHappens both in Debug or Release builds.\r\n\r\nThe tensorflow.dll we use is not built on Debug, in order to use a custom tensorflow.dll with a .pdb, replace the one in `CalcEventsTFS\\bin\\Debug\\net461 or netcoreapp2.2\\` and then run `CalcEventsTFS.exe`\r\n\r\nSimply build the solution and run the only project there is. \r\nI recommend to restart the program if it did not occur within 1 minute.\r\n\r\nIf you are getting `Unable to load DLL 'tensorflow': The specified module could not be found.`, \r\nCopy your own tensorflow.dll of version 1.40 CPU for windows to the output directory: \r\n> CalcEventsTFS\\bin\\Debug\\net461 or netcoreapp2.2\\\r\n\r\n**Other info / logs**\r\nWe managed to get a dump after the termination of the program available from [here]( https://mega.nz/#!VVF3HKrS!r5sxOBsbccdVn93KpwS7EtxWSvtLlim2GwExDF8L9h4).\r\n\r\n![image](https://user-images.githubusercontent.com/649919/64742960-c1d51c80-d506-11e9-9f2d-c4f7de2234b8.png)\r\n\r\n", "comments": ["Any update on the issue? This bug prevents our users from using Tensorflow in a multithreaded manner in production.", "@Nucs Latest [tensorflow.dll ](https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-653621878)for c# api ", "I experience similar symptoms. Is it at least reproduced? Confirmed? I guess it is not solved for sure.\r\n\r\nWe use the same graph and session, initialized once, to run inference with a call to `TF_SessionRun` from different threads. It is CPU inference in our case. The app is written in C++ and loads TF as a shared lib on Linux and Windows. Using more than one session adds a considerable amount or RAM usage (doubles, triples, etc - depending on the number of threads and sessions).\r\n\r\nI thought it is safe to use like that, but now I'm not sure - can't even recall where I've seen that it was safe after all. The header file (namely c_api.h) sure doesn't say so, there is almost no mentions of thread safety at all.\r\n\r\nCan somebody at least clarify if such usage is intended to work and is safe?\r\n\r\n**P.S.** by the way, we used the latest TF version, way beyond 2.0. Doesn't help a bit.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32449\">No</a>\n", "> I experience similar symptoms. Is it at least reproduced? Confirmed? I guess it is not solved for sure.\r\n> \r\n> We use the same graph and session, initialized once, to run inference with a call to `TF_SessionRun` from different threads. It is CPU inference in our case. The app is written in C++ and loads TF as a shared lib on Linux and Windows. Using more than one session adds a considerable amount or RAM usage (doubles, triples, etc - depending on the number of threads and sessions).\r\n> \r\n> I thought it is safe to use like that, but now I'm not sure - can't even recall where I've seen that it was safe after all. The header file (namely c_api.h) sure doesn't say so, there is almost no mentions of thread safety at all.\r\n> \r\n> Can somebody at least clarify if such usage is intended to work and is safe?\r\n> \r\n> **P.S.** by the way, we used the latest TF version, way beyond 2.0. Doesn't help a bit.\r\n\r\n@roman-kruglov \r\nHave you found any solution to that or you enforced somehow to run it on single thread? I am facing the same problem. My priority is to use it in multi-thread environment. ", "Problem was somewhere else in the code, nothing to do with TF_SessionRun. I can successfully run with multithreads. Using TF 2.5.1. "]}, {"number": 32448, "title": "AutoGraph could not transform pfor when using GradientTape.jacobian", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04.6 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **v2.0.0-rc0-0-gc75bb66**\r\n- Python version: **3.7**\r\n- Bazel version (if compiling from source): **0.25.2**\r\n- GCC/Compiler version (if compiling from source): **5.4.0 20160609**\r\n- CUDA/cuDNN version: **No**\r\n- GPU model and memory: **No**\r\n\r\n**Describe the current behavior**\r\n`GradientTape.jacobian` is showing warnings about `WARNING:tensorflow:Entity <function pfor.<locals>.f at 0x7f5e7807af28> could not be transformed and will be executed as-is. `\r\n\r\n**Describe the expected behavior**\r\nNo warnings shown.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nW = tf.random.normal((6,1), dtype=tf.float32)\r\nwith tf.GradientTape() as tape:\r\n    y = tf.reduce_sum(W ** 3)\r\ntape.jacobian(y, W)\r\n```\r\n**Other info / logs**\r\n\r\nlogs by setting `AUTOGRAPH_VERBOSITY=10`\r\n\r\n```\r\nINFO:tensorflow:Converted call: <function pfor.<locals>.f at 0x7f5e7807af28>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nConverted call: <function pfor.<locals>.f at 0x7f5e7807af28>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Entity <function pfor.<locals>.f at 0x7f5e7807af28> is not cached for key <code object f at 0x7f5deb7a2390, file \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 183> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f5dd8004eb8>, frozenset({'loop_fn', 'parallel_iterations', 'iters'}))\r\nEntity <function pfor.<locals>.f at 0x7f5e7807af28> is not cached for key <code object f at 0x7f5deb7a2390, file \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 183> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f5dd8004eb8>, frozenset({'loop_fn', 'parallel_iterations', 'iters'}))\r\nINFO:tensorflow:Converting <function pfor.<locals>.f at 0x7f5e7807af28>\r\nConverting <function pfor.<locals>.f at 0x7f5e7807af28>\r\nINFO:tensorflow:Source code of <function pfor.<locals>.f at 0x7f5e7807af28>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\ndef f():\r\n  return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n\r\n\r\nSource code of <function pfor.<locals>.f at 0x7f5e7807af28>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\ndef f():\r\n  return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n\r\n\r\nINFO:tensorflow:Error transforming entity <function pfor.<locals>.f at 0x7f5e7807af28>\r\nTraceback (most recent call last):\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n    free_nonglobal_var_names)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 699, in node_to_graph\r\n    node = converter.apply_(node, context, function_scopes)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 409, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 120, in transform\r\n    return FunctionBodyTransformer(ctx).visit(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 87, in visit_FunctionDef\r\n    node = self.generic_visit(node)\r\n  File \"/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py\", line 317, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 44, in visit_Return\r\n    value=node.value)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 261, in replace\r\n    replacements[k] = _convert_to_ast(replacements[k])\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 223, in _convert_to_ast\r\n    return gast.Name(id=n, ctx=None, annotation=None)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/gast/gast.py\", line 19, in create_node\r\n    format(Name, nbparam, len(Fields))\r\nAssertionError: Bad argument number for Name: 3, expecting 4\r\nError transforming entity <function pfor.<locals>.f at 0x7f5e7807af28>\r\nWARNING:tensorflow:Entity <function pfor.<locals>.f at 0x7f5e7807af28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <function pfor.<locals>.f at 0x7f5e7807af28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nTraceback (most recent call last):\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n    free_nonglobal_var_names)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 699, in node_to_graph\r\n    node = converter.apply_(node, context, function_scopes)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 409, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 120, in transform\r\n    return FunctionBodyTransformer(ctx).visit(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 87, in visit_FunctionDef\r\n    node = self.generic_visit(node)\r\n  File \"/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py\", line 317, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/youngw/.pyenv/versions/3.7.2/lib/python3.7/ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 44, in visit_Return\r\n    value=node.value)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 261, in replace\r\n    replacements[k] = _convert_to_ast(replacements[k])\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 223, in _convert_to_ast\r\n    return gast.Name(id=n, ctx=None, annotation=None)\r\n  File \"/home/youngw/.local/share/virtualenvs/DebugML-YyVGQQuJ/lib/python3.7/site-packages/gast/gast.py\", line 19, in create_node\r\n    format(Name, nbparam, len(Fields))\r\nAssertionError: Bad argument number for Name: 3, expecting 4\r\n```", "comments": ["getting same issue, any resolve? (though mine does run, and it runs way quicker than w/o tf.function but still annoying to see this each time)\r\n", "@mshlis I think I found the bug. The current Tensorflow implementation assumes `gast==0.2.2` but doesn't lock in the version. The newest `gast` is 0.3.0 which introduced some breaking changes. In order to fix this just run `pip install -U gast==0.2.2`.", "yes, gast version was updated on 9/7/19 and does introduce breaking changes. set to 0.2.2, but the next rc and final release will have this.  The r2.0 branch has been updated with the gast version set to 0.2.2. if you want to build again.", "@dovahcrow ,\r\nJust let us know if the issue can be closed as the solution is found? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32448\">No</a>\n", "Similar problem faced, but with a different function being used in the onset_frames model (https://github.com/tensorflow/magenta/tree/master/magenta/models/onsets_frames_transcription) in magenta. Some sox functions are being called to transform wav data (See audio_transform.py at the above link). The full log with verbosity=10 is too long to paste in here, but the gist is as follows:\r\n\r\n> WARNING:tensorflow:Entity <bound method Popen._communicate of <subprocess.Popen object at 0x7fdac7adaa20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nW1104 14:27:51.073054 140575974086400 ag_logging.py:146] Entity <bound method Popen._communicate of <subprocess.Popen object at 0x7fdac7adaa20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\noutput_file: /tmp/tmpldd7_qhn.wav already exists and will be overwritten on build\r\nW1104 14:27:51.089764 140575974086400 api.py:546] output_file: /tmp/tmpldd7_qhn.wav already exists and will be overwritten on build\r\nI1104 14:27:51.103564 140575974086400 api.py:546] Executing: sox -D -G -V2 /tmp/tmpsh85acsd.wav /tmp/tmpldd7_qhn.wav pitch -7.682483 contrast 78.521843 equalizer 32.158569 2.000000q -2.897976 equalizer 2681.614259 2.000000q -1.408564 reverb 5.793800 50.000000 100.000000 100.000000 0.000000 0.000000\r\nWARNING:tensorflow:Entity <bound method Popen._communicate of <subprocess.Popen object at 0x7fdac7c41b38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nW1104 14:27:51.188622 140575974086400 ag_logging.py:146] Entity <bound method Popen._communicate of <subprocess.Popen object at 0x7fdac7c41b38>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nI1104 14:27:51.221368 140575974086400 api.py:546] Created /tmp/tmpldd7_qhn.wav with effects: pitch contrast equalizer equalizer reverb\r\n\r\nThis goes on filling the buffer and finally results in OOM error on Nvidia Titan X"]}, {"number": 32447, "title": "How to use Install TensorFlow for C", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): https://tensorflow.google.cn/install/lang_c\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nUse Visual Studio 2017 Import TensorFlow C library Windows CPU only.  run code Example program , tensorflow.dll : fatal error LNK1107: 0x358\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Dd-ouni \r\n\r\nCan you please let us know the TensorFlow version you are using.Provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32447\">No</a>\n"]}, {"number": 32446, "title": "Bug in NonMaxSuppressionV3 GPU op added by PR #30893", "body": "It broke a test with some large network at head. I'll provide a repro later.", "comments": ["@aaroey Can you add a bit more specifics so that it helps to the investigation from our side as well. Does it lead to a crash, less boxes, more boxes or different set of boxes than expected?"]}, {"number": 32445, "title": "[r1.15 CherryPick]: [INTEL-MKL] Use tf_opts when compiling MKL related eager files.", "body": "MKL ops was accidentally disabled in eager mode (my bad!). This PR re-enables MKL ops in eager mode. Context: https://github.com/tensorflow/tensorflow/pull/32317", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32445) for more info**.\n\n<!-- need_author_consent -->", "Manually set CLA to yes since this is from a commit that's already merged in master.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32445) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 32444, "title": "[ROCm] fix CSB build", "body": "Co-authored with @deven-amd.  This PR fixes recent failures in the Linux AMD ROCm GPU Nightly CSB build.  This PR does not depend on https://github.com/tensorflow/tensorflow/pull/32296, however that PR is also required before CSB build is successful again.", "comments": ["Additional commits needed due to new CSB brokenness today.  (https://github.com/tensorflow/tensorflow/pull/32444/commits/d0561bb4bd404e053d6d2aa5483366c13b549951)", "@chsigg thank you for your earlier review approval.  Ubuntu Sanity test caught an error in our last commit, now fixed.  I hope you can review again soon.  Thanks.", "@smit-hinsu can we get more information on why this PR was rolled-back? \r\n\r\nWe need the changes in this PR to make ROCm CSBs pass. How do we go about getting this PR back in?\r\n\r\nThanks\r\n\r\ndeven", "Sorry about the rollback without any details.\r\n\r\nThis was causing some integration test to fail internally. I don't understand the test so I have asked @chsigg to follow-up with you to help roll-forward the PR.", "@chsigg, let me know how we can help out on our end. We would like to get this PR rolled back soon :)\r\n\r\n@smit-hinsu, Is there any insight/detail you can provide on the nature of the failure. We now have 2 PRs (this one + PR #32296 )that are needed to get ROCm CSBs back to working status, and both of them are running into internal failures...would love to help resolve them in an expedient manner.\r\n\r\nthanks\r\n\r\ndeven\r\n\r\n\r\n\r\n\r\n", "Gentle ping @chsigg @smit-hinsu ", "I will work on getting this rolled forward.", "The PR got rolled forward, but I had to disable the nccl_manager_test on ROCm completely (because the tag would have triggered on multi_gpu as well, and the test currently fails on CUDA). I hope this does unblock you though.", "Thank you @chsigg.  We're now unblocked to move on to the next community supported build failure.\r\n\r\nConcerning the nccl_manager_test failing for CUDA, we did test #32296 on CUDA before creating the PR to make sure it would work for both ROCm and CUDA paths.  Please let me know how I can help unblock the nccl_manager_test."]}, {"number": 32443, "title": "[r1.15 CherryPick]: Upgrading giflib to fix CVE-2019-15133", "body": "Add a patch file to fix giflib's compilation issue on Windows (replace a call to strtok_r with strtok_s).\r\nBased on PR: #32169.\r\n\r\n**NVD**: 2019/08/17 - CVSS v2.0 Base Score: 4.3 - CVSS v3.0 Base Score: 6.5\r\nIn GIFLIB before 2019-02-16, a malformed GIF file triggers a divide-by-zero exception in the decoder function DGifSlurp in dgif_lib.c if the height field of the ImageSize data structure is equal to zero.\r\n\r\nSource | Link | Type\r\n---- | ---- | ----\r\nMISC | bugs.chromium.org | Mailing List, Third Party Advisory\r\nUBUNTU | usn.ubuntu.com | Third Party Advisory\r\n\r\nPiperOrigin-RevId: 267533902", "comments": []}, {"number": 32442, "title": "[r1.15 CherryPick]: [INTEL MKL] Add support for Addv2", "body": "Recent changes to AddV2 in TF (https://github.com/tensorflow/tensorflow/commit/fc61fa8725db3229fc7dd08de3d2f664f87abbb4) caused up to ~50% regression in some of Intel's benchmarks. This PR fixes that. \r\n\r\nOriginal PR: #32124", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32442) for more info**.\n\n<!-- need_author_consent -->", "@agramesh1 Could you please post `@googlebot I consent.`? Thank you!", "@googlebot I consent", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32442) for more info**.\n\n<!-- ok -->"]}, {"number": 32441, "title": "[r2.0-CherryPick]:Cherrypick batch_dot behavior change", "body": "This is a somewhat significant API behavior change that we only detected belatedly. It should be cherrypicked.", "comments": []}, {"number": 32440, "title": "[r2.0 Cherrypick]: Cherrypick `cast_to_floatx` new behavior", "body": "Since this is an API change, it should preferably be cherrypicked.", "comments": []}, {"number": 32439, "title": "[r2.0] cherry-pick request: Exclude core, python, compiler, tools and examples from the public API ", "body": "", "comments": []}, {"number": 32438, "title": "[r2.0 CherryPick]: [INTEL-MKL] Use tf_opts when compiling MKL related eager files.", "body": "MKL ops was accidentally disabled in eager mode (my bad!). This PR re-enables MKL ops in eager mode. Context: https://github.com/tensorflow/tensorflow/pull/32317", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32438) for more info**.\n\n<!-- need_author_consent -->", "@agramesh1 Could you please post `@googlebot I consent.`? Thank you!", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32438) for more info**.\n\n<!-- ok -->"]}, {"number": 32437, "title": "Cannot cross-compile ARM64 on Mac host", "body": "Following https://www.tensorflow.org/lite/guide/build_arm64 is not possible on Mac, `./tensorflow/lite/tools/make/build_aarch64_lib.sh` returns\r\n\r\n```\r\n...\r\n/bin/bash: aarch64-linux-gnu-g++: command not found\r\n...\r\n```", "comments": ["Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32437\">No</a>\n"]}, {"number": 32436, "title": "r1.15 cherry-pick request: Exclude core, python, compiler, tools and examples from the public API", "body": "This needs to be cherrypicked because these symbols should not be visible in the public API.\r\n\r\nPiperOrigin-RevId: 266214332", "comments": []}, {"number": 32435, "title": "Eliminate no-op convert pair in algebraic simplifier", "body": "Few examples of cases that this optimization handles:\r\n\r\n1. convert(convert(A, $TYPE1), $TYPE2) is simplified to A if A is of $TYPE2 and convert(A, $TYPE1) is an upcast\r\n2. convert(convert(A, $TYPE1), $TYPE2) is NOT simplified to A even if A is of $TYPE2 since convert(A, $TYPE1) is a downcast\r\n3. convert(convert(A, $TYPE1), $TYPE2) is simplified to A if A is of $TYPE2 and convert(A, $TYPE1) is an upcast and is a conversion from unsigned to signed which is allowed\r\n4. convert(convert(A, $TYPE1), $TYPE2) is NOT simplified to A even if A is of $TYPE2 and convert(A, $TYPE1) is an upcast since convert(A, $TYPE1) is conversion from signed to unsigned which is NOT allowed\r\n5. Tuple(convert(A, $TYPE1) , floor(convert(convert(A, $TYPE1), $TYPE2)), convert(convert(A, $TYPE1), $TYPE2)) is simplified to Tuple(convert(A, $TYPE1) , floor(A), A) showing a case where the first convert has a fan-out\r\n6. Tuple(floor(convert(convert(A, $TYPE1),$TYPE2)), convert(convert(A, $TYPE1), $TYPE2)) is simplified to Tuple(floor(A), A) showing a case where the first convert ends up being removed by dead-code elimination\r\n7. convert(convert(A, $TYPE1), $TYPE2) is NOT simplified to A even if A is of $TYPE2 and convert(A, $TYP1) is an upcast since convert(A, $TYP1) is conversion from integral to float(type change) which is NOT allowed\r\n\r\nAdded tests as well", "comments": []}]