[{"number": 29966, "title": "GPU docs don't show how to test your install", "body": "https://www.tensorflow.org/install/gpu \r\n\r\nhow do you test your install to make sure everything went OK?", "comments": ["Hi,\r\nYou can follow the guides from here:\r\n[TF 1.x](https://www.tensorflow.org/guide/using_gpu)\r\n[TF 2.x](https://www.tensorflow.org/beta/guide/using_gpu)\r\n[stack overflow](https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell)", "@bionicles : Have a look on @shaolinkhoa's suggestion and let us know if that resolves the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29965, "title": "\"mean_squared_error\" gives incorrect results in conjunction with sample weights", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n\r\nThere is a difference between passing the string `\"mean_squared_error\"` or passing `tf.keras.losses.mean_squared_error` as loss function in conjunction with sample weights.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test_loss_function(loss_function):\r\n    print(\"Testing {}\".format(loss_function))\r\n\r\n    layer = tf.keras.layers.Input(shape=(1,))\r\n    model = tf.keras.Model(inputs=layer, outputs=layer)\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss=loss_function)\r\n\r\n    weights = np.array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\r\n    model.evaluate(np.zeros(10), np.ones(10), sample_weight=weights)\r\n\r\ntest_loss_function('mean_squared_error')\r\ntest_loss_function(tf.keras.losses.mean_squared_error)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTesting mean_squared_error\r\n10/10 [==============================] - 0s 4ms/sample - loss: 0.5000\r\nTesting <function mean_squared_error at 0x7f657d0d6a60>\r\n10/10 [==============================] - 0s 2ms/sample - loss: 1.0000\r\n```", "comments": ["It seems like `\"mean_squared_error\"` is simply not weighted by `sample_weight`. It gives the same result as `tf.keras.losses.mean_squared_error` as a (non weighted) metric.", "In TensorFlow 2.0 Beta all loss functions seem to behave in the way `\"mean_squared_error\"` behaves. This is the output in TensorFlow 2.0.0-beta1:\r\n\r\n```\r\nTesting mean_squared_error\r\n10/10 [==============================] - 0s 2ms/sample - loss: 0.5000\r\nTesting <function mean_squared_error at 0x7fb89b5a8ea0>\r\n10/10 [==============================] - 0s 2ms/sample - loss: 0.5000\r\n```\r\n\r\nI don't know much about the internal workings and I didn't look at the code but this is my interpretation:\r\n\r\nThe behavior of `\"mean_squared_error\"` in TensorFlow 1.13 and all loss functions in TensorFlow 2.0 can be described the following way: The losses per sample (and per time step if `sample_weight_mode='temporal'`) are weighted by `sample_weight`. Then they are used for SGD, everything good so far. Now the losses are averaged to provide them as a metric to the user. This is done by adding them up and dividing them by the number of losses (the batch size or the batch size times the number of time steps if `sample_weight_mode='temporal'`).\r\n\r\nConsider a fixed batch size, the last batch is just containing a few samples and is filled up with zero entries. Sample weights are used to mask these zero entries from SGD. The weighted losses are now consisting of the few losses from the samples and a lot of zeros. The user will see a much lower loss on the last batch. Or in my case with less and less samples per batch (I sort the samples by sequence length before batching) it looked like the loss was going down - if only! Although confusing for the user this is true to the mathematics behind the model.\r\n\r\nLoss functions in TensorFlow 1.13 (except `\"mean_squared_error\"`) instead of dividing by the number of losses divide by the sum of all sample weights. This avoids misinterpretation from the user but also hides the true loss value.\r\n\r\nYou can add your loss functions additionally as `weighted_metrics` and they will be calculated the intuitive way. So the TensorFlow 2.0 behavior is superior because it allows for both. Although a warning in the documentation of the `sample_weight` parameter explaining how to interpret the loss values would be helpful.", "@timakro I was able to reproduce the issue with TF1.13. But, in the most recent version TF1.14, the loss function is working as expected. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/07cb3ebf319020ff34764bfc568f5ff4/tf29965_metric_error.ipynb) here. Please let me know what you think. Thanks!", "Thanks, seems fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29965\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29965\">No</a>\n"]}, {"number": 29964, "title": "Different metrics and loss values in model.fit and model.evaluate for the same batch of data", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: cuda 10.0, cudnn 7.5.1\r\n- GPU model and memory: NVIDIA GeForce RTX 2070\r\n\r\n**Describe the current behavior**\r\n\r\nWhen performing the evaluation on a model I get values for metrics and loss that differ from those obtained at the last training epoch. \r\n\r\nThe code and the output is given [below](#issuecomment-504010990).\r\n\r\n**Describe the expected behavior**\r\n\r\nBecause the batch of data is the same and the metrics computed are the same, I expect to get exactly the same values.\r\n", "comments": ["Will it be possible for you to provide a full code snippet that can help us to reproduce the issue. Thanks!", "Sure. Here is a simple code to reproduce the issue.\r\nNote that I used only one big batch because I read somewhere that the metrics monitored during the training were computed after each batch update and averaged across the epoch.\r\n\r\nCode:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\n\r\n#######################\r\n# Get subset of MNIST #\r\n#######################\r\n\r\nbatch_size = 1024\r\nnum_classes = 10\r\nh, w = 28, 28\r\n\r\n(x_train, y_train), (_, _) = mnist.load_data()\r\nx_train = x_train[:1024]\r\nx_train = x_train.reshape(x_train.shape[0], h, w, 1) # default tf format is NHWC\r\ny_train = y_train[:1024]\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n    \r\n\r\n###############\r\n# build model #\r\n###############\r\n\r\ninputs = keras.Input(shape=(h, w, 1), batch_size=batch_size)\r\nx = layers.Conv2D(32, (3, 3))(inputs)\r\nx = layers.Activation('relu')(x)\r\nx = layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2))(x)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(100, activation='elu', name='logits')(x)\r\noutputs = layers.Dense(num_classes, activation='softmax')(x)\r\n    \r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.summary()\r\n\r\n\r\n################################################\r\n# define training parameters and compile model #\r\n################################################\r\n\r\nmodel.compile(\r\n    loss=keras.losses.categorical_crossentropy,\r\n    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\r\n    metrics=['accuracy']\r\n)\r\n\r\n\r\n#########\r\n# train #\r\n#########\r\n\r\nprint(\"***** Training *****\")\r\n\r\nmodel.fit(\r\n    x_train,\r\n    y_train,\r\n    epochs=10,\r\n    verbose=2\r\n)\r\n\r\n\r\n############\r\n# Evaluate #\r\n############\r\n\r\nprint(\"***** Evaluation *****\")\r\n\r\nev = model.evaluate(x_train, y_train)\r\nprint(ev)\r\n```\r\nOutput:\r\n```\r\nEpoch 1/10\r\n2019-06-20 14:33:27.217399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-20 14:33:27.394428: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n1024/1024 - 1s - loss: 40.9096 - acc: 0.1318\r\nEpoch 2/10\r\n1024/1024 - 0s - loss: 58.4202 - acc: 0.2168\r\nEpoch 3/10\r\n1024/1024 - 0s - loss: 49.5150 - acc: 0.3896\r\nEpoch 4/10\r\n1024/1024 - 0s - loss: 51.4337 - acc: 0.3301\r\nEpoch 5/10\r\n1024/1024 - 0s - loss: 39.2524 - acc: 0.4150\r\nEpoch 6/10\r\n1024/1024 - 0s - loss: 27.9284 - acc: 0.4648\r\nEpoch 7/10\r\n1024/1024 - 0s - loss: 18.2593 - acc: 0.5752\r\nEpoch 8/10\r\n1024/1024 - 0s - loss: 12.8214 - acc: 0.5957\r\nEpoch 9/10\r\n1024/1024 - 0s - loss: 8.0606 - acc: 0.6504\r\nEpoch 10/10\r\n1024/1024 - 0s - loss: 4.9667 - acc: 0.7031\r\n***** Evaluation *****\r\n1024/1024 [==============================] - 0s 25us/sample - loss: 5.3507 - acc: 0.6465\r\n[5.350688457489014, 0.6464844]\r\n```\r\n", "@julj : Thanks for the information. I have tried multiple times on Colab with TF-GPU version 1.14.0-rc1 but was able to get slightly better accuracy and slightly less loss during evaluation than during training. Please have a look on [Colab Link](https://colab.sandbox.google.com/drive/19YAG3jq9ZMxgAex91lZrozKG8uQd0FqG#scrollTo=fily-ATErvQW).  ", "The issue is not about the values themselves but about the difference between the values at the last training step and the values at the evaluation step. In the code I provided, I expect them to be equal.", "@julj I've also reproduced on Colab and found small differences. Furthermore, if trained a bit longer the differences get smaller and the Acc graph becomes more stable. https://colab.research.google.com/drive/1lGgPndW8pXFLaL4-9Ut66EnEUBXMfsUN\r\n\r\n```\r\n***** Training *****\r\nEpoch 1/10\r\n1024/1024 - 0s - loss: 56.3011 - acc: 0.0889\r\nEpoch 2/10\r\n1024/1024 - 0s - loss: 70.6232 - acc: 0.2344\r\nEpoch 3/10\r\n1024/1024 - 0s - loss: 73.6560 - acc: 0.2500\r\nEpoch 4/10\r\n1024/1024 - 0s - loss: 56.5223 - acc: 0.3340\r\nEpoch 5/10\r\n1024/1024 - 0s - loss: 45.7556 - acc: 0.3379\r\nEpoch 6/10\r\n1024/1024 - 0s - loss: 34.4217 - acc: 0.4746\r\nEpoch 7/10\r\n1024/1024 - 0s - loss: 30.5761 - acc: 0.5049\r\nEpoch 8/10\r\n1024/1024 - 0s - loss: 21.0374 - acc: 0.5127\r\nEpoch 9/10\r\n1024/1024 - 0s - loss: 13.5673 - acc: 0.6230\r\nEpoch 10/10\r\n1024/1024 - 0s - loss: 10.9355 - acc: 0.6162\r\n***** Evaluation *****\r\n1024/1024 [==============================] - 0s 30us/sample - loss: 9.8181 - acc: 0.6064\r\n[9.818145751953125, 0.6064453]\r\n```\r\n\r\nFor 30 epochs:\r\n```\r\n***** Training *****\r\nEpoch 1/30\r\n1024/1024 - 2s - loss: 49.0334 - acc: 0.1006\r\nEpoch 2/30\r\n1024/1024 - 0s - loss: 103.3966 - acc: 0.0938\r\nEpoch 3/30\r\n1024/1024 - 0s - loss: 72.9386 - acc: 0.2168\r\nEpoch 4/30\r\n1024/1024 - 0s - loss: 48.9603 - acc: 0.3711\r\nEpoch 5/30\r\n1024/1024 - 0s - loss: 40.9398 - acc: 0.4121\r\nEpoch 6/30\r\n1024/1024 - 0s - loss: 45.3177 - acc: 0.4219\r\nEpoch 7/30\r\n1024/1024 - 0s - loss: 39.7212 - acc: 0.4502\r\nEpoch 8/30\r\n1024/1024 - 0s - loss: 28.5958 - acc: 0.5381\r\nEpoch 9/30\r\n1024/1024 - 0s - loss: 22.0282 - acc: 0.6025\r\nEpoch 10/30\r\n1024/1024 - 0s - loss: 18.7421 - acc: 0.6221\r\nEpoch 11/30\r\n1024/1024 - 0s - loss: 16.2718 - acc: 0.6201\r\nEpoch 12/30\r\n1024/1024 - 0s - loss: 15.4877 - acc: 0.6133\r\nEpoch 13/30\r\n1024/1024 - 0s - loss: 13.6785 - acc: 0.6211\r\nEpoch 14/30\r\n1024/1024 - 0s - loss: 10.8438 - acc: 0.6445\r\nEpoch 15/30\r\n1024/1024 - 0s - loss: 8.0689 - acc: 0.6689\r\nEpoch 16/30\r\n1024/1024 - 0s - loss: 5.7298 - acc: 0.7168\r\nEpoch 17/30\r\n1024/1024 - 0s - loss: 4.0341 - acc: 0.7656\r\nEpoch 18/30\r\n1024/1024 - 0s - loss: 2.8764 - acc: 0.8135\r\nEpoch 19/30\r\n1024/1024 - 0s - loss: 2.3743 - acc: 0.8311\r\nEpoch 20/30\r\n1024/1024 - 0s - loss: 2.2624 - acc: 0.8516\r\nEpoch 21/30\r\n1024/1024 - 0s - loss: 2.3173 - acc: 0.8477\r\nEpoch 22/30\r\n1024/1024 - 0s - loss: 2.3842 - acc: 0.8369\r\nEpoch 23/30\r\n1024/1024 - 0s - loss: 2.3811 - acc: 0.8311\r\nEpoch 24/30\r\n1024/1024 - 0s - loss: 2.3094 - acc: 0.8330\r\nEpoch 25/30\r\n1024/1024 - 0s - loss: 2.2365 - acc: 0.8447\r\nEpoch 26/30\r\n1024/1024 - 0s - loss: 2.1789 - acc: 0.8496\r\nEpoch 27/30\r\n1024/1024 - 0s - loss: 2.1076 - acc: 0.8516\r\nEpoch 28/30\r\n1024/1024 - 0s - loss: 2.0055 - acc: 0.8535\r\nEpoch 29/30\r\n1024/1024 - 0s - loss: 1.8593 - acc: 0.8604\r\nEpoch 30/30\r\n1024/1024 - 0s - loss: 1.6750 - acc: 0.8682\r\n***** Evaluation *****\r\n1024/1024 [==============================] - 0s 26us/sample - loss: 1.4750 - acc: 0.8828\r\n[1.474952220916748, 0.8828125]\r\n```", "Because the batch of data is the same and the metrics computed are the same, I expect the values to be **exactly** the same. I will edit my first comment accordingly.", "Sorry, I should have explained myself better. The main difference between training (fit) and test (evaluate) is that the former computes the feed forward step on the NN and then updates the weights via back-propagation while the latter only does the first part. So, as I understand, the training step modifies the model after computing the loss (and the associated metrics) so when you do evaluation the model is different than the one used to calculate training loss. As training advances and the model becomes more stable the difference tends to be smaller and that's what I tried to show on the example. Of course, two calls to evaluation with the same model and data yields the exact same results. Hope this helps!", "I am familiar with the forward/backward steps. It seems more logical to me (but maybe I am wrong) when training a model to show the metrics after the backward steps.\r\nSo in your last example I would interpret the loss and metrics values in\r\n\r\n```\r\nEpoch 30/30\r\n1024/1024 - 0s - loss: 1.6750 - acc: 0.8682\r\n```\r\n\r\nas the values obtained after the last training step, i.e. the value for the model obtained on the training data.\r\n\r\nIn that case I would not understand why the evaluation on the same data gives different values.\r\n\r\nSince they differ, you think that the values displayed are computed before the corresponding training step ? I will save the model at step 29 to check this out....\r\n\r\n", "Those values should be calculated _after_ the forward but _before_ the back, that is, before updating weights based on the gradients.If you save at step 29, the same would apply: you will have the loss at that set but the model will be updated anyway. I've just come to this [tweet](https://twitter.com/fchollet/status/1132013928681050113) by @fchollet in which he shows how to implement a custom version of the fit step: calculate predictions, calculate loss, calculate gradients, update weights. Maybe you could calculate again the loss after the update and show it. My guess is that you should get same results as in evaluate.", "I've done something like that [here](https://colab.research.google.com/drive/1rBtQ-IlhEILU4fYpj8zM1XcayxR8x9CW). From the results you can see that the initial loss is the same as the \"post-loss\" at the previous step.\r\n\r\n```\r\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\r\noptimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\r\n\r\n@tf.function\r\ndef custom_fit():\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(x_train.astype(\"float32\"))\r\n    loss_value = loss_fn(y_train, predictions)\r\n   \r\n  grads = tape.gradient(loss_value, model.trainable_weights)\r\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n  \r\n  post_predictions = model(x_train.astype(\"float32\"))\r\n  post_loss = loss_fn(y_train, post_predictions)\r\n  return (loss_value, post_loss)\r\n\r\nfor i in range(10):\r\n  lv = custom_fit()\r\n  print(lv)\r\n\r\n(<tf.Tensor: id=914, shape=(), dtype=float32, numpy=52.713608>, <tf.Tensor: id=915, shape=(), dtype=float32, numpy=72.112686>)\r\n(<tf.Tensor: id=918, shape=(), dtype=float32, numpy=72.112686>, <tf.Tensor: id=919, shape=(), dtype=float32, numpy=73.35305>)\r\n(<tf.Tensor: id=922, shape=(), dtype=float32, numpy=73.35305>, <tf.Tensor: id=923, shape=(), dtype=float32, numpy=55.93804>)\r\n(<tf.Tensor: id=926, shape=(), dtype=float32, numpy=55.93804>, <tf.Tensor: id=927, shape=(), dtype=float32, numpy=38.91731>)\r\n(<tf.Tensor: id=930, shape=(), dtype=float32, numpy=38.91731>, <tf.Tensor: id=931, shape=(), dtype=float32, numpy=27.62747>)\r\n(<tf.Tensor: id=934, shape=(), dtype=float32, numpy=27.62747>, <tf.Tensor: id=935, shape=(), dtype=float32, numpy=19.682486>)\r\n(<tf.Tensor: id=938, shape=(), dtype=float32, numpy=19.682486>, <tf.Tensor: id=939, shape=(), dtype=float32, numpy=13.982845>)\r\n(<tf.Tensor: id=942, shape=(), dtype=float32, numpy=13.982845>, <tf.Tensor: id=943, shape=(), dtype=float32, numpy=10.246971>)\r\n(<tf.Tensor: id=946, shape=(), dtype=float32, numpy=10.246971>, <tf.Tensor: id=947, shape=(), dtype=float32, numpy=8.010065>)\r\n(<tf.Tensor: id=950, shape=(), dtype=float32, numpy=8.010065>, <tf.Tensor: id=951, shape=(), dtype=float32, numpy=8.71225>)\r\n```", "You're right, the metrics monitored are computed before the weight update. I was too much expecting those values to be computed afterward...\r\nI wrote a custom monitoring callback to find out:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n\r\n\r\n#################################\r\n# Generate fake multilabel data #\r\n#################################\r\n\r\nbatch_size = 1024\r\nnum_classes = 10\r\nh, w = 28, 28\r\n\r\n(x_train, y_train), (_, _) = mnist.load_data()\r\nx_train = x_train[:1024]\r\nx_train = x_train.reshape(x_train.shape[0], h, w, 1) # default tf format is NHWC\r\ny_train = y_train[:1024]\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n    \r\n\r\n###############\r\n# build model #\r\n###############\r\n\r\ninputs = keras.Input(shape=(h, w, 1), batch_size=batch_size)\r\nx = layers.Conv2D(32, (3, 3))(inputs)\r\nx = layers.Activation('relu')(x)\r\nx = layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2))(x)\r\nx = layers.Flatten()(x)\r\nx = layers.Dense(100, activation='elu', name='logits')(x)\r\noutputs = layers.Dense(num_classes, activation='softmax')(x)\r\n    \r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.summary()\r\n\r\n\r\n################################################\r\n# define training parameters and compile model #\r\n################################################\r\n\r\nmodel.compile(\r\n    loss=keras.losses.categorical_crossentropy,\r\n    optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\r\n    metrics=['accuracy']\r\n)\r\n\r\n\r\n#####################\r\n# custom monitoring #\r\n#####################\r\n\r\n\r\nclass CustomMonitoring(Callback):\r\n\r\n    def on_train_batch_end(self, batch, logs=None):\r\n        loss, acc = self.model.evaluate(x_train, y_train, batch_size=1024)\r\n        print('For end batch {}, loss is {:7.2f} and acc is {}.'.format(batch, loss, acc))\r\n\r\n\r\n#########\r\n# train #\r\n#########\r\n\r\nprint(\"***** Training *****\")\r\n\r\nmodel.fit(\r\n    x_train,\r\n    y_train,\r\n    epochs=10,\r\n    verbose=2,\r\n    callbacks=[CustomMonitoring()]\r\n)\r\n\r\n\r\n############\r\n# Evaluate #\r\n############\r\n\r\nprint(\"***** Evaluation *****\")\r\n\r\nev = model.evaluate(x_train, y_train, batch_size=1024)\r\nprint(ev)\r\n```\r\nOutput:\r\n\r\n```\r\nEpoch 1/10\r\n1024/1024 [==============================] - 0s 24us/sample - loss: 120.1128 - acc: 0.1514\r\nFor end batch 0, loss is  120.11 and acc is 0.1513671875.\r\n1024/1024 - 1s - loss: 59.1023 - acc: 0.1035\r\nEpoch 2/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 101.6853 - acc: 0.2588\r\nFor end batch 0, loss is  101.69 and acc is 0.2587890625.\r\n1024/1024 - 0s - loss: 120.1128 - acc: 0.1514\r\nEpoch 3/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 108.9812 - acc: 0.2500\r\nFor end batch 0, loss is  108.98 and acc is 0.25.\r\n1024/1024 - 0s - loss: 101.6853 - acc: 0.2588\r\nEpoch 4/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 93.5093 - acc: 0.3691\r\nFor end batch 0, loss is   93.51 and acc is 0.369140625.\r\n1024/1024 - 0s - loss: 108.9812 - acc: 0.2500\r\nEpoch 5/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 85.9527 - acc: 0.2939\r\nFor end batch 0, loss is   85.95 and acc is 0.2939453125.\r\n1024/1024 - 0s - loss: 93.5093 - acc: 0.3691\r\nEpoch 6/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 65.5312 - acc: 0.4141\r\nFor end batch 0, loss is   65.53 and acc is 0.4140625.\r\n1024/1024 - 0s - loss: 85.9527 - acc: 0.2939\r\nEpoch 7/10\r\n1024/1024 [==============================] - 0s 3us/sample - loss: 59.6325 - acc: 0.3525\r\nFor end batch 0, loss is   59.63 and acc is 0.3525390625.\r\n1024/1024 - 0s - loss: 65.5312 - acc: 0.4141\r\nEpoch 8/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 52.5296 - acc: 0.3838\r\nFor end batch 0, loss is   52.53 and acc is 0.3837890625.\r\n1024/1024 - 0s - loss: 59.6325 - acc: 0.3525\r\nEpoch 9/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 48.6921 - acc: 0.3516\r\nFor end batch 0, loss is   48.69 and acc is 0.3515625.\r\n1024/1024 - 0s - loss: 52.5296 - acc: 0.3838\r\nEpoch 10/10\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 39.5250 - acc: 0.3945\r\nFor end batch 0, loss is   39.53 and acc is 0.39453125.\r\n1024/1024 - 0s - loss: 48.6921 - acc: 0.3516\r\n***** Evaluation *****\r\n1024/1024 [==============================] - 0s 4us/sample - loss: 39.5250 - acc: 0.3945\r\n[39.5250129699707, 0.39453125]\r\n\r\n```", "Great!", "This is super helpful. Thank you guys! I also had this confusion but solved now. ", "Thanks for filing this issue. I just had this problem as well. Reading the reason why it happens, it makes a lot of sense. In order to report the metrics after the backpropagation, tensorflow would need to do an extra forward pass, which has performance implications."]}, {"number": 29963, "title": "TypeError: moments_v2() got an unexpected keyword argument 'keep_dims'", "body": "TypeError: moments_v2() got an unexpected keyword argument 'keep_dims'", "comments": ["@zsdonghao Can you provide more details about issue and context. Thanks!", "fixed by changing to `keepdims` , the online docs of TF need to be updated!!!"]}, {"number": 29962, "title": "I'm using tensorflow 2.0 beta, and trainings stop in the middle with an error\"Could not flush events file.\" I believe this is something internal as the training stops at different points in the training, and I trained for 100 epochs once before (and I started to have this issue without any change in code).", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@tomotomohiro Please provide the information asked in template. We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n"]}, {"number": 29961, "title": "tf.contrib.layers.group_norm() Error: Failed to convert object", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux  CentOS 3.10.0-693.2.2.el7.x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\n- CUDA/cuDNN version: cuda-10.0, cudnn-7.6\r\n- GPU model and memory: 16G(Nvidia-P100)\r\n\r\nthis is the repost from:[issues](29636)", "comments": ["Looks like a known issue. Please have a look on #24409 discussion regarding the same and let us know if that resolves the problem. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29961\">No</a>\n"]}, {"number": 29960, "title": "Update README for s390x stable release", "body": "Updated README for s390x stable release", "comments": ["@gunan Could you please review?"]}, {"number": 29959, "title": "ValueError: The channel dimension of the inputs should be defined. Found `None`.", "body": "`ngf = 64\r\nnz = 100\r\nnc = 3\r\nb_size = 128\r\ndef create_generator():\r\n  noise_input = layers.Input(shape = (nz,), batch_size=b_size)#TODO: maybe change the datatype\r\n  inter = tf.reshape(noise_input, tf.convert_to_tensor([-1, 1,1, nz])) \r\n  x = layers.Conv2DTranspose(filters=ngf*8, kernel_size=(4,4), strides=(1,1), padding='valid', bias_initializer=weights_initializer, use_bias=False, )(inter)\r\n  x = layers.BatchNormalization(-1)(x)\r\n  x = layers.ReLU()(x)\r\n  \r\n  x = layers.Conv2DTranspose(filters=ngf*4, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(x)\r\n  x = layers.BatchNormalization(-1)(x)\r\n  x = layers.ReLU()(x)\r\n  \r\n  #Now unshared layers begin\r\n  #the zeroth generator\r\n  g0 = layers.Conv2DTranspose(filters=ngf*2, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(x)\r\n  g0 = layers.BatchNormalization(-1)(g0)\r\n  g0 = layers.ReLU()(g0)\r\n  \r\n  g0 = layers.Conv2DTranspose(filters=ngf, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(g0)\r\n  g0 = layers.BatchNormalization(-1)(g0)\r\n  g0 = layers.ReLU()(g0)\r\n  \r\n  g0 = layers.Conv2DTranspose(filters=nc, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False, activation ='tanh')(g0)\r\n  \r\n  #the first generator\r\n  g1 = layers.Conv2DTranspose(filters=ngf*2, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(x)\r\n  g1 = layers.BatchNormalization(-1)(g1)\r\n  g1 = layers.ReLU()(g1)\r\n  \r\n  g1 = layers.Conv2DTranspose(filters=ngf, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(g1)\r\n  g1 = layers.BatchNormalization(-1)(g1)\r\n  g1 = layers.ReLU()(g1)\r\n  \r\n  g1 = layers.Conv2DTranspose(filters=nc, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False, activation ='tanh')(g1)\r\n  \r\n  #the second generator\r\n  g2 = layers.Conv2DTranspose(filters=ngf*2, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(x)\r\n  g2 = layers.BatchNormalization(-1)(g2)\r\n  g2 = layers.ReLU()(g2)\r\n  \r\n  g2 = layers.Conv2DTranspose(filters=ngf, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False)(g2)\r\n  g2 = layers.BatchNormalization(-1)(g2)\r\n  g2 = layers.ReLU()(g2)\r\n  \r\n  g2 = layers.Conv2DTranspose(filters=nc, kernel_size=(4,4), strides=(2,2), padding='same', bias_initializer=weights_initializer, use_bias=False, activation ='tanh')(g2)\r\n  \r\n  \r\n  output = layers.concatenate([g0, g1, g2], axis = 0)\r\n  \r\n  return models.Model(inputs = [noise_input], outputs = [output])`\r\n\r\n\r\nGetting the error \r\n\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-12-1b6405bf3d1f> in <module>()\r\n----> 1 gnet = create_generator()\r\n\r\n3 frames\r\n\r\n<ipython-input-9-ee1d9a68741d> in create_generator()\r\n      6   noise_input = layers.Input(shape = (nz,), batch_size=b_size)#TODO: maybe change the datatype\r\n      7   inter = tf.reshape(noise_input, tf.convert_to_tensor([-1, 1,1, nz]))\r\n----> 8   x = layers.Conv2DTranspose(filters=ngf*8, kernel_size=(4,4), strides=(1,1), padding='valid', bias_initializer=weights_initializer, use_bias=False, )(inter)\r\n      9   x = layers.BatchNormalization(-1)(x)\r\n     10   x = layers.ReLU()(x)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    614           # Build layer if applicable (if the `build` method has been\r\n    615           # overridden).\r\n--> 616           self._maybe_build(inputs)\r\n    617 \r\n    618           # Wrapping `call` function in autograph to allow for dynamic control\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   1964         # operations.\r\n   1965         with tf_utils.maybe_init_scope(self):\r\n-> 1966           self.build(input_shapes)\r\n   1967       # We must set self.built since user defined build functions are not\r\n   1968       # constrained to set self.built.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in build(self, input_shape)\r\n    749       channel_axis = -1\r\n    750     if input_shape.dims[channel_axis].value is None:\r\n--> 751       raise ValueError('The channel dimension of the inputs '\r\n    752                        'should be defined. Found `None`.')\r\n    753     input_dim = int(input_shape[channel_axis])\r\n\r\nValueError: The channel dimension of the inputs should be defined. Found `None`.", "comments": ["The following change solved the issue for me\r\n\r\n`inter = tf.reshape(noise_input, tf.convert_to_tensor([-1, 1,1, nz]))`\r\n\r\nwas changed to \r\n\r\n`inter = layers.Reshape(target_shape=(1,1,nz))(noise_input)`", "Hi Everyone, I am trying to implement Segnet in tensorflow 2. \r\nWhen I run the code I get this error:\r\n\r\n**fcn8_tf2/Scripts/SegNet_fab.py:518 segnet  *\r\n        dec = segnet_deconv(\r\n    fcn8_tf2/Scripts/SegNet_fab.py:278 segnet_deconv  *\r\n        dec = Conv2D(\r\n    venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:817 __call__\r\n        self._maybe_build(inputs)\r\n    venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:2141 _maybe_build\r\n        self.build(input_shapes)\r\n    venv/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py:153 build\r\n        raise ValueError('The channel dimension of the inputs '\r\n\r\n    ValueError: The channel dimension of the inputs should be defined. Found `None`.**\r\n\r\nBelow the scripts.\r\n**train.py**\r\nfrom Segnet_fab import segnet\r\ninputs = Input(shape=(*params['image_size'], params['num_channels']), name='input')\r\noutputs = segnet(inputs, n_labels=2, kernel=3, pool_size=(2, 2), output_mode=None)\r\n\t\t\t # we define our U-Net to output logits\r\nmodel = Model(inputs, outputs)\r\n\r\n**Segnet_fab.py**\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras import backend as K\r\n\r\ndef MaxUnpooling2D(updates, mask):\r\n\tsize = 2\r\n\tmask = tf.cast(mask, 'int32')\r\n\tinput_shape = tf.shape(updates, out_type='int32')\r\n\r\n\t#  calculation new shape\r\n\toutput_shape = (\r\n\t\t\tinput_shape[0],\r\n\t\t\tinput_shape[1]*size,\r\n\t\t\tinput_shape[2]*size,\r\n\t\t\tinput_shape[3])\r\n\t# calculation indices for batch, height, width and feature maps\r\n\tone_like_mask = tf.ones_like(mask, dtype='int32')\r\n\tbatch_shape = tf.concat(\r\n\t\t\t[[input_shape[0]], [1], [1], [1]],\r\n\t\t\taxis=0)\r\n\tbatch_range = tf.reshape(\r\n\t\t\ttf.range(output_shape[0], dtype='int32'),\r\n\t\t\tshape=batch_shape)\r\n\tb = one_like_mask * batch_range\r\n\ty = mask // (output_shape[2] * output_shape[3])\r\n\tx = (mask // output_shape[3]) % output_shape[2]\r\n\tfeature_range = tf.range(output_shape[3], dtype='int32')\r\n\tf = one_like_mask * feature_range\r\n\tupdates_size = tf.size(updates)\r\n\tindices = K.transpose(K.reshape(\r\n\t\ttf.stack([b, y, x, f]),\r\n\t\t[4, updates_size]))\r\n\tvalues = tf.reshape(updates, [updates_size])\r\n\treturn tf.scatter_nd(indices, values, output_shape)\r\n\r\ndef segnet_conv(\r\n\t\tinputs,\r\n\t\tkernel_size=3,\r\n\t\tkernel_initializer='glorot_uniform',\r\n\t\tbatch_norm = False,\r\n\t\t**kwargs):\r\n################################# block1 #################################\r\n\r\n\tconv1 = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_1'\r\n\t\t)(inputs)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv1 = BatchNormalization(name='bn_1')(conv1)\r\n\tconv1 = LeakyReLU(alpha=0.3, name='activation_1')(conv1)\r\n\r\n\tconv1 = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_2'\r\n\t\t)(conv1)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv1 = BatchNormalization(name='bn_2')(conv1)\r\n\tconv1 = LeakyReLU(alpha=0.3, name='activation_2')(conv1)\r\n\r\n\tpool1, mask1 = tf.nn.max_pool_with_argmax(\r\n\t\t\tinput=conv1,\r\n\t\t\tksize=2,\r\n\t\t\tstrides=2,\r\n\t\t\tpadding='SAME'\r\n\t\t)\r\n\t\r\n################################# block2 #################################\r\n\tconv2 = Conv2D(\r\n\t\t\tfilters=128,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_3'\r\n\t\t)(pool1)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv2 = BatchNormalization(name='bn_3')(conv2)\r\n\tconv2 = LeakyReLU(alpha=0.3, name='activation_3')(conv2)\r\n\r\n\tconv2 = Conv2D(\r\n\t\t\tfilters=128,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_4'\r\n\t\t)(conv2)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv2 = BatchNormalization(name='bn_4')(conv2)\r\n\tconv2 = LeakyReLU(alpha=0.3, name='activation_4')(conv2)\r\n\r\n\tpool2, mask2 = tf.nn.max_pool_with_argmax(\r\n\t\t\tinput=conv2,\r\n\t\t\tksize=2,\r\n\t\t\tstrides=2,\r\n\t\t\tpadding='SAME'\r\n\t\t)\r\n\r\n################################# block3 #################################\r\n\tconv3 = Conv2D(\r\n\t\t\tfilters=256,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_5'\r\n\t\t)(pool2)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv3 = BatchNormalization(name='bn_5')(conv3)\r\n\tconv3 = LeakyReLU(alpha=0.3, name='activation_5')(conv3)\r\n\t\r\n\tconv3 = Conv2D(\r\n\t\t\tfilters=256,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_6'\r\n\t\t)(conv3)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv3 = BatchNormalization(name='bn_6')(conv3)\r\n\tconv3 = LeakyReLU(alpha=0.3, name='activation_6')(conv3)\r\n\r\n\tconv3 = Conv2D(\r\n\t\t\tfilters=256,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_7'\r\n\t\t)(conv3)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv3 = BatchNormalization(name='bn_7')(conv3)\r\n\tconv3 = LeakyReLU(alpha=0.3, name='activation_7')(conv3)\r\n\r\n\tpool3, mask3 = tf.nn.max_pool_with_argmax(\r\n\t\t\tinput=conv3,\r\n\t\t\tksize=2,\r\n\t\t\tstrides=2,\r\n\t\t\tpadding='SAME'\r\n\t\t)\r\n\r\n################################# block4 #################################\r\n\tconv4 = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_8'\r\n\t\t)(pool3)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv4 = BatchNormalization(name='bn_8')(conv4)\r\n\tconv4 = LeakyReLU(alpha=0.3, name='activation_8')(conv4)\r\n\r\n\tconv4 = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_9'\r\n\t\t)(conv4)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv4 = BatchNormalization(name='bn_9')(conv4)\r\n\tconv4 = LeakyReLU(alpha=0.3, name='activation_9')(conv4)\r\n\r\n\tconv4 = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_10'\r\n\t\t)(conv4)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv4 = BatchNormalization(name='bn_10')(conv4)\r\n\tconv4 = LeakyReLU(alpha=0.3, name='activation_10')(conv4)\r\n\r\n\tpool4, mask4 = tf.nn.max_pool_with_argmax(\r\n\t\t\tinput=conv4,\r\n\t\t\tksize=2,\r\n\t\t\tstrides=2,\r\n\t\t\tpadding='SAME'\r\n\t\t)\r\n\r\n################################# block5 #################################\r\n\tconv5 = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_11'\r\n\t\t)(pool4)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv5 = BatchNormalization(name='bn_11')(conv5)\r\n\tconv5 = LeakyReLU(alpha=0.3, name='activation_11')(conv5)\r\n\r\n\tconv5 = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_12'\r\n\t\t)(conv5)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv5 = BatchNormalization(name='bn_12')(conv5)\r\n\tconv5 = LeakyReLU(alpha=0.3, name='activation_12')(conv5)\r\n\r\n\tconv5 = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='conv_13'\r\n\t\t)(conv5)\r\n\t\r\n\tif batch_norm:\r\n\t\tconv5 = BatchNormalization(name='bn_13')(conv5)\r\n\tconv5 = LeakyReLU(alpha=0.3, name='activation_13')(conv5)\r\n\r\n\tpool5, mask5 = tf.nn.max_pool_with_argmax(\r\n\t\t\tinput=conv5,\r\n\t\t\tksize=2,\r\n\t\t\tstrides=2,\r\n\t\t\tpadding='SAME'\r\n\t\t)\r\n\r\n\treturn pool5, mask1,mask2,mask3,mask4,mask5\r\n##########################################################################\r\n############################## decod block1 ##############################\r\n##########################################################################\r\n\r\ndef segnet_deconv(\r\n\t\t\tpool5,\r\n\t\t\tmask1,\r\n\t\t\tmask2,\r\n\t\t\tmask3,\r\n\t\t\tmask4,\r\n\t\t\tmask5,\r\n\t\t\tkernel_size=3,\r\n\t\t\tkernel_initializer='glorot_uniform',\r\n\t\t\tbatch_norm = False,\r\n\t\t\t**kwargs\r\n\t\t):\r\n\r\n\tdec = MaxUnpooling2D(pool5, mask5)\r\n\r\n\tdec = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_13'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_13')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_13')(dec)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_12'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_12')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_12')(dec)\r\n\r\n\tdec = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_11'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_11')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_11')(dec)\r\n\r\n############################## decod block2 ##############################\r\n\r\n\tdec = MaxUnpooling2D(dec, mask4)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_10'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_10')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_10')(dec)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=512,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_9'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_9')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_9')(dec)\r\n\r\n\tdec = Conv2D(\r\n\t\t\tfilters=256,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_8'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_8')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_8')(dec)\r\n\r\n############################## decod block3 ##############################\r\n\r\n\tdec = MaxUnpooling2D(dec, mask3)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=256,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_7'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_7')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_7')(dec)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=256,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_6'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_6')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_6')(dec)\r\n\r\n\tdec = Conv2D(\r\n\t\t\tfilters=128,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_5'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_5')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_5')(dec)\r\n\r\n############################## decod block4 ##############################\r\n\r\n\tdec = MaxUnpooling2D(dec, mask2)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=128,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_4'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_4')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_4')(dec)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_3'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_3')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_3')(dec)\r\n\r\n############################## decod block5 ##############################\r\n\r\n\tdec = MaxUnpooling2D(dec, mask1)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_2'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_2')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_2')(dec)\r\n\t\r\n\tdec = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tpadding='same',\r\n\t\t\tactivation=None,\r\n\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\tname='upconv_3'\r\n\t\t)(dec)\r\n\r\n\tif batch_norm:\r\n\t\tdec = BatchNormalization(name='upbn_3')(dec)\r\n\tdec = LeakyReLU(alpha=0.3, name='upactivation_3')(dec)\r\n\treturn dec\r\n##########################################################################\r\n############################### classifier ###############################\r\n##########################################################################\r\ndef classifier(\r\n\t\tdec,\r\n\t\tch_out=2,\r\n\t\tkernel_size=3,\r\n\t\tfinal_activation=None,\r\n\t\tbatch_norm = False,\r\n\t\t**kwargs\r\n\t):\r\n\tdec = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tactivation='relu',\r\n\t\t\tpadding='same',\r\n\t\t\tname='dec_out1'\r\n\t\t)(dec)\r\n\r\n\tdec = Dropout(0.3, name='drop_out1')(dec)\r\n\r\n\tx = Conv2D(\r\n\t\t\tfilters=64,\r\n\t\t\tkernel_size=kernel_size,\r\n\t\t\tactivation='relu',\r\n\t\t\tpadding='same',\r\n\t\t\tname='dec_out2'\r\n\t\t)(dec)\r\n\r\n\tdec = Dropout(0.3, name='drop_out2')(dec)\r\n\tdec = Conv2D(\r\n\t\t\tfilters=ch_out,\r\n\t\t\tkernel_size=1,\r\n\t\t\tactivation=final_activation,\r\n\t\t\tpadding='same',\r\n\t\t\tname='dec_output'\r\n\t\t)(dec)\r\n\treturn dec\r\n@tf.function\r\ndef segnet(\r\n\t\tinputs,\r\n\t\tch_out=2,\r\n\t\tkernel_size=3,\r\n\t\tkernel_initializer='glorot_uniform',\r\n\t\tfinal_activation=None,\r\n\t\tbatch_norm = False,\r\n\t\t**kwargs\r\n\t\t):\r\n\r\n\tpool5, mask1, mask2, mask3, mask4, mask5 = segnet_conv(\r\n\t\t\t\t\t\t\t\tinputs,\r\n\t\t\t\t\t\t\t\tkernel_size=3,\r\n\t\t\t\t\t\t\t\tkernel_initializer='glorot_uniform',\r\n\t\t\t\t\t\t\t\tbatch_norm = False\r\n\t\t\t\t\t\t\t)\r\n\tdec = segnet_deconv(\r\n\t\t\t\tpool5,\r\n\t\t\t\tmask1,\r\n\t\t\t\tmask2,\r\n\t\t\t\tmask3,\r\n\t\t\t\tmask4,\r\n\t\t\t\tmask5,\r\n\t\t\t\tkernel_size=kernel_size,\r\n\t\t\t\tkernel_initializer=kernel_initializer,\r\n\t\t\t\tbatch_norm = batch_norm\r\n\t\t\t)\r\n\r\n\toutput = classifier(\r\n\t\t\t\tdec,\r\n\t\t\t\tch_out=2,\r\n\t\t\t\tkernel_size=3,\r\n\t\t\t\tfinal_activation=None,\r\n\t\t\t\tbatch_norm = batch_norm\r\n\t\t\t)\r\n\treturn output\r\n\r\napparently the function MaxUnpooling2D gives none output.\r\nCan you help me, Please?"]}, {"number": 29958, "title": "[TF 2.0 API Docs] `tf.keras.callbacks` (sub)classes", "body": "TensorFlow version: 2.0 (beta1)\r\n\r\n#### (Sub)classes of `tf.keras.callbacks`:\r\n`BaseLogger`, `History`, `Callback`, `CSVLogger`, `ModelCheckpoint`, `ProgbarLogger`, `RemoteMonitor`, `TensorBoard` and `TerminateOnNaN`:\r\n\r\n#### Summary:\r\nSince `tf.keras.callbacks` are important for monitoring models during training, the API docs require extra attention imo.\r\n\r\nExamples - to add - see below\r\nDescriptions - to be defined better - see below (especially the `Callback` custom class)\r\nReturns/raises - to add for better UX when needed\r\n\r\n#### Suggested improvements: \r\n\r\n- _Missing examples_: one example per (sub)class would be enough for good UX and a link to a tutorial presents an extra step for a user. E.g. a short example inside the docs similar to the ones in `EarlyStopping` ([link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/EarlyStopping)), `ReduceLROnPlateau` ([link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)) or `LambdaCallback` ([link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/LambdaCallback))\r\n\r\nNote: `ModelCheckpoint` and `TensorBoard` have links to tutorials which have examples. `BaseLogger` and `History` are applied by default but that may not help understand them better.\r\n\r\nExample of a `Custom` callback from @fchollet Deep Learning with Python book:\r\n```python\r\nclass ActivationLogger(keras.callbacks.Callback):\r\n    def set_model(self, model):\r\n        self.model = model # Called by the parent model before training, to inform the callback of what model will be calling it\r\n        layer_outputs = [layer.output for layer in model.layers]\r\n        self.activations_model = keras.models.Model(model.input, layer_outputs) # Model instance that returns the activations of every layer\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        if self.validation_data is None:\r\n                raise RuntimeError('Requires validation_data.')\r\n        validation_sample = self.validation_data[0][0:1] # Obtains the first input sample of the validation data\r\n        activations = self.activations_model.predict(validation_sample)\r\n        f = open('activations_at_epoch_' + str(epoch) + '.npz', 'w') # Saves arrays to disk\r\n        np.savez(f, activations)\r\n        f.close()\r\n```\r\n\r\nExample of a `TensorBoard` callback from @fchollet Deep Learning with Python book:\r\n```python\r\ncallbacks = [\r\n    keras.callbacks.TensorBoard(\r\n        log_dir='my_log_dir', # Location of log files\r\n        histogram_freq=1, # Records activation histogram every 1 epoch\r\n        embeddings_freq=1, # Records embedding data every 1 epoch\r\n    )\r\n]\r\n\r\nhistory = model.fit(x_train, y_train, \r\n                epochs=20, \r\n                batch_size=128, \r\n                validation_split=0.2, \r\n                callbacks=callbacks)\r\n\r\n# Browse to http://localhost:6006 and look at your model training\r\n...\r\n```\r\n\r\n- _Descriptions_: to be defined better if needed. Recommend to use the following Medium post  which has quite decent descriptions of each `callback`: https://medium.com/singlestone/keras-callbacks-monitor-and-improve-your-deep-learning-205a8a27e91c\r\n\r\nNote: Mention in the `EarlyStopping` and `ModelCheckpoint` descriptions that they are/should be both typically used together (see `callbacks_list` example from @fchollet with 2 callbacks passed into `model.fit` below) to stop training when improvement stops and save the current best model during training (`save_best_only=True`):\r\n\r\n```python\r\n# A list of 2 or more callbacks that can be passed into `model.fit`\r\ncallbacks_list = [\r\n        keras.callbacks.EarlyStopping(\r\n                monitor='acc',\r\n                patience=1,\r\n        ),\r\n        \r\n        keras.callbacks.ModelCheckpoint( # Saves the current weights after every epoch\r\n                filepath='my_model.h5',\r\n                monitor='val_loss',\r\n                save_best_only=True, # These two arguments mean you won\u2019t overwrite the model file unless val_loss has improved\r\n        )\r\n]\r\n\r\nmodel.compile(optimizer='rmsprop',\r\n                loss='binary_crossentropy',\r\n                metrics=['acc'])\r\n\r\nmodel.fit(x, y,\r\n                epochs=10,\r\n                batch_size=32,\r\n                callbacks=callbacks_list,\r\n                validation_data=(x_val, y_val)\r\n                )\r\n```\r\n\r\n- _Returns/Raises_: to be defined/defined better if needed\r\n\r\n#### Doc links:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/BaseLogger\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/History\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/Callback\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/CSVLogger\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ModelCheckpoint\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/ProgbarLogger\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/RemoteMonitor\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TensorBoard\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/TerminateOnNaN", "comments": ["@8bitmp3, is there any possible way to dump model gradients and intermediate layer output through callback?", "how to do classificaiton using Fashion MNIST, a data set containing items of clothing. There's another, similar dataset called MNIST which has items of handwriting -- the digits 0 through 9.\r\n\r\nWrite an MNIST classifier that trains to 99% accuracy or above, and does it without a fixed number of epochs -- i.e. you should stop training once you reach that level of accuracy.\r\n\r\nSome notes:\r\n\r\nIt should succeed in less than 10 epochs, so it is okay to change epochs= to 10, but nothing larger\r\nWhen it reaches 99% or greater it should print out the string \"Reached 99% accuracy so cancelling training!\"\r\nIf you add any additional variables, make sure you use the same names as the ones used in the class\r\n\r\n\r\ncan you solve this", "import tensorflow as tf\r\nfrom os import path, getcwd, chdir\r\n\r\n# DO NOT CHANGE THE LINE BELOW. If you are developing in a local\r\n# environment, then grab mnist.npz from the Coursera Jupyter Notebook\r\n# and place it inside a local folder and edit the path to that location\r\npath = f\"{getcwd()}/../tmp2/mnist.npz\"\r\n\r\n\r\n# GRADED FUNCTION: train_mnist\r\nclass myCallback(tf.keras.callbacks.Callback):\r\n    def train_mnist(self, epoch, logs={}):\r\n        # Please write your code only where you are indicated.\r\n        # please do not remove # model fitting inline comments.\r\n        if(logs.get('acc')>0.99):\r\n              print(\"\\nReached 99% accuracy so cancelling training!\")\r\n        self.model.stop_training = True\r\n    mnist = tf.keras.datasets.mnist\r\n    (x_train, y_train),(x_test, y_test) = mnist.load_data(path=path)\r\n    # YOUR CODE SHOULD START HERE\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    # YOUR CODE SHOULD END HERE\r\n    model = tf.keras.models.Sequential([\r\n        # YOUR CODE SHOULD START HERE\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n        # YOUR CODE SHOULD END HERE\r\n    ])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n    \r\n    # model fitting\r\nhistory = model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])\r\n    # model fitting\r\nreturn history.epoch, history.history['acc'][-1]", "I am using tensorflow==2.1 and when I ran this algorithm I got error saying this:\r\n![1](https://user-images.githubusercontent.com/44919399/73130987-fca89e80-4028-11ea-85d7-b7e25c34e621.jpg)\r\nBriefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '>' this cannot compare.\r\nCan someone help me?", "> I am using tensorflow==2.1 and when I ran this algorithm I got error saying this:\r\n> ![1](https://user-images.githubusercontent.com/44919399/73130987-fca89e80-4028-11ea-85d7-b7e25c34e621.jpg)\r\n> Briefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '>' this cannot compare.\r\n> Can someone help me?\r\n\r\nSame Issue here, any solution found?", "For tensorflow==2.1 it is not log.get('acc') instead try log.get('accuracy'). It should work.", "> I am using tensorflow==2.1 and when I ran this algorithm I got error saying this:\r\n> ![1](https://user-images.githubusercontent.com/44919399/73130987-fca89e80-4028-11ea-85d7-b7e25c34e621.jpg)\r\n> Briefly it says that logs.get('acc') is Nonetype and 0.99 is float so, '>' this cannot compare.\r\n> Can someone help me?\r\n\r\nHi..\r\nUse if(logs.get('acc')>0.90):\r\n\r\nThis worked for me", "Hello..\r\nFor me it didn't, then I used 'accuracy' instead of 'acc', this worked for me.", "For tensorflow version 1.14.0, try log.get('acc')\r\nAnd as per the comments, if tf version is more than 2, then try log.get('accuracy')"]}, {"number": 29957, "title": "Filling up shuffle buffer (this may take a while): 9544 of 10000", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Windows 10 x64\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6\r\n- CUDA/cuDNN version:10.0/7.4\r\n- GPU model and memory: Nvidia Geforce 840m / 4.00 Go\r\n\r\n\r\nI used CNN facial landmarks for training my own dataset , when I start training I get some lines :\r\n> I tensorflow/core/kernels/data/shuffle_dataset_op.cc:101] Filling up shuffle buffer (this may take a while): 7024 of 10000\r\n2019-06-19 12:02:24.102469: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:101] Filling up shuffle buffer (this may take a while): 9544 of 10000\r\n\r\nI want to know, are those lines meaning errors?\r\nThanks.", "comments": ["Can you please confirm whether you are getting expected results.Also, please share with us full code snippet to reproduce the issue reported here. Thanks!", "I used CNN facial landmark project :\r\n+ https://github.com/yinguobing/cnn-facial-landmark\r\nI have modified the code : \r\n\r\n+ [landmark.zip](https://github.com/tensorflow/tensorflow/files/3314778/landmark.zip)\r\n", "I tried to reproduce the issue. It looks some datasets are missing to reproduce the issue.Request you to provide the datasets. Thanks.", "Did you mean upload datasets or just you want that I should print their names?", "Please upload the datasets for reproducing the issue. Thanks!", "Actually, I used many different datasets and their size is big (including whole images).\r\nBut I will give you examples of image:\r\n![001_03](https://user-images.githubusercontent.com/19480228/60178668-60ad7f00-981c-11e9-9941-b83052b0b369.jpg)\r\n![image_0010](https://user-images.githubusercontent.com/19480228/60178714-6d31d780-981c-11e9-8ff9-5112e2a7edf9.jpg)\r\n\r\n![008f](https://user-images.githubusercontent.com/19480228/60178868-b1bd7300-981c-11e9-9b9d-fbd151a9649f.jpg)\r\n", "I tried reproducing the issue with the above image placed in testiriss.record folder but i am getting the below error.Thanks! \r\n\r\nFailedPreconditionError (see above for traceback): testiriss.record; Is a directory [[node IteratorGetNext (defined at <ipython-input-4-ecad13bca4e7>:249) ]]", "What command you execute?", "I have tried executing the code(landmark.py) given by you by placing the image in testiriss.record folder.Thanks!", "You should following some instructions before executing landmark.py ", "Apologies for the delay in response. Those are log info and not errors. See https://github.com/tensorflow/tensorflow/blob/42b5da6659a75bfac77fa81e7242ddb5be1a576a/tensorflow/core/kernels/data/shuffle_dataset_op.cc#L138\r\n", "@ymodak, Thanks for your answer ", "@ymodak thanks - but what this log means?"]}, {"number": 29956, "title": "TensorflowJS cannot be installed properly", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- MAC OS:\r\n- TensorFlowJS version (1.2.1):\r\n- Python version 3.6:\r\n\r\nAfter installed `pip install tensorflowjs==1.2.1`\r\nI use `tensorflowjs_converter --help` it gives the errors:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0619 17:05:26.072402 140736116396928 __init__.py:309] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nW0619 17:05:26.101637 140736116396928 __init__.py:336] Limited tf.summary API due to missing TensorBoard installation.\r\nTraceback (most recent call last):\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/tf_v1.py\", line 30, in <module>\r\n    from tensorflow_estimator import estimator\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v2 import estimator\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/_api/v2/estimator/__init__.py\", line 31, in <module>\r\n    from tensorflow_estimator.python.estimator.estimator_lib import BinaryClassHead\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 67, in <module>\r\n    from tensorflow_estimator.python.estimator.tpu.tpu_estimator import TPUEstimator\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 86, in <module>\r\n    from tensorflow_estimator.python.estimator.tpu import _tpu_estimator_embedding\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/_tpu_estimator_embedding.py\", line 32, in <module>\r\n    from tensorflow.python.tpu import feature_column_v2 as tpu_fc_v2\r\nImportError: cannot import name 'feature_column_v2'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"~/anaconda3/bin/tensorflowjs_converter\", line 6, in <module>\r\n    from tensorflowjs.converters.converter import main\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/__init__.py\", line 21, in <module>\r\n    from tensorflowjs import converters\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/__init__.py\", line 24, in <module>\r\n    from tensorflowjs.converters.tf_saved_model_conversion_v2 import convert_tf_saved_model\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 38, in <module>\r\n    import tensorflow_hub as hub\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/__init__.py\", line 30, in <module>\r\n    from tensorflow_hub.estimator import LatestModuleExporter\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/estimator.py\", line 25, in <module>\r\n    from tensorflow_hub import tf_utils\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/tf_utils.py\", line 28, in <module>\r\n    from tensorflow_hub import tf_v1\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/tf_v1.py\", line 32, in <module>\r\n    from tensorflow import add_to_collection\r\nImportError: cannot import name 'add_to_collection'\r\n```", "comments": ["I am so frustrated that I wasted too much time on this buggy stuff.", "After following operations :\r\n```\r\n$ pip uninstall tf-nightly-2.0-preview\r\n$ pip install tensorflow==1.13.1\r\n```\r\nif gives the errors:\r\n```Traceback (most recent call last):\r\n  File \"~/anaconda3/bin/tensorflowjs_converter\", line 6, in <module>\r\n    from tensorflowjs.converters.converter import main\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/__init__.py\", line 21, in <module>\r\n    from tensorflowjs import converters\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/__init__.py\", line 24, in <module>\r\n    from tensorflowjs.converters.tf_saved_model_conversion_v2 import convert_tf_saved_model\r\n  File \"~/anaconda3/lib/python3.6/site-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 30, in <module>\r\n    from tensorflow.python.framework import convert_to_constants\r\nImportError: cannot import name 'convert_to_constants'\r\n```", "I think installed `tensorflow` version not match the `tensorflowjs` ", "I had the same issue (with tf2) - I deleted line 67 from `tensorflow_estimator/python/estimator/estimator_lib.py` and the conversion worked.", "After comparing the two branches `r1.13` and `r1.14`\r\nI upgrade `tensorflow==1.13.1` to `tensorflow==1.14.0`\r\nEverything works fine~~", "> I had the same issue (with tf2) - I deleted line 67 from `tensorflow_estimator/python/estimator/estimator_lib.py` and the conversion worked.\r\n\r\nTHANKS  FOR YOUR REPLY~ YOU SAVE ME~", "> I had the same issue (with tf2) - I deleted line 67 from `tensorflow_estimator/python/estimator/estimator_lib.py` and the conversion worked.\r\n\r\nWorks for me by commenting on line 69\r\n\r\n```\r\n#from tensorflow_estimator.python.estimator.tpu.tpu_estimator import TPUEstimator\r\n```", "This seems to be related to mixing `tensorflow` and `tf-nightly` in the same virtualenv... I started getting all kinds of bizarre errors after installing/uninstalling both packages, even though I never had them both installed at the same time. Could only fix it by wiping the virtualenv and reinstalling deps from scratch."]}, {"number": 29955, "title": "Create a low-level python tutorial for tf 2", "body": "## Description of issue (what needs changing):\r\n\r\nIt is unclear how to translate the following code to tf 2:\r\n\r\n```python\r\n%matplotlib inline\r\nimport tensorflow as tf\r\nfrom matplotlib import pyplot as plt\r\nfrom tqdm.auto import tqdm, trange\r\nimport numpy as np\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ny = np.array([[10., 1.], [6., 3.], [-1, 7]])\r\nx = np.array([[4., 15.], [1., 1.], [-7, 1]])\r\ntrajs = [[] for j in range(y.shape[0])]\r\n\r\ng = tf.compat.v1.Graph()\r\nwith g.as_default():\r\n\tsess = tf.compat.v1.Session(graph=g)\r\n\t\r\n\tyT = tf.compat.v1.placeholder(tf.float64)\r\n\txT = tf.compat.v1.Variable(x, trainable=True)\r\n\r\n\telementWiseLoss = tf.compat.v1.reduce_sum((yT - xT)**2, axis=1)\r\n\tloss = tf.compat.v1.reduce_sum(elementWiseLoss)\r\n\topt = tf.compat.v1.train.AdamOptimizer(0.1).minimize(loss)\r\n\tinit = tf.compat.v1.global_variables_initializer()\r\n\t\r\n\tsess.run(init)\r\n\tepochs = 300\r\n\tfor i in trange(epochs):\r\n\t\t_, lossR, x, elR = sess.run([opt, loss, xT, elementWiseLoss], feed_dict={yT: y})\r\n\t\tif not i % 10:\r\n\t\t\tfor j in range(y.shape[0]):\r\n\t\t\t\ttrajs[j].append([*x[j], elR[j]])\r\n\r\nprint(\"x\", x)\r\nprint(\"y\", y)\r\nprint(\"y - x\", y - x)\r\ntrajs = np.array(trajs)\r\nfor i, m in enumerate([\"o\", \"+\", \"*\"]):\r\n\tplt.scatter(trajs[i, :, 0], trajs[i, :, 1], marker=m, c=trajs[i, :, 2], cmap=\"rainbow\", label=str(i))\r\nplt.legend()\r\nplt.grid()\r\nplt.colorbar()\r\nplt.show()\r\n```\r\n\r\nThe solution:\r\n```python\r\n%matplotlib inline\r\nimport tensorflow as tf\r\nfrom matplotlib import pyplot as plt\r\nfrom tqdm.auto import tqdm, trange\r\nimport numpy as np\r\n\r\ny = np.array([[10., 1.], [6., 3.], [-1, 7]])\r\nx = np.array([[4., 15.], [1., 1.], [-7, 1]])\r\ntrajs = [[] for j in range(y.shape[0])]\r\n\r\n\r\nyT = tf.Variable(y)\r\nxT = tf.Variable(x)\r\n\r\n@tf.function\r\ndef elementWiseLoss():\r\n\treturn tf.reduce_sum((yT - xT)**2, axis=1)\r\n\r\n@tf.function\r\ndef loss():\r\n\treturn tf.reduce_sum(elementWiseLoss())\r\n\r\noptr = tf.optimizers.Adam(0.1)\r\n\r\nepochs = 300\r\nfor i in trange(epochs):\r\n\toptr.minimize(loss, [xT])\r\n\tlossR = loss().numpy()\r\n\tx = xT.numpy()\r\n\telR = elementWiseLoss().numpy()\r\n\tif not i % 10:\r\n\t\tfor j in range(yT.shape[0]):\r\n\t\t\ttrajs[j].append([*xT[j], elR[j]])\r\n\r\nprint(\"x\", x)\r\nprint(\"y\", y)\r\nprint(\"y - x\", y - x)\r\ntrajs = np.array(trajs)\r\nfor i, m in enumerate([\"o\", \"+\", \"*\"]):\r\n\tplt.scatter(trajs[i, :, 0], trajs[i, :, 1], marker=m, c=trajs[i, :, 2], cmap=\"rainbow\", label=str(i))\r\nplt.legend()\r\nplt.grid()\r\nplt.colorbar()\r\nplt.show()\r\n```\r\n\r\n### Clear description\r\n\r\nIt would be nice to have a tutorial page containing both v1-style solution and v2-style one.\r\n\r\n", "comments": ["@KOLANICH The link [Upgrade code to TensorFlow 2.0](https://www.tensorflow.org/beta/guide/upgrade) guide you to upgrade from 1.x to 2.0. The tf_upgrade_v2 script will take care upgrading the 1.x code to 2.0. Let us know if that helps. Thanks!", "tf_upgrade_v2 \"upgrades\" to legacy API.", "Several guides for upgrading code to TensorFlow 2.0 have been released:\r\n\r\n* [Effective TensorFlow 2.0](https://www.tensorflow.org/guide/effective_tf2)\r\n* [Migrate your TensorFlow 1 code to TensorFlow 2](https://www.tensorflow.org/guide/migrate)\r\n* [Better performance with tf.function and AutoGraph](https://www.tensorflow.org/guide/function)\r\n\r\nOne of our Google Summer of Code students also created [several examples](https://github.com/Vishal-V/GSoC) for migrating models. Closing out this issue for now; please feel free to reopen if the resources I mentioned are insufficient! \ud83d\ude42  "]}, {"number": 29954, "title": "TFLite Macro README instructions don't build. ", "body": "The README.md file at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/README.md\r\n\r\nindicates that a make call with test keyword will run all tests:\r\n```\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\n```\r\nBut this results in a syntax error:\r\n\r\n```\r\n~/tensorflow$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\n/bin/sh: 1: [[: not found\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\" \"02c64880acb89dbd57eebacfd67200d8\" tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers \r\ndownloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh: line 84: curl: command not found\r\ntensorflow/lite/experimental/micro/tools/make/Makefile:198: recipe for target 'tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers] Error 127\r\n```\r\n\r\nIn both my systems (Ubuntu 18.04 on WSL and Virtualmachine) make is defaulted to /bin/sh and not /bin/bash therefore `[[` is not interpreted. \r\n\r\nIt is not an issue, but still, I thought the makefile would just work (TM). Many others might also, since it is indicated in README.\r\nI the same line is available in the upcoming TinyML book, that is how I ran it.  \r\n@petewarden \r\n\r\n![ezgif com-optimize (4)](https://user-images.githubusercontent.com/551129/59755686-a8fef700-9288-11e9-85aa-527d5d6d78e1.gif)\r\n", "comments": ["Apart from [[ which sounds strange to me. [[ is a way in bash to express control flows. \r\n\r\nThere is another error on curl:\r\n\r\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh: line 84: curl: command not found\r\n\r\n\r\nCould you install curl and try again?", "Closing, since there's been no update for a while, and I believe installing curl should fix this."]}, {"number": 29953, "title": "Which license must be provided with my application when using the tensorflow C API?", "body": "I am developing a commercial application that uses the tensorflow C API. At all kinds of places, for example, https://github.com/tensorflow/tensorflow/blob/master/LICENSE, I found that tensorflow uses the Apache License 2.0. However, when I downloaded the C API from https://www.tensorflow.org/install/lang_c (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.13.1.zip) it came with a huge LICENSE file that lists many 3rd party libraries.\r\n\r\nWhich license do I need to distribute with my application? Do I need to add the Apache License 2.0 for tensorflow or do I need to add the content of the LICENSE file that came with the C API?", "comments": ["Has there been any progress on this issue? I still do not know which license I need to distribute with an application that uses the tensorflow C API.", "Hi @dagophil, \r\n\r\nOur license is in [LICENSE](https://github.com/tensorflow/tensorflow/blob/master/LICENSE).\r\n\r\nBeyond that, we can't provide any legal advice.  You may want to consult your own counsel if you have questions.\r\n\r\nGood luck.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29953\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29953\">No</a>\n"]}, {"number": 29952, "title": "Error when using tf.keras.backend.function() in 1.14.0-rc0/1.14.0", "body": "```\r\ndef input_derivative(model, x, goal):\r\n  \"\"\" Calculate derivatives wrt the intputs \"\"\"\r\n  y_true = Input(shape=(10,))\r\n  ce = categorical_crossentropy(y_true, model.output)\r\n  grad_ce = tf.keras.backend.gradients(ce, model.inputs)\r\n  func = tf.keras.backend.function(model.inputs + [y_true], grad_ce)\r\n  output = func([x, goal])\r\n  return output\r\n```\r\n\r\nHere is my code try to calculate gradients.\r\nIt is work in tensorflow1.13.1\r\nbut get error message at tensorflow1.14.0-rc0 like the following\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Expected size[0] in [0, 0], but got 1\r\n\t [[{{node softmax_cross_entropy_with_logits/Slice_1}}]]\r\n\t [[gradients/conv2d/Conv2D_grad/Conv2DBackpropInput/_101]]\r\n  (1) Invalid argument: Expected size[0] in [0, 0], but got 1\r\n\t [[{{node softmax_cross_entropy_with_logits/Slice_1}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "comments": ["As 1.14.0 is now released, please try running your code with it and let me know if you're still seeing issues.", "> As 1.14.0 is now released, please try running your code with it and let me know if you're still seeing issues.\r\n\r\nI just tried 1.14.0 and got the same issues.", "Please provide details about what platform you are using (operating system, architecture).  Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact code snippet to reproduce the issue included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I am able to reproduce the issue on colab with tensorflow 1.14.0. Thanks!", "> Please provide details about what platform you are using (operating system, architecture). Also, did you compile from source or install a binary?\r\n> \r\n> Make sure you also include the exact code snippet to reproduce the issue included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\ni tried on https://colab.research.google.com and ubuntu. Installed by pip install", "@bmiwcy, Is this still issue? Would you like try latest Tf 1.15.0 version. Let us know how it progresses. Thanks!", "@bmiwcy, Is this still an issue!", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29952\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29952\">No</a>\n"]}, {"number": 29951, "title": "Segmentation fault when using cpp custom op in tf.data.Dataset.map in tensorflow2.0", "body": "It seems if I have cpp custom op in a python function and I pass the python function to tf.data.Dataset.map it will crush.\r\nIf I only call this python function outside, It will be ok.\r\nI've spend a whole afternoon to find the bug. I'm really mad about this bug.\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\nTensorFlow installed from (source or binary):binary\r\nTensorFlow version (use command below): 2.0b1\r\nPython version:3.6\r\nCUDA/cuDNN version:10/7.4\r\nGPU model and memory:7.5/24gb\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport pdb\r\nextr_module = tf.load_op_library('./build/libextr_module.so')\r\nres = extr_module.test_bug() # ok\r\n\r\ndef aaa(filename):\r\n    res = extr_module.test_bug() # Segmentation fault (core dumped)\r\n\r\n    return tf.zeros([1], tf.float32)\r\n    \r\ndataset = tf.data.TextLineDataset(['aaa']).map(aaa)\r\n\r\n```\r\n```\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/framework/tensor_shape.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/util/work_sharder.h\"\r\n\r\n#include <iostream>\r\n#include <cmath>\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"TestBug\")\r\n    .Output(\"dummy: float\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        c->set_output(0, c->MakeShape({1}));\r\n        return Status::OK();\r\n    });\r\n\r\nclass TestBugOp : public OpKernel\r\n{\r\npublic:\r\nexplicit TestBugOp(OpKernelConstruction* context)\r\n        : OpKernel(context)\r\n{\r\n\r\n}\r\n\r\nvoid Compute(OpKernelContext* context) override\r\n{\r\n    Tensor* dummy = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, {1},\r\n                                                     &dummy));\r\n}\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(\r\n  Name(\"TestBug\").Device(DEVICE_CPU),\r\n  TestBugOp\r\n);\r\n```\r\n```\r\n\r\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)\r\nPROJECT(extr_module)\r\n\r\n\r\n# compiler flags\r\nSET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -O2 ${OpenMP_CXX_FLAGS} -Wall -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -DGOOGLE_CUDA=1\")\r\n\r\n# TensorFlow dependencies\r\nEXECUTE_PROCESS(COMMAND python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_include(), end='', flush=True)\"  OUTPUT_VARIABLE TF_INC)\r\n\r\nEXECUTE_PROCESS(COMMAND python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_lib(), end='', flush=True)\"  OUTPUT_VARIABLE TF_LIB)\r\n\r\n\r\nMESSAGE(STATUS \"Found TF_INC: \" ${TF_INC})\r\n#MESSAGE(STATUS \"Found TF_INC_EXTERNAL: \" ${TF_INC}/external/nsync/public)\r\nMESSAGE(STATUS \"Found TF_LIB: \" ${TF_LIB})\r\n\r\n\r\nINCLUDE_DIRECTORIES(${TF_INC})\r\n#INCLUDE_DIRECTORIES(${TF_INC}/external/nsync/public)\r\nLINK_DIRECTORIES(${TF_LIB})\r\n\r\n\r\nADD_LIBRARY(extr_module SHARED\r\n  testbug.cc\r\n)\r\n\r\nTARGET_LINK_LIBRARIES(extr_module tensorflow_framework)\r\n```", "comments": ["any feedback?", "Can you give us a stack trace? Hard to do anything without.\r\n\r\nFor safety, you must build your extension against the same version of all dependencies as TF. I *think* in your case the only thing that matters is TensorFlow itself, and if you're building against 2.0b1, you should be fine, just mentioning it as a thing to check.", "My tensorflow version:\r\n```\r\n>>> tf.version.VERSION\r\n'2.0.0-beta1'\r\n>>> tf.version.GIT_VERSION\r\n'v2.0.0-beta0-16-g1d91213fe7'\r\n```\r\nI don't have debug version of tensorflow, so I just attach gdb to my python, here's the result:\r\n```\r\nThread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n0x00007ff2905d4809 in tensorflow::shape_inference::InferenceContext::MakeShape(std::initializer_list<tensorflow::shape_inference::DimensionOrConstant>) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n```\r\nSo it seems stopped at SetShapeFn. If I don't call it in map member function, it can totally run without crush.", "Here's bt result:\r\n\r\n```\r\n#0  0x00007ff255dbe809 in tensorflow::shape_inference::InferenceContext::MakeShape(std::initializer_list<tensorflow::shape_inference::DimensionOrConstant>) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#1  0x00007ff24c6c0a3e in {lambda(tensorflow::shape_inference::InferenceContext*)#1}::_FUN(tensorflow::shape_inference::InferenceContext*) ()\r\n   from ./build/libextr_module.so\r\n#2  0x00007ff24c6c0ad0 in std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&) ()\r\n   from ./build/libextr_module.so\r\n#3  0x00007ff255dbc9bd in tensorflow::shape_inference::InferenceContext::Run(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)> const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#4  0x00007ff25f08b980 in tensorflow::ShapeRefiner::RunShapeFn(tensorflow::Node const*, tensorflow::OpRegistrationData const*, tensorflow::ExtendedInferenceContext*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007ff25f08d4a8 in tensorflow::ShapeRefiner::AddNode(tensorflow::Node const*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007ff25bea7d2a in TF_FinishOperation () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007ff25972fde6 in _wrap_TF_FinishOperation () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00000000004f858d in ?? ()\r\n#9  0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#10 0x00000000004f7a28 in ?? ()\r\n#11 0x00000000004f876d in ?? ()\r\n#12 0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#13 0x00000000004f6128 in ?? ()\r\n#14 0x00000000004f426e in _PyFunction_FastCallDict ()\r\n#15 0x00000000005a1481 in ?? ()\r\n#16 0x0000000000512a60 in ?? ()\r\n#17 0x000000000053ee21 in ?? ()\r\n#18 0x000000000057ec0c in _PyObject_FastCallKeywords ()\r\n#19 0x00000000004f88ba in ?? ()\r\n#20 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n#21 0x00000000004f6128 in ?? ()\r\n#22 0x000000000056ff4c in ?? ()\r\n#23 0x000000000057c2fe in PyObject_Call ()\r\n#24 0x00000000004facb1 in _PyEval_EvalFrameDefault ()\r\n#25 0x00000000004f6128 in ?? ()\r\n#26 0x00000000004f7d60 in ?? ()\r\n#27 0x00000000004f876d in ?? ()\r\n#28 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n#29 0x00000000004f6128 in ?? ()\r\n#30 0x00000000004f7d60 in ?? ()\r\n#31 0x00000000004f876d in ?? ()\r\n#32 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n#33 0x00000000004f6128 in ?? ()\r\n#34 0x00000000004f7d60 in ?? ()\r\n#35 0x00000000004f876d in ?? ()\r\n#36 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#37 0x00000000004f4944 in ?? ()\r\n#38 0x00000000004f876d in ?? ()\r\n#39 0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#40 0x00000000004f6128 in ?? ()\r\n#41 0x000000000056fd6b in ?? ()\r\n#42 0x000000000057c2fe in PyObject_Call ()\r\n#43 0x00000000004facb1 in _PyEval_EvalFrameDefault ()\r\n#44 0x00000000004f6128 in ?? ()\r\n#45 0x000000000056fd6b in ?? ()\r\n#46 0x000000000057c2fe in PyObject_Call ()\r\n#47 0x00000000004facb1 in _PyEval_EvalFrameDefault ()\r\n#48 0x00000000004f6128 in ?? ()\r\n#49 0x000000000056fe24 in ?? ()\r\n#50 0x000000000057c2fe in PyObject_Call ()\r\n#51 0x00000000004facb1 in _PyEval_EvalFrameDefault ()\r\n#52 0x00000000004f6128 in ?? ()\r\n#53 0x00000000004f7d60 in ?? ()\r\n#54 0x00000000004f876d in ?? ()\r\n#55 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n#56 0x00000000004f6128 in ?? ()\r\n#57 0x00000000004f7d60 in ?? ()\r\n#58 0x00000000004f876d in ?? ()\r\n#59 0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#60 0x00000000004f7a28 in ?? ()\r\n#61 0x00000000004f876d in ?? ()\r\n#62 0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#63 0x00000000004f6128 in ?? ()\r\n#64 0x00000000004f426e in _PyFunction_FastCallDict ()\r\n#65 0x00000000005a1481 in ?? ()\r\n#66 0x000000000057c2fe in PyObject_Call ()\r\n#67 0x00000000004facb1 in _PyEval_EvalFrameDefault ()\r\n#68 0x00000000004f6128 in ?? ()\r\n#69 0x00000000004f7d60 in ?? ()\r\n#70 0x00000000004f876d in ?? ()\r\n#71 0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#72 0x00000000004f6128 in ?? ()\r\n#73 0x00000000004f426e in _PyFunction_FastCallDict ()\r\n#74 0x00000000005a1481 in ?? ()\r\n#75 0x0000000000512a60 in ?? ()\r\n#76 0x000000000053ee21 in ?? ()\r\n#77 0x000000000057ec0c in _PyObject_FastCallKeywords ()\r\n#78 0x00000000004f88ba in ?? ()\r\n#79 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n#80 0x00000000004f6128 in ?? ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#81 0x00000000004f426e in _PyFunction_FastCallDict ()\r\n#82 0x00000000005a1481 in ?? ()\r\n#83 0x0000000000512a60 in ?? ()\r\n#84 0x000000000053ee21 in ?? ()\r\n#85 0x000000000057ec0c in _PyObject_FastCallKeywords ()\r\n#86 0x00000000004f88ba in ?? ()\r\n#87 0x00000000004fa6c0 in _PyEval_EvalFrameDefault ()\r\n#88 0x00000000004f6128 in ?? ()\r\n#89 0x00000000004f7d60 in ?? ()\r\n#90 0x00000000004f876d in ?? ()\r\n#91 0x00000000004f98c7 in _PyEval_EvalFrameDefault ()\r\n#92 0x00000000004f6128 in ?? ()\r\n#93 0x00000000004f9023 in PyEval_EvalCode ()\r\n#94 0x00000000006415b2 in ?? ()\r\n#95 0x000000000064166a in PyRun_FileExFlags ()\r\n#96 0x0000000000643730 in PyRun_SimpleFileExFlags ()\r\n#97 0x000000000062b26e in Py_Main ()\r\n#98 0x00000000004b4cb0 in main ()\r\n\r\n```", "It crashes in the shape function, in the MakeShape call. Maybe for some reason the InferenceContext isn't valid? \r\n\r\n@jsimsa is there anything special in how this is used by .map? ", "@martinwicke Same problem in `docker image` of tf 1.14. \r\n\r\nAll custom ops can work with tf 1.13.1 docker image, but fail in 1.14. \r\n\r\nI have test g++ 5, 7, 8, all failed.\r\n", "@martinwicke Same problem. This fatal is seriousness. Please fix it emergency.", "#13308", "@zh794390558 It may duplicate to https://github.com/tensorflow/tensorflow/issues/29820", "@mttbx maybe not. I have implement some custom ops, all become fatal, but can work under 1.13.1 docker image.\r\n\r\n```\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\t\r\n\t\r\n\tgsignal\r\n\t\r\n\t\r\n\tstd::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&)\r\n\ttensorflow::shape_inference::InferenceContext::Run(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)> const&)\r\n\ttensorflow::ShapeRefiner::RunShapeFn(tensorflow::Node const*, tensorflow::OpRegistrationData const*, tensorflow::ExtendedInferenceContext*)\r\n\ttensorflow::ShapeRefiner::AddNode(tensorflow::Node const*)\r\n\tTF_FinishOperation\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t_PyFunction_FastCallDict\r\n\t\r\n\t\r\n\t\r\n\t_PyObject_FastCallKeywords\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\tPyObject_Call\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\tPyObject_Call\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\tPyObject_Call\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\tPyObject_Call\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t_PyFunction_FastCallDict\r\n\t\r\n\tPyObject_Call\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t_PyFunction_FastCallDict\r\n\t\r\n\t\r\n\t\r\n\t_PyObject_FastCallKeywords\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t_PyFunction_FastCallDict\r\n\t\r\n\t\r\n\t\r\n\t_PyObject_FastCallKeywords\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t\r\n\t_PyEval_EvalFrameDefault\r\n\t\r\n\t_PyFunction_FastCallDict\r\n\t\r\n\tPyObject_Call\r\n\t_PyEval_EvalFrameDefault\r\n*** End stack trace ***\r\n```", "I think we should give it a try for gcc 4.8.", "The stack trace suggests that this is an error in shape inference (and it does not seem to have anything to do with tf.data).\r\n\r\nI took a look at the `MakeShape` [method](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/shape_inference.cc#L731) and my guess is that this is an instance of use-after-free race. In particular, given how trivial the method is, my guess is that `InferenceContext` object that is used is being destructed (or already destructed) by the time the call happens.\r\n\r\nI would bring this to the attention of someone familiar with what happens in `TF_FinishOperation`.", "@josh11b @gunan do you know what could be going on here?", "I believe I'm also seeing this issue. The InferenceContext that's passed to SetShapeFn is a null pointer for me. \r\n\r\nEdit: Note that I am using TF1.14, not 2.0b", "@martinwicke Any progress?", "To build the custom ops, did you use the native system compiler, or did you follow the instructions at https://github.com/tensorflow/custom-op", "problem solved!!! I use g++-4.8 instead!", "I'm gonna close this issue, because I've found the solution. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29951\">No</a>\n", "> @martinwicke Same problem in `docker image` of tf 1.14.\r\n> \r\n> All custom ops can work with tf 1.13.1 docker image, but fail in 1.14.\r\n> \r\n> I have test g++ 5, 7, 8, all failed.\r\n\r\n+1 \r\nCustom ops compile and run fine on TF1.13.1, but end with a seg-fault on TF1.14. Unable to move production workflow to TF1.14. Please take a look.", "> > @martinwicke Same problem in `docker image` of tf 1.14.\r\n> > All custom ops can work with tf 1.13.1 docker image, but fail in 1.14.\r\n> > I have test g++ 5, 7, 8, all failed.\r\n> \r\n> +1\r\n> Custom ops compile and run fine on TF1.13.1, but end with a seg-fault on TF1.14. Unable to move production workflow to TF1.14. Please take a look.\r\n\r\n@sjain-stanford I had the same issue; @mttbx discovered that compiling with g++-4.8 does do the trick for whatever reason. g++-4.8 is the same version used in the custom op docker container too.  ", "@Masterchef365 thanks, g++-4.8 did the trick. Still unclear why the previous builds using g++-5.4 against TF <=1.13.1 didn't result in seg-fault.", "The problem with these compiler incompatibilities is they're basically a\ngame of russian roulette. You never know when they hit you.\n", "@martinwicke  So when to release the fixed docker image for tf1.14?", "@angersson have we released docker containers for 1.14?\n\nThe custom op setup should work though, which container are you using?\n", "@martinwicke docker image for 1.14 has released, which tag is `latest-py3` and `latest-devel-py3`. And I using these docker image for testing, and deployment.\r\n\r\n\r\nAnother issue, when compile ops with g++-4.8 under 1.14 docker image:\r\nwhen I using root account to install g++-4.8 and compile ops, all pass. But after install g++-4.8 as root,  I add another user account, then compile error, log as below:\r\n```\r\ngitlab-runner@3dec7a24a2d4:/delta/delta/layers/ops$ ./build.sh delta\r\nprepare dependency\r\ng++   -fPIC -shared -O2 -std=c++11  -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -I/  -c kernels/fbank.cc -o linux_x86_64/obj/kernels/fbank.o  -L/usr/local/lib/python3.6/dist-packages/tensorflow -l:libtensorflow_framework.so.1 \r\nIn file included from /usr/include/x86_64-linux-gnu/c++/4.8/bits/os_defines.h:39:0,\r\n                 from /usr/include/x86_64-linux-gnu/c++/4.8/bits/c++config.h:426,\r\n                 from /usr/include/c++/4.8/bits/stl_algobase.h:59,\r\n                 from /usr/include/c++/4.8/vector:60,\r\n                 from /delta/delta/layers/ops/kernels/fbank.h:20,\r\n                 from kernels/fbank.cc:17:\r\n/usr/include/features.h:424:25: fatal error: sys/cdefs.h: No such file or directory\r\n #  include <sys/cdefs.h>\r\n                         ^\r\ncompilation terminated.\r\n```", "@zh794390558, can you try using `devel-py3` instead? `latest-devel-py3` is actually very old (our `devel` are just called `devel`; I'll update the docs to reflect this). Perhaps also `nightly-custom-op`, which is newer.", "@martinwicke Yep, they've been released. I'm not totally sure about this use case though, because our versioned containers just pre-install the libraries needed to run TF. `latest-devel` was superceded by `devel` some time ago (I guess I'll need to delete that tag, because it's confusing) and it looks like the `custom-op` images are mixed ages (with `nightly-custom-op` the youngest).", "@angersson I think unify the base image of `tf docker` with `ci` is important things.", "The solution to downgrade compilers seems a bit crazy and is very undocumented. While the nod to the incompatible ABI exists in the documentation, it is not at all clear on all the caveats that need to be dealt with. \r\n\r\nFor example, in our flow, we use c++14 stuff (constexpr) in our custom op implementation which worked swimmingly in tf 1.13 and g++7. Now, because of changes made and some possibly naughty out of scope allocation (potentially), we are being forced to use compilers that produce \"more appropriate\" code. This all seems sort of crazy to me. Without determinism, the programmer is surely destined for insanity. \r\n\r\nDo we know if the fix for this according to the tf gods has been decreed to be \"Follow our build flow\"? "]}, {"number": 29950, "title": "[TF 2.0/1.14 API Docs] `tf.keras.callbacks` - broken links to source", "body": "##### Description:\r\n`tf.keras.callbacks` TF r2.0 and r1.14 docs state that it's defined in an `__init__.py` but the links are broken (404): \r\n\r\nThere is no link to `__init__.py` in TF 1.13 stable (1.14 too? since it's just been released)\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks\r\n\r\n- URL(s) with the issue (404):\r\nr2.0: https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py\r\nr1.14: https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/api/_v1/keras/callbacks/__init__.py\r\n\r\n- Link to the documentation entry:\r\nr2.0: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks\r\nr1.14: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/callbacks\r\n", "comments": []}, {"number": 29949, "title": "Warning printing to stdout instead of stderr", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 19.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.0-rc2\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 7.0\r\n- GPU model and memory: Titan X GTI 12GB\r\n\r\n**Describe the current behavior**\r\nWhen importing from tf.contrib a warning is raised to do with the sunsetting of tf.contrib, as expected.\r\nHowever unlike every other error in tensorflow, it is written to STDOUT instead of STDERR.\r\nThis causes critical problems for applications that require STDOUT to transfer data.\r\n\r\n**Describe the expected behavior**\r\nIt is expected the warning is written to STDERR.\r\n\r\n**Code to reproduce the issue**\r\n```python3\r\nimport tensorflow as tf\r\nprint(tf.contrib.image.connected_components)\r\n```\r\n\r\n**Other info / logs**\r\nThe following warning is printed:                                                                                                                                                                         \r\n```\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.                                                                                                 \r\nFor more information, please see:                                                                                                                                              \r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md                                                                                        \r\n  * https://github.com/tensorflow/addons                                                                                                                                       \r\nIf you depend on functionality not listed there, please file an issue.                 \r\n```", "comments": ["Deprecation warning messages are needed to inform people who want to use an actual version of TensorFlow what is going to happen in the future, and give them time to adapt. However, we can disable these messages for us using private API : \r\n\r\nimport tensorflow.python.util.deprecation as deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False \r\n\r\nPlease let us know if that resolves the issue. Thanks!\r\n\r\n\r\n", "@achandraa I have gotten around the problem already by using contextlib.redirect_stdout.\r\nThank you for the better alternative using the deprecations private API.\r\n\r\nMy application works fine with either of these work-arounds, but is it intentional that this error outputs to STDOUT instead of STDERR?\r\nI think that is quite unusual and unexpected.", "As of now, we are providing these warnings just to make user aware of things which are going to happen in the future version and give them time to adapt. You can suppress them using the APIs provided. Hope this answered your query. Thanks!", "This has been fixed in master, there probably was a bug in 1.13-rc0.\r\n\r\n```\r\nmihaimaruseac@ankh:/tmp$ mkdir 5\r\nmihaimaruseac@ankh:/tmp$ cd 5\r\nmihaimaruseac@ankh:/tmp/5$ virtualenv .\r\nRunning virtualenv with interpreter /usr/bin/python2\r\nNew python executable in /tmp/5/bin/python2\r\nAlso creating executable in /tmp/5/bin/python\r\nInstalling setuptools, pkg_resources, pip, wheel...done.\r\nmihaimaruseac@ankh:/tmp/5$ source bin/activate\r\n(5) mihaimaruseac@ankh:/tmp/5$ pip install tf_nightly\r\n... <snip> ...\r\nSuccessfully installed absl-py-0.7.1 astor-0.8.0 backports.weakref-1.0.post1 enum34-1.1.6 funcsigs-1.0.2 futures-3.2.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.21.1 h5py-2.9.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.16.4 protobuf-3.8.0 six-1.12.0 tb-nightly-1.14.0a20190614 termcolor-1.1.0 tf-estimator-nightly-1.14.0.dev2019062701 tf-nightly-1.15.0.dev20190627 werkzeug-0.15.4 wrapt-1.11.2\r\n(5) mihaimaruseac@ankh:/tmp/5$ cat test.py \r\nimport tensorflow as tf\r\nprint(tf.contrib.image.connected_components)\r\n(5) mihaimaruseac@ankh:/tmp/5$ python test.py 2> /dev/null\r\n<function connected_components at 0x7fe6b0811410>\r\n```"]}, {"number": 29948, "title": "TF2.0beta distribute.MirroredStrategy hangs causing 100% GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I run the code provided in the tutorial https://www.tensorflow.org/beta/tutorials/distribute/keras\r\n\r\n== check python ===================================================\r\npython version: 3.6.7\r\npython branch: \r\npython build version: ('default', 'Feb 28 2019 09:07:38')\r\npython compiler version: GCC 7.3.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Thu Nov 29 14:49:43 UTC 2018\r\nos release version: 3.10.0-957.1.3.el7.x86_64\r\nos platform: Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-centos-7.6.1810-Core\r\nlinux distribution: ('CentOS Linux', '7.6.1810', 'Core')\r\nlinux os distribution: ('centos', '7.6.1810', 'Core')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='monod33.mbb.ki.se', release='3.10.0-957.1.3.el7.x86_64', version='#1 SMP Thu Nov 29 14:49:43 UTC 2018', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                    1.16.4              \r\nprotobuf                 3.8.0               \r\ntensorflow-datasets      1.0.2               \r\ntensorflow-gpu           2.0.0b1             \r\ntensorflow-metadata      0.13.0              \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nbinary (pip)\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n2.0.0b1 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda 10.0/cuDNN 7\r\n- GPU model and memory:\r\n4x Nvidia 1080Ti 11GB (MSI GeForce GTX) \r\n\r\n**Describe the current behavior**\r\n- Training on one GPUs works fine\r\n- All GPUs are recognized\r\n- When I select multiple GPUs example: `strategy = tf.distribute.MirroredStrategy(devices=[\"/device:GPU:0\",\"/device:GPU:1\"])` the processing hang and the GPUs are stuck at 100%\r\n\r\ncode output hangs at:\r\n```\r\n2019-06-19 09:03:44.971843: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-06-19 09:03:44.992837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-19 09:03:46.143152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n```\r\nnvidia-smi output\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.93       Driver Version: 410.93       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:1A:00.0 Off |                  N/A |\r\n| 29%   38C    P2    73W / 250W |  10883MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:1B:00.0 Off |                  N/A |\r\n| 29%   35C    P2    73W / 250W |  10883MiB / 11178MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:88:00.0 Off |                  N/A |\r\n| 29%   28C    P8     8W / 250W |    157MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:89:00.0 Off |                  N/A |\r\n| 29%   26C    P8     8W / 250W |    157MiB / 11178MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================||    0    195955      C   ...processing/anaconda3/envs/tf/bin/python 10873MiB |\r\n|    1    195955      C   ...processing/anaconda3/envs/tf/bin/python 10873MiB ||    2    195955      C   ...processing/anaconda3/envs/tf/bin/python   147MiB |\r\n|    3    195955      C   ...processing/anaconda3/envs/tf/bin/python   147MiB |+-----------------------------------------------------------------------------+\r\n\r\n**Describe the expected behavior**\r\nRun the training in parallel\r\n\r\n**Code to reproduce the issue**\r\nCode provided in the tutorial: https://www.tensorflow.org/beta/tutorials/distribute/keras for TF2.0 Beta\r\n**Other info / logs**\r\noutput when running the code\r\n``` python\r\n2019-06-19 09:03:37.964465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:1a:00.0\r\n2019-06-19 09:03:37.967415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:1b:00.0\r\n2019-06-19 09:03:37.970349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:88:00.0\r\n2019-06-19 09:03:37.973263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:89:00.0\r\n2019-06-19 09:03:37.973368: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-19 09:03:37.973414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-19 09:03:37.973455: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-19 09:03:37.973538: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-19 09:03:37.973578: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-19 09:03:37.973618: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-19 09:03:37.973679: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-19 09:03:37.995237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-06-19 09:03:37.998940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:1a:00.0\r\n2019-06-19 09:03:38.001616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:1b:00.0\r\n2019-06-19 09:03:38.004343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:88:00.0\r\n2019-06-19 09:03:38.007065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:89:00.0\r\n2019-06-19 09:03:38.007148: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-19 09:03:38.007219: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-19 09:03:38.007257: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-19 09:03:38.007293: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-19 09:03:38.007356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-19 09:03:38.007393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-19 09:03:38.007448: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-19 09:03:38.027019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-06-19 09:03:38.027513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-19 09:03:38.027550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 \r\n2019-06-19 09:03:38.027578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y \r\n2019-06-19 09:03:38.027603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y \r\n2019-06-19 09:03:38.027627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y \r\n2019-06-19 09:03:38.027651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N \r\n2019-06-19 09:03:38.041771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce G$X 1080 Ti, pci bus id: 0000:1a:00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.044220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce G$X 1080 Ti, pci bus id: 0000:1b:00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.046649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce G$X 1080 Ti, pci bus id: 0000:88:00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.049078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce G$X 1080 Ti, pci bus id: 0000:89:00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.287085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:1a:00.0\r\n2019-06-19 09:03:38.288567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:1b:00.0\r\n2019-06-19 09:03:38.290048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:88:00.0\r\n2019-06-19 09:03:38.291505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:89:00.0\r\n2019-06-19 09:03:38.291560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-19 09:03:38.291573: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-19 09:03:38.291586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-19 09:03:38.291599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-19 09:03:38.291611: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-19 09:03:38.291623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-19 09:03:38.291635: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-19 09:03:38.302880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-06-19 09:03:38.303091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-19 09:03:38.303106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 \r\n2019-06-19 09:03:38.303116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y \r\n2019-06-19 09:03:38.303126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y \r\n2019-06-19 09:03:38.303134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y \r\n2019-06-19 09:03:38.303143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N \r\n2019-06-19 09:03:38.314925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:1a:\r\n00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.317831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 10470 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:1b:\r\n00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.320690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 10470 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:88:\r\n00.0, compute capability: 6.1)\r\n2019-06-19 09:03:38.323571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 10470 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:89:\r\n00.0, compute capability: 6.1)\r\nTrain on None steps\r\nEpoch 1/12\r\n2019-06-19 09:03:44.971843: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CP\r\nU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_\r\nFLAGS=--xla_hlo_profile.\r\n2019-06-19 09:03:44.992837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-19 09:03:46.143152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n```\r\n\r\nThanks for the help!", "comments": ["Hi\r\nI forgot to mention that the previous code was run using ipython. Today I run the same code with a jupyter notebook and as standalone script from shell and everything worked fine. I guess the issue has to do with ipython.\r\nThanks for the help!\r\n"]}, {"number": 29947, "title": "For loop as 'unknown type' Tensorflow dimensions", "body": "I want to use the shape (size) of a tf.where for the bound of a for-loop using Tensorflow in Pycharm. \r\nHowever, when I try to do this, \r\nI get the error: **'Tensor' object cannot be interpreted as an integer.**\r\n\r\nHow can I solve this problem?\r\n**How can I get the size of idxCut to use a for-loop?**\r\n\r\n**Development contents.**\r\n1. Find the index (idxCut) corresponding to the threshold in the data.\r\n2. Check whether data corresponding to idxCut is TPR.\r\n\r\nI want to find the TPR (Turning Point Ratio) about idxCut in the data using a for-loop.\r\nI used a for-loop to obtain the TPR between idx, idx-1 and idx + 1.\r\nI want to find data[idx] is higher than the others data[idx-1, idx+1].\r\n\r\nHere's my code:\r\n\r\n> def funCalculate(data):\r\n>     ### Cut-off Threshold\r\n>     idxCut = tf.where(data > cutoff)\r\n>     idxCut = tf.squeeze(idxCut)   # The size of idxCut is always variable.\r\n> \r\n>     ### Compute by the size of idxCut\r\n>     valueCut = []\r\n>     for ii in range(0, tf.shape(idxCut)[0]):\r\n>         v1 = tf.where(data[idxCut[ii]] > data[idxCut[ii] - 1], 1, 0)\r\n>         v2 = tf.where(data[idxCut[ii]] > data[idxCut[ii] + 1], 1, 0)\r\n>         v3 = tf.where(v1 + v2 > 1, 1, 0)\r\n>         valueCut.append(v3)\r\n>     return valueCut\r\n\r\n", "comments": ["@HeewonChung92 Please provide details TensorFlow version. Also, did you compile from source or install a binary? In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29946, "title": "How to make remote worker load custom op?", "body": "I write a custom op and compile it to .so file. When I use it in to train model locally, it works fine. But when I use it to run in standalone mode, worker server alway tells me OP not registered, because the worker server started before, it have not load my custom op.\r\n\r\nSo is there anyway I can do to tell remote worker to load lib dynamic like tf.load_op_library in local mode?", "comments": ["@tensorflower-gardener ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and provide better help to such issues. Meanwhile you can also have a look on this [documentation.](https://www.tensorflow.org/guide/extend/op). Hope that helps. Thanks!\r\n", "@achandraa Thanks for your reply. But I think it is a bug. Because there is not way to rpc remote worker to load_op_library. Only way to do that is to stop remote worker, and restart it with load_op_library before join().\r\n"]}, {"number": 29945, "title": "build for Windows always fail", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 (try cmake and bazel,not work)\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.0.0 or 1.14\r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):0.25\r\n- GCC/Compiler version (if compiling from source):vs 2019\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no use\r\n\r\n\r\n\r\nbuild all fail,I followed the official documentation.However, it cannot be executed successfully.Some files cannot be downloaded or otherwise.\r\nPlease help me:\r\n* Tell me how to build c api\r\n* Give me the file for tensorflow.lib and tensorflow.dll.\r\n\r\nthank you very much!", "comments": ["Can you provide the error message you got when building TensorFlow on Windows?", "@meteorcloudy \r\nThanks for your reply, using cmake has been unsuccessful. I compiled it successfully after using bazel. Now I have a new problem. I need x86_32 dll. I don't know how to make it.\r\n\uff08\u611f\u8c22\u4f60\u7684\u56de\u590d\uff0c\u4f7f\u7528 cmake \u4e00\u76f4\u4e0d\u6210\u529f\uff0c\u6211\u4f7f\u7528bazel\u4e4b\u540e\u7f16\u8bd1\u6210\u529f\uff0c\u73b0\u5728\u6709\u4e2a\u65b0\u7684\u95ee\u9898\uff0c\u6211\u9700\u8981x86_32 \u7684dll\uff0c\u6211\u4e0d\u77e5\u9053\u5982\u4f55\u5236\u4f5c\u3002\uff09", "@kekxv This is an old issue. The  documentation has improved a lot since the issue opened. There is a Special Interest Group (SIG) for especially [build related issues](https://github.com/tensorflow/build). You can learn and contribute towards build related issues. Users can also attend months meetings or follow the progress [here](https://docs.google.com/document/d/10_3IQ5aF-88ADJNLF0WOpb09bZ15x-sBnRSnDHNCNr8/edit#heading=h.u7w1oawlfu32). \r\n\r\nAs this is an old issue, I am closing this issue. Please feel free to open a new issue if there is any build related issues with recent TF versions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29945\">No</a>\n"]}, {"number": 29944, "title": "[TF 2.0] tf.keras.optimizers.Adam", "body": "**System information**\r\n- TensorFlow version: 2.0.0-dev20190618\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nI am trying to minimize a function using tf.keras.optimizers.Adam.minimize() and I am getting a TypeError\r\n\r\n**Describe the expected behavior**\r\nFirst, in the TF 2.0 docs, it says the loss can be callable taking no arguments which returns the value to minimize. whereas the type error reads \"'tensorflow.python.framework.ops.EagerTensor' object is not callable\", which is not exactly the correct TypeError, it might be for some. \r\n\r\nBut the main issue is, I know I can do the same optimization by using GradientTape but I don't understand why I should or why the minimize() is not working. A similar issue I found related to this is linked: (https://github.com/tensorflow/tensorflow/issues/28068) and a stack overflow solution for how you can solve a similar problem using gradient tape for reference: (https://stackoverflow.com/questions/55060736/tensorflow-2-api-regression-tensorflow-python-framework-ops-eagertensor-object).\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nN  = 1000                               # Number of samples\r\nn  = 4                                  # Dimension of the optimization variable\r\nnp.random.seed(0) \r\nX = tf.Variable(np.random.randn(n, 1))  # Variables will be tuned by the optimizer\r\nC = tf.constant(np.random.randn(N, n))  # Constants will not be tuned by the optimizer\r\nD = tf.constant(np.random.randn(N, 1))\r\n\r\ndef f_batch_tensorflow(x, A, B):\r\n    e = tf.matmul(A, x) - B\r\n    return tf.reduce_sum(tf.square(e))\r\n\r\nfx = f_batch_tensorflow(X, C, D)\r\nprint(fx)\r\n\r\nadam_opt = tf.keras.optimizers.Adam()\r\noptimizer = adam_opt.minimize(fx, X)\r\nprint(optimizer)\r\n```\r\n\r\n**Other info / logs**\r\nFollowing is the error  I am getting:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-20-225de189a3ff> in <module>()\r\n      7 \r\n      8 adam_opt = tf.keras.optimizers.Adam()\r\n----> 9 optimizer = adam_opt.minimize(fx, X)\r\n     10 print(optimizer)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _compute_gradients(self, loss, var_list, grad_loss)\r\n    347       if not callable(var_list):\r\n    348         tape.watch(var_list)\r\n--> 349       loss_value = loss()\r\n    350     if callable(var_list):\r\n    351       var_list = var_list()\r\n\r\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable\r\n```\r\n", "comments": ["The function will have already been executed. You need to pass `f_batch_tensorflow` (or rather, a partial thereof) to `minimize`, not the result of executing it. \r\n\r\n@dynamicwebpaige @tanzhenyu This is probably a common mistake, we should catch and rethrow \r\n (or type check the input) this specific error to explain what went wrong, maybe \"EagerTensors cannot be passed to Optimizer.minimize. Please pass the callable representing the computation instead.\"", "Hey, @martinwicke if you meant passing the function directly like so:\r\n\r\n```\r\nN  = 1000                           \r\nn  = 4                                  \r\nX = tf.Variable(np.random.randn(n, 1))  \r\nC = tf.constant(np.random.randn(N, n)) \r\nD = tf.constant(np.random.randn(N, 1))\r\n\r\ndef f_batch_tensorflow(x, A, B):\r\n    return tf.reduce_sum(tf.square(tf.matmul(A, x) - B))\r\n\r\noptimizer = tf.keras.optimizers.Adam().minimize(f_batch_tensorflow(X, C, D), X)\r\n```\r\n\r\nThis results in the same error unless I am missing something. Because when you pass a callable with arguments it will get evaluated before the `minimize` gets computed right so minimize will end up with the result, doesn't it? Or is the minimize just meant to be used inside training loops? \r\n\r\nIf you have an example, that would be great, I kinda searched stack overflow but everything I could find on `minimize ` was using sessions which made sense\r\n", "What needs to be done here is:\r\nf_without_any_args = functools.partial(f_batch_tensorflow, A=C, B=D)\r\noptimizer.minimize(f_without_any_args, X)", "@martinwicke Though I've been thinking maybe we should support symbolic tensor in some sense, i.e., inside tf.function? loss_fn is not a nice pattern and we're basically pushing users towards the GradientTape path.\r\n(Supporting symbolic tensor is tricky though, because the forward pass has to be inside the same function)", "These particular tensors weren't symbolic. In general, yes, but I fear it\nwould cause more confusion. We should definitely improve the error message\nthough.\n", "perfect, thanks a lot @tanzhenyu that makes sense :) ", "It seems that \r\n```\r\nf_without_any_args = functools.partial(f_batch_tensorflow, x=X, A=C, B=D)\r\noptimizer.minimize(f_without_any_args, X)\r\n```\r\notherwise, it will raise error that f_batch_tensorflow() missing 1 required positional argument: 'x' in the optimizer.minimize function. ", "In the 1.x version, **minimize** function takes the loss as **a tensor**.\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/training/optimizer.py#L367\r\n\r\n\r\nAnd in the 2.x version, the **minimize** function takes the loss as **a callable function** \r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L299\r\nas it computes the loss in _compute_gradients\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L351\r\n\r\nIt really confused. Hope the minimize in v2 could take the loss as a tensor as well. \r\n\r\nUsing GradientTape should solve the problem. \r\n\r\n"]}, {"number": 29943, "title": "Segmentation Fault for GetAttr function using tensorflow2.0 framework ", "body": "get a Segmentation Fault for GetAttr function using tensorflow2.0 framework \r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\nTensorFlow installed from (source or binary):binary\r\nTensorFlow version (use command below):2.0b1\r\nPython version:3.6\r\nCUDA/cuDNN version:10/7.4\r\nGPU model and memory:7.4/24gb", "comments": ["testbug.cc:\r\n```\r\n\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/framework/tensor_shape.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/util/work_sharder.h\"\r\n\r\n#include <iostream>\r\n#include <cmath>\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"TestBug\")\r\n    .Attr(\"attr: int\")\r\n    .Output(\"dummy: float\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        int32_t attr;\r\n        TF_RETURN_IF_ERROR(c->GetAttr(\"attr\", &attr));\r\n    });\r\n\r\nclass TestBugOp : public OpKernel\r\n{\r\npublic:\r\nexplicit TestBugOp(OpKernelConstruction* context)\r\n        : OpKernel(context)\r\n{\r\n\r\n}\r\n\r\nvoid Compute(OpKernelContext* context) override\r\n{\r\n    Tensor* dummy = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, {1},\r\n                                                     &dummy));\r\n}\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(\r\n  Name(\"TestBug\").Device(DEVICE_CPU),\r\n  TestBugOp\r\n);\r\n```\r\n    \r\ntestbug.py:\r\n```\r\n\r\nimport tensorflow as tf\r\nextr_module = tf.load_op_library('./build/libextr_module.so')\r\n\r\ndummy = extr_module.test_bug()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run([dummy])\r\n```\r\nCMakeLists.txt:\r\n```\r\n\r\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)\r\nPROJECT(extr_module)\r\n\r\n\r\n# compiler flags\r\nSET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -O2 ${OpenMP_CXX_FLAGS} -Wall -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -DGOOGLE_CUDA=1\")\r\n\r\n# TensorFlow dependencies\r\nEXECUTE_PROCESS(COMMAND python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_include(), end='', flush=True)\"  OUTPUT_VARIABLE TF_INC)\r\n\r\nEXECUTE_PROCESS(COMMAND python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_lib(), end='', flush=True)\"  OUTPUT_VARIABLE TF_LIB)\r\n\r\n\r\nMESSAGE(STATUS \"Found TF_INC: \" ${TF_INC})\r\n#MESSAGE(STATUS \"Found TF_INC_EXTERNAL: \" ${TF_INC}/external/nsync/public)\r\nMESSAGE(STATUS \"Found TF_LIB: \" ${TF_LIB})\r\n\r\n\r\nINCLUDE_DIRECTORIES(${TF_INC})\r\n#INCLUDE_DIRECTORIES(${TF_INC}/external/nsync/public)\r\nLINK_DIRECTORIES(${TF_LIB})\r\n\r\n\r\nADD_LIBRARY(extr_module SHARED\r\n  testbug.cc\r\n)\r\n\r\nTARGET_LINK_LIBRARIES(extr_module tensorflow_framework)\r\n```\r\ntest:\r\n```\r\n\r\nmkdir build\r\ncd build\r\ncmake ..\r\nmake\r\ncd ..\r\npython3 testbug.py\r\n```\r\n", "> testbug.cc:\r\n> \r\n> ```\r\n> \r\n> #include \"tensorflow/core/framework/op_kernel.h\"\r\n> #include \"tensorflow/core/framework/register_types.h\"\r\n> #include \"tensorflow/core/framework/tensor.h\"\r\n> #include \"tensorflow/core/framework/tensor_shape.h\"\r\n> #include \"tensorflow/core/framework/register_types.h\"\r\n> #include \"tensorflow/core/framework/op.h\"\r\n> #include \"tensorflow/core/framework/shape_inference.h\"\r\n> #include \"tensorflow/core/util/work_sharder.h\"\r\n> \r\n> #include <iostream>\r\n> #include <cmath>\r\n> \r\n> using namespace tensorflow;\r\n> \r\n> REGISTER_OP(\"TestBug\")\r\n>     .Attr(\"attr: int\")\r\n>     .Output(\"dummy: float\")\r\n>     .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n>         int32_t attr;\r\n>         TF_RETURN_IF_ERROR(c->GetAttr(\"attr\", &attr));\r\n>     });\r\n> \r\n> class TestBugOp : public OpKernel\r\n> {\r\n> public:\r\n> explicit TestBugOp(OpKernelConstruction* context)\r\n>         : OpKernel(context)\r\n> {\r\n> \r\n> }\r\n> \r\n> void Compute(OpKernelContext* context) override\r\n> {\r\n>     Tensor* dummy = NULL;\r\n>     OP_REQUIRES_OK(context, context->allocate_output(0, {1},\r\n>                                                      &dummy));\r\n> }\r\n> };\r\n> \r\n> REGISTER_KERNEL_BUILDER(\r\n>   Name(\"TestBug\").Device(DEVICE_CPU),\r\n>   TestBugOp\r\n> );\r\n> ```\r\n> \r\n> testbug.py:\r\n> \r\n> ```\r\n> \r\n> import tensorflow as tf\r\n> extr_module = tf.load_op_library('./build/libextr_module.so')\r\n> \r\n> dummy = extr_module.test_bug()\r\n> \r\n> with tf.Session() as sess:\r\n>     sess.run([dummy])\r\n> ```\r\n> \r\n> CMakeLists.txt:\r\n> \r\n> ```\r\n> \r\n> CMAKE_MINIMUM_REQUIRED(VERSION 2.8)\r\n> PROJECT(extr_module)\r\n> \r\n> \r\n> # compiler flags\r\n> SET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -O2 ${OpenMP_CXX_FLAGS} -Wall -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -DGOOGLE_CUDA=1\")\r\n> \r\n> # TensorFlow dependencies\r\n> EXECUTE_PROCESS(COMMAND python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_include(), end='', flush=True)\"  OUTPUT_VARIABLE TF_INC)\r\n> \r\n> EXECUTE_PROCESS(COMMAND python3 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_lib(), end='', flush=True)\"  OUTPUT_VARIABLE TF_LIB)\r\n> \r\n> \r\n> MESSAGE(STATUS \"Found TF_INC: \" ${TF_INC})\r\n> #MESSAGE(STATUS \"Found TF_INC_EXTERNAL: \" ${TF_INC}/external/nsync/public)\r\n> MESSAGE(STATUS \"Found TF_LIB: \" ${TF_LIB})\r\n> \r\n> \r\n> INCLUDE_DIRECTORIES(${TF_INC})\r\n> #INCLUDE_DIRECTORIES(${TF_INC}/external/nsync/public)\r\n> LINK_DIRECTORIES(${TF_LIB})\r\n> \r\n> \r\n> ADD_LIBRARY(extr_module SHARED\r\n>   testbug.cc\r\n> )\r\n> \r\n> TARGET_LINK_LIBRARIES(extr_module tensorflow_framework)\r\n> ```\r\n> \r\n> test:\r\n> \r\n> ```\r\n> \r\n> mkdir build\r\n> cd build\r\n> cmake ..\r\n> make\r\n> cd ..\r\n> python3 testbug.py\r\n> ```\r\n\r\nHi, have you solved this yet? "]}, {"number": 29942, "title": "`tf.GradientTape.gradient` returns `None` when `sources` is a tensor (not a variable)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (see below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip (tf-nightly-gpu)\r\n- TensorFlow version (use command below): v1.12.1-3447-g5a0f1bbfb7 1.14.1-dev20190606\r\n- Python version: 3.6.8 (anaconda)\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: GTX 1050-ti\r\n\r\n**Describe the current behavior**\r\n`tf.GradientTape.gradient` is inconsistent with its documentation and `tf.gradients` when computing gradients with respect to tensors. It returns `None` rather than the partial derivative.\r\n\r\n**Describe the expected behavior**\r\nBehave as per `tf.gradients`.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nx0 = tf.get_variable('x0', shape=(), dtype=tf.float32)\r\nx1 = tf.constant(3.)\r\nx = x0 + x1\r\n# x = tf.constant(3.0)\r\ny = tf.constant(4.0)\r\nwith tf.GradientTape() as tape:\r\n    z = x + y\r\n    tape_grad = tape.gradient(z, x)\r\n    print(tape_grad)  # None\r\ntf_grad, = tf.gradients(z, x)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tf_grad))  # 1\r\n```", "comments": ["[GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) doesn't automatically watch Tensors. You need to add `tape.watch(x)`\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nx0 = tf.get_variable('x0', shape=(), dtype=tf.float32)\r\nx1 = tf.constant(3.)\r\nx = x0 + x1\r\n# x = tf.constant(3.0)\r\ny = tf.constant(4.0)\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(x)\r\n    z = x + y\r\ntape_grad = tape.gradient(z, x)\r\nprint(tape_grad)  # not None\r\ntf_grad, = tf.gradients(z, x)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tf_grad))  # 1\r\n```", "My apologies, and thank you."]}, {"number": 29941, "title": "Update nn_ops.py", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29941) for more info**.\n\n<!-- need_sender_cla -->", "@hzy9981 Please sign CLA in order to proceed with next steps. Thank you!", "@hzy9981 gentle ping to sign CLA. Thanks!", "Sorry to say but we cannot proceed until CLA reflects YES. Could you please open a new PR(which may resolve the CLA issue) and try. Closing this."]}, {"number": 29940, "title": "Link to SIRDS paper broken -> updated.", "body": "Old link to paper was broken recently.", "comments": []}, {"number": 29939, "title": "lite: fix buffer overrun of \"std::vector<int> sorted_indices\".", "body": "In NonMaxSuppressionMultiClassRegularHelper(), \r\nmore than (_max_detections_) elements of **sorted_indices** may be overwritten.\r\nbecause the value of _output_index_ can be up to (_num_boxes + max_detections_).\r\n\r\n>     int num_indices_to_sort = std::min(output_index, max_detections);\r\n>     DecreasingPartialArgSort(scores_after_regular_non_max_suppression.data(),  \r\n>                              output_index, num_indices_to_sort,\r\n>                              sorted_indices.data());\r\n\r\nHowever, **sorted_indices** has only (_max_detections_) capacity,\r\n\r\n>     sorted_indices.resize(max_detections);\r\n\r\nIt causes buffer overrun.\r\nSo I increased the size of **sorted_indices**.\r\n\r\n>     sorted_indices.resize(num_boxes + max_detections);\r\n\r\n", "comments": ["No problem, let's go ahead and land this and I can work on a test.", "@jdduke Thank you very much for your approval and cooperation.\r\n"]}, {"number": 29938, "title": "Update docstring of tf.sets.difference to include Raises", "body": "This fix updated docstring of tf.sets.difference to include\r\nconditions that raises exceptions.\r\n\r\nThis fix fixes #29897.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 29937, "title": "Iterator restoring hangs with `inter_op_parallelism_threads=1`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes; I have included a small example file below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `b'v1.13.1-0-g6612da8951' 1.13.1`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1.168/7.6.0.64 (installed via pacman)\r\n- GPU model and memory: Quadro P2000\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIf I set `inter_op_parallelism_threads=1` when I configure a session, I am unable to restore an Iterator from a basic `tensorflow_datasets` dataset; the restore operation hangs.\r\n\r\n**Describe the expected behavior**\r\nIf the iterator can run with `inter_op_parallelism_threads=1` then it should also be able to restore without issue.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow_datasets as tfds\r\nfrom sys import stderr\r\n\r\n# save/restore an iterator from a toy dataset\r\nmnist = tfds.image.MNIST()\r\nmnist.download_and_prepare()\r\nds = mnist.as_dataset()[\"train\"]\r\nds = ds.map(lambda x: x[\"image\"])\r\n\r\n# # Can't reproduce the issue using interleave/map/prefetch\r\n# ds = tf.data.Dataset.range(10)  # using this dataset makes it not hang\r\n# ds = ds.interleave(lambda x: tf.data.Dataset.from_tensors(x),\r\n#         cycle_length=64, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n# ds = ds.map(lambda x: x, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n# ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\ni = ds.make_one_shot_iterator()\r\n\r\n# create the operation\r\nnext_item = i.get_next()\r\n\r\n# create the savable and the saver\r\nsaveable = tf.data.experimental.make_saveable_from_iterator(i)\r\nsaver = tf.train.Saver({'iterator':saveable})\r\n\r\n# create a session config\r\nconfig = tf.ConfigProto(\r\n    inter_op_parallelism_threads=1, # this line makes saver.restore() hang.\r\n                                    # Also, 2 works fine, only 1 is a problem\r\n)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    # print the first five elements with a checksum\r\n    print('first five', file=stderr)\r\n    for _ in range(5):\r\n        image = sess.run(next_item)\r\n        print(np.sum(image*image), file=stderr)\r\n\r\n    # save the iterator\r\n    saver.save(sess, 'chkpt')\r\n\r\n    # restore the iterator\r\n    print('restoring', file=stderr)\r\n    saver.restore(sess, 'chkpt')\r\n\r\n    # (this line never prints:)\r\n    print('done restoring', file=stderr)\r\n\r\n\r\n    print('second five', file=stderr)\r\n    for _ in range(5):\r\n        # print the second five elements with a checksum\r\n        image = sess.run(next_item)\r\n        print(np.sum(image*image), file=stderr)\r\n```\r\n\r\n**Other info / logs**\r\nRuntime output:\r\n```\r\nWARNING:tensorflow:From /home/rb/.virtualenvs/pedl/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0618 16:01:19.861267 140558393956032 deprecation.py:323] From /home/rb/.virtualenvs/pedl/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-18 16:01:19.945928: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-18 16:01:19.965483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2905000000 Hz\r\n2019-06-18 16:01:19.966181: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d1be28d870 executing computations on platform Host. Devices:\r\n2019-06-18 16:01:19.966199: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nfirst five\r\n13301\r\n9638\r\n11128\r\n11245\r\n11787\r\nrestoring\r\nWARNING:tensorflow:From /home/rb/.virtualenvs/pedl/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0618 16:01:20.062302 140558393956032 deprecation.py:323] From /home/rb/.virtualenvs/pedl/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n```\r\n(now the process hangs indefinitely)\r\n", "comments": ["I think I have isolated the problem to the TFRecordDataset.  I can reproduce the behavior with the following code:\r\n```\r\nimport tensorflow as tf\r\nfrom sys import stderr\r\n\r\n# tfrecord was created by tensorflow_dataset.image.MNIST().as_dataset()\r\nds = tf.data.TFRecordDataset(\"mnist-train.tfrecord-00000-of-00010\")\r\n\r\ni = ds.make_one_shot_iterator()\r\n\r\n# create the operation\r\nnext_item = i.get_next()\r\n\r\n# create the savable and the saver\r\nsaveable = tf.data.experimental.make_saveable_from_iterator(i)\r\nsaver = tf.train.Saver({'iterator':saveable})\r\n\r\n# create a session config\r\nconfig = tf.ConfigProto(\r\n    inter_op_parallelism_threads=1, # this line makes saver.restore() hang.\r\n                                    # Also, 2 works fine, only 1 is a problem\r\n)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    # print the first five elements with a checksum\r\n    print('first item', file=stderr)\r\n    print(sess.run(next_item), file=stderr)\r\n\r\n    # save the iterator\r\n    saver.save(sess, 'checkpoint/chkpt')\r\n\r\n    # restore the iterator will hang\r\n    print('restoring', file=stderr)\r\n    saver.restore(sess, 'checkpoint/chkpt')\r\n\r\n    # (this line never prints:)\r\n    print('done restoring', file=stderr)\r\n\r\n    print('second item', file=stderr)\r\n    print(sess.run(next_item), file=stderr)\r\n```", "I can reproduce this in 1.14 as well.", "I have a workaround which sets a default thread pool of size > 1 with an alternative thread pool that is of size 1.  This works but it is very awkward because you have to pass a new RunOptions argument to every call to session.run():\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom sys import stderr\r\nfrom tensorflow.core.protobuf import config_pb2\r\n\r\n# tfrecord was created by tensorflow_dataset.image.MNIST().as_dataset()\r\nds = tf.data.TFRecordDataset(\"mnist-train.tfrecord-00000-of-00010\")\r\n\r\ni = ds.make_one_shot_iterator()\r\n\r\n# create the operation\r\nnext_item = i.get_next()\r\n\r\n# create the savable and the saver\r\nsaveable = tf.data.experimental.make_saveable_from_iterator(i)\r\nsaver = tf.train.Saver({'iterator':saveable})\r\n\r\nsingle_pool = config_pb2.ThreadPoolOptionProto(num_threads=1)\r\nmulti_pool = config_pb2.ThreadPoolOptionProto(num_threads=2)\r\n\r\n# create a session config\r\nconfig = tf.ConfigProto(\r\n    # it would be nice if saver.restore() supported a run_options keyword,\r\n    # rather than having to make the multi_pool the default pool and then\r\n    # specify single_runopts with every call to sess.run()\r\n    session_inter_op_thread_pool=[multi_pool, single_pool],\r\n)\r\n\r\n# inter_op_thread_pool is just an index into session_inter_op_thread_pool\r\nsingle_runopts = tf.RunOptions(inter_op_thread_pool=1)\r\n\r\nwith tf.Session(config=config) as sess:\r\n\r\n    print('first item', file=stderr)\r\n    print(sess.run(next_item, options=single_runopts), file=stderr)\r\n\r\n    saver.save(sess, 'checkpoint/chkpt')\r\n\r\n    print('second item', file=stderr)\r\n    print(sess.run(next_item, options=single_runopts), file=stderr)\r\n\r\n    # this will use the default (first) pool from session_inter_op_thread_pool\r\n    saver.restore(sess, 'checkpoint/chkpt')\r\n\r\n    print('second item again?', file=stderr)\r\n    print(sess.run(next_item, options=single_runopts), file=stderr)\r\n```", "@rb-determined-ai  I tried executing the above with Tf 1.13.1 but it throws error as `mnist-train.tfrecord-00000-of-00010; No such file or directory`. Please help us to reproduce the issue. Thanks!", "@gadagashwini,\r\n\r\nAh, I had a comment explaining where the file was from, but I see now that it wasn't quite correct.  First, make sure you have the `tensorflow_datasets` package.  (`tensorflow_datasets` is only required to download a TFRecord file)\r\n\r\n    pip3 install tensorflow_datasets\r\n\r\nYou can get the file by running the following command from the command line:\r\n\r\n    python -c \"import tensorflow_datasets as tfds; tfds.image.MNIST().download_and_prepare()\"\r\n\r\nAfter that, you should have the file in the place where `tensorflow_datasets` caches datasets.  In linux or mac, the following command should create copy the file to your current directory:\r\n\r\n    cp ~/tensorflow_datasets/mnist/1.0.0/mnist-train.tfrecord-00000-of-00010 .\r\n\r\nThen if you run the second script I included from the same directory, it should demonstrate the problem.\r\n\r\nAlso, the first script I attached should demonstrate the problem in either case, since it uses `tensorflow_datasets` to download and to access the TFRecord file.", "@rb-determined-ai I executed the above code on my system by downloading tensorflow_datasets I got the following result. Is this expected result? Thanks!\r\n![Screenshot from 2019-07-01 10-41-00](https://user-images.githubusercontent.com/48476109/60412018-39c0c580-9bed-11e9-8402-9777ab97dcdf.png)\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29937\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29937\">No</a>\n", "@gadagashwini please reopen, I was on vacation last week.\r\n\r\nOk, the screenshot you sent me was what to be expected based on what you ran.  The first element out of the dataset is a blob of binary data, followed by another blob, and the third blob is a copy of the second blob (which is correct).  The reason that it works is you ran my script which includes a workaround for the bug I reported.\r\n\r\nThere are three scripts that I have posted so far:\r\n- The first script was in the original post, and it demonstrates the bug but I didn't know the best way to demonstrate the bug.  If you run it, it will hang.\r\n- The second script was when I isolated the bug to TFRecordDataset and that script did not import `tensorflow_datasets`.  If you run it, it will hang.\r\n- The third script was when I posted a workaround for the bug.  It was the script where I call `from tensorflow.core.protobuf import config_pb2` near the top.  The workaround is that I do the `tf.train.Saver.restore()` operation using a different thread pool than the thread pool I use for every other call to `tf.Session.run()`.  If you run that script, it will not hang.  That appears to be the script that you ran when in the screenshot you sent.\r\n\r\nThe second script is the best example to demonstrate the bug, please focus on that one.\r\n\r\n", "@rb-determined-ai Thanks for the detailed analysis. I was able to reproduce your issue in TF 1.13.1      \r\nHowever I tried your script (2nd) in TF 1.15.0-rc1 version and it executed successfully.\r\nMay be you want to give it a try. Thanks!", "@ymodak I have confirmed that the bug is not fixed with TF 1.15.0-rc1.  These were the steps I took to reproduce:\r\n\r\n```\r\nmkvirtualenv --python /usr/bin/python3.6 --no-site-packages tf15\r\npip install tensorflow==1.15.0-rc1\r\npython --version  # prints \"Python 3.6.9\"\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"  # prints \"1.15.0-rc1\"\r\n\r\ncp ~/tensorflow_datasets/mnist/1.0.0/mnist-train.tfrecord-00000-of-00010 .\r\n\r\n# I'll feed the script in via stdin so there's no confusion which one got ran:\r\necho '\r\nimport tensorflow as tf\r\nfrom sys import stderr\r\n\r\n# tfrecord was created by tensorflow_dataset.image.MNIST().as_dataset()\r\nds = tf.data.TFRecordDataset(\"mnist-train.tfrecord-00000-of-00010\")\r\n\r\ni = ds.make_one_shot_iterator()\r\n\r\n# create the operation\r\nnext_item = i.get_next()\r\n\r\n# create the savable and the saver\r\nsaveable = tf.data.experimental.make_saveable_from_iterator(i)\r\nsaver = tf.train.Saver({\"iterator\":saveable})\r\n\r\n# create a session config\r\nconfig = tf.ConfigProto(\r\n    inter_op_parallelism_threads=1, # this line makes saver.restore() hang.\r\n                                    # Also, 2 works fine, only 1 is a problem\r\n)\r\n\r\nwith tf.Session(config=config) as sess:\r\n    # print the first five elements with a checksum\r\n    print(\"first item\", file=stderr)\r\n    print(sess.run(next_item), file=stderr)\r\n\r\n    # save the iterator\r\n    saver.save(sess, \"checkpoint/chkpt\")\r\n\r\n    # restore the iterator will hang\r\n    print(\"restoring\", file=stderr)\r\n    saver.restore(sess, \"checkpoint/chkpt\")\r\n\r\n    # (this line never prints:)\r\n    print(\"done restoring\", file=stderr)\r\n\r\n    print(\"second item\", file=stderr)\r\n    print(sess.run(next_item), file=stderr)\r\n' | python\r\n```\r\n\r\nThis is my output (same as before, the line `\"done restoring\"` never prints):\r\n```\r\nWARNING:tensorflow:From loader_bug.py:7: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\r\nWARNING:tensorflow:From loader_bug.py:14: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nWARNING:tensorflow:From loader_bug.py:17: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\n2019-09-27 11:54:37.965504: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-27 11:54:37.984166: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2905000000 Hz\r\n2019-09-27 11:54:37.985072: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555c9ec73930 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-09-27 11:54:37.985106: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-09-27 11:54:37.990081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-09-27 11:54:38.046077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-27 11:54:38.046347: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555c9ef32c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-09-27 11:54:38.046360: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P2000, Compute Capability 6.1\r\n2019-09-27 11:54:38.046498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-27 11:54:38.046727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Quadro P2000 major: 6 minor: 1 memoryClockRate(GHz): 1.468\r\npciBusID: 0000:01:00.0\r\n2019-09-27 11:54:38.046803: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-09-27 11:54:38.046862: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-09-27 11:54:38.046896: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-09-27 11:54:38.046929: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-09-27 11:54:38.046961: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-09-27 11:54:38.046994: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2019-09-27 11:54:38.050035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-27 11:54:38.050053: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-09-27 11:54:38.050087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-27 11:54:38.050106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-09-27 11:54:38.050109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\nfirst item\r\nb'\\n\\xad\\x02\\n\\x0e\\n\\x05label\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x9a\\x02\\n\\x05image\\x12\\x90\\x02\\n\\x8d\\x02\\n\\x8a\\x02\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x1c\\x08\\x00\\x00\\x00\\x00Wf\\x80H\\x00\\x00\\x00\\xd1IDAT(\\x91c`\\x18\\x9a@\\x7f\\xce\\x87\\x7f\\xff?\\xd8`\\x93b\\xaf\\xfa\\xfa\\xf7\\xc7\\xf5G\\x7f\\xa7b\\x91\\x93_\\xf6\\xf7g\\xb3\\x11\\x83\\xfaG\\x14I\\x16\\x08%\\xc6\\xdd\\xb5\\xf1\\x04\\x03\\x03\\x1b3\\x03\\x16\\xc9\\xd3\\xfe\\x0c\\x0c\\x0c\\x0c\\x0c^\\x9c(\\x92L(<\\r\\x86\\x9f\\xb8]\\xfc\\xe8\\xaf\\x05N\\xb9\\x82\\x7f\\x87QLBv\\x01K\\xa7\\xdcM9\\xa5\\x17_\\xb1jt\\xf8\\xfb\\xef\\xef\\xdf\\xbf\\xf7\\x85\\xb0J\\xde\\xfc\\xf7\\xffl\\xfc\\x86\\x7f=\\xd8\\xe4R~\\xfe\\x9d#\\xc1\\xa0\\xfa\\xf9\\x016\\xc9;\\x7f{\\x19\\x18\\x18\\x18\\x16\\xfe\\xb0\\x86;\\x02!\\x19\\xa6t\\x88\\x81\\x81\\x81\\xe1\\r\\xab\\x00\\x16\\xc9s\\xe7\\xd0\\xcdbB\\x17``\\xc7\\xeaX(x\\xfdM\\x1f\\xa7\\x9c\\xee\\xd7\\xab\\xcc\\xb8\\xe4\\x84\\x1f\\xff\\xc5\\x9a\\x18\\x18\\x18\\x18\\x18\\x18\\xba\\xfeN\\xe1\\xc1g)\\x91\\x00\\x00\\xf4/?R\\xed\\xc3\\xb43\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82'\r\nrestoring\r\n```", "Please take a look at this [GitHub Gist](https://colab.sandbox.google.com/gist/ymodak/bf49fe37b52e5fc9caab053a6d8fc395/github_issue29937.ipynb)\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29937\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29937\">No</a>\n"]}]