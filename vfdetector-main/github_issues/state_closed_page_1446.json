[{"number": 9580, "title": "Enable MKL in configure and various bug fixes", "body": "This pull request enables MKL in configure. Building with bazel using `--config=mkl` will utilize all our ops for existing frameworks, replaced via the graph pass. \r\n\r\nVarious bug fixes include:\r\n- Performance fix for relu/maxpool (grad)\r\n- Graph layout pass changes\r\n- LRN defaults to eigen when depth radius is not 2\r\n- Various style fixes\r\n- Unit test build failure fixes\r\n- Fixed missing _ in name that was missed during previous search & replace by Google\r\n\r\n... and other minor changes.", "comments": ["Can one of the admins verify this patch?", "Should we start the unit tests?", "@tensorflow-jenkins test this please", "Fixed sanity check issue - please restart the test.", "@tensorflow-jenkins test this please", "I'm not entirely sure but something doesn't seem to be building ... have you tested locally?", "Ah this might be because configure has new questions - enable mkl support and whether or not to download mkl from the web. You will have to modify your automation script to cater to those questions (both yes for mkl). Sorry, I should've mentioned  that.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/configured I suspect this needs to be edited -- can you do that in this PR?", "I've made changes to address this based on the assumption that setting the variable value in configured will set it in the configure script. Please can you take a quick look at my latest check-in and if that looks right, start a new CI run?\r\n\r\n[Adding support for google's CI automation](https://github.com/tensorflow/tensorflow/pull/9580/commits/2f7468636f3bfd0990f19406c191ff055165f686)", "@tensorflow-jenkins test this please\r\n\r\nI think this looks reasonable, let's give it a try!  (if it doesn't work, I'll rope in some other CI gurus)", "Looks like the \"locate\" binary is missing in your PATH. We also use \"sed\". Would it be possible to add those to the environment or do we need to use something different?\r\n\r\n`./configure: line 252: locate: command not found`", "Pinging @caisq; do you or someone else know who can help with this?", "Could you add 'mlocate' to this file https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_deb_packages.sh#L49\r\n\r\n", "Done. Please restart the tests.", "Thanks @vivek-rane! @tensorflow-jenkins test this please", "Looks like some sort of environment issue:\r\n\r\nlocate: can not stat () `/var/lib/mlocate/mlocate.db': No such file or directory\r\nBuild step 'Execute shell' marked build as failure\r\nUnable to get pull request builder trigger!!\r\nSetting status of 35b62f397a39bf19320073039e5ae0fb253abee5 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4114/ and message: 'FAILURE\r\n '\r\nUsing context: Sanity Checks\r\nFinished: FAILURE", "Hmm, I wonder if it's okay to use mlocate in docker. Looking it up.", "@vrv @yifeif We primarily have the locate in place for UBuntu support. If it helps expedite things, we can remove that from this PR for now and revisit it in a later PR. Let me know which path you would like to recommend.", "Tried in a docker container, \r\nIf we install 'locate' instead of 'mlocate', locate can run but it wasn't able to find \"libdl.so.2\".\r\n\r\nWhat worked was:\r\nKeep \"apt-get install mlocate\", then run \"updatedb\" after the installation to populate the database.\r\n\r\n@vivek-rane could you add \"updatedb\" to the same file as well? Thanks!\r\n\r\n", "Done - can you check now?", "@tensorflow-jenkins test this please.", "Fixed the buildifier issue - sorry! Please restart tests.\r\n\r\nEDIT: looks like the fix suggested by @yifeif has worked :)", "Great, and no worries! @tensorflow-jenkins test this please", "Looks like we have to configure the mac os builds to not use MKL, since it doesn't look to be supported yet?", "Actually, for now, we should disable use of MKL for Mac OS. MKL has started supporting Mac OS recently, but not sure about the current status/stability.", "Yeah, I think you can do that here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/osx/libtensorflow_cpu.sh#L30\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/osx/libtensorflow_gpu.sh#L31\r\n\r\nby setting TF_NEED_MKL=0 and then we can probably merge this :)", "have set TF_NEED_MKL=0 in that two files you mentioned. @vrv please let us know if it is ready for merge!", "@tensorflow-jenkins test this please", "apparently that's not the only place, let me look to see where else (sigh!)", "@caisq it looks like the Mac OS X build is still calling the 'configured' program; I'm not entirely sure where we should be disabling via environment variable for the MacOS CPU tests -- can you help us figure it out?", "While we're fixing the MacOS tests, lets also look at the Linux CPU python3 test that failed for the following:\r\n`//tensorflow/contrib/factorization:wals_test                             FAILED in 1 out of 2 in 54.5s`\r\n\r\nIs this a known issue? Is there a way to access the log to figure out the error? \r\n\r\n` /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/tensorflow/contrib/factorization/wals_test/test.log`", "Contrib code is pretty flaky in general, so in this case I wouldn't worry about it :)", "Awesome - we just have the MacOS issue to resolve then.", "@vrv that's a little weird. The code looks like it should work. I just pulled the branch and tested `./configure` on a mac and it seems to correctly set TF_NEED_MKL to 0.", "@vivek-rane I made changes in your `configured` script, so that the MKL options are enabled only on Linux. The build is running again.", "@tensorflow-jenkins test this please", "@vivek-rane , thanks!\r\n@tensorflow-jenkins test this please ", "Sweet! Thanks for the help, @caisq @vrv @yifeif and @andydavis1 \ud83d\udc4d "]}, {"number": 9579, "title": "MNIST Tutorial Appears to Not Toggle XLA Compilation", "body": "I have been using the JIT compilation/XLA tutorial (https://www.tensorflow.org/performance/xla/jit#step_3_run_with_xla), and it seems that whether or not XLA compilation happens doesn't depend on the statement on line 63 in mnist_softmax_xla.py. The comment above it says that line will turn on XLA JIT compilation. When I run with the two options explained on the page (--xla='' for compiling without XLA, and TF_XLA_FLAGS=--xla_generate_hlo_graph=.* for compiling with XLA), the second executes line 63 and the first does not. But, both seem to compute in exactly the same way, going through compiler/xla/service. The output for both runs is:\r\n```\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-05-01 12:50:25.203870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-01 12:50:25.203904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-01 12:50:25.232661: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-01 12:50:25.232702: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\r\n2017-05-01 12:50:25.233155: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\n2017-05-01 12:50:25.233165: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-01 12:50:25.233894: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-01 12:50:25.233903: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\r\n2017-05-01 12:50:25.234360: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Xpu. Devices:\r\n2017-05-01 12:50:25.234369: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234372: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (1): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234376: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (2): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234379: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (3): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234382: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (4): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234385: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (5): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234389: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (6): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234392: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (7): <undefined>, <undefined>\r\n0.9206\r\n```", "comments": ["Those are informational log messages that are emitted during startup. They do not indicate any code is actually running via XLA.\r\n\r\nTry setting the environment variable TF_CPP_MIN_VLOG_LEVEL=2 and looking for log messages from xla_device_launch_op.cc or xla_local_launch_op.cc.", "Okay, when I do that, there is quite a lot printed, but neither output has anything from either of those files. I'm running:\r\n`TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist.py &> out_xla`\r\nand\r\n`python mnist.py --xla='' &> out_xla`\r\nand searching for 'xla_device_launch_op' and 'xla_local_launch_op' in the file out_xla each time, and neither is found.", "Just to double-check: did you configure with XLA?", "Yes, I set TF_ENABLE_XLA=1 when I configure", "I think this is working as intended. By default, only the GPU JIT will be enabled by the --xla flag to that script. We do not enable the CPU JIT by default at the moment because it does not support inter-operator parallelism. And you do not have a GPU installed or are not compiling with CUDA.\r\n\r\nIf you wish to enable XLA on the CPU, you will need to modify the mnist_softmax_xla.py script to use a `with tf.device(\"device:XLA_CPU:0\"):` scope to force CPU operators to be compiled with XLA. The \"Placing operators on XLA devices\" section of the tutorial describes how to do this.\r\n\r\n(I cannot speak to what is happening with your custom \"Xpu\" device --- if I cannot see the source code, there is no way for me to debug it.)", "I am back to trying to run the mnist example with XLA, and I just tried your suggestion of forcing \"with tf.device('device:XLA_CPU:0')\". I get the error that it could not satisfy the explicit device specification. Am I putting the statement in the wrong place?\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys \r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.client import timeline\r\n\r\nFLAGS = None\r\n\r\n\r\ndef main(_):\r\n  # Import data\r\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n\r\n  # Create the model\r\n  x = tf.placeholder(tf.float32, [None, 784])\r\n  w = tf.Variable(tf.zeros([784, 10]))\r\n  b = tf.Variable(tf.zeros([10]))\r\n  y = tf.matmul(x, w) + b \r\n\r\n  # Define loss and optimizer\r\n  y_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\n  # The raw formulation of cross-entropy,\r\n  #\r\n  #   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\r\n  #                                 reduction_indices=[1]))\r\n  #\r\n  # can be numerically unstable.\r\n  #\r\n  # So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\r\n  # outputs of 'y', and then average across the batch.\r\n  cross_entropy = tf.reduce_mean(\r\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\r\n  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\n  config = tf.ConfigProto()\r\n  jit_level = 0 \r\n  if FLAGS.xla:\r\n    # Turns on XLA JIT compilation.\r\n    jit_level = tf.OptimizerOptions.ON_1\r\n    print('XLA on')\r\n  config.graph_options.optimizer_options.global_jit_level = jit_level\r\n  run_metadata = tf.RunMetadata()\r\n  with tf.device('device:XLA_CPU:0'):\r\n      sess = tf.Session(config=config)\r\n      tf.global_variables_initializer().run(session=sess)\r\n      # Train\r\n      train_loops = 1000\r\n      for i in range(train_loops):\r\n        batch_xs, batch_ys = mnist.train.next_batch(100)\r\n\r\n        # Create a timeline for the last loop and export to json to view with\r\n        # chrome://tracing/.\r\n        if i == train_loops - 1:\r\n          sess.run(train_step,\r\n                   feed_dict={x: batch_xs,\r\n                              y_: batch_ys},\r\n                   options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n                   run_metadata=run_metadata)\r\n          trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n          with open('timeline.ctf.json', 'w') as trace_file:\r\n            trace_file.write(trace.generate_chrome_trace_format())\r\n        else:\r\n          sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n\r\n      # Test trained model\r\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n      print(sess.run(accuracy,\r\n                     feed_dict={x: mnist.test.images,\r\n                                y_: mnist.test.labels}))\r\n      sess.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--data_dir',\r\n      type=str,\r\n      default='/tmp/tensorflow/mnist/input_data',\r\n      help='Directory for storing input data')\r\n  parser.add_argument(\r\n      '--xla', type=bool, default=True, help='Turn xla via JIT on')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Did you manage to solve the issue? I'm also having this problem (running Ubuntu 16.04 with 2x 1080 Ti GPUs)", "@aselle  @hawkinsp I have followed the above instructions, what's next? Any help would be really appreciated \ud83d\udc4d \r\n\r\nBelow is the output when I run both with and without XLA.\r\n\r\n---\r\n**Output**\r\n\r\n\u279c  mnist git:(r1.4) \u2717 python mnist_softmax_xla.py --xla=\"\"                           \r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2018-01-01 14:45:23.553389: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-01 14:45:23.781647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.61GiB\r\n2018-01-01 14:45:23.978426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2018-01-01 14:45:23.979139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\r\n2018-01-01 14:45:23.979175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 \r\n2018-01-01 14:45:23.979185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y \r\n2018-01-01 14:45:23.979192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y \r\n2018-01-01 14:45:23.979203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-01-01 14:45:23.979211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-01-01 14:45:25.534197: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n0.9198\r\n\r\n\u279c  mnist git:(r1.4) \u2717 TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py    \r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2018-01-01 14:45:30.756407: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-01 14:45:30.988718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.61GiB\r\n2018-01-01 14:45:31.171956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2018-01-01 14:45:31.172457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\r\n2018-01-01 14:45:31.172480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 \r\n2018-01-01 14:45:31.172486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y \r\n2018-01-01 14:45:31.172489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y \r\n2018-01-01 14:45:31.172495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-01-01 14:45:31.172501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-01-01 14:45:32.471294: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n0.9151\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 145 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Was this ever fixed? I'm seeing this issue. How did you get XLA to work on the CPU with confirmation? ", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So I found replacing tf.Variable with tf.get_variable and setting use_resource to True works. Also the flag command is TF_XLA_FLAGS=\"--xla_hlo_graph_path=PATH --xla_generate_hlo_graph=.*\"\r\n", "I'm not sure this issue is still relevant. Feel free to reopen if it still occures."]}, {"number": 9578, "title": "Building TensorFlow with CPU SIMD OPTIONS enabled on Windows 10 FAILED", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master\r\n- **Bazel version (if compiling from source)**: No\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**:MSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI was following the instructions to build tensorflow on Windows \r\nhttps://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\r\nThe error came when I tried to build the PIP package. I have VS2015.\r\nThe error is related to zlib at  \r\nCMake Error at C:/Projects/tensorflow/tensorflow/contrib/cmake/build/zlib/tmp/zlib-gitclone.cmake:84 (message):\r\n    Failed to init submodules in:\r\n    'C:/Projects/tensorflow/tensorflow/contrib/cmake/build/zlib/src/zlib'\r\nThere are more errors with the same issue (git failed to init submodules) for: highwayhash, and jasoncpp.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12\\swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:\\Users\\sergio.murillo\\AppData\\Local\\Programs\\Python\\Python35/PYTHON.EXE -DPYTHON_LIBRARIES=C:\\Users\\sergio.murillo\\AppData\\Local\\Programs\\Python\\Python35\\libs\\python35.lib  -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Projects/tensorflow/tensorflow/contrib/cmake/build\r\n\r\nMSBuild /p:Configuration=Release /filelogger tf_python_build_pip_package.vcxproj\r\n....\r\n\"C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib.vcxproj\" (default target) (11) ->\r\n(CustomBuild target) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1 [C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib.vcxproj]\r\n...\r\nmsbuild.log\r\nCreating directories for 'zlib'\r\n  Performing download step (git clone) for 'zlib'\r\n  Cloning into 'zlib'...\r\n  Note: checking out '50893291621658f355bc5b4d450a8d06a563053d'.\r\n  \r\n  You are in 'detached HEAD' state. You can look around, make experimental\r\n  changes and commit them, and you can discard any commits you make in this\r\n  state without impacting any branches by performing another checkout.\r\n  \r\n  If you want to create a new branch to retain commits you create, you may\r\n  \r\n[msbuild.txt](https://github.com/tensorflow/tensorflow/files/968656/msbuild.txt)\r\n\r\ndo so (now or later) by using -b with the checkout command again. Example:\r\n  \r\n    git checkout -b <new-branch-name>\r\n  \r\n  HEAD is now at 5089329... zlib 1.2.8\r\n  fatal: 'submodule' appears to be a git command, but we were not\r\n  able to execute it. Maybe git-submodule is broken?\r\n  CMake Error at zlib-gitclone.cmake:84 (message):\r\n    Failed to init submodules in:\r\n    'C:/Projects/tensorflow/tensorflow/contrib/cmake/build/zlib/src/zlib'\r\n  \r\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib.vcxproj]\r\nDone executing task \"CustomBuild\" -- FAILED.\r\nDone building target \"CustomBuild\" in project \"zlib.vcxproj\" -- FAILED.\r\nDone Building Project \"C:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib.vcxproj\" (default targets) -- FAILED.\r\n\r\n\r\n\r\n", "comments": ["It looks like a problem with your `git` installation:\r\n\r\n```\r\n  fatal: 'submodule' appears to be a git command, but we were not\r\n  able to execute it. Maybe git-submodule is broken?\r\n```\r\n\r\n...which appears for many different dependent projects. Can you test whether `git-submodule` is broken?", "Hi,\r\n  I agree that git submodule fails for different projects, but I am not sure how to test if it works correctly. Excuse me if I ask some very basic questions but all this is very new to me. Hopefully what I did tested whether or not git-submodule is broken. I found a script or shell command for git-submodule in \r\nC:\\Program Files\\Git\\mingw64\\libexec\\git-core\r\nI can open it on notepad++ -> git-submodule.sh\r\n\r\nWhen I write in command line git submodule init, nothing happens. After a second or so, it returns to the prompt\r\n\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build>git --version\r\ngit version 2.12.2.windows.2\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build>git submodule init\r\n\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build>git submodule summary\r\n\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib>git --help submodule\r\n(--This one send me to file:///C:/Program%20Files/Git/mingw64/share/doc/git-doc/git-submodule.html )\r\n\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib\\src\\zlib>git status\r\nHEAD detached at 5089329\r\nnothing to commit, working tree clean\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib\\src\\zlib>git submodule init\r\n\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib\\src\\zlib>git submodule summary\r\n\r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib\\src\\zlib>\r\n\r\nI'm not sure what else I could do. I searched online and did not find a good solution or at least one that I could implement.\r\nThanks.\r\n", "Forgot to add that I founda folder .git \r\nC:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib\\src\\zlib\\.git\r\nThat contains a config file:\r\n\r\n[core]\r\n\trepositoryformatversion = 0\r\n\tfilemode = false\r\n\tbare = false\r\n\tlogallrefupdates = true\r\n\tsymlinks = false\r\n\tignorecase = true\r\n[remote \"origin\"]\r\n\turl = https://github.com/madler/zlib\r\n\tfetch = +refs/heads/*:refs/remotes/origin/*\r\n[branch \"master\"]\r\n\tremote = origin\r\n\tmerge = refs/heads/master\r\n\r\nMaybe it helps. Thanks.", "Hi,\r\n Uninstalling Git and installing the latest version fixed the problem.\r\nThanks!"]}, {"number": 9577, "title": "Fix Community link in README", "body": "Linking to the rendered community page is more useful compared to the current link which seems to be out-of-date.", "comments": ["Can one of the admins verify this patch?", "Thanks!"]}, {"number": 9576, "title": "dilated convolution in 3D, error:  No algorithm without scratch worked", "body": "I'm using tf.nn.convolution to implement the dilated convolution in 3D. I got \"No algorithm without scratch worked\" error during training. Here is the related code\r\n\r\nTo define the model,\r\n```\r\ndef inference(self, images, is_training, keep_prob):\r\n        \"\"\" Forward inference\r\n        Return:\r\n\r\n        \"\"\"\r\n        c1 = self._dilation_conv(images, self._n_filters, scope = 'c1', dilation_rate = (1,1,1))\r\n        score = self._conv(c1, scope = 'score', filters = 2,  filter_size = (1,1,1) )\r\n\r\n        pred = tf.nn.softmax(score)\r\n\r\n        # pdb.set_trace()\r\n        return score, pred\r\n\r\n    def _conv(self, in_tensor, filters, scope, filter_size = None):\r\n        \"\"\" \"\"\"\r\n\r\n        if self._wd is None:\r\n            myreg = None\r\n        else:\r\n            myreg = tf.contrib.layers.l2_regularizer(float(self._wd))\r\n\r\n        if filter_size is None:\r\n            filter_size = self._filter_size\r\n\r\n        with tf.variable_scope(scope):\r\n            return tf.layers.conv3d(\r\n                in_tensor, filters = filters,\r\n                kernel_size = filter_size, padding = 'valid',\r\n                activation = None,\r\n                kernel_initializer  = tf.truncated_normal_initializer(stddev = self._stddev),\r\n                kernel_regularizer =myreg,\r\n                name = 'conv')        \r\n\r\n    def _dilation_conv(self, in_tensor, n_filters, scope, dilation_rate, filter_size = None):\r\n        \"\"\" dilated convolution filter with batch norm. \"\"\"\r\n        \r\n        if self._wd is None:\r\n            myreg = None\r\n        else:\r\n            myreg = tf.contrib.layers.l2_regularizer(float(self._wd))\r\n\r\n        if filter_size is None:\r\n            filter_size = self._filter_size\r\n\r\n        batch_size, H, W, D, in_channel = in_tensor.get_shape().as_list()            \r\n\r\n        with tf.variable_scope(scope):\r\n            kernel = tf.get_variable('weights', shape = self._filter_size + (in_channel, n_filters),\r\n                                     dtype = tf.float32,\r\n                                     initializer = tf.truncated_normal_initializer(stddev=self._stddev))\r\n            output = tf.nn.convolution(in_tensor, kernel, padding = 'SAME', strides = (1,1,1), dilation_rate = dilation_rate, name = 'dilation_conv')\r\n\r\n            # Somehow set training = True even for testing phase works\r\n            # better.\r\n            if self._bn:\r\n                output = tf.layers.batch_normalization(\r\n                    output, training = True, name = 'bn')\r\n\r\n            return tf.nn.relu(output, 'relu')\r\n            \r\n    def get_loss(self, labels, scores, beta = 0.9999):\r\n        \"\"\"\r\n        return total loss of the model, including data loss and\r\n        regularization loss.\r\n    \r\n        It looks that softmax_cross_entropy_with_logits does not need the\r\n        input logits 2 dimension. As long as the last dimension is for\r\n        classes, it should work. So, we do not need reshape the input tensor. \r\n\r\n        TF has a weighted_cross_entropy_with_logits, but this function is\r\n        for multi-class problem, i.e. a picture may have both a dog and a\r\n        truck.\r\n        \"\"\"\r\n\r\n        class_weight = tf.constant([beta, 1.0 - beta])\r\n        # TF suppot numpy's broadcasting. score array has dim BXY2,\r\n        # weights array has dim (2), which is broadcast to BXY2.\r\n        weighted_logits = tf.multiply(scores, class_weight)\r\n\r\n        # both logits and labels are BXY2 dimension. \r\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = weighted_logits, labels = labels)\r\n        # fromm dim BXY to dim 0 (scalar)\r\n        cross_entropy_mean = tf.reduce_mean(cross_entropy, name = 'cross_entropy')\r\n\r\n        reg_loss_list = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n        if reg_loss_list:\r\n            return cross_entropy_mean + tf.add_n(reg_loss_list)\r\n        else:\r\n            return cross_entropy_mean\r\n```\r\nAnd to train the model, I used tf.train.AdamOptimizer. I didn't paste the training related code, since I don't think they are relevant, but I can add them later if that helps. \r\n\r\nHere is the error logs I saw: \r\n``` \r\nIn [40]: train.train(train_dataset, test_dataset, model = model, out_ckpt='./ckpt_3d/dilation', summary_dir='./summary_3d/dilation')\r\n2017-05-01 15:08:10.813716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Graphics Device, pci bus id: 0000:85:00.0)\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1038     try:\r\n-> 1039       return fn(*args)\r\n   1040     except errors.OpError as e:\r\n\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1020                                  feed_dict, fetch_list, target_list,\r\n-> 1021                                  status, run_metadata)\r\n   1022 \r\n\r\n/usr/lib/python3.4/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nNotFoundError: No algorithm without scratch worked!\r\n\t [[Node: gradients/c1/dilation_conv_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/c1/dilation_conv_grad/Shape, c1/weights/read, gradients/AddN_3)]]\r\n\t [[Node: Adam/update/_26 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_89_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-40-c48a6e49a4b4> in <module>()\r\n----> 1 train.train(train_dataset, test_dataset, model = model, out_ckpt='./ckpt_3d/dilation', summary_dir='./summary_3d/dilation')\r\n\r\n/home/weiliu/projects/seismic/code/weiliu/train.py in train(train_dataset, test_dataset, model, init_lr, summary_dir, in_ckpt, out_ckpt)\r\n    141                          keep_prob_pl: 0.5}\r\n    142 \r\n--> 143             _, loss_value = sess.run([optimizer, loss], feed_dict = feed_dict)\r\n    144 \r\n    145             global_step_val = tf.train.global_step(sess, global_step)\r\n\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    776     try:\r\n    777       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 778                          run_metadata_ptr)\r\n    779       if run_metadata:\r\n    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    980     if final_fetches or final_targets:\r\n    981       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 982                              feed_dict_string, options, run_metadata)\r\n    983     else:\r\n    984       results = []\r\n\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1030     if handle is None:\r\n   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1032                            target_list, options, run_metadata)\r\n   1033     else:\r\n   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1050         except KeyError:\r\n   1051           pass\r\n-> 1052       raise type(e)(node_def, op, message)\r\n   1053 \r\n   1054   def _extend_graph(self):\r\n\r\nNotFoundError: No algorithm without scratch worked!\r\n\t [[Node: gradients/c1/dilation_conv_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/c1/dilation_conv_grad/Shape, c1/weights/read, gradients/AddN_3)]]\r\n\t [[Node: Adam/update/_26 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_89_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'gradients/c1/dilation_conv_grad/Conv3DBackpropInputV2', defined at:\r\n  File \"/usr/local/bin/ipython\", line 11, in <module>\r\n    sys.exit(start_ipython())\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/__init__.py\", line 119, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/terminal/ipapp.py\", line 348, in start\r\n    self.shell.mainloop()\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/terminal/interactiveshell.py\", line 440, in mainloop\r\n    self.interact()\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/terminal/interactiveshell.py\", line 431, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-40-c48a6e49a4b4>\", line 1, in <module>\r\n    train.train(train_dataset, test_dataset, model = model, out_ckpt='./ckpt_3d/dilation', summary_dir='./summary_3d/dilation')\r\n  File \"/home/weiliu/projects/seismic/code/weiliu/train.py\", line 116, in train\r\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step = global_step)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\", line 368, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/nn_grad.py\", line 77, in _Conv3DGrad\r\n    padding=op.get_attr(\"padding\")),\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 663, in conv3d_backprop_input_v2\r\n    padding=padding, name=name)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op 'c1/dilation_conv', defined at:\r\n  File \"/usr/local/bin/ipython\", line 11, in <module>\r\n    sys.exit(start_ipython())\r\n[elided 8 identical lines from previous traceback]\r\n  File \"<ipython-input-40-c48a6e49a4b4>\", line 1, in <module>\r\n    train.train(train_dataset, test_dataset, model = model, out_ckpt='./ckpt_3d/dilation', summary_dir='./summary_3d/dilation')\r\n  File \"/home/weiliu/projects/seismic/code/weiliu/train.py\", line 67, in train\r\n    images_placeholder, is_training = istraining_pl, keep_prob = keep_prob_pl )\r\n  File \"/home/weiliu/projects/seismic/code/weiliu/dilation_net_3d.py\", line 71, in inference\r\n    c1 = self._dilation_conv(images, self._n_filters, scope = 'c1', dilation_rate = (1,1,1))\r\n  File \"/home/weiliu/projects/seismic/code/weiliu/dilation_net_3d.py\", line 116, in _dilation_conv\r\n    output = tf.nn.convolution(in_tensor, kernel, padding = 'SAME', strides = (1,1,1), dilation_rate = dilation_rate, name = 'dilation_conv')\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/nn_ops.py\", line 661, in convolution\r\n    op=op)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/nn_ops.py\", line 331, in with_space_to_batch\r\n    return op(input, num_spatial_dims, padding)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/nn_ops.py\", line 653, in op\r\n    name=name)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/nn_ops.py\", line 140, in _non_atrous_convolution\r\n    name=name)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 529, in conv3d\r\n    strides=strides, padding=padding, name=name)\r\n  File \"/home/weiliu/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n\r\nNotFoundError (see above for traceback): No algorithm without scratch worked!\r\n\t [[Node: gradients/c1/dilation_conv_grad/Conv3DBackpropInputV2 = Conv3DBackpropInputV2[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/c1/dilation_conv_grad/Shape, c1/weights/read, gradients/AddN_3)]]\r\n\t [[Node: Adam/update/_26 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_89_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n```\r\nI also check the `atrous_convolution_test.py` and it seems I used it right. \r\n\r\nI'm on Ubuntu 14.04 64 bit, tensorflow version 1.1.0-rc2. \r\n\r\nIf anyone can point me direction how to debug, that would be great. ", "comments": ["Most likely an issue that this is not supported by the underlying Cudnn 5 library that is being used. Adding mjanusz@ to confirm.", "Thanks. As I know TF does not support cudnn 6 yet, but this dilated operation seems have been here for a while. ", "got same error using cudnn 6.", "The real error for me is out of memory, but it still raise this error.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this as there have been no updates since July and the bot has nagged three times in a row. @mjanusz feel free to reopen if you're still working on this. ", "I'm getting the error \"No Algorithm found!\" while using Dilated 3D convolutions. Does it have anything to do with the Compute Capability version? I'm running my model on a GTX 960M. But seeing from the previous comments, the method seems to be giving out the same issues with Compute Capability 6.0 as well(btw mine is 5.0). I'd like to know if there's a fix for this.", "I get `tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked! [Op:Conv3D]`\r\n- Error shows up when we use dilation > 1 and padding=SAME.\r\n- I've checked this on TFlow2.4 with CUDA11.0 and CuDNN=8.0\r\n\r\n```\r\nimport os\r\nimport pdb\r\nimport traceback\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \r\nimport tensorflow as tf\r\nif len(tf.config.list_physical_devices('GPU')):\r\n    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\r\nelse:\r\n    print (' - No GPU present!! Exiting ...')\r\n    import sys; sys.exit(1)\r\n\r\nclass Conv3DWS(tf.keras.layers.Conv3D):\r\n    \"\"\"\r\n    Ref\r\n     - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D\r\n     - https://www.tensorflow.org/api_docs/python/tf/nn/conv3d\r\n     - https://github.com/joe-siyuan-qiao/WeightStandardization\r\n    \"\"\"\r\n\r\n    def __init__(self, filters, kernel_size=(3,3,3), strides=(1, 1, 1), padding='same'\r\n                    , dilation_rate=(1,1,1)\r\n                    , activation='relu'\r\n                    , kernel_regularizer=None\r\n                    , name=''):\r\n        super(Conv3DWS, self).__init__(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding\r\n                        , dilation_rate=dilation_rate\r\n                        , activation=activation\r\n                        , kernel_regularizer=kernel_regularizer\r\n                        , name=name)\r\n    \r\n    def call(self,x):\r\n\r\n        # Step 1 - WS\r\n        kernel_mean = tf.math.reduce_mean(self.kernel, axis=[0,1,2,3], keepdims=True, name='kernel_mean')\r\n        kernel_std  = tf.math.reduce_std(self.kernel, axis=[0,1,2,3], keepdims=True, name='kernel_std')\r\n        kernel_new  = (self.kernel - kernel_mean)/(kernel_std + tf.keras.backend.epsilon())\r\n        \r\n        # Step 2 - Convolution\r\n        # [Does not works due to padding=same]\r\n        output = tf.nn.conv3d(input=x, filters=kernel_new, strides=list((1,) + self.strides + (1,)), padding=self.padding.upper(), dilations=(1,) + self.dilation_rate + (1,)) \r\n        # [Works due to padding=valid]\r\n        # output = tf.nn.conv3d(input=x, filters=kernel_new, strides=list((1,) + self.strides + (1,)), padding='VALID', dilations=(1,) + self.dilation_rate + (1,)) \r\n        \r\n        # Step 3 - Bias\r\n        if self.use_bias:\r\n            output = tf.nn.bias_add(output, self.bias, data_format=self._tf_data_format)\r\n        \r\n        # Step 4 - Activation and return\r\n        if self.activation is not None:\r\n            return self.activation(output)\r\n        else:\r\n            return output\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        x = tf.random.normal((1,140,140,40,1))\r\n        layers = Conv3DWS(filters=10, dilation_rate=(3,3,3))\r\n        y = layers(x)\r\n        print (' - y: ', y.shape)\r\n\r\n    except:\r\n        traceback.print_exc()\r\n        pdb.set_trace()\r\n```\r\n\r\n"]}, {"number": 9575, "title": "Catch exception on tf_session.TF_NewStatus()", "body": "\"try\" and \"finally\" without \"except\" is not sufficient to catch the exception.  If the exception gets thrown, it can stop output from other threads from being printed, and the exception makes TF appear to be broken.  Fixes #8652", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I am covered by the Google Corporate CLA.  (ldap: sffc)", "CLAs look good, thanks!\n\n<!-- ok -->", "Alternative: the exception that gets thrown in this case is a *AttributeError*:\r\n\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x112402c50>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 587, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TF_NewStatus'\r\n```\r\n\r\nI can change the `except` clause to catch only an AttributeError instead.", "@mrry can you take a look to figure out what exception type / strategy we would want to employ here?  Thanks!", "@tensorflow-jenkins test this please \r\n\r\n(this seems fine to me)"]}, {"number": 9574, "title": "Fix misspellings in tensorflow tools", "body": "I found three misspellings in the source codes.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9573, "title": "[issue#9328]fix invalid pointer when free()", "body": "Fix the [issue#9328](https://github.com/tensorflow/tensorflow/issues/9328) of TensorFlow v1.0 images.", "comments": ["Can one of the admins verify this patch?", "@caisq @girving Please check.", "@DjangoPeng Can you send the PR to r1.0 instead of master? Master (and r1.1 and future branches) are on ubuntu:16.04. Only r1.0 is on ubuntu:14.04, which requires this patch, if I understand correctly.", "(Closing since this needs to go into r1.0 branch)"]}, {"number": 9572, "title": "cuda_error_out_of_memory when trying to run tensorflow/tensorflow/examples/tutorials/layers/cnn_mnist.py", "body": "When I run tensorflow/tensorflow/examples/tutorials/layers/cnn_mnist.py on GPU I get the cuda_error_out_of_memory. \r\n\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 3.00G (3221225472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1002] failed to allocate 2.70G (2899102720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:404] error retrieving driver version: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n\r\nTo circumvent it, I thought of using `config.gpu_options.per_process_gpu_memory_fraction` or `config.gpu_options.allow_growth`, but I don't know where to set the session configuration as I don't see any sessions in the code. Also, is there any way I can set session configuration globally for one file?", "comments": ["What version of cuda and cudnn?\r\nWhat version of TensorFlow? From Source?\r\n@mrry, any ideas? It looks like the version reading code is linux/macos specific (reading proc).", "The most likely explanation is that there isn't enough memory. There is usually a log message that includes the amount of free memory on the device, which I believe is [printed before](https://github.com/tensorflow/tensorflow/blob/3ce228e46befca3a89441c7973a571108d2edf2f/tensorflow/core/common_runtime/gpu/gpu_device.cc#L900) the log messages that appear in the issue report. (The failure to read the version is benign, since we just use it to print a more descriptive error on Linux and Mac OS X.)", "Cuda - v8.0.60\r\ncudnn - 5.1 for cuda 8.0\r\nTensorFow - 1.0.1\r\nInstalled TensorFlow using pip\r\nOS - Windows 10, 64 bit\r\n\r\nCan you suggest me some workaround for now.", "Please re-open if you can reproduce on a GPU with enough memory."]}, {"number": 9571, "title": "model_dir deletion while running python script", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10\r\n- **TensorFlow installed from (source or binary)**:  binary, installed via pip\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:na\r\n- **CUDA/cuDNN version**: na\r\n- **GPU model and memory**: na\r\n- **Exact command to reproduce**:?\r\n\r\n### Describe the problem\r\nFeature request: As part of an hyperparameter optimization routine, I use tensorflow's high level api tf.contrib.learn.DNNRegressor() (should apply to the others as well). The problem is that it creates for each instance an own new model_dir, which I can't delete during running the python script (even after the model instance is overwritten and no longer in RAM). It is a problem because it consumes rather fast large amounts of disk storage. As far as I can see there is no way to delete the temp dir while running the program, only once it terminates the dir is released and removable. \r\n\r\n### Source code / logs\r\nhere is a pseudo code example:\r\nhttps://stackoverflow.com/questions/43639516/model-dir-deletion-in-tensorflow", "comments": ["Is this something desired @martinwicke? It seems to me this might be desirable because after you are done w/ your parameter search, you probably want the trained model and not to have to redo it again?", "This sounds like a problem we recently had in a unit test on Windows. The solution was to close file handles in a summary writer cache that were still open. You can see the solution here:\r\nhttps://github.com/tensorflow/tensorflow/pull/9281/files\r\n\r\nCan you verify that calling `FileWriterCache.clear()` before trying to remove the directory works? Try it at head -- #9185 will probably have to be in your code for this to work properly.\r\n\r\nWe considered doing this automatically after each `Estimator.train()` run -- but that seemed too much long term interference. However, it would make a lot of sense to do it in `Experiment.train_and_evaluate` and similar methods.", "I managed to delete after using `FileWriterCache.clear()`. Thanks and sorry for the late reply.", "Closing as this is resolved"]}, {"number": 9570, "title": "Tensorflow inconsistence results every run", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 9569, "title": "I updated tensorflow gpu version and jupyter notebook and I can not import tensorflow after updates. here is the error I got: ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I use another address in cmd window but in errors I see another address. many times ago I installed python in another account on this PC.\r\nbut before updates it was working correctly.\r\nplease help me.\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             return importlib.import_module(mname)\r\n     19         except ImportError:\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 41   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     42   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 21     _pywrap_tensorflow_internal = swig_import_helper()\r\n     22     del swig_import_helper\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     21     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     49 import numpy as np\r\n     50 \r\n---> 51 from tensorflow.python import pywrap_tensorflow\r\n     52 \r\n     53 # Protocol buffers\r\n\r\nc:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     50 for some common reasons and solutions.  Include the entire stack trace\r\n     51 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 52   raise ImportError(msg)\r\n     53 \r\n     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\users\\ekargar\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n```\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "Unfortunately, since you failed to give us any details except your error about what you updated (what was your previous version what was your new version), it's really hard to help you. You also did not fill out our issue template which might have given us more clues about what was going wrong.\r\n\r\nPlease try the items here \r\nhttps://www.tensorflow.org/install/install_windows\r\nIf that doesn't work I'd recommend removing all versions of tensorflow and al versions of python and start from as fresh of a machine as possible.\r\n"]}, {"number": 9567, "title": "Bazel error: building tensorflow 1.1 from source (CPU only)", "body": "### Problem description\r\n\r\nThe Docker build script below fails to build TensorFlow 1.1 from source with the following error messages:\r\n\r\n```\r\nERROR: /tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package.\r\n```\r\n\r\nComplete build log:\r\n\r\n```\r\n$ docker start tensorflow-builder\r\n$ docker attach tensorflow-builder\r\nroot@8eef137e3404:/tensorflow# git fetch -fapv\r\nFrom https://github.com/tensorflow/tensorflow\r\n = [up to date]      0.6.0      -> origin/0.6.0\r\n = [up to date]      docs-republishing -> origin/docs-republishing\r\n = [up to date]      estimator_windows -> origin/estimator_windows\r\n = [up to date]      fix-makefile-build -> origin/fix-makefile-build\r\n = [up to date]      master     -> origin/master\r\n = [up to date]      r0.10      -> origin/r0.10\r\n = [up to date]      r0.11      -> origin/r0.11\r\n = [up to date]      r0.12      -> origin/r0.12\r\n = [up to date]      r0.7       -> origin/r0.7\r\n = [up to date]      r0.8       -> origin/r0.8\r\n = [up to date]      r0.9       -> origin/r0.9\r\n = [up to date]      r1.0       -> origin/r1.0\r\n = [up to date]      r1.1       -> origin/r1.1\r\n = [up to date]      rn_delete  -> origin/rn_delete\r\nroot@8eef137e3404:/tensorflow# git checkout -b r1.1 origin/r1.1\r\nSwitched to a new branch 'r1.1'\r\nroot@8eef137e3404:/tensorflow# git pull\r\nAlready up-to-date.\r\nroot@8eef137e3404:/tensorflow# ./configure\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nUsing python library path: /usr/lib/python3/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] \r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\n.\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n..........\r\nERROR: /tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package.\r\n```\r\n\r\n### Dockerfile\r\n\r\n```\r\nFROM ubuntu:16.04\r\n\r\n# https://github.com/kubernetes/test-infra/blob/master/images/pull-kubernetes-bazel/Dockerfile\r\n\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n    build-essential \\\r\n    openjdk-8-jdk \\\r\n    pkg-config \\\r\n    zip \\\r\n    unzip \\\r\n    zlib1g-dev \\\r\n    bash-completion \\\r\n    git \\\r\n    wget \\\r\n    python && \\\r\n    apt-get clean\r\n\r\nENV BAZEL_VERSION 0.3.2\r\nRUN wget \"https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel_${BAZEL_VERSION}-linux-x86_64.deb\" && \\\r\n    dpkg -i \"bazel_${BAZEL_VERSION}-linux-x86_64.deb\" && \\\r\n    rm \"bazel_${BAZEL_VERSION}-linux-x86_64.deb\"\r\n\r\n# Fetch TensorFlow source code and build dependencies\r\n\r\nRUN apt-get install -y --no-install-recommends git python3-numpy swig python3-dev python3-wheel && \\\r\n    apt-get install -y --no-install-recommends python3-setuptools rsync && \\\r\n    apt-get clean && \\\r\n    ln -s /usr/bin/python3 /usr/local/bin/python && \\\r\n    git clone --quiet --recurse-submodules https://github.com/tensorflow/tensorflow.git /tensorflow\r\nWORKDIR /tensorflow\r\n\r\n# Steps to build:\r\n#\r\n# host$ docker build -t bazel .\r\n# host$ docker run -it --name bazel-builder bazel\r\n#\r\n# bazel-builder$ ./configure (press enter repeatedly)\r\n# bazel-builder$ bazel build -c opt --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package\r\n# bazel-builder$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n# bazel-builder$ exit\r\n#\r\n# host$ docker cp bazel-build:/tmp/tensorflow_pkg/tensorflow-0.11.0-py3-none-any.whl /tmp\r\n```", "comments": ["This seems to be a problem with the Bazel version. I am using 0.4.5 (latest) and it works.\r\n\r\nMy docker build is here:\r\n\r\nhttps://github.com/cirocavani/tensorflow-build/tree/master/1.1-cpu", "OK. Bazel changes a lot last time around I had to downgrade ;-) I'll try it out on my side.", "Seems to work just fine with 0.4.5!", "Glad you got it to work. Can we close this issue or is there some fix that needs to be propagated that you did?", "Possible the instructions on *which* basel version to install should be part of the instruction?\r\n\r\nhttps://www.tensorflow.org/install/install_sources just says\r\n\r\n> If bazel is not installed on your system, install it now by following [these directions](https://bazel.build/versions/master/docs/install.html).\r\n\r\nI think it can be motivated by the fact that there are multiple versions of basel that can be installed but TensorFlow will not build with all versions.", "I'm unable to reproduce this issue. Please reopen if it surfaces again."]}, {"number": 9566, "title": "Documentation pages are unnecessarily large", "body": "```\r\ncurl https://www.tensorflow.org/api_docs/python/tf/abs | wc -c  # 1841844 (1.8MB)\r\n```\r\nEach page in the documentation now contains a HUGE left navbar contributing over 99% of the size.\r\n\r\n1. This would waste a lot of network traffic on loading identical navbar over and over again.\r\n2. It creates a big trouble when I tried to build an offline version of the doc. The whole html documents used to be <100MB, now they are 2.5GB.", "comments": ["This topic brings up a lot of good points.\r\nWhere would I go into the REPO to find the code for the navbar?", "@wolffg, please take a look. cc @martinwicke.", "The whole documentation now has become 8GB.", "Thanks for reporting this.  If you check out your Network tab on Chrome, you aren't normally downloading 2.3Mb each time, but closer to about 700K.  That's still a lot, but less than you'd think looking at the final result.\r\n\r\nHaving said that, we're aware that there are just a lot of links on the API reference pages now, and that can make pages feel slow, especially on mobile.  We're discussing how we can keep the pages navigable but reduce the render speed.\r\n\r\nRather than scraping our site, it might be faster to generate the API reference yourself, which you can do with instructions here: \r\n\r\nhttps://www.tensorflow.org/community/documentation#generating_docs_and_previewing_links\r\n\r\nThe final leftnav generators generate exclusively for our internal serving architecture, so we haven't made them public, but you can run the core API generator scripts yourself with those instructions and you should get TOC files.", "Thanks for the comment! Indeed, the network traffic isn't huge because pages are downloaded as gzip.\r\n\r\nThe doc generation works for me, although I still prefer the official nice-looking html. I'll find my own way out for offline docs."]}, {"number": 9565, "title": "fix some trivial typos", "body": "Some backslashes should not be there. They are not expected by --transforms.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9564, "title": "No module named tensorflow (problem with python 2)", "body": "Hi,\r\n I'm trying to install tensorflow on this machine:\r\n\r\nUbuntu 16.04\r\ncuda 8.0\r\ncudnn 5.1\r\npython 2.7 (no Anaconda)\r\n\r\nI installed tensorflow on both python 3 and python 2. It works with python 3 but with python 2 always gives the import error: **No module named tensorflow**. \r\nWhat can I do to solve this?\r\n\r\nThanks", "comments": ["A few things\r\n * Could you show the exact commands you are running and their traceback?  Include the install and the check import? \r\n  * Are you using pip? Did you check the `pip list`. Did you check that the pip you are running corresponds to the python you are running.?\r\n  * Try virtualenv.\r\n  * Try what is listed here https://www.tensorflow.org/install/install_linux (under common install problems)", "Same problem as mine:\r\nThe pip and python do not correspond to each other.\r\nUsing \"python -m pip installl tensorflow\" for sure pip.", "These are the commands I used to install tensorflow with python 2:\r\n\r\n` sudo apt-get install python-virtualenv`\r\n` virtualenv --system-site-packages /media/hadi/tmpstore/tensorflow_vp2/`\r\n`source /media/hadi/tmpstore/tensorflow_vp2/bin/activate`\r\n`pip install --upgrade tensorflow-gpu`\r\n`deactivate`\r\n`source /media/hadi/tmpstore/tensorflow_vp2/bin/activate`\r\n`pip install ipython`\r\n\r\nNow I am trying to perform faster-RCNN for object detection based on github pages. But every time there is this error with python 2:\r\n\r\n> import tensorflow as tf\r\n\r\n> ImportError: No module named tensorflow\r\n\r\nIn the `pip list` there is line like this:\r\n\r\n> tensorflow-gpu (1.0.1)\r\n\r\n", "My problem was with `pip`. It always referred to python 3.I used `pip2 install --upgrade tensorflow-gpu` instead and it works now.", "Glad you got it working!", "I am also getting the same error while importing tensor-flow below are some details of my system.\r\n\r\n`deepakchawla35@deepak-server:~$ python -V\r\nPython 3.6.1 :: Anaconda 4.4.0 (64-bit)`\r\n\r\n`deepakchawla35@deepak-server:~$ pip -V\r\npip 9.0.1 from /home/deepakchawla35/anaconda3/lib/python3.6/site-packages (python 3.6)`\r\n \r\n`deepakchawla35@deepak-server:~$ pip3 show tensorflow\r\nName: tensorflow\r\nVersion: 1.3.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /home/deepakchawla35/.local/lib/python3.5/site-packages\r\nRequires: protobuf, six, tensorflow-tensorboard, wheel, numpy`\r\n\r\n`deepakchawla35@deepak-server:~$ python\r\nPython 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'`\r\n\r\nIn my system python and pip are of version 3.6 and tensorflow is install with python version 3.5 now what should I have to do to install it with python 3.6..?? ", "How can I install Keras after installing theano? I tried to install keras but after complitation when I am trying to import keras it shows me error: No module name tensorflow.\r\nI also changed the default backend from tensorflow to theano but still I am facing the same problem. Could you people please help me to fix it. I am working on Jetson TK1 Embedded Board and the board has some limitations of tensorflow. "]}, {"number": 9563, "title": "Add a simple BMP decoder and enable it on Android", "body": "Add a simple BMP decoder and enable it on Android so that we can test simple image classification in command line on Android platform. On Android, image decoding is not directly available to NDK C/C++ \r\nprograms. \r\n\r\n### Why bmp?\r\n.bmp can be decoded easily without relying on external libraries.\r\n", "comments": ["Can one of the admins verify this patch?", "I'm assuming that after @vrv 's suggested changes you won't need to modify tensorflow/core/BUILD. If you back out the tensorflow/core/BUILD changes, this is fine for API review.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "(Made a few changes to do validation of channels)", "(Also added use of internal::SubtleMustCopy).\r\n\r\n@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Can you run buildifier on the BUILD files so it passes the sanity checker?\r\nhttps://github.com/bazelbuild/buildtools", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n\r\nhopefully this works!", "This looks good, @rmlarsen for merging.", "Sorry for not merging this earlier. @freedomtan can you please rebase to resolve conflicts?", ":(.  Sorry @freedomtan.  There was an internal refactoring and I was hoping to get this change in first so that we'd have to do the refactoring, but looks like it didn't happen.\r\n\r\nThe context of the merge conflict is that all of the decode C++ implementations now are a single kernel (DecodeImage), because it was the case that too many people used decode_jpeg on images that were not jpegs; so now we have a single kernel that can decode all of them.  However, the python code still has separate python functions.  Let me know if you need any help making progress.", "@vrv @freedomtan Actually, please just resolve the merge conflicts in a simple way (they should be just a few lines in BUILD files?) and submit.  I can merge into the large op as a second pass.", "@vrv Thank you for lots of review. Is it possible to still keep bmp as a separate kernel. My original goal was to be able to have a simple image format for Android so that I can run something like label_image on Android without relying on external image libraries. Surely I can merge the bmp decoder into the single kernel if that's not possible.", "@freedomtan Yes, please leave it as a separate kernel.  One of us can do the further merge as a second pass.", "@girving OK. Thanks. Will do.", "@tensorflow-jenkins test this please", "rebased to resolve conflicts and test failures", "@tensorflow-jenkins test this please", "@freedomtan thanks for the contribution!", "FYI: We have to temporarily revert the changes to tensorflow/core/BUILD to avoid breaking several internal Android builds. \r\n\r\nI will ask somebody from @petewarden's team to take a look at how to re-enable it.", "@freedomtan Clarification: What I had to revert was the addition of whole_file_reader_op on Android, which I assume was just needed for a one-off test, and should not have been part of the original PR?", "@rmlarsen So what am I supposed to do? The whole_file_reader_op is used by label_image. It was added to make label_image work on Android. Surely you can remove whole_file_reader_op and I think label_image will still build without problem. Just label_image won't be able to find the whole_file_reader_op at run-time on Android. ", "Yes, we need to enable some file reader on Android and have label_image use that, but as far as I know whole_file_reader is not recommended. @josh11b @petewarden can you advice on what file reader to use? ", "@freedomtan feel free to open an issue and assign it to petewarden@ for triage.", "Instead of using a file reader, the example could use a placeholder in the graph instead of the file reader.  Then, pass the bytes of the image into the session.run call as a feed to that placeholder.   Those bytes can be obtained directly from available APIs (file loading or memmapping), or through the tensorflow API Env::Default).", "@cwhipkey Thanks. Will fiddle with that.", "@rmlarsen I created a patch to remove use of whole_file_reader_op, https://github.com/tensorflow/tensorflow/pull/10034. Follows what suggested by @cwhipkey."]}, {"number": 9562, "title": "Update beginners.md", "body": "Add wikipedia link for one-hot encoding, which does a better job of explanation then the following text.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!  I'm a googler, ldap: danielcar", "I signed it! I'm a googler, ldap: danielcar\n\nOn Sun, Apr 30, 2017 at 11:31 PM, googlebot <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9562#issuecomment-298293496>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACi92NFm-pUaKbiuQ0uef-AIaY2dgrE0ks5r1XxEgaJpZM4NM3Hc>\n> .\n>\n", "You need to set your git commit email to your google.com email @DanCard .  See git commit --amend for instructions on how to do so.", "(Talked offline, going to send another PR since this was done via web interface)."]}, {"number": 9561, "title": "Add two unit tests for the tensorboard.", "body": "It checks the number of scalars should be returned.\r\n```\r\ntestScalars()\r\ntestScalarsCsv()\r\n```", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please\r\n\r\nThanks!"]}, {"number": 9560, "title": "tf.pow(x, y) will freeze for negative integer y", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, and macOS 10.12.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0 on both operation systems\r\n- **Bazel version (if compiling from source)**: not compiled from source\r\n- **CUDA/cuDNN version**: CPU only on Windows, while CUDA 8.0 on macOS\r\n- **GPU model and memory**: Nvidia Titan X, 12GB\r\n- **Exact command to reproduce**: a short piece of Python code\r\n- **Python version**: 3.5.2 (Anaconda) on Windows, 3.6.0 (Anaconda) on macOS\r\n\r\n### Describe the problem\r\n\r\ntf.pow(x, y) will freeze for negative integer y (and of course, integer x).  It will not freeze for negative floating number y.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default(), tf.Session().as_default():\r\n    print(tf.pow(5, -2).eval())  # this will not stop\r\n```\r\n", "comments": ["I've confirmed and reproduced this, thanks for reporting.", "I think this issue is related to issue #12156. Also related to PR #11852.", "I've looked in the source code and the origin of this bug is in Eigen, specifically src/Core/MathFunctions.h. They specialize their template for pow to the case of both parameters integers, and provide an implementation which only works for positive y. The problem is the well-known bug-loop of shifting y to the left: if y < 0, then the bit sign extends and it continues indefinitely.\r\n\r\nHere is the code from Eigen:\r\n\r\n```\r\ntemplate<typename ScalarX,typename ScalarY>\r\nstruct pow_impl<ScalarX,ScalarY, true>\r\n{\r\n  typedef ScalarX result_type;\r\n  static EIGEN_DEVICE_FUNC inline ScalarX run(ScalarX x, ScalarY y)\r\n  {\r\n    ScalarX res(1);\r\n    eigen_assert(!NumTraits<ScalarY>::IsSigned || y >= 0);\r\n    if(y & 1) res *= x;\r\n    y >>= 1;\r\n    while(y)\r\n    {\r\n      x *= x;\r\n      if(y&1) res *= x;\r\n      y >>= 1;\r\n    }\r\n    return res;\r\n  }\r\n};\r\n```\r\n\r\n\r\nHowever, it would be difficult to fix this: the template specifically requests that the return type is Scalar, and not Real, and changing this in Eigen might break things for other people.\r\n\r\nWhat I could do, would be to change pow in cwise_ops.h in tensorflow/core to fix this bug. But then again, tf.pow would start returning float for integers inputs. Is this acceptable? I don't see any way to fix the bug without accepting that tf.pow should also return float for integer inputs.\r\n\r\nIf you agree with this change, I can fix the issue.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "PR #15607 is pending review. There are discussions about fixing it in Eigen vs. TensorFlow. Will take a look at Eigen.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@korepwx I think the issue is addressed in PR #15607 and this issues could be closed. "]}, {"number": 9559, "title": "no such package 'tensorflow/examples/image_retraining': BUILD file not found on package path.", "body": "### Describe the problem\r\nI use ubuntu, and install anaconda3 and tensorflow followed the instruction. After it, I am trying to learn how to retrain a model that follows the inception tutorial. It seems that I got the following errors\r\n\r\nERROR: no such package 'tensorflow/examples/image_retraining': BUILD file not found on package path.\r\n\r\nIs there a way to solve this issue?\r\n\r\n### Source code / logs\r\n\r\nparallels@ubuntu:~/anaconda3/lib/python3.6/site-packages$ bazel build tensorflow/examples/image_retraining:retrain\r\nERROR: no such package 'tensorflow/examples/image_retraining': BUILD file not found on package path.\r\nINFO: Elapsed time: 0.073s\r\n\r\n", "comments": ["It's helpful to provide the link to the instructions are following. However, I think your problem is that you need to download the source, not install tensorflow binary.", "Hi, \r\n\r\nFrom the path, /anaconda3/lib/python3.6/site-packages, it looks like you're trying to launch that `bazel build` from the tensorflow pip-installation. \r\n\r\n(Almost) none of the examples, and none of the BUILD files are included in the pip installation.\r\n\r\nFor that bazel command to work you need to launch it from a git clone of the tensorflow repository. That should work.\r\n\r\nGood luck."]}, {"number": 9558, "title": "https://github.com/tensorflow/tensorflow/issues/9553", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 9557, "title": "Wrong path for absolute variable scopes", "body": "Using absolute paths for scopes is useful when structuring large models.\r\n\r\nWe can enter a name scope from its absolute path by appending a slash:\r\n\r\n```python\r\nwith tf.name_scope('foo'):\r\n  with tf.name_scope('bar/') as scope:\r\n    print(tf.constant(0).name)  # bar/Const:0\r\n    print(bar)  # bar\r\n```\r\n\r\nHowever, this does not work with variable scopes:\r\n\r\n```python\r\nwith tf.variable_scope('foo'):\r\n  with tf.variable_scope('bar/') as bar:\r\n    print(tf.constant(0).name)  # bar/Const:0\r\n    print(bar.name)  # foo/bar/ (Expected: bar)\r\n```\r\n\r\nThe last line should print `bar` instead of `foo/bar/`.", "comments": ["@mrry, i'm not a scope expert, but it seems in `ops.py` there is `name_from_scope_name` that \r\nchecks for a trailing slash i.e. at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1883\r\n\r\nbut and I don't see any similar function for variable_scope. Perhaps this is intended for some good reason?\r\n", "If there's a good reason, @lukaszkaiser would be the one who knows it.", "It was a hack in name scopes, and it was left out of variable scopes on purpose. We managed to go without hacking-in absolute paths for a long time, and their lack has great advantages: it's much harder to make a model inaccessible as sub-model. If you use an absolute path, then your model immediately becomes unusable as a sub-model of anything. So it was a conscious decision and it looks like the right one for now - a little inconvenience at build time paid for by much easier maintenance. I'd advocate to keep it that way except if there is a very good reason not to."]}, {"number": 9556, "title": "Fix link in installation docs for OSX", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9555, "title": "Added output information with path to  wrong JPEG file", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I switched this to raising a RuntimeError instead of killing the program.  If lint passes, this seems okay to me."]}, {"number": 9554, "title": "TFRecord Writer Bug Saving Float List", "body": "OS: Linux Ubuntu 14.04\r\nTF: Version 1.1 (CPU) python 2.7\r\n\r\nWhen saving a list of floats to TFRecords format with \r\n`tf.train.Feature(float_list=tf.train.FloatList(value=value) \r\n`\r\n,where value it list of floats converted from a numpy array using tolist(), I get the following error:\r\n\r\nTypeError: [0.0, 41.95294952392578, 1.4004319906234741, 46.5711784362793, 33.32099914550781, 1.545793056488037, has type list, but expected one of: int, long, float\r\n\r\nwhich surprises me because it is explicitly a list of floats. \r\n\r\n```\r\ndef _int64_feature(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n\r\ndef _float32_feature(value):\r\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\r\n\r\ndef _bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n \r\n\r\n    with tf.Session() as sess:\r\n        # initialize variables\r\n        sess.run(init_op)\r\n        # Input threading\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n        print('Writing', filename)\r\n        recordWriter = tf.python_io.TFRecordWriter(filename)\r\n\r\n        try:\r\n            while not coord.should_stop():\r\n\r\n                img, latent = sess.run([img_op, latent_op])\r\n\r\n                example = tf.train.Example(features=tf.train.Features(feature={\r\n                    'image/width': _int64_feature(image_dims[0]),\r\n                    'image/height': _int64_feature(image_dims[1]),\r\n                    'image/channels': _int64_feature(image_dims[2]),\r\n                    'image/encoded': _bytes_feature(img[0]),\r\n                    'latent/size': _int64_feature(latent_dim),\r\n                    'latent/data': _float32_feature(latent.tolist())}))\r\n                recordWriter.write(example.SerializeToString())\r\n\r\n        except tf.errors.OutOfRangeError:\r\n            print('Done training -- epoch limit reached')\r\n\r\n        finally:\r\n            # When done, ask the threads to stop.\r\n            coord.request_stop()\r\n\r\n        recordWriter.close()\r\n        coord.join(threads)\r\n        sess.close()\r\n```", "comments": ["I think the issue maybe due to using the protobuf version that can be downloaded through tensorflow.", "OS: Linux Ubuntu 16.04\r\nTF: Version 1.1 (GPU) python 3.6 (Using Docker)\r\n\r\nI am having the exact same issue here but with an **Int64List**. I am trying to pass a list [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] without any success. I am getting the following error:\r\n\r\n> TypeError: [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] has type list, but expected one of: int, long", "I think this is related to the version of protobuf because I was able to get it to work on a different machine.", "@gdj0nes What version of protobuf worked for you?", "Turns out that isn't the issue. I have google.protobuf version 3.2 on both computers.", "Ok I found the solution to be \r\n[float(x) for x in array[0]]", "that is a great idea....@gdjOnes", "@gdj0nes Can you please tell what is `array[0]` in your mentioned fix ??\r\n\r\nIf `array[0]` is a scalar value, what is the need to put it inside a list comprehension ??", "Have exact the same problem, would like to know what should I do if I was to save a high-dimension array into float list.", "flatten or reshape before writing and after reading reshape back to N dimensions.", "Yeah this is not the solution.\r\n```\r\n    @staticmethod\r\n    def _int64_features(value):\r\n        return tf.train.FeatureList(feature=tf.train.Int64List(value=[i for i in value]))\r\n```\r\nWhere value is a list of floats, the following error still occurs.\r\nTypeError: Value must be iterable", "Don't use a list. \r\n\r\ndef _float32_feature(value):\r\n  return tf.train.Feature(float_list = tf.train.FloatList(value=value))", "Same problem~~my `token_ids`=`[[3842, 11658], [3080, 3541], [1557, 6895], [6913, 6802], [11407, 10301, 10301]]`,  when I  write it like that: `feature[\"question_token_ids\"] = tf.train.Feature(int64_list=                                                             tf.train.Int64List(value=token_ids))`.  The error comes:`TypeError: [3842, 11658] has type list, but expected one of: int, long`\u3002How to solve this problem? ", "> OS: Linux Ubuntu 16.04\r\n> TF: Version 1.1 (GPU) python 3.6 (Using Docker)\r\n> \r\n> I am having the exact same issue here but with an **Int64List**. I am trying to pass a list [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] without any success. I am getting the following error:\r\n> \r\n> > TypeError: [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] has type list, but expected one of: int, long\r\n\r\nHi, what is your finally  solution? "]}, {"number": 9553, "title": "configure does not work on macOS if sed is GNU sed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.4\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: Current head: 3ce228e\r\n\r\n### Describe the problem\r\nIf sed is GNU sed on macOS, configure fails with `sed: can't read : No such file or directory`.\r\n", "comments": ["Can you isolate the command that caused this? How did you switch to GNU sed?", "Is this the same issue as addressed in https://github.com/tensorflow/tensorflow/pull/8935?", "I installed GNU sed through homebrew and added the sed command on the PATH without the g-prefix. I prefer GNU sed over the BSD sed that macOS comes preinstalled with. \r\n\r\nThe command that cause the error:\r\n```\r\nsed_hyphen_i -e 's/WITH_JEMALLOC = True/WITH_JEMALLOC = False/' tensorflow/core/platform/default/build_config.bzl\r\n```\r\n\r\nThis is because this function only checks if the command is running on macOS and then assumes the sed that it is running is BSD sed.\r\n\r\nThis is not the same as #8935 because that issue just addresses that most of the time sed on macOS is BSD sed, but doesn't address that sometimes it can also be GNU sed.", "So what in this command is it that GNU sed doesn't like?", "Or, it is this\r\n```\r\nsed -i '' -e 's/WITH_JEMALLOC = True/WITH_JEMALLOC = False/' tensorflow/core/platform/default/build_config.bzl\r\n```\r\nThis stackexhange link explains the problem https://unix.stackexchange.com/questions/92895/how-to-achieve-portability-with-sed-i-in-place-editing", "So it looks like GNU sed and BSD sed's implementation of -i are\nfundamentally incompatible?\u200b Can you remove the space between -i and '' and\ncheck if that works? I have no GNU sed installed.\n", "Removing the space does seem to work with both GNU and BSD sed.", "@ljos Yea, sed isn't very portable - and the `-i` flag isn't even part of the [POSIX spec](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/sed.html). This PR should fix the issue https://github.com/tensorflow/tensorflow/pull/8935 for others.", "@ljos I'm pretty sure that the new version of #8935 should fix the issue for everybody. Can you confirm?", "The new version works for me. Thanks!"]}, {"number": 9552, "title": "Adding a property `trainable` to `tf.Variable`", "body": "Feature request: wouldn't be handy to have a property `trainable` in the class `tf.Variable`?\r\n\r\nP.S. sorry in advance for the question but... am I supposed/entitled to sketch an implementation in my TF fork and then issue a pull request?", "comments": ["TensorFlow tf.Variable objects are not specific to neural networks, so baking \"traininable\" property in there seems wrong. There's `tf.trainable_variables()` and `tf.add_to_collection(tf.GraphKeys.TRAINABLE_VARIABLES)`", "Hi @yaroslavvb, thanks for the reply. Yes, it makes sense. I will close the issue (and find a smart way to solve my problem using what is already there)."]}, {"number": 9551, "title": "Delay Compensated Asynchronous Stochastic Gradient Descent", "body": "This adds the feature requested by #8744 ", "comments": ["Can one of the admins verify this patch?", "Thanks for this change!\r\n\r\n- There are very few comments explaining what's going on.  Because this is a variation of a standard algorithm, it probably needs some more comments explaining what the equations are.\r\n\r\n- This is going to fail our linter because the lines are too long, indentation is not correct, etc.  See https://github.com/tensorflow/tensorflow/issues/7443#issuecomment-279182613 for what needs to be done!\r\n\r\n- There are no tests; we need tests to make sure this is working correctly from a unit-testing perspective.\r\n\r\n- People would want proof that this implementation can be used to validate the results of the paper -- have you done any validation yourself?  If so, can you provide some reproduction steps?  For example, having something like in the mxnet pull request linked from the issue will be useful to show: 1) that it is working, 2) that it actually helps.\r\n\r\nThanks again!", "ping for @alisidd !", "@vrv Hi, I'm still working on it, I should hopefully have a working version by tomorrow.", "Can one of the admins verify this patch?", "@vrv I've made the requested changes, and I found quite a few problems with my implementations that required a few changes that I wasn't sure if I should be making, like an addition of an optional parameter to a method in the Optimizer class. It allows for multithreaded processing using one optimizer object. Regardless, I'm sure you can see what else I had to change. \r\n\r\nFurthermore, I tested and validated the implementation using a modified version of mnist_softmax.py with 12 asynchronous workers, and a variance parameter of 5. I found there to be an improvement of around 1.5% in test accuracy over asynchronous stochastic gradient descent, similar to improvements found in the research paper.", "Thanks for making these changes -- though I think changing the public API of optimizers just for this one case suggests that the Optimizer interface is now doing too much.\r\n\r\nI'm not too familiar with the optimizer interface personally, but one thought: the idea of delay compensated updates could theoretically apply to any gradient descent algorithm; instead of having this version descend from Optimizer, does it make sense to:\r\n\r\n1) Subclass Optimizer to be a DelayCompensatedOptimizer, where you add your specific modifications based on worker_index \r\n\r\n2) Make GradientDescentDC descend from it?\r\n\r\nLet me know what you think!\r\n\r\n(Also, I'd probably be verbose on the names here: DelayCompensatedGradientDescent; the term \"DC\" isn't standard enough for it to be abbreviated).\r\n", "Also there aren't any tests that actually exercise more than 1 worker, so it's hard to tell the context in which it's needed.\r\n\r\n@jhseu in case he has any thoughts on optimizer interface", "@vrv A whole class for adding the ability to address updates by the worker index would have usage if more algorithms were added similar to this one that depended on having that kind of information. I need that information because I need to store a separate var for each worker, and keep them separate throughout the optimization. I don't think most gradient descent algorithms need that kind of information though.", "(I've assigned this the API review label so that the folks who look at API changes can assess whether this is something to accept.)", "@vrv I can also copy all relevant code to the delay_compensated_gradient_descent.py file and override the relevant methods if this is too drastic a change to the Optimizer class.", "It would certainly avoid an API review if you did this.  In addition, it might also make sense for this to be in contrib/opt like some other optimizers, so that we can evolve the delay compensated optimizers further.  All code in core has API stability guarantees that will far outlast the initial maintainers :)", "In terms of API review, +1 to the last two suggestions:\r\n\r\n- Put this optimizer in contrib\r\n- Avoid changing the Optimizer interface and override the relevant methods in the subclass\r\n", "@vrv I've made the required changes.", "@tensorflow-jenkins test this please", "@alisidd sorry it looks like you need to rebase again.", "@rmlarsen Should be good now", "No need to change the name!  @tensorflow-jenkins test this please", "#10055 seems to have had the same problem when it was approved, not sure if I'm supposed to do something.", "@tensorflow-jenkins test this please\r\n\r\n(it looks like no GPU kernels are registered, but perhaps that can be added as a future PR if someone wants.  it's pretty straightforward to copy the existing code for other optimizers).", "@alisidd @vrv \r\nI add supporting GPU kernels base your code, when I execute a test,it runs on CPU normally, but it occurs an error  on GPU\r\n\r\n     python tensorflow/tensorflow/examples/tutorials/mnist/mnist_softmax_dc_asgd.py\r\n\r\nthe error is:\r\n\r\n      **_2017-06-28 10:58:29.296340: E tensorflow/core/common_runtime/executor.cc:641] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: w/DelayCompensatedGradientDescent/_23 = _HostRecv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_103_w/DelayCompensatedGradientDescent\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^w/DelayCompensatedGradientDescent/_22, ^DelayCompensatedGradientDescent/learning_rate, ^gradients/MatMul_grad/tuple/control_dependency_1, ^DelayCompensatedGradientDescent/lambda); Op<name=_HostRecv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n\t [[Node: w/DelayCompensatedGradientDescent/_23 = _HostRecv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_103_w/DelayCompensatedGradientDescent\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](^w/DelayCompensatedGradientDescent/_22, ^DelayCompensatedGradientDescent/learning_rate, ^gradients/MatMul_grad/tuple/control_dependency_1, ^DelayCompensatedGradientDescent/lambda)]]_**\r\n\r\n\r\nthe test code is here : \r\n     https://github.com/zhuhyc/tensorflow/blob/dc-asgd/tensorflow/examples/tutorials/mnist/mnist_softmax_dc_asgd.py\r\n\r\nthe patch is here: \r\n     https://github.com/zhuhyc/tensorflow/blob/dc-asgd/dc-asgd.patch\r\ncan you give me some suggestions to reslove this error?\r\n", "I'm trying to learn more dcsgd and external optimizers for tf, I'm unsure how to implement this feature with the tf learn library examples. \r\n\r\n@zhuhyc \r\nDoes your testcode work for CPUs? When I run the softmax example on docker nightly CPU build, I get the following result:\r\n\r\n $ python mnist_softmax_dc_asgd.py\r\n\r\n> Traceback (most recent call last):\r\n  File \"mnist_softmax_dc_asgd.py\", line 84, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"mnist_softmax_dc_asgd.py\", line 64, in main\r\n    train_step = optimizer.minimize(cross_entropy,worker_index=0)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/delay_compensated_gradient_descent.py\", line 144, in minimize\r\n    name=name, worker_index=worker_index)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/delay_compensated_gradient_descent.py\", line 217, in apply_gradients\r\n    worker_index))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/delay_compensated_gradient_descent.py\", line 34, in update_op_asynchronous\r\n    return optimizer._apply_dense(g, self._v, index)\r\nTypeError: _apply_dense() takes exactly 3 arguments (4 given)\r\n\r\n> ", "@sesamesub  \r\n    Yes, it works for on CPUs correctly. My tf version is master branch and have applyed dc-asgd.patch\r\n    the patch is here:\r\n        https://github.com/zhuhyc/tensorflow/blob/dc-asgd/dc-asgd.patch\r\n    It looks like you don't apply the patch, please apply it and test again\r\n    ", "I had some issues on my machine which I fixed, your code/patch is fine for\nCPU tensorflow and works for per your example code.\n\nOn Wed, Jul 5, 2017 at 10:21 PM, FANG Yang <notifications@github.com> wrote:\n\n> ping to check status\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9551#issuecomment-313284643>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVfXA1oxoqzHcrjF81KAVGr1y-WsuxyNks5sLFLVgaJpZM4NMkM0>\n> .\n>\n", "Ping to check status @alisidd", "It looks like this PR was removed from master (commit 47949be1aae1d7c97f75712e0655da96fab90208), with the message \"Remove Delay Compensated Asynchronous Stochastic Gradient Descent (DCASGD). The implementation is broken in its current form.\"\r\n\r\nWhat was wrong with the implementation? The message only references this PR and issue #8744, none of which seem to be mentioning problems.", "IIRC, it was lacking an _apply_dense() implementation, there were typos that caused failures when used, etc.\r\n\r\nThe commit is obviously in the history for somebody to re-use and fix, if they wanted, but they'd need to write more tests and probably refactor it to make it something that the team could accept to TensorFlow.\r\n\r\n"]}, {"number": 9550, "title": "ctc_greedy_decoder inconsistent with ctc_beam_search_decoder", "body": "The following extract from the the `ctc_beam_search_decoder` documentation seems to be misleading:\r\n\"The `ctc_greedy_decoder` is a special case of the `ctc_beam_search_decoder` with `top_paths=1` and `beam_width=1` (but that decoder is faster for this special case).\"\r\n\r\nInstead, the following results can be observed: \r\n\r\n| Decoding \"AA<ctc_blank>AA\" using | `merge_repeated=True` | `merge_repeated=False` |\r\n| --- | --- | --- |\r\n| `tf.nn.ctc_beam_search_decoder(top_paths=1, beam_width=1)` | \"A\"   | \"AA\" |\r\n| `tf.nn.ctc_greedy_decoder()` | \"AA\" | \"AAAA\" |\r\n\r\nTo reproduce:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom unittest import TestCase\r\n\r\n\r\nclass CtcDecodersTest(TestCase):\r\n    def test(self):\r\n        def decode_greedily(beam_search: bool, merge_repeated: bool):\r\n            aa_ctc_blank_aa_logits = tf.constant(np.array([[[1.0, 0.0]], [[1.0, 0.0]], [[0.0, 1.0]],\r\n                                                    [[1.0, 0.0]], [[1.0, 0.0]]], dtype=np.float32))\r\n            sequence_length = tf.constant(np.array([5], dtype=np.int32))\r\n\r\n            (decoded_list,), log_probabilities = \\\r\n                tf.nn.ctc_beam_search_decoder(inputs=aa_ctc_blank_aa_logits,\r\n                                              sequence_length=sequence_length,\r\n                                              merge_repeated=merge_repeated,\r\n                                              beam_width=1) \\\r\n                    if beam_search else \\\r\n                    tf.nn.ctc_greedy_decoder(inputs=aa_ctc_blank_aa_logits,\r\n                                             sequence_length=sequence_length,\r\n                                             merge_repeated=merge_repeated)\r\n\r\n            return list(tf.Session().run(tf.sparse_tensor_to_dense(decoded_list)[0]))\r\n\r\n        self.assertEqual([0], decode_greedily(beam_search=True, merge_repeated=True))\r\n        self.assertEqual([0, 0], decode_greedily(beam_search=True, merge_repeated=False))\r\n        self.assertEqual([0, 0], decode_greedily(beam_search=False, merge_repeated=True))\r\n        self.assertEqual([0, 0, 0, 0], decode_greedily(beam_search=False, merge_repeated=False))\r\n```\r\n\r\nThis is confusing and probably not intended.\r\n\r\nHow to solve this:\r\n\r\n- Adapt the documentation or\r\n- Adapt the `ctc_beam_search_decoder` implementation to that of `ctc_greedy_decoder` or vice versa. Both directions would cover my use case (\"AA\"), this decision would depend on which of the other behaviors (\"A\" or \"AAAA\") is needed and which of them could be dropped.\r\n\r\n### System information\r\nOSX 12.4, TensorFlow 1.1.0 CPU from binary", "comments": ["I stumbled upon this today, too. Why are the results not fully equivalent when ctc_beam_search_decoder uses top_paths=1 and beam_size=1?", "Ping!\r\n@ebrevdo any comments?\r\nI know there are a lot of issues assigned to you, so could you reassign this one who may have time to investigate this?", "Also encounter this problem as well.\r\nAnd test with this data set:\r\nLogits: [ [[200.0,1.0]] , [[-100.0,200.0]] , [[200.0,1.0]] ]\r\nWhich should give 'AA',\r\nThe greedy decoder is correct, but the beam search decoder only give 'A'\r\n\r\nAnd the beam search deocder work on this dataset:\r\n[ [[log(0.4),log(0.6)]] , [[log(0.4),log(0.6)]]  ]\r\nGreedy decoder gives an empty output, where the beam search decoder gives the true output  'A'.\r\nWhich is known would fail in best path greedy decoding.\r\n\r\nBut they both fail on this dataset:\r\n[ [[log(0.4),log(0.6)]] , [[log(0.4),log(0.6)]], [[-200.0,200.0]] , [[200.0,-200.0]] ]\r\nWhere the true output is 'AA', but both decoders give 'A'.\r\nBut I guess they fail in the different place, beam_search decoder seems unable to give the correct answer for 'AbA'-type path, where greedy decoder failed in the first two logits.\r\n\r\n\r\nTo reproduce:\r\n```\r\nlogits = tf.Variable(initial_value = [[[np.log(0.4),np.log(0.6)]],[[np.log(0.4),np.log(0.6)]],[[-200.0,200.0]],[[200.0,-200.0]]])\r\noutput= tf.nn.ctc_beam_search_decoder(inputs = logits,sequence_length=[4],top_paths = 3,beam_width = 500)\r\ngreedy_output = tf.nn.ctc_greedy_decoder(inputs = logits,sequence_length = [4])\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ndecode_val = sess.run(output)\r\ngreedy_val = sess.run(greedy_output)\r\nprint decode_val,'\\n',greedy_val\r\n```\r\n\r\n##System information\r\nUbuntu 14.04, Tensorflow 1.3.0 CPU from pip install.\r\n\r\n", "same issue and look forward to solving that", "any updates? having the same issue as well.", "I spent a good fraction of the day debugging this before coming to a somewhat obvious realization. If you read the Graves paper, CTC beam decoding implicitly collapses repeated characters as part of calculating the optimal path (i.e. 'AAA' will contribute probability mass through the path 'A'). \r\n\r\nSo the correct CTC decoding behavior occurs when `merge_repeated=False`. In this case, it DOES merge repeated characters. The `merge_repeated` flag, when true, will merge repeated characters after characters have already been merged/blank symbols removed. This parameter should simply be removed. As it stands now, the behavior is _extremely_ misleading.\r\n\r\nI'm happy to provide a PR for this change if it's something a tensorflower would be amenable with. cc @ebrevdo @gunan ", "Let's use the example from above: `\"AA<ctc_blank>AA\"`\r\n\r\nExpected behavior:\r\n\r\nwhen `merge_repeated=False`, `\"AA<ctc_blank>AA\"` should give `AAAA`\r\nwhen `merge_repeated=True`, `\"AA<ctc_blank>AA\"` should give `AA`\r\n\r\nOnly `tf.nn.ctc_greedy_decoder()` is working as expected, but `tf.nn.ctc_beam_search_decoder(top_paths=1, beam_width=1)` is not. \r\n\r\n@ryanleary are you saying `tf.nn.ctc_beam_search_decoder(top_paths=1, beam_width=1)` merged symbols twice? I think I might be able to help out in amending the decoder.", "I agree that `merge_repeated=True` should give `AA` is the correct behavior. What I'm saying is that having a case where `merge_repeated=False` in a beam decoder doesn't make a whole lot of sense. Further, the merging is somewhat fundamental to the way the algorithm handles summing probabilities across multiple paths.\r\n\r\nIn the current implementation of the beam decoder, if `merge_repeated=False`, repeated characters are merged once. If `merge_repeated=True`, they are merged *again*, returning nonsensical results.\r\n\r\nI'm advocating for removing the flag, and the *only* behavior to be where repeated characters are merged (but equivalent to the `merge_repeated=False` current functionality).", "following up on PR", "Since this is a backwards breaking API change, I'll remove the argument for TF 2.0.  Thank you all for the report & detailed analysis.", "Nagging Assignee @ebrevdo: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by #21187."]}]