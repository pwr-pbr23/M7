[{"number": 2623, "title": "Add paper-dialog to TensorBoard's index.html imports", "body": "Warning! Warning! Warning! Your totally-automated TensorBoard Testing Service\u00ae has detected an issue with TensorBoard that would make users sad upon the next release. \n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Since I apparently haven't signed a CLA and the build instantly went red, I decided to submit this change through a different channel.\n"]}, {"number": 2622, "title": "Work on strings instead of binary in Python3", "body": "Running word2vec_basic on Python3 gives silly byte-ish output since it reads the sentences as binary:\n\n```\n$ python3 word2vec_basic.py \nFound and verified text8.zip\nData size 17005207\nMost common words (+UNK) [['UNK', 418391], (b'the', 1061396), (b'of', 593677), (b'and', 416629), (b'one', 411764)]\nSample data [5237, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] [b'anarchism', b'originated', b'as', b'a', b'term', b'of', b'abuse', b'first', b'used', b'against']\n3084 b'originated' -> 12 b'as'\n3084 b'originated' -> 5237 b'anarchism'\n12 b'as' -> 3084 b'originated'\n12 b'as' -> 6 b'a'\n6 b'a' -> 12 b'as'\n6 b'a' -> 195 b'term'\n195 b'term' -> 2 b'of'\n195 b'term' -> 6 b'a'\n```\n\n... etc.\n\nThis patch uses the 'compat' to_str() as a sane way to read the text in either Pyhton 2.x or 3.x.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2621, "title": "gpu support error(sampled_loss)", "body": "I'm now a little bit hesitated that I have found a similar issue before, however I can't find it now\n\nin the file seq2seq_model.py in translate foloder, I use    with device(\"/gpu:0')     instead of cpu, there's an error like this : \n\n``` javascript\nInvalidArgumentError                      Traceback (most recent call last)\n<ipython-input-10-84f5fbeb6a90> in <module>()\n      1 sess = tf.InteractiveSession()\n----> 2 nmt.train(sess, train_data, valid_data, EVALUATE_PER)\n\n<ipython-input-6-ca1988aa7469> in train(self, sess, train_data, valid_data, evaluate_per)\n    190 \n    191             _, cost = self.step(sess, train_encoder_batches[batch_i], train_decoder_batches[batch_i], \n--> 192                                train_target_batches[batch_i], train_target_weight_batches[batch_i], self.train_flag)\n    193 \n    194             # if the loss is equal to NaN(not a number), raise error\n\n<ipython-input-6-ca1988aa7469> in step(self, sess, encoder_data, decoder_data, target_data, tw_data, train_flag)\n    122                 output_feed.append(self.final_outputs[idx])\n    123 \n--> 124         output = sess.run(output_feed, input_feed)\n    125         return output\n    126 \n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    313         `Tensor` that doesn't exist.\n    314     \"\"\"\n--> 315     return self._run(None, fetches, feed_dict)\n    316 \n    317   def partial_run(self, handle, fetches, feed_dict=None):\n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict)\n    509     # Run request and get response.\n    510     results = self._do_run(handle, target_list, unique_fetches,\n--> 511                            feed_dict_string)\n    512 \n    513     # User may have fetched the same tensor multiple times, but we\n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict)\n    562     if handle is None:\n    563       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--> 564                            target_list)\n    565     else:\n    566       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n    584         # pylint: disable=protected-access\n    585         raise errors._make_specific_exception(node_def, op, error_message,\n--> 586                                               e.code)\n    587         # pylint: enable=protected-access\n    588       six.reraise(e_type, e_value, e_traceback)\n\nInvalidArgumentError: Cannot assign a device to node 'sequence_loss_by_example/sampled_softmax_loss_39/embedding_lookup_1': Could not satisfy explicit device specification '/device:GPU:0'\n     [[Node: sequence_loss_by_example/sampled_softmax_loss_39/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=\"/device:GPU:0\"](proj_b/read, sequence_loss_by_example/sampled_softmax_loss_39/concat)]]\nCaused by op u'sequence_loss_by_example/sampled_softmax_loss_39/embedding_lookup_1', defined at:\n  File \"/opt/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/opt/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/opt/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 403, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 151, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-bfcd832d993d>\", line 2, in <module>\n    BATCH_SIZE, NUM_SAMPLES, MAX_GRADIENT_NORM, LEARNING_RATE, decay_factor = LEARNING_RATE_DECAY_FACTOR)\n  File \"<ipython-input-6-ca1988aa7469>\", line 88, in __init__\n    self.loss_sequence = seq2seq.sequence_loss_by_example(self.outputs, self.targets, self.tw, self.target_voca, softmax_loss_function = softmax_loss_function)\n  File \"seq2seq_mod070_ver5.py\", line 974, in sequence_loss_by_example\n    crossent = softmax_loss_function(logit, target)\n  File \"<ipython-input-6-ca1988aa7469>\", line 66, in sampled_loss\n    self.target_voca)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 865, in sampled_softmax_loss\n    name=name)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 661, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 86, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 423, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n```\n\nwhat's the problem with this?\n\nI heard that there are many device related things modified in release 0.8.0 just because of many of ops are coded with cuda(?)  then why not this part, I mean sampled_loss part in seq2seq_model.py\n", "comments": ["Could you please include exact instructions on how you are running this (command line usage) so I can reproduce the problem. If you click \"Create New Issue\" you can find the template of what information we need and how to gather it. It is impossible for us to help diagnose without that information. Thank you.\n", "Actually the problem occured when I run my own code which is very similar to translate.py + seq2seq_model.py. Im now using ver 0.7.1 . I found that there is no error when i change all of the \"/cpu:0\" to \"/gpu:0\" (please refer to below code) from seq2seq_model.py but my own code . with my code when they are not changed, code runs well, but when changed, outputs the error (error above)\n\n``` javascript\nif num_samples>0 and num_samples<self.target_vocab_size:\nwith tf.device(\"/cpu:0\"):\nw = tf.get_variable(\"proj_w\", ..........\n.......\ndef sampled_loss(inputs, labels):\nwith tf.device(\"/cpu:0\"):\n...\n\n```\n\ncan you explain what does the error exactly mean?\nand I have one more question is that : even it works with those tf.device(\"/gpu:0\"), why there are still using tf.device(\"/cpu:0\")  (files in the path tensorflow/python/ops ), any other reason?\n\nThanks\n", "At this point I'd recommend trying things on 0.9 since 0.7.1 is quite old at this point. If you want further help, a complete reproducible test case would be necessary.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2620, "title": "cifar10 gpu core dump", "body": "$python cifar10_train.py --data_dir=/home/lli/cifar10_data\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:04:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 1 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:05:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 2 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:08:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 3 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:09:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nSegmentation fault (core dumped)\n", "comments": ["I am unable to reproduce this on my machine using the tensorflow master branch branch. I am using CUDA 7.0 with cuDNN 4.0 however. So perhaps try an older cudnn and let me know if that works for you.\n\n```\n$ python cifar10_train.py --data_dir=./cifar-data\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Graphics Device\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28fc190\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Quadro K620\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:03:00.0\nTotal memory: 2.00GiB\nFree memory: 1.04GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Graphics Device, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:782] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro K620, pci bus id: 0000:03:00.0)\n\n2016-06-02 14:22:38.249729: step 0, loss = 4.68 (6.7 examples/sec; 19.180 sec/batch)\n2016-06-02 14:22:39.837789: step 10, loss = 4.66 (1339.4 examples/sec; 0.096 sec/batch)\n2016-06-02 14:22:40.867866: step 20, loss = 4.64 (1101.5 examples/sec; 0.116 sec/batch)\n2016-06-02 14:22:41.923386: step 30, loss = 4.63 (1120.5 examples/sec; 0.114 sec/batch)\n2016-06-02 14:22:42.924298: step 40, loss = 4.60 (1186.7 examples/sec; 0.108 sec/batch)\n```\n", "Also, click \"Create Issue\" and look at the template of what information we need. Without that information it is very difficult for us to help you, because installation situation has many variables. Thank  you.\n"]}, {"number": 2619, "title": "tf.image.decode_png returns wrong values for uint16 images", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: CUDA 7.5, cuDNN R4\n\nIf installed from sources, provide the commit hash: d8eb4bb6470d4cb3d0f67f2111a39fa50f1c28e5\n### Steps to reproduce\n\nPlease see the below code. After saving numpy array as uint16 png image, I loaded it using tf.image.decode_png. It returns different values from the original array. \n\n``` python\nimport numpy as np\nimport tensorflow as tf\nfrom skimage import io\n\noriginal = np.array([[1,4096],[15000,30000]],dtype=np.uint16)\nio.imsave('test.png',original)\n\nsk_im = io.imread('test.png')\n\nimage = tf.image.decode_png(tf.read_file('test.png'),dtype=tf.uint16)\nsess = tf.Session()\ntf_im = sess.run(image)\n\nprint sk_im\nprint tf_im\n```\n\nResults are\n\n```\nsk_im = [[1, 4096], [15000, 30000]] #same as original\ntf_im[:,:,0] = [[256, 16], [38970, 12405]]\n```\n\nI checked the function works properly in case of uint8 png image.\n", "comments": ["What architecture are you running on (little endian or big endian)?\n", "It is little endian (x86 processors).\n\n> $ lscpu\n> Architecture:          x86_64\n> CPU op-mode(s):        32-bit, 64-bit\n> Byte Order:            Little Endian\n> CPU(s):                12\n> On-line CPU(s) list:   0-11\n> Thread(s) per core:    2\n> Core(s) per socket:    6\n> Socket(s):             1\n> NUMA node(s):          1\n> Vendor ID:             GenuineIntel\n> CPU family:            6\n> Model:                 63\n> Stepping:              2\n> CPU MHz:               2599.968\n> BogoMIPS:              4797.20\n> Virtualization:        VT-x\n> L1d cache:             32K\n> L1i cache:             32K\n> L2 cache:              256K\n> L3 cache:              15360K\n> NUMA node0 CPU(s):     0-11\n", "As a workaround, the values can be fixed by swapping the bytes via integer-operations\n\n```\n# Decode image\nimage = tf.image.decode_png(png, dtype=tf.uint16)\nimage = tf.reshape(image, [depth, height, width])\n\n# Swap Bytes \ntensor256 = tf.fill(image.get_shape(), 256)\nimage_div = tf.div(tf.cast(image, tf.int32), tensor256)\nimage_mod = tf.mod(tf.cast(image, tf.int32), tensor256)\nimage = tf.add(tf.mul(image_mod, tensor256), image_div)\n```\n\nThis is obviously a dirty hack and would break correct values on systems where the decode function works correctly \n", "@dave-andersen Could you take a look since you touched the png reading most recently? :)\n", "Urgh.  I'm concerned that we're not properly setting the IS_LITTLE_ENDIAN flag in the OSS build:\nhttps://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=IS_LITTLE_ENDIAN\n\nI'll take this on.  @girving - do you know off the top of your head if we have an existing flag for little endian I can repurpose here?  (I'll dig, just trying to be lazy. :)\n", "@dave-andersen I don't remember, nor do I envy you this task. :)\n", "I'm, um, just going to use the existing flag and assume it's someone else's responsibility to make the flag work. :)\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/host_info.h#L25\n\n(static const bool kLittleEndian = true; )\n", "Actually, I'm not sure that flag is the issue.  @beopst Are you on a little endian machine?\n", "@girving - the issue is that we don't use that flag in the PNG decoder, we use the IS_LITTLE_ENDIAN flag, which is only defined by some transitively included google-internal header.\n\nThe second part of the issue is that we don't have any tests for the png decode op that would catch this on jenkins.\n\nFixing.\n", "Fix submitted, should hit github at next sync.  Will let the fix close it.\n@beopst - huge thanks for providing an easy way to reproduce with your bug report!\n"]}, {"number": 2618, "title": "BUILD ERROR: bazel could not find \"highwayhash/sip_hash.h\"", "body": "Hi, could you tell me what I should correct?\n### Environment info\n\nOperating System: Fedora 23 Server\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): CUDA 7.5.18, cuDNN v4 (4.0.7)\n### Steps to reproduce\n1. prepare following to the instruction for \"install from source\" on web page\n2. downgrade gcc to 4.9.3 and save user's local (because upper version is still in /usr/bin)\n3. get bazel from GitHub\n4. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failure\n### What have you tried?\n1. found the error message which seems not to find the include file of \"sip_hash.cc\". \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n____Loading package: tensorflow/cc\n____Loading package: @bazel_tools//tools/android\nWARNING: /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/protobuf/WORKSPACE:1: Workspace name in /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\n____Loading package: @bazel_tools//third_party/java/jdk/langtools\n____Loading package: @protobuf//\nWARNING: /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/highwayhash/WORKSPACE:1: Workspace name in /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/re2/WORKSPACE:1: Workspace name in /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\n____Loading package: @re2//\n____Loading package: @jsoncpp_git//\n____Loading package: tensorflow/core/util/ctc\n____Loading complete.  Analyzing...\n____Found 1 target...\n____Building...\n____[0 / 1] BazelWorkspaceStatusAction stable-status.txt\n____[34 / 351] Creating source manifest for //tensorflow/cc:tutorials_example_trainer\n____[43 / 2,637] Creating source manifest for //tensorflow/cc:ops/array_ops_gen_cc [for host]\n____[45 / 2,969] Executing genrule @boringssl_git//:err_data_c\n____[46 / 2,973] Creating source manifest for //tensorflow/cc:ops/control_flow_ops_gen_cc [for host]\n____[50 / 2,977] Compiling external/highwayhash/highwayhash/sip_hash.cc\n____[50 / 2,977] Compiling external/jsoncpp_git/src/lib_json/json_reader.cpp\n____[50 / 2,977] Executing genrule @farmhash_archive//:configure\n**ERROR: /home/sio/.cache/bazel/_bazel_sio/20abde1548c7cbc7b364a6249f6601f2/external/highwayhash/BUILD:17:1: undeclared inclusion(s) in rule '@highwayhash//:sip_hash':\nthis rule is missing dependency declarations for the following files included by 'external/highwayhash/highwayhash/sip_hash.cc':**\n  '/home/sio/usr/local/include/c++/4.9.3/cstddef'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/c++config.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/os_defines.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/cpu_defines.h'\n  '/home/sio/usr/local/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include/stddef.h'\n  '/home/sio/usr/local/include/c++/4.9.3/cstring'\n  '/home/sio/usr/local/include/c++/4.9.3/memory'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_algobase.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/functexcept.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/exception_defines.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/cpp_type_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/type_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/numeric_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_pair.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/move.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/concept_check.h'\n  '/home/sio/usr/local/include/c++/4.9.3/type_traits'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_iterator_base_types.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_iterator_base_funcs.h'\n  '/home/sio/usr/local/include/c++/4.9.3/debug/debug.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_iterator.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/ptr_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/predefined_ops.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/allocator.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/c++allocator.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/new_allocator.h'\n  '/home/sio/usr/local/include/c++/4.9.3/new'\n  '/home/sio/usr/local/include/c++/4.9.3/exception'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/atomic_lockfree_defines.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/exception_ptr.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/nested_exception.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/memoryfwd.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_construct.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/alloc_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/alloc_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_uninitialized.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_tempbuf.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_raw_storage_iter.h'\n  '/home/sio/usr/local/include/c++/4.9.3/typeinfo'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/hash_bytes.h'\n  '/home/sio/usr/local/include/c++/4.9.3/iosfwd'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stringfwd.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/postypes.h'\n  '/home/sio/usr/local/include/c++/4.9.3/cwchar'\n  '/home/sio/usr/local/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include/stdarg.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/atomicity.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/gthr.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/gthr-default.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/atomic_word.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/concurrence.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_function.h'\n  '/home/sio/usr/local/include/c++/4.9.3/backward/binders.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/uses_allocator.h'\n  '/home/sio/usr/local/include/c++/4.9.3/functional'\n  '/home/sio/usr/local/include/c++/4.9.3/tuple'\n  '/home/sio/usr/local/include/c++/4.9.3/utility'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/stl_relops.h'\n  '/home/sio/usr/local/include/c++/4.9.3/initializer_list'\n  '/home/sio/usr/local/include/c++/4.9.3/array'\n  '/home/sio/usr/local/include/c++/4.9.3/stdexcept'\n  '/home/sio/usr/local/include/c++/4.9.3/string'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/char_traits.h'\n  '/home/sio/usr/local/include/c++/4.9.3/cstdint'\n  '/home/sio/usr/local/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include/stdint.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/localefwd.h'\n  '/home/sio/usr/local/include/c++/4.9.3/x86_64-unknown-linux-gnu/bits/c++locale.h'\n  '/home/sio/usr/local/include/c++/4.9.3/clocale'\n  '/home/sio/usr/local/include/c++/4.9.3/cctype'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/ostream_insert.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/cxxabi_forced.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/range_access.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/basic_string.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/string_conversions.h'\n  '/home/sio/usr/local/include/c++/4.9.3/cstdlib'\n  '/home/sio/usr/local/include/c++/4.9.3/cstdio'\n  '/home/sio/usr/local/include/c++/4.9.3/cerrno'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/functional_hash.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/basic_string.tcc'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/unique_ptr.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/shared_ptr.h'\n  '/home/sio/usr/local/include/c++/4.9.3/bits/shared_ptr_base.h'\n  '/home/sio/usr/local/include/c++/4.9.3/ext/aligned_buffer.h'\n  '/home/sio/usr/local/include/c++/4.9.3/backward/auto_ptr.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n____Elapsed time: 6.120s, Critical Path: 4.43s\n", "comments": ["I saw the page of \"highway hash\" on GitHub, and found the message keeping \"highwayhash/sip_hash.h\" because of bazel configuration. Should I correct the source temporarily in this case to build it?  \n", "This may be related to a recent change. I'll inquire about that, but it would be helpful to know if you were able to build with an older version successfully?\n", "Thanks, aslle! I'll try to build older version while waiting the response. I will report its result again. :-)\n", "What version of bazel are you using?\n\nThis looks familiar to \nhttps://github.com/tensorflow/tensorflow/issues/469\n\nCan you try downgrade bazel?\n", "Thanks, yasuematsu. \nNow I'm trying to remove newer bazel. I will report if I get the something progress.\n", "Hi, I have almost same problem, although some environments are different. Is threre any way to fix it ?\n\nMy environments are as follows...\n- OS : Centos 6.7\n- gcc : 4.8.2 (devtollset-2-gcc package)\n- bazel : 0.2.3\n- CUDA : 7.5.18\n- cuDNN : 5\n- TensorFlow : r0.9\n\nCPU version compiling succeeded after some modification, but GPU version compiling fails with same error messages above. (Actual error messages are as follows...)\n\n<pre>\n...\nERROR: /home1/chkim/.cache/bazel/_bazel_chkim/20a7d6c35f46e64c010eb39bcbc0e998/external/highwayhash/BUILD:17:1: undeclared inclusion(s) in rule '@highwayhash//:sip_hash':\nthis rule is missing dependency declarations for the following files included by 'external/highwayhash/highwayhash/sip_hash.cc':\n  '/opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/cstddef'\n  '/opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/x86_64-redhat-linux/bits/c++config.h'\n  '/opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/x86_64-redhat-linux/bits/os_defines.h'\n...\n</pre>\n\nI downgraded bazel to 0.1.1 version, but it failed at initial time with another errors.\n(It seems that there are many differences between 0.2.3 and 0.1.1...)\n", "I found a way to fix my problem. I'm not sure it's right way...\n\nI added my gcc header file path to third_party/gpus/crosstool/CROSSTOOL file.\n\n<pre>\n  cxx_builtin_include_directory: \"/opt/rh/devtoolset-2/root/usr/include\"\n  cxx_builtin_include_directory: \"/opt/rh/devtoolset-2/root/usr/lib/gcc/x86_64-redhat-linux/4.8.2/include\"\n</pre>\n\nI guess that my problem was due to using another version of gcc that was not installed in system directory.\nIf same problem, it may work...\n", "Yes, the problem happens when gcc is installed in other directory.\n\nThe issue to fix in configuration phase is tracked in #2413.\n", "Thank you for all of your comments.\nI had been tackled on this issue, but I could not resolve it.\n\nYes, I have installed the older gcc in my current directory. Maybe I should also install gcc in system directory. \n\nAnyway, I will try again with new version of tensorflow. Many thanks!  \n", "xPost from #469 \n\nFYI : I was able to build the GPU pip package using the changes listed in this install script : https://github.com/noisychannel/tensorflow_install/blob/master/tf_install.sh\n\nSpecifically, check the function fix_tf which makes changes in third_party/gpus/crosstool/CROSSTOOL, tensorflow/tensorflow.bzl and tensorflow/stream_executor/BUILD.\n", "Automatically closing this issue due to lack of recent activity.\n", "@SioIkzk My guess is that you want to add each of the include directories referenced in the error message to `CROSSTOOL`, as described by @chanhkim , in the appropriate \"toolchain\" subsection.\n\nDetail:\n\nI had a very similar problem.\nPer the original post, one might think the problem is that a header cannot be located, but this is probably **not** the case.\nLooking at the bazel source (v0.3.0), this message (\"undeclared inclusion(s) ... missing dependency declarations\") is produced when the includes, referenced by a source file, **are** found in the filesystem, but are in directories that are **not** declared to the build system.\nIn my case at least, `third_party/gpus/crosstool/CROSSTOOL` required additional `cxx_builtin_include_directory` lines to reference the include directories shown in the \"undeclared inclusions\" error message.\nIn particular, if you apply the type of changes mentioned by @noisychannel you may need to make sure you don't replace system include directories in which includes are still discovered -- instead you want to add the new `cxx_builtin_include_directory` lines before the old lines (unless you are sure you don't want to find things in the old directories).\n"]}, {"number": 2617, "title": "R0.8", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2616, "title": "[New Feature] Dimension Validation for CNNs ", "body": "In the deep MNIST samples for experts, the dimensions of the example network is defined as follows: \n\n[5, 5, 1, 32]\n[5, 5, 32,64]\n[7_7_64, 1024]\n[1024, n_classes]\n\nIt seems nice for us to implement functions that checks a validity of given network, if we've not done yet. \n\nExample Error or Warning Messages:\n\n1: u\" Dimension mismatch occurs at forward calculation from %s %s to %s %s: Input_dim=%s but Output_dim=%s  \"%( lower_component, lower_component,  upper_type, upper_name, Input_dim, Output_dim)\n\n\"lower_component\" and \"upper_componet\" are either function with out tf.Variable or anything with tf.Varriables.\n\n1-a: u\" Dimension mismatch occurs at forward calculations from linear-layer \"first-linear\" to linear-layer \"out\": Input_dim=1024 but Output_dim=1023\", if \n\n```\n'first-linear': tf.Variable(tf.random_normal([7*7*64, 1024])), \n'out': tf.Variable(tf.random_normal([1023, n_classes])) \n```\n\n2: u\" Dimension mismatch occurs at forward calculation:  Input_dim=%s  of %s %s is not divided by %s*%s, Note that output of %s %s reduce dimension from %s to %s \"%(Input_dim, lower_type, lower_name, out_width, out_height,  lower_type,  lower_name, Input_dim, Output_dim)\n\n2-a u\" Dimension mismatch occurs at forward calculation:  Input_dim=2048  of linear-layer \"wd1\" is not divided by 7*7, Note that output of function \"max_pool\" reduces dimension from 196 to 49 \"\n\n3: u\" Dimension mismatch occurs at forward calculation: At the conv-layer %s, for given weight of W2 =(W1-F+2P)/S+1 isn't an integer, where width W1=%s, F=%s, P=%s, S=%S  \"%(conv_layer_name, filter_size, padding, strides)\n", "comments": ["If you're just asking to re-write some error messages with more information, I'm all for it. You should specify which existing error messages should be modified and how.\n\nIf you're suggesting to add more checking code to the mnist example, that's also a good idea, I would change the title of this issue if that was the case.\n\nChecking the sizes in the general case is fairly hard (since sizes can only be checked once they are known, which is often when you run the graph, not when you define it). The ops do check size requirements, but because of their generality, often don't have a ton of information about the larger use case. \n", "Closing for now since we already check dimensions pretty thoroughly.  @konts6102 Please comment if there's something specific that we're missing and I'm happy to reopen.\n"]}, {"number": 2615, "title": "Push changes from internal: 123818693", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2614, "title": "CUDA 8.0 support", "body": "NVidia CUDA 7.5 distribution for Ubuntu only goes up to Ubuntu 15.04 which is EOL'ed already. So it would be useful to have CUDA 8.0 support which is provided for Ubuntu 16.04\n", "comments": ["There is already a bug for this I think. On mobile but you should be able to search\n", "sorry, should've searched better, https://github.com/tensorflow/tensorflow/issues/2559\n"]}, {"number": 2613, "title": "Added iOS example using the camera", "body": "", "comments": ["Jenkins, please test this.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Sadly, we can't merge anything until jenkins is fixed.\n\n@caisq @martinwicke  I assume you guys are looking at this :)\n", "I've been in touch with @martinwicke and they're working on it, so hopefully we can get this in soon.\n", "Jenkins, test this please.\n", "I have just downgraded jenkins. That should fix it.\n\n@tensorflow-jenkins test this please\n", "@jendap unfortunately it looks like it's still hitting the same error.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins seems to be fixed now. I have triggered this test, and it seems to be running better now.\nPlease let me know if the problem persists.\n", "Looking on Jenkins, it seems like the builds all succeeded but the checks here haven't updated.\n", "yeah, working on it - b/29081939\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n\n(this time for real, we have workaround the issue)\n"]}, {"number": 2612, "title": "Fix setup for development link step.", "body": "- This was a typo as agreed upon at https://github.com/tensorflow/tensorflow/issues/2591\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2611, "title": "Merge pull request #1 from tensorflow/master", "body": "update from origin\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2610, "title": "per_process_gpu_memory_fraction=1 does not allocate all of the memory", "body": "Seeing this on TF 0.8 with a Titan X. If I don't specify gpu_options at all, the memory allocated to TF is 11736MB. If I set per_process_gpu_memory_fraction=1.0, I only get 11127MB allocated. It's a small difference, but enough to make my large model crash, while the default allocations lets it just squeak in.\n", "comments": ["Can you log https://github.com/tensorflow/tensorflow/blob/ecc6c5ae3dcec7ea263a010ae2b6b1f7dc0da1a8/tensorflow/core/common_runtime/gpu/gpu_device.cc#L622 those two values there and let me know what they return in this case?  I'm a bit confused about how 'total' can ever be less than 'available'.\n", "Hmmm, it may not be memory allocation _per se_ that's changing. I haven't logged those values yet but I tried running a smaller model and it too ran out of memory. This new one consumes about half as much memory and so it should have no trouble running. The only other thing I changed is to set allow_growth=False, but that is the default anyway, isn't it? And along with per_process_gpu_memory_fraction=1.0, it should have no effect, no?\n\nI have no problem running much smaller models with per_process_gpu_memory_fraction set as low as 0.33, so I have gotten it to work, but it's the largish models that are giving me errors.\n\nI can log the variables you mentioned, but there's no way to do this other than changing the C++ code and recompiling correct? I can do that but it'll take me a little bit of time because I have jobs currently running.\n", "Yeah, there's no way to change it without a recompile -- basically my expectation of what is being reported by the device (through the StreamExecutor->DeviceMemory() call) appears to be wrong, so I don't know what the right fix until I can see what behavior you're running into.\n", "Cc @zheng-xq in case he knows better \n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2609, "title": "Feed Cifar10 tutorial with an external image (jpg/png) 32x32 image get label as output", "body": "Hello,\n\nI am trying to use the trained machine based on the Cifar10 tutorial and would like to feed\nit with an external image 32x32 (jpg or png). My goal is to be able to get the label as an output.\nIn other words, I want to feed the Network with a jpeg image of size 32 x 32 with no label as an input and have the inference process \u201cgive me\u201dthe tf.argmax(logits, 1). \n\nI have been trying to do that based on the CIfar10 Tutorial and unfortunately always have issues. especially with the Session concept and the batch concept.\n\nhere is the implemented code so far:\n\n``` python\n\n#!/usr/bin/env python\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport time\n\nimport tensorflow.python.platform\nfrom tensorflow.python.platform import gfile\nimport numpy as np\nimport tensorflow as tf\n\nimport cifar10\nimport cifar10_input\nimport os\nimport faultnet_flags\nfrom PIL import Image\n\nFLAGS = tf.app.flags.FLAGS\n\ndef evaluate():\n\n  filename_queue = tf.train.string_input_producer(['/home/tensor/.../inputImage.jpg'])\n\n  reader = tf.WholeFileReader()\n  key, value = reader.read(filename_queue)\n\n  input_img = tf.image.decode_jpeg(value)\n\n  init_op = tf.initialize_all_variables()\n\n# Problem in here with Graph / session\n  with tf.Session() as sess:\n    sess.run(init_op)\n\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n\n    for i in range(1): \n      image = input_img.eval()\n\n    print(image.shape)\n    Image.fromarray(np.asarray(image)).show()\n\n# Problem in here is that I have only one image as input and have no label and would like to have\n# it compatible with the Cifar10 network\n    reshaped_image = tf.cast(image, tf.float32)\n    height = FLAGS.resized_image_size\n    width = FLAGS.resized_image_size\n    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, width, height)\n    float_image = tf.image.per_image_whitening(resized_image)  # reshaped_image\n    num_preprocess_threads = 1\n    images = tf.train.batch(\n      [float_image],\n      batch_size=128,\n      num_threads=num_preprocess_threads,\n      capacity=128)\n    coord.request_stop()\n    coord.join(threads)\n\n    logits = faultnet.inference(images)\n\n    # Calculate predictions.\n    #top_k_predict_op = tf.argmax(logits, 1)\n\n    # print('Current image is: ')\n    # print(top_k_predict_op[0])\n\n    my_classification = sess.run(tf.argmax(logits, 1))\n\n    print ('Predicted ', my_classification[0], \" for your input image.\")\n\n\ndef main(argv=None):\n  evaluate()\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n\nThank your for your help.\nN.\n", "comments": ["Hi there! I'm going to close this issue as it doesn't sound like there's a bug or missing feature in TensorFlow itself, so this kind of question is better suited to Stack Overflow:\n\n> GitHub issues are for bugs / installation problems / feature requests.  \n> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\n> To make bugs and feature requests more easy to find and organize, we close issues that are deemed\n> out of scope for GitHub Issues and point people to StackOverflow.\n\nFeel free to repost this question on stackoverflow.com, with more details about exactly what is going wrong, and somebody will respond soon.\n"]}, {"number": 2608, "title": "Enable tf.rank() for SparseTensor", "body": "Added an override for `tf.rank()`, that takes care of `SparseTensor` objects as well. Added tests and verified locally. This partially addresses #1968.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins , test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "@concretevitamin Addressed all the comments, let me know if this is okay.\n", "@siddharth-agrawal thanks, just a few more minor comments, then we're good to go!\n", "@concretevitamin Updated!\n", "Jenkins, test this please.\n", "Looks good!  Could you rebase?\n\nOn Thursday, June 2, 2016, Siddharth Agrawal notifications@github.com\nwrote:\n\n> @concretevitamin https://github.com/concretevitamin Updated!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2608#issuecomment-223394523,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAkLHn69HstWKqMHAyxyDKYq8SqUcGvkks5qHy0MgaJpZM4Ir2qC\n> .\n", "Jenkins, test this please.\n", "@siddharth-agrawal: thanks, I just merged this in.\n"]}, {"number": 2607, "title": "latest TensorBoard broken in Safari", "body": "Since about a week now there seems something wrong with the TensorBoard demo at https://www.tensorflow.org/tensorboard/index.html#events. The graph shows nicely, but neither events nor histograms show up. This problem seems to be only with the demo -- everything shows up just fine when I run the corresponding code (`mnist_tensorboard.py`) locally.\n", "comments": ["It's working for me on both Chrome and Firefox. Can you provide some screenshots of how it looks on your machine, along with console output?\n", "Also, please include info on what operating system + browser you are using.\n", "I'm using Safari on OS X 10.11. I'm attaching screenshots below. It looks the same on Safari on iOS. I don't have any console output to show because I'm just referencing your own tensorboard demo that you're hosting at tensorflow.org. TensorBoard is working fine with that model when I run it locally, it's just this online demo of yours that seems to be broken.\n![screen shot 2016-06-02 at 19 27 45](https://cloud.githubusercontent.com/assets/6429851/15754399/4aeb3676-28f8-11e6-8a82-5456dfbf17d8.png)\n![screen shot 2016-06-02 at 19 28 09](https://cloud.githubusercontent.com/assets/6429851/15754400/4b0d1228-28f8-11e6-9b3e-43391b658e02.png)\n", "I see, I can repro that too now.\nThe issue is that Safari doesn't support ES6 fat arrow functions. \nIn general we don't test Safari and don't guarantee Safari will work; you should use Chrome or Firefox.\nHowever, this is quite annoying so I will make an exception and fix it for Safari :)\n", "OK, I've put in a fix upstream. Thanks for the report, otherwise the next release of TensorBoard would have been broken in Safari. \n\nNote, it may take several days before this change is reflected on the website. \n", "What do I need to do to install a version with this fix. Is an updated `.whl` available?\n\n(I've installed `mac/tensorflow-0.9.0rc0-py2-none-any.whl`.)\n", "We need https://github.com/tensorflow/tensorflow/pull/2786 to merge into 0.9 for 0.9 to be fixed.\nThen, you can rebuild the .whl yourself using the build_pip_package script, or wait for us to publish an updated whl. \n"]}, {"number": 2606, "title": "skflow  multiple_gpu.py error", "body": "When I execute python multiple_gpu.py I get following error. I have verified that my installation has cudnn v7 and cuda ( since I can run the translate.py example on gpu)\n\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'logistic_regression/HistogramSummary_3/tag': Could not satisfy explicit device specification\n '/device:GPU:1' because no supported kernel for GPU devices is available\n         [[Node: logistic_regression/HistogramSummary_3/tag = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values: logistic_regression.bias>, _device=\"/device:GPU:1\"]()]]\nCaused by op u'logistic_regression/HistogramSummary_3/tag', defined at:\n  File \"multiple_gpu.py\", line 39, in <module>\n    classifier.fit(X_train, y_train)\n  File \"/home/ubuntu/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 242, in fit\n", "comments": ["@vrv you worked on device assignment - what could lead to such error?\n", "We don't support datatype string on GPUs, so something is assigning the HistogramSummary or related op on GPU when it shouldn't be.  Not sure without seeing the source code.\n", "Code is here: https://github.com/tensorflow/tensorflow/blob/5a5a25ea3ebef623e07fb9a46419a9df377a37a5/tensorflow/examples/skflow/multiple_gpu.py\n\nBut I think problem is coming from:\n\n```\nwith tf.device('/gpu:2'):\n   ... some gpu computation ...\n   tf.histogram(..)\n   ... some more gpu computation ...\n```\n\nEither way, for this case we should just convert the test to the new `Estimator` that doesn't add automatic histograms.\n"]}, {"number": 2605, "title": "Feature request: space_to_depth with overlapping blocks (like im2col)", "body": "Currently [tf.space_to_depth](https://www.tensorflow.org/versions/r0.7/api_docs/python/array_ops.html#space_to_depth) only works for rearranging non-overlapping blocks into depth. It would be nice if also worked for overlapping blocks, with an extra parameter to set the stride:\n- A stride equal to the block size would be equivalent to the current non-overlapping behavior.\n- A stride equal to 1 would be equivalent to an im2col operation.\n\nAn im2col-like operation could be used to easily implement a locally connected layer (\"convolution with unshared weights\"), by combining it with an elementwise multiplication with the matrix that contains the local filters, followed by a summation along the channel dimension.\n", "comments": ["I tried to use[ tf.extract_image_patches](https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#extract_image_patches) to implement a locally layer and run into an error:\n\nLookupError: No gradient defined for operation 'local/ExtractImagePatches' (op type: ExtractImagePatches) \n\nI checked that  tf.space_to_depth has the gradient definition but  tf.extract_image_patches does not. And it seems not easy for me to implement the gradient. Any advice?  \n", "@jyshee did you get this working? "]}, {"number": 2604, "title": "Result of tf.reshape() with dynamic shape does not work with tf.train.shuffle_batch()", "body": "I wrote the images into TensorFlow format file, but when I read it like this: \n\n``` python\nfeatures = tf.parse_single_example(\n            serialized_example,\n            features = {\n                'height': tf.FixedLenFeature([], tf.int64),\n                'width': tf.FixedLenFeature([], tf.int64),\n                'channel': tf.FixedLenFeature([], tf.int64),\n                'image': tf.FixedLenFeature([], tf.string),\n                'gt': tf.FixedLenFeature([], tf.string),\n            })\n\n```\n\nfirst, the data has no shape infomation, only the batch size, so I reshape it, but parameter shape of tf.reshape() only support list of integers, not a tensor, I don't know why?\n\nsecond, the data of gt is list, maybe length(gt) > 2, because no shape information, so I can't unpack it\n\nso, why is not shape information available\n", "comments": ["The [`tf.reshape()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#reshape) op supports a `tf.Tensor` _or_ a list of integers for the `shape` argument. How are you invoking it and what error are you seeing?\n", "@mrry \uff0c yes, I do it like this: \n\n``` python\nimage = tf.reshape(image, tf.pack([height, width, channel]))\n```\n\nwhere height, width and channel are tensors, but it turns out that:\n`All shapes must be fully defined: [TensorShape([Dimension(None), Dimension(None), Dimension(None)]), TensorShape([Dimension(512), Dimension(512)]), TensorShape([Dimension(3)])]`\nin the line of `tf.train.shuffle_batch()` call\n\n I have no idea about it.\n", "Ah, so the `tf.reshape()` works as intended, but you then use the result in a call to `tf.train.shuffle_batch()` and that fails? I'm afraid that this is expected behavior.\n\nAs the [docs for `tf.train.shuffle_batch()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#shuffle_batch) say, you must either have fully-defined (i.e. static) shapes for all of the tensors you pass in the `tensor_list` argument (or you must specify explicit shapes for them). For TensorFlow to batch your images (or other tensors) together, they must all have the _same_ shape. You cannot pass the result of a dynamic `tf.reshape()`, because each element might have a _different_ shape, and TensorFlow cannot batch them.\n\nYour best option is to resize, crop, or pad the input images to a static shape before passing them to `tf.train.shuffle_batch()`. For example, the Inception [input pipeline](https://github.com/tensorflow/models/blob/eec79382c4d4dcf04804d22a0e337d5b5b0e398f/inception/inception/image_processing.py#L256) uses  [`tf.image.resize_images()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/image.html#resize_images) to resize images to a standard size, so that they can be batched (using `tf.train.batch_join()` in this case).\n", "sorry that it's my faulst. I think I know how to read tensorflow records\n\n``` python\ndef read_and_decode(filename_queue, image_shape):\n        reader = tf.TFRecordReader()\n        _, serialized_example = reader.read(filename_queue)\n\n        features = tf.parse_single_example(\n            serialized_example,\n            features = {\n                'height': tf.FixedLenFeature([], tf.int64),\n                'width': tf.FixedLenFeature([], tf.int64),\n                'channel': tf.FixedLenFeature([], tf.int64),\n                'image': tf.FixedLenFeature([], tf.string)\n            })\n\n        # Convert from a scalar string tensor\n        image = tf.decode_raw(features['image'],  tf.uint8)\n\n        #set static shape, or you can't reshape a image to [h, w, c]\n        image.set_shape(reduce(lambda x, y : x * y,  image_shape))\n\n       #reshape to [h, w, c]\n        image = tf.reshape(image,  image_shape)\n```\n\nalthough I write shape [height, width, channel] into records file, but it has nothing to do.\n", "If you know that `image` has a static shape `[h, w, c]`, where `h`, `w`, and `c` are Python `int` values, you can simply call:\n\n```\nimage.set_shape([h, w, c])\n```\n\n...and you don't need the `tf.reshape()` because it should be a no-op. Then your code will work with `tf.train.shuffle_batch()`.\n", "Closing this, as there's no bug. Feel free to ask on Stack Overflow if you have questions about how to build input pipelines, however.\n", "@mrry \nI have a question, since when writing the TFRecords file, the height, width and channels information have already been written into the file, how can I use the information from the file(not explicit specify from outside) to set shape of the flatten image data?\n", "same question as @wangchuan\nDid you find a way to tackle this problem? @mrry \n", "Look here @wangchuan http://stackoverflow.com/questions/35620157/how-can-i-use-values-read-from-tfrecords-as-arguments-to-tf-reshape\n@mrry already answered this question.\n", "In fact, `height` or `width` read from tfrecords is a tensor,  so we don't know its value until `sess.run(height)`. the problem is that when the feature of image is used in fc layer,  we can't initialize a weight of fc layer, because the shape of the feature is `None`.\n\nso when reading images with different shape, and keep the shape of image in training, I am not sure how to it. if using `tf.placeholder()`, maybe I/O is the problem, of course I can use multi-thread and a queue\n", "@mrry Hi, I use `image.set_shape([64, 256, 3])`.But it have a new ValueError: Shapes (?,) and (64, 256, 3) are not compatible.I don't know how to fix it.", "@JcmeLs It sounds like you want to *change* the shape of the tensor rather than updating its static information, so you should use `tf.reshape(image)` instead.", "@mrry I want to get image from tfrecord and put it in the batch.The code is:\r\n``` python\r\n        image=tf.decode_raw(features['image'],tf.uint8)\r\n        image=tf.reshape(image,tf.stack([64,256,3]))\r\n        label=tf.decode_raw(features['label'],tf.uint8)\r\n        image_batch,label_batch=tf.train.batch([image,label],batch_size=64,num_threads=64,capacity=2000)\r\n```\r\nBut,when I run this code.It will hava ValueError`: All shapes must be fully defined: [TensorShape([Dimension(64), Dimension(256), Dimension(3)]), TensorShape(None)]`.So I try `image.set_shape([64, 256, 3])` .But I get the new ValueError: `Shapes (?,) and (64, 256, 3) are not compatible`", "@JcmeLs That error message indicates that `label` is a rank-1 tensor (vector) of unknown length. You'll need to either set its shape or reshape it as well. I'm guessing that the following will work, but it's not entirely clear without more context:\r\n\r\n```\r\nlabel = tf.reshape(label, [])  # Reshape to a scalar.\r\n```", "@mrry Thanks,It can work now.But when I take data from the batch many times, it will be error.`OutOfRangeError (see above for traceback): FIFOQueue '_1_batch/fifo_queue' is closed and has insufficient elements (requested 64, current size 0)\r\n\t [[Node: batch = QueueDequeueManyV2[component_types=[DT_UINT8, DT_UINT8], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]`My queue code:\r\n``` python\r\n    coord=tf.train.Coordinator()\r\n    threads=tf.train.start_queue_runners(sess=sess,coord=coord)\r\n```\r\nbatch code:\r\n``` python\r\n    image_batch, label_batch = tf.train.batch([distorted_image, label],\r\n                                              batch_size=batch_size,\r\n                                              num_threads=4,\r\n                                              capacity=2000)\r\n```\r\nIs this queue not looping? Due to the small amount of data, I would like to increase my data through data augmentation. But it have `OutOfRangeError`.", "I fix it.Thanks again.", "@JcmeLs  Hi JcmeLs. My code worked well in tf v0.8.0. I updated the tf.pack in tf v0.8.2 to tf.stack in tf v1.2.0. It reported the same error: \r\n`OutOfRangeError (see above for traceback): FIFOQueue '_1_batch/fifo_queue' is closed and has insufficient elements (requested 1, current size 0) [[Node: batch = QueueDequeueManyV2[component_types=[DT_UINT8, DT_UINT8], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](batch/fifo_queue, batch/n)]]`\r\n\r\nThe following are all of my changes.\r\n**original code in tf v0.8.2:**\r\n```\r\nnew_shape = tf.pack([scale,scale])\r\nimg = tf.image.resize_images(img, new_shape[0], new_shape[1])\r\n```\r\n\r\n**changed code in tf v1.2.0**\r\n```\r\nnew_shape = tf.stack([scale,scale])\r\nimg = tf.image.resize_images(img, [new_shape[0], new_shape[1]])\r\n```\r\n\r\nSo how did you solve the  '_1_batch/fifo_queue' is closed and has insufficient elements' error?\r\nthanks.\r\n"]}, {"number": 2603, "title": "Feature request: Tensorboard streaming", "body": "The new Tensorboard refresh button looks great, and will certainly be a bit more civilized than rapidly swapping the horizontal axis view to update the data. :)\n\nBut what if Tensorboard could be optionally updated in real time, whenever a summary writer was flushed? I've written systems before where past data is loaded along with the page, and new data is added continuously via a web socket. Would this work in Tensorboard's current architecture?\n\nIf my understanding is correct, Tensorboard currently updates itself every 120 seconds by reading the event files in the log directory. Could Tensorboard also open a local socket and bounce summary updates from a connected Tensorflow process to websocket clients?\n\nI was thinking of hacking a streaming system together on my branch, but I figured I should reach out first to see if this is already in the roadmap. If it isn't, is this feature something you might like to merge if I can get it to work robustly?\n", "comments": ["Hi SpencerC,\n\nI'm currently working on a re-architecutre of the TensorBoard backend that will shift the underlying data pipe to be based on streams of data rather than async loading, with the option to connect TensorBoard directly to the TensorFlow process. Once this is setup, it will be feasible to have realtime updating in TensorBoard. However, it's a very involved change that involves changes to TensorFlow, the TensorBoard backend, and the TensorBoard frontend. So don't expect it to be released in the near future. \n\nYou're welcome to hack a streaming system together, but unless we coordinate closely there's a pretty good chance that our changes will clobber each other.\n\nAs a workaround, you can change the frequency with which TensorBoard loads events from the backend by setting `reload_interval` to something much shorter, e.g. tensorboard --reload_interval=2. Then you can click the refresh button and get data much faster (note you should increase the flush frequency on your summary writer too). \n\nRight now the reload frequency on the frontend is hardcoded to 120secs, I would like to setup a route so that it reads from TensorBoard and reloads as quickly as TensorBoard is reloading. If you feel like contributing that, it's a good candidate for a pull request.\n", "Sure, I can work on that. I assume it's fine to add a route to handler.py, and the modify the js behaviors in the tf-tensorboard and tf-backend folders? \n", "Sorry I disappeared. You're right that adding a route to handler.py and modifying the js behaviors is a good way to go. \n\nThere's no ETA on the broader TensorBoard changes that will switch to streaming.\n", "@SpencerC, anything I can do to help?", "Any update on this? It seems like a great and obvious improvement to tensorboard", "Supporting streaming isn't currently on the agenda, so no ETA.\r\nPinging @jart to take a look.", "Migrated to tensorflow/tensorboard."]}, {"number": 2602, "title": "tensorboard server: base_url support", "body": "### What have you tried?\n\nRunning the tensorboard service through a proxy server, which does map a subdirectory to the tensorboard server' root.\n### Desired outcome\n\nBy providing a way to specify a `base_url`, the server would understand how to treat incoming requests in such a situation. e.g. `://server/subdir/tensorboard/` \u2192 `://server/`\n", "comments": ["Hi haraldschilly,\n\nCan you give an example of exactly how TensorBoard misbehaves in your setup? I.e. what requests does the frontend make that 404?\n\nOverall, this seems like a nice candidate for a pull request.\n", "Hi, I can describe you my specific use case. It's about the online service https://cloud.sagemath.com where you're essentially running programs in a managed linux environment. When this tensorboard is being started, you cannot access it directly, but through a http proxy. I.e. in the anaconda environment, where tensorflow is being installed,\n\n```\n$ tensorboard --port=7878 --logdir=.\nStarting TensorBoard b'16' on port 7878\n(You can navigate to http://0.0.0.0:7878)\n```\n\nThen I go to https://cloud.sagemath.com/xxxx-xxxx-project-uuid-xxx-xxx/port/7878/ which causes requests to be mapped to the service running in the project on port 7878.\n\nand the page shows this:\n\n```\nError response\n\nError code: 404\n\nMessage: Not Found.\n\nError code explanation: 404 - Nothing matches the given URI.\n```\n\nand in the terminal is that:\n\n```\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/projects/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/xxxx-xxxx-xxxx-xxxx/port/7878' on path /projects/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/xxxx-xxxx-xxxx-xxxx/port/7878\n10.244.5.3 - - [02/Jun/2016 15:23:11] code 404, message Not Found\n10.244.5.3 - - [02/Jun/2016 15:23:11] \"GET /xxxx-xxxx-xxxx-xxxx/port/7878/ HTTP/1.1\" 404 -\n```\n", "I hope the problem is fixed by this diff: https://github.com/tensorflow/tensorflow/commit/c0fc1b998ddbc838a479e295dfd9c0ad75d28f48 though I haven't really tried it out yet. ", "@haraldschilly the latest tensorboard source uses all relative URLs. You can use a proxy with tensorboard if you remove the base path before proxying.\r\n\r\nexample:\r\n\r\nLet's say you have a proxy located at `www.server.example`, you have a path defined for Tensorboard like so: `http://www.server.example/base/path/` , and your Tensorboard server is listening at `192.0.0.20:6006`\r\n\r\nYou want to make sure your proxy forwards `http://www.server.example/base/path/` to `http://192.0.0.20:6006/`. When your client (firefox, chrome, etc) calls the URL, `http://www.server.example/base/path/`, you need to make sure you have a `/` on the end of the path, or else the relative URLs will chop off the last bit of the path.", "I have migrated this issue to tensorflow/tensorboard#69 because TensorBoard has moved into its own repository (outside of tensorflow/tensorflow)."]}, {"number": 2601, "title": "fix typo in setup for development", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2600, "title": "Support python 3.x based Tensorflow in docker image", "body": "I'm trying to use tensorflow on Docker environment, and it turns out that tensorflow package is missing in python 3.4 dist-package in official docker image. Is there any reason for not supporting it?\n", "comments": ["Yes, it would be great to have Python 3 docker images, since there are installations available for 3.4 and even 3.5.\n", "@caisq: Do you know how much effort it would be to add this?\n", "@caisq has already taken care of this, as far as I know.\r\nPlease see our *py3 docker images. in dockerhub."]}, {"number": 2599, "title": "DaskDataFeeder ignores shuffle argument", "body": "The shuffle argument should not be ignore, because it is confusing to users of the feeder.\n", "comments": ["@aselle Thanks for pointing out. Yeah I will revisit this after internal refactoring of data feeder and integration with tf.DataFrame, etc. \n", "Probably obsolete."]}, {"number": 2598, "title": "Hessian (calling tf.gradients twice) of tf.scan fails", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Mac OS X 10.11.2\n\nInstalled version of CUDA and cuDNN: None\n\nIf installed from sources, provide the commit hash:\n\n4455f81c3dc07a77ac133dec24690968e121370a\n### Steps to reproduce\n\nRun the following script:\n\n``` python\nimport tensorflow as tf\n\ntheta = tf.Variable(initial_value=1.)\n\n\ndef fn(x, prev):\n    return prev + x * theta\n\nresult = tf.scan(fn, [1., 2., 3.])\n\ngrad_theta = tf.gradients(result, theta)\n\ntf.gradients(grad_theta, theta)\n```\n\nwill result in the following error:\n\n```\nTraceback (most recent call last):\n  File \"sandbox/rocky/tf/small_example.py\", line 13, in <module>\n    tf.gradients(grad_theta, theta)\n  File \"/Users/dementrock/anaconda/envs/rllab/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 379, in gradients\n    to_ops, from_ops)\n  File \"/Users/dementrock/anaconda/envs/rllab/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 185, in _PendingCount\n    between_ops)\n  File \"/Users/dementrock/anaconda/envs/rllab/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 874, in MaybeCreateControlFlowState\n    loop_state.AddWhileContext(op, between_op_list, between_ops)\n  File \"/Users/dementrock/anaconda/envs/rllab/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 738, in AddWhileContext\n    for loop_exit in forward_ctxt.loop_exits:\nTypeError: 'NoneType' object is not iterable\n```\n### What have you tried?\n\nNothing beyond creating this minimal reproducible example\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Adding @yuanbyu@, @ebrevdo.\n", "We don't support taking gradients of nonscalars, so I wouldn't expect this to work.  However, this particular error message is pretty confusing.  @yuanbyu: Is there a way we could improve this error message if someone tries for a Hessian in the naive way and control flow is involved? \n", "Yes, the error message should be better. Let me see what I can do.\n", "@girving Here theta is just a scalar value though. What would be the work around here?\n", "@dementrock: That's true in this case, but we don't want to do extra work on the control flow ops if all it provides is higher order derivatives w.r.t. scalars.  What is your intended use case? \n", "@girving I was doing some hessian vector product computations and had the same error, but the code snippet above is simpler and highlights the issue.\n\nSo is there no way to get higher order derivatives w.r.t. scan right now?\n", "@dementrock: We don't support higher order gradients even ignoring control flow. \n", "@girving No support as in no official support or it won't work at all? Seems like the following code at least compiles:\n\n``` python\nimport tensorflow as tf\n\ntheta = tf.Variable(initial_value=1.)\n\n\ndef fn(x, prev):\n    return prev + x * theta\n\nresult = fn(fn(1.0, 2.0), 3.0)\n\ngrad_theta = tf.gradients(result, theta)\n\ntf.gradients(grad_theta, theta)\n```\n\nAny plan to support it in the future?\n", "@dementrock The problem is that the registered gradient routines would get significantly more complicated if both sides were nonscalar, and we don't want to support that kind of complexity.  As discussed in #675, it's possible one could implement registered gradients with some sort of automatic machinery to map scalar gradient routines to nonscalar gradient routines, but this is a lot of work and we don't have any plans to do it.\n\nThe other problem is that the applications I know of nonscalar gradients aren't that compelling as yet, since they tend to be impractically huge.  However, there are cases where higher order gradient information arises where you're differentiating a scalar, specifically Hessian-free Krylov-ish methods where one evaluates the gradient dotted with a suitably chosen vector.\n\nIf that last bit is what you're trying to do, or something similar, we'd be happy to accept pull requests to make control flow not interfere.  It might be pretty complicated, though.\n"]}, {"number": 2597, "title": "Odd crash in Android demo", "body": "### Environment info\n\nAndroid Demo from 57658eddfab7763058c5316b1af41e77b1bb3b17\n\nI've been trying to build an image classifier for android, and I ran into this exception in the TensorflowClassifier class.\n\nI haven't run into this before despite playing with this for a few days, but I added some logging and will upload more details if I can repro it.\n\nNot sure if anyone cares, but I thought I might as well capture this.\n### Logs or other output that would be helpful\n\nCaused by: java.lang.NumberFormatException: Invalid float: \"Android\"\n   at java.lang.StringToReal.invalidReal(StringToReal.java:63)\n   at java.lang.StringToReal.initialParse(StringToReal.java:164)\n   at java.lang.StringToReal.parseFloat(StringToReal.java:323)\n   at java.lang.Float.parseFloat(Float.java:306)\n   at org.tensorflow.demo.TensorflowClassifier.recognizeImage(TensorflowClassifier.java:51)\n\nFWIW the line number in recognizeImage is wrong since I've made changes to the file, but shouldn't have caused this.\n\n[EDIT]: I found that the log message I was getting was \"Android system is not using RGBA_8888 in default\", I guess I'll see what that's about.\n", "comments": ["Hi,\nThe native classifier code returns the results as a String, which is parsed by TensorflowClassifier.java. It sounds like something has changed about the return format with your new model. Can you post the entire result string?\n\nI don't think the RGBA_8888 warning is related. The bitmaps are created in 8888 format explicitly, so they don't rely on the system default.\n", "Sorry for the confusion, I have been modifying the android demo to recognize the photos stored on my device rather than from the camera and it seems that when the classifyImageBmp function in tensorflow_jni.cc gets an image that is not RGBA_8888, it returns the string \"Android system is not using RGBA_8888 in default\", which causes the exception above.\n\nIt might be good to make the JNI code raise an exception, rather than return a string that gets badly parsed, but otherwise this seems fine (though there is the TODO(jiayq): deal with other formats if necessary.)\n", "Thanks for the clarification, makes sense now.\n\nIt should be pretty easy to convert an rgb565 (or whatever) bitmap to rgba8888 prior to sending it to the classifier.\n\nYour suggestion to raise an exception is a good idea, I'll see about adding that.\n", "Native error conditions have been changed to LOG(FATAL) messages  in https://github.com/tensorflow/tensorflow/commit/c32ef5a6b7bbcacfe39f20bc7bea9514a505c74f, so that errors can be caught as soon as they happen now.\n\nI decided against adding 565 support for now because it's a little bit application specific how the resulting values are handled. Probably best to transform to RGBA8888 before passing to the native code, or make custom changes to tensorflow-jni.cc to handle the bitmap appropriately.\n"]}, {"number": 2596, "title": "Udacity readme docker command should not remove container", "body": "The Udacity readme at \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/README.md\nsuggests starting Docker like this:\n\n`docker run -p 8888:8888 -it --rm b.gcr.io/tensorflow-udacity/assignments:0.5.0`\n\nThe `--rm` causes the container to get removed once you end the session, which deletes all your progress so far in the class. It would be better to not suggest `--rm` so that the docker container can then be resumed at a later time.\n", "comments": ["Fair enough. I'll take a PR if you'd like.\n", "This issue should probably be closed.\n"]}, {"number": 2595, "title": "Immediate Mode execution in tensorflow", "body": "Implementation of immediate mode execution in TensorFlow. The idea is to wrap original tensorflow API, but provide session/run management logic so that commands execute immediately, and graph caching to avoid modifying graph when same op is run repeatedly. Data is kept in TensorFlow runtime whenever possible using persistent tensors and transferred to Python runtime on demand when needed for printing/control flow.\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib import immediate\n\ntfi = immediate.Env(tf).tf        # wraps \"tf\" namespace, saves it as \"tfi\"\nval1 = tfi.ones((3,))             # creates tensor on GPU\nval2 = val1 + val1                # runs into tf.add, keeps result on GPU\nval3 = tfi.ones((2, 2, 2, 2))\nval4 = tfi.nn.conv2d(val3, val3, [1, 1, 1, 1], \"VALID\")   # run CuDNN conv2d\nif (tfi.reduce_sum(val3)>0.):     # runs reduce_sum on GPU, transfers bool to CPU\n  print(val4)                     # transfers whole tensor to CPU for display\n\n```\n\nThis is a single commit rebase of a previous pull request from https://github.com/tensorflow/tensorflow/pull/2346\n\n@keveman \n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "The failures are due to https://github.com/tensorflow/tensorflow/issues/2586, @yuanbyu  are you able to reproduce?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Hm, I may need to move this to a new pull request, every time I do rebase, it includes all the changes fast-forwarded by the rebase into the pull request.\n\nI've asked question here with all the commands I've used to get into this weird state:\nhttp://stackoverflow.com/questions/37707605/doing-rebase-adds-replays-master-branch-commits-into-my-pull-request\n", "Sorry, I'm opening new request since I don't know how to fix this:\n\nhttps://github.com/tensorflow/tensorflow/pull/2747\n\nIn my current work-flow, when I rebase, every commit that's included during fast forwarding goes into the pull request. Here's the sequence of commands I've been using so that people know not to do this: http://stackoverflow.com/questions/37707605/doing-rebase-adds-replays-master-branch-commits-into-my-pull-request\n"]}, {"number": 2594, "title": "add tf.assert for GPU (maybe also host-memory Const[string] for GPU) was: dynamic_rnn GPU support error ", "body": "When attempting to place dynamic_rnn on the gpu, I get the following error:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'RNN/Assert/data_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n         [[Node: RNN/Assert/data_2 = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values:  but saw shape: >, _device=\"/device:GPU:0\"]()]]\n```\n\nFull traceback:\n\n```\nTraceback (most recent call last):\n  File \"test_model2_parallel_buffered.py\", line 84, in <module>\n    sess.run(init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 333, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 573, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 648, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 668, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'RNN/Assert/data_2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n         [[Node: RNN/Assert/data_2 = Const[dtype=DT_STRING, value=Tensor<type: string shape: [] values:  but saw shape: >, _device=\"/device:GPU:0\"]()]]\nCaused by op u'RNN/Assert/data_2', defined at:\n  File \"test_model2_parallel_buffered.py\", line 47, in <module>\n    tower_logits_t = model.inference(frames_t, seq_length_t, BATCH_SIZE)\n  File \"/home/woodward/ml/football_plays/model2.py\", line 73, in inference\n    _, rnn_state_final_t = tf.nn.dynamic_rnn(rnn_cell, rnn_inputs_t, sequence_length=seq_length_t, initial_state=rnn_initial_state_t, swap_memory=True, time_major=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 567, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 562, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/logging_ops.py\", line 58, in Assert\n    return gen_logging_ops._assert(condition, data, summarize, name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 37, in _assert\n    summarize=summarize, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 410, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 664, in convert_n_to_tensor\n    ret.append(convert_to_tensor(value, dtype=dtype, name=n, as_ref=as_ref))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1224, in __init__\n    self._traceback = _extract_stack()\n```\n\nIt seems to have some problem with a tensor shape, but I can't deduce why. This error does not occur if I enclose the tf.nn.dynamic_rnn(...) call in a tf.device('/cpu:0') statement. I am on commit 0050a205bc5521a563ee66baa3b73373d4c0e62e from mid last week.\n\nI am doing data parallel processing and am trying to keep as much as possible on each GPU.\n", "comments": ["It looks like you might need `allow_soft_placement=True` in your Session configuration (`tf.Session(config=tf.ConfigProto(allow_soft_placement=True))`), but let's check with @ebrevdo to see if there's a more systematic solution.\n", "Thank you. I agree, as with explicitly placing the dynamic_rnn call on the cpu, the error does not occur when I include soft_placement in the config. But my goal is actually to put it on the gpu, if possible.\n", "@markpwoodward: If you do soft placement, it will put anything it can on the GPU and use the CPU for the rest, which is exactly what you want here (since assert is CPU only).\n", "`dynamic_rnn` runs some dynamic assertions on the input, making sure things are within bounds, etc.  the error you're seeing is the static assertion message (a string) trying to get allocated on the GPU.  Since we don't mix strings with GPUs, we lack a const string op mapped to the GPU.  That's the error you're seeing.  I think it's fine for error checking and error strings to sit on the CPU, but perhaps we should consider a Const[string] that is declared for GPU but stores data on the host?  Either way, you should be fine with soft placement here.  @girving  @mrry  think host-memory const string GPU op is a reasonable thing?\n", "@ebrevdo: We should avoid adding more lies about which things are on the CPU vs. GPU (too many of these related to int32 already).  In any case, the actual problem here is that `tf.assert` itself is CPU only.  Maybe the ideal fix is to add a GPU version of `tf.assert`, specifying HostMemory for the string argument?\n", "Thank you. This makes sense. I do understand the distinction between hard placement and soft placement.\n\nSeeing that something in dynamic_rnn did not have a GPU implementation (requiring soft placement) made me hope that, if only there were a GPU implementation for that operation, then I would get a huge speedup. :) Of course, those operations might not have any impact on performance.\n\nI should probably just look at the graph for dynamic_rnn and see what was placed on the CPU, rather than forcing dynamic_rnn onto the GPU and then posting a github issue when it complains. My apologies. Feel free to close this.\n", "@markpwoodward: No worries, thank you for the report!  We might close, but we might also rename it to \"add tf.assert for gpu\" in order to fix the other underlying issue. \n", "Thanks!\n", "@girving can you assign to appropriate folks?\n", "I'll leave it as contributions welcome for now.  Happy to accept PRs if someone wants to make a GPU version of `tf.assert`!\n", "I can't really quite understand the implications of this. Does this prevent LSTMs from being placed on the GPU?\r\n\r\nI'm trying to do multi-GPU which requires explicit assignment of GPU devices, and suddenly when I start explicitly designing devices I start hitting this assert op that's apparently not supported on GPU. But I don't understand how it was running fine on a single GPU in the first place? Or does it mean that the RNN code was running on the CPU? I can't believe that would be the case. But if this were to be the case, wouldn't it kind of be...a big deal and a big enough performance hit to be worth looking at? I haven't really figured out what elements of the LSTM I need to be \"placing\" on the GPU for best performance. My LSTMCells are on the GPU but because this function always gives errors I have to leave it on the CPU, which also limits me from placing functions depending on it on the GPU as well, eventually killing the performance. What I do know is that I commented out the assert, explicitly defined GPU devices and multi-GPU code is working very well now. But I am left with a lot of confusion as well with what's being placed where.\r\n\r\nI'm not sure what it takes to make an assert op on the GPU, but it would be very nice in the meantime if this were documented somehow or if an alternate dynamic_rnn function were provided without the assert so that we could reap performance benefits of GPU at our own risk until assert on GPU works properly. It does sound messy, but from the developer's point of view, networks using Bi-LSTM and CRF (crf code also hits this assert) are exactly the ones we might want to use multi-GPU with and there are some workloads that have a big need for multi-GPU. Again, I'm not sure if this has implications for single GPU also or just multi-GPU, but I'm just relaying my experiences.\r\n\r\nUpdate:   I realized there was an issue with my soft placement that caused the assert to still occur. Apparently I didn't put soft placement config every time I called tf.Session and even when I put it in the training part, the assert still occurred. odd.. anyways the performance seems to be about the same as with commenting out the assert, so I'm assuming the solution for this is to just use soft device placement and that there is no big issue with using RNN on GPU", "You need to enable soft placement when creating the session.  It's a\nconfiguration o parameter in tf.ConfigProto.\n\nOn Fri, Dec 15, 2017, 8:15 AM Andrew Matteson <notifications@github.com>\nwrote:\n\n> I can't really quite understand the implications of this. Does this\n> prevent LSTMs from being placed on the GPU?\n>\n> I'm trying to do multi-GPU which requires explicit assignment of GPU\n> devices, and suddenly when I start explicitly designing devices I start\n> hitting this assert op that's apparently not supported on GPU. But I don't\n> understand how it was running fine on a single GPU in the first place?\n>\n> What I do know is that I commented out the assert, explicitly defined GPU\n> devices and multi-GPU code is working very well now. But I am left with a\n> lot of confusion as well.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2594#issuecomment-352045893>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0vOVI2fEUTYAYZUX9mjuW3gaG8iks5tAps3gaJpZM4Iq0IV>\n> .\n>\n", "Closing as soft placements solves the issue."]}]