[{"number": 9366, "title": "Check in tensorboard_bower_dependency_sync.py file", "body": "Hi TF Developers,\r\n\r\nCould you kindly check in the `tensorboard_bower_dependency_sync.py` file to the repo? We need to customize the build process and having that file would be really helpful. Thanks a lot in advance!\r\n\r\nYing", "comments": ["I don't think we still use the tensorboard_bower_dependency_sync to manage our dependencies, since we aren't updating bower. Pinging @jart for comment.", "That script is built in a way that's very specific to how Google's internal source code repository works, so it would not be something we could open source.\r\n\r\nWould it be possible to tune the build by hand?\r\n\r\nI also must warn you that the TensorBoard build process is very much in a state of flux right now. We're in the process of reworking the whole thing. So caution is advised."]}, {"number": 9365, "title": "TensorFlow Retrain  Android Error", "body": "I have my own custom created .pb file and string text using TensorFlow Retrain Model\r\nhttps://www.tensorflow.org/versions/r0.10/how_tos/image_retraining/\r\nThen i have created .so file By building tensorflow\r\nbazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\n \r\nWhen i load both into androidstudio and run the app i get below error\r\n\r\nNo Operation named [input] in the Graph\r\n                                                                                  at org.tensorflow.Session$Runner.operationByName(Session.java:297)\r\n                                                                                  at org.tensorflow.Session$Runner.feed(Session.java:115)\r\n                                                                                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.addFeed(TensorFlowInferenceInterface.java:439)\r\n                                                                                  at org.tensorflow.contrib.android.TensorFlowInferenceInterface.fillNodeFloat(TensorFlowInferenceInterface.java:188)\r\n                                                                                  at com.mindorks.tensorflowexample.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:146)\r\n                                                                                  at com.mindorks.tensorflowexample.MainActivity$1.onPictureTaken(MainActivity.java:81)\r\n                                                                                  at com.flurgle.camerakit.CameraView$CameraListenerMiddleWare.onPictureTaken(CameraView.java:296)\r\n                                                                                  at com.flurgle.camerakit.Camera1$2.onPictureTaken(Camera1.java:185)\r\n                                                                                  at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1201)\r\n                                                                                  at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                                                                                  at android.os.Looper.loop(Looper.java:135)\r\n                                                                                  at android.app.ActivityThread.main(ActivityThread.java:5253)\r\n                                                                                  at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                                  at java.lang.reflect.Method.invoke(Method.java:372)\r\n                                                                                  at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:900)\r\n                                                                                  at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:695)\r\n\r\nI have created input_node as \"Mul:0\" and Out_put node as \"Final_result\" while building\r\n\r\n", "comments": ["If you switch from Inception-v1 to Inception-v3 you'll also need to update the values in ClassifierActivity according to the comment there. The name of the input node is no longer \"input\".", "Yes i very much agree with you. I have made corresponding changes in classifierActivity as well..Like below\r\n\r\nprivate static final int NUM_CLASSES = 1001;\r\n  private static final int INPUT_SIZE = 224;\r\n  private static final int IMAGE_MEAN = 117;\r\n  private static final float IMAGE_STD = 1;\r\n  private static final String INPUT_NAME = \"Mul:0\";\r\n  private static final String OUTPUT_NAME = \"final_result:0\";\r\n\r\n  private static final String MODEL_FILE = \"file:///android_asset/tensorflow_inception_graph.pb\";\r\n  private static final String LABEL_FILE =\r\n      \"file:///android_asset/imagenet_comp_graph_label_strings.txt\";\r\n\r\n\r\nand in ImageClassifier.java also i have made the same changes as well\r\n\r\nprivate static final int NUM_CLASSES = 1001;\r\n  private static final int INPUT_SIZE = 224;\r\n  private static final int IMAGE_MEAN = 117;\r\n  private static final float IMAGE_STD = 1;\r\n  private static final String INPUT_NAME = \"Mul:0\";\r\n  private static final String OUTPUT_NAME = \"final_result:0\";\r\n\r\n  private static final String MODEL_FILE = \"file:///android_asset/tensorflow_inception_graph.pb\";\r\n  private static final String LABEL_FILE =\r\n      \"file:///android_asset/imagenet_comp_graph_label_strings.txt\";\r\nI then build the tensorflow..", "If you've updated the code as described I'm not sure what's going on, as the logcat clearly shows that it's looking for a node named \"input\". Are you sure the APK is actually getting rebuilt properly and pushed to device?\r\n\r\nAlso you should only have to edit ClassifierActivity.java to update the default values. Just adding new constant fields to TensorFlowImageClassifier.java (if that's what you meant) won't do anything by itself.", "Okay..DO i need to change anything c++ code ..Like add input_name or out_put name in any .cc code so that it links between java and c++ code?\r\n", "No, changing the graph to use is done entirely in Java in the demo src code.", "Okay..Thanks for replying ..Just confirm me is this correct...Sorry to bother\r\n1. I will retrain my custom model generat .pb file and txt file and give input_Name and out_putname as \"Mul\" and \"final_result\" during label_image build.\r\n2. Then as per your help will change ClassifierActivity.java  to update Defaut values. Then build to generate infernce.so file by \r\n\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n\r\n3. Create Java jar file by \r\n\r\nbazel build //tensorflow/contrib/android:android_tensorflow_inference_java\r\n\r\n4. Open android studio create my sample app. Add jar file in libs and .so file in jniLibs and graphs in assets folder.\r\n\r\nNow run the app..Correct right?\r\n", "The build.gradle file should handle all of the building for you, assuming you're running on Linux. You should just need to update the values in ClassifierActivity.java to point to your retrained graph assuming you have Bazel installed correctly etc.", "The problem is i am building in linux VM and then need to load on android studio in windows.. Could you help me with that?\r\n\r\nIs there a tutorial for that?", "Building the native libraries stays exactly the same regardless of the\ngraph you use (unless you're building with selective registration). To\nbuild android tensorflow on windows follow this workaround:\nhttps://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600\n\nOr just manually add the libraries as the comment above suggests.\n\nOn Apr 22, 2017 12:10 AM, \"ManoharKowshik\" <notifications@github.com> wrote:\n\n> The problem is i am building in linux VM and then need to load on android\n> studio in windows.. Could you help me with that?\n>\n> Is there a tutorial for that?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9365#issuecomment-296353798>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADOGsTBLJESIVRX1XQWj63jJg7ZntmwTks5ryaf9gaJpZM4NEXWz>\n> .\n>\n", "Code is working..Thanks for the info", "@andrewharp \r\nI try to use a model which require multi-input , how can I configure multi-input for this model in tensorflow android?[details is here](https://stackoverflow.com/questions/44693500/tensorflow-android-demo-use-cumstom-model-with-multi-input)"]}, {"number": 9364, "title": "Error", "body": "Trying to get tensorflow to work for my final bit on dissertation but anytime I try to install it in virtual box ubuntu linux 64bit, I get this error I've tried the CPU/GPU version but I can only seem to get the python3 version working but I need the python2.\r\n\r\nI get this error message each time..  \r\n\r\n 99% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44.1MB 14.8MB/s eta 0:00:01Exception:\r\nTraceback (most recent call last):\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/commands/install.py\", line 353, in run\r\n    wb.build(autobuilding=True)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 620, in _prepare_file\r\n    session=self.session, hashes=hashes)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 821, in unpack_url\r\n    hashes=hashes\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 659, in unpack_http_url\r\n    hashes)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 882, in _download_http_url\r\n    _download_url(resp, link, content_file, hashes)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 605, in _download_url\r\n    consume(downloaded_chunks)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 870, in consume\r\n    deque(iterator, maxlen=0)\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 571, in written_chunks\r\n    for chunk in chunks:\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/utils/ui.py\", line 139, in iter\r\n    for x in it:\r\n  File \"/home/tay/tensorflow/local/lib/python2.7/site-packages/pip/download.py\", line 560, in resp_read\r\n    decode_content=False):\r\n  File \"/home/tay/tensorflow/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 432, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"/home/tay/tensorflow/share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/urllib3/response.py\", line 380, in read\r\n    data = self._fp.read(amt)\r\n  File \"/home/tay/tensorflow/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 63, in read\r\n    self._close()\r\n  File \"/home/tay/tensorflow/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/filewrapper.py\", line 50, in _close\r\n    self.__callback(self.__buf.getvalue())\r\n  File \"/home/tay/tensorflow/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/controller.py\", line 275, in cache_response\r\n    self.serializer.dumps(request, response, body=body),\r\n  File \"/home/tay/tensorflow/share/python-wheels/CacheControl-0.11.7-py2.py3-none-any.whl/cachecontrol/serialize.py\", line 87, in dumps\r\n    ).encode(\"utf8\"),\r\nMemoryError", "comments": ["Are you using ubuntu 16? My guess is that you need to increase memory allocation on virtualbox.", "I ended up installing a real version of linux but i assume memory allocation was the reason!\r\n\r\nThanks for the reply.\r\n"]}, {"number": 9363, "title": "When I use the Bazel to compile the image_retraining project ,the error occurred.", "body": "Please go to Stack Overflow for help and support. http://stackoverflow.com/questions/tagged/tensorflow\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:no\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*:Linux Ubuntu 16.04.2\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version* (use command below):tensorflow-1.1.0rc1-cp35-cp35m-linux_x86_64.whl\r\n- *Bazel version (if compiling from source)*:0.4.5\r\n- *CUDA/cuDNN version*:no\r\n- *GPU Model and Memory*:no\r\n- *Exact command to reproduce*:yes\r\n\r\nYou can collect some of this information using our environment capture script https://github.com/tensorflow/tensorflow/blob/master/tools/\r\nYou can collect the TensorFlow version with\r\n```sh\r\npython -c \"import tensorflow as tf; print (tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n\r\n### Describe the problem clearly\r\nWhen I use the Bazel to compile the image_retraining project when the error occurred, how to solve, spec.json, head, branch_ref these documents are not in that directory, did you made some change that now cannot compile directory:\r\nwei@wei-TA960:~/myprogramfile/TF-0.12/tensorflow-r0.12-1$ bazel build tensorflow/examples/image_retraining:retrain\r\nERROR: /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/core/BUILD:1117:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/wei/myprogramfile/TF-0.12/tensorflow-r0.12-1/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["Are you sure you are running `1.1`? It looks from the paths that you have `0.12`. From the form you said you have installed from binary, but this is a bazel build command?", "Looks like you did not run `./configure` before building.\r\nPlease make sure to run `./configure` before running `bazel build`"]}, {"number": 9362, "title": "[Bug report] ValueError: Variable does not exist on tf.layers.dense(reuse=True) after graph switch", "body": "### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: macOS Sierra\r\n- *TensorFlow installed from (source or binary)?*: binary, pip\r\n- *TensorFlow version* (use command below): 1.0.0\r\n- *Bazel version (if compiling from source)*: None\r\n- *CUDA/cuDNN version*: not using GPU\r\n- *GPU Model and Memory*: not using GPU\r\n- *Exact command to reproduce*: None\r\n\r\n\r\n### Describe the problem clearly\r\nI built 2 networks that share some layers/variables. The best way that I learned was to treat them as separate graphs. I wasn't sure about whether 2 graphs can share variables but I tried `tf.get_variable(resue=True)` anyway. At least my program show create 2 separate graph with no variable sharing. But a ValueError(variable does not exist) was thrown instead.\r\n\r\n### Source Code / Logs\r\n* source code:\r\n```python\r\n# test code\r\nmodel = DPGModel(3, 4)\r\n\r\n# source code\r\nclass DPGModel(object):\r\n    def __init__(self, state_size, action_size):\r\n        self.sess = tf.Session()\r\n        self.state_size = state_size\r\n        self.action_size = action_size\r\n        self.create_network()\r\n        # other irrelevant code\r\n\r\n    def create_network(self):\r\n        self.state_tensor = tf.placeholder(tf.float64, [None, self.state_size], name=\"state\")\r\n        self.action_tensor = tf.placeholder(tf.float64, [None, self.action_size], name=\"action\")\r\n        self.actor_graph = tf.Graph()\r\n        with self.actor_graph.as_default():\r\n            print tf.get_variable_scope()\r\n            state_h1 = tf.layers.dense(inputs=self.state_tensor, units=64, activation=tf.nn.relu, name=\"state_h1\",\r\n                                       reuse=True)\r\n            state_h2 = tf.layers.dense(inputs=state_h1, units=32, activation=tf.nn.relu, name=\"state_h2\", reuse=True)\r\n            self.policy_tensor = tf.layers.dense(inputs=state_h2, units=self.action_size, activation=tf.nn.softmax,\r\n                                                 name=\"policy\")\r\n\r\n        self.critic_graph = tf.Graph()\r\n        with self.critic_graph.as_default():\r\n            print tf.get_variable_scope()\r\n            state_h1 = tf.layers.dense(inputs=self.state_tensor, units=64, activation=tf.nn.relu, name=\"state_h1\",\r\n                                       reuse=True)\r\n            state_h2 = tf.layers.dense(inputs=state_h1, units=32, activation=tf.nn.relu, name=\"state_h2\", reuse=True)\r\n            action_h1 = tf.layers.dense(inputs=self.action_tensor, units=64, activation=tf.nn.relu, name=\"action_h1\")\r\n            action_h2 = tf.layers.dense(inputs=action_h1, units=32, activation=tf.nn.relu, name=\"action_h2\")\r\n            fc = tf.layers.dense(inputs=[state_h2, action_h2], units=32, activation=tf.nn.relu,\r\n                                 name=\"fully_connected\")\r\n            self.value_tensor = tf.layers.dense(inputs=fc, units=1, activation=None, name=\"value\")\r\n```\r\n\r\n* error log:\r\n```\r\n<tensorflow.python.ops.variable_scope.VariableScope object at 0x10fd20790>\r\nTraceback (most recent call last):\r\n  File \"/Users/niyan/code/routerRL/test.py\", line 16, in <module>\r\n    model = DPGModel(state_dim, action_dim)\r\n  File \"/Users/niyan/code/routerRL/DPGModel.py\", line 10, in __init__\r\n    self.create_network()\r\n  File \"/Users/niyan/code/routerRL/DPGModel.py\", line 37, in create_network\r\n    reuse=True)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 216, in dense\r\n    return layer.apply(inputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 303, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 269, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 123, in build\r\n    trainable=True)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 258, in variable_getter\r\n    variable_getter=functools.partial(getter, **kwargs))\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 208, in _add_variable\r\n    trainable=trainable and self.trainable)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 657, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable state_h1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["Typically, we use the same graph, but use a name scope.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@drpngx I think it **is** a bug. At least there should be no ValueError thrown, or the document saying '_you should never attempt to share variables among different graphs_'", "Hi @leckie-chn , were you able to fix this? I'm having the same problem with tf.layers.dense.. "]}, {"number": 9361, "title": "Fix a typo.", "body": "Fix typo", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9360, "title": "Understanding cast() in tensorflow", "body": "### System Information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary):**\r\nInstalled from source (v1.0.1)\r\n- **TensorFlow version (use command below):**\r\n('v1.0.1-0-ge895d5c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\n bazel release 0.4.4-2017-04-10 (@80a07b5)\r\n- **CUDA/cuDNN version:**\r\nIn disable mode\r\n- **Exact command to reproduce**:\r\nbazel test //tensorflow/python/kernel_tests:cast_op_test\r\n\r\n\r\n### Describe the problem clearly \r\nThis is regarding failure of test case testInfNan in cast_op_test.py file.While executing this test case on ppc64le, it was observed that following line returns unexpected results:\r\n\r\n`self._compare(np.inf, np.int32, i4.min, False)`\r\n\r\ni4.min value on x86 as well as on ppc64le is -2147483648. However \"np.inf\" on x86 is \"signed\" by default whereas on ppc64le it is \"unsigned\" by default. To make the results compatible with x86, somehow np.inf should be cast as \"signed\" on ppc64le. In my opinion there could be two ways of doing this.\r\n\r\n1. Use a proper cast (python equivalent of \"(signed int) var\" in C) which    would always interprete np.inf as of type \"signed\" on ppc64le\r\n-- or --\r\n2. if we are on ppc64le platform, when dealing with np.inf, convert it explicitly to signed as \"-np.inf\" and then perform subsequent operations\r\n\r\nThough I have not yet decided on which one to implement, I am trying to find a right place first to put this fix in tensorflow code. I guess the right place would be somewhere in the code related to following 2 lines in cast_op_test.py (line# 57 and 58, in function _cast):\r\n\r\n```\r\nval = constant_op.constant(x, self._toDataType(np.array([x]).dtype))\r\nreturn math_ops.cast(val, self._toDataType(dtype), name=\"cast\").eval()\r\n\r\n```\r\nHowever I am unable to grasp code details about constant() in python/framework/constant_op.py and cast() in python/ops/math_ops.py, similarly there is REGISTER_OP(\"Cast\") in core/ops/math_ops.cc which I guess is the heart of cast functionality. Is my understanding correct?\r\n\r\nSo if I have to implement the changes for ppc64le, which could be the right place to do so?\r\n\r\n### Source Code / Logs\r\n```\r\n`$ bazel test //tensorflow/python/kernel_tests:cast_op_test\r\n\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py:62: ComplexWarning: Casting complex values to real discards the imaginary part\r\n  np_ans = x.astype(dtype)\r\n....F.W tensorflow/core/framework/op_kernel.cc:983] Unimplemented: Cast int64 to string is not supported\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n........\r\n======================================================================\r\nFAIL: testInfNan (__main__.CastOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py\", line 150, in testInfNan\r\n    self._compare(np.inf, np.int32, i4.min, False)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/cast_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/cast_op_test.py\", line 124, in _compare\r\n    x, dst_dtype, use_gpu=use_gpu), dst_dtype(expected))\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 425, in assert_equal\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nItems are not equal:\r\n ACTUAL: 2147483647\r\n DESIRED: -2147483648\r\n\r\n----------------------------------------------------------------------\r\nRan 14 tests in 2.485s\r\n\r\nFAILED (failures=1)`\r\n```", "comments": ["We have been aligned with numpy compatibility, but in that case, numpy seems to result in inconsistent behavior.\r\n\r\nMy opinion would be to submit a patch for ppc upstream in numpy to make it have consistent behavior.\r\n\r\n@aselle or @martinwicke might have some comment.", "I agree, this looks like a bug in numpy on PPC, although it is perfectly possible that the numpy maintainers have good reasons for this behavior.\r\n\r\nI would not want to deviate from numpy for things like this, but if the test failure is annoying, we can special case that test to omit this check for ppc.", "Thanks for the pointers, I will look into numpy. ", "I could able to reproduce this issue in numpy, now I will take this with numpy community.\r\n\r\nthanks all for your comments/suggestions!", "Hi @drpngx , @martinwicke,\r\n\r\nI have discussed this issue with numpy community - https://github.com/numpy/numpy/issues/9040\r\n\r\n**Issue** :  np.float64(\"inf\").astype(np.int32) is negative on x86 but positive on ppc64le\r\n\r\n**They mentioned as follows** -\r\n\r\n1. Note that doing this cast incurs undefined behaviour, so it's hard to say which platform is incorrect here.I'd argue that we should just issue a warning and leave the platform-specific behaviour.\r\n\r\n2. This is also true of any C program that tries to cast float to int. If the author of a python package included a C extension that does this cast, they still have the problem.\r\n The same argument you use for \"we should fix this is numpy, not in the user package\" can be applied to \"we should fix this in C, not in numpy\". Presumably for speed reasons, that argument was rejected during the design of C.\r\nHaving said that, we already diverge from C behaviour in some place (eg, integer promotion for arithmetic and comparison), so it wouldn't be unreasonable to add special casing here.\r\n\r\nTo understand more clearly, please go through numpy discussion i.e . https://github.com/numpy/numpy/issues/9040\r\n\r\nSo as per this discussion -  I think we should add the power specific condition in tensorflow source code  : https://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/python/kernel_tests/cast_op_test.py#L150-L151\r\n\r\n```\r\ndef testInfNan(self):\r\n    i4 = np.iinfo(np.int32)\r\n    i8 = np.iinfo(np.int64)\r\n\r\n    self._compare(np.inf, np.float32, np.inf, False)\r\n    self._compare(np.inf, np.float64, np.inf, False)\r\n    if sys.byteorder == \"big\":\r\n      self._compare(np.inf, np.int32, i4.max, False)\r\n      self._compare(np.inf, np.int64, i8.max, False)\r\n    else:\r\n        self._compare(np.inf, np.int32, i4.min, False)\r\n        self._compare(np.inf, np.int64, i8.min, False)\r\n\r\n```\r\nIt could be changed to : \r\n```\r\n`def testInfNan(self):\r\n    i4 = np.iinfo(np.int32)\r\n    i8 = np.iinfo(np.int64)\r\n\r\n    self._compare(np.inf, np.float32, np.inf, False)\r\n    self._compare(np.inf, np.float64, np.inf, False)\r\n    if sys.byteorder == \"big\":\r\n      self._compare(np.inf, np.int32, i4.max, False)\r\n      self._compare(np.inf, np.int64, i8.max, False)\r\n    else:\r\n      if platform.machine() == \"ppc64le\":\r\n        self._compare(-np.inf, np.int32, i4.min, False)\r\n        self._compare(-np.inf, np.int64, i8.min, False)\r\n      else:\r\n        self._compare(np.inf, np.int32, i4.min, False)\r\n        self._compare(np.inf, np.int64, i8.min, False)\r\n`\r\n```\r\n\r\nAfter this change test is passing on ppc64le. Please provide your comments/suggestions on this.Thanks!", "Hi @drpngx , @martinwicke,\r\n\r\nShould I raise a PR for this ?", "That is fine. Please add excessive comments to that test modification, including references to numpy/numpy#9040 that you linked.", "Code changes submitted for review in PR #10522 to fix this.\r\nOnce approved I will push the changes to Github so PR #10522 should resolve this.", "Community accepted the changes and merged in master (relevant PR #10522 and merged commit d0d2308). Hence closing this issue as resolved."]}, {"number": 9359, "title": "Equality operator not overloaded", "body": "It's unexpected and a gotcha that some comparison operators (`>`, `<`, `<=`, `=>`) are overloaded but not all (`==`, `!=`). Is there a particular reason for this? I think either all standard operators should be overloaded, or none at all.\r\n\r\n```python\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tensor = tf.zeros((3,5))\r\n\r\nIn [3]: tensor > 0\r\nOut[3]: <tf.Tensor 'Greater:0' shape=(3, 5) dtype=bool>\r\n\r\nIn [4]: tensor == 0\r\nOut[4]: False\r\n```\r\n\r\nThis is on TensorFlow 1.1.0rc2.", "comments": ["I think the reason is that we thought it might be confusing to have statements like `if foo`, which check for `None`.\r\n\r\n@aselle may have some comment.\r\n\r\n", "Surely for those cases it would be better with the less ambiguous `if foo is None:`\r\n\r\n", "The google style guide mandates implicit [comparisons](https://google.github.io/styleguide/pyguide.html?showone=True/False_evaluations#True/False_evaluations), but not for integers. A lot of code has been written that way.\r\n\r\nWhatever the case, reading the description again, it looks to me that case `4` doesn't make sense in the current state no matter what.", "This may be a complication of fact that tensors can be used as keys in dictionaries, which I believe use `==` to find the matching object with the same hash:\r\nhttps://docs.python.org/2/reference/datamodel.html#object.__hash__\r\n\r\n`if foo` actually calls `bool(foo)` -> `foo.__bool__()`, so that's something different.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "but how would you suggest to write the code if we want `== `operator?", "`tf.equal` I guess? I wish this was reopened though.", "@alextp @josh11b @drpngx to check and see if we would like to reopen this. But it has been almost one year with no activity, so it seems to be very low priority. ", "I've seen several bugs caused by this inconsistency. I even managed to be tripped up by this myself just last week, and I would not be surprised if these bugs are very common across people's experiments,  where the Python expression always evaluates to `False` and an implicit `tf.convert_to_tensor` on the expression just becomes a runtime constant.", "We cannot override == and != because that would make tensors no longer work as dictionary keys, which would break a lot of internal and external code :-/", "Agreed with @alextp, this is unfortunately a non-starter.\r\n\r\nThe most obvious use of tensors as dictionary keys is placeholders in `feed_dict`.", "I just spent more than an hour debugging this. :/ \r\n\r\nStrongly agree that the inconsistency is misleading (I saw some code with `a <= b` and modified it with `a==b` and always got `0`).", "This is an _extremely_ tricky wart.\r\n\r\nFor TensorFlow 2.0 maybe tensors could stop being hashable and instead users would have to explicitly call e.g. `tf.constant(9).hashable()`? That would free up `==` and `!=` for the expected use as overloaded operators for graph construction.\r\n\r\nThat's unfortunately the best I could come up with.\r\n\r\n```py\r\nx = tf.placeholder(...)\r\nfeed_dict = {x.hashable(): np.array(...)}\r\n```", "Would it be possible to at least produce a warning?", "If we add a warning it'll trigger all the time for innocuous code since TF internally uses hash tables keyed by tensors in many places :-/", "Stumbled upon this issue when I found out `tensor_x == 0` returns a `False`...\r\nWell, here goes my time :(\r\nCan't we at least warn people about the operator's inconsistent behavior? Maybe somewhere in the doc, suggest them to use tf functions instead.", "It's `String` comparisons in Java all over again. :worried: ", "We have recently been looking at this and been trying things to work around the hashable issue, but we haven't found anything that doesn't break a lot of code. In addition to the hashing issue, I've also seen problems with code like:\r\n\r\ntensorflow/python/ops/embedding_ops.py, line 117, in _embedding_lookup_and_transform\r\n    if params is None or params in ((), []):\r\n\r\nwhich is relying on Tensor.__eq__ being the same as \"is\" when params is a Tensor.", "This behavior could benefit from documentation.\r\n\r\nNo mention could be found in the following pages, where, presumably, a user may expect to discover it:\r\n\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/math/equal\r\nhttps://www.tensorflow.org/versions/r1.14/api_docs/python/tf/math/not_equal\r\nhttps://www.tensorflow.org/guide/eager\r\nhttps://www.tensorflow.org/guide/autograph\r\nhttps://www.tensorflow.org/alpha/guide/eager\r\nhttps://www.tensorflow.org/alpha/guide/autograph\r\nhttps://www.tensorflow.org/alpha/tutorials/eager/basics\r\nhttps://www.tensorflow.org/alpha/tutorials/eager/tf_function\r\nhttps://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises\r\nhttps://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb\r\n\r\nThe palm being awarded to:\r\nhttps://www.tensorflow.org/alpha/tutorials/eager/basics\r\nwhich contains the sentence \"Operator overloading is also supported.\" with no qualification whatsoever. It also contains a section titled \"NumPy Compatibility\" which omits to mention this tiny little wrinkle (and yes, I understand this section is technically about converting numpy arrays. but readers infer things from what could be seen as subtext here: \"it's basically the same\").\r\n\r\nA lot of the examples casually use tf.equal(), ==, >= in the same breadth, without even hinting to the subtelty underlying their respective use.\r\n\r\nThe only tangeantial reference I could find is with respect to autograph.experimental.Feature.EQUALITY_OPERATORS mentioned in one of the links above, although the need for this is not explained in the least. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/experimental/Feature states that EQUALITY_OPERATORS is going away as support will be directly in the tf.Tensor class, contradicting @josh11b 's comment above.\r\n\r\nIf I may, I would advise bright red flashing letters in a giant pulsating warning box.\r\n\r\nP.S. @carlthome At least in Java, it's not like a >= b works and a == b doesn't...", "> P.S. @carlthome At least in Java, it's not like a >= b works and a == b doesn't...\r\n\r\nTrue, this one is worse. :cry: ", "Frankly, you have a great opportunity to fix this historical wart with TensorFlow 2. Make `Tensor.__hash__()` throw an error, and require the use of `Tensor.hashable()` or `Tensor.handle()`. Debug builds can instrument `__contains__` on standard containers to catch the use of `in` with Tensors items. You will break code, but in a (mostly) safe manner. And will save a lot of trouble down the road. And document (!) the change in TensorFlow 2 release notes.\r\n\r\nIf you really must, only enable the new sane definition of `__eq__` and `__ne__` in eager execution mode and within autograph transformations where the use of Tensor as an abstract handle is a lot less likely (and sensible) than in the traditional manual graph building mode.\r\n\r\nInternal Google code can be updated using static analysis. Internal pedestrian constraints shouldn't drive the design of a tool aiming for universality. Presumably, if I understand TensorFlow's growth plans correctly, a lot more TensorFlow code is going to be written in the future than was written to this day.", "Thank you everyone for requesting this feature. Due to feedback from the community we have built this feature behind the `tf.enable_tensor_equality()` toggle. This will be enabled by default in 2.0.\r\n\r\nPlease provide any feedback on this feature with the upcoming nightly builds.", "Fantastic! Looking forward to trying it out @jaingaurav. But so how was the dict key hash usage rectified? :interrobang: ", "@carlthome: We solved dictionary hashing in a couple of different ways:\r\n1) With short-lived/locally scoped dictionaries, using `id()` as the key is fine\r\n2) Exposed a wrapper object that can be used as a key with a reference to the Tensor: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Tensor#experimental_ref\r\n3) Changed certain dictionaries to be a map of `id(key)` to a tuple of `(key, value)`\r\n4) Refactored code to avoid unnecessary use of a dictionary.\r\n\r\nAll of these options are reasonable for user code to use as well and we have been able to successfully apply the different approaches to various projects. As a result of enabling tensor equality we uncovered a number of bugs in code as well as tests where code wasn't really testing what it thought it was testing.", "The openness to community feedback is truly appreciated. Now if you could\nhave these discussions in the open, TensorFlow could become a real\nopen-source project!\n\nYou would go from having a community of users to having a community of\ndevelopers. As it is, it's impossible to contribute more than small bug\nfixes to a project that doesn't open up its technical discussions and offer\ndetailed, day-to-day insights into its work and, most importantly, its\nthinking.\n\nOn Thu, Aug 29, 2019, 00:17 Gaurav Jain <notifications@github.com> wrote:\n\n> @carlthome <https://github.com/carlthome>: We solved dictionary hashing\n> in a couple of different ways:\n>\n>    1. With short-lived/locally scoped dictionaries, using id() as the key\n>    is fine\n>    2. Exposed a wrapper object that can be used as a key with a reference\n>    to the Tensor:\n>    https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Tensor#experimental_ref\n>    3. Changed certain dictionaries to be a map of id(key) to a tuple of (key,\n>    value)\n>    4. Refactored code to avoid unnecessary use of a dictionary.\n>\n> All of these options are reasonable for user code to use as well and we\n> have been able to successfully apply the different approaches to various\n> projects. As a result of enabling tensor equality we uncovered a number of\n> bugs in code as well as tests where code wasn't really testing what it\n> thought it was testing.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9359?email_source=notifications&email_token=AAHCA6DPHSRQ65VWDL3FS43QG5ZZPA5CNFSM4DIQT6F2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5NPX4A#issuecomment-526056432>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAHCA6HX5ECMXNHJJNKLDH3QG5ZZPANCNFSM4DIQT6FQ>\n> .\n>\n", ">You would go from having a community of users to having a community of\r\ndevelopers. As it is, it's impossible to contribute more than small bug\r\nfixes to a project that doesn't open up its technical discussions and offer\r\ndetailed, day-to-day insights into its work and, most importantly, its\r\nthinking.\r\n\r\nAFAIK @ewilderj is doing a large push for opening up the Google-centric work process to a wider open source community. You might be interested in joining one of the SIGs for example: https://www.tensorflow.org/community/forums#special_interest_groups"]}, {"number": 9358, "title": "What is the process for converting my data set to the MINIST data set?", "body": "I have run \u201c[Deep MNIST for Experts  |  TensorFlow](https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/)\u201d this file. I have X_train ; Y_train ; X_text ;Y_text ; Now I want to convert those 4 files into one \u201cmist\u201d variable like link which is MNIST format . ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9357, "title": "init_op and concurrent.futures freeze forever", "body": "## **System Information:**\r\n\r\nOS Platform and Distribution: MAC OSX\r\nTensorFlow installed from: `pip install tensorflow` \r\nTensorFlow version : 1.0.0\r\nPython version : Python 3.6.1\r\n\r\nI found a  bug trying to run multiple agent in parallel using python, it boiled down to the code below:\r\n- If i try to init an agent asynchronously after init an agent synchronously, it freezes forever\r\n- if i do it the other way around, everything is fine\r\n\r\nDoes anyone has an idea on this one?\r\n\r\n## **Source Code:**\r\n```python\r\nimport tensorflow as tf\r\nimport concurrent.futures\r\n\r\n# Very basic model\r\nclass Agent(object):\r\n    def __init__(self):\r\n        graph = tf.Graph()\r\n        with graph.as_default():\r\n            self.Qs = tf.get_variable('Qs', shape=[1, 1])\r\n            self.init_op = tf.global_variables_initializer()\r\n        self.sess = tf.Session(graph=graph)\r\n\r\n        print('Before init_op')\r\n        self.sess.run(self.init_op)\r\n        print('After init_op')\r\n\r\n\r\ndef execute_run():\r\n    print('In execute')\r\n    agent = Agent()\r\n\r\nprint('*** First: we create an agent asynchronously')\r\nwith concurrent.futures.ProcessPoolExecutor(1) as executor:\r\n    concurrent.futures.wait([executor.submit(execute_run)])\r\n\r\nprint('*** Then: we create an agent synchronously')\r\nagent = Agent()\r\n\r\nprint('So far, everything is fine')\r\n\r\n\r\nprint('*** Finally: we create an agent asynchronously again')\r\nwith concurrent.futures.ProcessPoolExecutor(1) as executor:\r\n    concurrent.futures.wait([executor.submit(execute_run)])\r\n\r\nprint('You\\'ll never see this as we  can\\'t get passed the init_op')\r\n```\r\n\r\n## **output:**\r\n```bash\r\n*** First: we create an agent asynchronously\r\nIn execute\r\nBefore init_op\r\nAfter init_op\r\n*** Then: we create an agent synchronously\r\nBefore init_op\r\nAfter init_op\r\nSo far, everything is fine\r\n*** Finally: we create an agent asynchronously again\r\nIn execute\r\nBefore init_op\r\n```", "comments": ["Can you tell where it freezes in init_op? Can you also run python 3.5 which is the supported version?", "Is there any way to get more information on a TensorFlow call?\r\n\r\nAnyway, i didn't saw it last time but it seems related to: https://github.com/tensorflow/tensorflow/issues/5448\r\n\r\nThat's a problem, i can't move around the `import tensorflow` piece and the set statement: `multiprocess.set_start_method('spawn')`gives me this error `Runtimes Error: Context has already been set`", "Running Python inside a debugger should give you information on where the freeze happens.  I'm not sure if multiple processes are involved; if so you may need to make sure the debugger follows forks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9356, "title": "decode_image return tensor without shape in python", "body": "\r\n### System Information\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: CentOS Linux release 7.0.1406 (Core)\r\n- *TensorFlow installed from (source or binary)?*: python3 pip\r\n- *TensorFlow version* : v1.0.0-65-g4763edf-dirty 1.0.1\r\n- *Python version* : Python 3.5.2 :: Anaconda custom (64-bit)\r\n\r\n### Describe the problem clearly\r\n\r\nWhen I decode a jpeg file to a tensor with decode_jpeg, the code ran normally.\r\nbut if I use decode_image instead of decode_jpeg for more compatibilities, It raised a ValueError as follow.\r\nIs It a bug?\r\nThanks\r\n\r\n### Source Code / Logs\r\n```python\r\ndef preprocess_image(image_path):\r\n    file_content = tf.read_file(image_path)\r\n    \r\n    #image = tf.image.decode_jpeg(file_content)\r\n    image = tf.image.decode_image(file_content)\r\n    image = tf.image.per_image_standardization(image)\r\n    image = tf.image.resize_images(image, [480, 640])\r\n```\r\noutput : \r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/home/zhangbowen/is_vehicle/classify.py\", line 29, in <module>\r\n    main()\r\n  File \"/data/home/zhangbowen/is_vehicle/classify.py\", line 26, in main\r\n    image = preprocess_image(image_file)\r\n  File \"/data/home/zhangbowen/is_vehicle/classify.py\", line 15, in preprocess_image\r\n    image = tf.image.resize_images(image, [480, 640])\r\n  File \"/data/home/zhangbowen/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 643, in resize_images\r\n    raise ValueError('\\'images\\' contains no shape.')\r\nValueError: 'images' contains no shape.\r\n```", "comments": ["The `decode_image` is a cascade of `tf.cond`. \r\n`resize_images` needs to pull the shape object. However, we don't know which one it's going to be upfront.\r\n\r\n@cwhipkey any comment here?", "I think the issue is that in the decode_image cond, one branch leads to decode_gif.  decode_gif returns a 4-d tensor:  [num_frames, height, width, channels],  which is not the same as the 3-d tensor returned by decode_png and decode_jpeg.\r\n\r\nOne possible fix would be to make decode_gif have an attribute controlling whether to handle animated gifs ,and use that in decode_image?  (possibly also with an option?  It's not clear to me someone would want to use decode_image with a mix of animated and non-animated images - so maybe just changing it would be fine?).\r\n\r\n+raingo and wicke", "@raingo @martinwicke ", "We could definitely add \"frame=None\" which is without effect for png and jpg, but for gif will return all frames if None, of the given frame number if not None and the gif is animated. This would work for other animated formats if those become available. \r\n\r\nIf frame != None we can set the rank to [None, None, None], maybe even [None, None, 3] -- not sure that'll help.", "@martinwicke I can send PR to add the `frame=None` to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L1338.\r\n\r\n```\r\n      with ops.control_dependencies([assert_decode, assert_channels]):\r\n        res = gen_image_ops.decode_gif(contents)\r\n        if frame is not None:\r\n           res = res[frame]\r\n        return res\r\n``` \r\n\r\nIt makes the least changes to the code, and add some flexibility.\r\n\r\nNot sure if that is the best way to do it.", "I think the right way to fix this is to make the image resizing machinery not need static shape.  I'll take a look at how difficult that is.", "Having decode_image sometimes return 4-d and sometimes return 3-d does not seem useful though - should it always just return frame 0 of a gif?  (this was a change in behavior of decode_image when animated gif support was added)", "@cwhipkey It was a backwards incompatible change?", "No, I don't think so.  DecodeGif always returned 4D tensor.  decode_image in python was added after that, and always returned a 4D for GIF and 3D for png or jpg.  So nm on what I wrote earlier about having it always return frame 0.", "CC: @girving ", "I'm going to restrict my current change to unify `decode_jpeg` and `decode_png`: both ops will handle both formats.", "`decode_png` and `decode_jpeg` now decode all formats.", "thanks", "> I'm going to restrict my current change to unify `decode_jpeg` and `decode_png`: both ops will handle both formats.\r\n\r\nhow to judge the format of the image ?", "I found the solution\r\n```\r\ntf.enable_eager_execution()\r\n```\r\nadd this to the top of your code.\r\n\r\nor\r\n```\r\nimg_tensor = tf.image.decode_jpeg(img_raw,channels=3)\r\nimg_final = tf.image.resize_images(img_tensor, [192, 192])\r\nimg_final = img_final/255.0\r\nprint(img_final.shape)\r\n```", "> I found the solution\r\n> \r\n> ```\r\n> tf.enable_eager_execution()\r\n> ```\r\n> \r\n> add this to the top of your code.\r\n> \r\n> or\r\n> \r\n> ```\r\n> img_tensor = tf.image.decode_jpeg(img_raw,channels=3)\r\n> img_final = tf.image.resize_images(img_tensor, [192, 192])\r\n> img_final = img_final/255.0\r\n> print(img_final.shape)\r\n> ```\r\nThis worked for me. Thanks\r\n\r\ntf.enable_eager_execution()"]}, {"number": 9355, "title": "list index out of range", "body": "i am getting these warning while running neuroner\r\n\r\n`epoch_elapsed_training_time: 331.644915 seconds\r\nassess_model on dataset_type: train\r\nC:\\Users\\erame\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\matplotlib\\artist.py:233: MatplotlibDeprecationWarning: get_axes has been deprecated in mpl 1.5, please use the\r\naxes property.  A removal date has not been set.\r\n  stacklevel=1)\r\nC:\\Users\\erame\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\r\n  'precision', 'predicted', average, warn_for)\r\nC:\\Users\\erame\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\r\n  'recall', 'true', average, warn_for)\r\nassess_model on dataset_type: valid\r\nassess_model on dataset_type: test\r\nshell_command: perl .\\conlleval < ..\\output\\en_2017-04-21_07-36-25-322798\\000_014986_train.txt > ..\\output\\en_2017-04-21_07-36-25-322798\\000_014986_train.txt_conll_evaluation.txt\r\n'perl' is not recognized as an internal or external command,\r\noperable program or batch file.\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 442, in <module>\r\n    main()\r\n  File \"main.py\", line 383, in main\r\n    conll_parsed_output = utils_nlp.get_parsed_conll_output(conll_output_filepath)\r\n  File \"C:\\Users\\erame\\AppData\\Local\\Programs\\Python\\Python35\\NeuroNER-master\\src\\utils_nlp.py\", line 42, in get_parsed_conll_output\r\n    line = conll_output[1].split()\r\nIndexError: list index out of range`", "comments": ["None of the packages mentioned in the error message appear to be part of TensorFlow. It would probably be more appropriate to raise this issue on [the NeuroNER repository](https://github.com/Franck-Dernoncourt/NeuroNER)."]}, {"number": 9354, "title": "Error: tensorflow/core/framework/resource_handle.pb.h file not found", "body": "I have followed following instructions from this link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\r\n\r\n- Before you start (All Platforms)\r\n- Building on iOS\r\n- Copy files from inception to data folders in benchmark, simple and camera project.\r\n- Building by hand. (error in terminal: failed with exit code 1) in both compile_ios_protobuf and run make file. \r\n\r\n- download_dependencies.sh worked successfully.\r\n\r\nIf I open project from Simple or camera folder, I received the error:\r\n\r\ntensorflow/core/framework/resource_handle.pb.h file not found.\r\n\r\nThis Post: https://github.com/tensorflow/tensorflow/issues/5095 \r\nindicates that the problem is related to macOS Sierra. after that I installed the update from AppStore.\r\n\r\nAfter that I updated macOS update which updated command line tools and now I receive following errors by running all things from scratch. (Delete old project, unzip new project and follow all instructions).\r\n\r\n![error-list](https://cloud.githubusercontent.com/assets/27809940/25264970/f8eb8720-2687-11e7-90ee-f677e59d79df.png)", "comments": ["The `resource_handle.pb.h` is generated by the protobuf compiler, `protoc`.\r\n\r\nIt's either in\r\n`$(MAKEFILE_DIR)/gen/protobuf-host/bin/protoc`\r\nor somewhere on your path.\r\n\r\nIt should be generated [here](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L544).\r\n\r\nPlease check if they're there, if not why aren't they generated, and if they are there, whether there is a problem in the include path.\r\n\r\n/CC @satok16 ", "Right, just check the file location after building code.\r\n$ find . -name \"*resource_handle*.pb.h\"\r\n", "Thank you responders. But the problem is solved another way.\r\nThe issue is resolved. I was failed to build the project by following instructions from \"building by hand\" in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile.\r\n\r\nThen I tried the installation from \"building all at once\". And somehow it worked. The build is successful and I can run the program.", "Cool!  Feel free to create a new ticket if you meet further problems."]}, {"number": 9353, "title": "Fix typos", "body": "#8943 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 9352, "title": "fix slim README error", "body": "vgg.vgg_16(images) will return two values.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 9351, "title": "TensorBoard Cherrypick", "body": "Fix issue where TensorBoard run selector can become non-responsive.", "comments": []}, {"number": 9350, "title": "Fix issue where TensorBoard run selector can become non-responsive.", "body": "Various dashboards in TensorBoard store the same runSelector state via the url bar. However, the text dashboard may have a different number of runs, because it only shows runs that have text data to display. Some logic that I added to try to keep the runSelector state short actually causes state updates to be lost when the text dashboard and scalar dashboard have very different numbers of runs. I removed the clever update logic in this commit.", "comments": ["core_rnn_cell_test unrelated"]}, {"number": 9349, "title": "Fix TF-Slim TFExampleDecoder link/references in README", "body": "The slim/data README references the tfexample_decoder.py file as tfexample_data_decoder.py.\r\n\r\nThis commit fixes the corresponding broken link and adjusts any additional references accordingly.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks."]}, {"number": 9348, "title": "Fix tflearn doc incorrect comment", "body": "The issue link: https://github.com/tensorflow/tensorflow/issues/9336", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9347, "title": "[Feature request] Add conv3d/conv3d_transpose wrapper to slim", "body": "Please go to Stack Overflow for help and support. http://stackoverflow.com/questions/tagged/tensorflow\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*:\r\nLinux Ubuntu 14\r\n- *TensorFlow installed from (source or binary)?*:\r\nbinary\r\n- *TensorFlow version* (use command below):\r\n1.0\r\n- *Bazel version (if compiling from source)*:\r\nN/A\r\n- *CUDA/cuDNN version*:\r\n8\r\n- *GPU Model and Memory*:\r\nTitan-X\r\n- *Exact command to reproduce*:\r\n\r\nYou can collect some of this information using our environment capture script https://github.com/tensorflow/tensorflow/blob/master/tools/\r\nYou can collect the TensorFlow version with\r\n```sh\r\npython -c \"import tensorflow as tf; print (tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n\r\n### Describe the problem clearly\r\nSimply request to add conv3d/conv3d_transpose wrapper to slim\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["Sergio, any plans to support this in Slim? Thanks.", "Sure, if any one wants to sent a PR, I'll review it quick.", "I have added it.\r\n[Conv3D](https://github.com/tensorflow/tensorflow/pull/9404)", "@jmchen-g @sguada @Kongsea Thank you all!\r\nMay I know the status of integrating conv_3d/conv3d_transpose into slim? The reason I ask is that will benefit many people (like me) who are working on 3D stuff :-)", "@xcyan I have sent a [pull request](https://github.com/tensorflow/tensorflow/pull/9477), but the tensorflow members think there may be too little people who have the need to use the 3D operations of Slim. So please go there to support the PR if you need the feature too.", "@Kongsea The TF developers mentioned that your pull request will cause ~20 breakages. Can you possibly modify your code a little bit.", "@xcyan OK, I will modify it as quickly as possible.", "@Kongsea Do you have any update on this?", "@bhattad2 I have made a new [PR](https://github.com/tensorflow/tensorflow/pull/9773) for many days. But now it is still awaiting the tensorflower's review."]}, {"number": 9346, "title": "Branch 153752424", "body": "", "comments": ["The test failure in //tensorflow/contrib/xla_tf_graph:xla_tf_graph_util_test in the Mac build is pre-existing. Merging."]}, {"number": 9345, "title": "Cherrypicks for 1.1", "body": "New TB patch and switching args for bincount", "comments": []}, {"number": 9344, "title": "Branch 153741338", "body": "", "comments": ["The test failure in core_rnn_cell_test is pre-existing. I'm fixing that internally. All other tests passed.", "Should be okay to merge."]}, {"number": 9343, "title": "Android Demo app & Yolo 2 - OutOfMemoryError ", "body": "I have successfully run Android Demo with Tiny Yolo (_graph-tiny-yolo-voc.pb_ - 60mb),:\r\n\r\n```\r\n  private static final String YOLO_MODEL_FILE = \"file:///android_asset/graph-yolo-voc.pb\";\r\n  private static final int YOLO_INPUT_SIZE = 416;\r\n  private static final String YOLO_INPUT_NAME = \"input\";\r\n  private static final String YOLO_OUTPUT_NAMES = \"output\";\r\n  private static final int YOLO_BLOCK_SIZE = 32;\r\n  private static final boolean USE_YOLO = true;\r\n```\r\n\r\nI also decided to try Yolo 2 (_graph-yolo-voc.pb_ - 193mb),\r\nbut I get _OutOfMemoryError_ when I run _TF Detect_\r\nMy phone has 3 GB of RAM \r\nBut I know that is just android java limit and has nothing to do with current available/free memory\r\nand we can use NDK to bypass this limit.\r\n\r\nI also tried to run with:\r\n```\r\n     <activity android:name=\"org.tensorflow.demo.DetectorActivity\"\r\n                  android:largeHeap=\"true\"\r\n```\r\nbut didn't help\r\n\r\nAnd also:\r\n```\r\n<activity android:name=\"org.tensorflow.demo.DetectorActivity\"\r\n                  android:largeHeap=\"true\"\r\n                  android:hardwareAccelerated=\"false\"\r\n```\r\nApp runs ok, but nothing happens, it just shows black screen, no camera frames..\r\n\r\nIs there a way to run app with big .pb files?  \r\n_TensorFlowInferenceInterface_ needs _AssetManager_ and _string file name_, then I guess it just uses default java IO to read file,\r\nmb it's better to process all of this with NDK somehow?", "comments": ["@andrewharp in case there is an easy answer. The graph doesn't seem that big.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@anonym24 \r\n193mb is bigger than anything I've ever run on Android :)  I'd suggest trying to [quantize](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#eight-bit-calculations) the graph to bring the size down as much as possible as a first step.\r\n\r\nCan you provide the relevant logcat section? Is this just while the file is being read to the buffer or during graphdef initialization?", "@anonym24 Also, we actually did used to have a [native codepath](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/android/jni/jni_utils.cc) for loading graphs, but simplified to use pure Java when we switched contrib/android to the Java API.\r\n\r\nYou could try resurrecting it, and if it resolves your issue then we can consider re-adding it as an augmentation to the default codepath.\r\n\r\n@asimshankar If you have any comments on integrating this smoothly with the Java API.\r\n", "@andrewharp here's an error code:\r\n\r\n```\r\n04-21 09:40:52.731 31597-31597/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                     Process: org.tensorflow.demo, PID: 31597\r\n                                                                     java.lang.OutOfMemoryError: Failed to allocate a 202728962 byte allocation with 14422192 free bytes and 171MB until OOM\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:377)\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:96)\r\n                                                                         at org.tensorflow.demo.TensorFlowYoloDetector.create(TensorFlowYoloDetector.java:111)\r\n                                                                         at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:131)\r\n                                                                         at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:159)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:421)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:428)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\r\n                                                                         at android.view.TextureView.getHardwareLayer(TextureView.java:368)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:15175)\r\n                                                                         at android.view.View.draw(View.java:15971)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3610)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3400)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:15193)\r\n                                                                         at android.view.View.draw(View.java:15971)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3610)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3400)\r\n                                                                         at android.view.View.draw(View.java:16204)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:15198)\r\n                                                                         at android.view.View.draw(View.java:15971)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3610)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3400)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:15193)\r\n                                                                         at android.view.View.draw(View.java:15971)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3610)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3400)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:15193)\r\n                                                                         at android.view.View.draw(View.java:15971)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3610)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3400)\r\n                                                                         at android.view.View.draw(View.java:16204)\r\n                                                                         at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:15198)\r\n                                                                         at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:282)\r\n                                                                         at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:288)\r\n                                                                         at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:323)\r\n                                                                         at android.view.ViewRootImpl.draw(ViewRootImpl.java:2642)\r\n                                                                         at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2461)\r\n                                                                         at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2094)\r\n                                                                         at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1134)\r\n                                                                         at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6045)\r\n                                                                         at android.view.Choreographer$CallbackRecord.run(Choreographer.java:860)\r\n                                                                         at android.view.Choreographer.doCallbacks(Choreographer.java:672)\r\n                                                                         at android.view.Choreographer.doFrame(Choreographer.java:608)\r\n                                                                         at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:846)\r\n                                                                         at android.os.Handler.handleCallback(Handler.java:739)\r\n                                                                         at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                         at android.os.Looper.loop(Looper.java:148)\r\n                                                                         at android.app.ActivityThread.main(ActivityThread.java:5441)\r\n                                                                         at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                         at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:738)\r\n                                                                         at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:628)\r\n04-21 09:40:52.740 1501-2367/? E/ActivityManager: Invalid thumbnail dimensions: 0x0\r\n```\r\nmy normal free memory (1.0-1.4 gb): \r\n\r\n![screenshot_2017-04-21-09-42-38-371_com miui home](https://cloud.githubusercontent.com/assets/8851301/25265837/8886bd48-2677-11e7-99df-d1e3261e6cbd.png)\r\n", "You'll need to run configure first before building TF desktop binaries.\n\nOn Apr 21, 2017 12:08 AM, \"anonym24\" <notifications@github.com> wrote:\n\n> @andrewharp <https://github.com/andrewharp>\n>\n> I'd suggest trying to quantize the graph to bring the size down as much as\n> possible as a first step.\n>\n> I get an error:\n>\n> ~/Desktop/tensorflow-master$ bazel build tensorflow/tools/graph_transforms:transform_graph\n> ERROR: /home/osboxes/Desktop/tensorflow-master/tensorflow/core/BUILD:1357:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/osboxes/Desktop/tensorflow-master/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\n> ERROR: /home/osboxes/Desktop/tensorflow-master/tensorflow/core/BUILD:1357:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/osboxes/Desktop/tensorflow-master/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\n> ERROR: /home/osboxes/Desktop/tensorflow-master/tensorflow/core/BUILD:1357:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/osboxes/Desktop/tensorflow-master/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\n> ERROR: Analysis of target '//tensorflow/tools/graph_transforms:transform_graph' failed; build aborted.\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9343#issuecomment-296101507>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADOGsWru3fGfqNtrE5okeeAtm2FfDz-rks5ryFXmgaJpZM4NDbAD>\n> .\n>\n", "yeah found out already that's why I removed my last comment", "@andrewharp : Regarding https://github.com/tensorflow/tensorflow/issues/9343#issuecomment-295959477 - I wonder that instead of doing something like that, should we get the saved model format going on Android and use that instead? ", "> You'll need to run configure first before building TF desktop binaries.\r\n\r\n@andrewharp I know I can build it on linux. I have vmware with ubuntu where I also have built .pb files, but can I do it in Windows? I  found https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\r\nbut will `cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release` be equal to `./configure`?", "I'm not sure; no harm in running ./configure anyway though. Perhaps you could try the docker solution at the bottom of the readme, as it seems easier?", "@andrewharp  \r\n\r\n> quantize the graph\r\n\r\nI have built: `bazel build tensorflow/tools/graph_transforms:transform_graph`\r\nNow I'm trying the second line, but I'm not sure what happens, looks like it asks for different parameters:\r\n\r\n```\r\nosboxes@osboxes:~/Desktop/tensorflow-master$ bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n> --in_graph=graph-yolo-voc.pb \\\r\n> --out_graph=graph-yolo-voc-opt.pb \\\r\n> --inputs='Mul' \\\r\n> --outputs='softmax' \\\r\n> --transforms='\r\n>   add_default_attributes\r\n>   strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n>   remove_nodes(op=Identity, op=CheckNumerics)\r\n>   fold_constants(ignore_errors=true)\r\n>   fold_batch_norms\r\n>   fold_old_batch_norms\r\n>   quantize_weights\r\n>   quantize_nodes\r\n>   strip_unused_nodes\r\n>   sort_by_execution_order'\r\n2017-04-23 10:35:57.903947: I tensorflow/tools/graph_transforms/transform_graph.cc:257] Applying add_default_attributes\r\n2017-04-23 10:35:58.162621: I tensorflow/tools/graph_transforms/transform_graph.cc:257] Applying strip_unused_nodes\r\n2017-04-23 10:35:58.162877: E tensorflow/tools/graph_transforms/transform_graph.cc:203] Input node softmax not found in graph\r\n2017-04-23 10:35:58.162919: E tensorflow/tools/graph_transforms/transform_graph.cc:204] usage: bazel-bin/tensorflow/tools/graph_transforms/transform_graph\r\nFlags:\r\n\t--in_graph=\"\"                    \tstring\tinput graph file name\r\n\t--out_graph=\"\"                   \tstring\toutput graph file name\r\n\t--inputs=\"\"                      \tstring\tinputs\r\n\t--outputs=\"\"                     \tstring\toutputs\r\n\t--transforms=\"\"                  \tstring\tlist of transforms\r\n\t--output_as_text=false           \tbool\twhether to write the graph in text protobuf format\r\n\r\nTransforms are:\r\nadd_default_attributes\r\nbackport_concatv2\r\nbackport_tensor_array_v3\r\nfold_batch_norms\r\nfold_constants\r\nfold_old_batch_norms\r\nfreeze_requantization_ranges\r\nfuse_pad_and_conv\r\nfuse_resize_and_conv\r\nfuse_resize_pad_and_conv\r\ninsert_logging\r\nmerge_duplicate_nodes\r\nobfuscate_names\r\nquantize_nodes\r\nquantize_weights\r\nremove_attribute\r\nremove_device\r\nremove_nodes\r\nrename_attribute\r\nrename_op\r\nrewrite_quantized_stripped_model_for_hexagon\r\nround_weights\r\nset_device\r\nsort_by_execution_order\r\nsparsify_gather\r\nstrip_unused_nodes\r\n\r\nosboxes@osboxes:~/Desktop/tensorflow-master$ \r\n\r\n```\r\n\r\n", "@andrewharp \r\n\r\n> have a native codepath for loading graphs\r\n\r\nAlso could you help me with resurrecting it? I just don't work with C++/NDK in Android Studio. I have successfully set bazel/worksplace/ndk for Android demo on Ubuntu and I guess I can build project without errors (though it takes some time, have not build yet, before this I just used precompiled nightly libraries)"]}, {"number": 9342, "title": "Update resnet_v2.py", "body": "I've added \"resnet_arg_scope = resnet_utils.resnet_arg_scope\" line to make sure that user can call resnet_v2.resnet_arg_scope(is_training). It is also present in resnet_v1.py", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@tensorflow-jenkins test this please", "@harungunaydin while this PR is waiting for review, it would be great if you could sign the CLA.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 9341, "title": "Tensorflow looks for wrong libcupti.so version", "body": "- OS: Ubuntu 16.04\r\n- *TensorFlow installed from: source\r\n- *TensorFlow version*: 1.1.0-rc2\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: 8.0/6.0\r\n- *GPU Model and Memory*: NVIDIA GTX 1060, 6 GB RAM\r\n- *Exact command to reproduce*:\r\n------------------------\r\nMNIST examples works fine, bute MNIST example with summaries crashes with:\r\n\r\n```\r\n(tensorflow) stefano@stefano-linux:~/Dokumente/Programming/Python/TensorflowCoreTutorial$ /home/stefano/tensorflow/bin/python3 /home/stefano/Dokumente/Programming/Python/TensorflowCoreTutorial/src/mnist_with_summaries.py\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/train-images-idx3-ubyte.gz\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/train-labels-idx1-ubyte.gz\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/t10k-images-idx3-ubyte.gz\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/t10k-labels-idx1-ubyte.gz\r\n2017-04-20 19:16:15.388639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-04-20 19:16:15.389019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7085\r\npciBusID 0000:22:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 4.97GiB\r\n2017-04-20 19:16:15.389036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0\r\n2017-04-20 19:16:15.389042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y\r\n2017-04-20 19:16:15.389055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:22:00.0)\r\nAccuracy at step 0: 0.0981\r\nAccuracy at step 10: 0.7258\r\nAccuracy at step 20: 0.8151\r\nAccuracy at step 30: 0.8572\r\nAccuracy at step 40: 0.879\r\nAccuracy at step 50: 0.8949\r\nAccuracy at step 60: 0.9052\r\nAccuracy at step 70: 0.9058\r\nAccuracy at step 80: 0.915\r\nAccuracy at step 90: 0.9137\r\n2017-04-20 19:16:20.708545: I tensorflow/stream_executor/dso_loader.cc:129] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: /usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\n2017-04-20 19:16:20.708600: F tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Non-OK-status: ::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f) status: Not found: /usr/local/lib/python3.5/dist-packages/tensorfl\r\now/python/_pywrap_tensorflow_internal.so: undefined symbol: cuptiActivityRegisterCallbackscould not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n```\r\n\r\nsource code:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nProblem is, that tensorflow looks for libcupti.so.8.0, but only libcupti.so.7.5 is installed:\r\n\r\n```\r\nstefano@stefano-linux:~$ ls /usr/lib/x86_64-linux-gnu | grep libcupti*\r\nlibcupti.so\r\nlibcupti.so.7.5\r\nlibcupti.so.7.5.18\r\n```\r\n\r\nI installed libcupti like described in the Install section on the Tensorflow website:\r\n\r\n```\r\nsudo apt-get install libcupti-dev\r\n```\r\n\r\nlibcupti-dev 8.0 is not available for Ubuntu 16.04.", "comments": ["It looks for `DSO_CUDA_VERSION` which is the one you built it for.", "Yes, I build it for CUDA 8.0. CUDA 8.0 is available for Ubuntu 16.04, but libcupti-dev is only available in version 7.5 for Ubuntu 16.04.\r\n\r\nDo I have to install libcupti-dev 8.0 somehow?", "@yifeif @jart we set the `DSO_CUDA_VERSION` from the CUDA lib, but apparently the profiler tools can have a different version. (the `DSO_CUDA_VERSION` is used to load both cuda lib and cupti). Since ubuntu 16.04 is the primary version for us, it should work out of the box. Should we `configure` to split the versions?", "For everyone who has the same issue:\r\n\r\nFirst download the libcupti8.0 debian package and install it:\r\nhttp://packages.ubuntu.com/yakkety/amd64/libcupti8.0/download\r\n\r\nAfterwards download libcupti-dev8.0 debian package in order to install it:\r\nhttps://ubuntu.pkgs.org/16.10/ubuntu-multiverse-amd64/libcupti-dev_8.0.44-0ubuntu1_amd64.deb.html\r\n\r\nThis should solve this issue. Of course it would be still better if everything would work out of the box.", "Would anyone happen to know where someone not running a Debian derivative can get the source for libcupti?\r\nEdit: nevermind... seems like its nvidia closed source... extracted the deb and copying libs by hand on my system felt quite dirty but did the trick (gentoo amd64)", "Looks like @StefanoD was able to find the resolution to this issue.\r\n\r\nFor posterity, You have to have the same version of CUDA and libcupti on your system. Latest version of libcupti or cuda may not be on debian repositories yet, so you may need to download these from nvidia, or more recent ubuntu distribution's repositories.", "`libcupti` is available in Ubuntu 16.04 xenial only for CUDA 7.5, not CUDA 8. We can use deb packages from Ubuntu zesty repo:\r\n\r\n```\r\nwget http://cz.archive.ubuntu.com/ubuntu/pool/multiverse/n/nvidia-cuda-toolkit/libcupti8.0_8.0.44-3_amd64.deb\r\nwget http://cz.archive.ubuntu.com/ubuntu/pool/multiverse/n/nvidia-cuda-toolkit/libcupti-dev_8.0.44-3_amd64.deb\r\nsudo dpkg -i libcupti8.0_8.0.44-3_amd64.deb libcupti-dev_8.0.44-3_amd64.deb\r\n```\r\n\r\nIt can be tested eg. by the scripts in https://medium.com/towards-data-science/howto-profile-tensorflow-1a49fb18073d.\r\n\r\n```\r\n[...]\r\n2017-08-22 16:17:42.714397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\n2017-08-22 16:17:42.740833: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n```", "Just adding another error string so this will turn up in search for people in the future:\r\n\r\n```\r\n2017-12-20 18:18:50.156691: F ./tensorflow/stream_executor/lib/statusor.h:212] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.8.0; dlerror: libcupti.so.8.0: cannot open shared object file: No such file or directory\r\n```"]}, {"number": 9340, "title": "Updating version to 1.1.0 and patching update_version", "body": " and patching update_version to include the extension from version.h", "comments": ["Is TF 1.1.0 formally out? We see the packages on PyPI and the images on Docker Hub.", "We are in the process of officially releasing it with docs as well."]}, {"number": 9339, "title": "Enable Fused Winograd by Default", "body": "Right now [fused Winograd](https://github.com/tensorflow/tensorflow/pull/4901) is [disabled by default](https://github.com/tensorflow/tensorflow/blob/e69f71759adac4a794d5b159358af5253cb243bf/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1970-L1976). This is even though enabling this speeds up models considerable in the 3x3 case (see https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295779638). What remains as far as issues, etc to get this faster conv enabled by default?\r\n\r\n/CC @yangzihao @tfboyd", "comments": ["@tfboyd it might be good to see internally whether we want to accelerate this.", "Working on it.  I found a comment in the code about some failing unit tests and I am following up.  On K80s this makes a big difference.  I am also sending out a note to tensorflow-discuss in the super near future and update the Performance Guide for the near-term.  ", "This is so close.  Is is submitted internally and I am working to get it synced to github.  Why say anything until it is in master?  Because I am so happy this is finally nearly done.  :-)  I will update and close as soon as I see this in master. ", "Done https://github.com/tensorflow/tensorflow/pull/10359  It will most likely NOT be 1.2 but will be in 1.3.", "This is old but I saw it and we had to turn it off again by default in 1.3 but at head and 1.4 going forward everything is all good.  We had issues with a few corner cases used by teams inside Google that would fail.  We worked with NVIDIA to identify all of the possible corner cases and the issue is finally closed.  An FYI if anyone sees this.  We had the same problem with fused batch norm, which will also be the default in a couple of weeks if current progress continues.  With the large amount of production level products using TensorFlow what seems like an easy change can have unforeseen impacts.  ", "Thanks for looping back!\n\nOn Sep 9, 2017 4:02 PM, \"Toby Boyd\" <notifications@github.com> wrote:\n\n> This is old but I saw it and we had to turn it off again by default in 1.3\n> but at head and 1.4 going forward everything is all good. We had issues\n> with a few corner cases used by teams inside Google that would fail. We\n> worked with NVIDIA to identify all of the possible corner cases and the\n> issue is finally closed. An FYI if anyone sees this. We had the same\n> problem with fused batch norm, which will also be the default in a couple\n> of weeks if current progress continues. With the large amount of production\n> level products using TensorFlow what seems like an easy change can have\n> unforeseen impacts.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9339#issuecomment-328308523>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbbyUwSTW-ERNSvlwq1_gGj_I0GQdks5sgxkZgaJpZM4NDReM>\n> .\n>\n"]}, {"number": 9338, "title": "ideas - feature request", "body": "**idea1**\r\n\r\nfeeding wavenet implementation in tensorflow simulatenous with the following things to do advanced music gestural recognition:\r\n\r\n- elastic fusion dense slam\r\n- audio data + audio advanced gestural recognition spectral classifiers\r\n\r\n**plus:**\r\ndoing training in real time\r\n\r\n**idea2**\r\n\r\nimplementing the following artificial intelligence model in tensorflow:\r\n\r\n- deep convolutional recursive swarm of hybrid bdi and artificial neural networks", "comments": ["These are fantastic ideas! For now, I would recommend implementing on your own github repo."]}, {"number": 9337, "title": "wide_n_deep meets kint32max bug.", "body": "\r\nI am trying to run a wide_n_deep model on a large dataset ------ I copy and paste adult.data 100 times as training data(adult.data.new 3256200 lines), and take adult.test as test data.\r\n\r\nwith command:\r\npython wide_n_deep_tutorial.py --model_type=wide_n_deep --train_data=data/adult.data.new --test_data=data/adult.test\r\n\r\nWhen I run the same model on original set of this, i.e. ~ 32562 set, the model runs fine. But when it tries on this 3256200 set, it throws up the following stack trace and exits.\r\n\r\nAm I missing something here? The memory stats look fine when the program is running.\r\n\r\n[libprotobuf ERROR google/protobuf/io/zero_copy_stream_impl_lite.cc:173] Cannot allocate buffer larger than kint32max for StringOutputStream.\r\nTraceback (most recent call last):\r\n  File \"/var/dl/runtime/script/wide_n_deep_tutorial.py\", line 234, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/var/dl/runtime/script/wide_n_deep_tutorial.py\", line 197, in main\r\n    FLAGS.train_data, FLAGS.test_data)\r\n  File \"/var/dl/runtime/script/wide_n_deep_tutorial.py\", line 186, in train_and_eval\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 426, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 984, in _train_model\r\n    _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 462, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 786, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 744, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 883, in run\r\n    feed_dict, options)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 909, in _call_hook_before_run\r\n    request = hook.before_run(run_context)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 340, in before_run\r\n    \"graph.pbtxt\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_io.py\", line 67, in write_graph\r\n    file_io.atomic_write_string_to_file(path, str(graph_def))\r\nValueError: Unable to convert message to str", "comments": ["Basically, graph is too big. This is not a bug in tensorflow itself, I would suggest you ask on stackoverflow for ways to reduce the graph size.\r\n\r\n```\r\nCannot allocate buffer larger than kint32max for StringOutputStream.\r\n```"]}]