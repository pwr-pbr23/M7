[{"number": 38990, "title": "Tensorflow is not detecting GPU, while it is detected in PyTorch", "body": "**System information**\r\n- Kali GNU/Linux 2020.1\r\n- TensorFlow installed from (source or binary): pip source\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip in virtualenv\r\n- CUDA/cuDNN version: (deepl) root@root:~/Documents/deepl# nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Apr_24_19:10:27_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.168\r\n- GPU model and memory:\r\ndeepl) root@root:~/Documents/deepl# nvidia-smi\r\nTue Apr 28 23:46:25 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:01:00.0  On |                  N/A |\r\n| 41%   37C    P8     7W / 225W |    632MiB /  7981MiB |     35%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n\r\n\r\n**Problem Description**\r\nimport tensorflow as tf\r\ntf.test.gpu_device_name()\r\n\r\nit throws a blank string not detecting a gpu while drivers are installed perfectly as PyTorch detected the gpu\r\n\r\ntf.test.gpu_device_name()\r\n''\r\n\r\nwhile in PyTorch\r\nimport torch\r\ntorch.cuda.is_available()\r\ntrue\r\n\r\n\r\n**Any other info / logs**\r\nThere is no error in loading modules here are installation logs\r\n\r\n`(deepl) root@root:~/Documents/deepl# pip install tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Using cached tensorflow_gpu-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\r\nRequirement already satisfied: wrapt>=1.11.1 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.12.1)\r\nRequirement already satisfied: astor>=0.6.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (0.8.1)\r\nRequirement already satisfied: absl-py>=0.7.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (0.9.0)\r\nRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.4.1)\r\nRequirement already satisfied: gast==0.2.2 in ./lib/python3.7/site-packages (from tensorflow-gpu) (0.2.2)\r\nRequirement already satisfied: protobuf>=3.8.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (3.11.3)\r\nRequirement already satisfied: google-pasta>=0.1.6 in ./lib/python3.7/site-packages (from tensorflow-gpu) (0.2.0)\r\nRequirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (2.1.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement already satisfied: keras-preprocessing>=1.1.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in ./lib/python3.7/site-packages (from tensorflow-gpu) (0.34.2)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in ./lib/python3.7/site-packages (from tensorflow-gpu) (3.2.1)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.18.3)\r\nRequirement already satisfied: keras-applications>=1.0.8 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.0.8)\r\nRequirement already satisfied: tensorboard<2.2.0,>=2.1.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (2.1.1)\r\nRequirement already satisfied: six>=1.12.0 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.14.0)\r\nRequirement already satisfied: grpcio>=1.8.6 in ./lib/python3.7/site-packages (from tensorflow-gpu) (1.28.1)\r\nRequirement already satisfied: setuptools in ./lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.1.3)\r\nRequirement already satisfied: h5py in ./lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\r\nRequirement already satisfied: requests<3,>=2.21.0 in ./lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.23.0)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in ./lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.14.1)\r\nRequirement already satisfied: werkzeug>=0.11.15 in ./lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.1)\r\nRequirement already satisfied: markdown>=2.6.8 in ./lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\r\nRequirement already satisfied: idna<3,>=2.5 in ./lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in ./lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.25.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2020.4.5.1)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in ./lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in ./lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in ./lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.1.0)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in ./lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\r\nRequirement already satisfied: oauthlib>=3.0.0 in ./lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\r\nInstalling collected packages: tensorflow-gpu\r\nSuccessfully installed tensorflow-gpu-2.1.0`\r\n\r\n\r\n>> I have tried installing pip install tensorflow-gpu==2.0.0 as well.\r\n", "comments": ["try this in tf v2 `tf.config.list_physical devices()`.\r\n\r\nDocs are [here](https://www.tensorflow.org/api_docs/python/tf/config/list_physical_devices)", "Here's the result\r\n\r\ntf.config.list_physical_devices(\r\n    device_type=None\r\n)\r\n\r\n`[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]`\r\n\r\nand for \r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\nprint(\"Num GPUs:\", len(physical_devices)) \r\n\r\n`Num GPUs: 0`", "Similar issues were reported earlier #12388 #13793. Have a look at them. It can happen because of CUDA Version mismatch.", "> Similar issues were reported earlier #12388 #13793. Have a look at them. It can happen because of CUDA Version mismatch.\r\n\r\n@rootsid,\r\nCould you please check @oke-aditya's comment and let us know if it works. Thanks!", "Could you please tell the reason so that I would know what solved it.", "Issue is still not resolved, i see on website that tensorflow works with 10.1 cuda version not with 10.2 and its a kali box, i mostly use it for security and need that speed boost of 10.2. So installing ubuntu on a separate hard drive for tensorflow.", "I had the same problems with no GPU detected as physical device, only XLA_GPU. \r\nMy problems were fixed by building Tensorflow from source, (see https://www.tensorflow.org/install/source#common_installation_problems ) which took quite a long time, but after this  it works OK!"]}, {"number": 38989, "title": "[r2.2:Cherrypick] Remove **kwargs from `fit`. It was never validated, which has the unfortunate side effect that misspelled keyword arguments are accepted despite doing nothing (e.g. `val_data`, `call_backs`, `epohcs` or whatever else).", "body": "PiperOrigin-RevId: 308839038\nChange-Id: I140a18661a90e96e9f04b46c2412936fc466ca03", "comments": []}, {"number": 38988, "title": "AttributeError: 'dict' object has no attribute 'name'", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Catalina\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: ` 2.2.0.dev20200427` \r\n- Python version: 3.7.5\r\n\r\n**Describe the current behavior**\r\n\r\nI get the error\r\n\r\n> AttributeError: 'dict' object has no attribute 'name'\r\n\r\nwhen I try to plot a model, with `tf.keras.utils.plot_model`, where I specify the loss to the `compile` method as a dictionary. According to [the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#compile), this should be possible and this was actually possible in TF 2.1 (i.e. until recently I was using the exact same code with TF 2.1 and no error was thrown).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_model(input_shape, num_classes=10):\r\n    model_input = tf.keras.layers.Input(shape=input_shape)\r\n    x = tf.keras.layers.Conv2D(6, 3)(model_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    model_output = tf.keras.layers.Dense(num_classes, name=\"my_output_layer\")(x)\r\n    model = tf.keras.Model(model_input, model_output)\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef train():\r\n    model = get_model((28, 28, 1))\r\n\r\n    model.compile(loss={\"my_output_layer\": \"categorical_crossentropy\"})\r\n\r\n    # IF YOU COMMENT THIS, NO ERROR OCCURS!!!\r\n    tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n```\r\n\r\nI installed `pydot` with `pip install pydot`. The version that was installed is `1.4.1`.\r\n\r\nClearly, the error is due to the fact that `plot_model` thinks that the loss is also a layer.\r\n\r\nI need to use TF 2.2 because I am using another library that requires the nightly version of TF.", "comments": ["@nbro\r\nI ran the code shared above and do not face any error on nightly, please find the [gist here.](https://colab.sandbox.google.com/gist/Saduf2019/cdf44239edc5dd83d9574f7b2ad6f5ef/untitled162.ipynb)", "@Saduf2019 If I run your notebook, I get the error I am reporting above. Please, run it again!", "This error also comes up in tf 2.1 using tf.keras.utils.model_to_dot(). The first code block gives the AttributeError mentioned above:\r\n\r\n```\r\ndef custom_loss(y_true, y_pred, w):\r\n  return w * tf.math.square(y_true - y_pred)\r\n\r\nopt=tf.keras.optimizers.SGD\r\nlr=1e-4\r\n\r\ninput = Input(shape=(1000,), dtype=tf.float32, name='input')\r\ny_true = Input(shape=(1,), dtype=tf.float32, name='y_true')\r\ny_pred = Dense(1, name='y_pred')(input)\r\nw = Input(shape=(1,), dtype=tf.float32, name='weights')\r\n\r\nmodel = tf.keras.Model(\r\n    inputs=tuple([input, y_true, w]),\r\n    outputs=y_pred)\r\nmodel.add_loss(custom_loss(y_true, y_pred, w))\r\nmodel.compile(optimizer=opt(learning_rate=lr))\r\n\r\nimg = model_to_dot(\r\n    model, show_shapes=True, show_layer_names=False).create(prog='dot', format='png')\r\n```\r\n\r\nThe second code block does not throw this error and everything works fine, so I think it's something to do with using a custom loss function or the fact that I'm passing `y_true` as a model input:\r\n```\r\nopt=tf.keras.optimizers.SGD\r\nlr=1e-4\r\n\r\ninput = Input(shape=(1000,), dtype=tf.float32, name='input')\r\ny_pred = Dense(1, name='y_pred')(input)\r\n\r\nmodel = tf.keras.Model(\r\n    inputs=input, outputs=y_pred)\r\nmodel.compile(optimizer=opt(lr=lr), loss='mean_squared_error')\r\n\r\nimg = model_to_dot(\r\n    model, show_shapes=True, show_layer_names=False).create(prog='dot', format='png')\r\n```", "@nbro\r\nYes i confirm on rerunning the gist i am able to see the error.", "When this happens, tensorflow appears to have appended an empty dictionary to `model._layers`. A workaround for the moment is, once the model has been compiled, to use:\r\n\r\n```\r\nmodel._layers = [layer for layer in model._layers if not isinstance(layer, dict)]\r\n```\r\n\r\n\r\n> **System information**\r\n> \r\n> * Have I written custom code: No\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Catalina\r\n> * TensorFlow installed from: binary\r\n> * TensorFlow version: ` 2.2.0.dev20200427`\r\n> * Python version: 3.7.5\r\n> \r\n> **Describe the current behavior**\r\n> \r\n> I get the error\r\n> \r\n> > AttributeError: 'dict' object has no attribute 'name'\r\n> \r\n> when I try to plot a model, with `tf.keras.utils.plot_model`, where I specify the loss to the `compile` method as a dictionary. According to [the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#compile), this should be possible and this was actually possible in TF 2.1 (i.e. until recently I was using the exact same code with TF 2.1 and no error was thrown).\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> No error.\r\n> \r\n> **Standalone code to reproduce the issue**\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> \r\n> def get_model(input_shape, num_classes=10):\r\n>     model_input = tf.keras.layers.Input(shape=input_shape)\r\n>     x = tf.keras.layers.Conv2D(6, 3)(model_input)\r\n>     x = tf.keras.layers.Flatten()(x)\r\n>     model_output = tf.keras.layers.Dense(num_classes, name=\"my_output_layer\")(x)\r\n>     model = tf.keras.Model(model_input, model_output)\r\n>     model.summary()\r\n>     return model\r\n> \r\n> \r\n> def train():\r\n>     model = get_model((28, 28, 1))\r\n> \r\n>     model.compile(loss={\"my_output_layer\": \"categorical_crossentropy\"})\r\n> \r\n>     # IF YOU COMMENT THIS, NO ERROR OCCURS!!!\r\n>     tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=True)\r\n> \r\n> \r\n> if __name__ == '__main__':\r\n>     train()\r\n> ```\r\n> \r\n> I installed `pydot` with `pip install pydot`. The version that was installed is `1.4.1`.\r\n> \r\n> Clearly, the error is due to the fact that `plot_model` thinks that the loss is also a layer.\r\n> \r\n> I need to use TF 2.2 because I am using another library that requires the nightly version of TF.\r\n\r\nWhen this happens, tensorflow appears to have appended an empty dictionary to `model._layers`. A workaround for the moment is, once the model has been compiled, to use: `model._layers = [layer for layer in model._layers if not isinstance(layer, dict)]`\r\n\r\nYour example would therefore read:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_model(input_shape, num_classes=10):\r\n    model_input = tf.keras.layers.Input(shape=input_shape)\r\n    x = tf.keras.layers.Conv2D(6, 3)(model_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    model_output = tf.keras.layers.Dense(num_classes, name=\"my_output_layer\")(x)\r\n    model = tf.keras.Model(model_input, model_output)\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef train():\r\n    model = get_model((28, 28, 1))\r\n\r\n    model.compile(loss={\"my_output_layer\": \"categorical_crossentropy\"})\r\n    model._layers = [layer for layer in model._layers if not isinstance(layer, dict)]\r\n\r\n# this should now work\r\n    tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n```\r\n", "For some reasons, the last layer in my model._layers is a ```ListWrapper```, and I am using multiple loss functions.\r\nThus, I am using the following snippet as a workaround:\r\n```python\r\nfrom tensorflow.keras.layers import Layer\r\nmodel._layers = [\r\n    layer for layer in model._layers if isinstance(layer, Layer)\r\n]\r\nplot_model(model)\r\n```", "This is fixed with tf-nightly 2.4.0-dev20200828.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/c20e2929379074ffc1f3fa80ae258b09/github_38988.ipynb) for your reference. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38988\">No</a>\n"]}, {"number": 38987, "title": "Sigmoid returns negative values", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOs Linux 7 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip3 installed tensorflow-gpu\r\n- TensorFlow version (use command below):\r\n2.1.0\r\n- Python version:\r\n3.6.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda 10.1 cudnn 7.6.4\r\n- GPU model and memory:\r\nTesla P100 75GB\r\n\r\n**Describe the current behavior**\r\nThe last output layer is a sigmoid function, but the AUC metric says output is not >= 0 element-wise. I've also tried this using `activation='sigmoid'` in the output layer directly, which also results in the same error.\r\n\r\nI've verified that after removing AUC as a metric,  the code works but the min output value becomes negative after a few epochs. \r\n\r\n**Describe the expected behavior**\r\nSigmoid output should be 0-1\r\n\r\n**Last part of the code **\r\n\r\n```\r\n    ypred = tf.keras.layers.Dense(                                              \r\n            1, name=\"%s_out\" % target_variable,                                 \r\n            kernel_regularizer=tf.keras.regularizers.l2(beta))(x)               \r\n                                                                    \r\n    ypred = tf.nn.sigmoid(ypred)                                            \r\n\r\n                                                                                \r\n    model = tf.keras.models.Model(inputs=inputs, outputs=ypred)                 \r\n    optimizer = tf.keras.optimizers.Adam(lr)                                    \r\n    model.compile(optimizer=optimizer,                                          \r\n                  loss='binary_crossentropy',                                   \r\n                  metrics=['accuracy', 'AUC', 'mse'])             \r\n                                                                                                                                        \r\n    early_stopping = tf.keras.callbacks.EarlyStopping(patience=100)             \r\n                  \r\n    callbacks = [early_stopping]                                                \r\n                                                                                \r\n    model_start_time = time.time()                                              \r\n   \r\n    model.evaluate(test_ds, verbose=2)                                      \r\n                                                                                \r\n    history = model.fit(                                                        \r\n            train_ds, validation_data=(test_ds), epochs=num_epochs,             \r\n            verbose=2, callbacks=callbacks)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"lol_embed_ntf_champs.py\", line 162, in <module>\r\n    save_results=args.save_results, **datum)\r\n  File \"/auto/rcf-40/yioujian/ntf-embed/lol_embedding_ntf_champs.py\", line 160, in run_embed_ntf\r\n    verbose=2, callbacks=callbacks)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/home/rcf-proj2/ef/yioujian/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/tf_op_layer_Sigmoid/Sigmoid:0) = ] [[0.323994398][0.177097648][0.000274539]...] [y (metrics/AUC/Cast_1/x:0) = ] [0]\r\n\t [[{{node metrics/AUC/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]]\r\n\t [[metrics/AUC/assert_less_equal/Assert/AssertGuard/pivot_f/_13/_39]]\r\n  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/tf_op_layer_Sigmoid/Sigmoid:0) = ] [[0.323994398][0.177097648][0.000274539]...] [y (metrics/AUC/Cast_1/x:0) = ] [0]\r\n\t [[{{node metrics/AUC/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_4015]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```", "comments": ["Update: it seems that the issue might be related to AUC and not sigmoid, because after removing AUC there were no negative outputs. However, I also realized the result is not deterministic (even though I've seeded everything, I don't know why)\r\n\r\nI did manage to fix the problem by manually clipping the value to be between 0 and 1, however.\r\n", "@julie-jiang \r\n\r\nPlease, let us know   is this still an issue. Please, close this thread if your issue was resolved.\r\n\r\nIf the issue was not resolved request you to share colab link or simple standalone to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38987\">No</a>\n", "@julie-jiang I'm running into the same issue. Can you share how you clip the value to be between 0 and 1? ", "@Namzakku You can either use [tf.clip_by_value](https://www.tensorflow.org/api_docs/python/tf/clip_by_value), [tf.clip_by_norm](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm), or [tf.clip_by_global_norm](https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm). Here's a complete [example](https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow) "]}, {"number": 38986, "title": "The app crash when i apply my mobilenet model.", "body": "Device: HUAWEI nova 3i\r\nI downloaded the \"TensorFlowLiteInceptionTutorial\" from https://github.com/soum-io/TensorFlowLiteInceptionTutorial. I change to my mobilenet model.\r\n\r\nLabels is show but Confidence value = 0% all.\r\n\r\nChooseModel.java\r\n```\r\npackage com.soumio.inceptiontutorial;\r\n\r\nimport android.Manifest;\r\nimport android.content.ContentValues;\r\nimport android.content.Intent;\r\nimport android.content.pm.ActivityInfo;\r\nimport android.content.pm.PackageManager;\r\nimport android.net.Uri;\r\nimport android.os.Build;\r\nimport android.provider.MediaStore;\r\nimport android.support.annotation.NonNull;\r\nimport android.support.v4.app.ActivityCompat;\r\nimport android.support.v4.content.ContextCompat;\r\nimport android.support.v7.app.AppCompatActivity;\r\nimport android.os.Bundle;\r\nimport android.util.Log;\r\nimport android.view.View;\r\nimport android.widget.Button;\r\nimport android.widget.Toast;\r\n\r\nimport com.soundcloud.android.crop.Crop;\r\n\r\nimport java.io.File;\r\n\r\npublic class ChooseModel extends AppCompatActivity {\r\n\r\n    // button for each available classifier\r\n    private Button inceptionQuant;\r\n\r\n    // for permission requests\r\n    public static final int REQUEST_PERMISSION = 300;\r\n\r\n    // request code for permission requests to the os for image\r\n    public static final int REQUEST_IMAGE = 100;\r\n\r\n    // will hold uri of image obtained from camera\r\n    private Uri imageUri;\r\n\r\n    // string to send to next activity that describes the chosen classifier\r\n    private String chosen;\r\n\r\n    //boolean value dictating if chosen model is quantized version or not.\r\n    private boolean quant;\r\n\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        super.onCreate(savedInstanceState);\r\n        setContentView(R.layout.activity_choose_model);\r\n\r\n        // request permission to use the camera on the user's phone\r\n        if (ActivityCompat.checkSelfPermission(this.getApplicationContext(), android.Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED){\r\n            ActivityCompat.requestPermissions(this, new String[] {android.Manifest.permission.CAMERA}, REQUEST_PERMISSION);\r\n        }\r\n\r\n        // request permission to write data (aka images) to the user's external storage of their phone\r\n        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M\r\n                && ContextCompat.checkSelfPermission(this, Manifest.permission.WRITE_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) {\r\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.WRITE_EXTERNAL_STORAGE},\r\n                    REQUEST_PERMISSION);\r\n        }\r\n\r\n        // request permission to read data (aka images) from the user's external storage of their phone\r\n        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M\r\n                && ContextCompat.checkSelfPermission(this, Manifest.permission.READ_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) {\r\n            ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.READ_EXTERNAL_STORAGE},\r\n                    REQUEST_PERMISSION);\r\n        }\r\n\r\n        // on click for inception quant model\r\n        inceptionQuant = (Button)findViewById(R.id.inception_quant);\r\n        inceptionQuant.setOnClickListener(new View.OnClickListener() {\r\n            @Override\r\n            public void onClick(View view) {\r\n                // filename in assets\r\n                chosen = \"mushroom_mobilenetv1_batch270-2.tflite\";\r\n                // model in not quantized\r\n                quant = true;\r\n                // open camera\r\n                openCameraIntent();\r\n            }\r\n        });\r\n    }\r\n\r\n    // opens camera for user\r\n    private void openCameraIntent(){\r\n        ContentValues values = new ContentValues();\r\n        values.put(MediaStore.Images.Media.TITLE, \"New Picture\");\r\n        values.put(MediaStore.Images.Media.DESCRIPTION, \"From your Camera\");\r\n        // tell camera where to store the resulting picture\r\n        imageUri = getContentResolver().insert(\r\n                MediaStore.Images.Media.EXTERNAL_CONTENT_URI, values);\r\n        Intent intent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);\r\n        intent.putExtra(MediaStore.EXTRA_OUTPUT, imageUri);\r\n        setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_UNSPECIFIED);\r\n        // start camera, and wait for it to finish\r\n        startActivityForResult(intent, REQUEST_IMAGE);\r\n    }\r\n\r\n    // checks that the user has allowed all the required permission of read and write and camera. If not, notify the user and close the application\r\n    @Override\r\n    public void onRequestPermissionsResult(final int requestCode, @NonNull final String[] permissions, @NonNull final int[] grantResults) {\r\n        super.onRequestPermissionsResult(requestCode, permissions, grantResults);\r\n        if (requestCode == REQUEST_PERMISSION) {\r\n            if (!(grantResults.length > 0 && grantResults[0] == PackageManager.PERMISSION_GRANTED)) {\r\n                Toast.makeText(getApplicationContext(),\"This application needs read, write, and camera permissions to run. Application now closing.\",Toast.LENGTH_LONG);\r\n                System.exit(0);\r\n            }\r\n        }\r\n    }\r\n\r\n    // dictates what to do after the user takes an image, selects and image, or crops an image\r\n    @Override\r\n    protected void onActivityResult(int requestCode, int resultCode, Intent data){\r\n        super.onActivityResult(requestCode, resultCode, data);\r\n        // if the camera activity is finished, obtained the uri, crop it to make it square, and send it to 'Classify' activity\r\n        if(requestCode == REQUEST_IMAGE && resultCode == RESULT_OK) {\r\n\r\n            Intent i = new Intent(ChooseModel.this, Classify.class);\r\n            // put image data in extras to send\r\n            i.putExtra(\"resID_uri\", imageUri);\r\n            // put filename in extras\r\n            i.putExtra(\"chosen\", chosen);\r\n            // put model type in extras\r\n            i.putExtra(\"quant\", quant);\r\n            // send other required data\r\n            startActivity(i);\r\n\r\n        }\r\n```\r\nClassify.java\r\n```\r\npackage com.soumio.inceptiontutorial;\r\n\r\nimport android.content.Intent;\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.Matrix;\r\nimport android.graphics.drawable.BitmapDrawable;\r\nimport android.net.Uri;\r\nimport android.provider.MediaStore;\r\nimport android.support.v7.app.AppCompatActivity;\r\nimport android.os.Bundle;\r\nimport android.util.Log;\r\nimport android.view.View;\r\nimport android.widget.Button;\r\nimport android.widget.ImageView;\r\nimport android.widget.TextView;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\n\r\nimport java.io.BufferedReader;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.io.InputStreamReader;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\nimport java.util.AbstractMap;\r\nimport java.util.ArrayList;\r\nimport java.util.Comparator;\r\nimport java.util.List;\r\nimport java.util.Map;\r\nimport java.util.PriorityQueue;\r\n\r\npublic class Classify extends AppCompatActivity {\r\n\r\n    // presets for rgb conversion\r\n    private static final int RESULTS_TO_SHOW = 3;\r\n    private static final float IMAGE_MEAN = 127.5f;\r\n    private static final float IMAGE_STD = 127.5f;\r\n\r\n\r\n    // options for model interpreter\r\n    private final Interpreter.Options tfliteOptions = new Interpreter.Options();\r\n    // tflite graph\r\n    private Interpreter tflite;\r\n    // holds all the possible labels for model\r\n    private List<String> labelList;\r\n    // holds the selected image data as bytes\r\n    private ByteBuffer imgData = null;\r\n    // holds the probabilities of each label for quantized graphs\r\n    private float[][] labelProbArrayB = null;\r\n    // array that holds the labels with the highest probabilities\r\n    private String[] topLables = null;\r\n    // array that holds the highest probabilities\r\n    private String[] topConfidence = null;\r\n\r\n\r\n    // selected classifier information received from extras\r\n    private String chosen;\r\n    private boolean quant;\r\n\r\n    // input image dimensions for the Inception Model\r\n    private int DIM_IMG_SIZE_X = 224;\r\n    private int DIM_IMG_SIZE_Y = 224;\r\n    private int DIM_PIXEL_SIZE = 3;\r\n\r\n    // int array to hold image data\r\n    private int[] intValues;\r\n\r\n    // activity elements\r\n    private ImageView selected_image;\r\n    private Button classify_button;\r\n    private Button back_button;\r\n    private TextView label1;\r\n    private TextView label2;\r\n    private TextView label3;\r\n    private TextView Confidence1;\r\n    private TextView Confidence2;\r\n    private TextView Confidence3;\r\n\r\n    // priority queue that will hold the top results from the CNN\r\n    private PriorityQueue<Map.Entry<String, Float>> sortedLabels =\r\n            new PriorityQueue<>(\r\n                    RESULTS_TO_SHOW,\r\n                    new Comparator<Map.Entry<String, Float>>() {\r\n                        @Override\r\n                        public int compare(Map.Entry<String, Float> o1, Map.Entry<String, Float> o2) {\r\n                            return (o1.getValue()).compareTo(o2.getValue());\r\n                        }\r\n                    });\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        // get all selected classifier data from classifiers\r\n        chosen = (String) getIntent().getStringExtra(\"chosen\");\r\n        quant = (boolean) getIntent().getBooleanExtra(\"quant\", false);\r\n\r\n        // initialize array that holds image data\r\n        intValues = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\n\r\n        super.onCreate(savedInstanceState);\r\n\r\n        //initilize graph and labels\r\n        try{\r\n            tflite = new Interpreter(loadModelFile(), tfliteOptions);\r\n            labelList = loadLabelList();\r\n        } catch (Exception ex){\r\n            ex.printStackTrace();\r\n        }\r\n\r\n        // initialize byte array. The size depends if the input data needs to be quantized or not\r\n        if(quant){\r\n            imgData =\r\n                    ByteBuffer.allocateDirect(\r\n                            4 * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE );\r\n        }\r\n        imgData.order(ByteOrder.nativeOrder());\r\n\r\n        // initialize probabilities array. The datatypes that array holds depends if the input data needs to be quantized or not\r\n        if(quant){\r\n            labelProbArrayB= new float[1][labelList.size()];\r\n        }\r\n\r\n        setContentView(R.layout.activity_classify);\r\n\r\n        // labels that hold top three results of CNN\r\n        label1 = (TextView) findViewById(R.id.label1);\r\n        label2 = (TextView) findViewById(R.id.label2);\r\n        label3 = (TextView) findViewById(R.id.label3);\r\n        // displays the probabilities of top labels\r\n        Confidence1 = (TextView) findViewById(R.id.Confidence1);\r\n        Confidence2 = (TextView) findViewById(R.id.Confidence2);\r\n        Confidence3 = (TextView) findViewById(R.id.Confidence3);\r\n        // initialize imageView that displays selected image to the user\r\n        selected_image = (ImageView) findViewById(R.id.selected_image);\r\n\r\n        // initialize array to hold top labels\r\n        topLables = new String[RESULTS_TO_SHOW];\r\n        // initialize array to hold top probabilities\r\n        topConfidence = new String[RESULTS_TO_SHOW];\r\n\r\n        // allows user to go back to activity to select a different image\r\n        back_button = (Button)findViewById(R.id.back_button);\r\n        back_button.setOnClickListener(new View.OnClickListener() {\r\n            @Override\r\n            public void onClick(View view) {\r\n                Intent i = new Intent(Classify.this, ChooseModel.class);\r\n                startActivity(i);\r\n            }\r\n        });\r\n\r\n        // classify current dispalyed image\r\n        classify_button = (Button)findViewById(R.id.classify_image);\r\n        classify_button.setOnClickListener(new View.OnClickListener() {\r\n            @Override\r\n            public void onClick(View view) {\r\n                // get current bitmap from imageView\r\n                Bitmap bitmap_orig = ((BitmapDrawable)selected_image.getDrawable()).getBitmap();\r\n                // resize the bitmap to the required input size to the CNN\r\n                Bitmap bitmap = getResizedBitmap(bitmap_orig, DIM_IMG_SIZE_X, DIM_IMG_SIZE_Y);\r\n                // convert bitmap to byte array\r\n                convertBitmapToByteBuffer(bitmap);\r\n                // pass byte data to the graph\r\n                if(quant){\r\n                    try {\r\n                        tflite.run(imgData, labelProbArrayB);\r\n                    } catch (Exception e) {\r\n                        Log.e(\"Error on Contact\", e.getMessage());\r\n                    }\r\n                }\r\n                // display the results\r\n                printTopKLabels();\r\n            }\r\n        });\r\n\r\n        // get image from previous activity to show in the imageView\r\n        Uri uri = (Uri)getIntent().getParcelableExtra(\"resID_uri\");\r\n        try {\r\n            Bitmap bitmap = MediaStore.Images.Media.getBitmap(getContentResolver(), uri);\r\n            selected_image.setImageBitmap(bitmap);\r\n            selected_image.setRotation(selected_image.getRotation());\r\n        } catch (IOException e) {\r\n            e.printStackTrace();\r\n        }\r\n    }\r\n\r\n    // loads tflite grapg from file\r\n    private MappedByteBuffer loadModelFile() throws IOException {\r\n        AssetFileDescriptor fileDescriptor = this.getAssets().openFd(chosen);\r\n        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n        FileChannel fileChannel = inputStream.getChannel();\r\n        long startOffset = fileDescriptor.getStartOffset();\r\n        long declaredLength = fileDescriptor.getDeclaredLength();\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n    }\r\n\r\n    // converts bitmap to byte array which is passed in the tflite graph\r\n    private void convertBitmapToByteBuffer(Bitmap bitmap) {\r\n        if (imgData == null) {\r\n            return;\r\n        }\r\n        imgData.rewind();\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        // loop through all pixels\r\n        int pixel = 0;\r\n        for (int i = 0; i < DIM_IMG_SIZE_X; ++i) {\r\n            for (int j = 0; j < DIM_IMG_SIZE_Y; ++j) {\r\n                final int val = intValues[pixel++];\r\n                // get rgb values from intValues where each int holds the rgb values for a pixel.\r\n                // if quantized, convert each rgb value to a byte, otherwise to a float\r\n                if(quant){\r\n                    imgData.putFloat((((val >> 16) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                    imgData.putFloat((((val >> 8) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                    imgData.putFloat(((val & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n                }\r\n\r\n            }\r\n        }\r\n    }\r\n\r\n    // loads the labels from the label txt file in assets into a string array\r\n    private List<String> loadLabelList() throws IOException {\r\n        List<String> labelList = new ArrayList<String>();\r\n        BufferedReader reader =\r\n                new BufferedReader(new InputStreamReader(this.getAssets().open(\"labels_without_background_mushroom.txt\")));\r\n        String line;\r\n        while ((line = reader.readLine()) != null) {\r\n            labelList.add(line);\r\n        }\r\n        reader.close();\r\n        return labelList;\r\n    }\r\n\r\n    // print the top labels and respective confidences\r\n    private void printTopKLabels() {\r\n        // add all results to priority queue\r\n        for (int i = 0; i < labelList.size(); ++i) {\r\n            if(quant){\r\n                    sortedLabels.add(new AbstractMap.SimpleEntry<>(labelList.get(i), ((int) labelProbArrayB[0][i] & 0xff) / 255.0f));\r\n            }\r\n            if (sortedLabels.size() > RESULTS_TO_SHOW) {\r\n                sortedLabels.poll();\r\n            }\r\n        }\r\n\r\n        // get top results from priority queue\r\n        final int size = sortedLabels.size();\r\n        for (int i = 0; i < size; ++i) {\r\n\r\n            Map.Entry<String, Float> label = sortedLabels.poll();\r\n            topLables[i] = label.getKey();\r\n            topConfidence[i] = String.format(\"%.1f%%\",label.getValue()*100.0f);\r\n        }\r\n\r\n        // set the corresponding textviews with the results\r\n        label1.setText(\"1. \"+topLables[2]);\r\n        label2.setText(\"2. \"+topLables[1]);\r\n        label3.setText(\"3. \"+topLables[0]);\r\n        Confidence1.setText(topConfidence[2]);\r\n        Confidence2.setText(topConfidence[1]);\r\n        Confidence3.setText(topConfidence[0]);\r\n    }\r\n\r\n\r\n    // resizes bitmap to given dimensions\r\n    public Bitmap getResizedBitmap(Bitmap bm, int newWidth, int newHeight) {\r\n        int width = bm.getWidth();\r\n        int height = bm.getHeight();\r\n        float scaleWidth = ((float) newWidth) / width;\r\n        float scaleHeight = ((float) newHeight) / height;\r\n        Matrix matrix = new Matrix();\r\n        matrix.postScale(scaleWidth, scaleHeight);\r\n        Bitmap resizedBitmap = Bitmap.createBitmap(\r\n                bm, 0, 0, width, height, matrix, false);\r\n        return resizedBitmap;\r\n    }\r\n}\r\n```\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 38985, "title": "model.save raise ValueError: If specifying TensorSpec names for nested structures, either zero or all names have to be specified.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nI am using colab to reproduce the issue and the ipynb is attached below.\r\n\r\nYou can collect some of this information using our environment capture\r\ntf.version.GIT_VERSION: v1.12.1-30689-g428cdeda09\r\ntf.version.VERSION: 2.2.0-dev20200428\r\n\r\n\r\n**Describe the current behavior**\r\ncannot save the keras model with tft layer for serving.\r\n\r\n**Describe the expected behavior**\r\nsuccessifully save the model and serve it like this example: https://github.com/tensorflow/transform/blob/master/examples/census_example_v2_test.py\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://drive.google.com/open?id=1h2QIX_QZetIzSuG0J6lNWkHoSa2nnIyS\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nerror is show in the last cell of the colab notebook.", "comments": ["There is a similar issue I found here: https://github.com/tensorflow/tensorflow/issues/38465. But I am not using the graphics api and I cannot find a solution from that ticket neither.", "@neilteng \r\nI ran the code shared, and do not face the same error faced by you. Please find [the gist ](https://colab.sandbox.google.com/gist/Saduf2019/c87c26439e009db172c8408a38d2f5ee/untitled162.ipynb)for the same.", "@Saduf2019  Sorry for the inconvenience, the old one relies on some files in the directory. I have delete unnecessary code. And here is the new version:\r\n\r\nhttps://drive.google.com/open?id=1h2QIX_QZetIzSuG0J6lNWkHoSa2nnIyS", "I guess it is related to tf.feature_column?", "@neilteng \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/e2a7ae5200a3a1868013c56feff09cc7/untitled165.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38985\">No</a>\n"]}, {"number": 38984, "title": "[TF2.2rc3] dict of ragged tensors as an input of keras layer does not support serialization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2rc3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWe have a layer that takes a dict of ragged tensors as input:\r\n\r\n```\r\nclass EmbeddingMerger(tf.keras.layers.Layer):\r\n  def __init__(self, list_features, **kwargs):\r\n    super().__init__(**kwargs)\r\n    self._supports_ragged_inputs = True\r\n    self.embeddings = {feature: Embedding(10, 3) for feature in list_features}\r\n    self.mean = tf.keras.layers.Lambda(tf.reduce_mean, arguments=dict(axis=1))\r\n  def call(self, inputs):\r\n    tensors = [self.embeddings[col](inputs[col]) for col in inputs]\r\n    tensors = [self.mean(inp) for inp in tensors]\r\n    return Add()(tensors)\r\n```\r\nbefore saving it the model works well:\r\n```\r\nlist_features = ['feature_1', 'feature_2']\r\nfeature_1 = tf.ragged.constant([[0], [1, 3]])\r\nfeature_2 = tf.ragged.constant([[1, 2], [4]])\r\nf = {'feature_1': feature_1,\r\n     'feature_2': feature_2}\r\nf_inputs = {'feature_1': Input(shape=(), name='feature_1', ragged=True),\r\n            'feature_2': Input(shape=(), name='feature_2', ragged=True)}\r\nout = EmbeddingMerger(list_features)(f_inputs)\r\nmodel = Model(f_inputs, out)\r\n```\r\n\r\n```\r\ntruth = model.predict(f)\r\ntruth\r\n```\r\n> array([[-0.01802131, -0.01703345,  0.0267079 ],\r\n       [ 0.0096956 , -0.05128085, -0.04486313]], dtype=float32)\r\n\r\nIf we serialize the model and reload it : \r\n\r\n```\r\nmodel.save('/tmp/test')\r\nmodel_reloaded = tf.keras.models.load_model('/tmp/test')\r\nmodel_reloaded.predict(f)\r\n```\r\n>     ValueError: Layer embedding_merger does not support RaggedTensors as input. Inputs received: {'feature_1': tf.RaggedTensor(values=Tensor(\"model/Cast_1:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)), 'feature_2': tf.RaggedTensor(values=Tensor(\"model/Cast_3:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant_1/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64))}. You can try converting your input to an uniform tensor.\r\n\r\n**Describe the expected behavior**\r\nThe model should work as well serialized and not serialized and keep its support for ragged inputs in the dict.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/tanguycdls/552347a77758674c39b37471f72c3bdf/copie-de-untitled5.ipynb\r\n\r\n**TF nighlty**\r\nI found that https://www.tensorflow.org/guide/concrete_function#nested_arguments that stands that TF 2.3 will support that usage in a concrete function. I tried but its actually worse the model does not even support dict of ragged in the non serialized model: https://colab.research.google.com/gist/tanguycdls/e78f2c5329f85b84a8d8db7d1b6a40f1/copie-de-untitled5.ipynb\r\n\r\n**TF 2.1.0**\r\nSame as nightly breaks even non serialized on another error:\r\n\r\n> ValueError: Error when checking input: expected feature_1 to have 1 dimensions, but got array with shape (2, None)\r\n\r\nhttps://colab.research.google.com/gist/tanguycdls/996ea1f53de753c14cb837f1071e509c/copie-de-untitled5.ipynb\r\n", "comments": ["@tanguycdls \r\n\r\nI have tried in colab in TF 2.1.0 and 2.2-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/41b639aa2a57c78b9332f19bab1df756/untitled838.ipynb).Thanks!", "There's a bug in your code: the `shape` you pass in when constructing `Input` isn't correct.  In particular, you're using `shape=()`, which means that each batch has a single scalar value.  But in reality, each batch has a vector value of unknown size -- i.e., `shape=[None]`.  So you should change it to:\r\n\r\n```\r\nf_inputs = {'feature_1': Input(shape=[None], name='feature_1', ragged=True),\r\n            'feature_2': Input(shape=[None], name='feature_2', ragged=True)}\r\n```\r\n\r\nAnd then it works as expected.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38984\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38984\">No</a>\n"]}, {"number": 38983, "title": "[TF2.2rc3] dict of tensors as an input of a keras layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary pip\r\n- TensorFlow version (use command below): 2.2rc3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen we create a keras layer which takes a dict as input the behavior of the layer is not always consistent: \r\nwe create a custom layer such as : \r\n\r\n```\r\nclass EmbeddingMerger(tf.keras.layers.Layer):\r\n  def __init__(self, list_features, **kwargs):\r\n    super().__init__(**kwargs)\r\n    self.embeddings = {feature: Embedding(10, 3) for feature in list_features}\r\n  def call(self, inputs):\r\n    tensors = [self.embeddings[col](inputs[col]) for col in inputs]\r\n    return Add()(tensors)\r\n```\r\n\r\nWe can create a model from it:\r\n\r\n```\r\nlist_features = ['feature_1', 'feature_2']\r\nfeature_1 = tf.constant([0, 1, 3])\r\nfeature_2 = tf.constant([1, 2, 4])\r\nf = {'feature_1': feature_1,\r\n     'feature_2': feature_2}\r\nf_inputs = {'feature_1': Input(shape=(), name='feature_1'),\r\n            'feature_2': Input(shape=(), name='feature_2')}\r\nout = EmbeddingMerger(list_features)(f_inputs)\r\nmodel = Model(f_inputs, out)\r\n```\r\n\r\nIf we pass to the model:\r\n- the dict with the two features with the correct names **it works as expected.** (called truth) ({'feature_1'; feature_1, 'feature_12: feature_2})`\r\n- the dict with the two features but dict inserted in the wrong order : **gives same result as above** `({'feature_2'; feature_2, 'feature_1': feature_1})`\r\n- the dict with only one feature  breaks with an assertion error because ** it cannot compute such result its the good behavior ** `({'feature_1': feature_1})`\r\n- the dict with the two features and one additional key of name 'test' : **same result as truth which is the correct behavior ( ignoring the feature not used by the layer.)** `({'feature_1': feature_1, 'feature_2': feature_2, 'test': feature_2})`\r\n- a dict with key feature_1 but not feature_2 and one additional key 'test' of value feature_2 : **it gives a result which is not the same as the truth...** `({'feature_1': feature_1, 'test': feature_2})`\r\n> That one should not be calculable since we did not pass 'feature_2'\r\n\r\nThe same behavior happens with 3, 4 etc... keys I think i can extrapolate that if the correct keys are available everything is fine and the calculation is correct. However if one feature is missing it will take another feature in the dict and use it as it is. \r\n\r\n**Describe the expected behavior**\r\nIf the keys needed to run are not available always break with an assertion error. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/tanguycdls/552d6c5f02d8aff0b03fe1ad93825b52/untitled5.ipynb \r\nCode to do the experiments above. ping me for more use cases. \r\n", "comments": ["This behavior is not seen in TF 2.1.0 version. See [GitHub gist](https://colab.research.google.com/gist/ymodak/ec2d0015292fc6b433881fd28bb74439/untitled9.ipynb)", "@tanguycdls With the latest tf-nightly this issue has been fixed. Am closing this issue, but please reopen if thats not the case for you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38983\">No</a>\n"]}, {"number": 38982, "title": "MirroredStrategy Keras Example Hangs", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Jupyter Nodebook of a official kubeflow image\r\n`gcr.io/kubeflow-images-public/tensorflow-2.1.0-notebook-gpu:1.0.0` on my kubeflow platform.\r\n- TensorFlow version (use command below): 2.1.0-gpu\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: release 10.1, V10.1.243, but I can't find cuDNN libraries using a command `cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2`\r\n- GPU model and memory: T4 / 16G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: `v1.12.1-30591-g2d3828de27 2.2.0-dev20200427`\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nIn my notebook having 4 GPUs, I ran a MirroredStrategy Keras example documented in [here](https://www.tensorflow.org/tutorials/distribute/keras) and all GPUs's memories are occupied, but after printing below logs it hangs.\r\n- logs\r\n```\r\nmkc_choi@hbseo-m$ kubectl -nhanbae-seo logs -cdemo02 demo02-0\r\n[W 09:04:55.882 NotebookApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.\r\n[I 09:04:56.129 NotebookApp] JupyterLab extension loaded from /usr/local/lib/python3.6/dist-packages/jupyterlab\r\n[I 09:04:56.129 NotebookApp] JupyterLab application directory is /usr/local/share/jupyter/lab\r\n[I 09:04:56.349 NotebookApp] Serving notebooks from local directory: /home/jovyan\r\n[I 09:04:56.349 NotebookApp] The Jupyter Notebook is running at:\r\n[I 09:04:56.349 NotebookApp] http://demo02-0:8888/notebook/hanbae-seo/demo02/\r\n[I 09:04:56.349 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[I 09:05:07.112 NotebookApp] 302 GET /notebook/hanbae-seo/demo02/ (127.0.0.1) 0.85ms\r\n[I 09:05:12.368 NotebookApp] Creating new notebook in\r\n[I 09:05:13.255 NotebookApp] Kernel started: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:07:14.315 NotebookApp] Saving file at /Untitled1.ipynb\r\n[I 09:07:31.203 NotebookApp] Starting buffering for 85970d11-8ffb-4259-a0f9-29614d194712:f7966791845643a9bcb0bc02a3b60f8c\r\n2020-04-28 09:07:46.689575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 09:07:54.871181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-28 09:07:55.100937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.101951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties:\r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:55.102094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.103013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties:\r\npciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:55.103126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.104070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 2 with properties:\r\npciBusID: 0000:00:06.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:55.104191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.105138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 3 with properties:\r\npciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:55.105182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 09:07:55.108010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-28 09:07:55.110338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-28 09:07:55.111236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-28 09:07:55.113969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-28 09:07:55.115693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-28 09:07:55.120921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 09:07:55.121047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.122079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.123059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.124069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.125118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.126190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.127161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.128182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:55.129220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-04-28 09:07:55.130294: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-04-28 09:07:55.138816: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\r\n2020-04-28 09:07:55.139503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58629a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-28 09:07:55.139544: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-28 09:07:56.141806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.157822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.171327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.191060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.193694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x208f270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-28 09:07:56.193722: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2020-04-28 09:07:56.193728: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\r\n2020-04-28 09:07:56.193733: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5\r\n2020-04-28 09:07:56.193738: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5\r\n2020-04-28 09:07:56.199679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.201632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties:\r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:56.201722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.203759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties:\r\npciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:56.203839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.205793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 2 with properties:\r\npciBusID: 0000:00:06.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:56.205876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.207830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 3 with properties:\r\npciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 09:07:56.207873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 09:07:56.207902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-28 09:07:56.207918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-28 09:07:56.207933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-28 09:07:56.207942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-28 09:07:56.207955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-28 09:07:56.207965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 09:07:56.208030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.209924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.211728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.212713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.213698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.214697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.215598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.216512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:56.217462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-04-28 09:07:56.217520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 09:07:59.142161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-28 09:07:59.142208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 2 3\r\n2020-04-28 09:07:59.142216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N Y N N\r\n2020-04-28 09:07:59.142221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   Y N N N\r\n2020-04-28 09:07:59.142226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 2:   N N N Y\r\n2020-04-28 09:07:59.142230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 3:   N N Y N\r\n2020-04-28 09:07:59.142584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.143574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.144696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.145672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.146611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.147517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13969 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2020-04-28 09:07:59.148310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.149214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13969 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\r\n2020-04-28 09:07:59.149788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.150683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 13969 MB memory) -> physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)\r\n2020-04-28 09:07:59.151243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 09:07:59.152188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 13969 MB memory) -> physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)\r\n2020-04-28 09:07:59.157773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n[I 09:09:33.647 NotebookApp] Saving file at /Untitled1.ipynb\r\n[I 09:10:40.103 NotebookApp] 302 GET /notebook/hanbae-seo/demo02/ (127.0.0.1) 0.62ms\r\n[I 09:12:43.364 NotebookApp] Saving file at /Untitled1.ipynb\r\n[I 09:14:42.569 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:44.638 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:46.183 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:50.198 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:52.650 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:54.470 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:55.575 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n[I 09:14:58.246 NotebookApp] Kernel interrupted: 85970d11-8ffb-4259-a0f9-29614d194712\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/logging/__init__.py\", line 1945, in shutdown\r\n    h.flush()\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/logging/__init__.py\", line 892, in flush\r\n    self._current_handler.flush()\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/logging/__init__.py\", line 785, in flush\r\n    self.stream.flush()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\", line 341, in flush\r\n    if self.pub_thread.thread.is_alive():\r\nAttributeError: 'NoneType' object has no attribute 'thread'\r\n[I 09:15:06.715 NotebookApp] Kernel restarted: 85970d11-8ffb-4259-a0f9-29614d194712\r\n2020-04-28 09:15:12.082666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 09:15:42.166324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-28 09:15:42.402363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n.\r\n.\r\n.\r\n.\r\n.\r\n2020-04-28 11:50:42.127628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.130778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties:\r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:42.130944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.135680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties:\r\npciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:42.135781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.143098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 2 with properties:\r\npciBusID: 0000:00:06.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:42.143189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.150307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 3 with properties:\r\npciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:42.150351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 11:50:42.152504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-28 11:50:42.154902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-28 11:50:42.155426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-28 11:50:42.157576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-28 11:50:42.158808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-28 11:50:42.163515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 11:50:42.163622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.170033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.173795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.178082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.187495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.194851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.202891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.208997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:42.213464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-04-28 11:50:42.214783: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-04-28 11:50:42.222662: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\r\n2020-04-28 11:50:42.223394: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5d3a7c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-28 11:50:42.223423: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-28 11:50:43.541123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.558363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.569507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.585723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.587840: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5669ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-28 11:50:43.587865: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2020-04-28 11:50:43.587871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\r\n2020-04-28 11:50:43.587876: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5\r\n2020-04-28 11:50:43.587880: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5\r\n2020-04-28 11:50:43.593961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.595914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties:\r\npciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:43.595983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.597987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 1 with properties:\r\npciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:43.598069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.600110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 2 with properties:\r\npciBusID: 0000:00:06.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:43.600215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.602185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 3 with properties:\r\npciBusID: 0000:00:07.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-04-28 11:50:43.602229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 11:50:43.602250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-28 11:50:43.602265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-28 11:50:43.602275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-28 11:50:43.602287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-28 11:50:43.602296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-28 11:50:43.602305: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-28 11:50:43.602363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.604392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.606481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.608546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.610742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.612800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.615044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.616887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:43.618745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-04-28 11:50:43.618810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-28 11:50:45.761140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-28 11:50:45.761202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 2 3\r\n2020-04-28 11:50:45.761210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N Y N N\r\n2020-04-28 11:50:45.761216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   Y N N N\r\n2020-04-28 11:50:45.761220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 2:   N N N Y\r\n2020-04-28 11:50:45.761225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 3:   N N Y N\r\n2020-04-28 11:50:45.761562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.762656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.763675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.764693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.765747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.766736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13969 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2020-04-28 11:50:45.767518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.768518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13969 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\r\n2020-04-28 11:50:45.769142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.770076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 13969 MB memory) -> physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)\r\n2020-04-28 11:50:45.770664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-28 11:50:45.771659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 13969 MB memory) -> physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)\r\n2020-04-28 11:50:47.091531: I tensorflow/core/profiler/lib/profiler_session.cc:154] Profiler session started.\r\n2020-04-28 11:50:47.091683: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1372] Profiler found 4 GPUs\r\n2020-04-28 11:50:47.093991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\n2020-04-28 11:50:47.194680: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1422] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-04-28 11:50:47.283735: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.302659: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.321059: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.341426: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.342187: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.344125: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.345834: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.347581: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.404901: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.424905: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.444686: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.463115: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.463881: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.465529: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.467173: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:47.468860: W tensorflow/core/kernels/data/captured_function.cc:458] Disabling multi-device execution for a function that uses the experimental_ints_on_device attribute.\r\n2020-04-28 11:50:51.405795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-28 11:50:52.668100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n```\r\n\r\n- Outputs generated with the example on Nodebook\r\n```\r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\n2.2.0-dev20200427\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\r\nNumber of devices: 4\r\nEpoch 1/12\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n```\r\n\r\n- The MirroredStrategy Keras example I applied (https://www.tensorflow.org/tutorials/distribute/keras)\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\nimport os\r\n\r\nprint(tf.__version__)\r\n\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\n# You can also do info.splits.total_num_examples to get the total\r\n# number of examples in the dataset.\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n\r\n  return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n\r\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\n# Define the checkpoint directory to store the checkpoints\r\ncheckpoint_dir = './training_checkpoints'\r\n# Name of the checkpoint files\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\n# Function for decaying the learning rate.\r\n# You can define any decay function you need.\r\ndef decay(epoch):\r\n  if epoch < 3:\r\n    return 1e-3\r\n  elif epoch >= 3 and epoch < 7:\r\n    return 1e-4\r\n  else:\r\n    return 1e-5\r\n\r\n# Callback for printing the LR at the end of each epoch.\r\nclass PrintLR(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\r\n                                                      model.optimizer.lr.numpy()))\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\r\n                                       save_weights_only=True),\r\n    tf.keras.callbacks.LearningRateScheduler(decay),\r\n    PrintLR()\r\n]\r\n\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\n```\r\n\r\n- And GPU memories are occupied but all GPU's `GPU-Util Compute M.` is 0%.\r\n```\r\nmkc_choi@hbseo-g1:~/tf-operator/examples/v1/multi$ nvidia-smi\r\nTue Apr 28 13:50:50 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   71C    P0    32W /  70W |  14612MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\r\n| N/A   41C    P0    27W /  70W |  14612MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |\r\n| N/A   41C    P0    26W /  70W |  14612MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\r\n| N/A   41C    P0    27W /  70W |  14612MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     24210      C   /usr/bin/python3                           14601MiB |\r\n|    1     24210      C   /usr/bin/python3                           14601MiB |\r\n|    2     24210      C   /usr/bin/python3                           14601MiB |\r\n|    3     24210      C   /usr/bin/python3                           14601MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n- When I specify `CUDA_VISIBLE_DEVICES` number as below, it run, but on only one GPU.\r\n```\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n```\r\n\r\n- And I tried to set \"tf.data.experimental.AutoShardPolicy.OFF\" described in an existing issue [here](https://github.com/tensorflow/tensorflow/issues/35878), but the result is same. codes are below\r\n```\r\nimport os\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.MirroredStrategy()\r\n# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n    metrics=['accuracy'])\r\n  return model\r\nGLOBAL_BATCH_SIZE = 64 * 2\r\nwith strategy.scope():\r\n  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()\r\n  options = tf.data.Options()\r\n  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n  train_datasets = train_datasets.with_options(options)\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n- The example runs with multiple GPUs\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["I've tried running mirroredstrategy with my keras model using ai-platform and instead of hanging, I get a segmentation fault with no clues as to what the source of the issue is.", "@Abhipray I am not what is the root-cause. I ran the tutorial's colab and it is working without any issue. Did you change any parameters in the model? Have you ever used all four GPU earlier for other models? Can you run `physical_devices = tf.config.list_physical_devices('GPU') ` and see what are the `phyical_devices` seen by TF. Thanks", "I ran the example with `os.environ['NCCL_DEBUG'] = 'INFO'`\r\nand I got `NET/Plugin : No plugin found (libnccl-net.so)` message as below.\r\n```\r\nmkc_choi@hbseo-m:~$ kubectl -nhanbae-seo logs -cdemo02 demo02-0\r\n.\r\n.\r\n.\r\n.\r\n2020-04-29 02:13:05.272761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-29 02:13:06.699714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\ndemo02-0:1416:1495 [0] NCCL INFO Bootstrap : Using [0]eth0:10.46.0.2<0>\r\ndemo02-0:1416:1495 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n\r\ndemo02-0:1416:1495 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\ndemo02-0:1416:1495 [0] NCCL INFO NET/Socket : Using [0]eth0:10.46.0.2<0>\r\nNCCL version 2.5.7+cudaCUDA_MAJOR.CUDA_MINOR\r\ndemo02-0:1416:1655 [1] NCCL INFO Setting affinity for GPU 1 to 03ff\r\ndemo02-0:1416:1654 [0] NCCL INFO Setting affinity for GPU 0 to 03ff\r\ndemo02-0:1416:1656 [2] NCCL INFO Setting affinity for GPU 2 to 03ff\r\ndemo02-0:1416:1657 [3] NCCL INFO Setting affinity for GPU 3 to 03ff\r\n```\r\nso, I tried to set `NCCL_P2P_DISABLE=1` as described in a [issue](https://github.com/tensorflow/tensorflow/issues/32654), but result is same.\r\n\r\n@jvishnuvardhan In my case, \r\nI got outputs on my notebook\r\n```\r\n##In\r\nimport tensorflow as tf\r\ntf.config.list_physical_devices('GPU')\r\n##Out\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\r\n```", "Hi - I am a little confused - you pasted 2 logs in the original issue description. one seems to be the hanging case and one seems to be the working case. Is that right? If so, what is the difference between the two? \r\n\r\nAlso, is it possible to test with the latest TF nightly? \r\n\r\nI also see some profiler related errors in your logs (CUPTI_ERROR_INSUFFICIENT_PRIVILEGES), but I am not sure if those are related to the hanging at all or not.. likely not. \r\n", "@guptapriya Sorry for the confusion.\r\nAll logs I pasted are the hanging case,\r\nfirst in `logs` section is container's STDOUT,\r\nthe other in `Outputs generated with the example on Nodebook` is outputs printed on nodebook.\r\nAll logs were captured at the same time, the hanging case.\r\n\r\nI will test with the latest TF nightly, and post the result here soon.\r\nThanks.", "@guptapriya I have just run with the latest TF nightly, but it also got stuck at the same point.", "> When I specify CUDA_VISIBLE_DEVICES number as below, it run, but on only one GPU.\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n\r\nAre you saying the code is working with single GPU:0 and hangs when you use 4 GPUs? Thanks!", "Thanks for clarifying @jazzsir . In order to help narrow down the issue can you try 2 things:\r\n1) remove all callbacks and try  (to see if the issue is with checkpointing or tensorboard etc)\r\n2) try `strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute. ReductionToOneDevice())` - this is to check whether NCCL is the culprit. \r\n", "@jvishnuvardhan,Yes, that\u2019s right. Thanks\r\n@guptapriya, after replacing cross_device_ops with ReductionToOneDevice(), it works, NCCL is the culprit, is there any way to fix NCCL on GCP? Thanks.", "Hm, I remember on GCP there may sometimes be issues when 4 GPUs are used. Can you perhaps try using 8 GPUs and see if you still see the problem? \r\nI will meanwhile also try to check with someone from GCP.", "Did you already try running these: https://github.com/nvidia/nccl-tests as suggested in https://github.com/tensorflow/tensorflow/issues/32654#issuecomment-536229390, to make sure NCCL itself is working?", "I was experiencing a similar issue with the MirroredStrategy example and it turned out to be the `TensorBoard` callback. If I commented out that callback, the example seemed to work as expected.", "@guptapriya sorry for my late response, I finally found it!, actually I ran the Strategy on my Kubeflow platform where Istio controls traffic and istio-proxy sidecar cut off NCCL communication.\r\nI appreciate everything you've done for me so far.\r\n@davidrpugh Thanks, I checked, but it's because of istio-proxy.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38982\">No</a>\n", "Thanks @jazzsir for the update! Glad you found the root cause. ", "If ISTIO is causing problems you can add the annotation to your pods\r\n\r\n```\r\n      annotations:\r\n        sidecar.istio.io/inject: \"false\"\r\n```\r\n\r\nTo disable istio sidecar injection.\r\n\r\nMore information \r\nhttps://istio.io/docs/setup/additional-setup/sidecar-injection/", "Thanks @jlewi ", "> TensorBoard\r\n\r\nI am also facing the same issue. Did you try using Tensorflow/Tensorboard with some other versions? @davidrpugh "]}, {"number": 38981, "title": "Build won't start.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.13.6(17g65)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.14\r\n- Python version:3.6.5\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.24.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory:GTX GeForce 1080ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to build tensorflow 1.14, but I get an error like this and I can't start it.\r\nPlease tell why it is\r\n\r\n`ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1033, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 607, in _find_libs\r\n\t\t_find_cuda_lib(\"cuda\", repository_ctx, cpu_value, (cu...), ...)\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 588, in _find_cuda_lib\r\n\t\tfind_lib(repository_ctx, [(\"%s/%s\" % (based...))], ...)))\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 565, in find_lib\r\n\t\tauto_configure_fail((\"No library found under: \" + \",...)))\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 324, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/local/cuda/lib/stubs/libcuda.dylib\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1033, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 607, in _find_libs\r\n\t\t_find_cuda_lib(\"cuda\", repository_ctx, cpu_value, (cu...), ...)\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 588, in _find_cuda_lib\r\n\t\tfind_lib(repository_ctx, [(\"%s/%s\" % (based...))], ...)))\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 565, in find_lib\r\n\t\tauto_configure_fail((\"No library found under: \" + \",...)))\r\n\tFile \"/Users/zen/tensorflow/third_party/gpus/cuda_configure.bzl\", line 324, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/local/cuda/lib/stubs/libcuda.dylib\r\n`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@zenboooooon \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38981\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38981\">No</a>\n"]}, {"number": 38980, "title": "Fixed some of the XLA tests for IBM's Power architecture", "body": "On IBM Power, some of the XLA tests used to fail with error \"no target available for host triple x86..\".\r\nSo, the PR tries to fix it in a way that it works on Power as well as x86. Other platforms can add their cases in cpu_compiler.h for these tests to work for them.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38980) for more info**.\n\n<!-- need_sender_cla -->", "@npanpaliya Thank you for your contribution. Can you please sign CLA? Thanks!", "Being an IBM Contractor, I've previously committed to this repo with appropriate Corporate CLA. It's just that my email has been changed now from \"nishidha@us.ibm.com\" to \"npanpa23@in.ibm.com\".", "@npanpaliya you should sign with all the emails associated, thank you ", "@googlebot I signed it!\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38980) for more info**.\n\n<!-- ok -->", "@sanjoy and @joker-eph - Could you please review this PR?", "> Can you share if you're using XLA CPU on IBM Power? Please feel free to follow up over private email (sanjoy AT {my employer}.com).\r\n> \r\n> Thanks!\r\n\r\nYes.", "@npanpaliya Any update on this PR? Please. Thanks!", "@gbaned - I'm sorry for the delay. I've done the suggested changes. But due to some other high priority work, could not test and push those. I'll do it tomorrow for sure.", "I'm really sorry for not being able to push my changes which address the review comments. I've been facing the build failure since yesterday due to ruy update. The latest ruy version being used by TF doesn't work on Power.", "I've put a PR to fix ruy on Power. https://github.com/google/ruy/pull/83\r\nOnce this is accepted and merged, I'll raise another PR to bump up the version of ruy to the one that has fix for Power.", "Thanks for the review @sanjoy !", "Thanks @sanjoy for the review. I just pushed one more commit to fix one build error after you approved the changes. Could you please take a minute to look at it and approve this again? ", "@npanpaliya can you please check build failures ?", "@rthadur - I've fixed the build error. Thanks!", "@npanpaliya Can you please address Ubuntu Sanity errors? Thanks!", "I've been looking at them but I don't really understand what is the problem now. ", "I've fixed the BUILD files using bazel buildifier utility. Hopefully all checks should be fine now.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 38979, "title": "Completed validation_data entry for .fit()", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38979) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38979) for more info**.\n\n<!-- ok -->", "@WurmD Can you please resolve conflicts? Thanks!", "Is this the problem of this PR or the problem of the CIs? It looks like that the CIs are stuck.", "> Is this the problem of this PR or the problem of the CIs? It looks like that the CIs are stuck.\r\n\r\n@tqa236 It is waiting for review. Thanks!", "@WurmD  Can you please check @fchollet's comments and keep us posted ? Thanks!", "any update on this? \r\n\r\naccording to https://github.com/tensorflow/tensorflow/issues/39797,  validation_data CAN in fact be a Sequence object?   if so, should we merge this docstring update and amend the last sentence in the param documentation:\r\n\r\n\"Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\"\r\n\r\n", "We're fixing the docstring as part of a separate change. Sorry for the confusion with the initial review, I meant that the statement *\"Note that validation_data does not support all the data types that are supported in x, eg, dict, generator or keras.utils.Sequence\"* was no longer accurate. "]}, {"number": 38978, "title": "mismatching quantization parameters(scale and zero point)", "body": "**System information**\r\n- Linux Ubuntu 18.04  x86-CPU\r\n- TensorFlow installed from binary\r\n- TensorFlow version 1.15 - GPU\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n`import tensorflow as tf\r\nimport numpy as np\r\n\r\ndef representative_dataset_gen():\r\n  for _ in range(100):\r\n    fake_image = np.random.random((1,432,368,3)).astype(np.float32)\r\n    yield [fake_image]\r\n\r\ngraph_pb = 'graph_freeze.pb'\r\ninp = ['image']\r\nout = ['Openpose/concat_stage7']\r\nconverter=tf.lite.TFLiteConverter.from_frozen_graph(\r\n        graph_pb, inp, out,input_shapes={\"image\":[1,432,368,3]})\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\n\r\nf = open(\"tflite_model/mobilenet_thin_openpose_opt_fullint_tf1.tflite\", \"wb\")\r\nf.write(tflite_model)\r\n\r\nf.close()\r\n\r\nprint(\"conversion complete\")\r\n`\r\n**GraphDef used for conversion**\r\nhttps://drive.google.com/file/d/1uMdkXXklwDNczD5ZGQR73Rkg9MFEax82/view?usp=sharing\r\n\r\n**converted model**\r\nhttps://drive.google.com/file/d/1BZ0gVX00Vnqhthx_wxmMXQAbkisSAftN/view?usp=sharing\r\n\r\n**Failure details**\r\nI'm currently trying to convert a graghdef file into a fully quantized tflite model. I want to use Coral Edge TPU for inference for which the converted TFLite model needs to be passed through EdgeTPU Compiler. Unfortunately the converted TFlite model is rejected by the compiler. Upon Investigation with a person from Coral Team he pointed me out to this \"There is a layer name Openpose/MConv_Stage3_concat with 2 preceeded layers that has mismatching quantization parameters(scale and zero point)\"  \r\n\r\nYou can also take a look at the discussion in the issues thread pertaining to the Coral EdgeTPU Implementation. #https://github.com/google-coral/edgetpu/issues/100#issue-604992155\r\nWould be nice if I could fix the issue.\r\n\r\nThanks", "comments": ["Was able to reproduce the issue. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/6a2647d6e1e42f6e91e77f547480460c/38978.ipynb). Thanks!", "@jvishnuvardhan Is the issue fixed by any chance?", "Does this issue occurs on TF nightly?", "I actually converted the model using a stable 1.x version of tf, haven't tried with tf-nightly", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38978\">No</a>\n"]}, {"number": 38977, "title": "Ifx/pr back to back fq", "body": "Improvements to QAT support in TFLite conversion.\r\n\r\n- Adds a warning if redundant back-to-back fake quantization ops are detected + associated imrpovements - typically as sign of an error in construction of a quantization-aware-traininable model construction is detect. \r\n\r\n- mlir::TFL::Optimize: suppress premature elimination of Dequant ; Quant pairs that are not no-ops (i.e. applied to value of differing quantization).\r\n\r\n- Handling of FakeQuantWithMinMaxArgsOp in mlir::TFL::PrepareTFPass is now consistent with that of constant min/max input FakeQuantWithMinMaxVarsPerChannelOp and FakeQuantWithMinMaxVarsOp.\r\n\r\n- tf_tfl_translate test application enhanced  to expose command-line options controlling MLIR::PassManager logging.\r\n\r\nImplementation follows guidance by Feng Liu <fengliuai@google.com>.", "comments": ["@andrewstevens-infineon please open the PR against master branch , we will not be accepting changes to release branches unless it is a security fix.\r\nCC @mihaimaruseac"]}, {"number": 38976, "title": "How to check if a tensor is empty in TensorFlow", "body": "Hi there,\r\n\r\nI am just using TensorFlow to train a CNN model. I used a command:\r\n\r\n    t_iteration = tf.placeholder('int32', [], name='t_iteration') to initialize a tensor with shape of (0,).\r\n\r\nI would want to know how to judge if the tensor t_iteration is empty? I have tried to use\r\n\r\n    x = tf.constant(3)\r\n    y = tf.constant(5)\r\n     idx0 = tf.shape(t_iteration)\r\n            A = tf.cond(tf.cast(idx0 == 0, tf.bool),\r\n                                        lambda: tf.multiply(x, 17),\r\n                                        lambda: tf.add(y, 23))\r\nBut I found that this method didn't work-as the `idx0' always equals to 0 even the tensor has not been assigned any value.\r\n\r\nHow to solve this tricky issue?\r\n\r\nThanks!", "comments": ["Refer to stackoverflow here:\r\nhttps://stackoverflow.com/questions/50529309/how-to-check-if-a-tensor-is-empty-in-tensorflow\r\n\r\nQuick Note:\r\nplaceholder is a tf1.x module.  if you are using tf2.x, you have to use tf v1 compatibility, refer to doc here:\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder", "@Dee-Ma \r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n\r\nplease check to [link](https://stackoverflow.com/questions/50529309/how-to-check-if-a-tensor-is-empty-in-tensorflow) for reference", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Tensorflow requires a function to check if a tensor is empty"]}, {"number": 38975, "title": "Add seed parameter to a Dense layer so that kernel initializers could use it for reproducibility", "body": "Please see corresponding feature request https://github.com/tensorflow/tensorflow/issues/38974 for more details", "comments": ["Closing this PR since the related issue can be addressed without updating the code. Feel free to reopen it if you feel otherwise. Thanks."]}, {"number": 38974, "title": "Add seed parameter to a Dense layer so that kernel initializers could use it for reproducibility", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): tf.version.GIT_VERSION = v2.1.0-rc2-17-ge5bf8de410\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now `initializers.py` package has initializers that could be seeded but there is no way for users to specify seed when model and its layers are being specified.\r\n\r\n**Will this change the current api? How?**\r\nYes, adds seed parameter to a Dense layer. So docs would probably need to be updated.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who will want to have reproducible results from their neural network. Some initializers already have `seed` parameter in their API so this PR is just an attempt to make integration of APIs more complete.\r\n**Any Other info.**\r\n\r\n- As there probably are other Layers that have kernel initializers we need to check them and add support for  them as well\r\n- regularizers and constraints theoretically can also leverage availability of seed but in current implementation I don't see any of them to be using seeds\r\n- I was trying to run all tests in tensorflow by using Docker but not sure that I'm doing it correctly. It was taking too long and I could see C++ tests to be running. I would appreciate some guidance here. So I was able to run only my own test that I will submit within upcoming PR\r\n", "comments": ["Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@ravikyram here is a PR https://github.com/tensorflow/tensorflow/pull/38975 .  During benchmarking you might want to fix everything in your NN and see effects from changes in a particular part of the model. This helps during testing and research. ", "@ravikyram people out there are trying to do this: https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\r\n\r\nextract from this article\r\n\r\n>What if I Am Still Getting Different Results?\r\nTo re-iterate, the most robust way to report results and compare models is to repeat your experiment many times (30+) and use summary statistics.\r\nIf this is not possible, you can get 100% repeatable results by seeding the random number generators used by your code. The solutions above should cover most situations, but not all.\r\nWhat if you have followed the above instructions and still get different results from the same algorithm on the same data?\r\nIt is possible that there are other sources of randomness that you have not accounted for.\r\n\r\nSo it means people we go to check other libraries for randomness but there is still randomness in tensorflow itself that users don't have easy control over", "If you want to have an initializer that is reproducible, you can create the initializer and pass it to the dense layer, as the param for kernel_initializer or bias_initalizer. I think the current API is flexible enough without adding the seed param. I am closing this issue for now. Let me know if this is any other issue.\r\n\r\nEg:\r\n```\r\ninit = tf.keras.initializers.GlorotNormal(seed=123)\r\ndense = tf.keras.layers.Dense(4, kernel_initializer=init)\r\n```\r\n"]}, {"number": 38973, "title": "Error of installation", "body": "**System information**\r\n- Windows 10 Pro\r\n- Python version: 3.8.2\r\n- GPU model and memory: \r\n   -GeForce GTX 1080 Ti\r\n   - 19281 MB\r\n\r\n**Describe the problem**\r\n\r\nHi everyone, I had a problem on installing Tensorflow 2.1.0 via pip on Windows; as soon as i use this command i get this error:\r\n    pip install tensorflow\r\n    ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\n    ERROR: No matching distribution found for tensorflow\r\nHow can I fix this problem?", "comments": ["@Sim0xx001,\r\nTensorFlow 2.1 does not support Python 3.8. Instead could you please try installing TensorFlow v2.2.0rc3 using the following command.\r\n\r\n`pip install tensorflow==2.2.0rc3`\r\n\r\nThanks!", "Sure, I will try. Thank you so much! ", "It doesn't work, it returns the same error. Could it be the version of pip? I've installed pip 20.0.2", "You need MS Visual C++ redistributable as well. Please have a look at this issue as well #38916", "So if I install MS Visual C++ I will be able to execute pip install tensorflow? If I need to wait until it will be supported for python 3.8 it is not a problem because at the moment it is only a school project and I'm using Google Colab, that works perfectly. Anyway thanks you all for your help!", "> \r\n> \r\n> So if I install MS Visual C++ I will be able to execute pip install tensorflow? If I need to wait until it will be supported for python 3.8 it is not a problem because at the moment it is only a school project and I'm using Google Colab, that works perfectly. Anyway thanks you all for your help!\r\n\r\nNo see the complete installation steps [here](https://www.tensorflow.org/install/pip#windows)\r\n\r\nYou need Visual C++ as well. And supported python version 3.8 is not supported as of now. So supported python version with proper visual C++ installation is required. Then a `pip install tensorflow` will be fine.", "Ok, so I will install it; thank you so much!", "@Sim0xx001,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Yes, it is. But it's all about time, I will wait!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38973\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38973\">No</a>\n"]}, {"number": 38972, "title": "Adam optimizing performance issue", "body": "Hi,\r\nUsing tensorflow 2.2.0-rc3 on Ubuntu 18.04 and Python 3.8.2, I've converted my code for both compat.v1 and v2 which was required by tf2onnx.\r\nThere is a huge difference in speed between v1 and v2 and I was wondering how to solve this.\r\nMy v2 implementation is:\r\n```python\r\n\tbuilt_model = tf.keras.Model(inputs=model.inputs, outputs=model.outputs)\r\n\tmodel.load_weights()\r\n\r\n\t@tf.function\r\n\tdef train_fn(inputs, targets):\r\n\t\twith tf.GradientTape() as tape:\r\n\t\t\toutputs = built_model(inputs)\r\n\t\t\tloss, metrics, _ = dataset.loss(outputs, targets)\r\n\r\n\t\tgradients = tape.gradient(loss, built_model.trainable_variables)\r\n\t\toptimizer.apply_gradients(zip(gradients, built_model.trainable_variables))\r\n\t\treturn loss, metrics, learning_rate, global_step\r\n```\r\n\r\nwhile my v1 is:\r\n\r\n```python\r\n\tsession = tf.compat.v1.Session()\r\n\ttf.compat.v1.keras.backend.set_session(session)\r\n\tmodel.load_weights(session=session)\r\n\r\n\tloss, metrics, targets_pl = dataset.loss(model.outputs)\r\n\ttvars = tf.compat.v1.trainable_variables()\r\n\tgvs = optimizer.get_gradients(loss, tvars)\r\n\ttrain_op = optimizer.apply_gradients(zip(gvs, tvars))\r\n\r\n\tdef train_fn(inputs, targets):\r\n\t\tloss_, _, global_step_, metrics_, lr_ = session.run([loss, train_op, global_step, metrics, learning_rate], feed_dict=dict(zip(model.inputs+targets_pl, inputs+targets)))\r\n\t\treturn loss_, metrics_, lr_, global_step_\r\n```\r\n\r\nI also have a predict function that is as fast. It looks like the derivative is slow. \r\nI would like to migrate everything to v2.\r\nThank you for your help.\r\n", "comments": ["@christopher5106\r\ncode shared is not indented and incomplete to replicate, can you please provide complete code for us to replicate or if possible please share a colab gist for us to analyse. Thanks!", "Here it is: https://colab.research.google.com/drive/1qeTGvmfYbf5yjmCJzQVrgg0vvKAail4Z\r\nJust set eager=True to use API v2, otherwise it will use compat.v1 runs.\r\n\r\nRunning this code on our server with compat.v1 gives a duration of 21 s per epoch on GPU:\r\n\r\nT. 100/100 - il 9.25     - lr 0.001epoch duration 26.57380199432373\r\n\r\nEpoch 2/10\r\nT. 100/100 - il 9.55     - lr 0.001epoch duration 21.644425630569458\r\n\r\nEpoch 3/10\r\nT. 100/100 - il 9.15     - lr 0.001epoch duration 21.62645936012268\r\n\r\nwhile using it version 2 gives almost 2 times the duration:\r\n\r\nT. 100/100 - il 9.3      - lr 0.001epoch duration 49.872514486312866\r\n\r\nEpoch 2/10\r\nT. 100/100 - il 9.32     - lr 0.001epoch duration 38.29377722740173\r\n\r\nEpoch 3/10\r\nT. 100/100 - il 9.28     - lr 0.001epoch duration 38.43030595779419", "@christopher5106 \r\nI am unable to access the shared code due to denied permission.", "Here is a new link: https://colab.research.google.com/drive/1qeTGvmfYbf5yjmCJzQVrgg0vvKAail4Z", "I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/e5ca930415d270d760186e8277f76d52/38972ipynb.ipynb)", "@christopher5106 Can you please take a look at this [issue](https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1) and let me know if it helps.", "I'm not sure what you mean by that. Of course, I'm using the TF2 @function as you can see in the code.\r\n", "I was able to identify and debug the issue - the problem was that we were forcing all the embedding layer variables onto the CPU and that was slowing down the model. https://github.com/tensorflow/tensorflow/commit/46f7108d78c6a3c0854fe66ce1cd92e5ebb3d6e2?diff=split should fix the issue and from some preliminary testing its probably even 5-10% faster than before (not rigorously tested though)", "I confirm it is faster by  about 5% now. Thank you!"]}, {"number": 38970, "title": "RaggedTensor raises error with Keras TimeDistributed Layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, pip installed\r\n- TensorFlow version (use command below): pip install tensorflow-gpu==2.2.0rc3\r\n- Python version: 3.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5\r\n- GPU model and memory: GTX 1080 Ti 11G / 128G RAM\r\n\r\n**Describe the current behavior**\r\n\r\nI builded a hierarchical lstm model for binary classification:\r\n\r\n```python\r\nmodel = Sequential()\r\nmodel.add(layers.TimeDistributed(layers.Masking(-1),input_shape=(None,20,1)))\r\nmodel.add(layers.TimeDistributed(layers.LSTM(num_units_1,dropout=0.4)))\r\nmodel.add(layers.LSTM(num_units_2))\r\nmodel.add(layers.Dense(1))\r\nmodel.summary()\r\n```\r\n\r\nI then generated my `x_train` as a tf.RaggedTensor, with shape `[10000, None, 20, 1]`, each slice over the out most dimension of this ragged tensor is a tf.Tensor with shape `[x, 20, 1]`.\r\n\r\nThe reason I have to use RaggedTensor is I can't figure out a way to do padding over the 2nd dimension. My each input sequence is a variable length sequence of variable length sequences, I padded the lower time dimension and then mask it with time distributed masking, but there seems no apparent way how to do masking before the 2nd lstm layer.\r\n\r\nNow if run it with:\r\n\r\n```python\r\nhistory = model.fit(train_data, epochs=epochs, verbose=1, steps_per_epoch=-(-sample_count//batch_size))\r\n```\r\n\r\nI'd get:\r\n\r\n>ValueError: All inputs to `ConcreteFunction`s must be Tensors; on invocation of __backward_standard_lstm_1179546, the 0-th input (IndexedSlices(indices=Tensor(\"gradient_tape/sequential_29/lstm_55/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_29/lstm_55/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_29/lstm_55/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) was not a Tensor.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nTimeDistributed layer should support RaggedTensor if the underlying model can process slice of RaggedTensor and output dimension is matched.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, Model, Sequential\r\nimport numpy as np\r\n\r\nx = tf.RaggedTensor.from_row_splits(np.ones((100,20,1)),[0,4,20,100])\r\ny = np.ones((3,1))\r\n\r\nmodel = Sequential()\r\nmodel.add(layers.TimeDistributed(layers.LSTM(32,dropout=0.4),input_shape=(None,20,1)))\r\nmodel.add(layers.LSTM(24))\r\nmodel.add(layers.Dense(1,activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\r\nhistory = model.fit(x=x, y=y, epochs=10, verbose=1)\r\n```\r\n", "comments": ["@atodniAr,\r\nWas able to reproduce the issue with TF v2.1, [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/e17377657a6ae2efbaf000993cb9bd94/38970.ipynb#scrollTo=0W_BpvlRGGH1) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/daff2b4e3797d5729698833f2b67cd4a/38970-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@atodniAr  I can confirm that it is solved in TF v2.3 with a minor addition: \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, Model, Sequential\r\nimport numpy as np\r\n\r\nx = tf.RaggedTensor.from_row_splits(np.ones((100, 20, 1)), [0, 4, 20, 100])\r\ny = np.ones((3, 1))\r\n\r\nmodel = Sequential()\r\nmodel.add(layers.Input(shape=(None, 20, 1), ragged=True)) #this input with ragged=True is important\r\nmodel.add(layers.TimeDistributed(layers.LSTM(32, dropout=0.4)))\r\nmodel.add(layers.LSTM(24))\r\nmodel.add(layers.Dense(1, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\r\nhistory = model.fit(x=x, y=y, epochs=10, verbose=1)\r\n```", "@atodniAr \r\nI ran the code on tf-nightly (2.4.0-dev20200812) and the issue seems resolved, please verify with [this gist](https://colab.research.google.com/gist/Saduf2019/dfd2481aa8142d108e34dc079d840805/untitled367.ipynb), and confirm if the issue can be moved to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38970\">No</a>\n"]}, {"number": 38969, "title": "Thread hang when setting inter_op_parallelism_threads=1", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow version (use command below): tf-nightly\r\n- Python version: 3.6.8\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n\r\n\r\nThread will hang if setting `inter_op_parallelism_threads=1`.\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.eager import function\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import test_util\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import cond_v2\r\nfrom tensorflow.python.platform import test\r\n\r\ndisable_eager_execution()\r\ntf.config.threading.set_inter_op_parallelism_threads(num_threads=1)  # pass if set 2\r\n\r\n\r\nclass ThreadHangTest(test_util.TensorFlowTestCase):\r\n    \"\"\"reproduce thread hang when setting inter_op=1.\"\"\"\r\n\r\n    def testLoweringDisabledWithSingleThreadedExecutorContext(self):\r\n        with self.session() as sess:\r\n            @function.defun\r\n            def _add_cond(y):\r\n                return cond_v2.cond_v2(constant_op.constant(True, name=\"pred\"),\r\n                                       lambda: y,\r\n                                       lambda: y + 1)\r\n\r\n            x = array_ops.placeholder(shape=None, dtype=dtypes.float32)\r\n            with context.function_executor_type(\"SINGLE_THREADED_EXECUTOR\"):\r\n                out_cond = _add_cond(x)\r\n            sess.run(out_cond, feed_dict={x: 1.0})\r\n            \r\n \r\nif __name__ == '__main__':\r\n    test.main()\r\n\r\n```\r\n\r\n**Other info**\r\n\r\nCall stack on C++ side: `PartitionedCallOp::ComputeAsync() -> PartitionedCallOp::RunFunction() -> FunctionLibirayRuntimeImpl::Run()`. \r\n\r\n\r\n", "comments": ["@GHGmc2 It looks like you are using an older Version of Tensorflow (2.2). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.6) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38969\">No</a>\n"]}, {"number": 38968, "title": "Adding per-replica dataset distribution", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_sender_cla -->", "@kushanam Thank you for your contribution. Can you please sign CLA? Thanks!", "@gbaned it's been done.\r\n\r\n", "Also added @anj-s as reviewer as she has the most familiarity with the distributed input iterators etc", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\r\n\r\n@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->", "@kushanam it still shows CLA is pending , can you use please make sure to use same GitHub username and email-id associated with it.", "@googlebot I fixed it.", "@kushanam is this supposed to work for TF2 as well? Or just TF1?", "hi @kushanam  - any updates on this?", "Hi @guptapriya, Thanks for the follow up. yes I will submit the changes\nthis week...\n\nOn Sat, Jun 13, 2020 at 10:26 PM guptapriya <notifications@github.com>\nwrote:\n\n> hi @kushanam <https://github.com/kushanam> - any updates on this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38968#issuecomment-643720700>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKDMBKKME7EYXGKJ54TPW53RWRNRNANCNFSM4MSQJF7Q>\n> .\n>\n", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- cla_yes -->", "@kushanam Can you please check @anj-s's comments and resolve conflicts?. Thanks!", "@kushanam Can you please resolve conflicts? Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_consent -->", "> \r\n> \r\n> All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\r\n> \r\n> We need to confirm that all authors are ok with their commits being contributed to this project. Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\r\n> \r\n> _Note to project maintainer:_ There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent. In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\r\n\r\n@googlebot I consent", "@kushanam Can you please check @mihaimaruseac's comments and resolve conflicts?. Thanks!", "This seems like a broken rebase, bringing in commits from other PRs.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->", "@kushanam Can you please sign CLA. Thanks!\r\n", "> \r\n> \r\n> @kushanam Can you please sign CLA. Thanks!\r\n\r\n@gbaned Have signed it before and it shows in my contributor agreement!", "You have at least one commit using an `@ngvpn01-165-85.dyn.scz.us.nvidia.com` address. That one does not have CLA signed. Seems it was a misconfigured `git.email` address, maybe a `git amend` can help?", "We still need the CLA fix", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->", "> \r\n> \r\n> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\r\n@googlebot I fixed it.\r\n", "@kushanam Can you please sign CLA. Thanks!", "@kushanam  It still shows CLA is pending , can you please sign CLA. Thanks!", "> \r\n> \r\n> @kushanam It still shows CLA is pending , can you please sign CLA. Thanks!\r\n\r\n@gbaned Could you let me know again which email address/commit is giving the CLA issue? I can see the agreement signed on my ttps://cla.developers.google.com/clas page and last time the CLA was verified: https://github.com/tensorflow/tensorflow/pull/38968#issuecomment-664512078", "> @googlebot I fixed it.\r\n\r\n@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- ok -->", "> \r\n> \r\n> > @kushanam It still shows CLA is pending , can you please sign CLA. Thanks!\r\n> \r\n> @gbaned Could you let me know again which email address/commit is giving the CLA issue? I can see the agreement signed on my ttps://cla.developers.google.com/clas page and last time the CLA was verified: [#38968 (comment)](https://github.com/tensorflow/tensorflow/pull/38968#issuecomment-664512078)\r\n\r\nNevermind. it is fixed.", "@kushanam Can you please resolve conflicts? Thanks!", "@kushanam  Can you please resolve conflicts? Thanks!", "@kushanam Can you please resolve conflicts? Thanks!", "> \r\n> \r\n> @kushanam Can you please resolve conflicts? Thanks!\r\n\r\nConflicts resolved.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38968) for more info**.\n\n<!-- need_author_cla -->", " API change LGTM to @tensorflow/api-owners .\r\n\r\nImplementation LGTM is pending.", "@kushanam Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 38967, "title": "tensorflow ", "body": "![error](https://user-images.githubusercontent.com/60228576/80450122-7446dc00-893e-11ea-8520-6ad83835c874.png)\r\n\r\n", "comments": ["@DeepVasoya08 -\r\n\r\nI bet you have installed TF version 2.x and not 1.x. Can you please confirm? Try this: `tf.version.VERSION`.\r\n\r\nIf so, then, you simply have to refactor your code as follows:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfoo = tf.constant(90.0)\r\nbar = tf.constant(9.0)\r\n\r\n@tf.function\r\ndef multiply(x, y):\r\n    return x * y\r\n\r\nprint(multiply(x=foo, y=bar))\r\n```\r\n\r\nAs you can see [here](https://www.tensorflow.org/guide/effective_tf2#functions_not_sessions), session are just functions in TF 2.x.\r\n\r\nHope this helps.", "Not a code issue but a user using TF1.x API in TF2.x code. Closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38967\">No</a>\n"]}, {"number": 38966, "title": "How to convert TenorFlow model with functional ops to graph definition with no Tensorflow version installed?", "body": "In TensorFlow V2, functional control ops such if/while/for/case are introduced, if the model contains such functional control ops, how to convert them to graph definition?\r\nIn TensorFlow, there are some methods to transmit original node names correctly in file.\r\nInstantiateFunction - AddDefaultAttrs\r\n                                - helper.BuildNodeOutputIndex\r\n\t\t\t\t- helper.InstantiateNode\r\nBut in some other circustances if there is no TensorFlow installed, how to convert TenorFlow model with functional ops to graph definition?", "comments": ["It's a fairly complicated graph lowering step, and we currently don't have a standalone tool to do that."]}, {"number": 38965, "title": "[ROCm] Adding no_rocm tag to tests currently failing on ROCm platform", "body": "@cheshire @chsigg @nvining-work ", "comments": ["@chsigg gentle ping", "@chsigg  gentle ping", "@chsigg gentle ping"]}, {"number": 38964, "title": "Provide NVIDIA CUDA build data in metadata and API", "body": "This change:\r\n\r\nFirst exposes //third_party/gpus:find_cuda_config as a library.\r\n\r\nThen, it extends gen_build_info.py with find_cuda_config to provide package\r\nbuild information within TensorFlow's API. This is accessible as a\r\ndictionary:\r\n\r\n    from tensorflow.python.platform import build_info\r\n    print(build_info.cuda_build_info)\r\n    {'cuda_version': '10.2', 'cudnn_version': '7', 'tensorrt_version': None, 'nccl_version': None}\r\n\r\nFinally, setup.py pulls that into package metadata (I also formatted setup.py). The same wheel's\r\nlong description ends with:\r\n\r\n    TensorFlow 2.1.0 for NVIDIA GPUs was built with these platform\r\n    and library versions:\r\n\r\n      - NVIDIA CUDA 10.2\r\n      - NVIDIA cuDNN 7\r\n      - NVIDIA NCCL not enabled\r\n      - NVIDIA TensorRT not enabled\r\n\r\nIn lieu of [NVIDIA CUDA classifiers][1], the same metadata is exposed in the\r\nnormally-unused \"platform\" tag:\r\n\r\n    >>> import pkginfo\r\n    >>> a = pkginfo.Wheel('./tf_nightly_gpu-2.1.0-cp36-cp36m-linux_x86_64.whl')\r\n    >>> a.platforms\r\n    ['cuda_version:10.2', 'cudnn_version:7', 'tensorrt_version:None', 'nccl_version:None']\r\n\r\nI'm not 100% confident this is the best way to accomplish this. It\r\nseems odd to import like this setup.py, even though it works, even in\r\nan environment with TensorFlow installed.\r\n\r\nOne caveat for RBE: the contents of genrules still run on the local\r\nsystem, so I had to syncronize my local environment with the RBE\r\nenvironment I used to build TensorFlow. I'm not sure if this is going to\r\nrequire intervention on TensorFlow's current CI.\r\n\r\nCurrently tested only on Linux GPU (Remote Build) for Python 3.6. I'd\r\nlike to see more tests before merging.\r\n\r\nResolves https://github.com/tensorflow/tensorflow/issues/38351.\r\n\r\n[1]: https://github.com/pypa/trove-classifiers/issues/25", "comments": ["Tagging parties I know are interested: @gabrieldemarmiesse, @gunan, @sub-mod, @perfinion", "@alextp for API review\r\n", "@gunan I don't see any API changes", "@angerson, are you planning to add these to tf.config in a later change?", "Ah, I hadn't fully understood what makes something part of the public API. I've added `tf.config.get_cuda_version_used_to_compile_tf()` and `tf.config.get_cudnn_version_used_to_compile_tf()`.", "You don't have to take exactly the names that I proposed, it was just an example :)", "Thanks for your feedback! I replaced the functions with `tf.config.get_build_config()`, which returns a dictionary. I had planned to use a NamedTuple instead, but I feel that a Tuple implies more stability than a dictionary; the build info exposed here is not guaranteed to contain all of these fields, and a dictionary has a standard for using that (`dict.get()`).\r\n\r\nAlso, storing all of the information in a dict seems to remove the need to store any other module-level variables in the generated python file, so I've moved everything into the dict as key/value pairs, including the is_config_xxx flags, which don't seem to warrant needing specific flags.", "@angerson Can you please fix build failures ? Thanks!", "@angerson is it possible to backport this to 1.15.x branch ?", "@sub-mod Sure, we can backport and merge when ready to do a patch release", "Unfortunately, no.\r\nAll past release branches are closed for new features, even 2.2", "Oh, Gunhan is right, this is a new feature, not a bug fix", "@angerson @alextp @gunan These new API calls should be under `tf.sysconfig.foo` not under `tf.config.foo`\r\n\r\nThe other build-time type things are already under tf.sysconfig eg `tf.sysconfig.get_include()` or `tf.sysconfig.get_compile_flags()`\r\n\r\nTheres also `tf.test.is_built_with_cuda()` I think those should move under `tf.sysconfig` too but they are already released so they'd have to stay as aliases.", "I don't thinkin tf.sysconfig is a public namespace.\n\nOn Tue, May 5, 2020 at 11:06 AM Jason Zaman <notifications@github.com>\nwrote:\n\n> @angerson <https://github.com/angerson> @alextp\n> <https://github.com/alextp> @gunan <https://github.com/gunan> These new\n> API calls should be under tf.sysconfig.foo not under tf.config.foo\n>\n> The other build-time type things are already under tf.sysconfig eg\n> tf.sysconfig.get_include() or tf.sysconfig.get_compile_flags()\n>\n> Theres also tf.test.is_built_with_cuda() I think those should move under\n> tf.sysconfig too but they are already released so they'd have to stay as\n> aliases.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38964#issuecomment-624216895>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMENSKBS5WPSV2PKJLRQBIRPANCNFSM4MSLY43A>\n> .\n>\n\n\n-- \n - Alex\n", "Oh nevermind it is a public namespace.\n\nOn Tue, May 5, 2020 at 11:27 AM Alexandre Passos <apassos@google.com> wrote:\n\n> I don't thinkin tf.sysconfig is a public namespace.\n>\n> On Tue, May 5, 2020 at 11:06 AM Jason Zaman <notifications@github.com>\n> wrote:\n>\n>> @angerson <https://github.com/angerson> @alextp\n>> <https://github.com/alextp> @gunan <https://github.com/gunan> These new\n>> API calls should be under tf.sysconfig.foo not under tf.config.foo\n>>\n>> The other build-time type things are already under tf.sysconfig eg\n>> tf.sysconfig.get_include() or tf.sysconfig.get_compile_flags()\n>>\n>> Theres also tf.test.is_built_with_cuda() I think those should move under\n>> tf.sysconfig too but they are already released so they'd have to stay as\n>> aliases.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/38964#issuecomment-624216895>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AAABHRMENSKBS5WPSV2PKJLRQBIRPANCNFSM4MSLY43A>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n", "https://www.tensorflow.org/api_docs/python/tf/sysconfig I've never heard its not public? Its been in the docs for ages and theres no indication its not to be relied on? There isnt much in it so its likely overlooked most of the time but its definitely listed publicly.\r\n\r\nI know for sure custom-op uses it to make the build rules so it cant be removed either.", "It's not reflected here in this PR, but I've moved the call into tf.sysconfig in the being-internally-reviewed change.", "I'm proposing a rollback internally - I think the right way to do this is to fill in a template file from the repo rule (cuda_configure.bzl) instead of trying to call into find_cuda_config as part of the action, which as a genrule is marked \"local\" and can thus run on a different machine, which is brittle.", "Just noticed that this rule also has no dependency on running locally, so I think just making it a normal genrule and removing local=1 will work fine.\r\n\r\nThat said, I still think that we should not compute this info twice, as that seems more brittle than it needs to be."]}, {"number": 38963, "title": "image.image_gradients error \"Duplicate node name in graph: 'stack'\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10.0.17134\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- **Python version**: 3.7.7\r\n- **CUDA/cuDNN version**: Running on CPU\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen trying to use tf.image.image_gradients, get an error: \"ValueError: Duplicate node name in graph: 'stack'\", when trying to predict the model.\r\n\r\n### Source code / logs\r\nThe source code can be found below:\r\n\r\n```\r\nfrom tensorflow import image\r\nfrom tensorflow.keras import layers, Model\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nimport numpy as np\r\n\r\nmyInput = layers.Input(shape=(4, 4, 1))\r\n\r\n[deltaP_t_i, deltaP_t_j] = image.image_gradients(myInput[:, :, :, 0:1])\r\nloss_grad = tf.norm(deltaP_t_i, axis=(1, 2)) + tf.norm(deltaP_t_i, axis=(1, 2))\r\nloss_grad = K.mean(loss_grad, axis=-1)\r\n\r\nmyModel = Model(inputs=[myInput], outputs=[loss_grad])\r\n\r\nx = np.array([[1, 2, 3, 4],\r\n                [5, 6, 7, 8.1],\r\n                [9, 10, 11, 12],\r\n                [13, 14, 15, 16]])\r\n\r\nx = np.reshape(x, (1, 4, 4, 1))\r\n\r\nout1 = myModel.predict([x])\r\n```\r\n\r\nand the output is below:\r\n\r\n``` \r\n2020-04-27 18:50:11.221070: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1610, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate node name in graph: 'stack'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:/test/TestPhysicalLossToReport.py\", line 9, in <module>\r\n    [deltaP_t_i, deltaP_t_j] = image.image_gradients(myInput[:, :, :, 0:1])\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py\", line 3456, in image_gradients\r\n    shape = array_ops.stack([batch_size, height, 1, depth])\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 1165, in stack\r\n    return gen_array_ops.pack(values, axis=axis, name=name)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 6303, in pack\r\n    \"Pack\", values=values, axis=axis, name=name)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1773, in __init__\r\n    control_input_ops)\r\n  File \"C:\\Users\\emiliocoutinho\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1613, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Duplicate node name in graph: 'stack'\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["@emiliocoutinho \r\ni ran the code shared by you and there is no error faced, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/285fc09d4c59bcd4c4181edade91bdf5/untitled160.ipynb)\r\n\r\nplease refer to these links with similar error:\r\n[link1](https://github.com/tensorflow/tensorflow/issues/36509#issuecomment-583247460)\r\n[link2](https://github.com/tensorflow/tensorflow/issues/20997)\r\n[link3](https://stackoverflow.com/questions/53172215/duplicate-node-name-in-graph-conv2d-0-kernel-adam)", "@Saduf2019, thanks for this feedback. Using the tf-nightly version works fine. I was previously using TensorFlow 2.0.0. This problem also happens with version 2.1. With the nightly version, works."]}, {"number": 38962, "title": "Pip could not find version under 2.2 on Windows", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version: 1.15\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GPU model and memory:NVIDIA GeForce GTX1070, 8GB\r\n\r\n*Describe the problem*\r\nWhen attempting to install tensorflow 1.15 with pip on Windows 10, I receive an error that only various versions of 2.2 are available.\r\n\r\n> C:\\>pip3 install tensorflow==1.15\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3)\r\n> ERROR: No matching distribution found for tensorflow==1.15\r\n\r\nProblem also exists when trying to install another package with tensorflow<2.2 as a dependency - pip install magenta stops and gives the same error:\r\n\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow<2.0.0,>=1.15.0 (from magenta) (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3)\r\n> ERROR: No matching distribution found for tensorflow<2.0.0,>=1.15.0 (from magenta)\r\n\r\n", "comments": ["@CalePlut \r\n\r\nPlease upgrade pip and try.\r\n`python -m pip install --upgrade pip setuptools`.\r\n\r\n PIP install TF 1.15 is available.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/ravikyram/b10a5760a72ec20d5dd793c91d4523bd/untitled835.ipynb). \r\nCan you downgrade python to 3.6 and see if it works.Thanks!", "Downgrading to Python 3.6 worked, thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38962\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38962\">No</a>\n"]}, {"number": 38961, "title": "EdgeTPU compiler creates a model with different behaviour", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Docker image latest-gpu-py3-jupyter\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia RTX 2080TI / 12 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I try to compile fot edgeTPU a full-quatized and fully working model it just shows a different not expected output\r\n\r\n**Describe the expected behavior**\r\nSame output for same input in both models: full quantized and compiled for EdgeTPU from the previous full quantized.\r\n\r\n**Standalone code to reproduce the issue**\r\nYou can see the problem here\r\nhttps://gist.github.com/ianholing/cbda145cfeb03124d6a286c619302aa6\r\n\r\nAnd here there are anything you need to reproduce the notebook:\r\nhttps://drive.google.com/open?id=1u9fH3PVmfmR4xXIJnXOPlx789eyKhVOS\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["This should due to currently the input & output is float32 for full-quantization for tf.compat.v2.lite.TFLiteConverter in TF 2.1 even setting:\r\n```\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\nTFLite team will add this feature soon. \r\n\r\nThe current workaround is to modify the type of input & output [here](https://github.com/tensorflow/tensorflow/blob/a07ca66517313dc0ed0f20c074f762e9d3129112/tensorflow/lite/tools/optimize/python/modify_model_interface.py)\r\n\r\n", "I tried in some TF versions, even the nightly, and always same error, I don't know where this package is:\r\nModuleNotFoundError: No module named 'tensorflow.lite.tools'", "@ianholing It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "Hi @sushreebarsa, you can close this issue if you want, this was more than one year ago.. at that time i tried all the latest versions but nowadays I don't keep the code to try again sorry.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38961\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38961\">No</a>\n"]}, {"number": 38960, "title": "go: Add input mapping option when importing Graph", "body": "I added a test, but I'm not able to run the test because it says\r\n\r\n`cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\"`", "comments": ["Can you run `go generate github.com/tensorflow/tensorflow/tensorflow/go/op` and then run the test? (As mentioned in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md)", "It should be working now. PTAL.", "@jhseu Can you please review this PR ? Thanks!", "Thanks! I'm new to this so let me know if there is anything else required from me. Will this be included in v2.4?"]}]