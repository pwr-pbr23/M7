[{"number": 12582, "title": "Double check after regex search for cudnn_path in configure.py", "body": "If chose wrong `cuDNN` version on `linux`, `configure.py` will throw an exception as shown below:\r\n\r\n```shell\r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1021, in <module>\r\n    main()\r\n  File \"configure.py\", line 997, in main\r\n    set_tf_cunn_version(environ_cp)\r\n  File \"configure.py\", line 688, in set_tf_cunn_version\r\n    cudnn_path_from_ldconfig).group(1)\r\nAttributeError: 'NoneType' object has no attribute 'group'\r\n```\r\n\r\nThis is because these code return `None`:\r\n\r\n```python\r\ncudnn_path_from_ldconfig = re.search('.*libcudnn.so .* => (.*)',\r\n                                     cudnn_path_from_ldconfig)\r\n```\r\n\r\nThis PR add double check after regex search.\r\n\r\nRelated PR: https://github.com/tensorflow/tensorflow/pull/12347 \r\n\r\n@wangqr @yifeif PTAL.", "comments": ["@ScorpioCPH, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @wangqr and @yongtang to be potential reviewers.", "Can one of the admins verify this patch?", "Maybe the next [if block](https://github.com/ScorpioCPH/tensorflow/blob/ab635bcafaa42f599e49ceb6ed324e32996d8b45/configure.py#L691-L693) should only run when `cudnn_path_from_ldconfig` is not None, since some file named `None.6.0` is not what we want", "@wangqr Yes, but before this block, exception has been thrown out on calling `.group(1)`", "Yes I cam confirm that we should check for the None value returned by `search`, and your commit does the check. I just suggest that maybe moving the following if block into the if block you've created is more proper.", "@wangqr Sorry, I don't understand what do you mean, could you show me the code?\r\n\r\nI think these two `if` blocks are doing different things, and the second one depends on the first one, in another word, `cudnn_path_from_ldconfig` is output of the first `if` block which is input of the second one.", "From\r\n```python\r\n      if cudnn_path_from_ldconfig != None:\r\n        cudnn_path_from_ldconfig = cudnn_path_from_ldconfig.group(1)\r\n      if os.path.exists('%s.%s' % (cudnn_path_from_ldconfig, tf_cudnn_version)):\r\n        cudnn_install_path = os.path.dirname(cudnn_path_from_ldconfig)\r\n        break\r\n```\r\nto\r\n```python\r\n      if cudnn_path_from_ldconfig != None:\r\n        cudnn_path_from_ldconfig = cudnn_path_from_ldconfig.group(1)\r\n        if os.path.exists('%s.%s' % (cudnn_path_from_ldconfig, tf_cudnn_version)):\r\n          cudnn_install_path = os.path.dirname(cudnn_path_from_ldconfig)\r\n          break\r\n```", "@wangqr PR updated, could you take another look, please?", "@ScorpioCPH I don't have write access to this repo...", "@ScorpioCPH let me know if you still cannot see the review comment.", "@yifeif I see your comment now, thanks! And fix it by amending last commit. PTAL. \r\n\r\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 12581, "title": " rnn.MultiRNNCell problems and solution", "body": "ValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x11d32cbd0> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/gru_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/gru_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([GRUCell(...)] * num_layers), change to: MultiRNNCell([GRUCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\nthe origin code:\r\nfrom tensorflow.contrib import rnn\r\n        inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name=\"inputs\")\r\n        keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\r\n        cell = rnn.GRUCell(10)\r\n       cell = rnn.DropoutWrapper(cell=cell, input_keep_prob=keep_prob)\r\n       cell = rnn.MultiRNNCell([cell for _ in range(5)], state_is_tuple=True)\r\n\r\n      outs, states = tf.nn.dynamic_rnn(cell=cell, inputs=look_up, dtype=tf.float32)\r\n\r\nsolution:\r\n      inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name=\"inputs\")\r\n      keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\r\n      cell = rnn.MultiRNNCell([rnn.DropoutWrapper(rnn.GRUCell(10), input_keep_prob=keep_prob) for _ in range(5)] , state_is_tuple=True)\r\n", "comments": []}, {"number": 12580, "title": "Add kernels for FusedBatchNormGrad when is_training=False", "body": "#10857 ", "comments": ["Can one of the admins verify this patch?", "@ppwwyyxx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zhangyaobit, @tensorflower-gardener and @keveman to be potential reviewers.", "Let's wait a bit to see if zheng-xq has any comments (This PR may affect lots of users, let's be extra careful :) ). Thanks!", "Any updates?", "Thanks for the patience, Yuxin! zheng-xq will respond soon.", "/CC @zheng-xq with the `@` sign to trigger notification.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Please merge this change. Thanks!"]}, {"number": 12579, "title": "[building tensorflow with bazel ] Error:C++ compilation of rule '@boringssl//:crypto' failed.", "body": "  **Environment:\r\n    GCC 4.9.1\r\n    glibc :2.11.3\r\n    bazel:0.4.0/0.4.5/0.5.3(these versions have been tried)\r\n    OS: SUSE \r\n It seems that boringssl can't work with command `bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package`. Here is the  log:**\r\n\r\n  WARNING: /hdata/users/rll/tensorflow/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /hdata/users/rll/tensorflow/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /var/lib/hive/.cache/bazel/_bazel_hive/e5053b6fc588ac2d9981b522e9f221e1/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nIn file included from /usr/include/fcntl.h:38:0,\r\n                 from external/boringssl/src/crypto/bio/socket_helper.c:21:\r\n/usr/include/sys/stat.h:372:56: error: array type has incomplete element type\r\n extern int futimens (int __fd, __const struct timespec __times[2]) __THROW;\r\n                                                        ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 47.092s, Critical Path: 17.01s\r\n", "comments": ["@jart Have you seen this build issue?", "Met the same error.\r\nEnvironment:\r\nGCC 5.3.1\r\nglibc :2.22\r\nbazel:0.5.3\r\nOS: openSUSE Leap 42.3", "The same here on \r\nGCC: 7.1.1\r\nglibc: 2.25\r\nbazel: 0.5.3\r\nOS: Fedora 26\r\n\r\nThe problem seems to be solved by adding `--copt=-O` to the bazel build.\r\ncf. [https://stackoverflow.com/a/40374431/6555620](https://stackoverflow.com/a/40374431/6555620)", "@VarIr  I tried it ,but the same error has been still there by adding `--copt=-O` .", "This error disappeared when I have upgraded all the software to the latest version.\r\nThe environment works:\r\nGcc gcc version 5.3.1 \r\nglibc: glibc-2.22-8.4.x86_64\r\nbazel: 0.6.1\r\nOS: openSUSE Leap 42.3\r\nTF: commit 9ff05e9e7f471a8487cdd8a7bb6fdd554055e2dd", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Met the same error\r\n\r\ngcc   4.8.5 20150623 (Red Hat 4.8.5-16)\r\nglibc  2.17-196.\r\nbazel  0.8.1- (@non-git)\r\nOS  Red Hat Enterprise Linux Server release 7.4 (Maipo)\r\n\r\n", "Have the same error\r\n\r\ngcc 4.9.1 \r\nbazel 0.9.0\r\nCentos 6.9 2.6.32-696.18.7.el6.x86_64 #1 SMP Thu Jan 4 17:31:22 UTC 2018 ", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Solved this error via patches.  See [this link](http://hwengineer.blogspot.kr/2018/01/ac922-redhat-python3-tensorflow-141.html).\r\n\r\n", "The patch could be inlined into the TensorFlow build but should ideally be upstreamed into Google's BoringSSL repo and then bump the version in our workspace.bzl. Contributions welcome SUSE friends.", "Please note this is being marked contributions welcome because RHEL 6 and old SUSE is outside official support matrix, which currently starts at RHEL 7 / Ubuntu 14 IIRC.", "@jart  I am not the writer of the patches I mentioned.  I expect the author - who is an IBM employee - will upstream the patches sooner or later.", "I ran into this issue when building tensorflow 1.4.1 using a toolchain which targets glibc 2.11.3.\r\n\r\nI was able to fix this by adding `#include <time.h>` before the `#include <fcntl.h>` line in the BoringSSL src/crypto/bio/socket_helper.c file.  \r\n\r\nFrom [this bug report](https://sourceware.org/bugzilla/show_bug.cgi?id=21371) it sounds as if this may effect other glibc versions.", "I appreciate the efforts of folks on this.  Just mentioning that we are dropping SyntaxNet and just going with spaCy in our project because SyntaxNet is essentially unbuildable without non-trivial effort from what is currently in GitHub.  This issue is just one example.  Alas!", "@ruanlele \r\nCould you please let us know if this is still an issue, as many bugs/issues have been fixed in the latest version", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/12579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/12579\">No</a>\n"]}, {"number": 12578, "title": "In the line 240, the string 'conv_3' miss the %d", "body": "I think the author missed the '%d' to name the related net scope, so the correct code is :\r\nnet = slim.conv2d(net, 256, [3, 3], scope='conv3_%d' % (i+1)) NOT net = slim.conv2d(net, 256, [3, 3], scope='conv3_' % (i+1))", "comments": ["Can one of the admins verify this patch?", "@Eric2016Lv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @asimshankar to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please address the CLA. Thanks!", "I have finished the CLA, then anything I need to do next? ", "Are the commits using the same email address you signed a CLA for? If now, you should change the email in the commit, and the CLAbot should pick that up.", "@Eric2016Lv I think you have to comment with \"I signed it!\". Please try that.", "I signed it!", "@Eric2016Lv please verify the CLA issue as we can't merge the change otherwise.", "@Eric2016Lv I did check the CLA database and you're not in. Can you check again? Did you sign via an organization?", "I will close this pull request due to inactivity.\r\nPlease feel free to reopen once you have taken care of the CLA.", "Acutally, I have signed the CLA.  The folowing link  I cliked and push the \"I Agree\" putton. Then, the reason why not successful which confused me. Would you help me? \r\nhttps://cla.developers.google.com/ \r\nThe attachment which is the picture that I took from the website, plz check.\r\n\r\nThx,\r\nEric Lv\r\n\r\n2017-11-04 \r\n\r\ndidi_lv \r\n\r\n\r\n\r\n\u53d1\u4ef6\u4eba\uff1agunan <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4\uff1a2017-11-04 11:35\r\n\u4e3b\u9898\uff1aRe: [tensorflow/tensorflow] In the line 240, the string 'conv_3' miss the %d (#12578)\r\n\u6536\u4ef6\u4eba\uff1a\"tensorflow/tensorflow\"<tensorflow@noreply.github.com>\r\n\u6284\u9001\uff1a\"Eric2016Lv\"<didi_lv@126.com>,\"Mention\"<mention@noreply.github.com>\r\n\r\nClosed #12578.\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or mute the thread.", "Is the email address you used to sign the CLA match the one you created the commit with?", "No I used the gmail to sign it.", "Now, I have added the gmail address into the Github email list. Please check it, thanks a lot.", "If not, I will give up to sign it, too complex for me. anyway, thanks all for the help.", "Can one of the admins verify this patch?", "CLA bot usually checks the commit email address, but let's see if this fixes it.\r\nCould you comment exactly with the words `I signed it!` in this thread?", "All commits have to be made with an email that's associated with a CLA. If\nthere's more than one email in a PR, clabot won't be able to check it, but\nit's message will change.\n\nOn Nov 4, 2017 12:37, \"gunan\" <notifications@github.com> wrote:\n\n> CLA bot usually checks the commit email address, but let's see if this\n> fixes it.\n> Could you comment exactly with the words I signed it! in this thread?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12578#issuecomment-341923955>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_ZpLXIvgInv40dGmr22W-2smMkA7ks5szLzvgaJpZM4PCInT>\n> .\n>\n", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "windows cmake issue is a known file availability issue.\r\nMerging."]}, {"number": 12577, "title": "remove unused member vairable, and clear compiler warning", "body": "I remove `highest_eax_` and clear this compiler warning, because it is unused.", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @tensorflower-gardener and @yuefengz to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "Right, this came from a copy-paste."]}, {"number": 12576, "title": "remove unused cpu allocator factory", "body": "Because `MakeCpuAllocator` was replaced with `cpu_allocator`,  so I remove unused `MakeCpuAllocator` factory method.", "comments": ["@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @martinwicke and @yuanbyu to be potential reviewers.", "Can one of the admins verify this patch?", "I do not see any benefits brought by this patch.\r\n\r\nNote that there is some bits of TF not open sourced (i.e. internal to Google, or other private branches), it is not always a good idea to judge if a piece of code is unused solely by what you see here.\r\n\r\nSee #8153 for a little bit of background why it is introduced in the first place.", "cc @reedwm who knows about memory allocators", "Jenkins, test this please", "GPU failure is unrelated."]}, {"number": 12575, "title": "Usage of bit_casted_tensor() function with C++ api?(implementation on IOS)", "body": "### Any tutorial on this bit_casted_tensor function?\r\nI am building an app on IOS based on ios-tensorflow model. The model generated a tensor with type float. However, for my model, a unit8 format is required. So I want to transfer the tensor's format.\r\n\r\nI found this, `bit_casted_tensor()`, maybe useful for directly transformation. But I when tried to use it in the code like the following,\r\n`   tensorflow::TTypes<tensorflow::uint8>::Tensor a_new_tensor = bit_casted_tensor(image_tensor);` the issue `use of undeclared identifier 'bit_casted_tensor' ` always showed up.\r\n\r\nAccording to reference, bit_casted_tensor function should use like this:\r\n\r\n`TTypes< T, NDIMS >::ConstTensor bit_casted_tensor() const `\r\n\r\n> Return the tensor data to an _Eigen::Tensor_ with the same size but a bitwise cast to the specified _dtype T_.\r\n> \r\n> Using a bitcast is useful for move and copy operations. NOTE: this is the same as tensor() except a bitcast is allowed. \r\n[TF Reference Link](https://www.tensorflow.org/api_docs/cc/class/tensorflow/tensor#classtensorflow_1_1_tensor_1afced940422a1e726d9487cb3cb039630)\r\n\r\nI am not sure if I use it correctly? Or it is due to the limited support of tensorflow function on IOS platform? \r\n\r\n## Keen for any kind of help !!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12574, "title": "Add support of int8/uint8/int16/uint16 for tf.subtract", "body": "This fix adds support of int8/uint8/int16/uint16 for `tf.subtract` (on CPU only)\r\n\r\nThis fix fixes #12571.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Jenkins, test this please.", "I tested locally for `//tensorflow/python/kernel_tests:matrix_solve_ls_op_test` and it passes without any issues. I think the Jenkins timeout failure (`matrix_solve_ls_op_test`) is not related to this PR.", "Yes, that failure is unrelated. Thanks!"]}, {"number": 12573, "title": "Updated iOS build to handle threading correctly, requires iOS 9 or later", "body": "There was a long-standing issue with Apple's version of clang not supporting thread local attributes, which are used inside Eigen in some places. As a 'temporary' hack, I defined these attributes out of existence, but this lingered for a lot longer than it should have, and I believe is the cause of issues like #12298. From comments there testing this fix, it looks like it does help.\r\n\r\niOS 9 and later support `thread_local`, so as an improvement this change moves to that. It will block building on older versions, but now that v11 is almost out I think supporting two versions back should be ok for most developers (though feedback is welcome if I'm wrong).", "comments": ["@petewarden, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @tensorflower-gardener and @vrv to be potential reviewers.", "Jenkins, test this please"]}, {"number": 12572, "title": "Updating the README to include tf_nightly information.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @alanyee and @vrv to be potential reviewers."]}, {"number": 12571, "title": "tf.subtract doesn't work for uint8 and uint16 images (such as PNG)", "body": "It seems that tf.subtract doesn't support uint8 and unit16 image.\r\nCould someone please add a PR to enable the uint16 and int16 support for subtraction.\r\n\r\nThanks,", "comments": ["Added PR #12574 for the support."]}, {"number": 12569, "title": "missing Documentation of the method AttentionWrapper.zero_state(...)", "body": "Hello , \r\n\r\n\r\nI have noticed that the method AttentionWrapper.zero_state( batch_size,dtype) does not have any description of its functionality in the  documentation website , below is a reference link : \r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper\r\n\r\nI really hope that this gets fixed , I have spent a couple of days trying to debug a code that I have written until I realized that I was misusing the method . \r\n\r\n\r\nthank you ", "comments": ["Can you describe more about how you were misusing the method? (adding @ebrevdo ...) ", "I did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below : \r\n```\r\ndef decoding_layer(dec_input, encoder_state,\r\n                   target_sequence_length, max_target_sequence_length,\r\n                   rnn_size,\r\n                   num_layers, target_vocab_to_int, target_vocab_size,\r\n                   batch_size, keep_prob, decoding_embedding_size , encoder_outputs):\r\n    \"\"\"\r\n    Create decoding layer\r\n    :param dec_input: Decoder input\r\n    :param encoder_state: Encoder state\r\n    :param target_sequence_length: The lengths of each sequence in the target batch\r\n    :param max_target_sequence_length: Maximum length of target sequences\r\n    :param rnn_size: RNN Size\r\n    :param num_layers: Number of layers\r\n    :param target_vocab_to_int: Dictionary to go from the target words to an id\r\n    :param target_vocab_size: Size of target vocabulary\r\n    :param batch_size: The size of the batch\r\n    :param keep_prob: Dropout keep probability\r\n    :param decoding_embedding_size: Decoding embedding size\r\n    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\r\n    \"\"\"\r\n    # 1. Decoder Embedding\r\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\r\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\r\n\r\n    # 2. Construct the decoder cell\r\n    def create_cell(rnn_size):\r\n        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\r\n                                            initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\r\n        drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\r\n        return drop\r\n\r\n\r\n    dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\r\n    #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  \r\n\r\n    #attention details \r\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) \r\n\r\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)\r\n\r\nattn_zero = attn_cell.zero_state(batch_size , tf.float32 )\r\n\r\nattn_zero = attn_zero.clone(cell_state = encoder_state)\r\n\r\nnew_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())\r\n\r\n\"\"\"out_cell = tf.contrib.rnn.OutputProjectionWrapper(\r\n            attn_cell, target_vocab_size, reuse=True\r\n        )\"\"\"\r\n\r\n    #end of attention \r\n\r\n    output_layer = Dense(target_vocab_size,\r\n                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\r\n\r\n    with tf.variable_scope(\"decode\"):\r\n        train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, \r\n                         target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\r\n\r\n    with tf.variable_scope(\"decode\", reuse=True):\r\n        infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, \r\n                             target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, \r\n                             target_vocab_size, output_layer, batch_size, keep_prob)\r\n\r\n    return (train_decoder_out, infer_decoder_out)\r\n\r\n\"\"\"\r\nDON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\r\n\"\"\"\r\n#tests.test_decoding_layer(decoding_layer)\r\n```\r\n", "OK, we'll mark it for adding documentation. FWIW we provide source code, and so you can also see the return type for that function (though its not a substitute for documentation, you can always go read the code as well)...", "Thats great . Thanks ", "Added better documentation and an example to the docstrings for both `BeamSearchDecoder` and `AttentionWrapper.__init__` and `AttentionWrapper.zero_state`.", "Should show up in a day or two.", "Nagging Assignee @ebrevdo: It has been 342 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed by aae34fa7e35d9c3931cae49bfc20384dd20dffec."]}, {"number": 12568, "title": "'module' object has no attribute 'sparse_column_with_vocabulary_file'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Running on Cloud ML Engine\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**:  2.7\r\n\r\n\r\n### Describe the problem\r\nI am trying to use tf.contrib.layers.sparse_column_with_vocabulary_file() and I am getting an error that it does not exist. I recognize that it is not showing up when I search for it in the API, but it is showing up in the source code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column.py#L669) -- I even specified the r1.3 branch and it was still showing up.  Has this been removed and I am just looking in the wrong place? It seems like this may be a bug and the function should exist in 1.3?\r\n\r\nIf it was deprecated is it because there are workarounds when trying to generate a feature column from a sparse tensor of words? I can create the hash table with:\r\n\r\n```\r\ntable = tf.contrib.lookup.index_table_from_file(vocabulary_file = vocab_file)\r\ntable.lookup(word)\r\n``` \r\nbut since I am trying to add this in the feature columns, I need something to read in the sparse tensor of words in ie. \r\n`words = tf.contrib.layers.???` or `words = tf.feature_column.???`\r\n\r\nOr (Assuming deprecation) is the suggested implementation just to do all of these transformations in the `input_fn():` and just pass a sparse_column_with_integerized_feature() directly. Personally this feels awkard to perform half of the transformation in the input function but without sparse_column_with_vocabulary_file it feels like there is no other choice:\r\n\r\nWith this function it should be easy to go from:\r\n\"This is a sentence\" --> tf.string_split() yields [\"This\", \"is\", \"a\", \"sentence\"] (sparse tensor of strings) within the input_fn --> and then tf.contrib.sparse_column_with_vocabulary_file (sparse tensor of ids) --> tf.contrib.layers.embedding_column() which yields the embedding from a sparse tensor of id's.\r\n", "comments": ["It seems that this has been replaced by tf.feature_column.categorical_column_with_vocabulary_file() which can now handle both sparse and categorical columns? This does appear to be working, but it was a struggle to find the updated functionality as the source code seems to suggest this functionality exists. Is it possible to add any comments to the source code suggesting to look at tf.feature_column.categorical_column_from_vocabulary_file() to help anybody else who may run into this?", "Thanks, @jcomfort4 . It seems that the method is forget to expose when introduced.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@sguada @jcomfort4  I think the issue has been resolved. Could you close it?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 12567, "title": "Tensorflow should not depend on tensorboard", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: irrelevant\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux\r\n- **TensorFlow installed from (source or binary)**: binary (the issue is about pip dependencies)\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: irrelevant\r\n- **Exact command to reproduce**: pip install tensorflow\r\n\r\n### Describe the problem\r\nI wish to install tensorflow without having to install tensorboard.\r\n\r\nCurrently there is a circular dependency between tensorflow and tensorboard.  However, tensorflow runs perfectly well without tensorboard, therefore the dependency should be removed. Additionally having a simpler dependency structure would facilitate packaging for NixOS.\r\n\r\n### Source code / logs\r\nirrelevant", "comments": ["@jart can you comment on this?", "@jart ?", "TensorFlow and TensorBoard are two peas in the same pod. TensorBoard was originally checked-in here until the new [repo](https://github.com/tensorflow/tensorboard) was created. We fixed the circular dependency issue. The dependency continues to exist due to user expectations and PyPi concerns.\r\n\r\nWe might reconsider this for the 1.6 release. I like the idea of things being lightweight. Maybe we should go even further and make TensorFlow\u2194TensorBoard optional in both directions.\r\n\r\nWe're actively involved in helping distributors. A group was recently formed for this purpose, with folks from various distros, chipmakers, etc. NixOS takes a very creative first-principles approach to distribution. Maybe it deserves representation. Ping @martinwicke for further details.\r\n\r\nI have no qualms with packagers making this dependency optional in the interim. I would however ask that anyone who packages TensorFlow consider packaging TensorBoard too. I believe that's what's best for users. Also note other important factors need to be taken into consideration when distributing TensorFlow, such as how static linkage relates to security updates.", "I am unconvinced by \"user expectations\" (If tensorboard is wanted; just install it).\r\n\r\nHowever can you elaborate on \r\n- PyPi concerns (what are they?)\r\n- \"important factors need to be taken into consideration when distributing TensorFlow\" (what are they?)\r\nYou mention static linkage and security, but I can't see how depending on tensorboard improves that. On the contrary, not dependending on tensorboard would reduce the surface area of tensorflow and thus harden its security.", "We have always shipped TensorBoard with TensorFlow, and we will keep it that way. We have split it out into a separate package, but the expectation is that TensorFlow comes with a visualization tool.\r\n\r\nThe circular dependency is not a problem for pip, so I don't see that causing trouble. We are working on pulling the dependencies out such that TB does not depend on TF. \r\n\r\nIf you don't want TB, you can always tell pip to not install it, or remove it without causing harm. I'm not sure what the advantage of this would be though. For packaging, the pip dependency structure should not matter at all -- if you are making a deb, rpm, or other package, feel free to include dependencies as you see fit.", "I agree with @martinwicke. TensorBoard has always been committed to keeping its footprint tiny, because TensorFlow depends on it. It's relatively free in terms of dependencies and offers much value.\r\n\r\nThe only KI with our pip structure right now is https://github.com/tensorflow/tensorboard/issues/588 where we'll likely need to vendor html5lib and bleach to guarantee no diamond conflicts.", "@martinwicke  Thanks for confirming that tensorflow does not need tensorboard, and that the dependency is in fact technically the other way around.\r\n\r\n> It's relatively free in terms of dependencies\r\n\r\nThis is incorrect. Tensorflow depends on 7 packages. Tensorboard depends on 5 more packages ---  some of which are old (unmaintained?) versions. Bleach 1.5.0 is not compatible with python 3.6. Until https://github.com/tensorflow/tensorboard/issues/588 is resolved the Tensorboard  dependency is a burden for those who do not use it.\r\n\r\n"]}, {"number": 12566, "title": "enable use of transform_graph tool with contrib/rnn", "body": "This is a simple fix for using contrib/rnn with the transform_graph (and possibly other tools).\r\n\r\nIn particular, it fixes the following error:\r\n```\r\n2017-08-24 16:07:41.015752: E tensorflow/tools/graph_transforms/transform_graph.cc:210] Op type not registered 'BlockLSTM' in binary running on ip-172-31-30-181. Make sure the Op and Kernel are registered in the binary running in this process.\r\n```\r\n\r\nIt's also possibly a remedy for issue #11847.\r\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "Please address the CLA before we look at the change. Thanks!", "@pks, unfortunately, it doesn't solve a `ValueError: No op named BlockLSTM in defined operations.` problem mentioned in the issue. But `from tensorflow.contrib.rnn import *` does. Thank you!", "@dkurt the proposed fix here fixes the transform_graph tool, not freeze_graph.py -- but glad that the import works :-)", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "That works as well. Thanks!", "Jenkins, test this please.\r\n\r\n@petewarden any opinion on this PR?", "@petewarden ping?", "MacOS failure is unrelated."]}, {"number": 12565, "title": "Fix the XLA build", "body": "I merged in a pull request prematurely. Tested locally.", "comments": []}, {"number": 12564, "title": "Partially fixes #10838", "body": "The following warnings seem to print for compiling many ops using gcc-4.9.\r\n\r\nFor example, in `cuda_solvers_gpu.cu.cc`:\r\n\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/cuda_solvers_gpu.cu.cc:\r\n./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/kernel.h(307): warning: variable \"result\" is used before its value is set\r\n\r\n./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/kernel.h(307): warning: variable \"result\" is used before its value is set\r\n\r\n./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless\r\n```\r\n\r\nIn `beam_search_ops_gpu.cu.cc`:\r\n\r\n```\r\nINFO: From Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc:\r\n./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/kernel.h(307): warning: variable \"result\" is used before its value is set\r\n\r\n./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/core/framework/op_kernel.h(313): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/kernel.h(307): warning: variable \"result\" is used before its value is set\r\n\r\n./tensorflow/stream_executor/device_description.h(85): warning: type qualifier on return type is meaningless\r\n\r\n./tensorflow/stream_executor/device_description.h(144): warning: type qualifier on return type is meaningless\r\n```\r\n\r\nI believe this fix will address many (but not all) of the warnings during the build.", "comments": ["Can one of the admins verify this patch?", "@byronyi, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @josh11b to be potential reviewers.", "Jenkins, test this please"]}, {"number": 12563, "title": "Windows docs", "body": "", "comments": []}, {"number": 12562, "title": "Fix bug in DataFeeder constructor", "body": "This fix fixes #12525\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12561, "title": "Updating the Windows install documents in master", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @girving and @Carmezim to be potential reviewers.", "Yes that is the plan!"]}, {"number": 12560, "title": "after ops_to_register.h changed, IOS camera example still return No OpKernel support 'Less' op", "body": "### System information\r\nRun on MacOS 10.12\r\nXcode 8.3.3\r\nPython 3.5\r\ntensorflow 1.2.1 installed with anaconda \r\nbazel 0.5.2-homebrew\r\n\r\n### Main Problem: \r\nAlthough I changed ops_to_register.h file and recompile the static library, IOS camera example code still returned No OpKernel support 'Less' op error. **The same library works fine** in another project [JieHe's ios-tensorflow object detection project](https://github.com/JieHe96/iOS_Tensorflow_ObjectDetection_Example) when I replace the model load the model. I can't tell why I got the issues with official tensorflow ios code.\r\n\r\n### Describe the problem\r\nI trained my model based on the ssd_mobilenet network. T hen the model was optimized for usage on ios. Freeze it with `export_inference_graph.py`, optimize it with` optimize_for_inference.py`, and then binary reduced the size with `bazel build -c tensorflow/tools/graph_transforms:transform_graph`\r\n\r\nIn that, the model was well-generated as desired. \r\n\r\n\r\n**For the static library:**\r\nAnd then I try to import the model into my app. So I print all the ops and put the file under tensorflow/core/framework. \r\n\r\n>   bazel build tensorflow/python/tools:print_selective_registration_header \r\n  bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n    --graphs=path/to/graph.pb > ops_to_register.h\r\n\r\nIn the Makefile, first delete the line \"-D__ANDROID_TYPES_SLIM__ \" under \"# Settings for iOS.\" for all \"$(IOS_ARCH)\". And run\r\n\r\n```\r\ntensorflow/contrib/makefile/download_dependencies.sh\r\ntensorflow/contrib/makefile/compile_ios_protobuf.sh \r\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-O3  -DANDROID_TYPES=ANDROID_TYPES_FULL -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION\"\r\n```\r\nAfter I generated the static tensorflow library, I `pod install` the podfile in camera example as required. Modified the input/output tensor and try to build the app. However, the same error with \"No kernel registed XXX\" still occurs .\r\n\r\nI also tried my model and the same library on [JieHe's ios-tensorflow code](https://github.com/JieHe96/iOS_Tensorflow_ObjectDetection_Example), it can run smoothly. So I assumed] my static library was well generated to including all the ops I need. So I don't really know how to solve it with the example code?\r\n\r\nMy error log: \r\n\r\n> 2017-08-24 17:21:18.679927: F /Users/yingjie/tensorflow-master/tensorflow/examples/ios/App-test/camera/CameraExampleViewController.mm:730] Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'Less' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]", "comments": ["I abandoned all the thing related to Pods and add all the files needed from source. Now this problem is solved. \r\n\r\nBut, is the Pods package manager problematic as it seems not synchronize after I changed the static library? Is this some kinds of bug or it is just me?", "@petewarden can you comment on Pods?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm actually not sure what the underlying issue is here with mixing the Pod manager and the selective registration, but since the OP managed to work around it by skipping Pods, closing."]}, {"number": 12559, "title": "#12537 issue solution", "body": "* printing prefix message by parameter value", "comments": ["Can one of the admins verify this patch?", "Closing, since it's not clear that this achieves the desired result. Feel free to cleanup and reopen with comments. Thanks!"]}, {"number": 12558, "title": "convert Dimension to Int for sparse_merge in _IndicatorColumn", "body": "The PR is aimed to fix #12557 .\r\n\r\n`sparse_merge` cannot handle `Dimension`,  hence casting Dimension to int when invoked.\r\n\r\n### How to test\r\n\r\n+ [x] add an unit test.\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?", "Thanks for you approval, @000drax !", "Jenkins, test this please."]}, {"number": 12557, "title": "`indicator_column` raises a TypeError when `weighted_categorical_column` is used as its input.", "body": "### Describe the problem\r\n\r\n`indicator_column` raises a TypeError when `weighted_categorical_column` is used as its input.\r\n\r\nI have fixed the bug, and the PR is coming later.\r\n\r\n\r\n### Source code / logs\r\n\r\n```python\r\n    indicator_tensor = _transform_features(features, [indicator])[indicator]\r\n  File \"/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py\", line 377, in _transform_features\r\n    outputs[column] = builder.get(column)\r\n  File \"/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py\", line 1533, in get\r\n    transformed = column._transform_feature(self)  # pylint: disable=protected-access\r\n  File \"/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py\", line 2476, in _transform_feature\r\n    vocab_size=self._variable_shape[-1])\r\n  File \"/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/feature_column/feature_column_test.runfiles/org_tensorflow/tensorflow/python/ops/sparse_ops.py\", line 1140, in sparse_merge\r\n    type(vocab_size))\r\nTypeError: vocab_size has to be a Tensor or Python int. Found <class 'tensorflow.python.framework.tensor_shape.Dimension'>\r\n```\r\n", "comments": ["@000drax Can you take a look and maybe review the PR?", "PR looks good! I approved it, but I don't think I'm on the list of allowed\napprovers.\n\nOn Thu, Aug 24, 2017 at 4:32 PM, andydavis1 <notifications@github.com>\nwrote:\n\n> @000drax <https://github.com/000drax> Can you take a look and maybe\n> review the PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12557#issuecomment-324784648>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ab4bt5pOsR0epSNymldR_TulmwgaPGUDks5sbgf4gaJpZM4PBOpX>\n> .\n>\n"]}, {"number": 12556, "title": "MonitoredTrainingSession can't add summary and checkpoint hook", "body": "I trying to add summary and checkpoint to my distributed tensorflow training with custom data \r\n\r\n<pre>summary_hook = tf.train.SummarySaverHook(save_secs=600,output_dir=FLAGS.log_dir,summary_op=summary_op)\r\ncheckpoint_hook = tf.train.CheckpointSaverHook(save_steps=test_timing,checkpoint_dir=FLAGS.log_dir,saver=saver)\r\nwith tf.train.MonitoredTrainingSession(server.target,is_chief=is_chief,hooks=[sync_replicas_hook,summary_hook,checkpoint_hook],config=sess_config) as sess:</pre>\r\n\r\nwith above I can't run the sess with error of below\r\n<pre>2017-08-24 19:38:31.250556: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,13] has negative dimensions\r\n2017-08-24 19:38:31.250633: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,13] has negative dimensions\r\n      [[Node: mfcc_input = Placeholder[dtype=DT_FLOAT, shape=[?,13], _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]\r\n2017-08-24 19:38:31.251688: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,13] has negative dimensions\r\n2017-08-24 19:38:31.251734: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,13] has negative dimensions\r\n      [[Node: mfcc_input = Placeholder[dtype=DT_FLOAT, shape=[?,13], _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]\r\n2017-08-24 19:38:31.253478: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,3] has negative dimensions\r\n2017-08-24 19:38:31.253522: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,3] has negative dimensions\r\n      [[Node: labels = Placeholder[dtype=DT_INT32, shape=[?,3], _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]</pre>\r\n\r\nhow ever if I run without the summary and checkpoint hook it just work fine....\r\nI want to specify which timing and which directory to view my summary and save the checkpoint.\r\nDoes anyone have an idea how to solve this problem?\r\n", "comments": ["[This ](https://github.com/tensorflow/tensorflow/issues/11367) could be useful.\r\n\r\nIf you provide your full code, the community can provide more help.", "here is my full source:\r\ninfo:\r\n4 Nodes PC with Ubuntu 16.04LTS\r\n1 parameter server and 3 worker\r\nParameter server specs:\r\n2 GPUs\r\nworkers specs:\r\n1: 1GPU,   2: 2 GPUs, 3: 1 GPU\r\nThe reason i use the 2Gpus for parameters server is because it has faster CPU for my code to process the queue input pipeline faster.\r\n<pre>\r\n`'''\r\n192.168.1.7$ python example.py --job-name=\"ps\" --task_index=0 \r\n192.168.1.2$ python example.py --job-name=\"worker\" --task_index=0 \r\n192.168.1.8$ python example.py --job-name=\"worker\" --task_index=1 \r\n192.168.1.9$ python example.py --job-name=\"worker\" --task_index=2 \r\n\r\nreference from ischlag.github.io\r\n'''\r\nfrom __future__ import print_function\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\n\r\n#cluster specification\r\nparameter_servers = [\"192.168.1.7:2223\"]\r\nworkers = [\"192.168.1.2:2223\",\r\n           \"192.168.1.8:2223\",\r\n           \"192.168.1.9:2223\"]\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# start a server for a specific task\r\nif FLAGS.job_name == \"ps\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\nelse:\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    print(\"calling join to serve\")\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n#-----function definition-------\r\n    def extract_data_set(filename, batch_size, shuffle_batch=False,name=None):\r\n        filename_queue = tf.train.string_input_producer([filename])\r\n        reader = tf.TextLineReader()\r\n        key, value = reader.read(filename_queue)\r\n        record_default = [[1], [1.0], [1.0], [1.0], [1.0], [1.0], [\r\n            1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]\r\n        labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(\r\n            value, record_defaults=record_default)\r\n        mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                                mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n        labels = tf.one_hot(labels,depth=3,dtype=tf.int32,name=\"labels_hot\")\r\n        labels_batch, mfcc_features_batch = tf.train.batch(\r\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\r\n        return labels_batch, mfcc_features_batch\r\n\r\n    def hidden_layer(input, size_in, size_out, name=\"hidden_layer\",reuse=False):\r\n        with tf.name_scope(name):\r\n            w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=\"W\")\r\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\r\n            act = tf.nn.tanh(tf.add(tf.matmul(input, w), b))\r\n            tf.summary.histogram(\"weights\", w)\r\n            tf.summary.histogram(\"bias\", b)\r\n            tf.summary.histogram(\"activations\", act)\r\n            return act\r\n\r\n\r\n    def fully_connected_layer(input, size_in, size_out, name=\"fc\",reuse=False):\r\n        with tf.name_scope(name):\r\n            w = tf.Variable(tf.truncated_normal(shape=[size_in,size_out],stddev=0.1),name=\"W\")\r\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\r\n            act = tf.add(tf.matmul(input, w), b)\r\n            tf.summary.histogram(\"weights\", w)\r\n            tf.summary.histogram(\"bias\", b)\r\n            tf.summary.histogram(\"activations\", act)\r\n            return act\r\n#-------start program------------\r\n    print(\"calling worker\")\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        print(\"Calculating data size...\")\r\n        with gzip.open('csv_files/mfcc_combine_train_shuffled.gz') as f:\r\n            text = f.readlines()\r\n            size1 = len(text)\r\n            print(\"train:{}\".format(size1))\r\n        with gzip.open('csv_files/mfcc_combine_test_shuffled.gz') as f:\r\n            text = f.readlines()\r\n            size2 = len(text)\r\n            print(\"train:{}\".format(size2))\r\n\r\n\r\n        del text\r\n        test_timing = (size2 / FLAGS.batch_size) / \\\r\n            (size1 / FLAGS.batch_size / FLAGS.test_timing_point)\r\n        print(\"test timing:{}\".format(test_timing))\r\n        steps = size1 / FLAGS.batch_size\r\n        print(\"steps:{}\".format(steps))\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        train_labels_batch, train_mfcc_batch = extract_data_set(\r\n            \"csv_files/mfcc_combine_train_shuffled.csv\", FLAGS.batch_size,\"train\")\r\n        test_labels_batch, test_mfcc_batch = extract_data_set(\r\n            \"csv_files/mfcc_combine_test_shuffled.csv\", FLAGS.batch_size,\"test\")\r\n\r\n    # def sound_classifier_model(learning_rate, number_of_h_layer, h_layer_units, hparam):\r\n        # tf.reset_default_graph()\r\n    #saver = tf.train.Saver(sharded=True)\r\n    \r\n    # Between-graph replication model\r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        global_step = tf.get_variable('global_step', [], \r\n                                      initializer = tf.constant_initializer(0),dtype=tf.int32)\r\n        # global_step = tf.Variable(0,name='global_step')\r\n        #global_step=tf.get_variable('global_step', shape=[], initializer=tf.zeros_initializer(), dtype=tf.int32, trainable=False)\r\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\r\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\r\n        is_training = tf.placeholder(dtype=bool, shape=(), name=\"is_training\")\r\n        q_selector = tf.cond(is_training,lambda:[train_mfcc_batch,train_labels_batch],lambda:[test_mfcc_batch,test_labels_batch])\r\n\r\n        h = []\r\n        h.append(hidden_layer(x,num_input,h_layer_units))\r\n        for i in range(number_of_h_layer):\r\n            h.append(hidden_layer(h[i],h_layer_units,h_layer_units))\r\n\r\n        output = fully_connected_layer(h[number_of_h_layer],h_layer_units,num_classes)\r\n        saver = tf.train.Saver()\r\n        with tf.name_scope(\"x-entropy\"):\r\n            xent = tf.reduce_mean(\r\n            tf.nn.softmax_cross_entropy_with_logits(\r\n                    logits=output, labels=y), name=\"x-entropy\")\r\n\r\n        with tf.name_scope(\"train\"):\r\n            grad_op = tf.train.AdamOptimizer(learning_rate)\r\n            rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                    replicas_to_aggregate=len(workers),\r\n                                                    total_num_replicas=len(workers))\r\n            train_op = rep_op.minimize(xent,global_step=global_step)\r\n            sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n            # grads = rep_op.compute_gradients(xent)\r\n            # apply_gradients_op = rep_op.apply_gradients(grads,global_step=global_step)\r\n            # with tf.control_dependencies([apply_gradients_op]):\r\n                # train_op=tf.identity(xent,name='train_op')\r\n            # train_step = rep_op.minimize(xent,global_step=global_step,aggregation_method=tf.AggregationMethod.ADD_N)\r\n        init_token_op = rep_op.get_init_tokens_op()\r\n        chief_queue_runner = rep_op.get_chief_queue_runner()\r\n        with tf.name_scope(\"accuracy\"):\r\n            correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n        tf.summary.scalar(\"x-entropy\", xent)\r\n        tf.summary.scalar(\"accuracy\",accuracy)\r\n        \r\n        summary_op = tf.summary.merge_all()\r\n        global_init_op = tf.global_variables_initializer()\r\n        local_init_op = tf.local_variables_initializer()\r\n        \r\n    #------------prepare for session------------\r\n    # coord = tf.train.Coordinator()\r\n    print(\"creating Supervisor...\")\r\n    # sv = tf.train.Supervisor(is_chief=is_cheif,\r\n    #                         global_step=global_step,\r\n    #                         saver = saver,\r\n    #                         logdir=FLAGS.log_dir)\r\n    print(\"making config...\")\r\n    sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,device_filters=[\"/job:ps\",\"/job:worker/task:%d\"%FLAGS.task_index])\r\n    begin_time = time.time()\r\n    summary_hook = tf.train.SummarySaverHook(save_secs=600,output_dir=FLAGS.log_dir,summary_op=summary_op)\r\n    checkpoint_hook = tf.train.CheckpointSaverHook(save_steps=test_timing,checkpoint_dir=FLAGS.log_dir,saver=saver)\r\n\r\n    #---------------training session--------------\r\n    print(\"waiting for session prepare....\")\r\n    with tf.train.MonitoredTrainingSession(server.target,is_chief=is_chief,hooks=[sync_replicas_hook,summary_hook,checkpoint_hook],config=sess_config) as sess:\r\n        while not sess.should_stop():\r\n            '''\r\n            # is cheif\r\n            if FLAGS.task_index == 0:\r\n                sv.start_queue_runners(sess,[cheif_queue_runner])\r\n                sess.run(init_token_op)\r\n            '''\r\n            print(\"in session\")\r\n            # while not sv.should_stop():\r\n                # coord = tf.train.Coordinator()\r\n            # sess.run(tf.global_variables_initializer())\r\n            # sess.run(tf.local_variables_initializer())\r\n            # coord = tf.train.Coordinator()\r\n            # if FLAGS.task_index == 0:\r\n            tf.train.start_queue_runners(sess=sess)\r\n             sess.run(init_token_op)\r\n            # sv.start_queue_runners(sess=sess)\r\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                f = open(FLAGS.log_dir + \"accuracy_\" + str(learning_rate) +\r\n                        \"_with_\" + str(number_of_h_layer) + \"layers\", \"w+\")\r\n                print(\"loading data to queue....\")\r\n            \r\n            print(\"done loading to queue\")\r\n            print(\"start adding summary\")\r\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                f = open(FLAGS.log_dir+\"accuracy_\"+str(learning_rate)+\"_with_\"+str(number_of_h_layer)+\"layers\",\"w+\")\r\n\r\n                # writer = tf.summary.FileWriter(FLAGS.log_dir,graph=tf.get_default_graph())\r\n                # writer = tf.summary.FileWriter(FLAGS.log_dir + \"lr_%.0E layers:%d\"%(learning_rate,6))\r\n                # writer.add_graph(sess.graph)\r\n                #perform training cycles\r\n                start_time = time.time()\r\n\r\n            print(\"start training\")\r\n            for i in range(steps):\r\n                print(\"read data\")\r\n                mfcc_batch,label_batch = sess.run(q_selector,feed_dict={is_training:True})\r\n                # label_batch = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"labels\"))\r\n                [_,loss,glob_step,train_accuracy] = sess.run([train_op,xent,global_step,accuracy],feed_dict={x:mfcc_batch,y:label_batch})\r\n                # [train_accuracy,s] = sess.run([accuracy,summary_op],feed_dict={x:mfcc_batch,y:label_batch})\r\n                print(\"done\")\r\n                elapsed = round(time.time()-start_time,2)\r\n                sys.stdout.write(\"\\r\")\r\n                sys.stdout.write(\"learning_rate:{0},{1} layers , training:{2}/{3} , train_accuracy:{4} [elapsed_time:{5:.2f}] \".format(learning_rate,number_of_h_layer,i,steps,train_accuracy,elapsed))\r\n                sys.stdout.flush()\r\n                # writer.add_summary(summary,i)\r\n                print(\"before train\")\r\n                # sess.run(global_step,init_token_op,chief_queue_runner)\r\n                # sess.run(train_op,xent)\r\n                print(\"after train\")\r\n                if i != 0 and i % test_timing == 0:\r\n                    # saver.save(sess, os.path.join(FLAGS.log_dir, \"model.ckpt\"), i)\r\n                    test_time += 1\r\n                    for j in range(test_timing):\r\n                        mfcc_batch,y_batch = sess.run(q_selector,feed_dict={is_training:False})\r\n                        test_label = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"test_label\"))\r\n                        test_accuracy = accuracy.eval(feed_dict={x:mfcc_batch,y:test_label})\r\n                        sys.stdout.write(\"\\r\")\r\n                        sys.stdout.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\".format(number_of_h_layer,test_time,j,test_accuracy))\r\n                        sys.stdout.flush()\r\n                        with tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                            f.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\\n\".format(number_of_h_layer,test_time,j,test_accuracy))\r\n                            f.flush()\r\n\r\n    f.write(\"total_training for lr {},{}layers time:{} minutes\".format(learning_rate,number_of_h_layer,((time.time()-start_time)/60.0)))\r\n    f.flush()\r\n    f.close()\r\n    print(\"-----%s minutes ------\"%((time.time()-start_time)/60.0))\r\n#    sv.stop()\r\n    print(\"Done!\")\r\n`\r\n</pre>\r\n\r\nI run as the code provided comments in respective IP PC but it shows the error as previous\r\nThe Worker with task index 0 just hangs at the error.\r\n\r\nIs this the problem of my Placeholder without explicitly define the size?The [None,13] part.", "I don't have a definite answer yet. I suspect that there is some problem with the data feeding mechanism in your code which will cause this error. You mention that it runs fine if you disable them. Have you checked that the data really goes in and some meaningful training is done?\r\n\r\nHere is my suggestion:\r\n\r\n1. MonitoredTrainingSession should create a Coordinator which start the queue runners. tf.train.start_queue_runners(sess=sess) looks duplicate to me and if you start the queue runners twice, I am not sure what will happen. Refer to [this](https://stackoverflow.com/questions/43245231/how-do-monitored-training-sessions-work) for what MonitoredTrainingSession can do\r\n\r\n2. You can define the size for the placeholder. But as I understand it, it should not be the problem. No harm in trying :)\r\n\r\n3. I dont know what these are doing:\r\n```\r\nwith tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                f = open(FLAGS.log_dir + \"accuracy_\" + str(learning_rate) +\r\n                        \"_with_\" + str(number_of_h_layer) + \"layers\", \"w+\")\r\n                print(\"loading data to queue....\")\r\n```\r\nIf you want to log accuracy, the worker (not ps) should log accuracy since the worker is doing the training.\r\n\r\n4. Have you tried the non-distributed version of your code, does it work?", "```\r\nimport tensorflow as tf\r\n\r\nfilename_queue = tf.train.string_input_producer([\"file0.csv\", \"file1.csv\"])\r\n\r\nreader = tf.TextLineReader()\r\nkey, value = reader.read(filename_queue)\r\n\r\nrecord_defaults = [[1], [1], [1], [1], [1]]\r\ncol1, col2, col3, col4, col5 = tf.decode_csv(\r\n    value, record_defaults=record_defaults)\r\nfeatures = tf.stack([col1, col2, col3, col4])\r\ntf.summary.scalar(\"label\", col5)\r\nglobal_step = tf.get_variable('global_step', [],initializer = tf.constant_initializer(0),dtype=tf.int32)\r\n\r\nserver = tf.train.Server.create_local_server()\r\nsummary_op = tf.summary.merge_all()\r\nsummary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='.',summary_op=summary_op)\r\n\r\nwith tf.train.MonitoredTrainingSession(master=server.target, hooks=[summary_hook], is_chief=True) as sess:\r\n#     tf.train.start_queue_runners(sess=sess)\r\n\r\n    for i in range(30):\r\n    # Retrieve a single instance:\r\n        example, label, step = sess.run([features, col5, global_step])\r\n        print example, label, step\r\n```\r\nCan you try my code in your environment?\r\n\r\n1. Create two csv `file0.csv`, `file1.csv`. Each entry is just 5 ints separated by \",\". (Example is 1,2,3,4,5)\r\n2. Run the code with `tf.train.start_queue_runners(sess=sess)` commented out and not commented out.\r\n3. Check whether `events.out.tfevents.XXXXXX` is created in the same directory.", "My code is fine in non-distributed version, which why I'm trying to improve the training speed , I'm converting my code to distributed version\r\n\r\nI'm actually trying to create 2 queues, one for training another one for testing after the test_timing for the file. which I found in [stackoverflow](https://stackoverflow.com/questions/41162955/tensorflow-queues-switching-between-train-and-validation-data)\r\n\r\nHow does the string_input_producer really work when two files is given?\r\nWhat I'm trying to do is :\r\n[Queue 1(Train)  from csv file 1 ]   ,    [Queue 2(Test)   from csv file 2]\r\nusing these two Queue for training and testing within a program.\r\n\r\n\r\nI tried your code with below\r\n<pre>\r\n`'''\r\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \r\npc-02$ python example.py --job-name=\"worker\" --task_index=0 \r\npc-03$ python example.py --job-name=\"worker\" --task_index=1 \r\npc-04$ python example.py --job-name=\"worker\" --task_index=2 \r\n\r\nreference from ischlag.github.io\r\n'''\r\nfrom __future__ import print_function\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\n\r\n#cluster specification\r\nparameter_servers = [\"192.168.1.7:2223\"]\r\nworkers = [\"192.168.1.2:2223\",\r\n           \"192.168.1.8:2223\",\r\n           \"192.168.1.9:2223\"]\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# start a server for a specific task\r\nif FLAGS.job_name == \"ps\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\nelse:\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    print(\"calling join to serve\")\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n    \r\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n    record_default = [[1.0] for _ in range(14)]\r\n    record_default[0] = [1]\r\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\r\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n    # labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\r\n    tf.summary.scalar(\"label\",labels)\r\n    global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32)\r\n    \r\n    # server = tf.train.Server.create_local_server()\r\n    summary_op = tf.summary.merge_all()\r\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test',summary_op=summary_op)\r\n    previous=[]\r\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\r\n        tf.train.start_queue_runners(sess=sess)\r\n        for i in range(1024):\r\n            example,label,step = sess.run([mfcc_features,labels,global_step])\r\n            if set(previous) == set(example):\r\n                print(\"it's same!\")\r\n                sys.exit()\r\n            else:\r\n                previous = example\r\n                print(example,label,step)\r\n`\r\n</pre>\r\n\r\nthis code work fine when the `tf.one_hot(labels,depth=3,dtype=tf.int32)` is not used.\r\nhowever if I include the tf.one_hot transformation it show the error that I'm giving wrong op u'label'\r\n<pre>\r\n`Caused by op u'label', defined at:\r\n  File \"sound_classifier_distributed_test_input.py\", line 74, in <module>\r\n    tf.summary.scalar(\"label\",labels_hot)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 129, in scalar\r\n    tags=scope.rstrip('/'), values=tensor, name=scope)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 265, in _scalar_summary\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [3] (tag 'label')\r\n         [[Node: label = ScalarSummary[T=DT_INT32, _device=\"/job:ps/replica:0/task:0/cpu:0\"](label/tags, one_hot)]]\r\n         [[Node: Merge/MergeSummary_S13 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-9152570865690878554, tensor_name=\"edge_72_Merge/MergeSummary\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:2/cpu:0\"]()]]`\r\n</pre>\r\n\r\nBut I'm trying to make the label to be one_hot for classification.\r\nHow am I suppose to:\r\n1) Read data from csv file ---> data structure [label , feature , .... feature]\r\n2) Convert label to one_hot\r\n3) Batch up the one_hot labels and features to 1024 Batch size?", "@kinsumliu btw the code just work fine with and without `tf.train.start_queue_runners(sess=sess)`\r\n\r\n3 workers run and events.out.tfevents.XXXXXX is created in same directory.....", "`tf.one_hot` return a tensor which is not a scalar.  `tf.summary.tensor_summary` this should work\r\n\r\nfor the batch, you should use https://www.tensorflow.org/api_docs/python/tf/train/batch", "For the training and testing, you can run your program twice. First time training + checkpoint the model. Second time load up the checkpoint and do the inference on the testing data to get accuracy.\r\n\r\nI believe the issues you faced is not related to the bug inside tensorflow. So stackoverflow should be a better place for your problem.", "However with the graph I build for training.\r\n\r\n<pre>with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32)\r\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\r\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\r\n        # is_training = tf.placeholder(dtype=bool, shape=(), name=\"is_training\")\r\n        # q_selector = tf.cond(is_training,lambda:[train_mfcc_batch,train_labels_batch],lambda:[test_mfcc_batch,test_labels_batch])\r\n\r\n        h = []\r\n        h.append(hidden_layer(x,num_input,h_layer_units))\r\n        for i in range(number_of_h_layer):\r\n            h.append(hidden_layer(h[i],h_layer_units,h_layer_units))\r\n\r\n        output = fully_connected_layer(h[number_of_h_layer],h_layer_units,num_classes)\r\n        saver = tf.train.Saver()\r\n\r\n        with tf.name_scope(\"x-entropy\"):\r\n            xent = tf.reduce_mean(\r\n            tf.nn.softmax_cross_entropy_with_logits(\r\n                    logits=output, labels=y), name=\"x-entropy\")\r\n\r\n        with tf.name_scope(\"train\"):\r\n            grad_op = tf.train.AdamOptimizer(learning_rate)\r\n            rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                    replicas_to_aggregate=len(workers),\r\n                                                    total_num_replicas=len(workers))\r\n            train_op = rep_op.minimize(xent,global_step=global_step)\r\n            sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n            \r\n        init_token_op = rep_op.get_init_tokens_op()\r\n        chief_queue_runner = rep_op.get_chief_queue_runner()\r\n        with tf.name_scope(\"accuracy\"):\r\n            correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n        tf.summary.scalar(\"x-entropy\", xent)\r\n        tf.summary.scalar(\"accuracy\",accuracy)\r\n        \r\n        summary_op = tf.summary.merge_all()\r\n        global_init_op = tf.global_variables_initializer()\r\n        local_init_op = tf.local_variables_initializer()</pre>\r\n\r\nIt still shows the following error \r\n`2017-08-25 16:04:20.162666: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,13] has negative dimensions\r\n2017-08-25 16:04:20.162748: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,13] has negative dimensions\r\n         [[Node: mfcc_input = Placeholder[dtype=DT_FLOAT, shape=[?,13], _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]\r\n2017-08-25 16:04:20.163809: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,13] has negative dimensions\r\n2017-08-25 16:04:20.163855: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,13] has negative dimensions\r\n         [[Node: mfcc_input = Placeholder[dtype=DT_FLOAT, shape=[?,13], _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]\r\n2017-08-25 16:04:20.165598: W tensorflow/core/framework/op_kernel.cc:1148] Invalid argument: Shape [-1,3] has negative dimensions\r\n2017-08-25 16:04:20.165643: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1,3] has negative dimensions\r\n         [[Node: labels = Placeholder[dtype=DT_INT32, shape=[?,3], _device=\"/job:worker/replica:0/task:0/gpu:0\"]()]]`\r\n\r\nwhich I asked at the 1st comment.\r\nIt's shows the placeholder with [None,num_input] make's the error comes up again.\r\nWasn't the placeholder with [None] value should handle the arbitrary shape of the input?\r\nHow come it be [-1] dimension while training.", "I build up my code step by step. While the code to the `xent` calculation it still works fine.\r\nBut when I added the definition of the train_op as below:\r\n<pre>\r\ngrad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\nrep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                replicas_to_aggregate=3,\r\n                                                total_num_replicas=3)\r\ntrain_op = rep_op.minimize(xent,global_step=global_step)\r\nsync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\ninit_token_op = rep_op.get_init_tokens_op()\r\nchief_queue_runner = rep_op.get_chief_queue_runner()\r\n</pre>\r\nIt's start show the error message getting the -1 negative dimension\r\nlooks like this issue have been open at  #11823\r\nIs this the tensorflow version bugs?\r\n\r\nHere is my new full source code:\r\n<pre>\r\n'''\r\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \r\npc-02$ python example.py --job-name=\"worker\" --task_index=0 \r\npc-03$ python example.py --job-name=\"worker\" --task_index=1 \r\npc-04$ python example.py --job-name=\"worker\" --task_index=2 \r\n\r\nreference from ischlag.github.io\r\n'''\r\nfrom __future__ import print_function\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\nimport math\r\n#cluster specification\r\nparameter_servers = [\"192.168.1.7:2223\"]\r\nworkers = [\"192.168.1.2:2223\",\r\n           \"192.168.1.8:2223\",\r\n           \"192.168.1.9:2223\"]\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# start a server for a specific task\r\nif FLAGS.job_name == \"ps\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\nelse:\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\nbatch_size = 1024\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    print(\"calling join to serve\")\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n#-----function definition-------\r\n    \r\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n    record_default = [[1.0] for _ in range(14)]\r\n    record_default[0] = [1]\r\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\r\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n    labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\r\n    labels_batch, mfcc_features_batch = tf.train.batch(\r\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\r\n    tf.summary.tensor_summary(\"label\",labels)\r\n    \r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False) \r\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\r\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\r\n        w1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W\")\r\n        b1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\r\n        act1 = tf.nn.tanh(tf.add(tf.matmul(x,w1),b1))\r\n\r\n        w2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\r\n        b2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\r\n        act2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\r\n\r\n        w3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\r\n        b3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\r\n        act3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\r\n\r\n\r\n\r\n        w4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W\")\r\n        b4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B\")\r\n        act4 = tf.add(tf.matmul(act3,w4),b4)\r\n\r\n        # #----loss----\r\n        xent = tf.reduce_mean(\r\n            tf.nn.softmax_cross_entropy_with_logits(\r\n                logits=act4,labels=y),name=\"x-entropy\")\r\n\r\n\r\n        # ----train----\r\n        grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n        rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                replicas_to_aggregate=3,\r\n                                                total_num_replicas=3)\r\n        # # rep_op.compute_gradients()\r\n        train_op = rep_op.minimize(xent,global_step=global_step)\r\n        sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n        init_token_op = rep_op.get_init_tokens_op()\r\n        chief_queue_runner = rep_op.get_chief_queue_runner()\r\n        #----- accuracy\r\n        # correct_prediction = tf.equal(tf.argmax(act4,1),tf.argmax(y,1))\r\n        # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n    \r\n    tf.summary.scalar(\"x-entropy\",xent)\r\n    # tf.summary.scalar(\"accuracy\",accuracy)\r\n\r\n \r\n\r\n    # server = tf.train.Server.create_local_server()\r\n    # session_run_hook = tf.train.SessionRunHook(chief_queue_runner)\r\n    summary_op = tf.summary.merge_all()\r\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test3',summary_op=summary_op)\r\n    previous=[]\r\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\r\n        tf.train.start_queue_runners(sess=sess)\r\n        # sess.run(init_token_op\r\n        \r\n        for i in range(1024):\r\n            example,label_to_train = sess.run([mfcc_features_batch,labels_batch])\r\n            print(example.shape,label_to_train.shape)\r\n            # print(sess.run([act4],feed_dict={x:example}))\r\n            loss,glob_step= sess.run([xent,global_step],feed_dict={x:example,y:label_to_train})\r\n            \r\n            print(loss,glob_step)\r\n            if np.array_equiv(previous,example):\r\n                print(\"it's same!\")\r\n            else:\r\n                previous = example\r\n                print(\"not same!\")\r\n</pre>\r\n", "There are a few things in your code that need to be changed, especially for your data feeding mechanism. So your first session run will fetch the data but `xent` cannot be computed so `tf.summary.scalar(\"x-entropy\",xent)` causes problem. \r\n\r\nCheck out my code below. It should work. Note that I use `train_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)` because I am doing it on a single machine. After you change to use `SyncReplicasOptimizer`, first turn off training to see the data feeding works by using `loss,glob_step= sess.run([xent,global_step])`.\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport os\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\nimport math\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\nbatch_size = 1\r\n\r\n#-----function definition-------\r\ndef readData():\r\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n    record_default = [[1.0] for _ in range(14)]\r\n    record_default[0] = [1]\r\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\r\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n    labels1hot = tf.one_hot(labels,depth=3,dtype=tf.int32)\r\n    labels_batch, mfcc_features_batch = tf.train.batch(\r\n            [labels1hot, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\r\n    return (mfcc_features_batch, labels_batch)\r\n\r\nis_chief = True\r\nglobal_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False)\r\n(x,y) = readData()\r\nw1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W1\")\r\nb1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B1\")\r\nact1 = tf.nn.tanh(tf.add(tf.matmul(tf.cast(x, tf.float32),w1),b1))\r\n\r\nw2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W2\")\r\nb2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B2\")\r\nact2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\r\n\r\nw3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W3\")\r\nb3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B3\")\r\nact3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\r\n\r\nw4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W4\")\r\nb4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B4\")\r\nact4 = tf.add(tf.matmul(act3,w4),b4)\r\n\r\n# #----loss----\r\nxent = tf.reduce_mean(\r\n    tf.nn.softmax_cross_entropy_with_logits(\r\n        logits=act4,labels=y),name=\"x-entropy\")\r\n\r\n# # ----train----\r\n# grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n# rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n#                                         replicas_to_aggregate=3,\r\n#                                         total_num_replicas=3)\r\n# train_op = rep_op.minimize(xent,global_step=global_step)\r\n\r\ntrain_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)\r\n\r\n\r\n# sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n# init_token_op = rep_op.get_init_tokens_op()\r\ntf.summary.scalar(\"x-entropy\",xent)\r\n\r\nserver = tf.train.Server.create_local_server()\r\nsummary_op = tf.summary.merge_all()\r\nsummary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='.',summary_op=summary_op)\r\nwith tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\r\n    tf.train.start_queue_runners(sess=sess)\r\n    for i in range(2):\r\n        _,loss,glob_step= sess.run([train_op,xent,global_step])\r\n        print (loss, glob_step)\r\n```", "@kinsumliu Thanks you for taking time to solve the problem! It works with your code in single local PC! It seems like it is the problem with placeholder and feeding data. I will try to ask in [StackOverflow](https://stackoverflow.com/) and try not using the feed_dict. Thank you for help!\r\n\r\nSince it is not a bug of tensorflow , closing this issue.", "@luvwinnie Good to know. If you have further problems related to this code, you can @ my username (LKS) in stackoverflow."]}, {"number": 12555, "title": "[[Node: pool_3 = AvgPoolT=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 8, 8, 1], padding=\"VALID\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]", "body": "iOS App - I have changes All the file name and Size , but still getting below error.\r\n\r\nSource Code URL:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios/camera\r\n\r\nChanges Code : const int wanted_input_width = 229;\r\nconst int wanted_input_height = 229;\r\nconst int wanted_input_channels = 3;\r\nconst float input_mean = 128.0f;\r\nconst float input_std = 128.0f;\r\nconst std::string input_layer_name = \"Mul\";\r\nconst std::string output_layer_name = \"final_result\";\r\n\r\ncomputed output size would be negative\r\n[[Node: pool_3 = AvgPoolT=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 8, 8, 1], padding=\"VALID\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\r\n\r\nPlease suggest what we are doing wrong .\r\nIn build pb and txt are working fine (imagenet_comp_graph_label_strings.txt, tensorflow_inception_graph.pb)\r\n\r\nbut created new pb and .txt is not working(rounded_graph.pb and retrained_labels.txt).\r\nNote: I also rename the pb and txt file ,", "comments": ["@petewarden, @andrewharp can you comment on this?", "My Mistake, Fixed Issue."]}, {"number": 12554, "title": "Fix deprecation warnings of *_global_step functions.", "body": "In `tensorflow/contrib/layers/python/layers/optimizers.py`, use `*_global_step` functions from `tf.train` instead of `tf.contrib.framework`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 12553, "title": "Object detection evaluation warning", "body": "Hi,\r\nHere is my system details:\r\n\r\n- macOS10\r\n- no GPU\r\n- python3.6\r\n- tensorflow1.3\r\n- installed tf from source\r\n\r\nI'm running the evaluation of object detection, but every time I run into this warning:\r\n\r\n```\r\nWARNING:root:The following classes have no ground truth examples: 0\r\n/Users/Mohamad/Projects/Python/tensorflow/models/object_detection/utils/metrics.py:145: RuntimeWarning: invalid value encountered in true_divide\r\n  num_images_correctly_detected_per_class / num_gt_imgs_per_class)\r\n```\r\nI want to know how to solve this warning? Is it something serious?\r\n\r\nThanks.", "comments": ["@derekjchow can you comment on this one?", "@modanesh , were you able to fix that problem?\r\nI am having the same problem. I did the training of Oxford-IIIT pets dataset as mentioned in the dataset. Then during evaluation I am getting the same error. \r\nMy system details:\r\nUbuntu 16.04, Python 3.5, Tensorflow 1.3.0, I have GPU support. I installed Tensorflow using Pip.\r\n\r\nAlso, after that warning comes, the evaluation stops for me. Please can somebody help me on this?", "@Prasad9 No, I haven't and I didn't find anything useful. I'm still waiting for someone to shade light on it.", "I have the same issue!!!", "See https://github.com/tensorflow/models/issues/1696 for an explanation. Basically this warning is expected as label map ids now start with 1.", "@derekhh, this is how my label map id file looks like. \r\n\r\n```\r\nitem {\r\n  id: 1\r\n  name: 'One'\r\n}\r\n\r\nitem {\r\n  id: 2\r\n  name: 'Two'\r\n}\r\n\r\nitem {\r\n  id: 3\r\n  name: 'Three'\r\n}\r\n\r\nitem {\r\n  id: 4\r\n  name: 'Four'\r\n}\r\n\r\nitem {\r\n  id: 5\r\n  name: 'Five'\r\n}\r\n\r\nitem {\r\n  id: 6\r\n  name: 'Six'\r\n}\r\n\r\nitem {\r\n  id: 7\r\n  name: 'Seven'\r\n}\r\n\r\nitem {\r\n  id: 8\r\n  name: 'Eight'\r\n}\r\n\r\nitem {\r\n  id: 9\r\n  name: 'Nine'\r\n}\r\n\r\nitem {\r\n  id: 10\r\n  name: 'Zero'\r\n}\r\n\r\n```\r\n\r\nI am not having any id with zero. Also, when I evaluate, I get two .tfevents file and during the third one, this warning(may be error) comes and then there happens to be no more processing after that. I can check in tensorboard the evaluation done for the first image.\r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from /my/path/train/model.ckpt-200000\r\nINFO:tensorflow:Restoring parameters from /my/path/train/model.ckpt-200000\r\nWARNING:root:The following classes have no ground truth examples: 0\r\n/my/path/models/object_detection/utils/metrics.py:145: RuntimeWarning: invalid value encountered in true_divide\r\n```\r\n", "Have you  changed  **label_id_offset** from 0 to 1 ?\r\n./object_detection/evaluator.py:187:     categories=categories, label_id_offset=0)\r\n\r\n **categories=categories, label_id_offset=1)**\r\n", "same error when running eval.py \r\nWARNING:root:The following classes have no ground truth examples: 0\r\nany solution even i try to run eval.py and train.py parallely still same error.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@derekjchow can you comment on this one?\r\n\r\n", "Close due to inactivity. "]}, {"number": 12552, "title": "#12537 issue solution", "body": "*print prefix message by flag value", "comments": ["Can one of the admins verify this patch?", "@yoon-hyoung, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @jhseu and @tensorflower-gardener to be potential reviewers.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}]