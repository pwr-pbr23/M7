[{"number": 31825, "title": "KeyError while loading image data", "body": "Error:\r\nFile \"DataCollection.py\", line 38, in caption_image\r\nreturn \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\nKeyError: 'sunflowers\\6166888942_7058198713_m.jpg'\r\n\r\nFunction:\r\ndef caption_image(image_path):\r\nimage_rel = pathlib.Path(image_path).relative_to(data_root)\r\nreturn \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\n\r\nIt gives a KeyError while returning the parameter.\r\nAnd every time it gives a KeyError with different image name and path.\r\n\r\nAny idea on this?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Sure,\r\nHere are the details:\r\n\r\nI am following this tutorial - https://www.tensorflow.org/tutorials/load_data/images \r\nto learn about how to train the model using sets of images.\r\n\r\nBut when I started inspecting the images as shown here: https://www.tensorflow.org/tutorials/load_data/images#inspect_the_images\r\n\r\nIt is not working, it gives me the following error:\r\nFile \"DataCollectionError:.py\", line 38, in caption_image\r\nreturn \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\nKeyError: 'sunflowers\\6166888942_7058198713_m.jpg'\r\n\r\nIt gives a KeyError while returning the parameter.\r\nAnd every time it gives a KeyError with different image name and path.\r\n\r\nAny idea on this?\r\n\r\nI am using Windows10 and 64-bit architecture.\r\nAnd I am using TensorFlow r1.14.", "@DKWIPL,\r\nI tried executing the colab but i didn't get any error. Just to verify, are you executing the code in colab or jupyter notebook locally on your system. Thanks!", "NO, I am doing it on my Local System.", "And can you also help me with the tutorial to create and train the model using our own image dataset?\r\nIt will be really useful for us if can help us with this.\r\n\r\nThank You.", "> NO, I am doing it on my Local System.\r\n\r\nIf your doing it on local system, please verify the path of the input data. Thanks!", "Hi,\r\n\r\nI checked my data path. It is proper. And as I skipped that step and moved forward, I started getting the following error. \r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"DataCollection.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nI don,t know what is going on in this. It was working by Friday and its suddenly stop working on Monday.", "@DKWIPL \r\n`ImportError: DLL load failed: The specified module could not be found.`\r\nLooks like the tensorflow is not installed properly, Can you uninstall tensorflow and python and reinstall both. Thanks! ", "Hey,\r\n\r\nIt worked from me installing and reinstalling again. But what was the issue?\r\nIt was working before and suddenly it stopped working.", "@DKWIPL, Glad it worked. \r\nFrom the error it looks like Tensorflow and its dependencies are not installed properly. \r\nClosing the issue since its resolved. Thanks ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31825\">No</a>\n", "#The path problem needs to be solved under windows !!!!!!!!!!!!!!!\r\ndef caption_image(image_path):\r\n\timage_rel = pathlib.Path(image_path).relative_to(data_root)\r\n\timage_rel = str(image_rel).replace(\"\\\\\",\"/\")#################look here\uff01\r\n\timage_rel = image_rel + \" \"\r\n\treturn \"Image (CC BY 2.0) \"+ \" - \".join(attributions[image_rel].split(\" - \")[:-1])", "-----------------------------------------------------------\nAttention: Non-Delivery Report\n-----------------------------------------------------------\n\nThis report is generated by the email server at:\n\n       gophygital.io\n\nThe message with subject:\n\n       \"Re: [tensorflow/tensorflow] KeyError while loading image data (#31825)\"\n\nand attached to this report was not delivered to \nthe following recipients:\n\nAddress: ***@***.***\nReason:  User not found (550)\n--------------\n\n"]}, {"number": 31824, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64x  \r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.3\r\n- Installed using : pip\r\n\r\n\r\nTensoreflow import error \r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Vikas rathod\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@vikasrathod4299 ,\r\nPlease refer this [link](https://www.tensorflow.org/install/pip).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31824\">No</a>\n"]}, {"number": 31823, "title": "AutoGraph Map Infinite Loop", "body": "**System information**\r\nI used google colab. [Here's the link to reproduce](https://colab.research.google.com/drive/13g1AMmLsCK3dcmuUdYiDhyv9PqajFRlc)\r\n\r\nGiven:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.data import Dataset\r\ntf = tf.compat.v2\r\ntf.enable_v2_behavior()\r\n\r\n@tf.function\r\ndef test1(ds):\r\n  for val in ds.map(lambda x: x+1):\r\n    print(val)\r\n    \r\n@tf.function\r\ndef test2(ds):\r\n  for val in map(lambda x: x+1, ds):\r\n    print(val)\r\n\r\nds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])\r\n```\r\n**Describe the current behavior**\r\n```\r\nprint('start')\r\ntest1(ds) # NOT HANG\r\nprint('end')\r\n\r\nprint('start')\r\ntest2(ds) # HANG\r\nprint('end')\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should not hang.\r\n\r\n**Other info / logs**\r\nI've been thinking about a fix much like [overriding python built-in zip function](https://github.com/tensorflow/tensorflow/pull/31290). The problem is, [tf.data.Dataset's map only applies to one dataset](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map) while python built-in map function can accept more than 1 data. For example:\r\n```\r\nnumbers1 = [1, 2, 3] \r\nnumbers2 = [4, 5, 6] \r\n  \r\nresult = map(lambda x, y: x + y, numbers1, numbers2) \r\nprint(list(result)) \r\n```\r\n\r\nWhat's the solution? \r\nPlease correct me if I'm wrong or I'm missing something. Thank you.", "comments": ["Issue replicating for TF-2.0beta1.", "AutoGraph doesn't currently override `map`, but the usage you describe could be added, in a fashion similar to how we handle `zip` and `enumerate`.", "@mdanatg But how to handle if we need to apply `map` to more than 2 datasets? Unlike [`Dataset.zip`](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/data/ops/dataset_ops.py#L708-L744), [`map` in Datasets](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/data/ops/dataset_ops.py#L1099-L1214) can only take 1 Dataset.", "I think we could use a combination of `Dataset.map` and `Dataset.zip`.\r\n\r\nIn other words:\r\n\r\n`map(fn, dataset1, dataset2)`\r\n\r\ncould translate to (pseudo-code, the exact API calls may differ):\r\n\r\n```\r\ndef fn_wrapper(args_tuple):\r\n  fn(*args_tuple)\r\nDataset.zip(dataset1, dataset2).map(fn_wrapper)\r\n```\r\n\r\nWould this work?", "Oh yea, you're right! Working on it right now", "I'll close this issue as #32121 has been merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31823\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31823\">No</a>\n"]}, {"number": 31822, "title": "r2.0-CherryPick:test fixes for the windows release builds.", "body": "", "comments": []}, {"number": 31821, "title": "EmptyTensorList, RandomUniform, TensorListFromTensor, TensorListReserve, TensorListStack, While ops have not be supported", "body": "**System information**\r\n- Linux Ubuntu 16.04:\r\n- TensorFlow installed from anaconda:\r\n- TensorFlow version 1.14.0:\r\n\r\nhi,it seems that EmptyTensorList, RandomUniform, TensorListFromTensor, TensorListReserve, TensorListStack, While ops have not be supported .\r\n\r\noutput from tflite_convert:\r\n\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, FULLY_CONNECTED, GATHER, GREATER_EQUAL, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SHAPE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: EmptyTensorList, RandomUniform, TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n```\r\nimport os\r\nos.environ[\"TF_ENABLE_CONTROL_FLOW_V2\"] = \"1\"\r\nimport tensorflow as tf\r\n\r\n\r\ngraph_def_file = \"./vgg/frozen_graph_meta2.pb\"\r\ninput_arrays = [\"images_pre\"]#image_widths_pre\r\noutput_arrays = [\"fully_connected/transpose_time_major\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n#converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n\r\n\r\n", "comments": ["Hi,\r\n\r\nTOCO currently doesn't support control-flow ops and TensorList ops. We are currently working on a new converter to support all those use cases.\r\n\r\nAre you trying to convert an RNN model?", "> Hi,\r\n> \r\n> TOCO currently doesn't support control-flow ops and TensorList ops. We are currently working on a new converter to support all those use cases.\r\n> \r\n> Are you trying to convert an RNN model?\r\n\r\nyep, i trying to convert an rnn model. and i have use tf.lite.experimental.nn.bidirectional_dynamic_rnn instead of tf.nn.bidirectional_dynamic_rnn, use tf.lite.experimental.nn.TfLiteLSTMCell instead of tf.nn.rnn_cell.LSTMCell references https://tensorflow.google.cn/lite/convert/rnn. But it doesn't seem to be working.", "Hi,\r\n\r\nI believe currently you need to add ophints in your model to make the conversion work. Could you please check out:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/examples/lstm/g3doc\r\n\r\nThanks!", "Hi,\r\n\r\nSorry for a late update. We have just released a beta TF Lite converter, you can try it with:\r\ncoverter.experimental_new_converter = True (make sure you install the latest tf-nightly).\r\n\r\nWith this converter, you can just use the native tf.nn.bidirectional_dynamic_rnn API rather than tf.lite.experimental* ones. Can you give it a try?"]}, {"number": 31820, "title": "tflite output different result with pbfile when using only one convolutional layer ?", "body": "https://github.com/tensorflow/tensorflow/issues/31359", "comments": ["Can we track with that issue#31359 and close this one. It will help us to follow easily. Please let us know. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31820\">No</a>\n"]}, {"number": 31819, "title": "keras.model.fit does not work with SparseTensor inputs with functional API", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n\r\n- TensorFlow version (use command below):\r\n1.14.0 \r\n\r\n**Describe the current behavior**\r\n\r\nThe program exits with the following error: \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'sparse_tensor/indices' with dtype int64 and shape [?,2]\r\n\t [[{{node sparse_tensor/indices}}]]\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error occurs and training continues.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow.compat.v2 as tf\r\n\r\ndef dummy_parse_fn(iterable):\r\n  features = {}\r\n  # the input is always constant\r\n  features['sparse_tensor'] = tf.SparseTensor(\r\n    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),\r\n    values=tf.constant([1.0, 1.0], dtype=tf.float32),\r\n    dense_shape=tf.constant([2, 2], dtype=tf.int64))\r\n  features['sparse_tensor/indices'] = features['sparse_tensor'].indices\r\n  features['sparse_tensor/values'] = features['sparse_tensor'].values\r\n  features['sparse_tensor/dense_shape'] = features['sparse_tensor'].dense_shape\r\n  labels = tf.constant([1.0, 1.0], dtype=tf.float32)\r\n  return features, labels\r\n\r\n\r\ndef get_dummy_dataset():\r\n  iterable =  np.random.random((128, 1)).astype(np.float32)\r\n  return (\r\n    tf.data.Dataset\r\n    .from_tensor_slices(iterable)\r\n    .map(dummy_parse_fn)\r\n    .take(1024)\r\n  )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n  print(tf.__version__)\r\n\r\n  inputs = tf.keras.layers.Input(shape=(2, ), sparse=True, name=\"sparse_tensor\")\r\n  weights = tf.Variable(name='weights', shape=(2, 1), initial_value=[[1.0], [1.0]])\r\n  output = tf.sparse.sparse_dense_matmul(inputs, weights)\r\n  model = tf.keras.Model([inputs], output)\r\n\r\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\r\n  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\n  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n  model.fit(get_dummy_dataset(), epochs=2)\r\n```\r\n", "comments": ["This is failing in tf-nightly as well:\r\n\r\n```\r\n1.15.0-dev20190821\r\nW0821 10:51:36.244404 139992655496832 deprecation.py:506] From /home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2019-08-21 10:51:36.246914: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-21 10:51:36.266789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2112500000 Hz\r\n2019-08-21 10:51:36.267215: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55948d8b0c80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-08-21 10:51:36.267235: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/tmp/sparse_model.py\", line 36, in <module>\r\n    model = tf.keras.Model([inputs], output)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 147, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 164, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 267, in _init_graph_network\r\n    base_layer_utils.create_keras_history(self._nested_outputs)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 184, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 229, in _create_keras_history_helper\r\n    constants[i] = backend.function([], op_input)([])\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/pyalamanchili/.local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'sparse_tensor/indices' with dtype int64 and shape [?,2]\r\n\t [[{{node sparse_tensor/indices}}]]\r\n```", "Alternative way to do this  in tf-nightly can be seen here: https://gist.github.com/pavanky/3e1635edb322c81436d66ac9d9bdf69f", "I could able to reproduce the issue with Tensorflow 1.14.0. Please see the gist [here](https://colab.research.google.com/drive/1RvCNPW8pDR0vUD23d2Q8rzqjYWMQb5sj). Thanks!", "Took a quick look at the issue and seems like we may need special case handling for SparseTensor/SparseOps in the InputLayer/TensorFlowOpLayer implementation as the inputs of the sparse ops are (indices, values, dense_shape, other inputs) and not the sparse tensor itself.\r\n\r\nWhen creating input layer for sparse tensor we set `_keras_history` on the sparse tensor. When we try to convert the sparse matmul op into a TensorFlow Op layer, we check if the inputs to the layer have `_keras_history` property set. The property was set on the sparse tensor, however op.inputs on sparse matmul op will return (indices, values, dense_shape, variable) tensors and not the sparse tensor.\r\n\r\n\r\n", "@pavithrasv would a potential workaround be that I create a custom `TensorflowOpLayer` with the right inputs?", "Nvm, I guess no matter what we do in the functional API, this will be a problem with sparse operations.", "@pavanky @pavithrasv \r\n\r\nThis is what worked for me (only tested in 2.0), note that any stateful operations in the Functional API have to be contained inside a Layer:\r\n\r\n```python\r\n!pip install tf-nightly-2.0-preview\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef dummy_parse_fn(iterable):\r\n  # the input is always constant\r\n  features = tf.SparseTensor(\r\n    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),\r\n    values=tf.constant([1.0, 1.0], dtype=tf.float32),\r\n    dense_shape=tf.constant([2, 2], dtype=tf.int64))\r\n  labels = tf.constant([1.0, 1.0], dtype=tf.float32)\r\n  return features, labels\r\n\r\n\r\ndef get_dummy_dataset():\r\n  iterable =  np.random.random((128, 1)).astype(np.float32)\r\n  return (\r\n    tf.data.Dataset\r\n    .from_tensor_slices(iterable)\r\n    .map(dummy_parse_fn)\r\n    .take(1024)\r\n  )\r\n\r\nprint(tf.__version__)\r\n\r\ninputs = tf.keras.layers.Input(shape=(2, ), sparse=True, name=\"sparse_tensor\")\r\n# Variables and any stateful ops in the Functional API have to be defined inside Layers,\r\n# even if just a lambda layer.\r\noutputs = tf.keras.layers.Lambda(\r\n    lambda x: tf.sparse.sparse_dense_matmul(x, tf.Variable(name='weights', shape=(2, 1), initial_value=[[1.0], [1.0]])))(inputs)\r\nmodel = tf.keras.Model([inputs], outputs)\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\r\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\nmodel.fit(get_dummy_dataset(), epochs=2)\r\n```", "> When we try to convert the sparse matmul op into a TensorFlow Op layer, we check if the inputs to the layer have _keras_history property set. The property was set on the sparse tensor, however op.inputs on sparse matmul op will return (indices, values, dense_shape, variable) tensors and not the sparse tensor.\r\n\r\n@pavithrasv Yeah I think this is still a problem for the TensorFlowOpLayers. We should probably `nest.flatten(..., expand_composites=True)` when setting the _keras_history in the InputLayer, and also in `_add_inbound_node` / wherever else we set it", "I have a workaround by patching `GraphExecutionFunction` from 1.15 into 1.14 and using the [layer API](https://gist.github.com/pavanky/3e1635edb322c81436d66ac9d9bdf69f) so this isnt a blocker.\r\n\r\n", "@pavanky For now we are raising an exception disallowing usage of sparse ops without wrapping it in Lambda layer and providing the workaround information in the error message. ", "Closing this issue as we are raising a clear exception disallowing the use case instead of failing with a random error. We will look into whether we can support the usage of sparse ops with TensorFlow op layer in the long run. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31819\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31819\">No</a>\n", "@pavithrasv I am not 100% sure the suggested alternative works in tensorflow 1.14:\r\n\r\nI originally created this to simplify the repro code, but this fails in 1.14 too\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nclass Sparse(keras.layers.Layer):\r\n  def __init__(self, units, activation=None, use_bias=True,\r\n               kernel_initializer='glorot_uniform',\r\n               kernel_regularizer=None,\r\n               **kwargs):\r\n    super(Sparse, self).__init__(**kwargs)\r\n    self.units = units\r\n    self.kernel_initializer = keras.initializers.get(kernel_initializer)\r\n    self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\r\n\r\n  def build(self, input_shape):\r\n\r\n    if self.built:\r\n      return\r\n\r\n    input_shape = tf.compat.v2.TensorShape(input_shape)\r\n    input_shape = input_shape.with_rank(2)\r\n    input_size = input_shape[-1].value\r\n\r\n    if input_size is None:\r\n      raise ValueError(\"Sparse layer expects the last dimension of Input to be known\")\r\n\r\n    self.kernel = self.add_weight(\r\n      name=\"kernel\",\r\n      shape=[input_size, self.units],\r\n      initializer=self.kernel_initializer,\r\n      regularizer=self.kernel_regularizer,\r\n      trainable=True)\r\n    self.built = True\r\n\r\n  def compute_output_shape(self, input_shape):\r\n    input_shape = tf.compat.v2.TensorShape(input_shape)\r\n    input_shape = input_shape.with_rank(2)\r\n    return input_shape[:-1].concatenate(self.units)\r\n\r\n  def call(self, inputs):\r\n    outputs = tf.sparse.sparse_dense_matmul(inputs, self.kernel)\r\n    return outputs\r\n\r\n\r\ndef dummy_parse_fn(iterable):\r\n  # the input is always constant\r\n  features = tf.SparseTensor(\r\n    indices=tf.constant([[0,0],[1,1]], dtype=tf.int64),\r\n    values=tf.constant([1.0, 1.0], dtype=tf.float32),\r\n    dense_shape=tf.constant([2, 2], dtype=tf.int64))\r\n  labels = tf.constant([1.0, 1.0], dtype=tf.float32)\r\n  return features, labels\r\n\r\n\r\ndef get_dummy_dataset():\r\n  iterable =  np.random.random((128, 1)).astype(np.float32)\r\n  return (\r\n    tf.data.Dataset\r\n    .from_tensor_slices(iterable)\r\n    .map(dummy_parse_fn)\r\n    .take(1024)\r\n  )\r\n\r\nprint(tf.__version__)\r\n\r\n# not setting the batch size doesnt work??\r\ninputs = tf.keras.layers.Input(shape=(2, ), batch_size=2, sparse=True, name=\"sparse_tensor\")\r\n# Variables and any stateful ops in the Functional API have to be defined inside Layers,\r\n# even if just a lambda layer.\r\noutputs = Sparse(1)(inputs)\r\nmodel = tf.keras.Model([inputs], outputs)\r\n\r\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\r\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\nmodel.fit(get_dummy_dataset(), epochs=2)\r\n```\r\nBut this fails with a different error that seems to be fixed in 1.15? \r\n\r\nAre there any plans to patch 1.14 or is a fix only expected in the next release?", "The exception is in the commit https://github.com/tensorflow/tensorflow/commit/e3805e70c4634f89af772524b4135953fa27dc2b#diff-8bfbe586f2ba61d9aa9e39676c7f69ee . This is not included in 1.14. Since this is just raising an exception with a workaround suggestion and the workaround is provided here already we decided not to cherrypick this into a release. Please let me know if that is ok.", "@pavithrasv thanks, will wait for the next release. We have work arounds for now.", "BTW a quicker workaround for this is to use Keras 2.2.5 instead of tf.keras.\r\n\r\nFor some reason, the implementation of Keras that is shipped with Tensorflow does not have all the features of original Keras (which supported sparse inputs, even when using functional API and `Model.fit`).\r\n\r\nAlso worth noticing that TF 1.15.0rc1 does not fix the issue.", "I'm stills seeing this issue with keras 2.5.0rc1 and tensorflow 2.5.0-rc3", "I posted a reproducible example here: https://github.com/tensorflow/tensorflow/issues/32737#issuecomment-841583926"]}, {"number": 31818, "title": "[WIP] Migrating F1 Score", "body": "This PR is for migrating F1 score from addons to the core.\r\n\r\n@pavithrasv will be reviewing the PR\r\n@seanpmorgan @karmel  for your reference", "comments": ["Did not add test scripts yet. Will do after the review comments", "Hi @pavithrasv \r\nDid you get a chance to review the PR?", "Thanks @pavithrasv \r\nWill work on the changes.\r\nDo you have any sample test scripts that I can refer to? I have written one for addons and will combine both.", "Hi @pavithrasv \r\n\r\nI have updated the following:\r\n\r\n1.  Corrected lines > 80 characters.\r\n2. Changed all `tf.math` to `math_ops` and `tf.reduce_sum` to `math_ops.reduce_sum`\r\n3. Added usage for Keras API\r\n4. Changed `tfa.metrics` to `tf.keras.metrics`.\r\n5. Changed indentation for docstrings\r\n\r\nClarification:\r\n\r\n1. We are not exposing `sample_weight` parameter now. Do we need to add exception for the same?\r\n2. I have test cases for F1 in addons. Can we modify the same or I need to refer some existing sample scripts?\r\n\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/metrics/f1_test.py\r\n\r\n\r\nPlease let me know if the current changes look good", "I will be adding a `threshold` parameter today. Will ping again once I update", "There are still issues in the implementation. See here: https://github.com/tensorflow/addons/issues/490", "@pavithrasv The issue has been taken care. we are good now. I will update the PR", "Waiting for the f1 score update PR to get merged. Will update the scripts right after that", "We have an updated version of the code. It is almost ready. I will update the here and we can take it forward from here.\r\nSorry for the delay.", "Refractor PR got merged. Now, we are good to go with migration.\r\nUpdated new scripts. Need to do some minor formatting work. \r\n\r\nBTW, where should I add my test cases?", "You can add tests in the file metrics_test.py", "Thanks, @pavithrasv will update.", "I have added a test. Will do a final cleanup in a day and we can have a final review on this.\r\nThanks", "This is WIP. Will be completed this week", "F1 metric in addons still seems to have a very ugly bug: https://github.com/tensorflow/addons/issues/825", "@SSaishruthi Any update on this PR, please. Thanks!", "@gbaned Hi\r\n\r\nSorry I was on vacation. We have some clarification to make. Will keep till we clear that\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@SSaishruthi Any update on this PR and can you please resolve conflicts? Thanks!", "@gbaned No offence but I think @SSaishruthi does not have the time at the moment. Someone else should pick up the migration work. The TF Addons implementation seems to be ok. Want me to create a new PR for this migration?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31817, "title": "The tensorflow code on jupyter notebook can run successfully, but there is a lot of warning information", "body": "code:\r\nimport tensorflow as tf\r\na = 2\r\nb = 3\r\nc = tf.add(a, b, name='Add')\r\nprint(c)\r\n\r\n**Warning information**\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n\r\n\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nG:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n\r\n\r\n\r\n\r\nWhat should I do to get rid of the warning message?\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Thanks!\r\n\r\n\r\n", "> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Thanks!\r\n\r\nWindows 10,jupyter notebook.\r\nTensorflow version:1.14.0 ", "@wwfwwfwwf3 \r\n\r\nCan you please confirm which version of numpy you are using? If numpy version is 1.17 in your system can you downgrade to 1.16.4 and check whether the issue still persists?Thanks!\r\n\r\n\r\n", "@wwfwwfwwf3 The issue should have been fixed in PR #30559.", "@wwfwwfwwf3 \r\n\r\nPlease, let us know if the issue still persists.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31817\">No</a>\n"]}, {"number": 31816, "title": "Processing batches with different sequence lengths using stacked LSTM layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.4\r\n- TensorFlow installed from (source or binary):  Source\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.65\r\n\r\n**Describe the current behavior**\r\n\r\nAn exception is raised when trying to stack multiple `tf.keras.layers.LSTM`, while the sequence length changes across batches. This behavior occurs if the `tf.keras.Model` is built with model subclassing. On the other hand, if the model is built using the functional API, everything works as intended.\r\n\r\n**Describe the expected behavior**\r\n\r\nBecause of the identical implementations, besides the difference in the way the model is built (subclassing and functional API), I would expect the results to be the same. In other words, I am confused why an exception is raised at all if using model subclassing.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef train_generator():\r\n    while True:\r\n        sequence_length = np.random.randint(10, 100)\r\n        x_train = np.random.random((1000, sequence_length, 5))\r\n        y_train = np.random.random((1000, sequence_length, 2))\r\n\r\n        yield x_train, y_train\r\n\r\n# Works as intended\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(None, 5)))\r\nmodel.add(tf.keras.layers.LSTM(8, return_sequences=True))\r\nmodel.add(tf.keras.layers.Dense(2))\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\r\nmodel.fit_generator(train_generator(), steps_per_epoch=2, epochs=2, verbose=1)\r\n\r\n# Throws an exception\r\nclass LSTMModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(LSTMModel, self).__init__()\r\n        self._lstm_0 = tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(None, 5)) \r\n        self._lstm_1 = tf.keras.layers.LSTM(8, return_sequences=True)\r\n        self._dense = tf.keras.layers.Dense(2)\r\n\r\n    def call(self, inputs, training=False):\r\n        output = self._lstm_0(inputs)\r\n        output = self._lstm_1(output)\r\n        output = self._dense(output)\r\n\r\n        return output\r\n\r\nmodel = LSTMModel()\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\r\nmodel.fit_generator(train_generator(), steps_per_epoch=2, epochs=2, verbose=1)\r\n```\r\n\r\n**Other info / logs**\r\n>InvalidArgumentError:  [_Derived_]  Operation expected a list with 58 elements but got a list with 88 elements.\r\n\t [[{{node gradients/TensorArrayUnstack/TensorListFromTensor_grad/TensorListStack}}]]\r\n\t [[Adam/gradients_24/lstm_model_22/lstm_56/StatefulPartitionedCall_grad/StatefulPartitionedCall]] [Op:__inference_keras_scratch_graph_75269]", "comments": ["Was able to replicate the issue using Tensorflow 2.0.0.beta1 on Colab. Please see the gist [here](https://colab.research.google.com/drive/1yaYGJbGwKghWUCo-R0f9icTkz3Mk8Q1C). Thanks!", "@gorjanradevski I cannot reproduce the issue with `!pip install tf-nightly-gpu-2.0-preview`. I think this was resolved in `!pip install tf-nightly-gpu-2.0-preview`. Please take a loot at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/78ed85d5cce4286d7bd8d04247f4c819/tf_31816_fit_generator.ipynb). \r\n\r\nI am closing the issue. Please feel free to open if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31816\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31816\">No</a>\n"]}, {"number": 31815, "title": " r2.0 cherry-pick request: Autograph: zip() with tf.data fix ", "body": "The following code enters infinite loop without this change.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf = tf.compat.v2\r\ntf.enable_v2_behavior()\r\n\r\n@tf.function\r\ndef test(ds):\r\n  for val in zip(ds):\r\n    print(val)\r\n\r\nds1 = tf.data.Dataset.from_tensor_slices([-11, -12, 4])\r\nprint('start')\r\ntest(ds1)\r\nprint('end')\r\n```", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31815) for more info**.\n\n<!-- need_author_consent -->", "@ilhamfp We're requesting to merge your change to 2.0 release, could you follow the googlebot's instruction above?", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31815) for more info**.\n\n<!-- cla_yes -->", "@googlebot I consent."]}, {"number": 31814, "title": "How can I convert image to arrays using Java code?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: Samsung A3 2016 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0/7.4\r\n- GPU model and memory: Nvidia GeForce 840m 3 Go \r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to set up an MLKit detection project on Android using Tensorflow library, I have got false values on the output using Tensorflow lite ( different values than Frozen model inference ).\r\n\r\nI doubt that the problem is with the input ( image ), So I want to compare the two same images that I have.\r\n\r\nTo do this task, I have used PIL image and numpy libraries on python to get bytes arrays, and I have converted the drawable image to bitmap and from bitmap to bytes arrays.\r\n\r\nI don't know if the function np.asarray should give the same value as those two functions below:\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n+ **Java code:**\r\n\r\n```java\r\nprivate float[][][][] bitmapToInputArray() {\r\n    // [START mlkit_bitmap_input]\r\n    Bitmap bitmap= getYourInputImage();\r\n    int batchNum = 0;\r\n    float[][][][] input = new float[1][112][112][3];\r\n    for (int x = 0; x < 112; x++) {\r\n        for (int y = 0; y < 112; y++) {\r\n            int pixel = bitmap.getPixel(x, y);\r\n            // Normalize channel values to [-1.0, 1.0]. This requirement varies by\r\n            // model. For example, some models might require values to be normalized\r\n            // to the range [0.0, 1.0] instead.\r\n            input[batchNum][x][y][0] = (Color.red(pixel))/ 255.0f;\r\n            input[batchNum][x][y][1] = (Color.green(pixel)) / 255.0f;\r\n            input[batchNum][x][y][2] = (Color.blue(pixel))/ 255.0f;\r\n            Log.i(\"Input\",\"input\"+input[batchNum][x][y][0]);\r\n            Log.i(\"input\",\"input\"+input[batchNum][x][y][1]);\r\n\r\n        }\r\n    }\r\n    // [END mlkit_bitmap_input]\r\n\r\n    return input;\r\n}\r\npublic byte[] convertBitmapToByteArray(Bitmap bitmap) {\r\n    ByteArrayOutputStream stream = null;\r\n    try {\r\n        stream = new ByteArrayOutputStream();\r\n        bitmap.compress(Bitmap.CompressFormat.JPEG, 100, stream);\r\n\r\n        return stream.toByteArray();\r\n    }finally {\r\n        if (stream != null) {\r\n            try {\r\n                stream.close();\r\n            } catch (IOException e) {\r\n                Log.e(ThemedSpinnerAdapter.Helper.class.getSimpleName(), \"ByteArrayOutputStream was not closed\");\r\n            }\r\n        }\r\n    }\r\n}\r\nprivate Bitmap getYourInputImage() {\r\n    // This method is just for show\r\n    BitmapDrawable drawable = (BitmapDrawable) image2.getDrawable();\r\n    Bitmap bitmap = drawable.getBitmap();\r\n    Bitmap bitmapp=Bitmap.createScaledBitmap(bitmap,112,112,true);\r\n    Bitmap bitmap2= bitmapp.copy(Bitmap.Config.ARGB_8888, true);\r\n    return bitmap2;\r\n}\r\n\r\n    byte[] bytes=convertBitmapToByteArray(bitmap1);\r\n    Log.i(\"byte\",\"\"+ Arrays.toString(bytes));\r\n    float[][][][] inp = new float[1][112][112][3];\r\n    inp=bitmapToInputArray();\r\n    Log.i(\"byte2\",\"\"+Arrays.toString(inp[0]));\r\n```\r\n\r\n+ **Python code:**\r\n\r\n```python\r\nimg = Image.open(\"irisdata-300VW_Dataset_2015_12_14-017-000880.jpg\")\r\nimg.load()\r\nimg = img.resize((112, 112), PIL.Image.ANTIALIAS)\r\nprint(str(image_to_byte_array(img)))\r\n\r\n\r\n# Normalize to [0, 1]\r\ndata = np.asarray( img, dtype=\"float32\")\r\nprint(data)\r\n```\r\n**Other info / logs**\r\n\r\n+ Java output:\r\n\r\n>2019-08-20 19:01:13.589 1513-1513/com.example.irisdetection I/byte: [-1, -40, -1, -32, 0, 16, 74, 70, 73, 70, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, -1, -37, 0, 67, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -37, 0, 67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -64, 0, 17, 8, 0, 112, 0, 112, 3, 1, 34, 0, 2, 17, 1, 3, 17, 1, -1, -60, 0, 29, 0, 0, 2, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 6, 9, 4, 5, 10, 3, 2, 1, -1, -60, 0, 69, 16, 0, 1, 3, 3, 1, 5, 5, 4, 7, 5, 6, 5, 5, 0, 0, 0, 3, 1, 2, 4, 5, 6, 17, 33, 0, 7, 18, 19, 49, 8, 34, 65, 81, 97, 20, 35, 113, -127, 9, 21, 50, 51, -111, -95, -79, 36, 66, -63, -47, -16, 22, 23, 67, 82, 83, -31, 37, 52, 98, 114, -15, 99, 115, -126, -125, -94, -1, -60, 0, 28, 1, 0, 2, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 7, 4, 5, 2, 3, 8, 0, 1, -1, -60, 0, 55, 17, 0, 0, 4, 3, 6, 3, 8, 1, 3, 4, 3, 1, 0, 0, 0, 0, 1, 2, 3, 4, 5, 17, 33, 0, 6, 49, 65, 81, 97, 18, 19, 113, 7, 20, 35, -127, -111, -95, -79, -16, -47, 34, -63, -31, 21, 36, 67, -15, 8, 22, 50, 51, -1, -38, 0, 12, 3, 1, 0, 2, 17, 3, 17, 0, 63, 0, -28, -105, 115, 116, 26, 45, 98, -97, 117, 74, -71, -31, -57, -88, 40, -73, 93, 88, -83, 83, 89, 39, 41, -20, -43, 40, -7, 28, 115, 12, -100, -52, -116, -88, 78, 53, -45, 84, -49, -53, 102, 114, 124, 34, -52, -20, -3, -72, 91, -115, 6, -58, -120, -76, -5, -102, -126, 71, 36, -107, 43, -97, -11, 61, 93, -17, 78, 96, 63, 115, -35, -52, 8, -3, -30, -89, -40, -26, 117, -40, 29, -70, 107, 106, 21, 112, 86, -36, 7, 73, 36, 120, -9, 30, -19, -18, -48, -56, 86, -79, -88, -31, -106, -105, 36, -53, -53, 103, 30, -92, 121, 24, -120, 76, 47, -7, -15, -99, 54, 103, 44, -88, -107, 42, -57, 98, 107, 90, 83, -98, 105, 20, -21, 43, 126, 53, -117, 106, 58, -71, -116, 84, -114, 58, -11, 22, 53, 77, -84, 119, 47, -34, -14, -52, 72, 102, 127, -68, -57, -61, 77, -78, 126, -35, 18, -79, 112, 9, 38, 41, 40, 50, 3, 104, 63, -7, -64, 66, 97, -100, -66, -115, -115, 59, 43, -119, 21, 43, -50, -111, 79, 62, 82, -55, -72, 64, 70, -76, -92, -6, 5, 119, -54, -42, 15, -40, -126, -36, -81, 86, 109, 8, 55, 69, -62, 114, 31, -39, -30, 125, 65, 65, 9, 120, -47, -111, -24, -48, 36, -55, 56, -58, 49, -12, 103, 52, -14, 77, -24, -68, 12, -41, 24, -38, -49, 45, -6, 122, -80, -104, -106, -88, 32, -76, 92, 72, -67, 51, -13, 95, 92, -2, 105, -90, 112, -85, -1, 0, 101, -21, 110, -105, 72, -35, 117, -93, 21, -22, -48, 56, 84, 56, 50, 9, -62, -33, -74, 105, 35, -25, -99, 126, 42, 71, -89, -126, -86, -2, 91, 77, 119, -59, -67, 56, 86, 101, 29, -50, -116, -41, 28, -83, 78, 1, 2, 59, 85, 78, 103, -82, -120, -120, -119, -16, 84, -8, -81, -82, -43, -119, 2, 73, -74, -32, 34, 124, -95, -91, 103, -120, -56, 48, -108, -3, 55, 10, -128, 72, 44, 54, -11, 68, -43, -117, 63, 85, 41, 75, -67, -81, -115, 104, 11, -128, 98, 30, 84, -99, -90, 117, -103, -47, 13, 32, -111, 99, 59, 8, -30, 120, -85, 59, -38, -8, 42, 116, -50, -67, 51, -25, -116, 109, -17, 6, -128, 85, 123, 72, 34, 46, 60, 28, -72, -8, 39, 69, -49, -97, -13, -38, -77, -28, -17, 107, 125, -75, -103, -113, -97, 108, 110, -34, -19, -88, 0, 69, -53, 80, 84, -87, 74, -114, 69, 34, -82, -124, 81, -94, 97, 124, -4, -70, 109, 40, -94, 118, -112, -19, 67, 109, 76, -114, -54, -2, -25, 36, -118, -104, -30, 35, -122, -39, -111, -90, 10, 79, 39, 61, 73, -53, 69, -41, 25, 93, 116, -12, -38, -87, 70, -4, -59, 84, -83, 100, 30, 67, 47, -65, 103, 51, 120, 107, -94, 36, 68, -124, -63, -31, -56, 39, 44, -58, -127, -112, -53, 110, -125, -106, 118, 97, 30, -117, 46, 58, 100, -68, 46, 110, 123, -51, -41, -89, 84, -4, -4, -68, 126, 91, 123, -44, -30, 3, -39, -107, 80, 92, 92, 89, -18, -89, -34, 105, -29, -4, 83, -11, -38, 47, -70, -19, -17, 10, -4, -89, 8, -43, 90, 49, 40, -14, -36, 1, -72, -111, -56, -113, 86, 101, 90, -103, 31, 125, 62, -13, -29, -32, -102, -90, -60, -55, -43, 90, 28, 126, 23, 56, 106, -118, -115, -30, 114, 47, -106, 5 2019-08-20 19:01:13.590 1513-1513/com.example.irisdetection I/byte2: [[[F@8bc81c1, [[F@4123d66, [[F@d5d68a7, [[F@ed2a154, [[F@7ce78fd, [[F@c74c9f2, [[F@66de843, [[F@627ec0, [[F@50ea7f9, [[F@201933e, [[F@57cc59f, [[F@250c6ec, [[F@f88cab5, [[F@7ffa54a, [[F@d721cbb, [[F@1f965d8,\r\n\r\n+ Python output:\r\n\r\n![](https://i.stack.imgur.com/785vr.png)\r\n\r\nHow can I get the same output of the python code with Java code using bitmap as an input?\r\n\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31814\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31814\">No</a>\n", "you can do that :\r\n```java\r\n\r\npublic class FastRGB {\r\n\r\n    private int width;\r\n    private int height;\r\n    private boolean hasAlphaChannel;\r\n    private int pixelLength;\r\n    private byte[] pixels;\r\n\r\n    public FastRGB(BufferedImage image) { // image must be BufferedImage.TYPE_3BYTE_BGR \r\n        pixels = ((DataBufferByte) image.getRaster().getDataBuffer()).getData();\r\n        width = image.getWidth();\r\n        height = image.getHeight();\r\n        hasAlphaChannel = image.getAlphaRaster() != null;\r\n        pixelLength = 3;\r\n        if (hasAlphaChannel) {\r\n            pixelLength = 4;\r\n        }\r\n    }\r\n\r\n    //RGB \u5408\u6210\u4e00\u4e2a Int\r\n    int getRGB(int x, int y) {\r\n        int pos = (y * pixelLength * width) + (x * pixelLength);\r\n        int argb = -16777216; // 255 alpha\r\n        if (hasAlphaChannel) {\r\n            argb = (((int) pixels[pos++] & 0xff) << 24); // alpha\r\n        }\r\n\r\n        argb += ((int) pixels[pos++] & 0xff); // blue\r\n        argb += (((int) pixels[pos++] & 0xff) << 8); // green\r\n        argb += (((int) pixels[pos++] & 0xff) << 16); // red\r\n        return argb;\r\n    }\r\n\r\n    /**\r\n     * \u5206\u5f00\u5b58\u653e\u7684\r\n     *\r\n     * @param x\r\n     * @param y\r\n     * @return RGB \u6570\u7ec4\r\n     */\r\n    int[] getRGB2(int x, int y) {\r\n        int color = getRGB(x, y);\r\n        int red = ((color & 0xff0000) >> 16);\r\n        int green = ((color & 0xff00) >> 8);\r\n        int blue = (color & 0xff);\r\n        return new int[]{red, green, blue};\r\n    }\r\n\r\n    /**\r\n     * \u5b58 \u9ad8\u5bbdRGB\uff0c\u4e09\u7ef4\u6570\u7ec4\r\n     *\r\n     * @return \u4e09\u7ef4\u6570\u7ec4\r\n     */\r\n    public int[][][] toRGBPixels() {\r\n        int[][][] pixels = new int[this.height][][];\r\n        for (int y = 0; y < this.height; y++) {\r\n            int[][] width = new int[this.width][];\r\n            for (int x = 0; x < this.width; x++) {\r\n                int[] rgb = getRGB2(x, y);\r\n                width[x] = rgb;\r\n            }\r\n            pixels[y] = width;\r\n        }\r\n        return pixels;\r\n    }\r\n\r\n}\r\n\r\n```\r\n"]}, {"number": 31813, "title": "please add more activation functions", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0b1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nactivation functions are easy to add and quite handy\r\n\r\n**Will this change the current api? How?**\r\nsomething like this:\r\n```\r\nimport tensorflow as tf\r\n\r\nK = tf.keras\r\nB, L = K.backend, K.layers\r\n\r\nRRELU_MIN, RRELU_MAX = 0.123, 0.314\r\nHARD_MIN, HARD_MAX = -1., 1.\r\nSOFT_ARGMAX_BETA = 1e10\r\n\r\nfn_string = [\"swish\", \"gaussian\", \"sin\", \"cos\", \"hard_tanh\", \"lisht\", 'rrelu'] # etc etc\r\nactivators = [L.Activation(fn) for fn_string in fn_strings]  \r\n\r\ndef clean_activation(activation):\r\n    if callable(activation):\r\n        return activation\r\n    elif activation == 'soft_argmax':\r\n        fn = soft_argmax\r\n    elif activation == 'gaussian':\r\n        fn = gaussian\r\n    elif activation == 'swish':\r\n        fn = swish\r\n    elif activation == 'lisht':\r\n        fn = lisht\r\n    elif activation == 'sin':\r\n        fn = tf.math.sin\r\n    elif activation == 'cos':\r\n        fn = tf.math.cos\r\n    else:\r\n        fn = activation\r\n    return fn\r\n\r\n\r\ndef swish(x):\r\n    \"\"\"\r\n    Searching for Activation Functions\r\n    https://arxiv.org/abs/1710.05941\r\n    \"\"\"\r\n    return (B.sigmoid(x) * x)\r\n\r\n\r\ndef soft_argmax(x, beta=SOFT_ARGMAX_BETA):\r\n    \"\"\"\r\n    https://stackoverflow.com/questions/46926809/getting-around-tf-argmax-which-is-not-differentiable\r\n    https://lucehe.github.io/differentiable-argmax/\r\n    \"\"\"\r\n    x_range = tf.range(x.shape.as_list()[-1], dtype=x.dtype)\r\n    return tf.math.reduce_sum(\r\n        tf.nn.softmax(x * beta) * x_range, axis=-1)\r\n\r\n\r\ndef gaussian(x):\r\n    return B.exp(-B.pow(x, 2))\r\n\r\n\r\ndef hard_tanh(x, min=HARD_MIN, max=HARD_MAX):\r\n    if x > max:\r\n        return max\r\n    elif x < min:\r\n        return min\r\n    else:\r\n        return x\r\n\r\n\r\ndef lisht(x):\r\n    \"\"\"\r\n    LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent\r\n    https://github.com/swalpa/LiSHT\r\n    \"\"\"\r\n    return (B.tanh(x) * x)\r\n\r\n\r\ndef rrelu(x, min=RRELU_MIN, max=RRELU_MAX):\r\n    return x if x >= 0 else tf.random.uniform(min, max) * x\r\n\r\n\r\ndef tanhshrink(x):\r\n    return x - B.tanh(x)\r\n\r\n\r\ndef hardshrink(x, min=HARD_MIN, max=HARD_MAX):\r\n    if x > max:\r\n        return x\r\n    elif x < min:\r\n        return min\r\n    else:\r\n        return 0\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nanybody who likes to play around\r\nit would also be cool if we could set activations on a per-neuron basis instead of a per-layer basis. or per-channel, potentially... shrug!\r\n\r\n**Any Other info.**\r\nmight be better suited for addons... cc @seanpmorgan ", "comments": ["/cc @seanpmorgan for extended ops.", "We would be happy to include most of these in Addons.", "@bionicles Can you open a PR in `addons` repo [here](https://github.com/tensorflow/addons/pulls) and close the issue here. Thanks!", "@jvishnuvardhan @yongtang @seanpmorgan done, closed, here is updated code just for future searchers:\r\n\r\n```\r\nfrom tensorflow_addons.activations import sparsemax\r\nimport tensorflow as tf\r\n\r\nK = tf.keras\r\n\r\nB, L = K.backend, K.layers\r\n\r\nRRELU_MIN, RRELU_MAX = 0.123, 0.314\r\nHARD_MIN, HARD_MAX = -1., 1.\r\nSOFT_ARGMAX_BETA = 1e10\r\nFN = 'relu'\r\n\r\n\r\ndef swish(x):\r\n    \"\"\"\r\n    Searching for Activation Functions\r\n    https://arxiv.org/abs/1710.05941\r\n    \"\"\"\r\n    return (B.sigmoid(x) * x)\r\n\r\n\r\ndef soft_argmax(x, beta=SOFT_ARGMAX_BETA):\r\n    \"\"\"\r\n    https://stackoverflow.com/questions/46926809/getting-around-tf-argmax-which-is-not-differentiable\r\n    https://lucehe.github.io/differentiable-argmax/\r\n    \"\"\"\r\n    x_range = tf.range(x.shape.as_list()[-1], dtype=x.dtype)\r\n    return tf.math.reduce_sum(\r\n        tf.nn.softmax(x * beta) * x_range, axis=-1)\r\n\r\n\r\ndef gaussian(x):\r\n    return B.exp(-B.pow(x, 2))\r\n\r\n\r\ndef hard_tanh(x, min=HARD_MIN, max=HARD_MAX):\r\n    if x > max:\r\n        return max\r\n    elif x < min:\r\n        return min\r\n    else:\r\n        return x\r\n\r\n\r\ndef hard_lisht(x, min=HARD_MIN, max=HARD_MAX):\r\n    if x < min or x > max:\r\n        return max\r\n    else:\r\n        return tf.math.abs(x)\r\n\r\n\r\ndef lisht(x):\r\n    \"\"\"\r\n    LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent\r\n    https://github.com/swalpa/LiSHT\r\n    \"\"\"\r\n    return (B.tanh(x) * x)\r\n\r\n\r\ndef rrelu(x, min=RRELU_MIN, max=RRELU_MAX):\r\n    return x if x >= 0 else tf.random.uniform(min, max) * x\r\n\r\n\r\ndef tanh_shrink(x):\r\n    return x - B.tanh(x)\r\n\r\n\r\ndef hard_shrink(x, min=HARD_MIN, max=HARD_MAX):\r\n    if x > max:\r\n        return x\r\n    elif x < min:\r\n        return min\r\n    else:\r\n        return 0\r\n\r\n\r\nFUNCTION_LOOKUP = {\r\n    'soft_argmax': soft_argmax,\r\n    'log_softmax': tf.nn.log_softmax,\r\n    'sparsemax': sparsemax,\r\n    'hard_lisht': hard_lisht,\r\n    'hard_shrink': hard_shrink,\r\n    'tanh_shrink': tanh_shrink,\r\n    'hard_lisht': hard_lisht,\r\n    'hard_tanh': hard_tanh,\r\n    'gaussian': gaussian,\r\n    'swish': swish,\r\n    'lisht': lisht,\r\n    'rrelu': rrelu,\r\n    'lrelu': tf.nn.leaky_relu,\r\n    'crelu': tf.nn.crelu,\r\n    'relu6': tf.nn.relu6,\r\n    'sin': tf.math.sin,\r\n    'cos': tf.math.cos,\r\n}\r\n\r\n\r\ndef clean_activation(activation):\r\n    if callable(activation):\r\n        return activation\r\n    else:\r\n        fn = activation\r\n    return fn\r\n\r\n\r\ndef use_fn(fn):\r\n    fn = clean_activation(fn)\r\n    return L.Activation(fn)\r\n```", "Why not add the trainable version of Swish as well as the non-trainable version? keras-contrib has the code for the trainable version right here: [keras-contrib.layers.avanced_activations.swish.Swish](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/advanced_activations/swish.py). Adding the trainable version also gives access to the beta scaling factor, which makes Swish much more flexible."]}, {"number": 31812, "title": "r2.0-CherryPick:Create zeros grad of the correct shape for ResourceVariables.", "body": "Fixes #31297\r\n\r\nPiperOrigin-RevId: 264312145", "comments": []}, {"number": 31811, "title": "r2.0-CherryPick:Limit py_func on gpu to only take numeric types", "body": "Allow py_func with string tensors to work correctly when a GPU is attached. ", "comments": []}, {"number": 31810, "title": "Limit py_func on gpu to only take numeric types + typo fix", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31810) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 31809, "title": "Fix auto_mixed_precision handling of data flow ops", "body": "TensorArray and Stack ops are stateful and therefore not always safe to modify (e.g., they may get shared between training and validation graphs, which requires that their data types remain the same).\r\n\r\nThis commit removes support for these ops and replaces it with support for TensorList ops, which are non-stateful (and used by default as of TF 2.0).\r\n\r\nThis may cause performance regressions for models using auto_mixed_precision in TF 1, but this can be worked around by calling tf.enable_control_flow_v2() or switching to TF 2.\r\n\r\nAttn. @reedwm \r\ncc. @nluehr ", "comments": ["Sorry for the delay in getting back to this, I had to think about it for a while. Your feedback was very helpful, thanks. I decided to just work around the PushBackBatch and ConcatLists ops for now, as it will probably require some refactoring to support them properly (and as far as I can see they're not actually used anywhere in TF).\r\n\r\nOne thing I'm not sure about is what the semantics of read after write dependencies are with these tensor list ops. It may not matter, but I wondered if we should be taking it into account with the ephemeral edges somehow. I couldn't see any documentation on exactly how these ops behave."]}, {"number": 31808, "title": "Import Tensorflow Could not find 'cudart64_100.dll' - Model repository incompatible with CUDA 10.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- TensorFlow installed from (source or binary): piwheels\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 10.1\r\n- GPU model and memory: Nvidia P3200\r\n\r\n\r\n>  80               \"environment variable. Download and install CUDA %s from \"\r\n>      81               \"this URL: https://developer.nvidia.com/cuda-90-download-archive\"\r\n> ---> 82               % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\n>      83 \r\n>      84       if hasattr(build_info, \"cudnn_dll_name\") and hasattr(\r\n> \r\n> ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nIt seems like  C:\\...\\.conda\\envs\\tensorflow1\\Lib\\site-packages\\tensorflow\\python\\platform\\self_check.py runs with \".cuda_version_number\" that is locked to CUDA 10.0 looking for cudart64_100.dll when the latest CUDA version is 10.1. The Tensorflow github model repository (https://github.com/tensorflow/models) is still looking for CUDA version 10.0's  cudart64_100.dll while the CUDA 10.1 has  cudart64_101.dll. Because of this code below, Import Tensorflow will fail the self check each time.\r\n\r\n```\r\n        try:\r\n          ctypes.WinDLL(build_info.cudart_dll_name)\r\n        except OSError:\r\n          raise ImportError(\r\n              \"Could not find %r. TensorFlow requires that this DLL be \"\r\n              \"installed in a directory that is named in your %%PATH%% \"\r\n              \"environment variable. Download and install CUDA %s from \"\r\n              \"this URL: https://developer.nvidia.com/cuda-90-download-archive\"\r\n              % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\n```\r\n\r\n", "comments": ["@AeroWRX \r\nCan you please check by installing with CUDA 10.0 .Please,check the tensorflow website to install gpu [support](https://www.tensorflow.org/install/gpu) .Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31808\">No</a>\n"]}, {"number": 31807, "title": "tf.load_op_library unable to load manylinux2010 repaired custom ops", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No -- using https://github.com/tensorflow/custom-op (But it breaks for addons too)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf-nightly & tf-nighty-2.0-preview\r\n\r\n**Describe the current behavior**\r\nCurrently when I build a custom op in the `tensorflow/tensorflow:custom-op-ubuntu16` docker image using the defined steps I get an install-able pip package `tensorflow_zero_out-0.0.1-cp27-cp27mu-linux_x86_64.whl` \r\n\r\nThis works fine, however if I repair that wheel to be manylinux2010 compliant, then `tf.load_op_library` will fail to find the custom-op.\r\n```\r\npython -c \"import tensorflow as tf; print(dir(tf.load_op_library('manylinux/tensorflow_zero_out/python/ops/_zero_out_ops.so')))\"\r\n\r\n['LIB_HANDLE', 'OP_LIST', 'ZeroOut', '_IS_TENSORFLOW_PLUGIN', \r\n'_InitOpDefLibrary', '__builtins__', '__doc__', '__name__', '__package__', \r\n'_collections', '_common_shapes', '_context', '_core', '_dispatch', '_doc_controls', \r\n'_dtypes', '_errors', '_execute', '_kwarg_only', '_op_def_lib', '_op_def_library', \r\n'_op_def_pb2', '_op_def_registry', '_ops', '_pywrap_tensorflow', '_six', \r\n'_tensor_shape', 'deprecated_endpoints', 'tf_export', 'zero_out',\r\n 'zero_out_eager_fallback']\r\n```\r\n\r\n```\r\npython -c \"import tensorflow as tf;print(dir(tf.load_op_library('manylinux2010/tensorflow_zero_out/python/ops/_zero_out_ops.so')))\"\r\n\r\n['LIB_HANDLE', 'OP_LIST', '_IS_TENSORFLOW_PLUGIN', \r\n'_InitOpDefLibrary', '__builtins__', '__doc__', '__name__', '__package__', \r\n'_collections', '_common_shapes', '_context', '_core', \r\n'_dispatch', '_doc_controls', '_dtypes', '_errors', '_execute', '_kwarg_only', \r\n'_op_def_lib', '_op_def_library', '_op_def_pb2', '_op_def_registry', '_ops', \r\n'_pywrap_tensorflow', '_six', '_tensor_shape', 'deprecated_endpoints', 'tf_export']\r\n```\r\n\r\nNotice  `'zero_out'` &  `'zero_out_eager_fallback'` are not found in the loaded library for manylinux2010\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\ngit clone https://github.com/tensorflow/custom-op.git && cd custom-op\r\ndocker run -it --rm -v ${PWD}:/workspace -w /workspace tensorflow/tensorflow:custom-op-ubuntu16 /bin/bash\r\n\r\npip install tf-nightly\r\n./configure.sh\r\nbazel build build_pip_pkg\r\nbazel-bin/build_pip_pkg artifacts\r\n\r\n# Installed auditwheel is too old for manylinux2010\r\npip3 install --upgrade auditwheel\r\n\r\n# Libtensorflow framework needs to be on LD path\r\nexport LD_LIBRARY_PATH=\"/usr/local/lib/python2.7/dist-packages/tensorflow_core\"\r\n\r\n# Repair logs look more or less okay\r\nauditwheel -v repair --plat manylinux2010_x86_64 artifacts/tensorflow_zero_out-0.0.1-cp27-cp27mu-linux_x86_64.whl &> repair.txt\r\n```\r\n**Other info / logs**\r\nHere are the auditwheel repair logs: \r\n[repair.txt](https://github.com/tensorflow/tensorflow/files/3521714/repair.txt)\r\n\r\nHere are the readelf inspections of the so files:\r\n[readelf.txt](https://github.com/tensorflow/tensorflow/files/3521717/readelf.txt)\r\n[readelf-manylinux2010.txt](https://github.com/tensorflow/tensorflow/files/3521718/readelf-manylinux2010.txt)\r\n\r\nHere are the so files:\r\n[so-files.zip](https://github.com/tensorflow/tensorflow/files/3521726/so-files.zip)\r\n\r\ncc @perfinion @gunan @yifeif \r\n\r\n\r\n--------------------------EDIT--------------------\r\nHere are the extracted whl directories which will work with the python `tf.load_op_library` commands from above. (Manylinux2010 repair makes it so the custom op depends on a newly copied libtensorflow_framework.so which is part of the new whl):\r\n[custom-op-dirs.zip](https://github.com/tensorflow/tensorflow/files/3522649/custom-op-dirs.zip)\r\n\r\n", "comments": ["I remember I encountered an issue when there is a collision of names for added kernel ops. (used to be fine for 1.14, not with new tf-nightly) Wondering if there are multiple versions of zero_out kernel ops?", "> I remember I encountered an issue when there is a collision of names for added kernel ops. (used to be fine for 1.14, not with new tf-nightly) Wondering if there are multiple versions of zero_out kernel ops?\r\n\r\nThanks! Looking at the binaries' symbols I'm not seeing any duplication that isn't present in the .so before auditwheel repair though:\r\nhttps://www.diffchecker.com/pfJbJX8g\r\n\r\nIs there a way to increase the verbosity of the [load_library](https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/framework/load_library.cc) call so we could see if there is a conflict or something else?\r\n\r\nThe only major difference I see is that the repaired binary requires the newly copied `libtensorflow_framework-65610c2c.so.1` instead of the `libtensorflow_framework.so.1` that would get picked up from the TF install. I'm not sure what the implications of that are though and without being able to step through load_library function it's a bit tough.\r\n\r\n", "My previous issue was the LMDBDataset. I initially implemented LMDBDataset (C++) into TF's core rep (tf.contrib) some time ago. Later on since we try to modularize, the LMDBDataset has been moved to tensorflow/io. So there are two copies if both tensorflow and tensorflow-io are loaded.\r\n\r\nThat used to be fine. However, very recently I noticed that LMDBDataset in tensorflow/io is not working anymore with tf-nightly (couldn't remember which version but must be very recent), and I have to change the name in tensorflow/io to LMDBDatasetV2 to get around it.\r\n\r\nDon't know if this could be related as well.", "Ah the `libtensorflow_framework.so.1` is a known limitation of auditwheel.\r\n\r\nI wrote a patch for auditwheel, to get around the issue :  tensorflow/io@02dcf4a", "@yongtang Amazing thanks so much! Could you explain what that file edit does / why that patch works (I'm assuming somehow tricks auditwheel to thinking the sharedlib is a common one on all systems)?\r\n\r\nWe should probably describe this and include the patch in custom-op repo.", "EDIT -- Found out what policy.json was being editted:\r\nhttps://github.com/pypa/auditwheel/blob/master/auditwheel/policy/policy.json\r\n\r\nThanks again for the patch @yongtang!"]}, {"number": 31806, "title": "Added MNIST classifier example to Lite micro project", "body": "As we discussed in the Tensorflow lite micro SIG last week, I've tidied up my MNIST example project. I put this together originally to help me work out how to use TFL micro, so I tried to keep everything a simple as possible.\r\n\r\nUnlike the other example I've not been able to test this with any hardware apart from my disembodied LEON 3 emulator. I don't have access to any of the other hardware boards used, it would be great if we could add live camera input into this project at some point.\r\n\r\nThis demo includes a pull Tensorflow python project which generates and exports the model used. This includes a module called `flatbuffer_2_tfl_micro` which writes a flatbuffer directly to a pair of `.cc` and `.h` files avoiding the need to use the `xxd` utility and a bit of manual correction as described in [this tutorial](https://www.tensorflow.org/lite/microcontrollers/build_convert). If there isn't anything like this in the works then this would be good to move into the official process.\r\n\r\nFinally, I couldn't get the build to automatically run the tests from the Makefile. If anyone can let me know what I missed then I'll update this PR to include the fix.", "comments": ["@PeteBlackerThe3rd Can you please check reviewer comments and keep us posted. Thanks!", "@gbaned Will do, I should be able to get this recommendation added to this PR in a week hopefully.", "Yes this is still on-going. I've just got a lot on my plate at the moment. I'll try and get the python notebook into this PR over the weekend.", "I've just added the python notebook, and addressed @njeffrie s comments. \r\n\r\nI still need to merge this with the latest changes including the path change now that micro is no longer and experimental project. I hope to get this complete ready for the final reviews today.", "Well this got a bit messy, but I think I've managed to sort this out. I've forced this back to the latest tensorflow commit and re-added my changes.\r\n\r\nThese new commits also include changing the path of the example to the new non-experimental path.", "@PeteBlackerThe3rd  Can you please check @njeffrie's comments and keep us posted ? Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@PeteBlackerThe3rd, Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31805, "title": "Bidirectional does not merge RNN outputs if return_state is True", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Pre-installed on Google Colab GPU Python3 instance\r\n- TensorFlow version (use command below): `v1.14.0-0-g87989f6959 1.14.0`\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA Tesla T4\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen using GRU with Bidirectional wrapper, setting, `return_state=True, return_sequences=False` in GRU parameters and `merge_mode=\"sum\"` in Bidirectional parameters does not merge outputs. This makes it impossible to use in a Sequential model.\r\n**Describe the expected behavior**\r\nThis issue was resolved upstream in Keras in keras-team/keras#8977\r\nIf `merge_mode` is used in Bidirectional, it should merge the returned state from each wrapped GRU even if `return_state=True, return_sequences=False` is set.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```py\r\nfrom tensorflow.keras import layers, Sequential\r\nfrom tensorflow.keras.layers import Bidirectional, GRU\r\nmodel = Sequential()\r\nmodel.add(Bidirectional(\r\n    GRU(512, return_sequences=False, return_state=True),\r\n    merge_mode='sum',\r\n    input_shape=(None,13)\r\n))\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```py\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-26-a9c310b7c128> in <module>()\r\n      6     GRU(512, return_sequences=False, return_state=True),\r\n      7     merge_mode='sum',\r\n----> 8     input_shape=(None,13)\r\n      9 ))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\r\n    178         # If an input layer (placeholder) is available.\r\n    179         if len(nest.flatten(layer._inbound_nodes[-1].output_tensors)) != 1:\r\n--> 180           raise ValueError('All layers in a Sequential model '\r\n    181                            'should have a single output tensor. '\r\n    182                            'For multi-output layers, '\r\n\r\nValueError: All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\r\n```", "comments": ["Sorry misread the original Pull Request in keras"]}, {"number": 31804, "title": "build tensorflow from source -> screen freeze", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): 9.1\r\n- CUDA/cuDNN version: 10.1.168 / 7.6.1.34\r\n- GPU model and memory: GTX 850M / 2G\r\n\r\n**Describe the problem**\r\nI launched bazel to build tensorflow so that it can support CUDA and CUDNN for deep learning. But at around 20 minutes, my screen freezed, I could do nothing and it seemed that my pc was not really busy (as opposed to the first 20 minutes). I wait for 3 more hours but nothing happened. I had to shut down my computer via the power-on button. I maybe ran out of RAM !??\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**What I tried**\r\nI followed the indications from : https://www.tensorflow.org/install/source\r\nand add --local_ram_resources=2048 flag but bazel did not recognized that flag.\r\n\r\nDo you thing it is indeed a RAM problem or something else ?\r\n", "comments": ["That behavior is consistent with OOM, you've used all RAM available on your system.", "Ok, thank you for your answers. Is there a way to solve this ?\r\nI have no swap, perhaps create one could helps ?\r\nin addition, would it be usefull to increase my /tmp size ? (3G now)\r\n\r\nI tried bazel flag to use limited amount of RAM, but it did not recognized that flag", "Swap helps. Increasing /tmp size not really.\r\n\r\nThe most that helps is using bazel flags to limit number of jobs running in parallel and to limit resources. For this, you will have to consult the documentation for the specific Bazel version you are using as some of the flags got renamed during Bazel development.\r\n\r\nLooking at https://docs.bazel.build/versions/0.21.0/command-line-reference.html I see that you might want to try `--host_jvm_args=` (and pass memory flags to the JVM directly), `--jobs=1`, `--ram_utilization_factor=`, `--local_resources=`", "Thanks a lot. It worked with just --jobs=2 and --ram_utilization_factor=30.\r\n\r\nBut the build went for more than 3 hours. I thus went to bed, and at the morning the terminal where I launched the build was closed. I relaunched the build and it takes few minutes. I guess that most of the binary files were up to go. I then maked a pip package anyway with : \r\n````bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg````\r\n\r\nthen installed it with : \r\n````sudo pip3 install /tmp/tensorflow_pkg/tensorflow-1.13.2-cp37-cp37m-linux_x86_64.whl````\r\n\r\nBut it appeared that I had to uninstall tensorflow-gpu 1.14 and install older version. I thus did : \r\n````\r\nsudo pip3 uninstall tensorflow-gpu\r\nsudo pip3 install tensorflow-gpu==1.13.2\r\n````\r\n\r\nFinally, when trying my new installation in spyder and lunching the training of my neural network, I got the following (same message on Python shell when creating a tf.Session()). But the training did not start and packages were not imported anymore.\r\n\r\n````\r\n2019-08-21 11:42:42.676833: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2395140000 Hz\r\n2019-08-21 11:42:42.677420: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56459645fad0 executing computations on platform Host. Devices:\r\n2019-08-21 11:42:42.677439: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-21 11:42:43.230744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-21 11:42:43.231099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 850M major: 5 minor: 0 memoryClockRate(GHz): 0.9015\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.92GiB\r\n2019-08-21 11:42:43.231124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-08-21 11:42:43.248688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-21 11:42:43.248719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-08-21 11:42:43.248726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-08-21 11:42:43.248807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1742 MB memory) -> physical GPU (device: 0, name: GeForce GTX 850M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-08-21 11:42:43.250727: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x564592809850 executing computations on platform CUDA. Devices:\r\n2019-08-21 11:42:43.250745: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 850M, Compute Capability 5.0\r\n````", "Build taking longer: this is expected, as now you have fewer workers available\r\nTerminal being closed: this is strange, probably there was still an OOM and the oom-killer killed the terminal\r\nSecond build being faster: again expected, most of the files were compiled\r\n\r\nI don't understand why you need to uninstall and install a released pip package if you're compiling one. You can either use the compiled one or one from pypi, but not together.", "> I don't understand why you need to uninstall and install a released pip package if you're compiling one. You can either use the compiled one or one from pypi, but not together.\r\n\r\nI am not sure to understand what you mean. Actually, once I built tensorflow, create a pip package, and launched the installation with : \r\n\r\n````sudo pip3 install /tmp/tensorflow_pkg/tensorflow-1.13.2-cp37-cp37m-linux_x86_64.whl````\r\n\r\nI got this (as I installed tensorflow 1.13.2) :\r\n````\r\n...\r\ntensorflow-gpu 1.14.0 has requirement tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 1.13.1 which is incompatible.\r\ntensorflow-gpu 1.14.0 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\r\nInstalling collected packages: tensorboard, tensorflow-estimator, tensorflow\r\n  Found existing installation: tensorboard 1.14.0\r\n    Uninstalling tensorboard-1.14.0:\r\n      Successfully uninstalled tensorboard-1.14.0\r\n  Found existing installation: tensorflow-estimator 1.14.0\r\n    Uninstalling tensorflow-estimator-1.14.0:\r\n      Successfully uninstalled tensorflow-estimator-1.14.0\r\nSuccessfully installed tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\r\nsudo pip3 install tensorflow_pkg/tensorflow-1.13.2-cp37-cp37m-linux_x86_64.wh  8,16s user 1,26s system 66% cpu 14,075 total\r\n````\r\nI thus downgraded tensorflow-gpu\r\n\r\nBut the behavior on my CNN training is the same whatever the version is (as it is about tensorboard). And there is actually no training launched (It was perfectly working before CUDA installation and tensorflow build).", "tensorflow-gpu is tensorflow working with CUDA. You don't need to compile it yourself if that one satisfies your requirements.\r\n\r\nIf you want to install the wheel you compiled you either uninstall all tensorflow, tensorboard, tensorflow_estimator versions you have installed or you use a virtual env. Otherwise, you'll get conflicts in the environment", "Ok, I understand. I was confused because I thought that tensorflow-gpu was \"just\" a additional library working with tensorflow. Sorry for that.\r\n\r\nHowever, I have a problem to launch my CNN training anyway. What I did : \r\n````\r\nsudo pip3 uninstall tensorflow tensorflow-gpu tensorboard tensorflow_estimator\r\nsudo pip3 install tensorflow_pkg/tensorflow-1.13.2-cp37-cp37m-linux_x86_64.whl\r\n````\r\nthen, when I launch the training, it seems to see my GPU card, but nothing more happens. The training don't start, the session stop and the previously imported package are not imported anymore. I only got this : \r\n````\r\n2019-08-21 11:42:42.676833: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2395140000 Hz\r\n2019-08-21 11:42:42.677420: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56459645fad0 executing computations on platform Host. Devices:\r\n2019-08-21 11:42:42.677439: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-21 11:42:43.230744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-21 11:42:43.231099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 850M major: 5 minor: 0 memoryClockRate(GHz): 0.9015\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.92GiB\r\n2019-08-21 11:42:43.231124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-08-21 11:42:43.248688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-21 11:42:43.248719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-08-21 11:42:43.248726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-08-21 11:42:43.248807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1742 MB memory) -> physical GPU (device: 0, name: GeForce GTX 850M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n````", "Do you have a code sample? To me this looks like tensorflow executes something on the GPU device, but can't see what from the logs alone", "Just one note : It was working well before I installed CUDA and built tensorflow.\r\n````\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport time\r\n\r\n# Functions that creates new layers.\r\ndef new_conv_layer(input, num_input_channels, filter_size, num_filters, init=None, name=None):\r\n    with tf.variable_scope(name):\r\n        # Shape of the filter-weights for the convolution\r\n        shape = [filter_size, filter_size, num_input_channels, num_filters]\r\n        # Create new weights (filters) with the given shape\r\n        weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05), name=\"kernel\")\r\n        # Create new biases, one for each filter\r\n        biases = tf.Variable(tf.constant(0.05, shape=[num_filters]), name=\"bias\")\r\n        # TensorFlow operation for convolution\r\n        layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\r\n        # Add the biases to the results of the convolution.\r\n        layer += biases\r\n        \r\n        return layer, weights\r\n\r\ndef new_pool_layer(input, name=None):\r\n    with tf.variable_scope(name):\r\n        # TensorFlow operation for convolution\r\n        layer = tf.nn.max_pool(value=input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n        \r\n        return layer\r\n\r\ndef new_relu_layer(input, name=None):\r\n    with tf.variable_scope(name):\r\n        layer = tf.nn.relu(input)\r\n        \r\n        return layer\r\n\r\ndef new_fc_layer(input, num_inputs, num_outputs, name=None):\r\n    with tf.variable_scope(name):\r\n        # Create new weights and biases.\r\n        weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05), name=\"kernel\")\r\n        biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]), name=\"bias\")\r\n        \r\n        # Multiply the input and weights, and then add the bias-values.\r\n        layer = tf.matmul(input, weights) + biases\r\n        \r\n        return layer\r\n\r\n# %%\r\ndata = input_data.read_data_sets('machine_learning/datasets_zip/MNIST/', one_hot=True)\r\n\r\nprint(\"Size of:\")\r\nprint(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\r\nprint(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\r\nprint(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))    \r\n\r\n# Placeholder variable for the input images\r\nx = tf.placeholder(tf.float32, shape=[None, 28*28], name='X')\r\n# Reshape it into [num_images, img_height, img_width, num_channels]\r\nx_image = tf.reshape(x, [-1, 28, 28, 1])\r\n    \r\n# Placeholder variable for the true labels associated with the images\r\ny_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\r\ny_true_cls = tf.argmax(y_true, dimension=1)\r\n\r\n# Convolutional-Pooling-Relu Layer 1\r\nlayer_conv1, weights_conv1 = new_conv_layer(input=x_image, num_input_channels=1, filter_size=5, num_filters=6, name =\"conv1\")\r\nlayer_pool1 = new_pool_layer(layer_conv1, name=\"pool1\")\r\nlayer_relu1 = new_relu_layer(layer_pool1, name=\"relu1\")\r\n\r\n# Convolutional-Pooling-Relu Layer 2\r\nlayer_conv2, weights_conv2 = new_conv_layer(input=layer_relu1, num_input_channels=6, filter_size=5, num_filters=16, name= \"conv2\")\r\nlayer_pool2 = new_pool_layer(layer_conv2, name=\"pool2\")\r\nlayer_relu2 = new_relu_layer(layer_pool2, name=\"relu2\")\r\n\r\n\r\n# Flatten Layer\r\nnum_features = layer_relu2.get_shape()[1:4].num_elements()\r\nlayer_flat = tf.reshape(layer_relu2, [-1, num_features])\r\n\r\n\r\n# Fully-Connected-Relu Layer 1\r\nlayer_fc1 = new_fc_layer(layer_flat, num_inputs=num_features, num_outputs=128, name=\"fc1\")\r\nlayer_relu3 = new_relu_layer(layer_fc1, name=\"relu3\")\r\n\r\n# Fully-Connected Layer 2\r\nlayer_fc2 = new_fc_layer(input=layer_relu3, num_inputs=128, num_outputs=10, name=\"fc2\")\r\n\r\n\r\n# Use Softmax function to normalize the output\r\nwith tf.variable_scope(\"Softmax\"):\r\n    y_pred = tf.nn.softmax(layer_fc2)\r\n    y_pred_cls = tf.argmax(y_pred, dimension=1)\r\n\r\n# Use Cross entropy cost function\r\nwith tf.name_scope(\"cross_ent\"):\r\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)\r\n    cost = tf.reduce_mean(cross_entropy)\r\n\r\n# Use Adam Optimizer\r\nwith tf.name_scope(\"optimizer\"):\r\n    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(extra_update_ops):\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\r\n\r\n# Accuracy\r\nwith tf.name_scope(\"accuracy\"):\r\n    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\nnum_epochs = 100\r\nbatch_size = 100    \r\n\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as sess:\r\n    # Initialize all variables\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    train_plt, vali_plt = [], []\r\n    # Loop over number of epochs\r\n    for epoch in range(num_epochs):\r\n        start_time = time.time()\r\n        train_accuracy = 0\r\n        \r\n        for batch in range(0, int(len(data.train.labels)/batch_size)):\r\n            # Get a batch of images and labels\r\n            x_batch, y_true_batch = data.train.next_batch(batch_size)\r\n            # Put the batch into a dict with the proper names for placeholder variables\r\n            feed_dict_train = {x: x_batch, y_true: y_true_batch}\r\n            # Run the optimizer using this batch of training data.\r\n            sess.run(optimizer, feed_dict=feed_dict_train)\r\n            # Calculate the accuracy on the batch of training data                \r\n            train_accuracy += sess.run(accuracy, feed_dict=feed_dict_train)\r\n          \r\n        train_accuracy /= int(len(data.train.labels)/batch_size)\r\n        \r\n        # Generate summary and validate the model on the entire validation sets xs\r\n        vali_accuracy = sess.run(accuracy, feed_dict={x:data.validation.images, y_true:data.validation.labels})                \r\n        \r\n        end_time = time.time()\r\n        \r\n        print(\"Epoch \"+str(epoch+1)+\" completed : Time usage \"+str(int(end_time-start_time))+\" seconds\")\r\n        print(\"\\tAccuracy:\")\r\n        print(\"\\t- Training Accuracy:\\t{}\".format(train_accuracy))\r\n        print(\"\\t- Validation Accuracy:\\t{}\".format(vali_accuracy))        \r\n    \r\n    save_path = saver.save(sess, \"/home/adrien/machine_learning/Saved_model/CNN_trained.ckpt\")\r\n````", "Let's try minimizing this code, or add some print statements around to see where the lock/break happens.", "I added print statements and it seems that this is the variable initialization that fails.\r\nthe print(\"\\n2\\n\") is not reached. I'll carry on investigation and add more if required\r\n\r\n````\r\n...\r\nprint (\"\\n0\\n\")\r\n\r\nwith tf.Session() as sess:\r\n    # Initialize all variables\r\n       \r\n    print(\"\\n1\\n\")\r\n    \r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(\"\\n2\\n\")\r\n...\r\n````", "Can you change code to use eager execution instead? (The default in 2.0 and after)", "I'll do it in the day and come back to you. thanks", "here is my code with eager execution of a fully connected NN, with only one hidden layer. It is still failling due to variables. I added several print statements and it fails on the first call of ````def new_fc_layer(...)```` function. ````print(\\nlayer 1\\n)```` is not showing. all other print are ok untill then. I got the same log I gave you before, without error. The training does not launch, the run stops and the imported packages are not imported anymore.\r\n\r\n````\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\n# Enable Eager Execution\r\ntf.enable_eager_execution()\r\n\r\n# Create one fully connected layer.\r\ndef new_fc_layer(input, num_inputs, num_outputs, name=None):\r\n    print(\"\\nlayer 0\\n\")\r\n    # Create new weights and biases.\r\n    weights = tfe.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05), name=\"kernel\")\r\n    print(\"\\nlayer 1\\n\")\r\n    biases = tfe.Variable(tf.constant(0.05, shape=[num_outputs]), name=\"bias\")\r\n\r\n    # Multiply the input and weights, and then add the bias-values.\r\n    layer = tf.matmul(input, weights) + biases\r\n    \r\n    return layer\r\n\r\n# input MNIST images batch, one hidden layer 128 nodes, outpute 10 nodes.\r\ndef simple_fc(batch):\r\n    print(\"\\nfc 0\\n\")\r\n    # Fully-Connected Layer 1\r\n    layer_fc1 = new_fc_layer(input=batch, num_inputs=784, num_outputs=128, name=\"fc1\")\r\n    print(\"\\nfc 1\\n\")\r\n    # Fully-Connected Layer 2\r\n    logits = new_fc_layer(input=layer_fc1, num_inputs=128, num_outputs=10, name=\"fc2\")\r\n    \r\n    return logits\r\n\r\n# Use Softmax function to normalize the output\r\ndef softmax(logits):\r\n    y_pred = tf.nn.softmax(logits=logits)\r\n    y_pred_cls = tf.argmax(y_pred, dimension=1)\r\n    \r\n    return y_pred_cls\r\n\r\n# Use Cross entropy cost function\r\ndef cost(logits, labels):\r\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\r\n    return tf.reduce_mean(cross_entropy)\r\n\r\n# Use Adam Optimizer\r\ndef train_step(opt, batch, labels):\r\n    print(\"\\ntrain 0\\n\")\r\n    # Pass batch through the fc NN\r\n    logits_ = simple_fc(batch)\r\n    print(\"\\ntrain 1\\n\")\r\n    # Calcul the cost\r\n    cost_ = cost(logits_, labels)\r\n    # Optimize cost function\r\n    opt.minimize(cost_)\r\n\r\n# download MNIST data.\r\ndata = input_data.read_data_sets('machine_learning/datasets_zip/MNIST/', one_hot=True)\r\n\r\n# Initialize optimizer\r\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\r\n\r\n# Retrieve train and vali data.\r\nX_train, y_train = data.train.images, data.train.labels\r\nX_vali, y_vali = data.validation.images, data.validation.labels\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\n\r\nnum_epochs = 100\r\nbatch_size = 100    \r\n\r\n# Loop over number of epochs\r\nfor epoch in range(num_epochs):\r\n    train_accuracy = 0\r\n        \r\n    for X_batch, y_batch in tfe.Iterator(train_dataset.shuffle(100).batch(batch_size)):\r\n        \r\n        print(\"\\n0\\n\")\r\n        \r\n        # Launch training\r\n        train_step(optimizer, X_batch, y_batch)\r\n        # Batch accuracy                \r\n        train_accuracy += accuracy(X_batch, y_batch)\r\n        \r\n        print(\"\\n1\\n\")\r\n    ````\r\n\r\n", "Can you check the same code using the pip-installed tensorflow-gpu package?\r\n\r\nCan you test this if using the r2.0 branch or master? On both of these eager is default, so you don't need to get it from contrib. Can also try `pip install tensorflow_gpu==2.0.0-beta1`.\r\n\r\nMake sure the pip installs are in separate virtualenv so you can switch between them?\r\n\r\nCan you test using a much simpler script, such as (below is supposed to work on TF2.0)\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.get_variable(\"x\", tf.random.truncated_normal([3, 2], stddev=0.05))\r\ntf.print(x)\r\n```\r\n\r\nThen start altering the script, little by little until you discover why it doesn't work for you", "I created a virtual env with only tensorflow-gpu installed with:\r\n\r\n````sudo pip3 install tensorflow-gpu````\r\n\r\nand there is nothing that fails. There is simply NO improvement in training time. I guess this is because pip3 installs tensorflow-gpu 1.14, which work with CUDA 10.0, but I have CUDA 10.1\r\n\r\nSo, I guess this is because of my compiled version of tensorflow-gpu ?", "None of the released pips support CUDA 10.1", "Yes, that is why I compiled tensorflow by my own to support CUDA 10.1. But I assume that my compiled version is bad as tensorflow-gpu installed via pip does not fail but instead continues whithout GPU.\r\n\r\ndoes it worth to compile it again from the beginning or I carry on investigation as you mentioned earlier ?", "I'd try to use the compiled version with the investigation mentioned above.", "It works !! And it appears that this is not due to my installation. I was actually starting spyder3 from the desktop, but I came to do it via the terminal and then all went well. I did not know that distinctions are done between these tow types of launch.\r\n\r\nI got 3 seconds per epoch against 20 seconds without GPU. great improvement.\r\n\r\nThanks a lot for your great help. I now better understand how the whole thing works.", "@HattoriSan,\r\nGlad it resolved. \r\nClosing the issue. Feel free to reopen if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31804\">No</a>\n"]}, {"number": 31803, "title": "Change all 'hashlib.md5' to 'hashlib.sha1'", "body": "Tied to issue #31800 ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31803) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31803) for more info**.\n\n<!-- ok -->", "So what can I do to get this through? I've already made hot changes to my pip install of tensorflow (essentially applying this patch) and I've been able to train models and get them to predict and evaluate without much issues..", "@g19fanatic can you please resolve merge conflicts and check build failures ?", "Will do!\n\nOn Fri, Oct 25, 2019, 2:37 PM Rajeshwar Reddy T <notifications@github.com>\nwrote:\n\n> @g19fanatic <https://github.com/g19fanatic> can you please resolve merge\n> conflicts and check build failures ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31803?email_source=notifications&email_token=AAEAZU3LTZZKGFPYYJLHNC3QQM4EZA5CNFSM4INZOWZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECJFYYA#issuecomment-546462816>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAEAZU65K6XOIVVYYDTTO23QQM4EZANCNFSM4INZOWZA>\n> .\n>\n", "Please also consider the compatibility issue from my review", "@mihaimaruseac  & @fchollet \r\n\r\nI reverted back that one specific change in data_utils.\r\n\r\nA quick search shows 2 locations in the codebase (besides tests testing the md5 hashing) that utilizes md5 for the file_hashing. This shouldn't break anyone who is still using these datasets (namely, imdb and reuters) on a non-FIPS compliant machine.\r\n\r\nAnyone who is specifically caring about their hashing function would be broken under a FIPS compliant OS but eh? Don't use depreciated hashing functions in new stuff? If you're trying to use the imdb or reuters datasets, I imagine the codebase could be updated to have the proper hash for the files...\r\n\r\nThis PR doesn't break my project and I don't specifically set any sort of hashing function and the default goes to sha265 anyways.", "The specific function has this signature `def _hash_file(fpath, algorithm='sha256', chunk_size=65535):` It already has different behavior based on the value of `algorithm`. I would suggest to add the change back under an `elif` to make the function support more that just `sha256` and `md5` hashes.", "> \r\n> \r\n> The specific function has this signature `def _hash_file(fpath, algorithm='sha256', chunk_size=65535):` It already has different behavior based on the value of `algorithm`. I would suggest to add the change back under an `elif` to make the function support more that just `sha256` and `md5` hashes.\r\n\r\nThis is specifically what my last commit does...\r\n[Adds back the change](https://github.com/tensorflow/tensorflow/pull/31803/commits/aa170db3af310dcb8545d6ec7476163014d46808#diff-ba2b45d565934dc5120efeef5457ea2eR320) under the elif under the `_hash_file` function.", "There is no `elif` in that patch.", "> \r\n> \r\n> There is no `elif` in that patch.\r\n\r\nI see what you're saying. You'd like to support MORE than the originally supported sha265 and md5.\r\n\r\nThat would require many more locations to be updated (and documentation). Honestly, out of scope of what I was trying to do with this issue and PR (namely make tensorflow work for FIPS compliant machines). \r\n\r\nMy provided PR has reverted the one `else:` back to where it was before as was described as a potential issue with my previous patch.", "I was only suggesting to add the sha1 to the specific file. But let me investigate more to see which other functions would need this and I'll come back here.", "> \r\n> \r\n> I was only suggesting to add the sha1 to the specific file. But let me investigate more to see which other functions would need this and I'll come back here.\r\n\r\nAs far as I can tell,   functions `validate_file` and `get_file` are in the call chain to `_hash_file`.\r\n\r\nbesides tests, `_hash_file` is only used in `validate_file` and a tutorial file<br>\r\n`validate_file` is only used in `get_file` <br>\r\n`get_file` is only used in \r\n```\r\n./lite/g3doc/models/style_transfer/overview.ipynb\r\n./lite/g3doc/performance/post_training_quant.ipynb\r\n./python/keras/datasets/boston_housing.py\r\n./python/keras/datasets/cifar10.py\r\n./python/keras/datasets/cifar100.py\r\n./python/keras/datasets/fashion_mnist.py\r\n./python/keras/datasets/imdb.py\r\n./python/keras/datasets/mnist.py\r\n./python/keras/datasets/reuters.py\r\n./python/keras/utils/data_utils.py\r\n./python/keras/utils/data_utils_test.py\r\n```\r\n\r\nand only imdb and reuters use md5 as the hashing algo.. every other one uses default options...\r\n\r\nleaving it as is should be okay...", "There are also issue with Google internal usages. That's why it needs more look.", "@mihaimaruseac Any update on this PR, please. Thanks!", "I will try to import this manually over the weekend. Thanks for the ping", "@g19fanatic Can you please resolve conflicts? Thanks!", "I am in the process of importing this manually (though changing to sha256 instead of sha1).", "Hello all, it's been a while, any news on this PR?", "@g19fanatic Can you please resolve conflicts? Thanks!", "@mihaimaruseac, Any update on this PR? Please. Thanks!", "I too am waiting on this fix. Deploying any code requires a patch to be applied for FIPS to not complain\r\n", "Hi guys.\r\n\r\nAny updates on this PR? It's extremely time consuming to ensure FIPS compliance on our end and it seems like quite a few users of the library are running into a scenario where that is necessary.", "I am starting an internal process to change these. We should have this done by the 2.5 release early next year."]}, {"number": 31802, "title": "RuntimeError: Quantization not yet supported for op: FAKE_QUANT", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Docker image latest-gpu-py3\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- GPU model and memory: RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nI have trained an autoencoder and want to convert it to a tflite model. I successfully froze the graph and was able to convert the non-quantized 32-bit-float version. When trying to convert the very same frozen graph file with the uint8 option, I get an error:\r\n\r\nRuntimeError: Quantization not yet supported for op: FAKE_QUANT\r\n\r\n**Describe the expected behavior**\r\n\r\nThe frozen model does not seem to be corrupted because I was able to deploy the other version successfully. FAKE_QUANT should inherently be supported.\r\n\r\n**Code to reproduce the issue**\r\n\r\nConversion was attempted with:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, inputs, outputs)\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe error traceback:\r\n```\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nTraceback (most recent call last):\r\n  File \"train_and_save.py\", line 289, in <module>\r\n    vae.create_tflite_model()\r\n  File \"train_and_save.py\", line 249, in create_tflite_model\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 908, in convert\r\n    inference_output_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 200, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Quantization not yet supported for op: FAKE_QUANT\r\n```\r\n\r\nIf it helped, I could provide the frozen graph file.", "comments": ["For clarification, I am not using some fancy layers, just simple dense layers:\r\n\r\n```\r\ndef build_model(inputs):\r\n\r\n    fc_layer = tf.compat.v1.layers.dense\r\n    dims = [90, 40, 10]\r\n\r\n    with tf.name_scope('model'):\r\n        x = tf.identity(inputs, name='x')\r\n\toutputs = x\r\n\r\n\t# encoder\r\n\tfor i, size in enumerate(dims[1:]):\r\n\t\toutputs = fc_layer(outputs, size, name=str(i + 1), activation=tf.nn.leaky_relu)\r\n\t\t\t\r\n\t# decoder\r\n\tfor i, size in enumerate(dims[-2:0:-1], len(dims)):\r\n\t\toutputs = fc_layer(outputs, size, name=str(i), activation=tf.nn.leaky_relu)\r\n\t\t\t\r\n\t# prediction\r\n\toutputs = fc_layer(outputs, dims[0], name=str(2*(len(dims) - 1)), activation=None)\r\n\ty = tf.identity(outputs, name='y')\r\n\r\n    with tf.name_scope('loss'):\r\n\trecon_err = tf.reduce_sum(tf.abs(x - y), axis=1)\r\n\ttotal_loss = tf.reduce_mean(recon_err)\r\n```\r\n@liyunlu0618 \r\nI have tried this with different activation functions, as a similar keras model does seem to have problems with the leaky_relu activation function. Unfortunately, this is not the case here and I get the same error.\r\n\r\nI have attached the frozen graph protocol buffer, so that you can try out the conversion yourself.\r\n\r\n[frozen.tar.gz](https://github.com/tensorflow/tensorflow/files/3530849/frozen.tar.gz)\r\n", "It's weird that you have FAKE_QUANT in your original graph. Did you use the quantization rewriter when you train you model? It's not supposed to be used together with post-training integer quantization.\r\n\r\nMaybe try to figure out where the fake quant op comes from first and get rid of it.", "Hi DocDriven, liyunlu0618, and Everyone..  Is there a solution for this issue? I have the same issue... For my experiment, I took the Keras model from the @fchollet reference model train on mnist. During training I apply quantization aware model for all layer (1st image attached), then I did Post Quantization 8bit Integer only I show the error unsupported fake quantize (2nd attached image) \r\n\r\nAnyone could help to solve this issue really appreciate ??\r\n1 image \r\n![image](https://user-images.githubusercontent.com/45351965/98456620-e8e6b700-21ba-11eb-9740-9b7f87fb7de3.png)\r\n\r\n2 image \r\n![image](https://user-images.githubusercontent.com/45351965/98456681-880bae80-21bb-11eb-8ae7-260ef1b09991.png)\r\n", "Since your entire model is trained with QAT, you don't need post-training quantization. Try remove all optimization related options and re-convert your model.", "@liyunlu0618 Thank you a lot for your answers !!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31802\">No</a>\n"]}, {"number": 31801, "title": "Add fused kernel of ExperimentalUnbatchAndBatchDataset for static optimization", "body": "This PR is part of the effort to address the issue in #31548. In certain situations there might be a need to adjust the batch of Dataset, e.g., from `m-batch` to `n-batch`. This could be achieved through `dataset.unbatch().batch(n)` though a fused step could be desirable.\r\n\r\nThis PR adds ExperimentalUnbatchAndBatchDataset so that it is possible to use it for static optimization.\r\n\r\nThis PR fixes #31548\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@rachellim please take a look, thanks", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31800, "title": "FIPS enable computers fail due to md5 use", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise Linux Workstation release 7.7\r\n```\r\nPython 2.7.5 (default, Jun 11 2019, 14:33:56)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.version.GIT_VERSION\r\n'v1.14.0-rc1-22-gaf24dc91b5'\r\n```\r\n- Installed using virtualenv? pip? conda?: pipenv install tensorflow\r\n\r\n\r\ntflearn fails to import due to tensorflows use of md5 on an fips enabled machine\r\n```\r\nPython 2.7.5 (default, Jun 11 2019, 14:33:56)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tflearn\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../lib/python2.7/site-packages/tflearn/__init__.py\", line 4, in <module>\r\n    from . import config\r\n  File \".../lib/python2.7/site-packages/tflearn/config.py\", line 5, in <module>\r\n    from .variables import variable\r\n  File \".../lib/python2.7/site-packages/tflearn/variables.py\", line 7, in <module>\r\n    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\n  File \".../lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 31, in <module>\r\n    from tensorflow.contrib import cloud\r\n  File \".../lib/python2.7/site-packages/tensorflow/contrib/cloud/__init__.py\", line 28, in <module>\r\n    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient\r\n  File \".../lib/python2.7/site-packages/tensorflow/contrib/bigtable/__init__.py\", line 29, in <module>\r\n    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient\r\n  File \".../lib/python2.7/site-packages/tensorflow/contrib/bigtable/python/ops/bigtable_api.py\", line 44, in <module>\r\n    resource_loader.get_path_to_datafile(\"_bigtable.so\"))\r\n  File \".../lib/python2.7/site-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \".../lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 73, in load_op_library\r\n    module_name = hashlib.md5(wrappers).hexdigest()\r\nValueError: error:060800A3:digital envelope routines:EVP_DigestInit_ex:disabled for fips\r\n```\r\nreplacing all md5 calls with sha1 calls should work?\r\n", "comments": ["Thanks for creating the issue, @g19fanatic! Thanks for looking into it, @mihaimaruseac! Recently encountered this as well. Currently manually editing the source code to achieve FIPS compliance, but it would be far better for TensorFlow to work with FIPS out of the box.", "I ran into this in load_library. Adding the optional argument `used_for_security=False` solved this for me.", "Any movement on this?  I still get it importing the latest version of TF.", "#31803 was an attempt at this but it required more work due to internal usages.\r\n\r\nI'm currently working on that, should be done by the time TF 2.5 lands", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31800\">No</a>\n", "I am confused as to what was done to fix this and why my issue was closed?\r\nhttps://github.com/tensorflow/tensorflow/pull/31803/files\r\nSo instead of merging in my PR, you do the same thing (but in less places?).\r\nWhy couldn't this have been merged over a year ago if you were just going to do the same thing?", "There are internal fixes in parallel with these ones that didn't get exposed.\r\n\r\nAlso, a year ago there were more files that needed change but they got replaced in the meanwhile.\r\n\r\nThere is one more public change to arrive, was delayed by a broken test.", "Hi,\r\nCan someone confirm that TF 2.5 has this fix? Reading the release note for 2.5, and I don't see FIPS mentioned.\r\nThanks.", "I can confirm that with v2.5 I no longer see the FIPS errors whereas with 2.4.1 I do.\r\n\r\n2.4.1 errors:\r\nFile \"/data/users/jprosser/tf/venv/lib64/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 67, in load_op_library\r\n    module_name = hashlib.md5(wrappers).hexdigest()\r\nValueError: error:060800A3:digital envelope routines:EVP_DigestInit_ex:disabled for fips\r\n"]}, {"number": 31799, "title": "Build tensorflow lite for aarch64: Error in script download_dependencies.sh ", "body": "**System information**\r\n- OS Platform :Linux Ubuntu 16.04\r\n- TensorFlow version: Release 1.14.0\r\nWhen I run the script ./tensorflow/lite/tools/make/download_dependencies.sh  I am getting following error:\r\n\r\ndownloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\r\n./tensorflow/lite/tools/make/download_dependencies.sh: line 64: curl: command not found\r\n\r\ngzip: stdin: unexpected end of file\r\ntar: Child returned status 1\r\ntar: Error is not recoverable: exiting now\r\n", "comments": ["I have the same behavior, also trying to build Release 1.14.0\r\n\r\n```\r\nuser@computer:~/Work/GIT/tensorflow$ ./tensorflow/lite/tools/make/download_dependencies.sh \r\ndownloading http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\r\n\r\ngzip: stdin: not in gzip format\r\ntar: Child returned status 1\r\ntar: Error is not recoverable: exiting now\r\n```", "Looks like duplicate https://github.com/tensorflow/tensorflow/issues/28926.", "@Stormreaver \r\n\r\nCan you please confirm if the issue still persists.Thanks!", "I fixed this issue by using master code", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31799\">No</a>\n"]}, {"number": 31798, "title": "[ROCm] Fix for the broken `--config=rocm` build ", "body": "The `--config=rocm` build was broken by two different commits. \r\n\r\nThis PR has two commits that fix each of those two breaks. The commit message for each commit has the details on the error and the fix.\r\n\r\nNeither error seems to be ROCm specific, and I am curious about whether the same error was also detected in builds for other platforms. Is there a way to find this out? Thanks.\r\n\r\nThe fixes are trivial in nature. please review and merge. thanks.\r\n\r\n\r\n--------------------------------------------------------------------------------------\r\n\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": ["@gunan @chsigg \r\n\r\nSorry to bug about this so early in the process, but would it be possible to get this PR merged before the cutting of the r1.15 branch which is scheduled for later(?) today. \r\n\r\nthanks\r\n\r\ndeven", "I think this PR (the change to tensorflow/core/util/ctc/ctc_loss_calculator.h) will also fix the builds on Power, which are failing with:\r\n```\r\n./tensorflow/core/util/ctc/ctc_loss_calculator.h:453:68: error: no matching function for call to 'LogSumExp(const Scalar&, double)'\r\n                           log(y(l_prime[u], output_delay_ + t + 1)));\r\n```\r\n\r\n", "> Could you look into the windows bazel build failures?\r\n\r\nwill do once the logs become available again (CI runs were triggered again by your approval, removing the links the previous run :) ).\r\n\r\nIn the error you have posted, the first error (at line 144) is for a line that was not changed by this PR. My guess is that this particular struct member initialization syntax is not supported by the Windows compiler. Is there anyway to confirm that the Windows build was passing for the PR that introduced this change (the commit in question is : https://github.com/tensorflow/tensorflow/commit/8774c72741b9bc4a5ae0af8d2306c0013364a38e )\r\n\r\nI do not have access to a Windows setup, on which I can run TF builds. So reproducing / fixing the build will be difficult to do. What I can do instead, is change that code to use vanilla initialization syntax, which I think should work for all. Would that be acceptable?", "The MLIR changes will be fixed shortly with a change from our side. It will\nbe conflicting with this PR. Could you please drop that commit?\n\nOn Tue, Aug 20, 2019, 18:06 Gunhan Gulsoy <notifications@github.com> wrote:\n\n> *@gunan* requested changes on this pull request.\n>\n> Could you look into the windows bazel build failures?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31798?email_source=notifications&email_token=ABZM5DTR5M66PZHAGGPCJTLQFQI6RA5CNFSM4INXPP32YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCCDNJUI#pullrequestreview-277271761>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABZM5DSDC62DQKLZBH3CA4TQFQI6RANCNFSM4INXPP3Q>\n> .\n>\n", "> The MLIR changes will be fixed shortly with a change from our side. It will be conflicting with this PR. Could you please drop that commit?\r\n\r\nyes let me drop that commit...makes my life easier as well. thanks.", "@chsigg , dropped the commit. \r\n\r\nPlease re-approve to trigger the CI runs. thanks again.", "the CI runs are now running into the MLIR failures.\r\n\r\nguess we will need to wait for the MLIR fix to show up first.", "I believe this issue was resolved by this commit: https://github.com/tensorflow/tensorflow/commit/ab76b81a6320ed5cb474ce7e7e02f48667b304e2", "> I believe this issue was resolved by this commit: [ab76b81](https://github.com/tensorflow/tensorflow/commit/ab76b81a6320ed5cb474ce7e7e02f48667b304e2)\r\n\r\n@wdirons I think you are right. \r\n\r\nI have kicked-off a build to verify it for `--config=rocm`. will close out the PR, once the build completes successfully", "confirming that `--config=rocm` build is back to normal.\r\n\r\nclosing out this PR"]}, {"number": 31797, "title": "Deep Learning Image: TensorFlow 1.14.0 m33 on Google Cloud produces wrong and non deterministic loss after backpropagation ", "body": "**System information**\r\n- Have I written custom code: Yes, the code is attached.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux thomas-tf-14 4.9.0-9-amd64 #1 SMP Debian 4.9.168-1+deb9u4 (2019-07-19) x86_64 GNU/Linux`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Tested on google cloud\r\n- TensorFlow installed from (source or binary): Binary: `Deep Learning Image: TensorFlow 1.14.0 m33` on Google Cloud\r\n- TensorFlow version: v1.14.0-0-g87989f6 1.14.0\r\n- Python version: 2.7.13 / 3.5.3\r\n- Bazel version (if compiling from source): Not compiled from source\r\n- GCC/Compiler version (if compiling from source): Not compiled from source\r\n- CUDA/cuDNN version: Not used \r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\nCurrent behaviour on Google Cloud using `Deep Learning Image: TensorFlow 1.14.0 m33` is non deterministic. During multiple runs the loss after performing backpropagation and updating a variable is different for different runs and does not match the loss of the non optimized standard tensorflow installation. This behaviour exists when using python2 and when using python3.\r\n\r\nCreated instance with:\r\n`gcloud compute instances create \"tf-1-14-cpu\" --zone=\"us-west1-b\" --image-family=\"tf-1-14-cpu\" --image-project=deeplearning-platform-release`\r\n\r\nRun 1:\r\n```\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: -3.698721931466375e+19\r\nLoss during step 2: -7.38836981337104e+19\r\n```\r\nRun 2:\r\n```\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: -0.41999998688697815\r\nLoss during step 2: -0.41999998688697815\r\n```\r\nRun 3:\r\n```\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: 9.46872814455392e+21\r\nLoss during step 2: 1.8914226722229864e+22\r\n```\r\n\r\n**Describe the expected behavior**\r\nOn the the same machine using a virtualenv to force the use of non optimized tensorflow as follows:\r\n```\r\nvirtualenv -p python3 test\r\nsource test/bin/activate\r\npip3 install tensorflow==1.14.0\r\n```\r\n\r\nRun 1:\r\n```\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: -1.4199999570846558\r\nLoss during step 2: -2.4200000762939453\r\n```\r\nRun 2:\r\n```\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: -1.4199999570846558\r\nLoss during step 2: -2.4200000762939453\r\n```\r\nRun 3:\r\n```\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: -1.4199999570846558\r\nLoss during step 2: -2.4200000762939453\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    session = tf.Session()\r\n    image = tf.get_variable(name=\"image\", shape=[1, 1, 1, 1], initializer=tf.constant_initializer(0.42))\r\n    session.run(tf.global_variables_initializer())\r\n    kernel = tf.constant(1.0, shape=[1, 1, 1, 1], dtype=tf.float32)\r\n\r\n    conv_out = tf.nn.conv2d(image, kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\r\n    max_conv_out = tf.math.reduce_max(conv_out, axis=2)\r\n    loss = -tf.reduce_sum(max_conv_out)\r\n    opt = tf.train.GradientDescentOptimizer(learning_rate=1.0, name=\"sgd\")\r\n    optimizer = opt.minimize(loss, var_list=[image], name=\"sgd_minimize\")\r\n\r\n    for step in range(3):\r\n        loss_value, _ = session.run([loss, optimizer])\r\n        print(\"Loss during step {}: {}\".format(step, loss_value))\r\n```\r\n\r\n\r\n**Other info / logs**\r\n[python-code.txt](https://github.com/tensorflow/tensorflow/files/3520927/python-code.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3520903/tf_env.txt)\r\n\r\n", "comments": ["I tried your code snippet in Google Colab using TF 1.14 and produced same expected results on 3 consecutive runs.\r\n```python\r\nLoss during step 0: -0.41999998688697815\r\nLoss during step 1: -1.4199999570846558\r\nLoss during step 2: -2.4200000762939453\r\n```\r\nPerhaps this is an issue with GCP instance and not with TF 1.14 version specifically.", "@ymodak thanks for running the code in Google Colab. Given these results I assume it used the standard (non mkl optimized) version of TF 1.14. \r\n\r\nDo you have the possibility to test the code with the mkl optimized version of TF 1.14 and run it on a CPU?", "That's correct I used non optimized TF version. I will add TensorFlow MKL group to get more information. Thanks!", "We are able to reproduce your issue on  1.14. We'll keep you posted on the fix", "@preethivenkatesh Thanks for checking this. Was it already possible to track down the reason for the non deterministic outputs and in which situations these appear?", "This bug is also present in tensorflow 1.15 with mkl (Deep Learning Image: TensorFlow 1.15.0 m38).", "The issue is resolved and merged into the master branch #35201 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31797\">No</a>\n", "The non deterministic and incorrect loss for the example above resurfaced in tensorflow 1.15. This happens both when installing the python3 library directly and when using docker:\r\n- `pip3 install https://storage.googleapis.com/intel-optimized-tensorflow/intel_tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl`\r\n- `docker pull \"gcr.io/deeplearning-platform-release/tf-cpu.1-15\"`\r\n\r\n(CC: https://github.com/tensorflow/tensorflow/pull/35201)", "@thomasZen It looks like we have had perf regression on a few models,  and the commit has been reverted. While we try to resolve this w/o impacting our other critical accelerations, are you okay to use this as a patch for your build?\r\n#35619"]}, {"number": 31796, "title": "Learning Rate Scheduler with multi GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nWhen using single GPU model and Training the LearningRateScheduler Callback works fine and everything runs as expected.\r\nIf I change to multi gpu training I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 165, in <module>\r\n    workers=min(multiprocessing.cpu_count()//2, 32)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1433, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\", line 208, in model_iteration\r\n    callbacks.on_epoch_begin(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 295, in on_epoch_begin\r\n    callback.on_epoch_begin(epoch, logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1351, in on_epoch_begin\r\n    raise ValueError('Optimizer must have a \"lr\" attribute.')\r\nValueError: Optimizer must have a \"lr\" attribute.\r\n```\r\n\r\nOnly thing change is single to multi gpu.", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31796\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31796\">No</a>\n"]}]