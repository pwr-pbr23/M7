[{"number": 15098, "title": "improve tf.saved_model.loader.load exception", "body": "If the `tags` argument to `tf.saved_model.loader.load` is wrong, the exception does not help.\r\nFirst: It says use saved_model_cli, but it take a while to figure out that this is a executable and not a python function.\r\nSecond: The required information (allowed tags) is known inside `tf.saved_model.loader.load` and now it prints this error\r\n```python\r\n>>> with tf.Session(graph=tf.Graph()) as sess:\r\n>>>    tf.saved_model.loader.load(sess, ['wrong tag'], 'path/to/model')\r\nRuntimeError: MetaGraphDef associated with tags 'wrong tag' could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`available_tags: [{'serve'}]\r\n```", "comments": ["Can one of the admins verify this patch?", "@davidsoergel any luck with this?", "@boeddeker  Hi, sorry to say that we dropped this PR. Please sync with the current master branch and update the PR accordingly, we will take a look once you update. Thanks !", "@harshini-gadige I rebased the master branch", "@k-w-w, Kathy, can you take a look? I haven't really worked on saved model."]}, {"number": 15097, "title": "Fixing \u201cactivate the Virtualenv\u201d", "body": "in \u201cInstalling TensorFlow on macOS\u201d. Probably needs to be fixed for Linux as well.", "comments": ["Can one of the admins verify this patch?", "Thanks, @JanX2 "]}, {"number": 15096, "title": "java.lang.UnsatisfiedLinkError: No implementation found for void com.ppdai.tensorflow.tracking.ObjectTracker.initNative(int, int, boolean)", "body": "recently i begin to learn and use tensorflow but some errors i don't know why,  help me please, error as follow:\r\n\r\nProcess: com.ppdai.tensorflow, PID: 4226\r\n                                                 java.lang.UnsatisfiedLinkError: No implementation found for void com.ppdai.tensorflow.tracking.ObjectTracker.initNative(int, int, boolean) (tried Java_com_ppdai_tensorflow_tracking_ObjectTracker_initNative and Java_com_ppdai_tensorflow_tracking_ObjectTracker_initNative__IIZ)\r\n                                                     at com.ppdai.tensorflow.tracking.ObjectTracker.initNative(Native Method)\r\n                                                     at com.ppdai.tensorflow.tracking.ObjectTracker.init(ObjectTracker.java:257)\r\n                                                     at com.ppdai.tensorflow.tracking.ObjectTracker.getInstance(ObjectTracker.java:220)\r\n                                                     at com.ppdai.tensorflow.tracking.MultiBoxTracker.onFrame(MultiBoxTracker.java:211)\r\n                                                     at com.ppdai.tensorflow.DetectorActivity.processImage(DetectorActivity.java:250)\r\n                                                     at com.ppdai.tensorflow.CameraActivity.onPreviewFrame(CameraActivity.java:149)\r\n                                                     at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1285)\r\n                                                     at android.os.Handler.dispatchMessage(Handler.java:111)\r\n                                                     at android.os.Looper.loop(Looper.java:194)\r\n                                                     at android.app.ActivityThread.main(ActivityThread.java:5868)\r\n                                                     at java.lang.reflect.Method.invoke(Native Method)\r\n                                                     at java.lang.reflect.Method.invoke(Method.java:372)\r\n                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1019)\r\n                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:814)\r\n\r\n1\u3001firstly compile aar from jcenter 1.4.0\r\n2\u3001secondly add related so to jniLibs \r\n      libtensorflow_demo.so\r\n      libtensorflow_inference.so\r\n      libandroid_tensorflow_lib.lo\r\n      benchmark_model\r\n3\u3001then use offical demo code to test tensorflow detect function, error comes.\r\n\r\nwhat will i can do to solve this problem?\r\n\r\ni have add related so ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @derekjchow: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 60 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "having the same problem.. any updates ?", "having the same problem.. me too,  any updates ?", "having the same problem.. me too, any updates ? ", "Solved this here [#19384](https://github.com/tensorflow/tensorflow/issues/19384)", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15095, "title": "add label_image for tflite", "body": "label_image for TensorFlow Lite is inspired by TensorFlow's\r\n[label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image), a command line app to load and run classifier\r\nmodels.", "comments": ["Can one of the admins verify this patch?", "@freedomtan did you address all review comments?", "@drpngx Yes, I think I did. Last batch of changes reflected all issues mentioned by @andrehentz", "Jenkins, test this please."]}, {"number": 15094, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15093, "title": "tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Sub", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**:pip\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0/6.0\r\n- **GPU model and memory**:GTX 1060 with 6GB memory\r\n- **Exact command to reproduce**:bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/wh/gitmodel/tensorflow/wh/frozen_1.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/home/wh/gitmodel/tensorflow/wh/frozen_lite.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=image_1 --output_arrays=InferenceTower/output6 --input_shapes=1,320,480,3\r\n\r\n### Describe the problem\r\nI am trying to convert a graph from .pb to .lite format using toco, but I get this error:\r\n2017-12-04 20:14:38.202653: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: Sub\r\nI think Sub is the basic op, Lite shoud support it. Is it right?", "comments": ["/CC @aselle, can you comment?", "I use summarize_graph tool to my model. it output :\r\nFound 3681310 (3.68M) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: 99 Const, 47 Identity, 19 Conv2D, 18 BiasAdd, 13 Relu, 10 Conv2DBackpropInput, 10 MirrorPad, 10 Mul, 10 Shape, 10 StridedSlice, 4 MaxPool, 1 ConcatV2, 1 Placeholder, 1 Sigmoid, 1 Sub", "Lite unfortunately currently doesn't support Sub. We are planning to add support to it soon.\r\nIf you could share the python source of your model, we might be able to point out an alternative approach. You could possibly simulate a subtraction by doing a 1x1 conv with same padding with the filter -1 and then do an addition... i.e.\r\n\r\n`foo - bar `\r\nis equivalent to \r\n`foo + bar* -1`\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "sub is available now."]}, {"number": 15092, "title": "min_quantize lib and command line", "body": "a quantize/obfuscate lib\r\n1.without Quantize/DeQuantize ops\r\n2.could obfuscate node names\r\n3.use KMeans instead of simple average slice", "comments": ["Can one of the admins verify this patch?", "@suharshs Review this PR please?", "@caisq done", "Apologies for the long delay!\r\n\r\nThank you for this large contribution!\r\nCan this contribution be released as an external (non-TF) repository rather than being added to contrib? In particular, I don't know of any existing hardware backends that will leverage KMeans quantization's speed up. Additionally, there don't seem to be many dependencies on TF core that make it seem that this would be better off as being a nice, modular, decoupled package.\r\n\r\nWhat do you think? Thanks!", "Hello @ghost, we appreciate your contribution. As @suharshs suggested, please add this as an external repo. I will close this PR."]}, {"number": 15091, "title": "GatherNd InvalidArgumentError: flat indices[8, :] = [8, -1] does not index into param", "body": "### System information\r\n\r\n- **OS Platform and Distribution**: Ubuntu 17.10 (Linux-4.13.0-17-generic-x86_64-with-debian-stretch-sid)\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.3.0-rc2-20-g0787eee 1.3.0 (same on 1.4.0)\r\n- **Python version**: 3.6.3\r\n- **GPU model and memory**: N/A (CPU only)\r\n- **Numpy version:** 1.13.3\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **C++ compiler version**: (Ubuntu 7.2.0-8ubuntu3) 7.2.0\r\n- **Have I written custom code**: Yes\r\n- **Exact command to reproduce**: There is no single command. The error arises when trying to fetch a TensorFlow tensor. Please see the description below.\r\n\r\n### Describe the problem\r\n\r\nI am training a recurrent neural network (RNN) whereby I give it variable-length sequences of random steps in two dimensions and train it to recognize the quadrant in which the random walker ended up. This RNN works fine on a Windows machine with GPU (all other specs are otherwise the same as above). However, on a Linux machine with CPU only, I get an error which mysteriously traces back to a dimensionality hiccup with `GatherNd`. \r\n\r\nThe code for the full RNN is too convoluted to post, but as you can see from below, I am printing out the fetches to two tensors `tf_weights` and `tf_last` at each iteration of the batching. (In this case the training data is consumed one batch at a time for a total of 28 batches). The batches are very basic loops so you'd think that if a fetch works in one iteration of the loop it should also work for the next. This is indeed the case for `tf_weights` but not for `tf_last` which, for no apparent reason, fails to evaluate at batch 7/28. `tf_last` can be traced to a `GatherNd` operation.\r\n\r\nI thoroughly inspected the data and it looks fine. _Please keep in mind that my code does work under Windows with GPU with all other specs remaining the same, so it's hard to conceive of a bug from within the code itself._\r\n\r\n### Source code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n\r\n    tf_features = tf.placeholder(\r\n            tf.float32, [batch_size, max_seq_len, input_dim], \r\n            name = 'tf_features')\r\n\r\n    tf_targets = tf.placeholder(\r\n            tf.float32, [batch_size, target_len],\r\n            name = 'tf_targets')\r\n\r\n    tf_seq_len = tf.placeholder(\r\n            tf.int32, [batch_size],\r\n            name = 'tf_seq_len')\r\n\r\n    tf_cell = 'tf.contrib.rnn.'+cell_type+'('+str(num_hidden)+')'\r\n    tf_cell = eval(tf_cell)\r\n\r\n    tf_output, tf_state = tf.nn.dynamic_rnn(\r\n            tf_cell, tf_features, sequence_length = tf_seq_len, \r\n            dtype = tf.float32)\r\n  \r\n    tf_output = tf_output[:, :, :num_hidden]\r\n           \r\n    tf_last = tf.gather_nd(\r\n            tf_output, \r\n            tf.stack([tf.range(batch_size), tf_seq_len-1], axis = 1), # batch_size\r\n            name = 'tf_last')\r\n```\r\n\r\n### Logs\r\n\r\n```\r\n---------------- RUN 0/0\r\n ---------------- FOLD 0/2\r\n  ---------------- TRAIN\r\n  Epoch 0\r\n  - Optimizing optimize\r\n   ---------------- BATCH 0/28 [0, 50], len_data = 1499\r\n   *** tf_weights: -0.563565\r\n   *** tf_last: -0.192192\r\n   ---------------- BATCH 1/28 [50, 100], len_data = 1499\r\n   *** tf_weights: -0.553565\r\n   *** tf_last: -0.0799878\r\n   ---------------- BATCH 2/28 [100, 150], len_data = 1499\r\n   *** tf_weights: -0.546777\r\n   *** tf_last: -0.118384\r\n   ---------------- BATCH 3/28 [150, 200], len_data = 1499\r\n   *** tf_weights: -0.538511\r\n   *** tf_last: -0.142531\r\n   ---------------- BATCH 4/28 [200, 250], len_data = 1499\r\n   *** tf_weights: -0.529593\r\n   *** tf_last: -0.147268\r\n   ---------------- BATCH 5/28 [250, 300], len_data = 1499\r\n   *** tf_weights: -0.520294\r\n   *** tf_last: -0.014847\r\n   ---------------- BATCH 6/28 [300, 350], len_data = 1499\r\n   *** tf_weights: -0.510754\r\n   *** tf_last: -0.094138\r\n   ---------------- BATCH 7/28 [350, 400], len_data = 1499\r\n   *** tf_weights: -0.504503\r\n   *** Failed to evaluate tf_last\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-25-49e8720a879a>\", line 1, in <module>\r\n    runfile('/home/ala/Python/domains/main.py', wdir='/home/ala/Python/domains')\r\n\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 710, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/ala/Python/domains/main.py\", line 166, in <module>\r\n    metrics_valid = RNN_object.validate(data_train, KFolds)\r\n\r\n  File \"/home/ala/Python/domains/classifiers.py\", line 727, in validate\r\n    reinitialize = True))\r\n\r\n  File \"/home/ala/Python/domains/classifiers.py\", line 366, in train\r\n    self.optimize(data)\r\n\r\n  File \"/home/ala/Python/domains/classifiers.py\", line 1102, in optimize\r\n    optimized_driving_metric = self.evaluate(data, driving_metric)\r\n\r\n  File \"/home/ala/Python/domains/classifiers.py\", line 1796, in evaluate\r\n    evaluated_tf_var = self.sess.run(eval(tf_var), feed_dict)\r\n\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\nInvalidArgumentError: flat indices[8, :] = [8, -1] does not index into param (shape: [50,300,2]).\r\n\t [[Node: tf_last = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_2, stack)]]\r\n\r\nCaused by op 'tf_last', defined at:\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/ipython/start_kernel.py\", line 245, in <module>\r\n    main()\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/ipython/start_kernel.py\", line 241, in main\r\n    kernel.start()\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-25-49e8720a879a>\", line 1, in <module>\r\n    runfile('/home/ala/Python/domains/main.py', wdir='/home/ala/Python/domains')\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 710, in runfile\r\n    execfile(filename, namespace)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"/home/ala/Python/domains/main.py\", line 109, in <module>\r\n    RNN_object.define(data_test)\r\n  File \"/home/ala/Python/domains/classifiers.py\", line 1514, in define\r\n    name = 'tf_last')\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1338, in gather_nd\r\n    name=name)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ala/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): flat indices[8, :] = [8, -1] does not index into param (shape: [50,300,2]).\r\n\t [[Node: tf_last = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](strided_slice_2, stack)]]\r\n```", "comments": ["/CC @ebrevdo", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version", "Looks like you have a tf_seq_len of 0 at the failing site?  negative indices are not supported by gather_nd; and i'm not sure you'd want them anyway.", "It works on GPU because we do not perform index validation checking on GPU -- it's too expensive.  Instead we fill the output with zero for the corresponding value.  The CPU version caught the bad index and returned the appropriate error.", "@ebrevdo Yes indeed. Zero-length sequences turned out to be the culprits. I wonder how that lead to negative indices, though. I'm also intrigued at how the lack of validation on the GPU made the execution of the fetches so deceivingly seamless.", "The cause was here:\r\n\r\n```python\r\ntf.stack([tf.range(batch_size), tf_seq_len-1], axis = 1)\r\n```\r\n\r\nnote your `tf_seq_len-1`, which is `-1` if `tf_seq_len` is zero.", "@ebrevdo @laghaout is the issue resolved and can it be closed?", "Yup.\n\nOn Fri, Dec 8, 2017 at 12:12 PM, Reed <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> @laghaout\n> <https://github.com/laghaout> is the issue resolved and can it be closed?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15091#issuecomment-350360595>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_PyTFVc2JwSlnMPKeFchiY-WYY-ks5s-ZgTgaJpZM4Q0Xrb>\n> .\n>\n", "Thank you all for this discussion. This matched my exact scenario as well. Worked on Window GPU TF but failed on Linux CPU TF build. I found some None type objects from my sequencing algorithms in my input array (an array representing ASCII characters). A sample of one of my failing array vectors looked like:\r\n\r\n[34 51 90 26 9 31 40 72 78 77 46 92 19 **None** 73 45 **None** 93 14 16]"]}, {"number": 15090, "title": "RNNDROP?", "body": "Has anyone implemented RNNDROP (https://www.stat.berkeley.edu/~tsmoon/files/Conference/asru2015.pdf) in tensorflow?\r\nAccording to the paper this method show good performances in speech recognition tasks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Sorry for the very late reply, but I donnt think questions above are relevant...\r\nI still wonder has anyone implemented RNNDROP?\r\nThanks.  ", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Questions like these are more likely to get a response on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) which has better visibility. If you feel strongly that this functionality should be available in TensorFlow, please respond and I'll reopen the issue as contributions welcome."]}, {"number": 15089, "title": "Error when run label_image --graph=/tmp/quantized_graph.pb after quantized model ", "body": "tensorflow:1.4.0\r\nI followed the command line from tensorflow/tensorflow/docs_src/performance/quantization.md \r\nran the following command is ok and  produce a new model quantized_graph.pb:\r\n  bazel build tensorflow/tools/graph_transforms:transform_graph\r\n  bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\r\n  --out_graph=/tmp/quantized_graph.pb \\\r\n  --inputs=input \\\r\n  --outputs=InceptionV3/Predictions/Reshape_1 \\\r\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\r\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\r\n    strip_unused_nodes sort_by_execution_order'\r\n\r\nBut there is a problem when  I was running with command:\r\n  bazel build tensorflow/examples/label_image:label_image\r\n  bazel-bin/tensorflow/examples/label_image/label_image \\\r\n  --graph=/tmp/quantized_graph.pb    (Else: it worked if I replace quantized_graph.pb with inception_v3_2016_08_28_frozen.pb here.)\r\nThe error info is:\r\n  2017-12-04 08:27:02.891427: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op:    \"DenseToSparseBatchDataset\" device_type: \"CPU\"') for unknown op: DenseToSparseBatchDataset\r\n2017-12-04 08:27:02.891520: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"GroupByWindowDataset\" device_type: \"CPU\"') for unknown op: GroupByWindowDataset\r\n2017-12-04 08:27:02.891562: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"IgnoreErrorsDataset\" device_type: \"CPU\"') for unknown op: IgnoreErrorsDataset\r\n2017-12-04 08:27:02.891636: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"DatasetToSingleElement\" device_type: \"CPU\"') for unknown op: DatasetToSingleElement\r\n2017-12-04 08:27:02.891686: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"SerializeIterator\" device_type: \"CPU\"') for unknown op: SerializeIterator\r\n2017-12-04 08:27:02.891712: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"DeserializeIterator\" device_type: \"CPU\"') for unknown op: DeserializeIterator\r\n2017-12-04 08:27:02.891733: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"MapAndBatchDataset\" device_type: \"CPU\"') for unknown op: MapAndBatchDataset\r\n2017-12-04 08:27:02.891777: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"ParallelInterleaveDataset\" device_type: \"CPU\"') for unknown op: ParallelInterleaveDataset\r\n2017-12-04 08:27:02.891805: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"ScanDataset\" device_type: \"CPU\"') for unknown op: ScanDataset\r\n2017-12-04 08:27:02.891825: E tensorflow/core/framework/op_kernel.cc:1142] OpKernel ('op: \"SqlDataset\" device_type: \"CPU\"') for unknown op: SqlDataset\r\n2017-12-04 08:27:02.901109: E tensorflow/examples/label_image/main.cc:327] Invalid argument: Node 'InceptionV3/InceptionV3/Conv2d_1a_3x3/BatchNorm/batchnorm/mul_eightbit/input__port__0/reduction_dims': Unknown input node '^input:0'\r\nhow should I fix it?\r\n\r\n", "comments": ["--inputs=input  you need know input param name on graph", "I  am  unfamiliar with tensorflow quantize so that followed the command line and used default params ,what should I add for default --inputs's param ?There are errors when I changed --input='input:1'(sorry, I don't know what should I input param name, do you know default input param name?) , Could you give me a exmple (it can using  the default params) which is working well ?\r\nThanks for your help!", "/CC @petewarden, can you take a look?", "@petewarden I had been modified inception_v3_2016_08_28_frozen.pb  to quantized_graph.pb in tensorflow/examples/label_image/main.cc and label_image.py, then tried to follow command line from https://github.com/petewarden/tensorflow/tree/master/tensorflow/examples/label_image which commands are \"bazel build tensorflow/examples/label_image/...\" and \"bazel-bin/tensorflow/examples/label_image/label_image\"\uff0cThe same problem appears", "@boshanyiqiao   tensorboard  --logdir=tensorflow/examples/label_image/data  view the graph", "@arixlin \r\nerror info \u201cNo dashboards are active for the current data set.\u201d by web page view\r\n$ls tensorflow/examples/label_image/data    (defualt file)\r\ngrace_hopper.jpg          inception_v3_2016_08_28_frozen.pb\r\nimagenet_slim_labels.txt  inception_v3_2016_08_28_frozen.pb.tar.gz\r\n\r\nIs it and above problem because there is no input file? I use default command and file now, what should I do to fix it?\r\n", "@boshanyiqiao You need step by step, build  inception_v3_2016_08_28_frozen.pb  before bazel build quantized_graph.pb. Here is model pb link (https://github.com/tensorflow/models/blob/master/research/slim/README.md)   forzen.pb include checkpoint, can't view on tensorboard. ", "@arixlin thank you for your replay. But there is download  inception_v3_2016_08_28_frozen.pb  command, \"curl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |tar -C tensorflow/examples/label_image/data -xz\" in tensorflow/tensorflow/docs_src/performance/quantization.md. I had been download  inception_v3_2016_08_28_frozen.pb before bazel build quantized_graph.pb\r\n", "@boshanyiqiao \r\nYou try\r\nhttps://github.com/tensorflow/models/blob/master/research/slim/README.md\r\npython export_inference_graph.py \\\r\n  --alsologtostderr \\\r\n  --model_name=inception_v3 \\\r\n  --output_file=/tmp/inception_v3_inf_graph.pb\r\nview the inception_v3_inf_graph on tensorboard ,\r\nand then\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=/tmp/inception_v3_inf_graph.pb \\\r\n  --input_checkpoint=/tmp/checkpoints/inception_v3.ckpt \\\r\n  --input_binary=true --output_graph=/tmp/frozen_inception_v3.pb \\\r\n  --output_node_names=InceptionV3/Predictions/Reshape_1\r\n", "I met the same problem with @boshanyiqiao. but when I checkout to branch r1.4 and recompile the transform_graph binary, it worked properly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 15088, "title": "Make a State-ful LSTM with data input from TF Dataset", "body": "#14906 \r\nThis is to reference a closed issue.\r\nThanks for @mrry suggestion on using Dataset.flat_map to slice the input signal sequence.\r\nHowever, how could we know the beginning and ending of the original sequence after sliced?\r\nAnd how could we reset the LSTM state so that we can make a state-ful LSTM?\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "You should be able to use the existing [`tf.nn.static_state_saving_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/static_state_saving_rnn) function with a `tf.data` input pipeline and a [`tf.contrib.training.SequenceQueuingStateSaver`](https://www.tensorflow.org/api_docs/python/tf/contrib/training/SequenceQueueingStateSaver). Note that the latter relies on queues and queue-runners, so you'll still need to call `tf.train.start_queue_runners()`.", "@mrry Thank you very much! I am looking into it and will come back and update once succeed.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Hopefully you succeeded! I'm closing this issue now, but feel free to follow up on Stack Overflow if you have other usage questions."]}, {"number": 15087, "title": "Understanding LSTM cell Kernel values", "body": "Hi all,\r\n\r\nI want to understand better those values in LSTM cell Kernel values extracted from:\r\n<tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(97, 280) dtype=float32_ref>\r\nfrom tf.trainable_variables()\r\n\r\nMy model is very simple, input is a 27 element vector at each time step where the sequence length can be variable. The model is one layer LSTM model with 70 hidden states. \r\n```\r\n        def lstm_cell():\r\n            cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)\r\n            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropKeepRate)\r\n            return cell\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple=True)\r\n        \r\n```\r\n\r\nSo that is probably why the LSTM kernel is a matrix of 97 rows (27 input features and 70 hidden states). As to the columns, I can clearly see four groups by the pattern of values. I guess each group is consist of 70 columns, so totally 4 groups.\r\n\r\nSo my guess is that this LSTM kernel, extracted from tf.trainable_variables(), maps input and previous hidden states to current hidden states in order of forget, input, update cell states and output. However, it is hard time for me to figure out the correspondence of these gates and their matrices with the Kernel matrix extracted from tf.trainable_variables().\r\n\r\nCould somebody help? Thanks!\r\n\r\n\r\n---------------------\r\nUpdate:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\nTensorFlow installed from (source or binary): from pip\r\nTensorFlow version (use command below): 1.3\r\nPython version: 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 8.0, 6.1\r\nGPU model and memory: k2200", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\nTensorFlow installed from (source or binary): from pip\r\nTensorFlow version (use command below): 1.3\r\nPython version: 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 8.0, 6.1\r\nGPU model and memory: k2200", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Any links to where this question ended up on SO??? Yes, please google it for me, I couldn\u2019t find the goods "]}, {"number": 15086, "title": "ctc_loss value problem", "body": "------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 64bit\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nI used the source code below to calculate ctc loss. The logit and target should extremely have no loss between them. But the tf.nn.ctc_loss return 1.91309595. What's the meaning of the tf ctc_loss result? \r\nBut I use my custom ctc loss function, which shows the loss is nearly zero.\r\n\r\n### Source code / logs\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nlogit = np.array([[[0.00001, 0.9999, 0.00001, 0.00001], [0.9999, 0.00001, 0.00001, 0.00001], [0.00001, 0.00001, 0.9999, 0.00001]]])\r\nlogit = tf.constant(logit, dtype='float32')\r\ntarget = tf.SparseTensor(indices=[[0, 0], [0, 2]], values=[1, 2], dense_shape=[1, 3])\r\nseq = tf.constant(np.array([3]))\r\nloss_tf = tf.nn.ctc_loss(target, logit, seq, time_major=False, ctc_merge_repeated=True)\r\nsess = tf.InteractiveSession()\r\ntf.global_variables_initializer().run()\r\nprint(loss_tf.eval())\r\n", "comments": ["I have encountered the same issue, the built-in tensorflow CTC API cannot pass this simple test from Alex Graves paper.\r\n\r\nhttps://gist.github.com/marcoleewow/d7dc5078a36927b3f13dbf64ac9905ad ", "@xushenkun I have figured out the problem, the CTC loss performs softmax operation inside, so you should feed in the activations, not the probabilities after softmax.", "@xushenkun I have now used non-softmax tensor to feed into `ctc_loss` function, but I still cannot achieve 0 loss with your example. I think this is a tensorflow bug, please double check my code [here](https://gist.github.com/marcoleewow/037fc43733260300083aab0db2d7d9a8).\r\n\r\nupdate:\r\nAfter I swapped this line \r\n`target = tf.SparseTensor(indices=[[0, 0], [0, 2]], values=[1, 2], dense_shape=[1, 3])`\r\n\r\nwith\r\n`target = tf.SparseTensor(indices=[[0, 0], [0, 1], [0, 2]], values=[1, 0, 2], dense_shape=[1, 3])`\r\n\r\nI get 0 loss! ", "@marcoleewow Thanks for your remind, I forgot the softmax issue. And I think although these two kinds of targets mean same dense tensor value, tf'ctc_loss use the sparse tensor directly, which are different. "]}, {"number": 15085, "title": "Add an argument for additional linkopts to py_wrappers rule", "body": "This will help a repository that uses tensorflow as a submoudle to use existing tensorflow bazel rules in tensorflow.bzl", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15084, "title": "calculation gradients of tf.nn.embedding_lookup", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**: 2.7\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\ntypes_lookup_table = tf.get_variable(\"types_lookup_table\", shape=[234, 10],\r\n                                     initializer=tf.random_normal_initializer(0, 1), dtype=tf.float32,\r\n                                     trainable=True)\r\nembedding_types = tf.nn.embedding_lookup(types_lookup_table,[[2,3,4],[1,2,3]])\r\nopt = tf.train.GradientDescentOptimizer(0.1)\r\ngradients = tf.gradients(embedding_types, xs=types_lookup_table)\r\ntrain = opt.apply_gradients([(gradients[0], types_lookup_table)])\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    h = sess.run(gradients)\r\n    print(sess.run(train))                                                                              #right\r\n    print(sess.run(opt.apply_gradients([(h[0],types_lookup_table)]))).    # wrong\r\n\r\n### Describe the problem\r\nI tried to calculate the gradients of tf.nn.embedding_lookup, but the result shown is an IndexedSliceValue with 3 elements\r\n<img width=\"1250\" alt=\"2017-12-04 9 23 45\" src=\"https://user-images.githubusercontent.com/10001692/33532464-ec59088e-d8d4-11e7-9c08-d53c6d87abf5.png\">\r\nhowever the corresponding gradient(without sess.run) is an indexSliceValue with 1 elements.I don't know why.\r\n<img width=\"1226\" alt=\"2017-12-04 9 27 31\" src=\"https://user-images.githubusercontent.com/10001692/33532533-6aa0a5bc-d8d5-11e7-9b0d-69a950cb5fb0.png\">\r\n\r\nAnd therefore I can't sess.run(opt.apply_gradients([(h[0],types_lookup_table)]) because the shape of calculation value doesn't match the shape of types_lookup_table, however, when I didn't calculate the intermediate value, and directly \r\nsess.run(train) (ps:train = opt.apply_gradients([(gradients,types_lookup_table)]))\r\nthere is no problem.\r\n\r\nBut I need to calculate the intermediate value and do an add. I don't know how.\r\nThanks\r\n\r\n", "comments": ["Sorry, I cannot figure out what you want.\r\n\r\nIn your code,\r\n```python\r\ngradients = tf.gradients(embedding_types, xs=types_lookup_table)\r\nh = sess.run(gradients)\r\nprint(sess.run(opt.apply_gradients([(h[0],types_lookup_table)]))). # wrong\r\n```\r\nSince `types_lookup_table`'s dimension is 234 * 10, the corresponding `graidents` is a 234 * 10 tensor, as same as `h`. \r\nSo `h[0]` is 1 * 10, while `types_lookup_table` is 234 * 10, it seems wrong to apply `h[0]` to `types_lookup_table` because of the unmatched dimensions.\r\n\r\nPlease correct me if I'm wrong.\r\n", "yeah, I though h can be applied to calculate gradients directly.  I mean apply_gradients(h, types_lookup_table). But the size of h is different from gradients in h = sess.run(gradients), however, it seems that h is composed of three elements, and it needs some change to get the final gradients that can be applied to apply_gradients.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15083, "title": "DOC: Fix documentation for dataset.md", "body": "The code `image = tf.decode_jpeg(parsed[\"image_data\"])` in 738 lines is incorrect. It should be `tf.image.decode_jpeg` instead of `tf.decode_jpeg`.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "cc @mrry FYI", "@tensorflow-jenkins test this please"]}, {"number": 15082, "title": "getting attribute error", "body": "I executed the code and got this error-- \r\nFile \"new1.py\", line 131, in main\r\n    classifier = tf.estimator.Estimator(model_fn=model_fn)\r\nAttributeError: 'module' object has no attribute 'estimator'\r\n\r\npls help\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "same error.\r\nmacbook pro 2016(early), tf1.4", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Meet same problem when running https://www.tensorflow.org/versions/master/tutorials/recurrent_quickdraw#defining_the_model", "update your tensorflow and issue will be resolved as tf.estimator was added in tensorflow 1.3"]}, {"number": 15081, "title": "Update deprecated get_global_step in example", "body": "`WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/client/random_forest.py:193: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_global_step`\r\nthrown when running random_forest_mnist.py\r\n\r\nupdated to `training_util.get_global_step()` used elsewhere in the same file", "comments": ["Can one of the admins verify this patch?", "Done, sorry for the delay!", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 15080, "title": "Add S3 to the list of implemented file systems in doc", "body": "This fix adds S3 to the list of implemented file systems in doc.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Ignored unrelated failure."]}, {"number": 15079, "title": "could softmax_cross_entropy_with_logits's  label  has many 1.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Mac 10.13.1\r\n- **TensorFlow installed from (source or binary)**:pip\r\n- **TensorFlow version (use command below)**:v1.3.0-rc1-2456-g7abd587 1.4.0-dev20170922\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 0.0])\r\n# b = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 0.0])  \r\nb = np.array([0.0, 1.0, 1.0, 0.0, 1.0, 0.0])\r\nsess = tf.Session()\r\nprint(str(sess.run(tf.nn.softmax_cross_entropy_with_logits(labels=b, logits=a))))\r\n```\r\nI think it maybe should throw a warn. Because the softmax_cross_entropy_with_logits's labels should not accept the label which has many 1.\r\n\r\nIf we should fix it. could you let me try to fix it ?I want to try, thanks.\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I think in most cases only one class is expected for classification problem, however multiple labels are valid for label problem. Perhaps it could be better to support the multiple-ones case if we could.", "@facaiy  but it should't be np.array([0.0, 1.0, 1.0, 0.0, 1.0, 0.0]) . When we need express multiple labels. the b is np.array([0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0])?", "I mean that multiple ones can exits in `label`, not `logit`. However, I agree that `softmax` is designed for classification problem. As for multiple labels, people prefer to using `sigmoid`, https://stats.stackexchange.com/questions/207794/what-loss-function-for-multi-class-multi-label-classification-tasks-in-neural-n\r\n\r\nHence your proposal sounds reasonable for me: precheck label and throw a warning (or exception) that multiple ones exist.", "Oh, I checked the API of [` tf.nn.softmax_cross_entropy_with_logits `](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits). \r\n> NOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\r\n>\r\n> If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.\r\n\r\nIn all,\r\n+ ` tf.nn.softmax_cross_entropy_with_logits`\uff1a `label` must be valid probability distribution;\r\n+ ` tf.nn.sparse_softmax_cross_entropy_with_logits`: `label` must be class id.\r\n\r\nHence, if necessary, we should check that label must be nonnegative and their sum equals one.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nlogits = np.array([[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]])\r\nlabels = np.array([[0.0, 1.0, 1.0, 0.0, 1.0, 0.0]])    # invalid\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\nwith tf.Session() as sess:\r\n    print(sess.run(loss))\r\n    # [ 5.13077533]\r\n```", "This issue is mentioned in a TODO [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1765).\r\n\r\n/CC @ebrevdo what do you think of this proposal? I don't think we can cause an error due to backwards compatibility, and we typically do not print warnings due to incorrect arguments. Also there would be a performance cost.", "The documentation clearly states that the input should be a probability distribution.  It seems fine to add the check for the CPU version, without worrying about backwards compatibility guarantees (though we'd want to run internal tests before accepting the change).  However, on the GPU this check is sufficiently expensive that we can't implement it without a lot of trickery and, probably, some slowdown.  So my suggestion is:\r\n\r\nAdd the check for the CPU impl, run all unit tests; then we run all internal unit tests and see if there are basic models that rely on broken behavior.", "(the check should raise `errors::InvalidArgument` if the input is not sufficiently close to a probability distribution.  probably you can't check exact, but you can check the sum is w/in e.g. \r\n`10 * std::numeric_limits<T>::epsilon()` of 1.", "Hi, @ebrevdo . I checked the implementation of `SoftmaxCrossEntropyWithLogits`: kernel functions in `xent_op` seems totally shared by both CPU and GPU. So I'm afraid that only checking in CPU impl is impossible.", "The cpu kernel can run `if (std::is_same<Device, CPUDevice>::value) {\n<perform check> }`.\n\nOn Mon, Dec 4, 2017 at 8:37 PM, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> Hi, @ebrevdo <https://github.com/ebrevdo> . I checked the implementation\n> of SoftmaxCrossEntropyWithLogits: kernel functions in xent_op seems\n> totally shared by both CPU and GPU. So I'm afraid that only checking in CPU\n> impl is impossible.\n>\n> Is it good to check in python side, for example, using tf.reduce_sum?\n> Take account of performance, perhaps we'd better make it optional.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15079#issuecomment-349193079>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9CIO5enuzQ3RkpMlAb1AATlTWKnks5s9Mh9gaJpZM4Qzwxp>\n> .\n>\n", "Thanks, @ebrevdo .\r\nHi, @bringtree, contributions are welcome.", "Could I add a arg to judge to open label's check?\uff08Default:close the check\uff09 I think maybe some users don't want to check the sum. @ebrevdo ", "I guess maybe it will have a problem when we use the tf.nn.sampled_softmax_loss. \r\nI can't be sure that  tf.nn.sampled_softmax_loss will choose the labels can be add up to 1 in future.\r\nThough tf.nn.sampled_softmax_loss often is used to the word2vec, and In word2vec the tf.nn.sampled_softmax_loss's label sum will be 1.   ", "Sorry, I don't know how to correct it. Could you help me to correct it?  I want to know how you modify the codes?@facaiy \r\n`math_ops.equal(math_ops.reduce_sum.reducesum(labels),constant_op.constant(1.0,dtype=precise_logits.dtype))`\r\n", "@bringtree You can refer to my PR #15245 and `BinaryGradCommon` method in `cc/gradients/math_grad.cc`. Moreover, [C++ API | TensorFlow](https://www.tensorflow.org/api_guides/cc/guide) might be useful for you.", "Closing as this is resolved\r\n\r\n\r\n"]}, {"number": 15078, "title": "DOC: Fix documentation for inverse_time_decay.", "body": "Fix the example in the documentation of `inverse_time_decay` and correct the pseudo-code formula.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15077, "title": "The sequence of session.run and control_dependencies?", "body": "See three examples:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(1.0)\r\nx = tf.Print(x, ['x'])\r\ny = tf.constant(2.0)\r\ny = tf.Print(y, ['y'])\r\nz = tf.constant(3.0)\r\nz = tf.Print(z, ['z'])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run([x,y,z]))\r\n```\r\n\r\nThe sequence of output of `tf.Print` is indeterminate, which means `sess.run` don't executes tensors from left to right.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(1.0)\r\nx = tf.Print(x, ['x'])\r\ny = tf.constant(2.0)\r\ny = tf.Print(y, ['y'])\r\nwith tf.control_dependencies([x, y]):\r\n    z = tf.constant(3.0)\r\n    z = tf.Print(z, ['z'])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(z))\r\n```\r\n\r\nThe sequence  of `x` and `y` is indeterminate, which means `control_dependencies` don't executes tensors from left to right.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(1.0)\r\nx = tf.Print(x, ['x'])\r\ny = tf.constant(2.0)\r\ny = tf.Print(y, ['y'])\r\nd = tf.constant(3.0)\r\nd = tf.Print(d, ['d'])\r\nwith tf.control_dependencies([x, y]):\r\n    z = tf.add(d, 3.)\r\n    z = tf.Print(z, ['z'])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(z))\r\n```\r\n\r\nThe sequence of `x`, `y` and `d` is indeterminate.\r\n\r\nIs it intentional behavior or bug?  I find that if the sequence is indeterminate, the program may get different result.\r\n\r\n\r\nAnother example I can't explain:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32, [])\r\ny = tf.Variable(2.)\r\nop = tf.assign(y, x)\r\nop = tf.Print(op, ['op'])\r\n\r\nwith tf.control_dependencies([op]):\r\n    q = tf.Print(y, [y])\r\n\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    print(sess.run([q], feed_dict={x: -1.0}))\r\n```\r\n\r\nThe `[op]` is printed before `[y]`, but `q=2.0`, why? If `op` is executed before `y`, y will be assigned by `x`, which means `y=-1.0`, and `q` should be `-1.0`.\r\n\r\n", "comments": ["1. Intentional behavior.\r\n2. Duplicate of your own issue. #14498", "@ppwwyyxx I don't think it is duplicate of [#14498](https://github.com/tensorflow/tensorflow/issues/14498),\r\nCould you give me an explaination of my last example?", "I'll just quote again #4663 (comment) that tf.Variable is semi-broken (sometimes wrong execution order of var.assign and var.read_value) and ResourceVariable is better.", "The only guarantee tensorflow makes about order of execution is that all dependencies (either data or control) of an op are executed before that op gets executed.\r\n\r\nThe memory-aliasing behavior of tf.Variable is very hard to understand (which is why I recommend using tfe.Variable instead which has a well-defined memory model). So just switch your code to that and then the behavior of variables will make more sense.\r\n\r\nWhat is happening here is a complicated interaction between variables and devices and executors and such. \r\n\r\nBecause you have two devices here, a GPU and a CPU, the generated graph looks something like the following:\r\n\r\n```\r\nwith tf.device(\"gpu:0\"):\r\n  x = tf.placeholder(tf.float32, [])\r\n  y = tf.Variable(2.)\r\n  __y = tf._send(y, \"__y\")\r\n  __y2 = tf._send(y, \"__y2\")\r\n  op = tf.assign(y, x)\r\n  with tf.control_dependencies([op]):\r\n    tf._send(_no_op, \"_op\")\r\n\r\nwith tf.device(\"cpu:0\"):\r\n  op = tf._receive(\"_op\")\r\n  op = tf.Print(op, ['op'])\r\n  y = tf._receive(\"__y\")\r\n  with tf.control_dependencies([op]):\r\n      q = tf.Print(y, [y])\r\n\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    print(sess.run([q], feed_dict={x: -1.0}))\r\n```\r\n\r\nAnd then a valid execution order runs the sends for the variable value before it runs the assign op.", "Closing because this is intended behavior.", "Because the print op doesn't have a GPU implementation\n\nOn Dec 4, 2017 20:16, \"gauss-clb\" <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Could you tell why op = tf.Print(op,\n> ['op']) and q = tf.Print(y, [y]) are defined on cpu:0 ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15077#issuecomment-349190083>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxf-l3Zrej9_1h89wWIZtxmp3A2oPks5s9MOFgaJpZM4QzulH>\n> .\n>\n", "@ppwwyyxx What is resource variable?\r\n@alextp Is `tfe.Variable` available only in eager mode?", "@netheril96 You can think of it as a better implementation of `tf.Variable`.\r\n`tfe.Variable` is an alias of resource variable. You can use it in graph mode or eager mode.", "tf.Variable is so widely used that I don't believe we should use tfe.variable. Eager mode is just a small part of tensorflow and this approcach is not decent.\r\n", "tfe.Variable is a drop-in replacement for tf.Variable which has nothing to\ndo with eager mode and will become the default eventually\n\nOn Thu, Apr 26, 2018 at 1:16 AM Jackie Loong <notifications@github.com>\nwrote:\n\n> tf.Variable is so widely used that I don't believe we should use\n> tfe.variable. Eager mode is just a small part of tensorflow and this\n> approcach is not decent.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15077#issuecomment-384553122>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeV_P6b5gzw82B81ms_jFSm8WyBFks5tsYJogaJpZM4QzulH>\n> .\n>\n\n\n-- \n - Alex\n", "oh. really. I had thought it's for eager mode."]}, {"number": 15076, "title": "the source code to compile  about tensorflow1.1.0", "body": "I am very distressed~~~\r\nI need some help ~~~~\r\n\r\nI refer to \"TensorFlow Bindings for H2O.ai\"\r\nI execute \" ./gradlew clean tensorflowCompile\",but always has error,likes\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz, https://github.com/bazelbuild/rules_closure/archive/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz] to /root/.cache/bazel/_bazel_root/c0282f68c3c9fe7828209fa3ece89ea6/external/io_bazel_rules_closure/5ca1dab6df9ad02050f7ba4e816407f88690cf7d.tar.gz: Checksum was 5afc2087ab53b160fb58fde30339a2c2826c1a171404b7b8ff7227d5ebc8225c but wanted 60fc6977908f999b23ca65698c2bb70213403824a84f7904310b6000d78be9ce. :deepwater-tensorflow:tensorflowCompile FAILED\r\nerrors with checksum\r\nhow can I do ,fix this problem\r\nhow can I checksum disabled\r\n\r\nplease ~~~~\r\nthanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Please try again with a later version of TF.\r\nThis was due to github changing the compression method in their archives, and we did not go back to 1.1 to also fix this. 1.4 branch should work OK.", "thank you very much,I have resolved this issue,it's a network problem", "thank you very much,I have resolved this issue,is't a network problem\r\n\r\n\r\nziteng\r\n\r\n\r\n________________________________\r\n\u53d1\u4ef6\u4eba: Gunhan Gulsoy <notifications@github.com>\r\n\u53d1\u9001\u65f6\u95f4: 2017\u5e7412\u670814\u65e5 0:31:19\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow\r\n\u6284\u9001: zitengwm; Author\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] the source code to compile about tensorflow1.1.0 (#15076)\r\n\r\n\r\nPlease try again with a later version of TF.\r\nThis was due to github changing the compression method in their archives, and we did not go back to 1.1 to also fix this. 1.4 branch should work OK.\r\n\r\n\u2015\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/15076#issuecomment-351641717>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AgkfWQRpcInBXrdKQdf_p5zo9a5JT6yeks5tANzWgaJpZM4QzsKw>.\r\n"]}, {"number": 15074, "title": "When execute \"bash tensorflow/contrib/lite/build_ios_universal_lib.sh\", it gives the error: tensorflow/tensorflow/contrib/lite/ios_makefile.inc:2: *** missing separator.  Stop.", "body": "### System information\r\n- **Install problem**:\r\n- **Darwin MacBook-Pro-2.local 16.7.0 Darwin Kernel Version 16.7.0, xnu-3789.71.6~1/RELEASE_X86_64 x86_64**:\r\n- **TensorFlow installed from source**:\r\n- **v1.0.0-rc2-15-g47bba63-dirty 1.0.0**:\r\n- **Python 3.6.0 :: Anaconda custom (x86_64)**: \r\n- **Bazel version (None)**:\r\n- **Apple LLVM version 9.0.0 (clang-900.0.37)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n", "comments": ["I took a look at the contents of ios_makefile.inc, it's completely a mess. Hope tflite will fix this issue soon.", "I have corrected the file  and done it right locally.", "@voidguy Same problem here. How to fix it?", "@voidguy ok, i saw the solution: https://github.com/tensorflow/tensorflow/pull/15117 ", "This appears to be fixed in #15191. Thanks for the report."]}, {"number": 15073, "title": "when execute \"bash tensorflow/contrib/lite/build_ios_universal_lib.sh\", it has the following error: tensorflow/tensorflow/contrib/lite/ios_makefile.inc:2: *** missing separator.  Stop.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 15072, "title": "Have to reinstall numpy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nBefore I installed tensorflow, I have installed numpy through anaconda. But after I run the command provided in the website (https://www.tensorflow.org/install/install_windows), it automatically reinstalled numpy. Then when I import numpy or tensorflow, I got an import error \"cannot import name 'add_newdocs'\" which is required when importing numpy. I have to reinstall my numpy through anaconda to solve this problem. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@mrry any idea?", "No. Googling that error message (\"cannot import name 'add_newdocs'\") suggests it's a frequent problem when installing NumPy on Windows, so you might find something by searching NumPy forums.", "Closing this issue due to staleness. Please use the latest version of TensorFlow and build again.\r\nFeel free to report any issues you encounter with latest TensorFlow. Thanks!"]}, {"number": 15071, "title": "Tensorflow build fails with --config=sycl", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 17.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.3\r\nPython version: 2.7\r\nBazel version (if compiling from source): 0.5.1\r\nGCC/Compiler version (if compiling from source): 6.0.3\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:\r\n\r\n> bazel build -c opt --config=sycl //tensorflow:libtensorflow_cc.so\r\n\r\n**Logs**\r\n\r\n> \r\n> ERROR: /home/ashok/Ashok/tensorflow-c++/tensorflow/core/kernels/BUILD:3355:1: C++ compilation of rule '//tensorflow/core/kernels:sendrecv_ops' failed: computecpp failed: error executing command external/local_config_sycl/crosstool/computecpp -fPIE -fno-omit-frame-pointer -Wall -msse3 -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF ... (remaining 119 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1\r\n> In file included from tensorflow/core/kernels/sendrecv_ops.cc:16:\r\n> In file included from ./tensorflow/core/kernels/sendrecv_ops.h:19:\r\n> In file included from ./tensorflow/core/framework/op_kernel.h:19:\r\n> In file included from /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:55:\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:1404:14: error: no matching constructor for initialization of 'tuple<const tensorflow::Status &&, const tensorflow::Rendezvous::Args &&, const tensorflow::Rendezvous::Args &&, const tensorflow::Tensor &&, bool &&>'\r\n>     { return tuple<_Elements&&...>(std::forward<_Elements>(__args)...); }\r\n>              ^                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:992:13: note: in instantiation of function template specialization 'std::forward_as_tuple<const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>' requested here\r\n>               std::forward_as_tuple(std::forward<_Args>(__args)...),\r\n>                    ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:1731:2: note: in instantiation of function template specialization 'std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)>::operator()<const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool, void>' requested here\r\n>         (*_Base::_M_get_pointer(__functor))(\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/functional:2115:33: note: in instantiation of member function 'std::_Function_handler<void (const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool), std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)> >::_M_invoke' requested here\r\n>             _M_invoker = &_My_handler::_M_invoke;\r\n>                                        ^\r\n> tensorflow/core/kernels/sendrecv_ops.cc:154:38: note: in instantiation of function template specialization 'std::function<void (const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool)>::function<std::_Bind<(lambda at tensorflow/core/kernels/sendrecv_ops.cc:155:7) (std::function<void ()>, std::_Placeholder<1>, std::_Placeholder<2>, std::_Placeholder<3>, std::_Placeholder<4>, std::_Placeholder<5>)>, void, void>' requested here\r\n>   Rendezvous::DoneCallback done_cb = std::bind(\r\n>                                      ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:600:18: note: candidate template ignored: disabled by 'enable_if' [with _Dummy = void]\r\n>                  _TCC<_Dummy>::template\r\n>                  ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:611:18: note: candidate template ignored: disabled by 'enable_if' [with _Dummy = void]\r\n>                  _TCC<_Dummy>::template\r\n>                  ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:628:5: note: candidate template ignored: disabled by 'enable_if' [with _UElements = <const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]\r\n>                   _TC<sizeof...(_UElements) == 1, _Elements...>::template\r\n>                   ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:641:5: note: candidate template ignored: disabled by 'enable_if' [with _UElements = <const tensorflow::Status &, const tensorflow::Rendezvous::Args &, const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]\r\n>                   _TC<sizeof...(_UElements) == 1, _Elements...>::template\r\n>                   ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:737:19: note: candidate template ignored: disabled by 'enable_if' [with _Alloc = tensorflow::Rendezvous::Args, _UElements = <const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]\r\n>         enable_if<_TMC<_UElements...>::template\r\n>                   ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:748:19: note: candidate template ignored: disabled by 'enable_if' [with _Alloc = tensorflow::Rendezvous::Args, _UElements = <const tensorflow::Rendezvous::Args &, const tensorflow::Tensor &, bool>]\r\n>         enable_if<_TMC<_UElements...>::template\r\n>                   ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:579:17: note: candidate constructor template not viable: requires 0 arguments, but 5 were provided\r\n>       constexpr tuple()\r\n>                 ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:589:26: note: candidate constructor template not viable: requires 0 arguments, but 5 were provided\r\n>       explicit constexpr tuple()\r\n>                          ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:670:19: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided\r\n>         constexpr tuple(const tuple<_UElements...>& __in)\r\n>                   ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:682:28: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided\r\n>         explicit constexpr tuple(const tuple<_UElements...>& __in)\r\n>                            ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:694:19: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided\r\n>         constexpr tuple(tuple<_UElements...>&& __in)\r\n>                   ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:705:28: note: candidate constructor template not viable: requires single argument '__in', but 5 arguments were provided\r\n>         explicit constexpr tuple(tuple<_UElements...>&& __in)\r\n>                            ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:721:2: note: candidate constructor template not viable: requires 7 arguments, but 5 were provided\r\n>         tuple(allocator_arg_t __tag, const _Alloc& __a,\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:732:11: note: candidate constructor template not viable: requires 7 arguments, but 5 were provided\r\n>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,\r\n>                  ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:711:2: note: candidate constructor template not viable: requires 2 arguments, but 5 were provided\r\n>         tuple(allocator_arg_t __tag, const _Alloc& __a)\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:759:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided\r\n>         tuple(allocator_arg_t __tag, const _Alloc& __a, const tuple& __in)\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:763:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided\r\n>         tuple(allocator_arg_t __tag, const _Alloc& __a, tuple&& __in)\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:772:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided\r\n>         tuple(allocator_arg_t __tag, const _Alloc& __a,\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:784:11: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided\r\n>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,\r\n>                  ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:796:2: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided\r\n>         tuple(allocator_arg_t __tag, const _Alloc& __a,\r\n>         ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:808:11: note: candidate constructor template not viable: requires 3 arguments, but 5 were provided\r\n>         explicit tuple(allocator_arg_t __tag, const _Alloc& __a,\r\n>                  ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:654:17: note: candidate constructor not viable: requires 1 argument, but 5 were provided\r\n>       constexpr tuple(tuple&&) = default; \r\n>                 ^\r\n> /usr/lib/gcc/x86_64-linux-gnu/6.3.0/../../../../include/c++/6.3.0/tuple:652:17: note: candidate constructor not viable: requires 1 argument, but 5 were provided\r\n>       constexpr tuple(const tuple&) = default;\r\n>                 ^\r\n> 1 error generated.\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 1003.058s, Critical Path: 53.74s\r\n> FAILED: Build did NOT complete successfully\r\n", "comments": ["**Actual Requirement** : To cross-build Tensorflow using --config=sycl for arm 32 bit system\r\n\r\n**Things Done**\r\n1. Able to cross-build Tensorflow without --config=sycl for arm 32 bit system using below command\r\n\r\n`bazel build --crosstool_top=//armv7l-compiler:toolchain --cpu=armeabi-v7a tensorflow:libtensorflow_cc.so --cxxopt=\"-std=c++11\"`\r\n\r\n**Result** : Built Fine without errors\r\n\r\n2 cross-build Tensorflow without --config=sycl for arm 32 bit system using below command\r\n\r\n`bazel build -c opt --copt=\"-mfpu=neon\" --crosstool_top=//armv7l-compiler:toolchain --cpu=armeabi-v7a tensorflow:libtensorflow_cc.so --cxxopt=\"-std=c++11\" --config=sycl\r\n`\r\n**Result**\r\nBuild Failed at Last moment while trying to link libComputeCpp.so\r\n\r\n\r\nI have downloaded the sdk from codeplaysoftware website for Linux16.04 and arm64 bit. Can I know if there is 32 bit version\r\n\r\nI basically wanted to improve the speed of TensorFlow on embedded 32 bit arm device by using opencl or sycl. Could you also please suggest any other optimization flags to be used while building.\r\n\r\nThanks", "/CC @benoitsteiner can you take a look?", "OpenCL support is not official. Please reach out to CodePlay with the issue.\r\nHowever, looking at their platform support notes, I do not see ubuntu 17, and I suspect that may be your issue."]}, {"number": 15070, "title": "Multil-model restore in tensorflow", "body": "I have two model without scope name but they have same variable name, I want load them simultaneously, but it will crash and throw error, which is \"Variable XXX already exists\".\r\nI notice the issue[https://github.com/tensorflow/tensorflow/issues/3270](url), but the problem is solved by re-training the model again and save them under different scope.\r\nHowever, I do not want to re-train them again. So how can I load existing models and save them into two different scope?\r\nAny advise or help will be appreciated!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "reuse=True", "@arixlin I have tried setting reuse=True, but another error occurs.\r\nThe two models have both same and different named variables, so if set reuse=True, there are some variables does not pre-defined, and it seems that the followed variables will override the afore-restored variables.\r\nNow I just want to add prefix for all variables within a model.\r\nFor examples, there are exist\r\n'conv1/weight' in A model\r\nand\r\n'conv1/weight' in B model,\r\nI want to add prefix \"model_A\" and \"model_B\" for all variables in A and B separately\r\nThe previous variables now should be:\r\n'model_A/conv1/weight' in model A\r\nand\r\n'model_B/conv1/weight' in model B", "@CodesFarmer  load all variables to memory\uff0cI think it\u2019s feasible", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15069, "title": "DOC: underline that tf.Print behaves like tf.identity", "body": "As @alextp suggested in #14788, fix the docstring: print should have the same behavior as identity (and it does)\r\n\r\nThe pr is opposed to #15068 .", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15068, "title": "tf.Print supports Variable", "body": "CF #14788, #14874\r\n\r\nI agree that it might be not a good idea to pass Variable as `input_` for `tf.Print`, and the implementation might be incomplete, incorrect or even potentially dangerous.  So feel free to close the pr.\r\n\r\nAs we knwon, `tf.Print` is consistent with `tf.identity`, which always return a Tensor with the same type. However, the pr changes `tf.Print`'s behavior as:\r\n+ return Tensor for Tensor.\r\n+ return mutable Tensor for mutable Tensor.\r\n+ return Variable for Variable.\r\n\r\nThe pr is opposed to #15069.", "comments": ["Can one of the admins verify this patch?"]}]