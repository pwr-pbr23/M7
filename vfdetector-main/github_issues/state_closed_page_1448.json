[{"number": 9519, "title": "Runs regex filter doesn't work in tensorboard v1.1", "body": "### System information\r\n- Custom code, worked fine on 1.0\r\n- OS: Linux Ubuntu 16.04\r\n- installed binary via pip3\r\n- TensorFlow version v1.1.0-rc0-61-g1ec6ed5, 1.1.0\r\n- CUDA v8.0, cuDNN v5.1\r\n- GTX 1070, 8GB RAM\r\n- To reproduce: run tensorboard, try to filter runs in web interface, nothing happens\r\n\r\n### Problem description\r\nRunning tensorboard with v1.1 gives me the following warnings in the console (repeated four times) once the web interface is opened:\r\n\r\n`WARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404`\r\n\r\nAt first I ignored it, but it turns out that when examining the runs in the web interface, the regex filter for the runs doesn't work at all. This exact command in the exact same folder with the exact same logs worked without issue with v1.0.\r\n", "comments": ["This warning was also referenced in #9382.", "@dandelionmane, could you place take a look. Thanks!", "I'm seeing the same warnings and some major issues in viewing scalar data on tensorboard. I'm currently viewing tensorboard for 6 different training sessions all with different run id's. If I hide and show runs using the checkboxes the data will change for old runs that are no longer in progress. Also, some runs will show their data sometimes and others won't. \r\n\r\nSystem information\r\n\r\nOS: Linux Ubuntu 16.04\r\ninstalled binary via pip3\r\nTensorFlow GPU version v1.1.0\r\nCUDA v8.0, cuDNN v5.1\r\nTitan X\r\n", "Just upgraded from 1.0.1 to 1.1 and got the same issue.\r\n\r\nSame setup as @rightaditya, except for GTX 1080.\r\n\r\nNote, TensorBoard does seem to have problems finding the address (still available from localhost tho). Letting you know in case this issue is somehow connected with the warnings.\r\n\r\n```\r\nStarting TensorBoard b'47' at http://0.0.0.0:6007\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\n```", "I got the same issue when I use the Google Cloud Platform with tensorflow version  1.10\r\nI'm not sure, but the first time I use tensorboard, it works without the issue, and after that, tensor board cannot work properly at all.\r\n\r\n`Starting TensorBoard 47 at http://0.0.0.0:8080\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404`", "BTW, duplicate of #9285.", "Thanks for the report. Both of these issues are fixed at master, so they won't be present in the next release.", "@dandelionmane Can you mention the commit so it can be cherry-picked?", "hey, is this issue is fixed on master ? I have installed from pip, it is ok from local, but when i tried it with my domain and access from internet, i got same error", "hey, any feedback ?", "I have the same problem. runs regex works fine at localhost, but not when running on another host.", "Please pip install the latest tensorboard via `pip install tensorflow-tensorboard`. If it's still happening, open an issue at https://github.com/tensorflow/tensorboard with precise reproduction steps"]}, {"number": 9518, "title": "GPU version of self_adjoint_eig", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux RBSylaptop 4.9.0-1-amd64 #1 SMP Debian 4.9.6-3 (2017-01-28) x86_64 GNU/Linux\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n$ pip3 install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.1.0\r\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\n- **GPU model and memory**:\r\nFound device 0 with properties: \r\nname: GeForce GTX 1070\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.645\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.31GiB\r\n\r\n### Describe the problem\r\nIt looks like there is no eigen vector kernel that would run on GPU. Even the CPU version seems to be serial as it uses only one core for a single matrix.\r\n\r\n### Source code / logs\r\nThis code just create a random 10*10 matrix and try to compute its eigen values and vectors on the GPU.\r\n\r\n    mat = np.random.random((10, 10))\r\n    sess = tf.Session()\r\n    with tf.device('/gpu:0'):\r\n        eigen = tf.self_adjoint_eig(mat)\r\n    sess.run(eigen)\r\n\r\nWhich fails with the error:\r\n\r\n    InvalidArgumentError (see above for traceback): Cannot assign a device to node 'SelfAdjointEigV2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n         [[Node: SelfAdjointEigV2 = SelfAdjointEigV2[T=DT_DOUBLE, compute_v=true, _device=\"/device:GPU:0\"](SelfAdjointEigV2/input)]]\r\n\r\n\r\nAnd for a large enough matrix, it can be seen that the CPU kernel only uses one core.", "comments": ["@rmlarsen Comments?  Both parallel and GPU versions of this code sound tricky.", "For a speed reference, MKL (through scipy) does 4096x4096 SVD in 5.6 seconds on Xeon V4\r\nUnexpectedly, symmetric eigenvalue version is considerably slower than full SVD https://github.com/scipy/scipy/issues/7339\r\n\r\nFor a large enough matrix, it's going to be compute bound, so doing decomposition outside of TensorFlow and transferring using `feed_dict` would be not much slower than using native op", "It turns out I was actually looking into tensorflow because scipy was too slow. I don't know how you got your SVD in 5.6 seconds for a 4096x4096 matrix with scipy, but I got 232.7 seconds, and still only one core used.\r\n\r\nAnyway, unless there is a way to turn a SVD to eigenvectors, this wouldn't be of much help. (I don't much about this, though.)", "@Celelibi you need to make sure you are using MKL and not OpenBLAS. 5.6 seconds was for full SVD and it was peaking at 20 cores. Because of a feature of python installers, every time you install tensorflow, it'll overwrite fast MKL scipy with slow OpenBLAS scipy so you will need to restore MKL (conda install scipy).\r\n\r\nBTW, for a symmetric matrix, SVD gives eigenvectors and eigenvalues. It's more numerically stable than Eigendecomposition. For instance, applying to covariance based on 500 examples, I got all eigenvalues past 500 exactly zero using SVD in float64. Using spectral decomposition gives me numerical noise on the order of 1e-15\r\n\r\nTo check if you have MKL, make sure this talks about MKL and not OpenBLAS\r\n`python -c 'import numpy; numpy.__config__.show()'`", "Between my previous message and this one, I pip installed `pyamg`, which upgraded `scipy` and `numpy`. This upgrade gave a huge overall performance boost to scipy. Now SVD runs in 40.56 seconds and use all 8 cores. It now runs on OpenBLAS. I guess that before that, I was using the system-wide installation of numpy and scipy which I think uses CBLAS.\r\n\r\nYou're right, SVD produce the eigenvectors they were just not in the same order as `eigh` or `self_adjoint_eig`.", "Suppose I have a covariance matrix.\r\n\r\na = cov(a)\r\n\r\nI want to calculate eigenvectors of a. How can I do that? Thanks in advanced.", "cc @rmlarsen -- btw, here's how MXnet implements symmetric eigendecomposition on GPU -- [Auto-Differentiating Linear Algebra](https://arxiv.org/abs/1710.08717)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Is that still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Although a GPU version of the op was added in e3413de529c3f762885efd62932f76445ed22653, I am seeing that a slightly modified version of @Celelibi's test case randomly segfaults.\r\n\r\nHere is my modified test case:\r\n\r\n```py\r\nimport numpy as np\r\nimport tensorflow as tf\r\nrs = np.random.RandomState(2)\r\nN = 4\r\nmat = rs.uniform(-10.0, 10.0, size = (N, N))\r\n# https://stackoverflow.com/a/27331415/\r\nmat = np.tril(mat) + np.tril(mat, -1).T\r\nprint mat\r\nsess = tf.Session()\r\nwith tf.device('/gpu:0'):\r\n    eigen = tf.self_adjoint_eig(mat)\r\nprint sess.run(eigen)\r\nprint np.linalg.eig(mat)\r\n```\r\n\r\nOut of 5 tries, 4 segfaulted.\r\n\r\nHere is a backtrace:\r\n\r\n<pre>\r\nThread 29 Crashed:\r\n0   libtensorflow_framework.so    \t0x0000000102b39c10 void tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>::emplace_back<tensorflow::EventMgr::InUse const&>(tensorflow::EventMgr::InUse const&&&) + 176\r\n1   libtensorflow_framework.so    \t0x0000000102b3902c tensorflow::EventMgr::PollEvents(bool, tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>*) + 412\r\n2   libtensorflow_framework.so    \t0x0000000102ad4022 tensorflow::EventMgr::ThenExecute(perftools::gputools::Stream*, std::__1::function<void ()>) + 194\r\n3   libtensorflow_framework.so    \t0x0000000102ad4837 tensorflow::GPUUtil::CopyGPUTensorToCPU(tensorflow::Device*, tensorflow::DeviceContext const*, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 791\r\n4   libtensorflow_framework.so    \t0x0000000102ad6ac9 tensorflow::GPUDeviceContext::CopyDeviceTensorToCPU(tensorflow::Tensor const*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 121\r\n5   libtensorflow_framework.so    \t0x0000000102ae9397 tensorflow::(anonymous namespace)::CopyDeviceToHost(tensorflow::Tensor const*, tensorflow::Allocator*, tensorflow::Allocator*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, tensorflow::DeviceContext*, std::__1::function<void (tensorflow::Status const&)>) + 471\r\n6   libtensorflow_framework.so    \t0x0000000102ae8ac1 tensorflow::CopyTensor::ViaDMA(tensorflow::StringPiece, tensorflow::DeviceContext*, tensorflow::DeviceContext*, tensorflow::Device*, tensorflow::Device*, tensorflow::AllocatorAttributes, tensorflow::AllocatorAttributes, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 3441\r\n7   libtensorflow_framework.so    \t0x0000000102b22dbe tensorflow::IntraProcessRendezvous::SameWorkerRecvDone(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 1102\r\n8   libtensorflow_framework.so    \t0x0000000102b2385d std::__1::__function::__func<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0, std::__1::allocator<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0>, void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>::operator()(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&) + 813\r\n9   libtensorflow_framework.so    \t0x000000010262db36 tensorflow::LocalRendezvousImpl::Send(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) + 550\r\n10  libtensorflow_framework.so    \t0x0000000102b228aa tensorflow::IntraProcessRendezvous::Send(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) + 298\r\n11  _pywrap_tensorflow_internal.so\t0x0000000106611622 tensorflow::SendOp::Compute(tensorflow::OpKernelContext*) + 738\r\n12  libtensorflow_framework.so    \t0x0000000102ac9f34 tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 1284\r\n13  libtensorflow_framework.so    \t0x0000000102af83c9 tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4793\r\n14  libtensorflow_framework.so    \t0x0000000102af8ab5 std::__1::__function::__func<tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::$_1, std::__1::allocator<tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::$_1>, void ()>::operator()() + 37\r\n15  libtensorflow_framework.so    \t0x000000010273bdff Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 2047\r\n16  libtensorflow_framework.so    \t0x000000010273b4ff std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n17  libtensorflow_framework.so    \t0x00000001027608a0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n18  libsystem_pthread.dylib       \t0x00007fff7bdab6c1 _pthread_body + 340\r\n19  libsystem_pthread.dylib       \t0x00007fff7bdab56d _pthread_start + 377\r\n20  libsystem_pthread.dylib       \t0x00007fff7bdaac5d thread_start + 13\r\n</pre>", "Also, the GPU implementation uses cuSOLVER to compute the eigendecomposition (in particular, the [cusolverDn&lt;t&gt;syevd()](http://docs.nvidia.com/cuda/cusolver/index.html#cuds-lt-t-gt-syevd) family of routines).\r\n\r\nI have read and personally experienced that not all cuSOLVER routines have better performance than CPU counterparts. For example, the cusolverDn&lt;t&gt;gesvd() family of routines were not performance-competitive with MKL, but existed mainly for platforms where MKL might not be available, and for interface compatibility: https://devtalk.nvidia.com/default/topic/830894/cusolverdncgesvd-performance-vs-mkl/\r\n\r\nDoes anybody know if the cusolverDn&lt;t&gt;syevd() family of routines have competitive performance?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I should mention that I am observing the SEGFAULT issue with a macOS+GPU build. As macOS+GPU isn't officially supported, the SEGFAULT issue might not be a problem with officially-supported build configurations.\r\n\r\nCan someone try out the modified test case on, say, Linux+GPU, and see if there is a SEGFAULT issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Still seeing SEGFAULT on macOS+GPU build: \r\n```\r\nThread 20 Crashed:\r\n0   libtensorflow_framework.so    \t0x0000000110e3b090 void tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>::emplace_back<tensorflow::EventMgr::InUse const&>(tensorflow::EventMgr::InUse const&&&) + 176\r\n1   libtensorflow_framework.so    \t0x0000000110e3a43c tensorflow::EventMgr::PollEvents(bool, tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>*) + 300\r\n2   libtensorflow_framework.so    \t0x0000000110dbf252 tensorflow::EventMgr::ThenExecute(perftools::gputools::Stream*, std::__1::function<void ()>) + 194\r\n3   libtensorflow_framework.so    \t0x0000000110dbfa18 tensorflow::GPUUtil::CopyGPUTensorToCPU(tensorflow::Device*, tensorflow::DeviceContext const*, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 712\r\n4   libtensorflow_framework.so    \t0x0000000110dc1cd9 tensorflow::GPUDeviceContext::CopyDeviceTensorToCPU(tensorflow::Tensor const*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 121\r\n5   libtensorflow_framework.so    \t0x0000000110de4437 tensorflow::(anonymous namespace)::CopyDeviceToHost(tensorflow::Tensor const*, tensorflow::Allocator*, tensorflow::Allocator*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, tensorflow::DeviceContext*, std::__1::function<void (tensorflow::Status const&)>) + 471\r\n6   libtensorflow_framework.so    \t0x0000000110de3a9b tensorflow::CopyTensor::ViaDMA(tensorflow::StringPiece, tensorflow::DeviceContext*, tensorflow::DeviceContext*, tensorflow::Device*, tensorflow::Device*, tensorflow::AllocatorAttributes, tensorflow::AllocatorAttributes, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 3227\r\n7   libtensorflow_framework.so    \t0x0000000110e200e2 tensorflow::IntraProcessRendezvous::SameWorkerRecvDone(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 1074\r\n8   libtensorflow_framework.so    \t0x0000000110e20b8d std::__1::__function::__func<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0, std::__1::allocator<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0>, void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>::operator()(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&) + 813\r\n9   libtensorflow_framework.so    \t0x0000000110c9eb4c tensorflow::LocalRendezvousImpl::Send(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) + 380\r\n10  libtensorflow_framework.so    \t0x0000000110e1fb52 tensorflow::IntraProcessRendezvous::Send(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) + 162\r\n11  _pywrap_tensorflow_internal.so\t0x000000011625ebfa tensorflow::SendOp::Compute(tensorflow::OpKernelContext*) + 794\r\n12  libtensorflow_framework.so    \t0x0000000110db44a7 tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 1287\r\n13  libtensorflow_framework.so    \t0x0000000110df3dbd tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 5181\r\n14  libtensorflow_framework.so    \t0x0000000110df43b5 std::__1::__function::__func<tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::$_1, std::__1::allocator<tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::$_1>, void ()>::operator()() + 37\r\n15  libtensorflow_framework.so    \t0x0000000110e3fd5f Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 2047\r\n16  libtensorflow_framework.so    \t0x0000000110e3f45f std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n17  libtensorflow_framework.so    \t0x0000000110e66000 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n18  libsystem_pthread.dylib       \t0x00007fff7ff696c1 _pthread_body + 340\r\n19  libsystem_pthread.dylib       \t0x00007fff7ff6956d _pthread_start + 377\r\n20  libsystem_pthread.dylib       \t0x00007fff7ff68c5d thread_start + 13\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have three comments above with new information that has not been addressed. Can the issue be re-opened?", "Is the problem still happening with the latest version?", "Thank you for reopening. I am in the process of rebuilding TensorFlow on my dev machine. I will let you know.", "I am encountering tensorflow/tensorflow#9072:\r\n\r\n```\r\nStarting local Bazel server and connecting to it...\r\n.............\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'\r\nINFO: Elapsed time: 5.690s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n```\r\n\r\nTrying to find a solution...", "Unfortunately, I have not been able to resolve the error. I have opened a new issue: #19676.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Okay. I managed to build TensorFlow at 86e11d29dc76a09ff8bbc0b426ca78c98946bf23 with some minor local changes. Now, the [modified test case that I posted earlier](https://github.com/tensorflow/tensorflow/issues/9518#issuecomment-366854122) consistently segfaults within CopyGPUTensorToCPU(). This segfault issue might only affect macOS+GPU, as this is not officially supported. Maybe someone can test Linux+GPU or Windows+GPU?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 9517, "title": "Placing Variables on the cpu using `tf.contrib.layers` functions", "body": "Dear tensorflow team,\r\n\r\nAfter constructing my model using the functionality provided by `tf.contrib.layers` I now want to extend my model over several GPUs. I learned that it might be beneficary to place Variables on the CPU when doing that, to reduce data transfer overhead. After not seeing an easy way to do this I found a workaround I described on [stackoverflow](http://stackoverflow.com/questions/43678599/device-placement-of-kernels-and-biases-when-using-tf-contrib-layers/43685023#43685023). My solution is to generate Variable-nodes in the graph where the Variable-getter of the `fully_connected` layer for example would expect the variables to be.\r\n\r\nAs this is not a very nice solution, I messed with the `fully_connected` layer and the `_build_variable_getter` function to basically allow me to specify, where I want to place the variabels. Thus after \r\n\r\n- adding the kwarg `variable_device` to `tf.contrib.layer.fully_connected`\r\n- adding the kwarg `variable_device` to `tf.contrib.layer._build_variable_getter`\r\n- adding the kwarg `device` to `tf.contrib.layer._model_variable_getter`\r\n- and passing this as kwarg to `model_variable` defined in `tensorflow.contrib.framework.python.ops.variables`\r\n\r\nI get the desired functionality when using the `fully_connected` layer.\r\nBelow you find the modified version of `layers.py` \r\n\r\nIn my eyes this would be a very useful feature for all layers that contain trainable variables, which is why I would like to make a request for this feature.\r\nIf you think the supplied modification is good enough, I can also try to make a pull request, after updating the other layers.\r\n\r\nBest, Johannes\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: As described above, I did\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: ('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n- **CUDA/cuDNN version**: 8, 5.1\r\n- **GPU model and memory**: Titan X Pascal, 12GB\r\n\r\n### Source code / logs\r\nSee \r\n- [stackoverflow](http://stackoverflow.com/questions/43678599/device-placement-of-kernels-and-biases-when-using-tf-contrib-layers/43685023#43685023)\r\n- [modified layers.py](https://drive.google.com/file/d/0BxImZIuERGB2MHBMRDZkejM4LVU/view?usp=sharing)", "comments": ["@tfboyd, didn't you run into this as well? @martinwicke, do you have any api design thoughts on this?", "I meant to ask for this.  @martinwicke and I spoke about it but I never formally asked.  This would be a good add to be able to state where you want the variables to go.  Just this type of change should be a 10-12% speedup using the current stock slim models as tested with resnet-50 a couple months ago.  Putting the variables on cpu has been optimal for many models, e.g. inception and resnet, regardless of GPU.  It even works great for Tesla P100s on systems with high speed GPU links.  Putting shared variables on GPU:0 has only been effective in a limited number of situations and architectures that I have tested, e.g. AlexNet on AWS K80s, and VGG16 again only on AWS K80s.  ", "The canonical way to do this is to use a device function.\n\nOr variable_scope, but I think we should encourage device functions.\n\nOn Apr 28, 2017 11:42 AM, \"Toby Boyd\" <notifications@github.com> wrote:\n\n> I meant to ask for this. @martinwicke <https://github.com/martinwicke>\n> and I spoke about it but I never formally asked. This would be a good add\n> to be able to state where you want the variables to go. Just this type of\n> change should be a 10-12% speedup using the current stock slim models as\n> tested with resnet-50 a couple months ago. Putting the variables on cpu has\n> been optimal for many models, e.g. inception and resnet, regardless of GPU.\n> It even works great for Tesla P100s on systems with high speed GPU links.\n> Putting shared variables on GPU:0 has only been effective in a limited\n> number of situations and architectures that I have tested, e.g. AlexNet on\n> AWS K80s, and VGG16 again only on AWS K80s.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9517#issuecomment-298075668>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_QKBKJbFK2dLm_8dbplne01d42sqks5r0jMZgaJpZM4NLxX1>\n> .\n>\n", "Could you elaborate on this @martinwicke? Or guide me to the documentation? \r\nI understand how to place variables using context managers, but in this case I would like the Variable nodes to be created by the layer function.\r\n\r\nE.g. it would be desirable for me to be able do something like this:\r\n```\r\nwith tf.device('\\gpu:0):\r\n    fc = fully_connected(input, ..., variable_device='\\cpu:0')\r\n```\r\nHere the variables would be created on the CPU while the matrix multiplication and bias add are done on the GPU.\r\n\r\nFrom what I found reading through and experimenting with the code, this functionality is basically there, one only would need to add a kwarg to the layer and pass it to the variable getter, that creates the variables. This is what I did above.", "Hey everybody,\r\n\r\nAny news on this topic? \r\nI would be happy to supply a pull request, but I am not sure, my extensions of the code meet possible guidelines. Also I am not sure if my testing was sufficient. If you want you can have a look at [my fork of the tensorflow repository](https://github.com/jhaux/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py), where the changes can be found in all layers and getters, where I thought the `variable_device` keyword might make sense, e.g. where variables are created or fetched. Those include:\r\n\r\n- `bias_add`\r\n- `convolution`\r\n- `convolution2d_in_plane`\r\n- `convolution2d_transpose`\r\n- `_model_variable_getter`\r\n- `_build_variable_getter`\r\n- `fully_connected`\r\n- `separable_convolution2d`\r\n\r\nThe changes should be backward compatible as only kwargs with default values = `None` are used.\r\nI also added Documentation in the docstring:\r\n`variable_device: Device where the variables of the layer (e.g. weights and biases) should be placed, e.g. /cpu:0`\r\n\r\nI think this approach is more intuitive than using a `variable_scope` with a `custom getter`.", "@tfboyd you were working on a device function for this exact purpose, can you post it here?", "Any news on this issue? It would definitely be a useful feature!", "I will try to post example code tonight. It is not super complicated.  For image models (only thing I work with right now) CPU vs. GPU for the parameters can depend on the model and situation.  I will add a little more detail when I post the code sample.", "Here we go.  I have actually changed my mind and agree with Martin.  Although I am not an authority as you will see from my sloppy python.  Using this with the SLIM models can boost performance on systems where GPUs are not peered or Resnet which seems to often just work better with CPU:0 as the ps-server.  This is just how I do it and I am very open to criticism and a better approach.\r\n\r\n```python\r\ntf.app.flags.DEFINE_string('local_ps_device', 'CPU:0', \"\"\"Local parameter server GPU:0 if gpus are peered or CPU:0 otherwise try both.\"\"\")\r\n\r\nPS_OPS = [\r\n      'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\r\n      'MutableHashTableOfTensors', 'MutableDenseHashTable'\r\n]\r\n\r\ndef assign_to_device(device, ps_device=None):\r\n    \"\"\"Returns a function to place variables on the ps_device.\r\n\r\n    Args:\r\n        device: Device for everything but variables\r\n        ps_device: Device to put the variables on.  Example values are GPU:0 and\r\n        CPU:0.\r\n    \r\n    If ps_device is not set then the variables will be placed on the device.\r\n    The best device for shared varibles depends on the platform as well as the\r\n    model.  Start with CPU:0 and then test GPU:0 to see if there is an \r\n    improvement.  \r\n\r\n    \"\"\"\r\n    def _assign(op):\r\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\r\n        if node_def.op in PS_OPS:\r\n            return \"/\" + ps_device\r\n        else:\r\n            return device\r\n    return _assign\r\n\r\ndef train():\r\n       \"\"\" Incomplete example of a train method just to show the use of assign_to_device\r\n       \r\n       Note: The is from a ResNet and AlexNet training CIFAR-10 example on Multi-GPU. \r\n       I suspect there is some code \"smell\" but I was working with something that already \r\n       existed and the purpose is to illustrate the general usage.  Feedback welcome.  \r\n       \"\"\"\r\n        tower_grads = []\r\n        reuse_variables = None\r\n        losses = []\r\n        for i in six.moves.range(FLAGS.num_gpus):\r\n             with tf.device(assign_to_device(\r\n                            '/gpu:{}'.format(i),\r\n                            ps_device=FLAGS.local_ps_device)):\r\n                with tf.name_scope('{}_{}'.format('TOWER', i)) as n_scope:\r\n                    with tf.device('/cpu:0'):\r\n                        images, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)\r\n                    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\r\n                        logits = inference_small(images, is_training=True, num_blocks=9)\r\n                    tower_loss = loss(logits, labels)\r\n                    losses.append(tower_loss)\r\n                    grads = optimizer.compute_gradients(tower_loss)\r\n                    tower_grads.append(grads)\r\n                    reuse_variables = True\r\n....\r\n\r\n```\r\n\r\n", "I found that if the GPUs are not peered then CPU:0 is the often the best option.  If the GPUs are peered then GPU:0 was the best option for VGG16 and Alexnet.  InceptionV3 and Resnet were better on CPU:0 and even faster with replicated on GPU without NCCL.  For example: InceptionV3 was 214 imgs/sec on CPU vs. 218 images/sec using replicated with 8 GPUs batch-size 32 per GPU).  Maybe I am conservative but I wouldn't mess with replicated for an extra 4 images/second.  But if you are interested check out the [document](https://www.tensorflow.org/performance/performance_models) and sample code.  Replicated makes a much bigger difference for image models with more parameters (like VGG16) when using Tesla P100s.  CPU:0 is still the best choice (as of the time I wrote this message) for InceptionV3 and ResNet on P100s and replicated using NCCL for VGG16 and AlexNet.  \r\n\r\nLet me know if this function closes out your request and we will close the issue.  I want to wait for your response before closing.  I expect there is may be some more valuable discussion / feedback.", "It's not exactly the same but I've found that using SLIM one can simply do:\r\n```\r\nwith tf.device('/gpu:%d' % gpu):\r\n    with tf.name_scope('tower_%d' % gpu) as scope:\r\n        with slim.arg_scope([slim.model_variable, slim.variable], device='/cpu:0'):\r\n            net = slim.fully_connected(...)\r\n```\r\n\r\nWhich will pin the variables on `/cpu:0` while maintaining the actual computations on `/gpu:x`.\r\n\r\nAnd don't forget to set `colocate_gradients_with_ops=True` when computing the gradients.", "I learned and noticed something new today.  In the tf_cnn_benchmark code when we place the variables on the GPUs we spread them out even in the basic parameter server mode.  Here is a [link to the code](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/variable_mgr.py#L81) (long term the line number may change but for now the link goes to the correct function). This will get rolled into a much nicer example and I hope utility methods in the next 60 days or so.  \r\n\r\nPersonally, I am curious of the gain.  I hope to give this a test on a model I know works better with GPU as a parameter server and test GPU:0 vs. this method.  I hope this helps.  There are so many ways to approach these problems and variables that determine which is the best choice for each situation.  \r\n", "```\r\ndef net(x):\r\n\tres = tf.contrib.layers.conv2d(x, 10, [3, 3])\r\n\t...\r\n\treturn res\r\n\r\nwith tf.device('/CPU:0'):\r\n\twith tf.name_scope('CPU'):\r\n\t\tnet(x)\r\n\r\nfor i in range(num_gpus):\r\n\twith tf.device('/GPU:%d' % i):\r\n\t\twith tf.name_scope('GPU_%d' % i):\r\n\t\t\twith tf.variable_scope(tf.get_variable_scope(), reuse=True):\r\n\t\t\t\ty = net(x)\r\n\t\t\t\tloss = ...\r\n\t\t\t\tgrad = ...\r\n```\r\nIt works!\r\n[http://jzdss.club/2018/03/04/Multi-GPUs/](http://jzdss.club/2018/03/04/Multi-GPUs/)", "@JZDSS Thanks! I believe this is a simple practical implementation :thumbsup:"]}, {"number": 9516, "title": "Tensor flow cannot be imported properly in windows", "body": "I have installed CPU version of Tensor flow in my laptop using the command line using pip install.\r\n\r\nOS: Windows 8.1 64 bit Python 3.5.1\r\n\r\nBut, when I tried importing it in the python 3.5.1 it throws the following error message.\r\n\r\n> import _pywrap_tensorflow ImportError: No module named '_pywrap_tensorflow' Error importing tensorflow. Unless you are using bazel, you should not try to import tensorflow from its source directory; please exit the tensorflow source tree, and relaunch your python interpreter from there.\r\n\r\n I have tried getting answers in Stackoverflow, but no hope. I have even tried changing the directories but nothing works.\r\nI am new to python. Please help me to solve this issue !!", "comments": ["you can try install tensorflow 1.0.1.", "@harishjmhss thank you for reporting your issue. This error can derive from a couple of reasons and for better help you solve the problem please fill out all the information on the Issue template (OS, Python distribution, the entire log and steps you used to install TensorFlow etc). Without that it's hard to assess and debug what is happening. \r\nFor now make sure you have `MSVCP140.DLL` installed in your system. You can do that by downloading the [Microsoft Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/details.aspx?id=53587)\r\nThanks!", "@Carmezim  I am using Windows 8.1 - 64 bit - Python 3.5.1 distribution downloaded from python.Org\r\n \r\nI have installed using the command\r\n\r\n> pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl\r\n\r\n\r\nI just entered this code and the installation works well without any interruption. But when I am trying to import the tensorflow from the python software, the problem occurs.I have installed Microsoft Visual C++ 2015 Redistribute Update 3 also. But still not working !!", "Could you try upgrading to TensorFlow's latest realease, `1.1`? Do you have a clone of TF repository locally?\r\nCan I see the entire stack trace?", "I have updated tensorflow but I don't know whether it is 1.1...\r\n\r\nThe entire stack trace is as follows: \r\n\r\nPython 3.5.1 (v3.5.1:37a07cee5969, Dec  6 2015, 01:38:48) [MSC v.1900 32 bit (Intel)] on win32\r\nType \"copyright\", \"credits\" or \"license()\" for more information.\r\n\r\n>>> import tensorflow  as tf\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Program Files (x86)\\python\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow  as tf\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Program Files (x86)\\python\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files (x86)\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n>>> \r\n", "Please check common install issues:\r\nhttps://www.tensorflow.org/install/install_windows#common_installation_problems\r\nhttp://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso", "Problems are solved !!\r\n\r\nI have uninstalled Python 3.5.1 and Installed Python 3.5.2, there is no problem in integrating with tensorflow and 3.5.2. Python 3.6.1 is also not supporting tensorflow.\r\n\r\nSo use 3.5.2 and install tensorflow for ease working.", "I have installed the python 3.5.2 but i am still facing the same problem", "I Installed python 3.5.2 and it fixed the problem after I made it the default, which I did by removing my 3.6.2 in control panel and adding the new python file location to the PATH in environment variables", "thanks that worked just install 3.5.2 python and install tensorflow...\r\n\r\nlinks ->\r\n\r\npython 3.5.2   https://www.python.org/downloads/release/python-352/\r\ntensorflow https://www.tensorflow.org/install/install_windows", "its not working on my system installed with python version 3.7.0\r\n\r\nI have already tried uninstalling and instaliing again from ..googleapis.. link using pip command\r\n\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_amd64.whl`\r\n", "Note: Tensorflow only supported in 64bit python version, so download & use 64bit version of python.\r\nTo check your python version execute--> import struct;print(struct.calcsize(\"P\") * 8)\r\nif your output is 32bit then download 64bit version of python and then use pip install tensorflow.\r\nIt works for me", "> gracias que funcion\u00f3 solo instale 3.5.2 python e instale tensorflow ...\r\n> \r\nyes, I'm installing python 3.5.2 and I followed this link.:\r\n\r\nhttps://medium.com/intel-student-ambassadors/installing-tensorflow-on-windows-with-anaconda-af6fa6280a4b \r\n\r\nwork to me..\r\n", "1.Download Python 3.5.2 (3.5.2 only!!)\r\n2.In command prompt : pip install tensorflow \r\n(ignore this --Cache entry deserialization failed, entry ignored)\r\n3.:python\r\n4.>>import tensorflow as tf (\r\nif no error then successfully installed\r\n5.>>print(tf.__version__) \r\n(use double underscore above)\r\n![Screenshot (205)](https://user-images.githubusercontent.com/48626909/59749961-9fcb5580-929b-11e9-88de-7debe7c02f0d.png)\r\n)"]}, {"number": 9515, "title": "How do you generate tensorflow docs so you can confirm documentation fixes you make?", "body": "Example: Suppose I see a formatting error in a Tensorflow function's arguments on the web. I then make a change in the \"Args: ...\" section of the function's python comments.\r\nHow can I generate these html docs after making this fix?", "comments": ["Try running this script...\r\nhttps://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/tools/docs/gen_docs.sh", "Script seems out of date / didn't work - just made my terminal force close oddly.\r\n\r\nAnyway, I did find https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md though, which looks like the more official answer.", "Moving to #1574 ", "@dmonopoly, ah sorry for the confusion. Thanks for finding the right answer."]}, {"number": 9514, "title": "Dynamic ksize and strides with MaxPool and AvgPool", "body": "This fix tries to fix the issue raised in #4746 where ksize and strides is static (attr) with max_pool (and avg_pool).\r\n\r\nThis fix changes ksize and strides to input tensor with `MaxPoolV2` and `AvgPoolV2` so that it is dynamic now. This fix also deprecates `MaxPool` and `AvgPool`.\r\n\r\nThis fix fixes #4746.", "comments": ["Can one of the admins verify this patch?", "This is fine for API review, but we'd like the Python changes to be separated into a follow-on change after 3 weeks, where you also bump the GraphDef version in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/version.h . This is to accommodate our 3-week limited forward compatibility goal mentioned here:\r\nhttps://www.tensorflow.org/programmers_guide/data_versions#evolving_graphdef_versions and to support eventually deleting the first versions of these ops.", "Can one of the admins verify this patch?", "@yongtang any update on this? Could you please rebase to resolve conflicts?", "@rmlarsen @yzhwang The PR has been rebased and updated. Please take a look.", "@tensorflow-jenkins test this please", "@yongtang There are a number of test failures due to the changes, please address those.", "I'll close this, as it appears inactive. Please create a new PR if you would like to pick this back up.", "@yongtang is this still being worked on? It appears to be exactly what I need.", "@hgaiser A new PR #11875 has been created to try to address the Jenkins test failures."]}, {"number": 9513, "title": "4D and higher dimensional convolutional layers", "body": "Are there any plans to implement 4D or higher convolutional layers?", "comments": ["Not that I am aware of.  Conv2d's are clearly used a ton and Conv3ds much less so. What kinds of applications do you have in mind? Do you have any intention to work on them yourself?", "The specific research is unpublished but it is for computer vision. The models fundamentally require 4-D convolution and it would be great to get them working in TF. I understand cudnn supports this so i thought it might not be too much effort for someone who understands the inner workings of TF to implement it.", "It probably isn't a ton of effort, but we are not working on it currently, so I'm leaving it as contributions welcome, in case anybody wants to work on it. Thanks for bringing it up. @zheng-xq, confirming that we don't have any effort.", "@aselle I'm thinking of giving this a go myself, would you please be able to detail the steps to doing this?", "Best way is to look at another op that is similar (like conv). Try to get a prototype working quickly and submit a draft pull request. There is an API review process where we decide where this goes in the API (and/or cotnrib). @martinwicke, do we have a design document process yet?", "We don't. I think since this is a straightforward extension of existing ops, I don't think there's a huge design space to consider anyway. I'd guess it would go into contrib first, although if it is basically parallel to the 3D ops, we may just add them to core directly.", "Is it a duplicate of https://github.com/tensorflow/tensorflow/issues/1661?", "@alexgkendall I don't know if you [can divulgate](https://github.com/tensorflow/tensorflow/issues/9513#issuecomment-298033948) this info but is this for a follow up of GC-NET?", "I think this is unlikely to happen in a way that makes anyone happy, so closing. All applications of 4D convs that I know of will almost certainly be forced to use special factorizations for efficiency.", "@aselle  I think the high dimention in dataset is a good application. You can see about the future computer vision have the colorful 3-D sight in the time series. [batch, [x,y,z ],rgb-channel, time]", "I think that we can investigate to expand [separable approaches](https://github.com/tensorflow/tensorflow/issues/7278). \r\nWhat do you think?", "nD convolutions can be realized by summing a sequence of (n-1)D convolutions. See [here](https://github.com/funkey/conv4d) for an implementation of a 4D convolution layer using TensorFlow's `conv3d` layer. Details can be found in the [docstring](https://github.com/funkey/conv4d/blob/master/conv4d.py#L27).", "By the way, this issue is a duplicate of https://github.com/tensorflow/tensorflow/issues/1661.", "@funkey thank you for sharing a nice clean implementation of a 4D conv layer in tensorflow. I was wondering if you could help me with the transpose version of this 4D conv layer. How would that look like in this implementation? Also, do you think your suggested 4D conv can be used for image+time data, where the image is 3D and the 4th dimension is time?", "A transposed convolution version of `conv4d` would be great. Happy to review your solution.\r\n\r\nYes, all our use cases are 3D+t.", "@funkey thanks for your response. Do you have any suggestions on how the transpose would look like in your implementation? It seems that in the case of conv4d, frame_results is the output and is the result of summing the convolution of the current input frame with its previous kernel frame. The reverse would be the result of summing the 3D transpose layers? or there is more to it? any comments will be very appreciated, thanks."]}, {"number": 9512, "title": "Tensorboard cannot load more than two event file in logdir", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, Custom network structure and data pre-processing for my own task and dataset, modified based on current single GPU CIFAR-10 tutorial (which use the monitored session).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro 1703\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary, install locally by using `pip install .\\xxx.whl` in my miniconda environment, for the environment, `pip freeze` give the following information\r\n```\r\nappdirs==1.4.3\r\nbleach==1.5.0\r\ncycler==0.10.0\r\nhtml5lib==0.9999999\r\nMarkdown==2.2.0\r\nmatplotlib==2.0.0\r\nnumpy==1.12.1\r\nolefile==0.44\r\npackaging==16.8\r\nPillow==4.1.0\r\nprotobuf==3.2.0\r\npyparsing==2.2.0\r\npython-dateutil==2.6.0\r\npytz==2017.2\r\nsix==1.10.0\r\ntensorflow-gpu==1.1.0rc2\r\nWerkzeug==0.12.1\r\n```\r\n\r\n- **TensorFlow version (use command below)**:\r\nNightly build [#149](http://ci.tensorflow.org/view/Nightly/job/nightly-win/149/) (GPU Version), 1.1.0-rc2\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0, cuDNN 5.1\r\n\r\n- **GPU model and memory**:\r\nQuadro M1200, 4GB, WDDM mode\r\n\r\n### Describe the problem\r\nWhen restart the training (due to some hyper parameter adjustment) the third time, Tensorboard cannot load the new event file. It can only load the first two event file and after that scalar will stop refreshing.\r\n\r\nPowershell console gave the following output:\r\n```\r\n[tensor] PS D:\\Workspace\\ConsorFlow> tensorboard.exe --logdir '../input_data/lpr_train_exp_01'\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nStarting TensorBoard b'52' at http://DESKTOP-P7T44AT:6006\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nERROR:tensorflow:Unable to get size of D:\\Workspace\\input_data\\lpr_train_exp_01\\events.out.tfevents.1493274079.DESKTOP-P7T44AT: D:\\Workspace\\input_data\\lpr_train_exp_01\\events.out.tfevents.1493274079.DESKTOP-P7T44AT\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 17454 (timestamp: 1493310366.6493406). Removing 174 scalars, 76 histograms, 76 compressed histograms, 451 images, and 0 audio.\r\n```\r\nThe 'current step' 17454 in the output is the first step in my second restart.\r\n\r\nInformation about event files:\r\n1st:   events.out.tfevents.1493274079\r\n2nd:  events.out.tfevents.1493310339\r\n3rd:   events.out.tfevents.1493352650\r\n\r\nAbout this problem in Ubuntu:\r\nI just switch to windows several days ago, such problem did not exist in Ubuntu (at least 14.04). I was using the exact same script, but with tensorflow version 1.01 (GPU, not nightly version), install following the offical instruction. \r\n\r\nUnder windows, it was because of #7500, which leave me no choice but to install a nightly build. ", "comments": ["@Cooper-Yang: Can you clarify?  The last sentence makes it sound like the problem is fixed in nightly builds.", "@girving Sure. I choose nightly build because I encountered #7500 (OpKernel \"bla bla bla\" for unknown op: bla bla bla) while using tensorflow, and it was suggested to use a nightly build version for solving it. So I did.\r\n\r\nAnd then, I met this problem while using Tensorboard. \r\n\r\nUpdate: After some experiment, this problem exist in every version I tried, which include the current release 1.1.0, and nightly build 1.1.0-rc1, 1.1.0-rc2.", "@dandelionmane Any ideas about this TensorBoard+Windows problem?", "update: fresh install ubuntu 16.04 and tensorflow, turns out that tensorboard in ubuntu can load the same logdir without any problem, console output shows the following:\r\n\r\n```\r\n(tensor) coopery@WorkStation-CY:/media/coopery/X1/Workspace/input_data$ tensorboard --host 127.0.0.1 --logdir ./lpr_train_exp_01\r\nStarting TensorBoard b'47' at http://127.0.0.1:6006\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 17454 (timestamp: 1493310366.6493406). Removing 174 scalars, 76 histograms, 76 compressed histograms, 451 images, and 0 audio.\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\r\nWARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 64632 (timestamp: 1493381130.1441092). Removing 116 scalars, 76 histograms, 0 compressed histograms, 451 images, and 0 audio.\r\n``` ", "This is a known issue, TensorBoard doesn't like it when you write multiple event files from separate runs in the same directory. It will be fixed if you use a new subdirectory for every run (new hyperparameters = new subdirectory).", "@dandelionmane any plans to fix this ?\r\n\r\nWith this issue, one must maintain a single writer per run, which isn't possible when, for example, using Keras's `TensorBoard` callback along with other custom image/audio tensorboard callbacks. The other option is to have each writer in a separate directory but then Tensorboard will show them as separate runs.", "This issues comes up with estimators as well, which write to an `eval` subdirectory of `model_dir`.\r\n\r\nI think this should be reopened :)", "If possible, it would also be good to know where this happened (run directory etc.), exactly what data triggered this warning.", "This sample project below can be used to reproduce the warnings. It's an implementation of the model in [Getting Started with TensorFlow](https://www.tensorflow.org/get_started/premade_estimators) and uses `tf.estimator.DNNClassifier`.\r\n\r\nhttps://github.com/guildai/examples/tree/master/iris\r\n\r\nSteps:\r\n\r\n```\r\ngit clone https://github.com/guildai/examples.git /tmp/tb-issue\r\ncd /tmp/tb-issue/iris\r\npython train.py\r\ntensorboard --logdir model\r\n```", "I'm also getting this issue with `tf.estimator.DNNClassifier`...\r\n\r\nany news?\r\n", "I see same problem, why was this closed?", "I have the same Problem with tf.estimator.DNNRegressor", "I'm encountering the same problem when employing the TensorBoard callback for training a Keras model. Is there a workaround for this issue that doesn't involve creating a separate subdirectory for logging event files generated by each run?", "> This is a known issue, TensorBoard doesn't like it when you write multiple event files from separate runs in the same directory. It will be fixed if you use a new subdirectory for every run (new hyperparameters = new subdirectory).\r\n\r\nHi, can someone please elaborate on how to this? thanks", "I am also having this problem when using the tf.estimator.Estimator with the tf.estimator.RunConfig saving checkpoints. There had been an issue that was already closed without a solution: https://github.com/tensorflow/tensorflow/issues/17272", "Same question", "When using tf.estimator.train_and_evaluate(...) with an tf.estimator.Estimator, I have the same problem, any ideas?", "two bad ~~", "> When using tf.estimator.train_and_evaluate(...) with an tf.estimator.Estimator, I have the same problem, any ideas?\r\n\r\nTwo possible reasons triggers that:\r\n    **Reason1:** you haven't clean the model repo but you train a new model. \r\n    **Solution:** just clean the model repo before you train another model.\r\n    **Reason2:** you are using models like WDL that made up with more than one graphs\r\n    **Solution:** In this situation, you cannot use tensorboard graph visualization ", "> two bad ~~\r\n\r\nTwo possible reasons triggers that:\r\n**Reason1:** you haven't clean the model repo but you train a new model.\r\n**Solution: **just clean the model repo before you train another model.\r\n**Reason2:** you are using models like WDL that made up with more than one graphs\r\n**Solution: **In this situation, you cannot use tensorboard graph visualization", "by using torch vision 1.3.0.dev20190924 \r\ntensorboard  works now.\r\n\r\n", "I had the same problem using tensorboardX for pytorch. I notice that the code was writing two logs when the code starts. I was instantiating the summary writer in a main.py and writing from a train.py. Instantiating the writer in the trian.py solved.", "As mentioned by @mohamed-ezz: \r\n\r\n> With this issue, one must maintain a single writer per run, which isn't possible when, for example, using Keras's TensorBoard callback along with other custom image/audio tensorboard callbacks. The other option is to have each writer in a separate directory but then Tensorboard will show them as separate runs.\r\n\r\nIn order to properly log runs with custom callbacks this needs to be addressed. I believe this should be reopen or a new issue needs to be created  ", "Same issue... I'm very surprised that such a trivial problem has not been addressed yet. \r\nIt makes most sense to me to use the same directory for the same experiment. I'm thinking about a script that combines the tensorboard files from one directory into two single ones so that tensorboard doesn't have a problem.", "Edit:\r\nI found a solution. When starting your tensorboard server add the following flag:\r\n`--purge_orphaned_data false`\r\n\r\nHope that helps!", "For anyone looking for a naive solution. After every run, just remove the logfile folder or log file itself.  Something like \"rm -rf tf_logs\", so each time the tf_logs would be created like it is the first time, and then remove it after review. Since we have to call the tensorboard from teminal anyway, I think for now this would hold. \r\nsample code\r\n`tensorboard --logdir tf_logs/`\r\n`rm -rf tf_logs`"]}, {"number": 9511, "title": "Cannot import tensorflow in IPython3 (while in normal python3 IDLE works fine) after pip installation", "body": "I have installed tensorflow-gpu via `pip` installation and I am experiencing this issue. I cannot import it in `IPython3` but it works fine with the regular `python3`.\r\n\r\n```bash\r\npetrux@orion:~$ ipython -c \"import tensorflow; print(tensorflow.__version__)\"\r\n1.1.0\r\npetrux@orion:~$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\npetrux@orion:~$ python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.1.0-rc0-61-g1ec6ed5 1.1.0\r\npetrux@orion:~$ ipython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\npetrux@orion:~$ ipython3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-5730349aae22> in <module>()\r\n----> 1 import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py in <module>()\r\n     52 \r\n     53 # Protocol buffers\r\n---> 54 from tensorflow.core.framework.graph_pb2 import *\r\n     55 from tensorflow.core.framework.node_def_pb2 import *\r\n     56 from tensorflow.core.framework.summary_pb2 import *\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/core/framework/graph_pb2.py in <module>()\r\n      4 import sys\r\n      5 _b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\r\n----> 6 from google.protobuf import descriptor as _descriptor\r\n      7 from google.protobuf import message as _message\r\n      8 from google.protobuf import reflection as _reflection\r\n\r\nImportError: No module named 'google.protobuf'; 'google' is not a package\r\n```\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: `pip`\r\n- **TensorFlow version (use command below)**: `('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')`", "comments": ["Are you using virtualenv? It seems like ipython may be using a different install of python try \r\n```\r\nimport sys;\r\nprint(sys.path)\r\n```\r\nin all python and ipython", "No, no virtualenv.  \r\nHere is the `sys.path`:\r\n\r\n```bash\r\npetrux@orion:~$ python -c \"import sys; print(sys.path)\"\r\n['', '/usr/local/spark-2.1.0-bin-hadoop2.7/python',\r\n'/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip',\r\n'/home/petrux', '/usr/lib/python2.7',\r\n'/usr/lib/python2.7/plat-x86_64-linux-gnu',\r\n'/usr/lib/python2.7/lib-tk',\r\n'/usr/lib/python2.7/lib-old',\r\n'/usr/lib/python2.7/lib-dynload',\r\n'/home/petrux/.local/lib/python2.7/site-packages',\r\n'/usr/local/lib/python2.7/dist-packages',\r\n'/usr/lib/python2.7/dist-packages',\r\n'/usr/lib/python2.7/dist-packages/PILcompat',\r\n'/usr/lib/python2.7/dist-packages/gst-0.10',\r\n'/usr/lib/python2.7/dist-packages/gtk-2.0',\r\n'/usr/lib/python2.7/dist-packages/ubuntu-sso-client']\r\n\r\npetrux@orion:~$ ipython -c \"import sys; print(sys.path)\"\r\n['', '/usr/local/bin', '/usr/local/spark-2.1.0-bin-hadoop2.7/python',\r\n'/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip',\r\n'/home/petrux',\r\n'/usr/lib/python2.7',\r\n'/usr/lib/python2.7/plat-x86_64-linux-gnu',\r\n'/usr/lib/python2.7/lib-tk',\r\n'/usr/lib/python2.7/lib-old',\r\n'/usr/lib/python2.7/lib-dynload', \r\n'/home/petrux/.local/lib/python2.7/site-packages', \r\n'/usr/local/lib/python2.7/dist-packages', \r\n'/usr/lib/python2.7/dist-packages', \r\n'/usr/lib/python2.7/dist-packages/PILcompat', \r\n'/usr/lib/python2.7/dist-packages/gst-0.10',\r\n'/usr/lib/python2.7/dist-packages/gtk-2.0', \r\n'/usr/lib/python2.7/dist-packages/ubuntu-sso-client', \r\n'/usr/local/lib/python2.7/dist-packages/IPython/extensions', \r\n'/home/petrux/.ipython']\r\n\r\npetrux@orion:~$ python3 -c \"import sys; print(sys.path)\"\r\n['', '/usr/local/spark-2.1.0-bin-hadoop2.7/python', \r\n'/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip', \r\n'/home/petrux', \r\n'/usr/lib/python35.zip', \r\n'/usr/lib/python3.5', \r\n'/usr/lib/python3.5/plat-x86_64-linux-gnu', \r\n'/usr/lib/python3.5/lib-dynload', \r\n'/home/petrux/.local/lib/python3.5/site-packages', \r\n'/usr/local/lib/python3.5/dist-packages', \r\n'/usr/lib/python3/dist-packages']\r\n\r\npetrux@orion:~$ ipython3 -c \"import sys; print(sys.path)\"\r\n['', '/usr/local/bin', \r\n'/usr/local/spark-2.1.0-bin-hadoop2.7/python', \r\n'/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip', \r\n'/home/petrux', \r\n'/usr/lib/python35.zip', \r\n'/usr/lib/python3.5', \r\n'/usr/lib/python3.5/plat-x86_64-linux-gnu', \r\n'/usr/lib/python3.5/lib-dynload', \r\n'/home/petrux/.local/lib/python3.5/site-packages', \r\n'/usr/local/lib/python3.5/dist-packages', \r\n'/usr/lib/python3/dist-packages', \r\n'/usr/local/lib/python3.5/dist-packages/IPython/extensions', \r\n'/home/petrux/.ipython']\r\n```", "hm the ipython3 and python3 are different. try setting sys.path in ipython3 to be exactly the python3 one and see if that works.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9510, "title": "Fix Bazel CI / TensorFlow project", "body": "See https://github.com/bazelbuild/bazel/issues/2892", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Unfortunately there is a merge conflict, can you try resolving the conflict?", "@vrv : Done, PTAL\r\n\r\n@martinwicke : FYi @meteorcloudy is on vacation, that's why I worked on this bug in the first place", "@tensorflow-jenkins test this please"]}, {"number": 9509, "title": "Tensorflow with XLA hangs with both GPU and CPU at ~0% usage when training.", "body": "I am using the newest tensorflow which I built from source as of yesterday in an attempt to fix this issue.  Originally I had a source build of tensorflow 1.1.0.  I am running Ubuntu 16.04 with CUDA 8 and CUDNN 5.  My GPU is a GTX 1080.\r\n\r\nThe problem I am having is when I try to train my character based translator model using the XLA compiler.  The code makes it all the way through the initialize variables, etc up to the first run command which contains my train step and then just freezes.  Both my GPU and CPU are idle.  I attached gdb to my process and it seems to be stuck waiting for some sort of notification.  My model builds and runs fine if I am running it without training in predict mode but still with XLA.  It also runs fine if I train it without XLA.  Just the combo of XLA and training is the issue.\r\n\r\nI attached to this my code plus some sample training data.  This problem should be reproducible by running the train.py.\r\n\r\n[CharacterTranslator.zip](https://github.com/tensorflow/tensorflow/files/963946/CharacterTranslator.zip)", "comments": ["Did you manage to make it anywhere debugging this?", "I have similar problem after compiling tf-1.1.0 with XLA support. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Similar problem with Tensorflow 1.5.0-rc0 with XLA support. Model trains just fine with XLA turned off but hangs with XLA on. Running on K80.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 134 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 149 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 164 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 179 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 194 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @hawkinsp: It has been 209 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing issue as stale. Please reopen if you still experience this problem."]}, {"number": 9508, "title": "Problem running Tensorflow graph using C++ API", "body": "I have retrained Inception-v3 model using (Tensorflow) Python API, and saved a standalone Graph in .pb form. I have also used a dropout layer before the final layer. I can successfully run inference on the graph in python. The code to generate predictions in python is as follows:\r\n`softmax_tensor = sess.graph.get_tensor_by_name('final_layer/final_result/Softmax:0')\r\npredictions = sess.run(softmax_tensor, { 'DecodeJpeg/contents:0': image_data, 'final_layer/dropout/Placeholder:0': 1.})`\r\nThe C++ counterpart of the python code is as follows:\r\n` string input_layer = \"Mul\";\r\n  string output_layer = \"final_layer/dropout/Placeholder:0\";\r\n  Status run_status = session->Run({{input_layer, resized_tensor}},\r\n                                   {output_layer}, {}, &outputs);`\r\n\r\nThe C++ code ends up with the following error message:\r\n`Running model failed: Invalid argument: You must feed a value for placeholder tensor 'final_layer/dropout/Placeholder'`\r\n\r\nWhat should I change in the above C++ code to remove this error?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9507, "title": "AttributeError: 'Tensor' object has no attribute '_displayhook'", "body": "I'm running TensorFlow on Zeppelin with Python 3.5.2, the TensorFlow version is 1.0.1.\r\n\r\nIt shows up AttributeError: 'Tensor' object has no attribute '_displayhook' error when I was running a Gaussian Distribution.\r\n\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nn_values = 32\r\nx = tf.linspace(-3.0, 3.0, n_values)\r\n\r\nsigma = 1.0\r\nmean = 0.0\r\n\r\na = tf.exp(tf.negative(tf.pow(x - mean, 2.0) / (2.0 * tf.pow(sigma, 2.0)))) \r\nb = (sigma * tf.sqrt(2.0 * 3.1415))\r\n\r\nz = a / b", "comments": ["We don't have much experience with Zeppelin so it is tough to support this. Perhaps your problem is your are trying to graph unevaluated Tensors. Try eval or  sess.run() on your tensor before trying to display it, however. i.e. try\r\n```python\r\nimport tensorflow as tf\r\nx = tf.linspace(-3.0, 3.0, n_values)\r\nsess=tf.Session()\r\nxevald = sess.run(x)\r\nplt.plot(xevald)\r\n\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9506, "title": "No supported kernel for GPU devices is available for assigning a variable of int32 type", "body": "**tensorflow version**  v1.1.0-rc0-61-g1ec6ed5\r\n**code to reproduce**\r\n```py\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\r\nimport tensorflow as tf\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\nfor device in ['cpu', 'gpu']:\r\n    print(device)\r\n    with tf.device('/{}:0'.format(device)):\r\n        var = tf.get_variable('var{}'.format(device), shape=[1], dtype='int32')\r\n        vari = tf.assign(var, [23])\r\n\r\n    sess = tf.Session()\r\n    sess.run(vari)\r\n```\r\n**error**\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'vargpu': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n```\r\n\r\nThis should be caused by [TF_CALL_GPU_NUMBER_TYPES](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/register_types.h#L171) only iterating over float types, and it is used to [generate the assign kernels](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/dense_update_ops.cc#L174). However [TF_CALL_NUMBER_TYPES](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/register_types.h#L154) iterates over all types including integers. Is this asymmetry a deliberate design choice?", "comments": ["`int32` is weird on GPU for unfortunate historical reasons.  @yuanbyu Do you know of any plans to clean this up?  I'm surprised we don't already have an open Github issue about it.", "+1 general support for int types on GPU would be great across the board", "Any updates from TF team?", "Still no activity from the TF team, unfortunately.", "I'm stuck with this problem. I would like to [pre-load data into GPU](https://www.tensorflow.org/programmers_guide/reading_data#preloaded_data) to keep communication overhead low and my data (natural language) is all integers. I tried to cast it into float32 to fit in the GPU but `tf.nn.embedding_lookup` would not run (while Torch does allow this use). Could somebody help?", "I also ran into this problem.  \r\n\r\nMy use case is similar.  I am attempting to preload training data into the GPU.  Very odd bug", "Same problem as @minhlab. Still no update on this?", "Same problem. Is there any way to fix this error?", "Same issue. Attempting to do some indexing with `tf.gather_nd` but I can't seem to put an integer tensor on the GPU. ", "Sadly we still have that int32 oddity. Can you use an int64?", "I'm using the following workaround. I allocated my variable as a float32 and later cast it to an int32 using `tf.to_int32`.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "If you are still facing the problem of invalid argument error, then try installing the tensorflow version 1.3.0 and then do it.", "We are now at 1.10 I believe, could you try that?", "I tried with the latest versions but the error was still there.. ", "How about using the workaround with a different type?", "Just out of curiosity @girving, what are the historical reasons for this situation? Is the problem that some kernels won't compile? Or do certain parts of the system assume int32 ops will always be placed on CPU? Or is it even weirder than that?", "@ed-alertedh I believe it was a hack where initially `bool` didn't work on GPU and `int32` was somehow reserved for control flow reasons, but I don't hold that belief with high confidence.\r\n\r\n**Edit:** That hack was back when tensorflow didn't have a mechanism for assigning ops to particular devices.  It was `int32` that didn't exist on GPU, so control flow could use `int32` ops to ensure they'd stay on the CPU.", "The `int32` are pinned to CPU because they are typically metadata operations. So, you will expect (silently) slower operations if you use `int32`, on GPU.\r\n\r\nThat said, I don't see a reason why we can't have that operation and we should add it.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "This solution seemed to work https://stackoverflow.com/questions/44813939/could-not-satisfy-explicit-device-specification-devicegpu0-because-no-devic", "Closing as this is resolved, free to reopen if problem persists.", "That doesnt really solve the problem, that hides the problem\r\n\r\nHopefully this is resolved at some point, but it's not"]}, {"number": 9505, "title": "Check failed: NDIMS == dims() (2 vs. 1) when I build a svm model", "body": "when I build a svm model with tf.learn, it get error like this: \r\n`F tensorflow/core/framework/tensor_shape.cc:36] Check failed: NDIMS == dims() (2 vs. 1)Asking for tensor of 2 dimensions from a tensor of 1 dimensions\r\n`\r\nI have ask a question in stackoverflow, It could be a issue according the reply:\r\nhttp://stackoverflow.com/questions/43638488/check-failed-ndims-dims-2-vs-1-when-i-build-a-svm-model\r\n\r\nall the reproduce code here:\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\r\n\r\n\r\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n    column_name='detailed_occupation_recode', \r\n    hash_bucket_size = 1000\r\n)\r\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n    column_name='education',\r\n    hash_bucket_size=1000\r\n)\r\n# Continuous base columns\r\nage = tf.contrib.layers.real_valued_column('age')\r\nwage_per_hour = tf.contrib.layers.real_valued_column('wage_per_hour')\r\n\r\n\r\n\r\ncolumns = ['age', 'detailed_occupation_recode', 'education', 'wage_per_hour','label']\r\nFEATURE_COLUMNS = [\r\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\r\n    age, detailed_occupation_recode, education, wage_per_hour\r\n\r\n]\r\n\r\n\r\nLABEL_COLUMN = 'label'\r\n\r\nCONTINUOUS_COLUMNS = ['age', 'wage_per_hour']\r\n\r\nCATEGORICAL_COLUMNS = ['detailed_occupation_recode','education']\r\n\r\n\r\ndf_train = pd.DataFrame([[12,'12','7th and 8th grade',40,'- 50000'],\r\n                [40,'45','7th and 8th grade',40, '50000+'],\r\n                [50,'50','10th grade',40,'50000+'],\r\n                [60,'30','7th and 8th grade',40,'- 50000']],\r\n                columns=['age', 'detailed_occupation_recode', 'education', 'wage_per_hour', 'label'])\r\n\r\n\r\ndf_test = pd.DataFrame([[12,'12','7th and 8th grade',40,'- 50000'],\r\n                [40,'45','7th and 8th grade',40, '50000+'],\r\n                [50,'50','10th grade',40,'50000+'],\r\n                [60,'30','7th and 8th grade',40,'- 50000']],\r\n                columns=['age', 'detailed_occupation_recode', 'education', 'wage_per_hour', 'label'])\r\ndf_train[LABEL_COLUMN] = (df_train[LABEL_COLUMN].apply(lambda x: '+' in x)).astype(int)\r\ndf_test[LABEL_COLUMN] = (df_test[LABEL_COLUMN].apply(lambda x: '+' in x)).astype(int)\r\ndtypess = df_train.dtypes\r\n\r\n\r\ndef input_fn(df):\r\n    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\r\n    categorical_cols = {k: tf.SparseTensor(\r\n        indices=[[i, 0] for i in range(df[k].size)],\r\n        values=df[k].values,\r\n        dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}    \r\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\r\n    feature_cols['example_id'] = tf.constant([str(i+1) for i in range(df['age'].size)])\r\n    label = tf.constant(df[LABEL_COLUMN].values)\r\n    return feature_cols, label\r\n\r\ndef train_input_fn():\r\n    return input_fn(df_train)\r\n\r\ndef eval_input_fn():\r\n    return input_fn(df_test)\r\n\r\nmodel_dir = '../svm_model_dir'\r\n\r\nmodel = svm.SVM(example_id_column='example_id', feature_columns=FEATURE_COLUMNS, model_dir=model_dir)\r\nmodel.fit(input_fn=train_input_fn, steps=10)\r\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\r\nfor key in sorted(results):\r\n    print(\"%s: %s\" % (key, results[key]))\r\n```\r\nand the all error output text:\r\n\r\n```\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \r\n\r\n\"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\r\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you\r\n resize your input, as this behavior may change.\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you\r\n resize your input, as this behavior may change.\r\nWARNING:tensorflow:From /Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:882: hinge_loss (from t\r\nensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.hinge_loss instead.\r\nWARNING:tensorflow:From /Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (f\r\nrom tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-dupl\r\nicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nWARNING:tensorflow:From /Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:882: hinge_loss (from t\r\nensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\r\nInstructions for updating:\r\nUse tf.losses.hinge_loss instead.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machi\r\nne and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machi\r\nne and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine\r\nand could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine\r\n and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine\r\nand could speed up CPU computations.\r\nF tensorflow/core/framework/tensor_shape.cc:36] Check failed: NDIMS == dims() (2 vs. 1)Asking for tensor of 2 dimensions from a tensor of 1 dimensions\r\n[1]    66225 abort      python simple-tf-svm.py\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "This seems like a bug to me though. @burness Does a similar setup run successfully for other canned estimators? ", "This is definitely a bug, either with one of the FeatureColumn processing ops or with the way the SVM optimizer is using them. I didn't trace it through completely with GDB to figure out what's wrong exactly (probably equivalent effort to fixing the bug), but the fact that this is required is indicative; even if there's something wrong with the usage, we need to do better than a CHECK failure.", "@terrytangyuan  LinearClassifier seems all right, I will try other estimators later ", "I can reproduce at head with a single real_valued_column:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef input_fn():\r\n    feature_cols = {'age': tf.zeros([4]),\r\n                    'example_id': tf.constant(['1', '2', '3', '4'])}\r\n    label = tf.constant([0, 1, 1, 0])\r\n    return feature_cols, label\r\n\r\nmodel = tf.contrib.learn.SVM(\r\n    example_id_column='example_id', \r\n    feature_columns=[tf.contrib.layers.real_valued_column('age')])\r\nmodel.fit(input_fn=input_fn, steps=10)\r\n```\r\n\r\nApparently the error is indicating that only vectors are supported for real_valued_columns. A similar CHECK failure pops up if the column is a matrix (`tf.zero([4, 1, 1])`). So you can get your code working by adding a dummy second dimension to your real_valued_columns.\r\n\r\nWe, however, need to work on our argument checking. Thank you for the report!", "Hi @petrosmol,\r\n\r\nCould you take a look at this issue? It looks like LinearClassifier accepts scalar real_valued_columns (i.e. input with just a batch dimension). Do we just want to add a reshape here to keep behavior consistent?", "Thanks for reporting! I am looking into it. Will update this thread shortly.", "Update: The problem is not with the estimator per se, rather it is with the underlying optimizer (SDCA) which accepts only matrices (2-dimensional tensors) for dense features. I am working on a fix. \r\nIn the meantime, a simple fix so that you are not blocked is to reshape your real valued columns so that they always have rank one. For instance, in your input_fn, you can replace:\r\n\r\ncontinuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS} with\r\ncontinuous_cols = {k: tf.expand_dims(tf.constant(df[k].values), 1) for k in CONTINUOUS_COLUMNS}", "@petrosmol  Thanks! It seems ok now. And In a randomforest test case as follow:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nfrom tensorflow.contrib.tensor_forest.client import random_forest\r\n\r\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n    column_name='detailed_occupation_recode', hash_bucket_size=1000)\r\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n    column_name='education', hash_bucket_size=1000)\r\n# Continuous base columns\r\nage = tf.contrib.layers.real_valued_column('age')\r\nwage_per_hour = tf.contrib.layers.real_valued_column('wage_per_hour')\r\n\r\ncolumns = [\r\n    'age', 'detailed_occupation_recode', 'education', 'wage_per_hour', 'label'\r\n]\r\nFEATURE_COLUMNS = [\r\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\r\n    age,\r\n    detailed_occupation_recode,\r\n    education,\r\n    wage_per_hour\r\n]\r\n\r\nLABEL_COLUMN = 'label'\r\n\r\nCONTINUOUS_COLUMNS = ['age', 'wage_per_hour']\r\n\r\nCATEGORICAL_COLUMNS = ['detailed_occupation_recode', 'education']\r\n\r\ndf_train = pd.DataFrame(\r\n    [[12, '12', '7th and 8th grade', 40, '- 50000'],\r\n     [40, '45', '7th and 8th grade', 40, '50000+'],\r\n     [50, '50', '10th grade', 40, '50000+'],\r\n     [60, '30', '7th and 8th grade', 40, '- 50000']],\r\n    columns=[\r\n        'age', 'detailed_occupation_recode', 'education', 'wage_per_hour',\r\n        'label'\r\n    ])\r\n\r\ndf_test = pd.DataFrame(\r\n    [[12, '12', '7th and 8th grade', 40, '- 50000'],\r\n     [40, '45', '7th and 8th grade', 40, '50000+'],\r\n     [50, '50', '10th grade', 40, '50000+'],\r\n     [60, '30', '7th and 8th grade', 40, '- 50000']],\r\n    columns=[\r\n        'age', 'detailed_occupation_recode', 'education', 'wage_per_hour',\r\n        'label'\r\n    ])\r\ndf_train[LABEL_COLUMN] = (\r\n    df_train[LABEL_COLUMN].apply(lambda x: '+' in x)).astype(int)\r\ndf_test[LABEL_COLUMN] = (\r\n    df_test[LABEL_COLUMN].apply(lambda x: '+' in x)).astype(int)\r\ndtypess = df_train.dtypes\r\n\r\nprint df_train\r\nprint df_test\r\n\r\n\r\ndef input_fn(df):\r\n    # continuous_cols = {\r\n    #     k: tf.expand_dims(tf.constant(df[k].values), 1)\r\n    #     for k in CONTINUOUS_COLUMNS\r\n    # }\r\n    continuous_cols = {\r\n        k: tf.constant(df[k].values)\r\n        for k in CONTINUOUS_COLUMNS\r\n    }\r\n    categorical_cols = {\r\n        k: tf.SparseTensor(\r\n            indices=[[i, 0] for i in range(df[k].size)],\r\n            values=df[k].values,\r\n            dense_shape=[df[k].size, 1])\r\n        for k in CATEGORICAL_COLUMNS\r\n    }\r\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\r\n    label = tf.constant(df[LABEL_COLUMN].values)\r\n    return feature_cols, label\r\n\r\n\r\ndef train_input_fn():\r\n    return input_fn(df_train)\r\n\r\n\r\ndef eval_input_fn():\r\n    return input_fn(df_test)\r\n\r\n\r\nmodel_dir = '../rf_model_dir'\r\n\r\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\r\n    num_trees=10, max_nodes=1000, num_classes=2, num_features=4)\r\nclassifier = random_forest.TensorForestEstimator(hparams, model_dir=model_dir)\r\nclassifier.fit(input_fn=train_input_fn, steps=10)\r\nresults = classifier.evaluate(input_fn=eval_input_fn, steps=1)\r\nfor key in sorted(results):\r\n    print(\"%s: %s\" % (key, results[key]))\r\n```\r\nIt will get the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"simple-tf-rf.py\", line 93, in <module>\r\n    classifier.fit(input_fn=train_input_fn, steps=10)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/client/random_forest.py\", line 264, in fit\r\n    max_steps=max_steps)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 426, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 934, in _train_model\r\n    model_fn_ops = self._call_legacy_get_train_ops(features, labels)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1003, in _call_legacy_get_t\r\nrain_ops\r\n    train_ops = self._get_train_ops(features, labels)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1162, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/client/random_forest.py\", line 116, in _model_fn\r\n    graph_builder.inference_graph(features)\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py\", line 444, in inference_graph\r\n    data_ops.ParseDataTensorOrDict(input_data))\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/tensor_forest/python/ops/data_ops.py\", line 141, in ParseDataTensorOrDict\r\n    col_spec.size = data[k].get_shape()[1].value\r\n  File \"/Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 502, in __getitem__\r\n    return self._dims[key]\r\nIndexError: list index out of range\r\n```\r\nand I replace:\r\ncontinuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS} with\r\ncontinuous_cols = {k: tf.expand_dims(tf.constant(df[k].values), 1) for k in CONTINUOUS_COLUMNS}\r\n\r\nIt seems ok now!\r\nIt may need to enhance this reshape problem with the process of input_fn", "It looks like @thomascolthurst has added a reshape to TensorForest input, which I assume just hasn't made it into a TensorFlow release yet.", "FYI, I have submitted a change that should fix the problem for SVM (and any other estimator that uses SDCA as an optimizer). You should be able to provide rank-1 tensors for 1-dimensional real valued columns.\r\n\r\nLet me know if you encounter any other problems", "@petrosmol  Thanks "]}, {"number": 9504, "title": "README.md: make the settings.gradle and build.gradle example code match", "body": "As currently written, the example code that README.md recommends adding to settings.gradle and build.gradle doesn't match. Adding the code as-is results in a Gradle error.\r\n\r\nThis change fixes the discrepancy, making it easier to get started using the TensorFlow Android interface.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "(If you can sign the CLA, please feel free to ping this change and/or reopen a new one with the signed CLA -- we can't take a look at this without it)."]}, {"number": 9502, "title": "Internally inconsistent results on GPU, not on CPU", "body": "### System Information\r\n```\r\nUbuntu 16.04 LTS\r\nCUDA 8.0\r\ncuDNN v5.1\r\nNVIDIA Tesla K80 (11439MiB)\r\ntensorflow 1.0.1 (bug also present in 1.1.0) [pip]\r\nkeras 2.0.3 [python setup.py install]\r\n```\r\n\r\n### Problem\r\nI can define a convnet that returns reasonable results with CPU, but returns a mixture of expected and nonsense results with GPU. The problem with the GPU processing is made clear by passing a set of \"all ones\" images to the net; the output should be identical for all 16 images, yet the outputs for the last one or two images differ drastically from the rest.\r\n\r\nI see this problem when using the GPU, but not with CPU.  No exceptions are raised, and there are no warnings beyond the usual `TensorFlow library wasn't compiled to use ___ instructions, but these are available on your machine and could speed up CPU computations.`.\r\n\r\nI see this problem using `tensorflow` as my `keras` backend, but not when using `theano` as my backend.  That statement is true whether I set `image_data_format` to `channels_last` or `channels_first`, for either backend.\r\n\r\nI've found three changes to the model architecture defined below that make the problem go away: (1) change the number of filters, as described in more detail below, (2) remove the `UpSampling2D` layer, or (3) remove the  `AveragePooling2D` layer.  Given (3), this issue may be related to https://github.com/tensorflow/tensorflow/issues/8566 .\r\n\r\n### Code\r\nConsider the following convnet.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D\r\nfrom keras.models import Model\r\nimport numpy as np\r\n\r\ndef my_model(filters=65):\r\n    np.random.seed(0)\r\n    inputs = Input((None, None, 3))\r\n    conv1 = Conv2D(filters, (5,5), strides=(1,1), padding='same', activation='relu')(inputs)\r\n    conv1 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv1)\r\n    conv2 = Conv2D(filters, (5,5), strides=(1,1), padding='same', activation='relu')(conv1)\r\n    conv2 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv2)\r\n    conv3 = Conv2D(filters, (3,3), strides=(1,1), padding='same', activation='relu')(conv2)\r\n    conv3 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv3)\r\n    conv4 = Conv2D(filters, (3,3), strides=(1,1), padding='same', activation='relu')(conv3)\r\n    conv4 = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(conv4)\r\n    conv5 = Conv2D(filters, (3,3), strides=(1,1), padding='same', activation='relu')(conv4)\r\n    conv5_up = UpSampling2D(size=(8, 8))(conv5)\r\n    conv5_up = AveragePooling2D(pool_size=(8, 8), strides=(1,1), padding='same')(conv5_up)\r\n    return Model(inputs=inputs, outputs=conv5_up)\r\n```\r\n\r\nWe can test the self-consistency of this net by feeding it 16 \"all ones\" images.  The output features should be identical for each input image, since the input images are simply all ones.\r\n\r\n```\r\ndef test(filters=65, device='gpu'):\r\n    # Make a batch of 16 \"images\", ALL ONES.\r\n    img = np.ones((16, 512, 512, 3), dtype='float32')\r\n    with tf.device('/%s:0'%device):\r\n        model = my_model(filters)\r\n        features = model.predict(img)\r\n    # Compare features of 1st and 16th image.\r\n    # Since the input was all ones, they should agree.\r\n    inconsistent = np.any(features[0] != features[15])\r\n    if inconsistent:\r\n        print \"Inconsistent!\"\r\n```\r\n\r\nThe model architecture is parameterized by a single parameter, `filters`, the number of filters in each layer.  Interestingly, I see that the model inconsistency cares about powers of 2 in `filters`:\r\n\r\n\u2022\u00a0consistency for `filters=1 through 64`,\r\n\u2022\u00a0inconsistency for `filters=65 through 127`,\r\n\u2022\u00a0consistency for `filters=128`, \r\n\u2022\u00a0inconsistency for `filters=129 through 150`.\r\n\r\nThe smallest model that shows the problem is at `filters=65`.  Below I show a single feature output for the 1st and 16th input images.  They should agree, but they don't.\r\n\r\nFeatures for 1st image (which is identical for images 1-15):\r\n![features_1st_image](https://cloud.githubusercontent.com/assets/4852957/25504294/33eb3ad6-2b52-11e7-9b1d-636bd576ed23.png)\r\n\r\nFeatures for 16th image:\r\n![features_16th_image](https://cloud.githubusercontent.com/assets/4852957/25504297/3969ab6e-2b52-11e7-87e7-67bed7a8c7b3.png)\r\n\r\nAs can be seen, the feature map for the 16th image differs dramatically from images 1-15, even though the inputs are identical (all ones).\r\n\r\nIf I repeat this test with a `filters=129` instead of `filters=65`, the bug appears earlier in the batch; the features for images 1-14 are identical, while the features for images 15 and 16 differ from the rest, and from each other.\r\n\r\n\r\n", "comments": ["@zheng-xq Would you mind commenting on this and https://github.com/tensorflow/tensorflow/issues/8566?  Andy asked a while back but didn't get a reply.", "I've commented on #8566, please try setting TF_AVGPOOL_USE_CUDNN=1 and see if it fixes the bug.", "Yes, setting TF_AVGPOOL_USE_CUDNN=1 fixes the bug.  Thanks for looking into this."]}, {"number": 9501, "title": "Securing Tensorflow models on Android", "body": "### Describe the problem\r\nI am using android to deploy my Tensorflow model. I am obfuscating names already but that wont stop people stealing and just plugging and playing graph file in their apps. Feature request for accepting encrypted graph files to prevent our graph files from leaking or any pointers on how to implement it.\r\n", "comments": ["@aselle @petewarden @andrewharp \r\n\r\nThat's an interesting use case. In general I am not sure that there is no good way to do this as people can always debug your app to find out. You could run part of the network as a service, but that has obvious latency costs.", "How about this?\r\n\r\nchange input/output node names and then your server dynamically sends those values as well as some other parameters for initializing when the app executed?  don't store those values in the app.\r\n\r\nOne issue left though, people can still extract those data from the graph file when loaded directly from desktop except the graph file is memory mapped graph, and android can't load memory mapped file (maybe?)", "This is a bit out of my domain, but my suggestion would be to use a simple XOR or other obsfucation of the file on disk, and then at runtime load the file into memory, 'decrypt' that memory, and then deserialize the protobuf from RAM. This won't stop a determined malicious actor, but at least complicates the process for them, and at some point there will be an in-memory copy of the graph anyway, whatever you do.", "@petewarden Xor encryption, really?", "Closing as out of scope for TensorFlow.", "I'll give Pete the benefit of the doubt and assume that by xor he meant AES-GCM."]}, {"number": 9500, "title": "Tensorflow Video Decoding on a Separate Thread in a Distributed System ", "body": "Having a distributed system, I need to enqueue frames on a CPU device, while processing the frames, that is, training the network, has to be on a GPU device. Could this be performed in parallel?\r\n\r\nCurrently, tensorflow enables Audio coding through FFMPEG(contrib), are there any features for video encoding and decoding which is multi-threaded? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9499, "title": "No name 'debug' in module 'tensorflow'", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu --upgrade\r\n- **TensorFlow version (use command below)**: 1.0.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Cuda:8, CuDnn:5.1\r\n- **GPU model and memory**: Pascal TitanX (x2)\r\n- **Exact command to reproduce**: \r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWon't import.  This is the exact import from the docs; I have done nothing fancy; just normal install of everything.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nyou can replicate this really easy.\r\n\r\npython -c \"from tensorflow.python import debug as tf_debug;\"", "comments": ["@caisq: This works for me with 1.0.0 on Mac.  Not sure if this is a Windows issue or a version issue.", "TFDBG is not functional in 1.0.x on Windows. This is a known issue. It should have been fixed in 1.1.0."]}, {"number": 9498, "title": "tf.metrics.accuracy maintains a running accuracy?", "body": "I use the `tf.metrics.accuracy`, however it is a bit **counter-intuitive** in that it maintains a **running** accuracy (the [doc](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) agrees with this).  The following simple script illustrates the situation\r\n\r\n```python\r\nimport os\r\n# supress tensorflow logging other than errors\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nprint(tf.__version__)\r\n# 1.1.0\r\n\r\nx = tf.placeholder(tf.int32, [5])\r\ny = tf.placeholder(tf.int32, [5])\r\nacc, acc_op = tf.metrics.accuracy(labels=x, predictions=y)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\n\r\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\r\n                                       y: [1, 0, 0, 0, 1]})\r\nprint(v)\r\n# [0.0, 0.8]\r\n\r\nv = sess.run(acc)\r\nprint(v)\r\n# 0.8\r\n\r\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\r\n                                       y: [0, 1, 1, 1, 1]})\r\nprint(v)\r\n# [0.8, 0.4]\r\n\r\nv = sess.run(acc)\r\nprint(v)\r\n# 0.4\r\n```\r\n\r\nMy concerns are\r\n1. the use of accuracy is bit **surprising**, are we supposed to manually construct the normal accuracy?\r\n2. IMHO, it is better to \r\n    1. implement the normal accuracy behavior or\r\n    2. provide a clean way to **reset** the local variables created by `tf.metrics.accuracy`, i.e., the `count` and `total`.", "comments": ["@sguada Do you have opinions here?  The current API is designed around an evaluation process that restarts from scratch each time; it does seem hard to use in other contexts.", "For what it's worth, I agree with @gongzhitaao here, though we'll be stuck with these for a while given their use in TF 1.0. I think the metric names prefaced with `streaming` (back when this was `tf.contrib.metrics`) are much more intuitive. I suppose you could also rename the entire module, i.e., `tf.streaming_metrics`.", "Yeah when we created them they were streaming_metrics so it was streaming_accuracy.\r\n\r\nI think streaming metrics should have a simple way to reset them.\r\n\r\nFYI: Implementing non streaming accuracy is simple, ex:\r\n`tf.reduce_mean(tf.to_float32(predictions == labels))`\r\n\r\n@martinwicke \r\n", "@sguada Thanks.  That's what I'm using in my code.", "+1 for `reset_op` for *all* streaming metrics (i.e., all from `tf.metrics`) -- that allows both\r\n  - to create a non-streaming metrics (by running a `reset_op` followed by `update_op`)\r\n  - repeatedly evaluate a multi-batch test set (often needed in our work)\r\n\r\nMaybe a new named argument could be added, which causes the metrics to return an additional (third) result, which would be the `reset_op`? That would be backward compatible.\r\n\r\nI could also imagine another named argument which would cause the metrics to return just one value -- one-time metrics call (i.e., `reset_op` combined by an `update_op`); but I can easily implement this if the `reset_op` is available.", "Note that this is tightly coupled to #4814.", "Nice suggestion! This has puzzled me for weeks.\r\nI just asked a question earlier in the stackover flow.\r\n\r\n\r\nSuppose we set every_n_steps = 100 so the monitor will be called every 100 steps.\r\n\r\nAlso, suppose the input_fn function for the validation monitor will yield a streaming of data. Let's assume 10 batches.\r\n\r\nCase 1: the auc state is reset every time the validation monitor is called, there for, the streaming is done for the 10 batches in each validation step.\r\n\r\nCase 2: the auc state is NOT reset, so the streaming auc is computed from the first call of validation monitor. Namely, the first output (at 100 steps) is computed from 10 batches, the second validation output (at 200 steps) is computed based on the streaming auc after the first call and also the 10 batches fed in. The third output (at 300 steps) is computed based on the streaming auc after the second call and also the 10 batches fed in.\r\n\r\nQuestion1, which one of the scenarios is implemented?\r\n\r\nQuestion2, if we use tf.metrics.auc, what is the difference? In this doc they say:\r\n\r\nFor estimation of the metric over a stream of data, the function creates an update_op operation that updates these variables and returns the auc.\r\nso this also computes the streamed auc?!\r\n\r\nSo, the streaming is from the very beginning or within each call for the validation monitor?\r\nIf it is from the very beginning, i guess we have to initialize mannerly and do things like\r\nfor i in range(n_epochs):\r\n   tf.initilize.local_variables()\r\n  m.fit(..)\r\nwhich is not efficient....", "Also, from tensorflow output,\r\n\r\nINFO:tensorflow:Saving dict for global step 39807: accuracy = 0.85421, accuracy/baseline_label_mean = 0.14821, accuracy/threshold_0.500000_mean = 0.85421, **auc = 0.686321**, global_step = 39807, labels/actual_label_mean = 0.14821, labels/prediction_mean = 0.146081, loss = 0.39175, precision/positive_threshold_0.500000_mean = 0.580026, recall/positive_threshold_0.500000_mean = 0.0591728, validate_confusion_matrix = [[84052   699]\r\n [14430   819]], validate_streaing_precision = 0.580026, validate_streaing_recall = 0.0591728, **validate_streaming_auc = 0.525859**\r\nWARNING:tensorflow:Skipping summary for validate_confusion_matrix, must be a float or np.float32.\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\nINFO:tensorflow:Validation (step 40000): loss = 0.39175, accuracy = 0.85421, labels/prediction_mean = 0.146081, labels/actual_label_mean = 0.14821, accuracy/baseline_label_mean = 0.14821, auc = 0.686321, accuracy/threshold_0.500000_mean = 0.85421, precision/positive_threshold_0.500000_mean = 0.580026, recall/positive_threshold_0.500000_mean = 0.0591728, validate_confusion_matrix = [[84052   699]\r\n [14430   819]], validate_streaing_precision = 0.580026, validate_streaing_recall = 0.0591728, validate_streaming_auc = 0.525859, global_step = 39807\r\n\r\nWhy are this two AUC differs so much? When i use model.evaluate on train, validate, test data set, all the output AUCs are very close to the **first** auc shows in bold above. \r\nSo what is the validation_streaming_auc is calculating? I have reset the local variables in each epoch.", "All the metrics are streaming, which is why we removed the name. The non-streaming use case is typically trivial to implement (see @sguada's example). It is also almost entirely useless if you use these in evaluation, since you're almost never interested in a single batch result. \r\n\r\n@lancerts, the ValidationMonitor will start a completely new evaluation each time, which should reset the state. \r\n\r\nI will close this issue -- this is working as intended. If you have a specific feature request, please file a new issue. Thanks!", "When running @gongzhitaao code snippet at the top multiple times, I sometimes get different outputs for `acc`.\r\n\r\n```python\r\nv = sess.run([acc, acc_op], feed_dict={x: [1, 0, 0, 0, 0],\r\n                                       y: [1, 0, 0, 0, 1]})\r\nprint(v)\r\n# [0.0, 0.8] # shouldn't the first be 0.8 as well?\r\n# [0.8, 0.8] # running this several times will sometimes produce this output, but less frequently\r\n```\r\nI have this code running in standalone script, each run is completely independent from the previous run.\r\n\r\nAny ideas anyone?", "The fuzzy function behavior is sucks!", "@kashefy If you execute both `acc` and `acc_op` (the latter being the update op), in the same `session.run` call, the execution order of the two is undefined. \r\n\r\nIf `acc_op` is executed first, you will see `[0.8, 0.8]`, if `acc` is executed first, you will see `[0.0, 0.8]`.\r\n\r\nThis is turning into more of a StackOverflow discussion, I suggest we take additional questions there.", "For people who are still looking to reset and manage the tensorflow variables: `total` and `count` in the `tf.metrics.accuracy` , please refer to Ronny Restrepo's blog post of how to do that: \r\n\r\n_Avoiding headaches with tf.metrics_ Sept. 11, 2017 [http://ronny.rest/blog/post_2017_09_11_tf_metrics/](url) \r\n\r\nI find he provides a nice intuition as well on the `accuracy` and `update_op`.", "It looks like the article linked by @Knight-H is now moved to https://steemit.com/machine-learning/@ronny.rest/avoiding-headaches-with-tf-metrics\r\n\r\nTo add to the article, Tensorflow also automatically adds the local variables of tf.metrics to the collection tf.GraphKeys.METRIC_VARIABLES. Sadly this fact is not mentioned in the current documentation.", "For simple, one-batch accuracy: \r\n\r\n```\r\ndef non_streaming_accuracy(predictions, labels):\r\n    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\r\n\r\n```\r\n@sguada's answer uses deprecated functions. ", "> @kashefy If you execute both `acc` and `acc_op` (the latter being the update op), in the same `session.run` call, the execution order of the two is undefined.\r\n> \r\n> If `acc_op` is executed first, you will see `[0.8, 0.8]`, if `acc` is executed first, you will see `[0.0, 0.8]`.\r\n> \r\n> This is turning into more of a StackOverflow discussion, I suggest we take additional questions there.\r\n\r\nSorry but this does not make sense to me the acc and update_op they are both part of the computational graph and no matter where we put them in the run call, ie. session.run([acc, update_op]) vs  session.run([update_op, acc]), the order of execution will be the same, i'll get surprised if it is the other way. In my view this is a bug."]}, {"number": 9497, "title": "small change to allow multiple wrappings with EmbeddingWrapper", "body": "Hi,\r\nI tried wrapping an LSTM in an attention wrapper followed by an embedding wrapper, however I got an error here as state[0] is still a tuple so i wrote this small change to fix the problem", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@ebrevdo can you approve or request further changes?", "this PR is to a release branch.\r\nWe do not accept contributions directly into release branches.\r\nPlease recreate the PR against TF master branch."]}, {"number": 9496, "title": "icc Compilation Errors in tf1.0: usage of \"typename\" on TTypes", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.0\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: not configured\r\n- **GPU model and memory**: not configured\r\n- **Exact command to reproduce**: `CC=icc bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nWhen compiling tensorflow using icc (version 17.0.3 20170404) using the command above, errors appear concerning the usage of `typename` associated with `TTypes`. For instance: \r\n\r\n> tensorflow/core/kernels/depthtospace_op.cc(88): error: type name is not allowed\r\n      typename TTypes<T, 4>::ConstTensor Tinput = input.tensor<T, 4>();\r\n\r\nA more constrained example, however, makes the problem more apparent. Command:\r\n`CC=icc bazel build -c opt //tensorflow/core/kernels:resize_nearest_neighbor_op`\r\nOutput (partial):\r\n> tensorflow/core/kernels/resize_nearest_neighbor_op.cc(57): error: type name is not allowed\r\n      typename TTypes<T, 4>::ConstTensor input_data = input.tensor<T, 4>();\r\n\r\nI can change line resize_nearest_neighbor_op.cc(57) to:\r\n`TTypes<T, 4>::ConstTensor input_data = input.tensor<T, 4>();`\r\nand repeat the command to get Output (partial):\r\n> tensorflow/core/kernels/resize_nearest_neighbor_op.cc(57): error: use the \"typename\" keyword to treat nontype \"tensorflow::TTypes<T, NDIMS, IndexType>::ConstTensor [with T=T, NDIMS=4, IndexType=Eigen::DenseIndex={std::ptrdiff_t={long}}]\" as a type in a dependent context\r\n      TTypes<T, 4>::ConstTensor input_data = input.tensor<T, 4>();\r\n\r\nI do not know if Intel Compiler compatibility is an intended feature of tensorflow, but I hoped this issue would still be of interest to the developers.\r\nThank you!", "comments": ["That seems like a bug in icc, given that `ConstTensor` is definitely a type.  However, we'd be happy to accept patches if you know how to fix it.  Removing the typename is not an option since the C++ standard requires it.", "I now agree that the change I attempted is incorrect c++. I was confused by \"type name\" appearing in the error message.\r\nAs far as I can tell, the correct version of this line is `typename TTypes<T, 4>::ConstTensor input_data = input.template tensor<T, 4>();`\r\nThat compiles under gcc and icc. The versions of this issue that appear in the codebase are `.tensor<...>()`, `->tensor<...>()`, and `.shaped<...>()` Correcting them all with a script allows the original command to complete without errors. I will submit a patch soon.", "If you're planning to submit a patch, let's leave this issue open to track.", "I've finished testing and am satisfied that the change as described above, applied globally, yields correct code. I won't be able to submit a patch for a little while due to other priorities, but if anyone is interested, the following script makes the necessary changes when run from the repository's root directory.\r\n```bash\r\n#!/bin/bash\r\n\r\nfor f in $(find ./tensorflow/ -type f)\r\ndo\r\n  perl -pi -e 's/\\.tensor</\\.template tensor</g' \"${f}\"\r\n  perl -pi -e 's/->tensor</->template tensor</g' \"${f}\"\r\n  perl -pi -e 's/\\.shaped</\\.template shaped</g' \"${f}\"\r\ndone\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9495, "title": "Inconsistent results when tf.sqrt() is applied to tensor versus element-wise ", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: fedora 24\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\nstart = 0 \r\nlinear_indices = (tf.range(start, 496) + 1) * 8 + 1 \r\nindex = 495 - start  \r\n\r\nrind  = tf.sqrt(tf.cast(linear_indices, tf.float32)) - 63  \r\nrind1 = tf.gather(rind, index)\r\n\r\nlinear_index = tf.gather(linear_indices, index)\r\nrind2 = tf.sqrt(tf.cast(linear_index, tf.float32)) - 63  \r\n\r\nsession = tf.Session()\r\nprint(session.run(rind1))\r\nprint(session.run(rind2))\r\n```\r\nOutput:\r\n```\r\nIn [5]: %run calculation_test.py\r\n3.8147e-06\r\n0.0\r\n```\r\n### Describe the problem\r\nThe order of operations gives different behavior in tf.gather. `rind1` and `rind2` tensors should have the same value (0).\r\n\r\ncc @altosaar", "comments": ["I don't see how those two computations are supposed to be the same. rind2 is a tf.sqrt(...) and rind1 is a gather. One is going through more (inexact) floating point arithmetic and one is not. So 3.8147e-6 is almost zero and 0.0 is zero. So they seem the same to me. Could you please explain your example more clearly.\r\n\r\nAnd also, what version of tesnorflow are you using?", " tf.sqrt(...) is an element-wise operation, so when it is applied to an array, it should be as if it is applied element by element. In this sense, the two result should be exactly the same. \r\n\r\nI am using tf 1.11\r\n\r\nThanks for look into this problem", "New finding: I think the problem is in tf.sqrt(...)\r\nHere is the code, taking sqrt of the array versus taking sqrt element by element, \r\n```\r\nimport tensorflow as tf\r\n\r\narr = tf.range(0, 1000, dtype=tf.float32)\r\n\r\nresult1 = tf.sqrt(arr)      # taking sqrt of the array\r\n\r\n\r\nalist = tf.unstack(arr)    # unpack\r\nrlist = []\r\nfor elm in alist:              # taking sqrt element by element, \r\n    rlist.append(tf.sqrt(elm))\r\nresult2 = tf.stack(rlist)  # then pack the results into an array \r\n\r\ndiff = tf.reduce_sum(tf.abs(result1 - result2))\r\n\r\nsession = tf.Session()\r\nprint('The difference is %f ' % session.run(diff))\r\n```\r\nand the output is, \r\n```\r\nThe difference is 0.001230\r\n```\r\n\r\nI expect the output to be 0. \r\n\r\ncc @altosaar", "Are you running this using a GPU or CPU? If GPU then try on the CPU and see if you get the same behavior. Please report more details about your GPU/system/etc. ", "The result is obtained from a CPU. If you need cpu info, here it is, \r\nprocessor   : 0 \r\nvendor_id   : GenuineIntel\r\ncpu family  : 6 \r\nmodel       : 78\r\nmodel name  : Intel(R) Core(TM) i7-6600U CPU @ 2.60GHz\r\n", "This test is not meaningful, you are summing errors of numbers that are not of similar magnitude...\r\n```\r\nimport tensorflow as tf\r\n\r\narr = tf.range(0, 1000, dtype=tf.float32)\r\n\r\nresult1 = tf.sqrt(arr)      # taking sqrt of the array\r\n\r\n\r\nalist = tf.unstack(arr)    # unpack\r\nrlist = []\r\nfor elm in alist:              # taking sqrt element by element, \r\n    rlist.append(tf.sqrt(elm))\r\nresult2 = tf.stack(rlist)  # then pack the results into an array \r\n\r\ndiff = tf.reduce_max(tf.abs(result1 - result2)/tf.abs(result1))\r\n\r\nsession = tf.Session()\r\nprint('The difference is %f ' % session.run(diff))\r\n\r\n```\r\nThis computes the max relative error over all elements and I get 0.0000002422335683 i.e. 2e-7 which is about floating point precision. The exact path in the cpu is different for small arrays because large arrays are done with sse for speed. ", "I see. It is interesting that CPU has different behaviors for large arrays and small arrays. \r\nI'm satisfied with this explanation. Thanks!\r\n\r\ncc @altosaar"]}, {"number": 9494, "title": "graph_transforms tool obfuscate_names won't work", "body": "### System information\r\n\r\n- **OS Platform and Distribution : Ubuntu 16.04**\r\n**GIT_VERSION: v1.1.0-rc0-61-g1ec6ed5**\r\n**Tensorflow Version: 1.1.0**\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=MYMODEL.pb \\\r\n--out_graph=MYMODEL_OPT.pb \\\r\n--inputs='input_feed:0,Squeeze_1:0,lstm/state_feed:0' \\\r\n--outputs='lstm/initial_state:0,softmax:0,lstm/state:0' \\\r\n--transforms='\r\n  obfuscate_names'\r\n```\r\n\r\n\r\n\r\n\r\n\r\nBug: Model as follows (Inception V3) -> (LSTM)\r\nI used the graph_transform tool to obfuscate names using the obfuscate_names command for android deployment. model seems to work fine on android but when I try to obfuscate names problems start to surface.\r\n\r\n\r\nlogs\r\n```\r\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Node 'rY' expects to be colocated with unknown node 'logits/biases'\r\n                                                                                 at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:392)\r\n                                                                                 at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:96)\r\n                                                                                 at com.example.thisismohit.local.MainActivity.onCreate(MainActivity.java:158)\u00a0\r\n                                                                                 at android.app.Activity.performCreate(Activity.java:6283)\u00a0\r\n                                                                                 at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1119)\u00a0\r\n                                                                                 at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2646)\u00a0\r\n                                                                                 at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2758)\u00a0\r\n                                                                                 at android.app.ActivityThread.access$900(ActivityThread.java:177)\u00a0\r\n                                                                                 at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1448)\u00a0\r\n                                                                                 at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n                                                                                 at android.os.Looper.loop(Looper.java:145)\u00a0\r\n                                                                                 at android.app.ActivityThread.main(ActivityThread.java:5942)\u00a0\r\n                                                                                 at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                                                                                 at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\r\n                                                                                 at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1400)\u00a0\r\n                                                                                 at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1195)\u00a0\r\n\r\n```", "comments": ["@petewarden Do you know what this error means?", "I think the issue is that the old node names (before the obsfucation) are still being used for colocation attributes. Since collocation not usually needed on mobile devices, you could try also running a graph transform to get rid of the attribute that defines it. Something like\r\n\r\n--transforms='remove_attribute(attribute_name=_class)'\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#remove_attribute\r\n\r\nLet me know if that workaround helps. The real fix is to update the node names in the attribute to the obsfucated versions, but that would be fairly involved.", "remove_attribute(attribute_name=_class) works automagically. Thanks..."]}, {"number": 9493, "title": "Merge1.1", "body": "Merge 1.1 back into master.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 9492, "title": "make configure smarter in detecting cuda", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9491, "title": "make configure smarter in detecting cuda", "body": "", "comments": ["Can one of the admins verify this patch?", "There is a conflicts in merging, I will rebase my patch on the most recent master."]}, {"number": 9490, "title": "Cleanup label mapping code in ios_example camera app", "body": "/CC @petewarden ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9489, "title": "failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n```\r\nLinux 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n- **TensorFlow version (use command below)**:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nv1.0.0-65-g4763edf-dirty 1.0.1\r\n```\r\n- **CUDA/cuDNN version**:  8.0\r\n\r\n- **GPU model and memory**:\r\n```\r\nname: GeForce GTX 980\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2785\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 145.50MiB\r\n```\r\n\r\n### Describe the problem\r\nIf I change the order of device usage, it would report error \r\n\r\n### Source code / logs\r\n- If I use GPU first then CPU, it would be fine\r\n```\r\nwith tf.device('/gpu:0'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\nwith tf.device('/cpu:0'):\r\n    e = tf.constant([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], shape=[2, 9],dtype=tf.float32, name='e')\r\n    f = tf.matmul(c,e)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(f))\r\n```\r\n- But if I use CPU first then GPU, it return error\r\n```\r\nwith tf.device('/cpu:0'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\nwith tf.device('/gpu:0'):\r\n    e = tf.constant([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18], shape=[2, 9],dtype=tf.float32, name='e')\r\n    f = tf.matmul(c,e)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(f))\r\n```\r\n- the error dump below\r\n```\r\n    print(sess.run(f))\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(2, 2), b.shape=(2, 9), m=2, n=9, k=2\r\n         [[Node: MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](MatMul/_1, e)]]\r\n\r\nCaused by op 'MatMul_1', defined at:\r\n  File \"m1_n0teb00k/tensorflow_palyground.py\", line 13, in <module>\r\n    f = tf.matmul(c,e)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1765, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1454, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(2, 2), b.shape=(2, 9), m=2, n=9, k=2\r\n         [[Node: MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](MatMul/_1, e)]]\r\n```", "comments": ["Check out #5354, #3600, #4196,\r\nhttp://stackoverflow.com/questions/38303974/tensorflow-running-error-with-cublas\r\nLet me know if those don't provide solution to your problem.", "I have the same problem, but the advice did not solve it for me. BTW, CUDA 8.0 is work well for my computer.\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=10816, n=64, k=128", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "This seems to possibly be an out-of-memory being masked by the CUBLAS_STATUS_NOT_INITIALIZED error. When I have low memory and ask for a new session for detection I hit this error, when I clear the gpu of other processes and free memory then I do not get the cublas error.  Since by default it seems tensorflow sucks up nearly all available memory this could happen a lot I imagine.\r\n\r\n\r\n```\r\nroot@746bb093df44:/data/jeremy/tensorflow/models# python /usr/lib/python2.7/dist-packages/variant/ml/tf_detect.py\r\ncwd /data/jeremy/tensorflow/models/object_detection\r\nusing gpu0\r\n2017-09-23 14:37:50.992104: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-23 14:37:50.992170: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-23 14:37:51.072360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-23 14:37:51.072588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: Quadro P6000\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.645\r\npciBusID 0000:00:05.0\r\nTotal memory: 23.87GiB\r\nFree memory: 805.44MiB\r\n2017-09-23 14:37:51.072625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-09-23 14:37:51.072643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-09-23 14:37:51.072664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P6000, pci bus id: 0000:00:05.0)\r\n2017-09-23 14:37:56.768781: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-09-23 14:37:56.768888: W tensorflow/stream_executor/stream.cc:1756] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/dist-packages/variant/ml/tf_detect.py\", line 264, in <module>\r\n    test_detect()\r\n  File \"/usr/lib/python2.7/dist-packages/variant/ml/tf_detect.py\", line 123, in test_detect\r\n    feed_dict={image_tensor: image_np_expanded})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=36300, n=80, k=64\r\n\t [[Node: FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Conv2d_3b_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/MaxPool_3a_3x3/MaxPool, FirstStageFeatureExtractor/InceptionResnetV2/Conv2d_3b_1x1/weights/read)]]\r\n\t [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame_52/Scale/concat/_391 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_21463_SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame_52/Scale/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Conv2d_3b_1x1/convolution', defined at:\r\n  File \"/usr/lib/python2.7/dist-packages/variant/ml/tf_detect.py\", line 70, in <module>\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : m=36300, n=80, k=64\r\n\t [[Node: FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/Conv2d_3b_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](FirstStageFeatureExtractor/InceptionResnetV2/InceptionResnetV2/MaxPool_3a_3x3/MaxPool, FirstStageFeatureExtractor/InceptionResnetV2/Conv2d_3b_1x1/weights/read)]]\r\n\t [[Node: SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame_52/Scale/concat/_391 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_21463_SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/ChangeCoordinateFrame_52/Scale/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```", "This didnt work for me. \r\n\r\nI have checked all my installation. Made sure my LD_LIBRARY_PATH is what it should be. Kept my memory down to a minimum. Nothing seems to be working. ", " python tf_p11_RNN.py\r\n\r\n\r\n2017-12-30 21:18:15.192881: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.195474: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.204124: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.205339: W tensorflow/stream_executor/stream.cc:1901] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"tf_p11_RNN.py\", line 60, in <module>\r\n    train_neural_network(x)\r\n  File \"tf_p11_RNN.py\", line 52, in train_neural_network\r\n    _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(128, 156), b.shape=(156, 512), m=128, n=512, k=156\r\n\t [[Node: rnn/rnn/basic_lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/rnn/basic_lstm_cell/concat, rnn/basic_lstm_cell/kernel/read)]]\r\n\t [[Node: Mean/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5059_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nI am getting the same error. But when I run as sudo there is no error. Can someone tell me why?\r\n eg: sudo python tf_p11_RNN.py\r\n", "Full log:\r\n2017-12-30 21:18:14.369282: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-12-30 21:18:14.444577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-12-30 21:18:14.444836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.455\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.94GiB freeMemory: 187.62MiB\r\n2017-12-30 21:18:14.523451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-12-30 21:18:14.530349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.4175\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 3.94GiB freeMemory: 256.88MiB\r\n2017-12-30 21:18:14.530392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\r\n2017-12-30 21:18:14.530422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 \r\n2017-12-30 21:18:14.530431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y N \r\n2017-12-30 21:18:14.530437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   N Y \r\n2017-12-30 21:18:14.530445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2017-12-30 21:18:14.530452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2017-12-30 21:18:14.531278: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 187.62M (196739072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-12-30 21:18:14.943584: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.946589: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.949442: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.952231: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.955194: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.959198: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.962078: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.964886: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.967683: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.970563: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.974798: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.977768: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.980869: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.983796: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.986634: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.989471: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.992614: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.995469: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:14.998239: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.001054: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.004009: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.006976: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.009987: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.012815: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.015577: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.018347: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.021108: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.023882: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.026638: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.029665: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.032291: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.035383: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.038359: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.041578: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.044224: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.047303: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.049913: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.052991: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.055599: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.058689: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.061304: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.064366: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.066979: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.070364: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.073239: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.076358: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.079006: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.082298: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.085009: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.088458: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.091159: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.094216: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.096819: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.099908: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.102792: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.106097: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.108747: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.111813: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.114407: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.117416: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.120014: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.122989: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.125596: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.128589: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.131208: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.134786: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.137745: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.140828: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.143435: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.146458: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.149067: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.152076: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.154658: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.157716: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.160319: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.163359: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.166263: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.169436: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.172070: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.175116: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.177728: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.180721: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.183997: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.187184: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.189828: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.192881: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.195474: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.204124: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-12-30 21:18:15.205339: W tensorflow/stream_executor/stream.cc:1901] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"tf_p11_RNN.py\", line 60, in <module>\r\n    train_neural_network(x)\r\n  File \"tf_p11_RNN.py\", line 52, in train_neural_network\r\n    _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(128, 156), b.shape=(156, 512), m=128, n=512, k=156\r\n\t [[Node: rnn/rnn/basic_lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/rnn/basic_lstm_cell/concat, rnn/basic_lstm_cell/kernel/read)]]\r\n\t [[Node: Mean/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5059_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op u'rnn/rnn/basic_lstm_cell/MatMul', defined at:\r\n  File \"tf_p11_RNN.py\", line 60, in <module>\r\n    train_neural_network(x)\r\n  File \"tf_p11_RNN.py\", line 39, in train_neural_network\r\n    prediction = recurrent_neural_network(x)\r\n  File \"tf_p11_RNN.py\", line 30, in recurrent_neural_network\r\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 1253, in static_rnn\r\n    (output, state) = call_cell()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 1240, in <lambda>\r\n    call_cell = lambda: cell(input_, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 441, in call\r\n    value=self._linear([inputs, h]), num_or_size_splits=4, axis=1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\r\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(128, 156), b.shape=(156, 512), m=128, n=512, k=156\r\n\t [[Node: rnn/rnn/basic_lstm_cell/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/rnn/basic_lstm_cell/concat, rnn/basic_lstm_cell/kernel/read)]]\r\n\t [[Node: Mean/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5059_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n\r\n", "I fixed this by running as root.", "As @jeremy-rutman said, this is likely an out-of-memory issue. When I get this error, I can fix it every time by unplugging my external monitor. This frees up GPU memory immediately, and I'm then able to run predictions with TF. Another fix that seems to work is if I close my million Chrome tabs.\r\n\r\nNote that running as root did not fix it for me.", "@DhashS It work with me. Thanks so much!", "In my case it was caused by running it under Windows with \r\n`session_config.gpu_options.allow_growth = True`\r\nwithout this I got another error, that was masked by \r\n` failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED`", "I solved this problem by cleaning mt device memory.", "I :+1: the out of memory error ! Same for me, cleaning up worked !", "In my case it was not a memory error, I fixed it by removing the cache folder `~/.nv/`", "Thanks @stbnps !\r\nI had the issue since a year almost and never found a better solution than running in root.\r\nThis directly solved my problem and I don't see drawbacks until now.\r\n\r\n", "Thanks @stbnps !\r\n\r\nThis has been frustrating me for a while and I could not find a way to run as a user. Removing .nv folder fixed this.", "@DhashS    Thank you so much! Running as root solve to my problem too!!!! ", "`2019-02-11 16:42:08.094047: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.101327: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.101418: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.101615: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.101720: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.102197: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.103385: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.104762: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.104859: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.105022: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.105433: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.105519: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.105676: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.106412: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.106499: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.106657: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.107383: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.107484: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.107643: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.108376: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.108460: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.108614: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.109330: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.109414: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.109581: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.110328: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.110414: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.110580: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.111328: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.111415: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.111605: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.112232: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.112308: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.112436: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.113061: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.113125: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.113253: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.113871: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.113948: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.114089: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.114680: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.114756: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.114885: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.115542: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.115621: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.115766: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.116345: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.116420: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.116550: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.117171: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.117259: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.117402: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.118083: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.118165: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.118299: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.118901: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.118977: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.119106: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.119737: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.119833: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.119962: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.120649: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.120728: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.120858: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.121487: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.121563: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.121691: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122289: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122336: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122395: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122427: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122497: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122542: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122602: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122634: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122694: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122727: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122799: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122830: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122900: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.122931: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123011: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123042: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123101: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123133: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123191: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123223: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123281: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123312: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123370: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123400: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123508: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123543: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123605: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123637: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123695: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123727: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123818: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123851: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123908: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123939: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.123997: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124029: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124086: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124117: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124449: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124484: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124529: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124560: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124743: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124810: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124914: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124950: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.124993: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125016: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125133: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125200: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125310: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125334: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125391: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125488: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125521: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125582: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125680: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125712: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125755: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125778: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125890: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125933: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.125957: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126070: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126113: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126147: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126276: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126300: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126341: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126365: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126489: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126550: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126658: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126682: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126723: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126746: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126871: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.126933: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127350: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127376: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127440: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127552: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127577: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127637: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127734: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127758: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127809: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127831: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127942: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.127985: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128007: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128137: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128161: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128202: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128224: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128352: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128396: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128419: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128530: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128574: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128652: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128828: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128852: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128894: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.128936: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129078: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129103: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129123: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129143: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129184: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129335: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129577: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129602: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129623: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129652: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129672: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129692: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129714: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129734: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129754: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129782: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129802: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129822: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129970: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.129993: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.130048: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.130382: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.130688: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.130802: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.131137: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.131180: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.131378: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.131546: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.131786: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.131895: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.132214: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.132258: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.132455: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.132634: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.132884: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.132993: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.133313: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.133356: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.133551: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.133711: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.133943: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.134050: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.134379: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.134431: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.134626: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.134804: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.135074: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.135191: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.135561: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.135615: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.135829: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.136008: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.136240: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.136347: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.136664: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.136707: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.136903: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.137078: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.137309: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.137414: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.137790: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.137845: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.138066: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.138253: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.138479: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.138585: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.138909: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.138963: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.139151: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.139324: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.139574: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.139691: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.140016: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.140059: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.140252: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.140448: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.140677: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.140783: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.141096: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.141139: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.141347: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.141534: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.141763: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.141870: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.142180: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.142224: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.142416: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.142589: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.142863: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.142998: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.143331: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.143374: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.143604: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.143795: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.144019: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.144125: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.144437: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.144480: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.144671: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.144842: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.145070: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.145176: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.145485: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.145527: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.145749: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.145940: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.146176: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.146282: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.146593: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.146636: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.146826: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.146997: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.147220: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.147326: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.147694: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.147739: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.147956: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.148159: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.148412: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.148519: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.148843: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.148887: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.149081: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.149268: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.149508: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.149627: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.149936: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.149979: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.150171: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.150344: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.150566: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.150672: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.151034: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.151091: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.151298: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.151482: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.151715: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.151822: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.152163: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.152188: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.152417: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.152609: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.152871: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.152914: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153053: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153094: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153277: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153309: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153541: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153574: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.153708: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.155756: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.155960: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.156480: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-11 16:42:08.156626: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nFloating point exception (core dumped)\r\n`", "I have tried 'rm ~/.nv' and 'config = tf.ConfigProto() config.gpu_options.allow_growth = True', no one work.", "> In my case it was not a memory error, I fixed it by removing the cache folder `~/.nv/`\r\n\r\n+1 - worked on linux", "Thanks @stbnps. Your solution worked like a charm.", "Was masking an out of memory error for me", "Had the same problem with `libcublas10=10.2.2.89-1`, solved it by downgrading to `10.1.0.105-1`", "Observed the similar problem with `libcublas10=10.2.2.89-1` / `libcublas-dev=10.2.2.89-1`.\r\nDowngrading to either `libcublas10=10.2.1.243-1` / `libcublas-dev=10.2.1.243-1` or `libcublas10=10.1.0.105-1` / `libcublas-dev=10.1.0.105-1` + `export TF_FORCE_GPU_ALLOW_GROWTH=true` ([Limiting GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)) has fixed the issue.\r\n`~/.nv` was empty.", "Same issue with PyTorch after nvidia `10.1-cudnn7-runtime-ubuntu18.04` images was updated 13 days ago.  Has `libcublas.so.10.2.2.89`, downgrade to `libcublas.so.10.2.1.243` and problem goes away.\r\n", "awesome, just found this and @angerson fix works for me! thanks!", "It is still broken for TF2.1rc2, and I confirmed that adding TF_FORCE_GPU_ALLOW_GROWTH=true to docker file doesn't solve the issue. \r\nAlso confirmed that changing base docker image to nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 and installing tensorflow-gpu==2.1.0rc2 works as expected.\r\nProbably need to downgrade cuda to 10.1", "Looks like latest change wasn't pushed to docker hub:\r\nhttps://hub.docker.com/layers/tensorflow/tensorflow/2.1.0rc2-gpu/images/sha256-e328b120aba6af551e1b9124cd7eeee3f05e4d41d115c139614f1ca4350c7953\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile\r\n\r\nMeaning that this will work if you build from docker file, but won't work if you pull container from docker hub. Probably not a huge issue sine TF2.1 is going to be released this week.", "I add `os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"` to the code and solve this problem.", "Had to downgrade to tensorflow version 2.0 `pip install tensorflow-gpu==2.0`", "@samon11 thanks a bunch, went around many threads to solve this problem, yours solved mine. Bless you, have a nice day", "I am experiencing this too, on tensorflow 2.1. A short bit of code to check if the problem persists is:\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\n```\r\nAttempting downgrade now.", "CUBLAS_STATUS_NOT_INITIALIZED can mask multiple issues, quite commonly is it a bad setup, especially when it crashes on initialization. I'd be nice to have a better error message.\r\n\r\nTF2.1 is using CUDA10.1 while TF2.0 is on CUDA10.0. So in some cases just updating TF2.0 -> T2.1 won't work, you'd also need to update CUDA and other libraries, and even possibly GPU drivers.\r\n\r\nThe easiest way to make sure that setup is correct:\r\nMake sure that you have https://github.com/NVIDIA/nvidia-docker installed.\r\nRun\r\ndocker run --gpus all tensorflow/tensorflow:2.1.0-gpu-py3 python -c 'import tensorflow as tf; from tensorflow.python.client import device_lib; device_lib.list_local_devices()'\r\n\r\nHope this helps.", "I wonder why it is not possible to emit a more useful, detailed error message?", "Bad error reporting? Yes, of course.", "The compatibility issue between CUDA version and TensorFlow version. In my case, My CUDA version is 10.0 and TensorFlow version is 2.1.0, and this issue occurs. After changing TensorFlow 2.1.0 to TensorFlow 2.0.0, this issue disappears.\r\n\r\nOr maybe this is the issue of out of memory, so try to reduce the batch size", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nThis in fact is still an issue, I followed [TF GPU install](https://www.tensorflow.org/install/gpu) for 18.04/CUDA 10.1, and for whatever reason the default linked library for libcublas10 is still libcublas10=10.2.2.89-1. So whatever pin that was done above did not work.\r\n\r\nFor others who encounter this cublas init error, do this:\r\n1. first check libcublas10  version:\r\n`dpkg -l | grep libcublas`\r\n2. if version is 10.2.2.89, remove it:\r\n`sudo apt remove libcublas10`\r\n3. install downgraded version:\r\n`sudo apt install libcublas10=10.2.1.243-1`\r\n4. it might say that it has to remove a bunch of cuda packages when you downgrade, I thought it might be an issue, but it did not impact my CUDA 10.1 installation.", "@SkookumAsFrig Thanks for your solution! Downgrading libcublas worked for me.\r\n\r\n", "Also maybe not directly relevant to cublas package version, but I did an apt upgrade today and it obliterated the tensorflow GPU packages in terms of compatibility. The ones impacted were libcublas10 and libcudnn (went from 6 to 7 cuda 10.2), even though I am on 10.1. So I thought I would share some useful commands for fixing things like this:\r\n1. check package version, say we are fixing libcudnn:\r\n`dpkg -l | grep libcudnn`\r\n2. remove incorrectly installed new ones:\r\n`sudo apt remove libcudnn7 libcudnn7-dev libcudnn7-doc`\r\n3. search for available libcudnn packages:\r\n`sudo apt-cache policy libcudnn7`\r\n4. install the correct one, for your CUDA version (don't forget the -dev version too):\r\n`sudo apt install libcudnn7=7.6.5.32-1+cuda10.1`\r\n5. put these annoying packages on hold:\r\n`sudo apt-mark hold libcublas10`\r\nreference links:\r\nhttps://chadrick-kwag.net/downgrading-cudnn-version-to-match-cuda-version/\r\nhttps://askubuntu.com/questions/18654/how-to-prevent-updating-of-a-specific-package", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9489\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9489\">No</a>\n", "i found another way to solve this problem   , i think the problem is that GPU havent been completely released  .\r\nso first  check the nvidia used by all the processing \r\nlsof /dev/nvidia*\r\n\r\nlsof: WARNING: can't stat() tracefs file system /sys/kernel/debug/tracing\r\n      Output information may be incomplete.\r\nlsof: WARNING: can't stat() fuse.gvfsd-fuse file system /run/user/108/gvfs\r\n      Output information may be incomplete.\r\nCOMMAND     PID USER   FD   TYPE  DEVICE SIZE/OFF  NODE NAME\r\nt2t-train 24674 dell  mem    CHR 195,255          28064 /dev/nvidiactl\r\nt2t-train 24674 dell  mem    CHR   195,0          17699 /dev/nvidia0\r\nt2t-train 24674 dell  mem    CHR   241,0          32956 /dev/nvidia-uvm\r\nt2t-train 24674 dell  mem    CHR   195,1          44248 /dev/nvidia1\r\nt2t-train 24674 dell  mem    CHR   195,2           2640 /dev/nvidia2\r\n\r\nand then  kill all the process \r\nkill -9 24674 \r\nand restart your work again  \r\n\r\ngood luck", "Heads up, if you're facing this problem in a conda env, make sure to pin the conda package `cudatoolkit=10.1.243`. We faced an issue where pytorch (also in our env) was installing a more recent cudatoolkit, and that was causing this CUBLAS_STATUS_NOT_INITIALIZED error when running tensorflow 2.1 and 2.2.", "@ankitvgupta Didn't work for me with `tensorflow-gpu` installed with conda on an RTX 2080 Max-Q. From my experience, this bug manifest itself mostly on RTX cards. With exactly the same configuration on a Tesla T4 GPU everything is fine.", "Got it, thanks for the heads up. Yeah, I tried it on a K80 (I can never remember if that's considered an RTX or not) on AWS (built off their deep learning base AMI on a p2), in case that's useful for repro. I suspect it's also pretty specific to whatever random other packages are being installed in my dockerfile and conda env, but figured I'd share just in case others were facing it.", "@ankitvgupta I'm trying to recompile from source with CUDA 10.1.243 right now for my RTX card, will let you know.\r\n\r\nK80 is not RTX, so maybe that's why it's working for you.\r\nI used exactly the same env between the T4 GPU (drivers 440.64) and my RTX card (drivers 440.82) with a docker image. Working on T4, not on RTX. They are both based on the Turing architecture so I don't really know what is happening here. I'll try to use the drivers 440.64 to see if anything changes.", "Ok somehow I can't install `440.64` on my RTX card so I can't align my drivers with the T4... and ofc `440.82` is not available for the T4. I'll try to use the latest version of CUDA 10.1/cudnn 7.5 with TF compiled from source on my RTX laptop.", "Nothing changes when compiling TF from source on CUDA `10.1.243`. So in the meantime I'm using this workaround:\r\n```\r\ndef is_rtx_card():\r\n    for device in device_lib.list_local_devices():\r\n        if device.device_type == \"GPU\" and \"RTX\" in device.physical_device_desc:\r\n            return True\r\n    return False\r\n\r\n\r\n# TF 2.2 is not working with RTX cards without this workaround...\r\nif is_rtx_card():\r\n    print(\"WARNING: RTX card detected! Using experimental device growth!\")\r\n    tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)\r\n```\r\n\r\nWhich work **if** you compiled TF 2.2 from source with CUDA 10.2 (doesn't work with CUDA 10.1.x)", "@EKami I'm a bit confused. Which part of your code block relies on CUDA 10.2? Is is specifically the memory growth that is bugged on older versions of CUDA or is it something else?", "> > It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n> \r\n> This in fact is still an issue, I followed [TF GPU install](https://www.tensorflow.org/install/gpu) for 18.04/CUDA 10.1, and for whatever reason the default linked library for libcublas10 is still libcublas10=10.2.2.89-1. So whatever pin that was done above did not work.\r\n> \r\n> For others who encounter this cublas init error, do this:\r\n> \r\n>     1. first check libcublas10  version:\r\n>        `dpkg -l | grep libcublas`\r\n> \r\n>     2. if version is 10.2.2.89, remove it:\r\n>        `sudo apt remove libcublas10`\r\n> \r\n>     3. install downgraded version:\r\n>        `sudo apt install libcublas10=10.2.1.243-1`\r\n> \r\n>     4. it might say that it has to remove a bunch of cuda packages when you downgrade, I thought it might be an issue, but it did not impact my CUDA 10.1 installation.\r\n\r\nthis method also works for me.\r\n\r\nmy Training env:\r\nos: ubuntu 16.04\r\nvideo card: GPU 0: GeForce GTX TITAN X (UUID: GPU-b27eec02-91c4-b1b7-7790-4e246f7e3cbe)\r\nframework version: tensorflow 2.3.0\r\ncuda version: 10.1.0\r\n\r\n", "TF 2.3 resolve the issue indeed, and no need for the `gpu_allow_growth` workaround either", "> TF 2.3 resolve the issue indeed, and no need for the `gpu_allow_growth` workaround either\r\n\r\nNo it didn't.", "> @EKami I'm a bit confused. Which part of your code block relies on CUDA 10.2? Is is specifically the memory growth that is bugged on older versions of CUDA or is it something else?\r\n\r\nIt's when you compile TF from source, you can pick whatever version of CUDA you want.", "Just an FYI. Solved this issue on TF 2.2.0, Cuda 10.1 by removing existing libcublas and installing a libcublas 10.2.1.243-1 instead. \r\nsample for Red Hat/CentOS:\r\n```\r\nyum remove libcublas\r\nyum install libcublas10-10.2.1.243-1.x86_64\r\n```", "In my case, I try to limit the GPU memory by adding these lines of code on top of my code. (tensorflow==2.4.0)\r\n`import tensorflow as tf\r\nfrom tensorflow.python.keras.backend import set_session\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)`", "Thanks @ngotra2710. I was able to bypass the problem with your solution (I'm testing [this](https://gist.github.com/68cdfd78149cbfcc0896a73244e4ecff) model).\r\n\r\nAnyhow it is strange, because it complains that it cannot allocate enough memory\r\n```\r\ntensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n```\r\n\r\nBut the card should have enough\r\n```\r\nGeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5                                                                                                     \r\ncoreClock: 1.335GHz coreCount: 24 deviceMemorySize: 5.80GiB\r\n```\r\n\r\n``nvtop`` shows how tensorflow request memory till it reaches 5.470Gi of the 5.805Gi available, then show the image and release the memory after closing.", "@sanjoy This OOM could be the same as the other issue Scott mentioned.", "I'm on cuda-11-0 and tensorflow 2.4 and got the same errror.\r\nFirst of all, removing the chache folder `~/.nv` did noting for me.\r\nSecondly, while \r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\ngot my code running, the actual error was that I had `libcudnn8=8.1.1.33` installed (which seems to rely on cuda-11-2). So downgrading libcudnn to 8.0.5.39 was the actual fix.", "it's a version match problem i meet it too. now solved as follows\r\ndriver: NVIDIA-Linux-x86_64-440.118.02.run\r\ncuda: cuda_10.2.89_440.33.01_linux.run\r\ncuda patch: cuda_10.2.2_linux.run\r\n\r\nlibcudnn: \r\nlibcudnn7_7.6.5.32-1+cuda10.2_amd64.deb\r\nlibcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb\r\nlibcudnn7-doc_7.6.5.32-1+cuda10.2_amd64.deb\r\n\r\npytorch:\r\npip3 install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2\r\n\r\n#do not install torch 1.8.0", "In my case with rtx3070 & tensorflow 2.4.1,\r\nbelow code worked for me\r\n\r\n```python\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\r\nconfig = tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```\r\n", "Got a same error on TF 2.5/CUDA 11.3. `set_memory_growth` doesn't work. TF 2.5/CUDA 10.1 works fine.", "@AxelBohm I got the same error and your codes work for me, though I have no idea what they mean.\r\nMy TF environment: TF version: 2.6.0, CUDA version: 11.2 and GPU driver version: 460.32.03.", "Another way to set auto-growth is: \r\n`os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'`\r\n", "Maybe this helps someone: `sudo apt-get remove libnvidia-encode-495` somehow did the trick for me. My code was working before I installed this, so I simply tried removing it again and it really was the culprit."]}]