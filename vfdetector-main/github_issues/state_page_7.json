[{"number": 55185, "title": "`tf.random.set_seed` lack input validation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nseed = None\r\ntf.random.set_seed(seed, )\r\na = tf.random.uniform([1, 2])\r\ntf.random.set_seed(seed, )\r\nb = tf.random.uniform([1, 2])\r\ntf.debugging.assert_near(a, b) # InvalidArgumentError\r\n```\r\n\r\n**Describe the current behavior**\r\n[`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) should be called with an integer, however, calling `tf.random.set_seed(None)` will not result in error, and it cannot guarantee reproducibility : `a` is not equal to `b`.\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.random.set_seed` should check properly that `seed` is an integer, and raise an error if `seed` is `None`.\r\n", "comments": ["@ArrowIntoTheSky Thanks for raising the issue. Added internal CL to check the input seed value and throw an error if it is `None`. Thanks"]}, {"number": 55184, "title": "Tensor evaluating to None", "body": "**System information**\r\n- TensorFlow version (you are using): TF1 and TF2\r\n- Are you willing to contribute it (Yes/No): Depends on the extent of the change necessary.\r\n\r\n**Describe the feature and the current behavior/state.** \r\nI have TF datasets (both TF1 and TF2) that I would like to transform (dataset.map). Transforms package the datasets into a new structure and this structure might contain some None elements. If I try to make elements None, I get some \"fetches errors\" saying None is not supported. I'm wondering if there is a value that would evaluate to None? If not, could we add support for one?\r\n\r\nAlternative would be to create an iterator and manually doing the map on fetched items, but that messes up external API (as we no longer work on tf.Datasets but Iterators instead).\r\n\r\n**Will this change the current api? How?**\r\nNot sure.\r\n\r\n**Who will benefit with this feature?**\r\nDeepMind internal.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @hamzamerzic ! Could you please share a use case or a simple code snippet that explain the above feature request?", "Thanks a lot for the quick reply! Here is an example:\r\n\r\n```\r\nfrom typing import NamedTuple, Optional\r\n\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nclass DatasetOutputs(NamedTuple):\r\n  image: np.array\r\n  label: np.array\r\n  metadata: Optional[np.array]\r\n\r\n\r\ndef _map_fn(data):\r\n  return DatasetOutputs(\r\n      image=tf.cast(data['image'], tf.float32) / 255.,\r\n      label=data['label'],\r\n      metadata=None)  # Problematic.\r\n\r\n\r\nds = tfds.load('mnist', split='train')\r\nds = ds.map(_map_fn)\r\ndata = next(iter(tfds.as_numpy(ds)))\r\n```\r\n\r\nThis gives an error\r\n`TypeError: Argument `fetch` = None has invalid type \"NoneType\". Cannot be None`\r\n\r\nThis simple reproducer seems to actually work well with TF2. But, in my particular use case the dataset that I'm trying to use seems to fail elsewhere when using TF2 though."]}, {"number": 55181, "title": "Make computation of higher order gradients through apply_gradients possible", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7 / 2.8\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, TF can not compute gradients through optimizer.apply_gradients calls - I suspect this would be caused by the underlying assign operation not being differentiable. However, this operation is in itself definitely differentiable and computing a gradient over this operation is a staple in multiple important works, especially in Meta-Learning and other fields actually making use of higher order gradients. \r\nCurrent behavior: While higher-order gradients can easily be implemented using e.g. nested gradient tapes (https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients), they can not be achieved through applying gradient updates - generally done via optimizer.apply_gradients(). \r\n\r\nWhile in theory one can create work-arounds by stitching together gradients by hand ( let 'theta' be parameters before update, theta_dash parameters after update, phi some term dependent on the updated parameters one wants the gradient for: [d_L/d_theta_dash] x [d_theta_dash/d_theta] x [d_theta/d_phi] can be computed because TF can compute [d_L/d_theta_dash] and [d_theta/d_phi] naturally, for SGD [d_theta_dash/d_theta] is just vec(-1) really. However this workaround really only works for SGD without Momentum, for anything used in practice (let's say Adam as a default) this workaround does not work anymore because [d_theta_dash/d_theta] is dependent on specific parameter update from the optimizer rather than the gradient itself, which is not naturally exposed. )\r\nIn summary: De facto TF currently makes it extremely difficult and messy to implement hypergradients through gradient updates. This could partially be tackled by exposing differentiable computed parameter updates from optimizers.\r\n\r\nGenerally what should be possible can lazily be summarized like this: \r\n```\r\n\r\nmodel = some_model()\r\ninner_optimizer=optimizers.Adam()\r\nwith tf.GradientTape() as outer_tape:\r\n    with tf.GradientTape() as inner_tape:\r\n        y = model(x)\r\n        inner_loss = loss(y)\r\n    inner_gradients = inner_tape.gradient(inner_loss, model.trainable_weights)\r\n    inner_optimizer.apply_gradients(zip(inner_gradients, model.trainable_weights))\r\n    \r\n    outer_loss = model(x')\r\nouter_gradients = outer_tape.gradient(outer_loss, model.trainable_weights) #!notworking\r\n\r\n        \r\n```\r\n\r\n\r\n**Will this change the current api? How?**\r\nThis should not change the API at all, in my opinion gradient computation through gradient updates in apply_gradient seems natural given the Graph nature of TF. Worst case this would create a boolean flag argument in apply_gradient to enable/disable gradients through the computation. \r\n\r\n\r\n**Who will benefit with this feature?**\r\nPrimarily the Meta-Learning Community and Researchers and Users relying from advances in this field. Two very impactful papers that showcase why this would be important: \r\n - Model-Agnostic Meta Learning for Fast Adaption of Deep Networks : https://arxiv.org/pdf/1703.03400.pdf (>5500 citations)\r\n - Teaching with commentaries : https://arxiv.org/pdf/2011.03037.pdf (still novel, but seems to be impactful)\r\n \r\n Also: Google as Developer of TF, as even research at Google has to default to PyTorch for research on topics like the above, see e.g. here: (https://github.com/googleinterns/commentaries)\r\n\r\n\r\n**Any Other info.**\r\nNothing technical, just love for TF as an awesome library that I wish stays ahead!", "comments": []}, {"number": 55180, "title": "[oneDNN] Fuse conv2d+biasAdd/FusedBatchNorm+sigmoid+mul into fusedConv2D", "body": "This PR fuses following patterns into FusedConv2D for oneDNN backend :\r\na) Conv2d + BiasAdd + Sigmoid + Mul\r\nb) Conv2d + FusedBatchNorm/V2/V3 + Sigmoid + Mul\r\n\r\nThis fusion shows up to 15% improvement for models with above patterns.", "comments": ["@sachinmuradi Can you please review this PR ? Thanks!", "@gbaned I think you added my name by mistake :)  , I am the author of this PR, would need @penpornk or other person to review it", "> @sachinmuradi My apologies. \r\n@penpornk  Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thank you!"]}, {"number": 55166, "title": "[TF:TRT] INT8 calibration in dynamic shape mode", "body": "This PR enables INT8 conversion with calibration in dynamic shape mode.\r\n\r\n- The profile collection logic is moved before the calibration step in TRTEngineOP, because TRT optimization profiles have to be defined before we can calibrate an engine.\r\n- INT8 test are now enabled in V2 mode both with and without dynamic shapes.\r\n\r\nTracker: #45481", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55166/checks?check_run_id=5469643898).", "Tagging @bixia1 for review. Thanks for @vict-guo for the initial implementation. \r\n\r\nA follow up PR is planned to refactor trt_engine_op.cc: there are repetitive steps in GetEngine / BuildEngine / AllocateCalibrationResources. These shall be factored out into helper functions.", "@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 I have fixed the issues.", "There are failing tests, here are the log\r\n[fail1.log](https://github.com/tensorflow/tensorflow/files/8326454/fail1.log)\r\n[fail2.log](https://github.com/tensorflow/tensorflow/files/8326458/fail2.log)\r\n.", "@tfeher Can you please resolve conflicts? Thank you!", "@bixia1 The CI failure was caused by an issue in the algorithm selector in TRT 7.x. We had a workaround in place, but it was only active for TRT 7.1. I have [adjusted it](https://github.com/tfeher/tensorflow/blob/f9467f4bc899b4f221f51e17cff691f07267ef07/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L1202) make it work with TRT 7.2 as well\r\n"]}, {"number": 55138, "title": "tf.image.adjust_jpeg_quality gives unexpected result", "body": "This issue is about the function `tf.image.adjust_jpeg_quality`. In particular, I am concerned about the docs example and I am surprised by the result with quality 100.\r\n\r\n1. The [example provided in the documentation](https://www.tensorflow.org/api_docs/python/tf/image/adjust_jpeg_quality) is misleading. The example uses a float tensor with values in range 1-12. The result of `tf.image.adjust_jpeg_quality` are all one. I assume this is happening because the data type conversion sees a float tensor and clips all values larger than 1. If the input was a uint8 tensor, then the result would differ significantly. In my opinion, the docs should clarify the expected input data type.\r\n\r\n2. The output of `tf.image.adjust_jpeg_quality` with quality 100 looks suspicious to me. I know that even with quality 100 JPEG compression is not necessarily lossless due to rounding errors. Nevertheless, I am surprised how much information is lost. In comparison, the results with Pillow at the same quality are far less lossy.\r\n \r\nPlease take a look at the example below. The input is a 8x8 grayscale image with values from 0 through 63. The pixel values after TensorFlow's JPEG compression show quite some difference to the input, whereas JPEG compression with Pillow preserves the input array. Can anyone verify whether TensorFlow is behaving as expected here? Is there any documentation on how TensorFlow implements JPEG compression?\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tempfile\r\nfrom PIL import Image\r\n\r\n\r\ndef compress_PIL(img, quality):\r\n    \"\"\"\r\n    Apply JPEG compression\r\n    :param img: image with pixel intensities in range [0, 255]\r\n    :return: img with intensities in range [0, 255]\r\n    \"\"\"\r\n\r\n    if len(img.shape) == 3:\r\n        # Remove singleton channel dimension\r\n        img = np.squeeze(img, axis=2)\r\n\r\n    # Apply JPEG compression\r\n    with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\r\n        im = Image.fromarray(img)\r\n        im.save(f.name, quality=quality)\r\n        # Read back in\r\n        im_recovered = Image.open(f.name)\r\n        return np.array(im_recovered)\r\n\r\n\r\nx = np.arange(64, dtype=np.uint8).reshape((8, 8, 1))\r\n\r\nquality = 100\r\n\r\nx_jpeg_tf = tf.image.adjust_jpeg_quality(x, quality)\r\nprint(\"TensorFlow\")\r\nprint(x_jpeg_tf.numpy().squeeze())\r\n\r\nx_jpeg_pil = compress_PIL(x, quality)\r\nprint(\"PIL\")\r\nprint(x_jpeg_pil)\r\n```\r\n\r\nOutput:\r\n```\r\nTensorFlow\r\n[[ 0  0  0  0  1  2  3  5]\r\n [ 5  7  8  9 10 11 12 13]\r\n [14 15 16 17 19 20 21 22]\r\n [23 24 25 26 27 28 29 30]\r\n [32 33 34 35 36 37 38 39]\r\n [40 41 42 43 45 46 47 48]\r\n [49 50 51 52 53 54 55 57]\r\n [57 59 60 61 62 63 64 65]]\r\nPIL\r\n[[ 0  1  2  3  4  5  6  7]\r\n [ 8  9 10 11 12 13 14 15]\r\n [16 17 18 19 20 21 22 23]\r\n [24 25 26 27 28 29 30 31]\r\n [32 33 34 35 36 37 38 39]\r\n [40 41 42 43 44 45 46 47]\r\n [48 49 50 51 52 53 54 55]\r\n [56 57 58 59 60 61 62 63]]\r\n```\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see example above\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: 3.7.11\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: RTX 2080 Ti", "comments": ["Hi @gadagashwini ! Could you please look at this issue? It's replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/518608e9dc2723748e2fbcd84b69d524/github_55138.ipynb#scrollTo=4KlEqJSH43a5) , [2.8](https://colab.sandbox.google.com/gist/mohantym/b80f9fd2bd7cb4f8c9e399df59d1f331/github_55138.ipynb#scrollTo=XPaOV4Iw42bb) and [nightly.](https://colab.sandbox.google.com/gist/mohantym/323f355dc45906b3d48e8793085d5fef/github_55138.ipynb#scrollTo=4KlEqJSH43a5) ", "It looks like this discrepancy is due to the fact that TensorFlow uses the [fast inverse DCT by default](https://github.com/tensorflow/tensorflow/blob/dd5cdcd306705f525205ceab6af63bd08aa02732/tensorflow/core/kernels/image/decode_image_op.cc#L119) for decoding JPEG images. However, this fact is completely opaque to someone using `tf.image.adjust_jpeg_quality`.\r\nI was able to reproduce the wrong output tensor by running libjpeg-turbo's `djpeg -dct fast` command.\r\n\r\nI see two options to solve this issue:\r\n\r\n1. Use the accurate integer DCT by default.\r\n2. Provide an argument to choose the DCT method in `tf.image.adjust_jpeg_quality` or at least mention this caveat in the method's documentation.\r\n\r\nBesides from that, it is very unsatisfactory that the example provided with `tf.image.adjust_jpeg_quality` uses a float tensor with values above 1, although `convert_image_dtype` [states that float tensors are expected to have values in the range \\[0, 1)](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype).", "On a side note, it also seems inconsistent to me that TensorFlow uses the accurate DCT (JDCT_ISLOW) for JPEG compression and the inaccurate IDCT (JDCT_IFAST) for decompression by default.", "> Provide an argument to choose the DCT method in tf.image.adjust_jpeg_quality\r\n\r\nDo you have any interest in contributing a PR for this?\r\n\r\n> at least mention this caveat in the method's documentation.\r\n\r\nI can add something to the documentation and add an example without clipping.\r\n\r\n> Use the accurate integer DCT by default.\r\n\r\nChanging the default (which would change behavior for existing users) is unlikely to be accepted.\r\n"]}, {"number": 55132, "title": "CVE-2022-23593 and CVE-2022-23592 are reported incorrectly for TensorFlow 2.7.1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RedHat Enterprise Linux 8.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.1\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nVulnerability scanners such as twistlock flag the following two vulnerabilities for TensorFlow 2.7.1:\r\n- https://nvd.nist.gov/vuln/detail/CVE-2022-23593\r\n- https://nvd.nist.gov/vuln/detail/CVE-2022-23592 \r\n\r\nThis appears to be incorrect. According to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/security , these two issues ([TFSA-2022-058](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-058.md) and [TFSA-2022-055](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2022-055.md)) _only_ affect TensorFlow 2.8.0. It's the only two issues that didn't get cherry-picked into 2.7.1 - most likely exactly because of that reason.\r\n\r\n**Describe the expected behavior**\r\nThe _Known Affected Software Configurations_ in the two CVEs above should be updated to have a \"From (including)\" of 2.8.0 instead of 2.7.0. Or the two CVEs should be pulled, as they only seem to affect Tensorflow 2.8 rc0 and rc1, not the final version. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nNot needed. The error is visible in the CVE descriptions. \r\n\r\n**Other info / logs** \r\n- \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@alexlang74 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa Done. Please review...", "Hi @mihaimaruseac , do you need any additional information for this issue?", "I was OOO for the entire period this issue existed\r\n\r\nSeems there was a typo when the GH advisories were created", "As these two advisories only affect the rc0 / rc1 version - do you think they should be removed, as no \"production version\" of TF is affected?", "We also release nightlies and there are users that depend on them. A while ago we were asked to issue CVEs for fixes before a release too.", "Ok, so what do you suggest? It appears to me that the CVE database is not able to correctly track CVEs that occur in RC versions. Flagging unaffected releases like 2.7.1 affects security-conscious TF users who can't easily upgrade to the 2.8 line...", "Can you add a local exemption for these CVEs?", "Unfortunately, not. Our customers run 3rd party security scanners, like twistlock and aquascan. We can't make these tools add exemptions, and customers are typically wary of discussing \"you know, this issue is not really an issue\". I'd really hope we can address this in the CVE itself, by changing the affected versions to the correct value...", "Would it be possible to flag something like \"TensorFlow 2.7.9\" as affected release for these CVEs, if you still want to keep them around? It doesn't feel right either, but it would at least ensure that TF 2.7.1 users are no longer affected.", "I already changed the affected versions on GitHub but it didn't seem to propagate.", "Thank you, @mihaimaruseac ! Hope you'll be able to find the \"propagation block\"...", "Hi @mihaimaruseac , were you able to push this forward?", "Hi @mihaimaruseac , Hi @sachinprasadhs ,\r\n\r\nsorry to be a pain in the neck - is there anything I can do to help move this forward? This is very relevant for our security posture, and for other security-conscious TF users as well, I'd assume...", "Unfortunately I don't think the blocker is in TF side. GitHub/NVD/Mitre would need to update the affected versions tag", "Thank you, @mihaimaruseac . I'll go ahead and create two CVE requests under https://cveform.mitre.org/ , to make them change the affected versions. Or do you have another idea?", "@alexlang74  I think requesting update on mitre side is the right path to get the issue resolved.\r\n", "I concur. Sorry for all this confusion and extra work caused by our typo", "All good, thanks for the guidance!", "CVE Request 1247607 created"]}, {"number": 55123, "title": "A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution)", "body": "**System information**\r\n- Android Device information (use `adb shell getprop ro.build.fingerprint`\r\n  if possible):\r\n- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):\r\n- Google Play Services version\r\n  (`Settings` > `Apps` > `Google Play Services` > `App details`):\r\n`\r\nD/goldfish-address-space: allocate: Ask for block of size 0x100\r\nD/goldfish-address-space: allocate: ioctl allocate returned offset 0x3fb9b8000 size 0x2000\r\nD/HostConnection: HostComposition ext ANDROID_EMU_CHECKSUM_HELPER_v1 ANDROID_EMU_native_sync_v2 ANDROID_EMU_native_sync_v3 ANDROID_EMU_native_sync_v4 ANDROID_EMU_dma_v1 ANDROID_EMU_direct_mem ANDROID_EMU_host_composition_v1 ANDROID_EMU_host_composition_v2 ANDROID_EMU_vulkan ANDROID_EMU_deferred_vulkan_commands ANDROID_EMU_vulkan_null_optional_strings ANDROID_EMU_vulkan_create_resources_with_requirements ANDROID_EMU_YUV_Cache ANDROID_EMU_async_unmap_buffer ANDROID_EMU_vulkan_ignored_handles ANDROID_EMU_vulkan_free_memory_sync ANDROID_EMU_vulkan_shader_float16_int8 ANDROID_EMU_vulkan_async_queue_submit GL_OES_vertex_array_object GL_KHR_texture_compression_astc_ldr ANDROID_EMU_host_side_tracing ANDROID_EMU_gles_max_version_2 \r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nI/super_resolution::: Interpreter is created successfully\r\nI/System.out: s-------------------------------------------------\r\nA/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution)`\r\n\r\n\r\nwhen i run a image(250 * 250), a very lagre image(default size 50 * 50), an error is  A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xb9124f40 in tid 9093 (superresolution), pid 9093 (superresolution). So how to run a big image with tflite in tensorflowlite\r\n\r\nprivate static final int LR_IMAGE_HEIGHT = 250;\r\nprivate static final int LR_IMAGE_WIDTH = 250;\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to or attach code demonstrating\r\nthe problem.\r\n`/*\r\n * Copyright 2020 The TensorFlow Authors\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     https://www.apache.org/licenses/LICENSE2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n\r\npackage org.tensorflow.lite.examples.superresolution;\r\n\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.BitmapFactory;\r\nimport android.graphics.drawable.BitmapDrawable;\r\nimport android.os.Bundle;\r\nimport android.os.SystemClock;\r\nimport androidx.appcompat.app.AppCompatActivity;\r\nimport android.util.Log;\r\nimport android.view.MotionEvent;\r\nimport android.view.View;\r\nimport android.widget.Button;\r\nimport android.widget.ImageView;\r\nimport android.widget.LinearLayout;\r\nimport android.widget.Switch;\r\nimport android.widget.TextView;\r\nimport android.widget.Toast;\r\nimport androidx.annotation.WorkerThread;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.io.InputStream;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\n\r\n/** A super resolution class to generate super resolution images from low resolution images * */\r\npublic class MainActivity extends AppCompatActivity {\r\n  static {\r\n    System.loadLibrary(\"SuperResolution\");\r\n  }\r\n\r\n  private static final String TAG = \"SuperResolution\";\r\n  private static final String MODEL_NAME = \"ESRGAN.tflite\";\r\n  private static final int LR_IMAGE_HEIGHT = 250;\r\n  private static final int LR_IMAGE_WIDTH = 250;\r\n  private static final int UPSCALE_FACTOR = 4;\r\n  private static final int SR_IMAGE_HEIGHT = LR_IMAGE_HEIGHT * UPSCALE_FACTOR;\r\n  private static final int SR_IMAGE_WIDTH = LR_IMAGE_WIDTH * UPSCALE_FACTOR;\r\n  private static final String LR_IMG_1 = \"lr-1.jpg\";\r\n  private static final String LR_IMG_2 = \"lr-2.jpg\";\r\n  private static final String LR_IMG_3 = \"lr-3.jpg\";\r\n\r\n  private MappedByteBuffer model;\r\n  private long superResolutionNativeHandle = 0;\r\n  private Bitmap selectedLRBitmap = null;\r\n  private boolean useGPU = false;\r\n\r\n  private ImageView lowResImageView1;\r\n  private ImageView lowResImageView2;\r\n  private ImageView lowResImageView3;\r\n  private TextView selectedImageTextView;\r\n  private Switch gpuSwitch;\r\n\r\n  @Override\r\n  protected void onCreate(Bundle savedInstanceState) {\r\n    super.onCreate(savedInstanceState);\r\n    setContentView(R.layout.activity_main);\r\n\r\n    final Button superResolutionButton = findViewById(R.id.upsample_button);\r\n    lowResImageView1 = findViewById(R.id.low_resolution_image_1);\r\n    lowResImageView2 = findViewById(R.id.low_resolution_image_2);\r\n    lowResImageView3 = findViewById(R.id.low_resolution_image_3);\r\n    selectedImageTextView = findViewById(R.id.chosen_image_tv);\r\n    gpuSwitch = findViewById(R.id.switch_use_gpu);\r\n\r\n    ImageView[] lowResImageViews = {lowResImageView1, lowResImageView2, lowResImageView3};\r\n\r\n    AssetManager assetManager = getAssets();\r\n    try {\r\n      InputStream inputStream1 = assetManager.open(LR_IMG_1);\r\n      Bitmap bitmap1 = BitmapFactory.decodeStream(inputStream1);\r\n      lowResImageView1.setImageBitmap(bitmap1);\r\n\r\n      InputStream inputStream2 = assetManager.open(LR_IMG_2);\r\n      Bitmap bitmap2 = BitmapFactory.decodeStream(inputStream2);\r\n      lowResImageView2.setImageBitmap(bitmap2);\r\n\r\n      InputStream inputStream3 = assetManager.open(LR_IMG_3);\r\n      Bitmap bitmap3 = BitmapFactory.decodeStream(inputStream3);\r\n      lowResImageView3.setImageBitmap(bitmap3);\r\n    } catch (IOException e) {\r\n      Log.e(TAG, \"Failed to open an low resolution image\");\r\n    }\r\n\r\n    for (ImageView iv : lowResImageViews) {\r\n      setLRImageViewListener(iv);\r\n    }\r\n\r\n    superResolutionButton.setOnClickListener(\r\n        new View.OnClickListener() {\r\n          @Override\r\n          public void onClick(View view) {\r\n            if (selectedLRBitmap == null) {\r\n              Toast.makeText(\r\n                      getApplicationContext(),\r\n                      \"Please choose one low resolution image\",\r\n                      Toast.LENGTH_LONG)\r\n                  .show();\r\n              return;\r\n            }\r\n\r\n            if (superResolutionNativeHandle == 0) {\r\n                superResolutionNativeHandle = initTFLiteInterpreter(gpuSwitch.isChecked());\r\n            } else if (useGPU != gpuSwitch.isChecked()) {\r\n              // We need to reinitialize interpreter when execution hardware is changed\r\n              deinit();\r\n              superResolutionNativeHandle = initTFLiteInterpreter(gpuSwitch.isChecked());\r\n            }\r\n            useGPU = gpuSwitch.isChecked();\r\n            if (superResolutionNativeHandle == 0) {\r\n              showToast(\"TFLite interpreter failed to create!\");\r\n              return;\r\n            }\r\n\r\n            int[] lowResRGB = new int[LR_IMAGE_HEIGHT * LR_IMAGE_WIDTH];\r\n            selectedLRBitmap.getPixels(\r\n                lowResRGB, 0, LR_IMAGE_WIDTH, 0, 0, LR_IMAGE_WIDTH, LR_IMAGE_HEIGHT);\r\n\r\n            final long startTime = SystemClock.uptimeMillis();\r\n            int[] superResRGB = doSuperResolution(lowResRGB);\r\n            final long processingTimeMs = SystemClock.uptimeMillis() - startTime;\r\n            if (superResRGB == null) {\r\n              showToast(\"Super resolution failed!\");\r\n              return;\r\n            }\r\n\r\n            final LinearLayout resultLayout = findViewById(R.id.result_layout);\r\n            final ImageView superResolutionImageView = findViewById(R.id.super_resolution_image);\r\n            final ImageView nativelyScaledImageView = findViewById(R.id.natively_scaled_image);\r\n            final TextView superResolutionTextView = findViewById(R.id.super_resolution_tv);\r\n            final TextView nativelyScaledImageTextView =\r\n                findViewById(R.id.natively_scaled_image_tv);\r\n            final TextView logTextView = findViewById(R.id.log_view);\r\n\r\n            // Force refreshing the ImageView\r\n            superResolutionImageView.setImageDrawable(null);\r\n            Bitmap srImgBitmap =\r\n                Bitmap.createBitmap(\r\n                    superResRGB, SR_IMAGE_WIDTH, SR_IMAGE_HEIGHT, Bitmap.Config.ARGB_8888);\r\n            superResolutionImageView.setImageBitmap(srImgBitmap);\r\n            nativelyScaledImageView.setImageBitmap(selectedLRBitmap);\r\n            resultLayout.setVisibility(View.VISIBLE);\r\n            logTextView.setText(\"Inference time: \" + processingTimeMs + \"ms\");\r\n          }\r\n        });\r\n  }\r\n\r\n  @Override\r\n  public void onDestroy() {\r\n    super.onDestroy();\r\n    deinit();\r\n  }\r\n\r\n  private void setLRImageViewListener(ImageView iv) {\r\n    iv.setOnTouchListener(\r\n        new View.OnTouchListener() {\r\n          @Override\r\n          public boolean onTouch(View v, MotionEvent event) {\r\n            if (v.equals(lowResImageView1)) {\r\n              selectedLRBitmap = ((BitmapDrawable) lowResImageView1.getDrawable()).getBitmap();\r\n              selectedImageTextView.setText(\r\n                  \"You are using low resolution image: 1 (\"\r\n                      + getResources().getString(R.string.low_resolution_1)\r\n                      + \")\");\r\n            } else if (v.equals(lowResImageView2)) {\r\n              selectedLRBitmap = ((BitmapDrawable) lowResImageView2.getDrawable()).getBitmap();\r\n              selectedImageTextView.setText(\r\n                  \"You are using low resolution image: 2 (\"\r\n                      + getResources().getString(R.string.low_resolution_2)\r\n                      + \")\");\r\n            } else if (v.equals(lowResImageView3)) {\r\n              selectedLRBitmap = ((BitmapDrawable) lowResImageView3.getDrawable()).getBitmap();\r\n              selectedImageTextView.setText(\r\n                  \"You are using low resolution image: 3 (\"\r\n                      + getResources().getString(R.string.low_resolution_3)\r\n                      + \")\");\r\n            }\r\n            return false;\r\n          }\r\n        });\r\n  }\r\n\r\n  @WorkerThread\r\n  public synchronized int[] doSuperResolution(int[] lowResRGB) {\r\n    return superResolutionFromJNI(superResolutionNativeHandle, lowResRGB);\r\n  }\r\n\r\n  private MappedByteBuffer loadModelFile() throws IOException {\r\n    try (AssetFileDescriptor fileDescriptor =\r\n            AssetsUtil.getAssetFileDescriptorOrCached(getApplicationContext(), MODEL_NAME);\r\n        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor())) {\r\n      FileChannel fileChannel = inputStream.getChannel();\r\n      long startOffset = fileDescriptor.getStartOffset();\r\n      long declaredLength = fileDescriptor.getDeclaredLength();\r\n      return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n    }\r\n  }\r\n\r\n  private void showToast(String str) {\r\n    Toast.makeText(getApplicationContext(), str, Toast.LENGTH_LONG).show();\r\n  }\r\n\r\n  private long initTFLiteInterpreter(boolean useGPU) {\r\n    try {\r\n      model = loadModelFile();\r\n    } catch (IOException e) {\r\n      Log.e(TAG, \"Fail to load model\", e);\r\n    }\r\n    return initWithByteBufferFromJNI(model, useGPU);\r\n  }\r\n\r\n  private void deinit() {\r\n    deinitFromJNI(superResolutionNativeHandle);\r\n  }\r\n\r\n  private native int[] superResolutionFromJNI(long superResolutionNativeHandle, int[] lowResRGB);\r\n\r\n  private native long initWithByteBufferFromJNI(MappedByteBuffer modelBuffer, boolean useGPU);\r\n\r\n  private native void deinitFromJNI(long superResolutionNativeHandle);\r\n}\r\n`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\n\r\nSDK version: 28\r\ndevice information\uff1a huawei p40\r\nlog\r\nhttps://github.com/douzaikongcheng/log/blob/main/test.txt\r\n", "comments": ["Hi @douzaikongcheng ! Can you check the instructions in this [thread](https://stackoverflow.com/a/65011469/11530462) to attach the log cat here too? Please update device information and SDK version in template too to help expedite the issue . Thanks!", "@[mohantym](https://github.com/mohantym)\uff0cok\uff0c\r\n\r\nSDK version: 28\r\ndevice information\uff1a huawei p40\r\nlog\r\nhttps://github.com/douzaikongcheng/log/blob/main/test.txt\r\n", "Hi @douzaikongcheng, if you change the size of the input image, you will also need to retrain/re-convert the model (see https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/ml/super_resolution.ipynb) and change the corresponding values in the native code. I suspect the mismatch between these is what is causing the crash.", "Hi@[mohantym](https://github.com/mohantym).This error was encountered when I was testing my training model. I think it may be a memory problem. Please test it again. Thank you", "I don't have the model that you've been using, but you will definitely also have to change the size in native code (https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.h#L35).", "@douzaikongcheng ! Did you check with @sheepmaster's comment on retraining the model with input size too?", "Hi@[mohantym](https://github.com/mohantym),Hi@[sheepmaster](https://github.com/sheepmaster)\r\nI have uploaded my model and the whole code to GitHub. The link is as follows, https://github.com/douzaikongcheng/test_tflite. Please check it. thank you", "Hi@[mohantym](https://github.com/mohantym),Hi@[sheepmaster](https://github.com/sheepmaster)\r\nYour reply would be appreciated!", "Sorry, I haven't had a chance yet to try the sample (I'm not directly on the TF team, I just came across this bug and found an obvious issue). Maybe @chunduriv can take a look?", "@[sheepmaster](https://github.com/sheepmaster),Thanks.@[chunduriv](https://github.com/chunduriv),Can you help me?", "@douzaikongcheng,\r\n\r\nTriage Notes : Here you are trying to use different dimensions than the original example (50x50 -> 200x200).\r\n\r\nCan you try to use the example model with different dimensions? \r\n\r\nIf it failed while reading a bigger image than the model would actually return (250x250 instead of 200x200), then you would need to train the model using a dataset of 250x250 images. Thanks!", "\r\n@[chunduriv](https://github.com/chunduriv)\r\nI didn't encounter this error when trying to deploy my model using pytorch mobile, but I encountered it when using tflite. I think it may be a memory problem. My model is not trained and does not need training. I just test whether it can run.Can you check it for me again? The model doesn't need to output the correct super-resolution image, as long as it can run, thank you.", "@[chunduriv](https://github.com/chunduriv)\r\nI have uploaded my model and the whole code to GitHub. The link is as follows, https://github.com/douzaikongcheng/test_tflite.", "@khanhlvg could you please help take a look at this issue?", "@windmaple Could you take a look? Thanks!", "@douzaikongcheng are you using the [ESRGAN model](https://tfhub.dev/captain-pool/lite-model/esrgan-tf2/1) from TF Hub? That model can only take [50,50] input image (you can see it in Netron) because the model author fixed the dimensions when he did the conversion. If you want to use another dimension (or dynamic shape), you are going to have to retrain and re-convert the model (it's non-trivial work). The source code link is on TF Hub page.\r\n\r\nIf you are using a model trained by PyTorch and then somehow converted to TFLite, there is not much we can do, since we are not responsible for the PyTorch -> TF/TFLite conversion.\r\n\r\nIf you suspect it's a memory issue, most likely it's OOM, in which case you can check memory usage using 'top' or the like.\r\n\r\nHope this helps.", "@[windmaple](https://github.com/windmaple)\r\nThanks", "@douzaikongcheng,\r\n\r\nCould you please confirm if this issue is resolved for you? Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@[windmaple](https://github.com/windmaple)Recently, I used tensorflow to train a super-resolution model, but this error still occurs, that is, an error will be reported when the image input size is too large, but there is no problem when the image is small", "I'm pretty you ran out of memory. You can confirm this by using benchmark tool to see how much memory it is using. \r\n\r\nSuper resolution models are quite memory-hungry and you should look into model optimization techniques like distillation."]}, {"number": 55118, "title": "Support pad_sequences in Autograph", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.8.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe following works in eager:\r\n```\r\ndef pad_function(x):\r\n    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(x,padding='post')\r\n    return padded_inputs\r\n\r\npad_function([tf.constant([[1, 2, 3], [1, 2, 3]]), tf.constant([[1, 2, 3]])])\r\n\r\n>> output:\r\narray([[[1, 2, 3],\r\n        [1, 2, 3]],\r\n\r\n       [[1, 2, 3],\r\n        [0, 0, 0]]], dtype=int32)\r\n```\r\nHowever, when having the @tf.function decorator, it does not work:\r\n```\r\n@tf.function\r\ndef pad_function(x):\r\n    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(x,padding='post')\r\n    return padded_inputs\r\n\r\n>> output:\r\n\r\nValueError: `sequences` must be a list of iterables. Found non-iterable: Tensor(\"x:0\", shape=(2, 3), dtype=int32)\r\n```\r\n\r\nIf you have any other workarounds, please let me know. Much appreciated! Thanks!\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nEveryone who uses tensor padding\r\n**Any Other info.**\r\n", "comments": ["@victorconan I tried to replicate the provided code on colab using TF v2.8.0 and didn't face the error as reported.Could you please have a look at  the [gist](https://colab.research.google.com/gist/sushreebarsa/0c0e28958acd29fbd418206b9a5d3919/55118.ipynb) and let us know if we are missing something to reproduce the issue?Thanks!", "> gist\r\n\r\nHi, can you run it again? it clearly threw the same error if you run the @tf.function decorated one. In the notebook you provided, it seems you didn't run the @tf.function decorated one... please run that one, thanks", "@victorconan Thank you for the response. \r\nI was able to reproduce this issue. \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "> @victorconan Thank you for the response. I was able to reproduce this issue. Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 Thanks!\r\n\r\nDo you believe this is a Keras issue? Keras guys believed this is a tensorflow issue..."]}, {"number": 55075, "title": "Issue created for Rollback of PR #54432: Add appropriate dtype check for `tf.boolean_mask`'s mask", "body": "Merged PR #54432 is rolled back in 6643f0796d1f9cde90eaa95593bb7f27f315262b.\n    Please follow up with the reviewer and close this issue once its resolved.", "comments": []}, {"number": 55072, "title": "Cannot find documentation on how to use FILE autosharding", "body": "## URL(s) with the issue:\r\n\r\nhttps://tensorflow.google.cn/guide/distributed_training?hl=en\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nI would like to use the FILE autoshard policy for my distributed training. However, I cannot find any examples in the documentation of how to do this. I am using the Librispeech dataset, which is split across many audio files. I have created a `tf.data.Dataset` that is the list of the file names (as strings), but when I try to use it, I get errors of the form ``AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy``. There are a number of Github issues discussing this problem, and I think the fact that there are no concrete examples of using FILE sharding is a contributor to this. (The other contributor is many people using Tensor-based datasets rather than file-based ones, and this is what was addressed in the discussion of those Github issues, not how to actually make a file-based dataset suitable for sharding.)\r\n\r\nThe fact that this warning even comes up in the examples on the \"Distributed Training\" page again highlights the fact that this warning is something users will see regularly. The warning itself tells users how to turn it off (good, though some including myself have had problems actually getting it to turn off), but nowhere have I seen anything pointing to how to actually get FILE sharding working.\r\n\r\n### Usage example\r\n\r\nNo, there is no usage example, which is the core problem here I believe. I would be happy to contribute one, except I cannot get this working myself, hence the request for an example.", "comments": ["@hunse, Try `tf.data.experimental.DistributeOptions()`, which has AutoShardPolicy and represents options for distributed data processing. For more information refer [link](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Thanks!", "Hi @gadagashwini. That documentation is about changing the AutoShardPolicy, which is not what I want to do. The default of AUTO attempts FILE-based sharding, and falls back on DATA-based sharding if that doesn't work. What I want to know is, why isn't FILE-based sharding working for my dataset? Obviously, that's an in-depth question that depends on the specifics of my dataset. However, what I am requesting is a general example of using FILE-based sharding with a dataset. How does one set up a `tf.data.Dataset` so that FILE-based sharding will work? This seems like a common use-case, and an example would be very useful addition to the documentation.", "@hunse, `tf.data.Dataset` has class called shard, which creates a Dataset that includes only `1/num_shards` of this dataset.\r\nTake a look at this [info](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#important_caveats_2). Thanks!", "@gadagashwini If I want to use `tf.data.Dataset.shard` to manually shard my data for multiple GPUs, how do I then provide those different shards to the different GPUs? Can I do this with `tf.keras.Model.fit`, or do I need to write a custom training loop? There's a basic example of a custom training loop [here](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy), but it's still not clear to me how I would modify that example to use the shards. Furthermore, it would be nice to be able to use `tf.keras.Model.fit`, since that's what I'm currently using for single-GPU training, and moving away from that is both more work and more potential for mistakes on my part.\r\n\r\nThere's also some information [here](https://www.tensorflow.org/tutorials/distribute/input), but that example also uses auto-sharding, not manual sharding. Again, this emphasizes the need for an example of using a `tf.data.Dataset` with FILE-based autosharding.\r\n\r\nI feel like you're getting away from the point, though. There's obviously a use-case for FILE-based autosharding, since otherwise it wouldn't be part of TensorFlow. Somebody wrote it, and there must be some way to use it. How? I really think that this warrants an example, especially since you're not able to point me towards a piece of documentation that details how to write a dataset suitable for FILE-based autosharding.", "Hi, You can follow [this](https://www.tensorflow.org/tutorials/distribute/input) tutorial for distribution strategies which shows the details and usage of `AutoShardPolicy` but like you mentioned above this is for `auto-sharding` . \r\n Also, you can refer [this](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) document for type of options to use in `AutoShardPolicy`.", "Hi @hunse. The example in https://www.tensorflow.org/tutorials/distribute/input#sharding demonstrates how to set the sharding policy:\r\n\r\n```python\r\ndataset = tf.data.Dataset.from_tensors(([1.],[1.])).repeat(64).batch(16)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.FILE\r\ndataset = dataset.with_options(options)\r\n```\r\n\r\nThis will \"just work\" as long as the dataset begins with a list of files. If the dataset doesn't begin with a list of files, you'll see an error along the lines of `Found an unshardable source dataset: name: \"foo\"`. What unshardable source dataset are you seeing?"]}, {"number": 55067, "title": "Initial running of TensorFlow on WSL Ubuntu 20.04 looks very very slow.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **WSL2 Ubuntu 20.04**\r\n- TensorFlow version (use command below)**: 2.1.0**\r\n- Python version: **Python 3.7**\r\n- CUDA/cuDNN version:\r\n       **nvcc: NVIDIA (R) Cuda compiler driver\r\n       Copyright (c) 2005-2019 NVIDIA Corporation\r\n       Built on Sun_Jul_28_19:07:16_PDT_2019\r\n       Cuda compilation tools, release 10.1, V10.1.243**\r\n- GPU model and memory: **RTX 3050 Ti Laptop GPU**\r\n\r\n# Issue\r\n\r\nDear all,\r\n\r\nI would like to ask for help to solve the issue I've been experiencing with Tensorflow on WSL2.\r\nI'm a relatively light user and a novice of Tensorflow and I've been using different machine learning (ML) models on Windows. Recently, I found that I can take some advantages of performing MLs using Linux on WSL2 instead of Windows, so I set the environment in WSL2 and was able to run the code with GPU. (It was a very painful process to set the same environment as it is on Windows.) \r\n\r\nHowever, when I run my code in WSL Ubuntu, I found that the **(1) initial running takes very very long compare to Windows.** Also, **(2) there are multiple lines showing which I didn't see with Windows. And it only appears in the initial run.** You can find them in the images below.\r\nSince the code is running, I believe that the code is using GPU properly on WSL2 system. But if it takes this long, there is no reason for using Linux on WSL2 instead of Windows. Maybe this is true since I'm a light user, but I want to learn a Linux and develop my capability. So I'm asking for help.\r\n\r\nI have copied the code I made for test at the end.\r\n\r\n# Questions\r\nTo summarise, there are three questions to be addressed:\r\n\r\n### Q1. Why does initial running take very very long compare to Windows? How can I fix this?\r\n\r\n### Q2. Why there are multiple warning lines with the initial run?\r\n\r\n### Q3. Did I properly set CUDA and Tensorflow-gpu on WSL2 system? Could you guide me how to set Tensorflow-gpu on WSL2?\r\n-> To use GPU and Tensorflow in WSL2, I went through a very hard time. And possibly I did make something wrong with set up. Let me briefly explain the process I went through:\r\nStep 1. Install CUDA v10.1\r\nStep 2. Install CuDNN v7.6.5\r\nStep 3. Set the environment with these commands:\r\n$ echo 'export LD_LIBRARY_PATH=/usr/lib/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\r\n$ echo 'export LD_LIBRARY_PATH=/usr/lib/cuda/include:$LD_LIBRARY_PATH' >> ~/.bashrc\r\nStep 4. Install tensorflow-gpu v2.1.0 under Miniconda environment with python v3.7\r\n\r\nI would appreciate any comment from all of you. Thanks.\r\n\r\n\r\n### image no.1 - from the initial run, time duration = 5m 53.3s\r\n![image](https://user-images.githubusercontent.com/49014051/157073727-0666bf17-a5ca-479d-ba74-b3d7a42adaee.png)\r\n### image no.2 - warnings from the initial run\r\n![image](https://user-images.githubusercontent.com/49014051/157073811-f33f43ea-daf1-45ef-b23f-85d1b6b7f608.png)\r\n\r\n# Test code\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\npredictions = model(x_train[:1]).numpy()\r\ntf.nn.softmax(predictions).numpy()\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nloss_fn(y_train[:1], predictions).numpy()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test,  y_test, verbose=2)", "comments": ["Hi @jinzzasol ! Did you check instructions for [WSL ubuntu](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) on Nvidia site too?", "> Hi @jinzzasol ! Did you check instructions for [WSL ubuntu](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) on Nvidia site too?\r\n\r\n@mohantym Yes, and I tried. But I don't think it helped.", "> Hi @jinzzasol ! Did you check instructions for [WSL ubuntu](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) on Nvidia site too?\r\n\r\n@mohantym I tried this again, but after I installed NVIDIA driver, now Tensorflow is using CPU, not GPU. And all the setting I made is gone now. WSL and Tensorflow don't recognise GPU anymore.", "@mohantym Now it is fixed and Tensorflow recognises GPU. However, nothing has changed. The original issues are still there. I also launched jupyter notebook in docker and run the examples, but nothing is different.", "Hi @gadagashwini ! Could you please look at this issue?", "@mohantym @gadagashwini Can you please look at this issue? or can assign someone else? Thanks,", "@gadagashwini Can you look at this issue? or can assign someone else?", "WSL is slow in reading files. There are a lot of files that need to be loaded when `import tensorflow as tf` gets executed.\r\n\r\nThis is likely not a TF issue.", "> WSL is slow in reading files. There are a lot of files that need to be loaded when `import tensorflow as tf` gets executed.\r\n> \r\n> This is likely not a TF issue.\r\n@mihaimaruseac \r\nYou don't get the point. `import tensorflow as tf ` works okay and fast in WSL too. It only takes less than 0.1s. Run the code line by line then you will understand what I'm saying.", "Same point applies for loading mnist data which is a bunch of image files, afaik", "> Same point applies for loading mnist data which is a bunch of image files, afaik\r\n\r\n@mihaimaruseac  I think you still don't understand the point.\r\n\r\nI tested tensorflow without using mnist data and it is still very slow. I used the data set I have in my laptop which has 3000 rows and 15 columns. I don't think the data loading takes that long. When I run the code line by line, loading tensorflow and mnist data doesn't take long as you said.\r\n\r\nWhen I say, the tensorflow takes too long means that activating (or initialising) model takes very long in WSL. Since the tensorflow uses CUDA with GPU on the laptop, I believe there is something in using GPU. That's my point. Hope   you understand the point of my question.\r\n\r\nFor you, I ran the code above line by line. See the image below. Importing tensorflow and mnist data only takes 3.3sec. Do you think loading tensorflow and mnist data make the code run slowly? really?\r\n\r\n(Image below) Loading tensorflow and data only takes 3.3s. Model takes 46.8s which not too slow, but it is slower than running the code on Windows.\r\n![image](https://user-images.githubusercontent.com/49014051/160733674-57026508-462c-43cd-9fb2-ee1fadfeffca.png)\r\n\r\n(Image below) The model takes 1m 33.3s. This should take less on Windows. WSL is very slow when initialising the model first time.\r\n![image](https://user-images.githubusercontent.com/49014051/160733780-6d3cc6e5-1a54-48b3-a571-b435b006e56d.png)\r\n\r\n(Image below) When I repeat this model, then it takes 19.8s.\r\n![image](https://user-images.githubusercontent.com/49014051/160733959-274a87fc-49f1-4e23-b40a-f3678ec7d970.png)\r\n\r\n"]}, {"number": 55065, "title": "Error during Inference of LSTM Tflite model", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tflite-runtime 2.7 compiled using pip \r\n- Device: Raspberry pi 4 32 bit armv7l\r\n\r\n### 2. Issue\r\n\r\nI did create a custom LSTM model using the below architecture\r\n\r\n```\r\ndef My_LSTM(X_train, y_train, X_test, y_test):\r\n    model = Sequential()\r\n    model.add(Masking(mask_value= -27/255 , input_shape=(None, 600)))\r\n    model.add(LSTM(32, return_sequences=True))\r\n    model.add(LSTM(32, return_sequences=False))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(16,activation='tanh'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(4,activation='softmax'))\r\n\r\n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer= Adam(learning_rate = 1e-3, decay = 1e-6),metrics = ['accuracy'])\r\n\r\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-9)\r\n    stop = EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\r\n\r\n    \r\n    model.fit(X_train, y_train, epochs=40, batch_size= 128, validation_data=(X_test, y_test), callbacks=[reduce_lr, stop], verbose =1)\r\n    \r\n    \r\n    return model\r\n```\r\n\r\nand then I did convert it to tflite extension using the following converter, which successfully converted the model.\r\n\r\n```\r\ndef Create_tflite_Model(ModelName, savePath):\r\n    \r\n    keras_model = tf.keras.models.load_model(ModelName, compile = False)\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.experimental_new_converter=True\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS]\r\n    tflite_model = converter.convert()\r\n    tflite_model_dir = pathlib.Path(savePath)\r\n    tflite_model_file = tflite_model_dir/\"My_model.tflite\"\r\n    tflite_model_file.write_bytes(tflite_model)\r\n    \r\n    return tflite_model\r\n```\r\n\r\nI did make inference using ``` tf.lite.Interpreter ``` in Tensorflow 2.7 on my PC and was working fine. But when I migrate to my Raspberrypi 4 with armv7l 32bit and try to make inference using ```tflite-runtime 2.7``` using the following code: \r\n\r\n```\r\nSuper_Interpreter = tf.lite.Interpreter(model_path='My_model.tflite')\r\nSuper_Interpreter.get_signature_list()\r\nSuper = Super_Interpreter.get_signature_runner('serving_default')\r\nResult = Super(masking_input = \"File to Classify\")\r\nobject = np.argmax(Result.get('dense_1'))\r\nprint(Classes[object])\r\n```\r\n\r\nIt returns this error: \r\n\r\n```\r\nRuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 14 (FlexTensorListReserve) failed to prepare.\r\n```\r\n\r\nI also tried using tflite-runtime 2.5 which returns this error:\r\n``` Unsupported data type 14 in tensor  ``` \r\n\r\nIn addition, I tried using the tensorflow 2.4 library on armv7l, which is the last version that can be installed on this system and tried making the same inference but I still get this error:\r\n``` Unsupported data type 14 in tensor  ``` \r\n\r\nIs there any solution for this issue? \r\n\r\n", "comments": ["@MTawfik93 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Ok so in order to reproduce the error, this is a code snippet as well as the model and a sample file\r\n\r\n```\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport tflite_runtime.interpreter as tflite\r\n\r\nClass = ['Vehicle_1', 'Vehicle_2', 'Vechile_3', 'Vechile_4']\r\n\r\nSuper_Interpreter = tflite.Interpreter(model_path='My_model.tflite')\r\nSuper_Interpreter.get_signature_list()\r\nSuper = Super_Interpreter.get_signature_runner('serving_default')\r\n\r\ndef Preprocess(txtfile):\r\n    \r\n    Sample = [] \r\n    Sample_padded = [] \r\n    frames = {} \r\n\r\n    array_from_file = np.loadtxt(txtfile, delimiter=',', dtype=float)\r\n    array_from_file = (array_from_file*100).astype(int)\r\n    array_from_file_list = array_from_file.tolist()\r\n        \r\n    for item in array_from_file_list: \r\n        if item[1] in frames:\r\n            frames[item[1]].append([item[0], item[2]])\r\n        else:\r\n            frames[item[1]] = [[item[0], item[2]]]\r\n    \r\n    for key in frames:\r\n        Sample.append(frames[key]) \r\n    \r\n    for points in Sample: \r\n        Sample_padded.append(np.pad(points,[(0,300 - len(points)), (0,0)], 'constant', constant_values=(-27, -27)))\r\n    \r\n    Detect = np.array(Sample_padded).reshape(1,-1,600)/255\r\n    Detect = Detect.astype(np.float32)\r\n\r\n    return Detect\r\n\r\nfor i in os.listdir():\r\n      if i.endswith('.txt'):\r\n            start_time = time.time()\r\n            Sample_file = Preprocess(i)\r\n            Result = Super(masking_input = Sample_file)\r\n            Vehicle = np.argmax(Result.get('dense_1'))\r\n            print(Class[Vehicle])\r\n            print(\"--- %s seconds ---\" % (time.time() - start_time))\r\n```\r\n\r\nThe compressed file contains the Tflite model and sample file to test the code. \r\n[Issue_1.zip](https://github.com/tensorflow/tensorflow/files/8253944/Issue_1.zip)", "@MTawfik93,\r\nUse tflite_runtime 2.6 or higher. There shouldn't be any additional step to run the delegate. Take a look at the guide to use Select TF ops is in https://www.tensorflow.org/lite/guide/ops_select#run_inference. Similar issue [#40157](https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-956002355). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @MTawfik93, Use tflite_runtime 2.6 or higher. There shouldn't be any additional step to run the delegate. Take a look at the guide to use Select TF ops is in https://www.tensorflow.org/lite/guide/ops_select#run_inference. Similar issue [#40157](https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-956002355). Thanks!\r\n\r\nI did try doing this but unfortunately still having the same error. I did solve it by upgrading the hardware to arch64 and installing a tensorflow 2.7 for Arm and using ```tf.lite.Interpreter``` function, but never succeeded in running the ```tflite_runtime``` using the LSTM tflite model, always running into the same error that I mentioned earlier. "]}, {"number": 55057, "title": "libstdc++ 6.0.24 is statically linked into libtensorflow_framework.so on PyPI", "body": "**System information**\r\n\r\n- OS Platform and Distribution: PyPI packages for Linux\r\n- TensorFlow installed from (source or binary): binary via PyPI\r\n- TensorFlow version: 2.4 and 2.6\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n**Describe the problem**\r\n\r\nThe PyPI packages for TensorFlow, at least versions 2.4.0 and 2.6.0, ship a compiled shared object `libtensorflow_framework.so`. This library is loaded when one does `import tensorflow` in Python. The TF shared library contains symbols from libstdc++ 6.0.24. When one uses a custom C++ library with Python via `ctypes`, the dynamic linker will resolves symbols that the custom library need first from `libtensorflow_framework.so`, and only then from the system wide installed `libstdc++.so`. This means that no matter which version of libstdc++ is installed system wide, one gets certain symbols from libstdc++ 6.0.24.\r\n\r\nThis a huge problem because that version of libstdc++ has a [bug in `std::execute_native_thread_routine`](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55917) which force unwinds the stack when an exception escapes a thread. Once the exception hits the top stack frame of the thread, `std::terminate()`/`abort()` will be called and a core dump triggered. Due to the force unwinding of the stack, the stack trace in the core dump is completely useless and only contains the following:\r\n\r\n```\r\n#0  0x00007f40fce54438 in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f40fce5603a in abort () from /lib/x86_64-linux-gnu/libc.so.6\r\n#2  0x00007f4076e65dde in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007f4076e717a6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007f4076e71811 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007f4078beaf2e in std::execute_native_thread_routine (__p=0x2da6d20)\r\n    at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:91\r\n#6  0x00007f40fd1f06ba in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#7  0x00007f40fcf2651d in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\nYou can see the `/dt7-src` there, which is the \u201cdev tools GCC 7\u201d which are used in the CD system of TensorFlow. The libstdc++ version with the fix is associated with GCC 8, so the GCC 7 one is still affected.\r\n\r\nOne can work around this by using `LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so python \u2026`. However, I would think that statically linking the standard library into a shared library is a bad thing to do. I am not sure why that was done, perhaps there are good reasons for that.\r\n\r\nI would think that changing the linking should be changed such that libstdc++ is not linked statically into the dynamic library when building Python packages for PyPI. If that is not possible, I would suggest to use GCC 8 such that this bug is fixed.\r\n\r\nThere is a [very enlightening blog post](https://le.qun.ch/en/blog/libstdc++-bug/), which describes the issue with that version of libstdc++ in a context without TensorFlow.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Write a C++ library that uses `std::thread` somewhere.\r\n2. Have a bug in your code which raises an exception that is not caught anywhere your code.\r\n3. Use both your C++ library and TensorFlow from within the same Python process.\r\n4. Have it crash your application and write a core dump.\r\n5. Look at the core dump using GDB (`gdb python PATH_TO_CORE_FILE`). Print the back trace. Notice that the whole stack within the tread is missing, although it should be present with Itanium ABI on Linux.", "comments": ["Can you reproduce the same failure with the latest tf-nightly? @nitins17 and @angerson and @perfinion have worked to switch TF to manylinux2014 / `dt9` (also tagging them in case they have some guidance here)"]}, {"number": 55051, "title": "Adding executable code for `tf.sparse.softmax`", "body": "Changed the documentation of tf.sparse.softmax with executable example code.\r\nThe values passed to tf.sparse.SparseTensor must be a rank-1 tensor instead of a rank-3 tensor.\r\n\r\nFixes #[55035](https://github.com/tensorflow/tensorflow/issues/55035)", "comments": ["Please use a proper title, not \"Update <file>\"\r\n\r\nhttps://cbea.ms/git-commit/"]}, {"number": 55043, "title": "Custom operation on extracted volume patches", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are ): I'm using TensorFlow version 2.8\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCustom operation on sliding kernels on volumes of data would be really helpful. I know off the function _tf.extract_volume_patches_ but there are some drawbacks to this. It collects the patches but consumes lots of memory resource. I mean what I had in mind was performing some kind of custom operation on data collected through a sliding window and using this function would eventually lead to exhausting my available resource. And also it would be great to add a kernel mask to specify how the sampling would be performed. In short two I recommend more arguments to this function :\r\n1) An argument to specify a function or a lambda to call upon every sampled window right after sampling and return and save the result instead of the sampled window itself\r\n2) A sampling mask. Specifying which elements should be sampled ( I know it can be done using the this very function but again the constrain is the memory usage) \r\n\r\n\r\n**Will this change the current api? How?**\r\nI think it wouldn't change anything \r\n\r\n**Who will benefit with this feature?**\r\nThis would be very helpful for image processing and feature extraction out of large volumes of data \r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @gadagashwini ! Could you please look at this feature request?", "@TheKiteFlier, Would you like to raise PR for proposed feature request? ", "@gadagashwini, I'd love to but unfortunately I don't have the technical knowledge and skills necessary to do :(\r\n\r\n"]}, {"number": 55040, "title": "TFLite Python:  interpreter._get_tensor_details(index) can't read tensor with sparsity", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.0\r\n- Bazel version (if compiling from source):  5.0.0\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIn the Python code below, load the [Mediapipe pose_detection tflie model](https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection.tflite), and try to get details of tensor index 15. The program crashes during `interpreter._get_tensor_details(15)`, and the `print(details)` never executes.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninterpreter = tf.lite.Interpreter(\"pose_detection.tflite\")\r\ndetails = interpreter._get_tensor_details(15)\r\nprint(details)\r\n```\r\n\r\nI analysed it and found the call stack is\r\n`interpreter._get_tensor_details(index)` ->\r\n`_interpreter.TensorSparsityParameters()` ->\r\n`InterpreterWrapper::TensorSparsityParameters()` ->\r\n`PyDictFromSparsityParam()` ->\r\n`PyArrayFromIntVector()`\r\n\r\ntensorflow\\lite\\python\\interpreter.py\r\n```\r\n  def _get_tensor_details(self, tensor_index):\r\n  ...\r\n    tensor_sparsity_params = self._interpreter.TensorSparsityParameters(\r\n        tensor_index)\r\n```\r\n\r\ntensorflow\\lite\\python\\interpreter_wrapper\\interpreter_wrapper.cc\r\n```\r\nPyObject* InterpreterWrapper::TensorSparsityParameters(int i) const {\r\n  TFLITE_PY_ENSURE_VALID_INTERPRETER();\r\n  TFLITE_PY_TENSOR_BOUNDS_CHECK(i);\r\n  const TfLiteTensor* tensor = interpreter_->tensor(i);\r\n  if (tensor->sparsity == nullptr) {\r\n    return PyDict_New();\r\n  }\r\n\r\n  return PyDictFromSparsityParam(*tensor->sparsity);\r\n}\r\n\r\n\r\nPyObject* PyDictFromSparsityParam(const TfLiteSparsity& param) {\r\n  PyObject* result = PyDict_New();\r\n  PyDict_SetItemString(result, \"traversal_order\",\r\n                       PyArrayFromIntVector(param.traversal_order->data,\r\n                                            param.traversal_order->size));\r\n  PyDict_SetItemString(\r\n      result, \"block_map\",\r\n      PyArrayFromIntVector(param.block_map->data, param.block_map->size));\r\n...\r\n}\r\n\r\nPyObject* PyArrayFromIntVector(const int* data, npy_intp size) {\r\n  void* pydata = malloc(size * sizeof(int));\r\n  memcpy(pydata, data, size * sizeof(int));\r\n  PyObject* obj = PyArray_SimpleNewFromData(1, &size, NPY_INT32, pydata);\r\n  PyArray_ENABLEFLAGS(reinterpret_cast<PyArrayObject*>(obj), NPY_ARRAY_OWNDATA);\r\n  return obj;\r\n}\r\n```\r\n\r\nIn the tensor index 15 of pose_detection.tflite, `tensor->sparsity` is not null, but `tensor->sparsity->traversal_order->data` is null.\r\nSo the `memcpy()` in `PyArrayFromIntVector()` crashed with null pointer exception.\r\n\r\nCould you check why pose_detection.tflite can be inferred normally in Mediapipe, but can't be read by `interpreter._get_tensor_details(index)`?\r\nThanks.\r\n", "comments": ["I could replicate this issue in 2.8 . Used two different pose net models  (one from [Tensorflow site ](https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite)and another from [mediapipe](https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection.tflite)) . I saw there is a difference on index value coming from gist and  with Netron view. Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/645dec944c5a06ea24c218039c753c7f/git_55040.ipynb) for reference. Thanks!"]}, {"number": 55035, "title": "The documentation of tf.sparse.softmax has inexecutable example code", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/sparse/softmax\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation of tf.sparse.softmax has inexecutable example code:\r\n```\r\nimport tensorflow as tf\r\n# First batch:\r\n# [?   e.]\r\n# [1.  ? ]\r\n# Second batch:\r\n# [e   ? ]\r\n# [e   e ]\r\nshape = [2, 2, 2]  # 3-D SparseTensor\r\nvalues = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])\r\nindices = np.vstack(np.where(values)).astype(np.int64).T\r\n\r\nresult = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape)) # ValueError\r\n# ...returning a 3-D SparseTensor, equivalent to:\r\n# [?   1.]     [1    ?]\r\n# [1.  ? ] and [.5  .5]\r\n# where ? means implicitly zero.\r\n```\r\nOutputs:\r\n```\r\nValueError: Shape (2, 2, 2) must have rank 1\r\n```\r\n\r\n**Reason**\r\nThe `values` passed to `tf.sparse.SparseTensor` must be a rank-1 tensor instead of a rank-3 tensor.\r\n\r\n**Fix**\r\nThe above code should be changed to:\r\n```\r\nimport tensorflow as tf\r\nshape = [2, 2, 2]  # 3-D SparseTensor\r\nvalues = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])\r\nindices = np.vstack(np.where(values)).astype(np.int64).T\r\nvalues = values[np.where(values)] # Flatten values\r\nresult = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape)) \r\nprint(tf.sparse.to_dense(result))\r\n```\r\nOutput as expected:\r\n```\r\ntf.Tensor(\r\n[[[0.  1. ]\r\n  [1.  0. ]]\r\n\r\n [[1.  0. ]\r\n  [0.5 0.5]]], shape=(2, 2, 2), dtype=float64)\r\n```", "comments": ["Will solve this", "@ArrowIntoTheSky Thank you for raising this issue!\r\nAdded a PR for fix .Thanks!"]}, {"number": 54979, "title": "Per thread allocation of MKL primitives", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\nFrom source using Docker container script from here: https://github.com/ARM-software/Tool-Solutions/tree/master/docker/tensorflow-aarch64 and container available here: https://hub.docker.com/r/armswdev/tensorflow-arm-neoverse\r\n- TensorFlow version (use command below):\r\nv2.8.0-0-g3f878cff5b6 2.8.0\r\n- Python version:\r\n3.8.10\r\n- Bazel version (if compiling from source):\r\n4.2.2\r\n- GCC/Compiler version (if compiling from source):\r\n10.3.0\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n\r\nWe have noticed when running a simple example (https://github.com/ARM-software/Tool-Solutions/blob/master/docker/tensorflow-aarch64/examples/py-api/detect_objects.py) on AArch64 using this container: https://hub.docker.com/r/armswdev/tensorflow-arm-neoverse, with the command line:\r\n\r\n`python detect_objects.py -m ./ssd_resnet34.yml -i https://raw.githubusercontent.com/zhreshold/mxnet-ssd/master/data/demo/street.jpg --inter_threads 64 -r 100`\r\n\r\nthat amount of resident memory grows to ~50GiB. The TF (MKL) build in the container uses oneDNN and Compute Library as backend. When setting TF `inter_op_parallelism_threads` to 1 (via the `--inter_threads` switch in the script) the amount of resident memory is ~2GiB.\r\n\r\nWe have also noticed that on x86, when running the same script using TF (MKL) build with oneDNN backend, memory grows from 900MiB, when then number of inter parallel threads is 1, to around 1.4GiB when it set to 64. The more significant increase observed on AArch64 is due to additional memory per operation required by Compute Library.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe size of allocated memory should not increase due to change in number of inter parallel threads.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nRun the command line referenced above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWe have tracked the issue down to the per-thread caching here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/mkl_util.h#L1860. For some models when using inter parallelism > 1 it seems that TF starts cloning working threads and each of these thread has its own cache; so it is not aware of same primitives created in other threads.\r\n\r\nFor this particular model we counted there are about 50 convolution operations. When setting inter parallelism to 64 the number of convolution operations allocated is `64*50`, where `63*50` are simply duplicated. By making this cache global and shared among these multiple threads we have noticed that resident memory doesn\u2019t grow with increased number of threads and it stays flat to ~2GiB on AArch64 when setting inter parallelism to be 64.", "comments": ["Filing issue as discussed with @penpornk ", "Hi @chunduriv, @jvishnuvardhan (cc: @agramesh1 as he helped us earlier with oneDNN-TF integration), another data point that might be useful is that on AArch64 with oneDNN reference build (without using Compute Library) we are also seeing increase in memory consumption growing from ~1.9GiB when using 2 `inter-threads` to ~7.7GiB when using 32 `inter-threads`. \r\n\r\nAs noted above this memory bloat doesn't happen if we use single global LRU cache. What we are not sure about with this approach is whether between creation of `MklPrimitive` via  `MklPrimitiveFactory` (e.g. creating `MklConvFwdPrimitive` inside of `MklConvFwdPrimitiveFactory`) and its execution there is anything that is not thread safe that will make two or more threads if using same primitive to get in corrupt state. ", "@milpuz01 yes, oneDNN does in some cases uses scratch buffers and allocates them during primitive creation and this could lead to memory bloat.  We are fixing it now so scratch buffer memory is allocated locally when needed.  Here is a PR for fixing it for the matmul primitive https://github.com/tensorflow/tensorflow/pull/54381   We will have PRs in a few days for other primitives.  That should fix the issue.  Also having a global cache may not be thread safe as you mentioned.\r\n\r\nCCing @penpornk ", "@milpuz01 Thank you for creating the issue! \r\n\r\n> For this particular model we counted there are about 50 convolution operations. When setting inter parallelism to 64 the number of convolution operations allocated is 64 * 50, where 63 * 50 are simply duplicated. \r\n\r\nWhat is your `intra_threads` setting? I'm guessing it's also 64? `inter_threads` controls the number of concurrent ops TF can execute at a time, and `intra_threads` controls the number of worker threads (in a separate pool) which all executing ops share. So we expect at most `inter_threads` + `intra_threads` threads running at a time in vanilla TF.\r\n\r\nIIUC, the `inter_threads * intra_threads` (50 * 64) threads here is because TF-oneDNN aarch64 backend is still using separate OpenMP/pthread thread pools from TF. So instead of sharing TF's global intra threads thread pool with other vanilla TF ops, each of the 50 oneDNN convolution ops has its own thread pool of size 64, creating 50*64 more threads. We should be able to avoid this threading explosion when the aarch64 backend switches to Eigen thread pool (sharing TF's intra-thread thread pool). We should fix this over-subscription issue regardless of this per-thread memory issue.\r\n\r\n> another data point that might be useful is that on AArch64 with oneDNN reference build (without using Compute Library) we are also seeing increase in memory consumption growing from ~1.9GiB when using 2 inter-threads to ~7.7GiB when using 32 inter-threads.\r\n\r\nIIRC the reference build is still using separate OpenMP thread pools, so it has the same threading explosion problem. Vanilla TF with oneDNN optimizations shouldn't have the thread explosion (but will still have the LRU cache issue is it is per each intra thread.)\r\n\r\n> oneDNN does in some cases uses scratch buffers and allocates them during primitive creation and this could lead to memory bloat. We are fixing it now so scratch buffer memory is allocated locally when needed. Here is a PR for fixing it for the matmul primitive https://github.com/tensorflow/tensorflow/pull/54381 We will have PRs in a few days for other primitives. That should fix the issue. Also having a global cache may not be thread safe as you mentioned.\r\n\r\n@agramesh1 Thank you for the quick answer and the PR! I'm looking at it now and will ask more questions there.", "Hi @penpornk,\r\n\r\nThank you very much for your reply.\r\n\r\n> What is your intra_threads setting? I'm guessing it's also 64?\r\n\r\nFor this particular example number of `intra_threads` was set to 64. \r\n\r\nThis thread explosion we have also noticed on x86. If we run the same test example in Intel optimised TF docker (available from here: https://hub.docker.com/r/intel/intel-optimized-tensorflow) then total number of threads created by TF when `inter_threads` is 64 and `intra_threads` is 16 is 4177. We have noticed that number of additional threads that is created when changing `inter_threads` both in x86 and AArch64 is function of `OMP_NUM_THREADS`. If `base` is base number of threads when `inter_threads` and `intra_threads`  are 1 then total number of created threads when `inter_threads`  and `intra_threads` are > 1 will be `base + inter_threads*OMP_NUM_THREADS +intra_threads`. This tread explosion we see for this particular model (MLCommons SSD-ResNet34), but for example do not see when using MLCommons ResNet-50 (both run in single stream mode). \r\n\r\nFrom the stack that we were able to obtain it seems these threads are created by TF's session executors (DirectionSession::CreateExecutors) and are independent to number of oneDNN primitives created (for SSD-ResNet34 model there are 117 primitives in total: 51 convolutions, 65 reorders and 1 pooling as reported by `DNNL_VERBOSE`).\r\n\r\nI hope this helps to clarify the issue. ", "Hi @penpornk and @agramesh1, we have been using patch from here: https://github.com/milpuz01/tensorflow/pull/1 that has proof of concept implementation of global cache so memory doesn't grow with increase in number of threads and we have also noticed performance improvement as well as we spend less time in allocating new primitives in parallel threads. Also we didn't notice contention issues where multiple threads are trying to access the same primitive.\r\n\r\nHope this helps."]}, {"number": 54977, "title": "add uint8 gpu kernel", "body": "align with cpu kernel", "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 54975, "title": "Replace insecure `tempfile.mktemp` with `tempfile.mkstemp`", "body": "This PR replace insecure tempfile.mktemp with tempfile.mkstemp,\r\nas the former are insecure and deprecated:\r\nhttps://docs.python.org/3/library/tempfile.html#tempfile.mktemp\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Hi @yongtang Can you please check @mihaimaruseac's comments and keep us posted ? Thank you!"]}, {"number": 54973, "title": "Tensor shape error using TF 2.8.0 with XLA enabled", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 20.04`\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `TF 2.8.0` / `TF 2.6.3` / `tf-nightly 2.9.0-dev20220303`\r\n- Python version: `3.8.10`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: `cuda-driver-dev-11-2, 11.2.152-1`, `libcudnn8, 8.3.2.44-1+cuda11.5`\r\n- GPU model and memory: `RTX 2080 Ti, 11016MiB` / `RTX 8000, 46080MiB`\r\n\r\n**Describe the current behavior**\r\nWhen I upgrade my `TF 2.6.3 -> 2.8.0`, my daily using training script throws me out with error `Must have updates.shape = indices.shape[:batch_dim] + buffer_shape[num_index_dims:], got updates.shape: [32], indices.shape: [320,2], buffer_shape:\r\n [32,10], num_index_dims: 2, and batch_dim: 1`, with setting `TF_XLA_FLAGS=\"--tf_xla_auto_jit=2\"` flag, which used to work well in `TF 2.6.3`. \r\n\r\n**Describe the expected behavior**\r\nExpect still working well like old `TF 2.6.3` time.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): have no idea...\r\n\r\n**Standalone code to reproduce the issue**\r\nThis is my standalone code for reproducing, that simplified most details:\r\n```py\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\nclass NormDense(keras.layers.Layer):\r\n    def __init__(self, units=1000, append_norm=False, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.units, self.append_norm = units, append_norm\r\n\r\n    def build(self, input_shape):\r\n        self.w = self.add_weight(name=\"norm_dense_w\", shape=(input_shape[-1], self.units), trainable=True)\r\n        super().build(input_shape)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        # tf.print(\"tf.reduce_mean(self.w):\", tf.reduce_mean(self.w))\r\n        norm_w = tf.nn.l2_normalize(self.w, axis=0)\r\n        norm_inputs = tf.nn.l2_normalize(inputs, axis=1)\r\n        output = tf.matmul(norm_inputs, norm_w)\r\n        if self.append_norm:\r\n            output = tf.concat([output, tf.norm(inputs, axis=1, keepdims=True) * -1], axis=-1)\r\n        return output\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        config.update({\"units\": self.units, \"append_norm\": self.append_norm})\r\n        return config\r\n\r\n\r\nclass NormDenseLoss(tf.keras.losses.Loss):\r\n    def __init__(self, from_logits=True, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.from_logits = from_logits\r\n\r\n    def call(self, y_true, y_pred):\r\n        if y_pred.shape[-1] == y_true.shape[-1]:\r\n            norm_logits = y_pred\r\n            margin = 0.3\r\n            regularizer_loss = 0.0\r\n        else:\r\n            norm_logits, feature_norm = y_pred[:, :-1], y_pred[:, -1] * -1\r\n            margin = 0.04 * (feature_norm - 10) + 10.0  # This triggers the error\r\n            regularizer_loss = feature_norm / 1e4 + 1.0 / feature_norm\r\n\r\n        pick_cond = tf.where(y_true > 0)\r\n        y_pred_vals = tf.gather_nd(norm_logits, pick_cond)\r\n        theta_valid = y_pred_vals - margin\r\n\r\n        # tf.print(\">>>>\", norm_logits.shape, pick_cond, tf.reduce_sum(tf.cast(y_true > 0, \"float32\")), theta_valid.shape)\r\n        logits = tf.tensor_scatter_nd_update(norm_logits, pick_cond, theta_valid)\r\n        # theta_one_hot = tf.expand_dims(theta_valid, 1) * tf.cast(y_true, dtype=tf.float32)\r\n        # logits = tf.where(tf.cast(y_true, dtype=tf.bool), theta_one_hot, norm_logits)\r\n        # tf.print(\">>>>\", norm_logits.shape, logits.shape, y_true.shape)\r\n        cls_loss = tf.keras.losses.categorical_crossentropy(y_true, logits, from_logits=self.from_logits)\r\n\r\n        # tf.print(\">>>>\", cls_loss.shape, regularizer_loss.shape)\r\n        return cls_loss + regularizer_loss * 35.0\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        config.update({\"from_logits\": self.from_logits})\r\n        return config\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import sys\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n    parser.add_argument(\"--append_norm\", action=\"store_true\", help=\"append norm\")\r\n    args = parser.parse_known_args(sys.argv[1:])[0]\r\n\r\n    xx = tf.random.uniform([160, 32, 32, 3])\r\n    yy = tf.one_hot(tf.cast(tf.random.uniform([160], 0, 10), \"int32\"), 10)\r\n    mm = keras.models.Sequential([keras.layers.Input([32, 32, 3]), keras.layers.Flatten(), keras.layers.Dense(32), NormDense(10, append_norm=args.append_norm)])\r\n    mm.compile(loss=NormDenseLoss(), optimizer=\"adam\")\r\n    mm.fit(xx, yy)\r\n```\r\nRun test using `TF 2.6.3`:\r\n```sh\r\n# Pass\r\nCUDA_VISIBLE_DEVICES='0' TF_XLA_FLAGS=\"--tf_xla_auto_jit=2\" python ./tf_280_xla_test.py\r\n# Pass\r\nCUDA_VISIBLE_DEVICES='0' TF_XLA_FLAGS=\"--tf_xla_auto_jit=2\" python ./tf_280_xla_test.py --append_norm\r\n```\r\nRun test using `TF 2.8.0` / `tf-nightly 2.9.0-dev20220303`:\r\n```sh\r\n# Pass\r\nCUDA_VISIBLE_DEVICES='0' TF_XLA_FLAGS=\"--tf_xla_auto_jit=2\" python ./tf_280_xla_test.py\r\n# Error\r\nCUDA_VISIBLE_DEVICES='0' TF_XLA_FLAGS=\"--tf_xla_auto_jit=2\" python ./tf_280_xla_test.py --append_norm\r\n# Must have updates.shape = indices.shape[:batch_dim] + buffer_shape[num_index_dims:], got updates.shape: [32], indices.shape: [320,2], buffer_shape: [32,10], num_index_dims: 2, and batch_dim: 1\r\n\r\n# Pass\r\nCUDA_VISIBLE_DEVICES='0' python ./tf_280_xla_test.py --append_norm\r\n```\r\nMaybe some part of this script is not needed for this reproduce, not sure. I think something went wrong with `margin = 0.04 * (feature_norm - 10) + 10.0`, but cannot tell what exactly happens here... Please take a check.\r\n", "comments": ["Hi @gadagashwini ! Could you look at this issue ?It is not replicating in [Colab 2.8 ](https://colab.sandbox.google.com/gist/mohantym/5ccec350acf5d226b6ecc9a409c76cd0/github_54973.ipynb)version though .", "@mohantym Uh, right, we can run scripts in colab... Try this [tf_280_xla_test.ipynb](https://colab.research.google.com/drive/1LTVJ7jRRzsODzMuPB-svocV4jcbx1SYY?usp=sharing). Just setting `CUDA_VISIBLE_DEVICES='1'` leaves it no GPU to use in yours, my bad. Updated commands.", "Just verified still exists in `TF 2.9.0-rc0`. Testing results updated in [tf_280_xla_test.ipynb](https://colab.research.google.com/drive/1LTVJ7jRRzsODzMuPB-svocV4jcbx1SYY?usp=sharing). For other versions, `TF 2.7.1` works, and `TF 2.8.0-rc0` throws  error."]}, {"number": 54969, "title": "tf.distribute.experimental.CentralStorageStrategy does not work with tf.keras.layers.Discretization", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.2\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n-   **Python version**: 3.8.8\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: N/A\r\n-   **CUDA/cuDNN version**: CUDA 11.2/cuDNN 8.2.1\r\n-   **GPU model and memory**: two A100 GPUs, each with 40GB GPU memory\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI wrote a subclassed Keras model and use tf.keras.layers.Discretization to preprocessing two numerical features. In the preprocessing layer, the code also uses Normalization/StringLookup/IntegerLookup to preprocess numerical and categorical features. The training data is in TFRecord format and read using TFRecordDataset API.\r\n\r\nThe model training works fine with default strategy, tf.distribute.OneDeviceStrategy(device=\"/gpu:0\"), and tf.distribute.MirroredStrategy(). However, the following error occurred if the distributed strategy is switched to CentralStorageStrategy.\r\n\r\n```\r\nINVALID_ARGUMENT:  indices[28] = -1 is not in [0, 10)\r\n\t [[{{node MMoE/u-age_dis_embedding/embedding_lookup}}]]\r\n```\r\nThis vocabulary size of the u-age_dis_embedding is 10, and the index is computed by the Discretization. I do not quite understand why Discretization output -1 under CentralStorageStrategy, and since the training code works fine with other distributed strategies, I guess this is a bug between CentralStorageStrategy and Discretization.\r\n\r\nHere is the compute device info with CentralStorageStrategy.\r\n`strategy = tf.distribute.experimental.CentralStorageStrategy()`\r\n`INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1'], variable_device = '/device:CPU:0'`\r\n\r\n### Source code / logs\r\nHere is the code snippets and the logs\r\n```\r\n# strategy = tf.distribute.MirroredStrategy() # this works fine\r\n# strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\") # this works fine\r\n# strategy = tf.distribute.get_strategy() # this works fine\r\nstrategy = tf.distribute.experimental.CentralStorageStrategy() #this strategy failed\r\nwith strategy.scope():\r\n    mmoe_model = MMoE(config, preprocessing_layer, name='MMoE')\r\n    optimizer=tf.keras.optimizers.Adam()\r\n\r\n    mmoe_model.compile(\r\n        optimizer=optimizer,\r\n    #     run_eagerly=True,\r\n    #     jit_compile=True,\r\n    )\r\ndataset_train = make_dataset_ceph_v0('2022-01-20/14', num_hours=1, batch_size=4096, block_length=None, data_dir='/group/30039/sample_tfrecord/')\r\nmmoe_model.fit(dataset_train, epochs=1)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-70-b48f111214ea> in <module>\r\n----> 1 mmoe_model.fit(dataset_train, epochs=1)\r\n\r\n/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     52   try:\r\n     53     ctx.ensure_initialized()\r\n---> 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     55                                         inputs, attrs, num_outputs)\r\n     56   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Graph execution error:\r\n\r\nDetected at node 'MMoE/u-age_dis_embedding/embedding_lookup' defined at (most recent call last):\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/threading.py\", line 890, in _bootstrap\r\n      self._bootstrap_inner()\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n      self.run()\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step\r\n      outputs = model.train_step(data)\r\n    File \"<ipython-input-22-cbffbfa5ad65>\", line 245, in train_step\r\n      y_pred = self(x, training=True)  # Forward pass y_pred[task].shape = (batch_size, 1)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 62, in call\r\n      for feature_name, config in self._config.items():\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 68, in call\r\n      if isinstance(embedding_fn, list):\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 69, in call\r\n      for layer in embedding_fn:\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 70, in call\r\n      bottom = layer(bottom)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/layers/embeddings.py\", line 197, in call\r\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\r\nNode: 'MMoE/u-age_dis_embedding/embedding_lookup'\r\nDetected at node 'MMoE/u-age_dis_embedding/embedding_lookup' defined at (most recent call last):\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/threading.py\", line 890, in _bootstrap\r\n      self._bootstrap_inner()\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n      self.run()\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step\r\n      outputs = model.train_step(data)\r\n    File \"<ipython-input-22-cbffbfa5ad65>\", line 245, in train_step\r\n      y_pred = self(x, training=True)  # Forward pass y_pred[task].shape = (batch_size, 1)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 62, in call\r\n      for feature_name, config in self._config.items():\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 68, in call\r\n      if isinstance(embedding_fn, list):\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 69, in call\r\n      for layer in embedding_fn:\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 70, in call\r\n      bottom = layer(bottom)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/layers/embeddings.py\", line 197, in call\r\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\r\nNode: 'MMoE/u-age_dis_embedding/embedding_lookup'\r\nDetected at node 'MMoE/u-age_dis_embedding/embedding_lookup' defined at (most recent call last):\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/threading.py\", line 890, in _bootstrap\r\n      self._bootstrap_inner()\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\r\n      self.run()\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step\r\n      outputs = model.train_step(data)\r\n    File \"<ipython-input-22-cbffbfa5ad65>\", line 245, in train_step\r\n      y_pred = self(x, training=True)  # Forward pass y_pred[task].shape = (batch_size, 1)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 62, in call\r\n      for feature_name, config in self._config.items():\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 68, in call\r\n      if isinstance(embedding_fn, list):\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 69, in call\r\n      for layer in embedding_fn:\r\n    File \"<ipython-input-23-0f0cbc0afd9c>\", line 70, in call\r\n      bottom = layer(bottom)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"/data/miniconda3/envs/env-3.8.8/lib/python3.8/site-packages/keras/layers/embeddings.py\", line 197, in call\r\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\r\nNode: 'MMoE/u-age_dis_embedding/embedding_lookup'\r\n3 root error(s) found.\r\n  (0) INVALID_ARGUMENT:  indices[28] = -1 is not in [0, 10)\r\n\t [[{{node MMoE/u-age_dis_embedding/embedding_lookup}}]]\r\n\t [[replica_1/MMoE/i-grade_embedding/embedding_lookup/_352]]\r\n  (1) INVALID_ARGUMENT:  indices[28] = -1 is not in [0, 10)\r\n\t [[{{node MMoE/u-age_dis_embedding/embedding_lookup}}]]\r\n\t [[Add_41/ReadVariableOp_1/_2648]]\r\n  (2) INVALID_ARGUMENT:  indices[28] = -1 is not in [0, 10)\r\n\t [[{{node MMoE/u-age_dis_embedding/embedding_lookup}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_562346]\r\n```", "comments": []}, {"number": 54958, "title": "Suspicious usage of clGetProgramInfo parameters in CLProgram::GetBinary", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Android 12\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S22\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  tensorflow lite tag 2.8.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Exynos Samsung GPU\r\n\r\n**benchmark build**\r\n- bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark:benchmark_model)\r\n\r\n**run**\r\n/data/local/tmp/benchmark_model --graph=/data/local/tmp/--mini_cnn.tflite --use_gpu=true --num_runs=10 --num_threads=1 --gpu_backend=cl --delegate_serialize_dir=/data/local/tmp/cache --delegate_serialize_token=mini\r\n\r\n**Describe the current behavior**\r\nclGetProgramInfo in CLProgram::GetBinary **parameters**\r\nCL_PROGRAM_BINARIES: 0x1166\r\n**binary_size: 5592** (the current codes are intended for kernel size not address size)\r\n\r\n**Describe the expected behavior**\r\nclGetProgramInfo in CLProgram::GetBinary **parameters**\r\nCL_PROGRAM_BINARIES: 0x1166\r\n**binary_size: sizof(binary_ptr) maybe**\r\n\r\nIn OpenCL Spec(https://www.khronos.org/registry/OpenCL/sdk/2.0/docs/man/xhtml/), it says \r\n- **param_value_size** (third param in clGetProgramInfo)\r\nUsed to specify the size in bytes of memory pointed to by param_value\r\n- **param_value** (forth param in clGetProgramInfo)\r\nReturn type: unsigned char *[] (double pointers) in case of clGetProgramInfo's CL_PROGRAM_BINARIES\r\n\r\nTherefore, param_value_size should be size of  unsigned char * (maybe address)\r\nsize of binary address, not binary itself.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n`  cl_int error_code = clGetProgramInfo(program_, CL_PROGRAM_BINARIES,\r\n                                       sizeof(binary_ptr), &binary_ptr, nullptr);`\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@rubber815 ,\r\n In order to expedite the trouble-shooting process, could you please provide a minimal code and the error log you are facing.Thanks!\r\n", "@tilakrayal \r\nThank you for reply. \r\n\r\nWhat I'm pointing out is that the implementation of CLProgram::GetBinary is different from what the actual OpenCL spec expects.\r\n\r\n`result->resize(result->size() + binary_size);\r\nuint8_t* binary_ptr = result->data() + result->size() - binary_size;\r\n cl_int error_code = clGetProgramInfo(program_, CL_PROGRAM_BINARIES,\r\n                                       binary_size, &binary_ptr, nullptr);`\r\n\r\nclGetProgramInfo in CLProgram::GetBinary parameters\r\nCL_PROGRAM_BINARIES: 0x1166\r\nbinary_size: 5592 (the current codes are intended for kernel size not address size)\r\n\r\nIn OpenCL Spec(https://www.khronos.org/registry/OpenCL/sdk/2.0/docs/man/xhtml/), it says\r\n\r\n- param_value_size (third param in clGetProgramInfo)\r\nUsed to specify the size in bytes of memory pointed to by param_value\r\n- param_value (forth param in clGetProgramInfo)\r\nReturn type: unsigned char *[] (double pointers) in case of clGetProgramInfo's CL_PROGRAM_BINARIES\r\n\r\nTherefore, param_value_size should be size of unsigned char * (maybe address)\r\nsize of binary address, not binary itself."]}, {"number": 54923, "title": "Calling `tf.random.set_seed` with invalid seed breaks the Context!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\na = tf.random.uniform([1, 20]) # All good\r\nprint(a)\r\ntry:\r\n  s = [[1,2]]\r\n  tf.random.set_seed(s) # This will fail\r\nexcept Exception as e:\r\n  print(\"Error:\"+str(e)) # Error:only size-1 arrays can be converted to Python scalars\r\n\r\nprint(tf.add(2,3)) # OK\r\nb = tf.random.uniform([1, 20]) # AttributeError: 'Context' object has no attribute '_rng'\r\n\r\n```\r\n\r\n**Describe the current behavior**\r\n[`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) should be called with an integer, however, although it has validity checking (which appeared to be working), calling `tf.random.set_seed([[1,2]])` will result in an error, but this would break some Context object and the random APIs are not working any more!\r\nError message is:\r\n```\r\nAttributeError: 'Context' object has no attribute '_rng'\r\n```\r\n\r\n", "comments": ["@gadagashwini I was able to reproduce this issue on colab using TF v2.8.0 and tf-nightly,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/dc8a6e97ea66f06ffd041281e92b15ff/54923.ipynb).Thanks!", "Looks like if https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L497 fails, https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/eager/context.py#L489 should also be reverted."]}, {"number": 54911, "title": "TFT convert error: ValueError: Input 0 of node StatefulPartitionedCall/my_model/conv_root/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_1:0 incompatible with expected resource.", "body": "**System information**\r\n- OS is Ubuntu 18.04\r\n- CUDA is 11.5\r\n- Code is being run under `nvcr.io/nvidia/tensorflow:21.12-tf2-py3` (`TensorFlow` version in it is `2.6.2`)\r\n\r\n**Describe the current behavior**\r\nI'm trying to convert a `TensorFlow` saved model to `TF-TRT` using [tf.experimental.tensorrt.Converter](https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter)\r\n\r\nI'm getting the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py\", line 496, in _import_graph_def_internal\r\n    results = c_api.TF_GraphImportGraphDefWithResults(\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node StatefulPartitionedCall/my_model/conv_root/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_1:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tmp_stdconv_model.py\", line 56, in <module>\r\n    converter.convert()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1198, in convert\r\n    frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 1153, in convert_variables_to_constants_v2\r\n    return _construct_concrete_function(func, output_graph_def,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 1078, in _construct_concrete_function\r\n    new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 650, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 621, in wrap_function\r\n    func_graph.func_graph_from_py_func(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 648, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py\", line 549, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py\", line 400, in import_graph_def\r\n    return _import_graph_def_internal(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node StatefulPartitionedCall/my_model/conv_root/AssignVariableOp was passed float from Func/StatefulPartitionedCall/input/_1:0 incompatible with expected resource.\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere's a fully reproducible code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass StdConv(tf.keras.layers.Conv2D):\r\n    \"\"\"Weight Standardization Conv2D.\r\n\r\n    See https://arxiv.org/pdf/1903.10520v1.pdf.\r\n\r\n    \"\"\"\r\n\r\n    def _standardize_wts(self, wts):\r\n        wts_mean = tf.math.reduce_mean(wts, axis=(0, 1, 2), keepdims=True)\r\n        wts_var = tf.math.reduce_variance(wts, axis=(0, 1, 2), keepdims=True)\r\n        return (wts - wts_mean) / tf.math.sqrt(wts_var + 1e-5)\r\n\r\n    def call(self, inputs):\r\n        standardized_wts = self._standardize_wts(self.kernel)\r\n        self.kernel.assign(standardized_wts)\r\n        return super().call(inputs)\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.root = StdConv(\r\n            64,\r\n            kernel_size=7,\r\n            padding=\"same\",\r\n            name=\"conv_root\",\r\n        )\r\n\r\n    def call(self, inputs, training=True):\r\n        return self.root(inputs, training=training)\r\n\r\n\r\nmodel = MyModel()\r\ninp = np.ndarray((1, 256, 256, 3))\r\nop = model(inp, training=False)\r\nprint(\"op shape: \", op.shape)\r\n\r\n# save tf model\r\ntf.keras.models.save_model(model, \"stdconvmodel\")\r\n\r\n\r\n# convert to trt\r\nparams = tf.experimental.tensorrt.ConversionParams(precision_mode=\"FP32\")\r\n\r\nconverter = tf.experimental.tensorrt.Converter(\r\n    input_saved_model_dir=\"stdconvmodel\", conversion_params=params\r\n)\r\n\r\nprint(\"\\nconverter.convert...\")\r\nconverter.convert()\r\nprint(\"\\nconverter.save...\")\r\nconverter.save(trt_model)\r\nprint(\"\\nsaved!\")\r\n```\r\n\r\nI've tried this with a couple of other tensorflow versions, and have also tried using `trt_convert.TrtGraphConverterV2` from `tensorflow.python.compiler.tensorrt`\r\n", "comments": ["@nikshar-symbio,\r\n\r\n> Sorry for encountering this issue in your side. Both TFRT and TFLite converter do not support mutable resource variable use cases yet. We are working on supporting the missing features in the MLIR converter.  [#43833](https://github.com/tensorflow/tensorflow/issues/43833#issuecomment-744087884).\r\n\r\nWorkaround, convert your `\"TF model to ONNX to TRT\"` instead of `\"TF to TRT\"`. Hope this method can help you. Thanks!", "Thanks for that suggestion.\r\nI tried it, and it works, but the model prediction time is significantly slower. The model I'm using is a modified version of ResNet that uses the `StdConv` block show in the code above. Using the standard `model.call` method, it takes `0.02 sec` per prediction, but with the `TRT` model it takes about `3 sec`. I don't know what's causing the issue.\r\n\r\nAs another test, I repeated the same comparison with a standard ResNet50, and I get about `0.004 sec` per prediction in both cases. Interestingly, if I convert this standard ResNet50 model to `TensorFlow-TRT` (using the code in my original question), it takes about `0.003 sec`!"]}, {"number": 54884, "title": "[oneDNN] Redesigning the API of quantized convolution ops/fusions", "body": "This PR consolidates many existing Convolution ops/fusions into few. With the new ops API, single op will cover several fusions. In this PR we introduce 2 ops that replaces about 16 existing ops. The plan is to get rid of the old ops at some point in the future.\r\nFor example all below existing ops/fusions are covered under the new op `_QuantizedConv2D`:\r\n_MklQuantizedConv2D\r\n_MklQuantizedConv2DAndRequantize\r\n_MklQuantizedConv2DWithBias\r\n_MklQuantizedConv2DWithBiasAndRequantize\r\n_MklQuantizedConv2DAndRelu\r\n_MklQuantizedConv2DAndReluAndRequantize\r\n_MklQuantizedConv2DWithBiasAndRelu\r\n_MklQuantizedConv2DWithBiasAndReluAndRequantize\r\n_MklQuantizedConv2DWithBiasSumAndRelu\r\n_MklQuantizedConv2DWithBiasSumAndReluAndRequantize\r\n_MklQuantizedConv2DWithBiasSignedSumAndReluAndRequantize\r\n_MklQuantizedConv2DPerChannel", "comments": ["@penpornk /@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 54854, "title": "[TF:TRT] Fix shape values profile handling", "body": "This PR fixes two issues with TRT profile handling for shape tensors:\r\n- Recognize if input tensor changes size, and mark it as non-shape tensor,\r\n- Do not check shape value profiles for tensors that are not shape value.", "comments": ["@tfeher This PR is in draft, any update on this? Please. Thank you!"]}, {"number": 54849, "title": "`tf.ragged.row_splits_to_segment_ids` lack input validation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nsplits = [-16, 4, 2, 5, 5, 7]\r\nresult = tf.ragged.row_splits_to_segment_ids(splits) # pass, but it should throw ValueError as splits starts with -16\r\nprint(result) \r\n```\r\n\r\n**Describe the current behavior**\r\n[`tf.ragged.row_splits_to_segment_ids`](https://www.tensorflow.org/api_docs/python/tf/ragged/row_splits_to_segment_ids?hl=en) should check `splits` starts with `0`, and throw `ValueError` if invalid.", "comments": ["Thanks @ArrowIntoTheSky ! Added a PR #54920 to address this issue. ", "Hi @chunduriv ! Could you please look at this issue?"]}, {"number": 54848, "title": "`tf.linalg.tensor_diag_part` lack input dimension checking", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ninput = 6.0\r\ntf.linalg.tensor_diag_part(input) # pass, but it should fail instead\r\n```\r\n\r\n**Describe the current behavior**\r\n[`tf.linalg.tensor_diag_part`](https://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag_part) should accept `input` with rank `2k`, and at least dim 2. It should throw an error in the example code where the shape in `[]`.", "comments": ["will fix this\r\n"]}]