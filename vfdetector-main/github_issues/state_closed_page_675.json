[{"number": 33348, "title": "Variable creation fails in TF-Nightly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):  > 2.1.0-dev20191012\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nVariable creation currently fails with:\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 261, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 255, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 236, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 2645, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 263, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1410, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1556, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 231, in eager_safe_variable_handle\r\n    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 167, in _variable_handle_from_shape_and_dtype\r\n    handle_data.shape_and_type.append(\r\nAttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'append'\r\n```\r\n**Code to reproduce the issue**\r\nFrom example: https://www.tensorflow.org/guide/variable#create_a_variable\r\n```\r\nimport tensorflow as tf\r\nvar = tf.Variable(tf.zeros([1., 2., 3.]))\r\n```\r\n\r\n**Other info / logs**\r\nI believe this is occured after aa25ad70c021968fb3a4a93ee814ca2fa699b32b \r\n\r\ncc @mrry \r\n", "comments": ["That's odd, because this code works in our CI. Which `protobuf` version are you using? It looks like `RepeatedCompositeContainer.append()` has been available since https://github.com/protocolbuffers/protobuf/commit/d8c2501b43c1b56e3efa74048a18f8ce06ba07fe , so we might need to bump a requirement.", "Hmmm this is for the binary pip installed `tf-nightly` so not sure what version protobuf that is built with. Quick glance looks [like maybe 3.3.1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/protobuf/protobuf_optimized_pip.sh#L17), but I'm not overly familiar with the toolchain for TF-Core. \r\n\r\nAs a data point... variable creation for today's MacOS nightly wheel works fine, however the linux wheel appears broken. Is there a mismatch in protobuf versions for the nightly builds?\r\nhttps://colab.research.google.com/drive/11swg1klzul_ozOQZa61Wz-zoBJg25He3\r\n\r\ncc @angerson \r\n", "For binaries, the PIP dependency is `'protobuf >= 3.6.1'`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L64\r\n\r\nRight enough, https://github.com/protocolbuffers/protobuf/commit/d8c2501b43c1b56e3efa74048a18f8ce06ba07fe only appears from 3.8.0 onwards.\r\n\r\nIf you do `pip install -U protobuf=3.8.0` in your Linux env, does that fix the problem?", "Yup upgrade protobuf solves the issue:\r\nhttps://colab.research.google.com/drive/11swg1klzul_ozOQZa61Wz-zoBJg25He3\r\n\r\nCan we bump the requirements in `setup.py`", "Thanks for confirming! I just sent an PR to bump the requirement.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33348\">No</a>\n", "When I upgrade protobuf from version 3.6.0 to any higher versions, its showing \r\n\"ImportError: DLL load failed: The specified procedure could not be found\". Earlier I was using 3.11 but that was giving me the above error but someone suggested to downgrade the version to 3.6. After downgrading its now showing this same attribute error.\r\n\r\n\r\nEDIT: Was using tensorflow version 2.1. Downgrading it to 2.0 resolved the issue. Though it is still telling me to upgrade protobuf version(currently using 3.6 to avoid DLL import failed issue).", "python3.6 (anaconda)\r\ntensorflow2.1.0\r\n![image](https://user-images.githubusercontent.com/52851180/80276101-c4dcf000-8718-11ea-9165-be33091e3157.png)\r\n![image](https://user-images.githubusercontent.com/52851180/80276107-d0c8b200-8718-11ea-82c1-05541eac48d0.png)\r\n\r\nThat(\"tf.Variable(5)\") used by tensorflow2.0.0 is ok, but it isn't used by tensorflow2.1.0.\r\nwhat I can do some change", "It might help other users if I report that I had error messages\r\n`AttributeError: 'RepeatedCompositeFieldContainer' object has no attribute 'append'`\r\ntrying to run basic Tensorflow functionalities, like  \r\n`model = tf.keras.models.Sequential(...)`\r\nor\r\n`pred = model.predict(dataset_X)`, where instead of training a model, I had specified it using `tf.constant_initializer(...)`.\r\n\r\nThis happened on a fresh install of Python 3.8 and Tensorflow 2.2, but also previously on Python (~)3.7 with Tensorflow 2.1, while it was all functioning in Tensorflow 2.0.\r\n\r\nThe solution was to upgrade `protobuf` from 3.6.1 to 3.11.3, and also to make sure that the `PYTHONPATH` was correct. My modified `~/.bashrc` had to be updated so that \r\n`export PYTHONPATH=\"/usr/local/lib64/python3.6/site-packages\"`\r\nwas replaced with\r\n`export PYTHONPATH=\"/usr/local/lib64/python3.8/site-packages\"`\r\nOtherwise Python found exclusively the older version of protobuf in /usr/local/lib64/python3.6/site-packages."]}, {"number": 33347, "title": "tf.data pipeline on TPU slows when upgrading from v1.12 -> v1.14", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12.0 & 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A (TPU)\r\n- GPU model and memory: N/A (TPU)\r\n\r\n**Describe the current behavior**\r\n\r\nTraining a custom estimator on TPU with a `tf.data` pipeline that reads from TFRecords performs well on v1.12. Profiling with the TPU profiler shows that very little of the step time is spent waiting for input.\r\n\r\nAfter upgrading to v1.14, training is slowed down, and the TPU profiler reports that much more of the step time is spent waiting for input (~67.1%). The global steps/sec falls from ~10 steps/sec to ~6 steps/sec.\r\n\r\nFixing deprecation warnings (e.g. not using `.make_one_shot_iterator().get_next()`) doesn't seem to help.\r\n\r\nOur pipeline appears to follow the [`tf.data` performance guidelines](https://www.tensorflow.org/guide/data_performance) except for not using `tf.data.experimental.AUTOTUNE` -- using this parameter doesn't recover performance either.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would not expect upgrading the library to slow down data infeed.\r\n\r\n**Code to reproduce the issue**\r\n\r\nOur `tf.data` pipeline looks roughly like:\r\n```python\r\ndataset = tf.data.TFRecordDataset(\r\n    tf.data.Dataset.list_files(pattern, shuffle=True),\r\n    buffer_size=(32 * 1024 * 1024),\r\n    num_parallel_reads=8,\r\n)\r\ndataset = dataset.shuffle(shuffle_buffer_size) \\\r\n                 .repeat() \\\r\n                 .map(parse_fn, num_parallel_calls=8) \\\r\n                 .batch(batch_size, drop_remainder=True) \\\r\n                 .prefetch(4)\r\n```\r\nTuning parameters after upgrading doesn't recover any performance, nor does using `tf.data.experimental.AUTOTUNE`.\r\n\r\n**Other info / logs**\r\n\r\nHere are the overview pages from the TPU profiler on TensorBoard:\r\n\r\nTPU profiler v1.12:\r\n![v1-12](https://user-images.githubusercontent.com/3229244/66775978-2a9f1400-ee7a-11e9-9a7c-66af8da8829d.png)\r\n\r\nTPU profiler v1.14:\r\n![v1-14](https://user-images.githubusercontent.com/3229244/66775984-2ecb3180-ee7a-11e9-8f77-435ce9b0236e.png)\r\n\r\nLet me know if there's any other information that I can provide!", "comments": ["@zo7 ,\r\nThank you for reporting, can you please provide complete code to replicate the above issue ? ", "Sure, I can see if I can create an example to replicate the issue sometime today. Didn't want to post company code here.", "@zo7 ,\r\nHi,any update on the issue ?Thanks!", "I've been working on a minimal example to share, but I can't seem to replicate the issue with it... I'll work on it some more, but it's not clear what might be causing the slowdown.", "@zo7 ,\r\nHi, no update on the minimal example yet?Thanks!", "No, I haven't had too much time to work on this and it's unclear what's causing it...", "@zo7 ,\r\nI will close the issue for now, once the new information is available kindly reopen the issue. Thanks!", "Sorry, I will reopen if I am able to reproduce.\r\n\r\nHowever I will leave one last nugget: Upgrading from v1.14 -> v1.15 makes it even *worse*. Our model goes from ~10 steps/sec (v1.12) -> ~6 steps/sec (v1.14) -> ~3 steps/sec (v1.15)\r\n\r\n<img width=\"1073\" alt=\"Screen Shot 2019-11-06 at 2 49 31 PM\" src=\"https://user-images.githubusercontent.com/3229244/68345010-35c61800-00a5-11ea-89a7-618e7ca41f05.png\">\r\n\r\nI cannot reproduce without sharing our code + data, but our experience as users upgrading the past few versions has been *terrible*."]}, {"number": 33346, "title": "Update Build instructions for Docker 19.03", "body": "## Description of issue (what needs changing):\r\n\r\nDocker Build from Source documentation seems to be out of date for docker 19.03.\r\nFor example there is no --runtime=nvidia flag available any more.", "comments": ["yes. as stated [here](https://www.tensorflow.org/install/docker) the command needs to be changed.\r\n\r\ndocker < 19.03:\r\ndocker run --runtime=nvidia -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel-gpu-py3 bash\r\n\r\ndocker >= 19.03:\r\ndocker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel-gpu-py3 bash\r\n", "Added PR #33367 with docs updated.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33346\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33346\">No</a>\n"]}, {"number": 33345, "title": "keras optimizer `apply_gradients` arg `grads_and_vars` has wrong type in documentation", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/516c98da7b7d8526c153827c426c675a4ece9543/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L414\r\n\r\n## Description of issue (what needs changing):\r\n\r\n`grads_and_vars` is documented as list but is actually passed as `zip object`.\r\n\r\n### Clear description\r\n\r\nThis can be problematic when writing custom optimizers that iterate over the `grads_and_vars` multiple times, as in that case a `zip object` will not give the intended behaviour.\r\n\r\n", "comments": ["@koenhelwegen, Since the associated PR has been merged, are you happy to close if no issue persists. Thanks!", "Hi, thanks for taking this up. PR is in a separate repo where the change from `list` to `zip object` resulted in a bug in a custom optimizer. The TF documentation is still incorrect.", "@koenhelwegen Are you willing to contribute through PR? Link this issue in the PR and it will automatically closes once the PR merges. Thanks!", "At a closer look it seems the `grads_and_vars` zip is usually converted to a list before it's passed to `apply_gradients` (e.g. [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L501), but not [here](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/training_eager.py#L272)), so I'm not sure if this is a documentation error or that this is a bug in `training_eager.py`.", "@koenhelwegen,\r\nSince the associated PR has been merged,please confirm if the issue can be closed. Thanks!"]}, {"number": 33344, "title": "Bugs in tf.data.experimental.make_csv_dataset", "body": "I believe these are two bugs in tf.data.experimental.make_csv_dataset. I am using the examples from https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset.\r\n\r\n- OS Platform and Distribution: Ubuntu 19.04\r\n- TensorFlow installed from (source or binary): binary via pip3\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: GTX1060 / 6GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have a CSV dataset consisting of 50K lines, and with each line having ~6500 integer valued columns such as this:\r\n\r\n```\r\n1,9720798423,89,3,7537296578,98,2...8470364829,91,0\r\n2,7984338623,32,1,4023987716,98,2...3876678829,91,3\r\n```\r\nBased on the length of each line and size of the dataset I can't store it in memory, so I am attempting to stream the CVS from disk using this function:\r\n\r\n```\r\nBATCHSIZE = 5\r\nSELECT_COLUMNS = ['index', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',...'aaab', 'aaac', 'aaad']\r\nDEFAULTS = [tf.int32, tf.int32, tf.int32, ... tf.int32]\r\n\r\ntrain_dataset = tf.data.experimental.make_csv_dataset(\r\n    sys.argv[1],\r\n    batch_size = BATCHSIZE,\r\n    column_names = SELECT_COLUMNS,\r\n    column_defaults = DEFAULTS,\r\n    label_name = LABEL_COLUMN,\r\n    field_delim=',',\r\n    use_quote_delim=True,\r\n    na_value='',\r\n    header=False,\r\n    num_epochs=1,\r\n    shuffle=True,\r\n    shuffle_buffer_size=500,\r\n    shuffle_seed=None,\r\n    num_parallel_reads=1,\r\n    sloppy=False,\r\n    num_rows_for_inference=1000,\r\n    compression_type=None,\r\n    ignore_errors=False\r\n)\r\n```\r\nHere is the error I am getting:\r\n```\r\n\r\n2019-10-14 13:06:40.698090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5183 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\n2019-10-14 13:07:08.579307: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at iterator_ops.cc:929 : Invalid argument: Field 5 in record is not a valid int32: 7537296578\r\nTraceback (most recent call last):\r\n  File \"/home/gp/dm/aae.py\", line 77, in <module>\r\n    for element in train_dataset:\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 622, in __next__\r\n    return self.next()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 666, in next\r\n    return self._next_internal()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 651, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 2673, in iterator_get_next_sync\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Field 5 in record is not a valid int32: 7537296578 [Op:IteratorGetNextSync]\r\n\r\n\r\n```\r\nI have inspected the records for control characters and whitespace, everything appears to be fine and I can copy and paste that integer value into an interactive Python terminal and assign it to an int32.\r\n\r\nAlso, it would appear prefetch_buffer_size=dataset_ops.AUTOTUNE is not a valid option:\r\n\r\n```\r\n/usr/bin/python3.7 /home/aj/ga.py dataset.csv\r\nLoading data...\r\nTraceback (most recent call last):\r\n  File \"/home/aj/ga.py\", line 68, in <module>\r\n    prefetch_buffer_size=dataset_ops.AUTOTUNE,\r\nNameError: name 'dataset_ops' is not defined\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nFor each each column in the CSV to be assigned an int32 during each row fetch, and for the prefetch_buffer_size option to be implemented according to the most recent documentation.", "comments": ["@tb438 I think `7537296578` is larger than `int32`, you may have to change it to `int64` instead.", "Yep looks like that was the issue with int32, thanks", "@tb438 \r\n\r\nCan we close the issue since the query is been answered. Let us know. Thanks!\r\n", "Sure thing, thanks"]}, {"number": 33343, "title": "symmetric 16-bit activation quantization", "body": "This PR extends the 8-bit quantization to support 16-bit activations within TensorFlow Lite. Each activation is of type int16_t and symmetric about zero. The weight tensor precision remains at 8-bit signed values.\r\n\r\nIn this PR we tested and enabled just one kernel reference function to run a simple convolution-based model with 16-bit activations:\r\n\r\n- CONV_2D\r\n\r\nThe new option TFLITE_BUILTINS_INT16 in OpsSet is added to enable quantization with 16-bit activations. The following is the example of the usage:\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT16]\r\ntflite_quant_model = converter.convert()", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33343) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33343) for more info**.\n\n<!-- ok -->", "@petewarden can you please review this PR ?", "We'd need to consider this further but to start, is there a motivating use case (e.g. a specific commonly-used model or task) on how 16-bit symmetric activation support in Conv would benefit a TensorFlow Lite user over the existing 8-bit asymmetric activation support from the [quantization spec]( https://www.tensorflow.org/lite/performance/quantization_spec)(with much greater op coverage and optimized kernel coverage)?", "Thanks @alanchiao for checking this pull request. Example use cases for 16-bit Conv op that we have come across are CNN models for image segmentation and image enhancement, such as super-resolution, HDR from a single image, and de-noising.\r\nWe have several more 16-bit operators implemented and ready for submission. We did not include them to this PR to make the initial review easier.", "> Example use cases for 16-bit Conv op that we have come across are CNN models for image segmentation and image enhancement, such as super-resolution, HDR from a single image, and de-noising.\r\n\r\n**Let us have meetings** to go into detail on these motivating use cases and then bring back the results here. In particular, we should examine the impact on model accuracy across these tasks when using the existing [8-bit scheme]( https://www.tensorflow.org/lite/performance/quantization_spec) versus the proposed 16-bit activation scheme as well as carefully inspect the cases with significant differences.\r\n\r\nWith a 16-bit activation scheme, it'd also be valuable to consider asymmetric (to more closely match the 8-bit scheme) vs symmetric. \r\n\r\nIntroducing a set of 16-bit operators with a particular quantization scheme has the following implications amongst others:\r\n- Increase in quantization-scheme related complexity (relevant to hardware vendors) due to extending the current spec from  [quantization spec](https://www.tensorflow.org/lite/performance/quantization_spec). If we have public information regarding impact on model accuracy on the tasks (ideally reproducible), then it'll be a way to clarify and motivate other hardware vendors to also add support and determine priorities. \r\n- Increase in TFLite binary size with default operators\r\n- Without other steps, an ambiguous user story (should they use 8-bit or 16-bit .. task coverage varies due to op coverage differences)? There are ways of mitigating this that can be discussed", "> Let us have meetings to go into detail on these motivating use cases and then bring back the results here.\r\n\r\nThanks @alanchiao. I'll set something up for the next week.", "@jdduke, @suharshs, can you please review this PR?", "> I think my main question at this point is the API:\r\n> \r\n> ```\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT16]\r\n> ```\r\n> \r\n> In my mind, that's a bit misleading, and doesn't really capture what is actually happening with conversion and how this flag is quite different than using TFLITE_BUILTINS_INT8. Did you all discuss the API in your offline discussion?\r\n\r\nHi Jared, \r\nThank you for the review! We have not discussed the API. I agree that this name is not ideal and could be misleading. It should be renamed, probably, to TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8 or something similar to reflect properly what is going. Or, should we use completely different mechanism to set this type of conversion ?   ", " > What happens in the conversion with this change if an operation doesn't have a 16 bit activation kernel? Does it get left in float or uses the \"hybrid\" kernels or is an error?\r\n\r\nThanks for the review !\r\n\r\nAll operations that are supported in 8-bit can be converted into 16-bit, but if reference kernel for this operation for 16-bit is absent, then it will be an error.\r\n\r\n\r\n", "Again, thanks for your patience, the holiday break certainly hasn't helped move things along.\r\n\r\nSuharsh will send out some notes/follow-up questions about the API and performance, but otherwise I think we're good to go ahead with a more detailed review.", "Hi, sorry for the delays, been in and out frequently during the break.\r\n\r\nLet's do the following when exposing this via the API, there should be two valid states.\r\n\r\nsupported_ops = [TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8] : should throw an error if an op doesn't have the 16bit activations 8 bits weight version.\r\n\r\nsupported_ops = [TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8, TFLITE_BUILTINS]: Should fall back to the floating point builtin operation and and add the corresponding quantize and dequantize operations in between 16bit and float operations, so that only supported 16bit operations are quantized and no error is thrown.\r\n\r\nDoes that make sense?", "@suharshs yes, it sounds good. We will need to update our mplementation.\r\nAbout this PR: Thanks for reviewing it. We realize that it is a big chunk of work and it is difficult to review. What is a best way to go forward ? Should we just update this PR according to the suggestion ? Or, should we create a new one ? Is it easy for review/merge if we create separate PRs with just 16-bit reference kernels implementations ?", "I think a separate PRs for the reference implementations and a separate PR for the tooling changes would be great! That way we can iterate on the tooling and pipeline review of each reference op implementation.\r\n\r\nAlso, are optimized CPU implementations on your roadmap as well for these ops after the reference code and tooling is submitted?", "Thanks, @suharshs.\r\n\r\n>I think a separate PRs for the reference implementations and a separate PR for the tooling changes would be great! That way we can iterate on the tooling and pipeline review of each reference op implementation.\r\n\r\nJust to clarify - are you suggesting that we can merge this PR as is, and introduce the changes you propose for supported_ops as a separate PR? Do you have any other issues that need to be resolved for this PR to progress?\r\n\r\n> Also, are optimized CPU implementations on your roadmap as well for these ops after the reference code and tooling is submitted?\r\n\r\nOur initial plans are to introduce the tooling and reference kernels for 16-bit activations. We have tentative plans for optimized implementations. Our other relevant efforts are currently focused on implementing corresponding kernels in TensorFlow Lite Micro, but that work depends on the progress with introducing the reference code to TFLite first.", "> Just to clarify - are you suggesting that we can merge this PR as is, and introduce the changes you propose for supported_ops as a separate PR? Do you have any other issues that need to be resolved for this PR to progress?\r\n> \r\nNo, I am suggesting that once the API comment is addressed in https://github.com/tensorflow/tensorflow/pull/33343#issuecomment-572251334, we can move forward with the tooling change and review this PR as is. I meant that for additional PRs one reference kernel implementation per PR would make sense and help us review things faster.\r\n> \r\n> Our initial plans are to introduce the tooling and reference kernels for 16-bit activations. We have tentative plans for optimized implementations. Our other relevant efforts are currently focused on implementing corresponding kernels in TensorFlow Lite Micro, but that work depends on the progress with introducing the reference code to TFLite first.\r\n\r\nMakes sense, thanks!\r\n", "Per discussion, I close this PR and open the first one with just CONV_2D reference kernel\r\nhttps://github.com/tensorflow/tensorflow/pull/35946"]}, {"number": 33342, "title": "Update Gradient function description", "body": "Update Gradient function description for #33326", "comments": ["Thank you for your contribution ,but we do are not accepting small document changes.\r\nCC @mihaimaruseac "]}, {"number": 33341, "title": "TF lite implementation for RandomStandardNormal", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Docker image tf-nightly-gpu-py3\r\n- TensorFlow version (or github SHA if from source): 3.6.8\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, EXP, FULLY_CONNECTED, LEAKY_RELU, LOG, MUL. Here is a list of operators for which you will need custom implementations: RandomStandardNormal.\r\n```\r\n\r\nVAE model was taken from your custom layers and models site ([here](https://www.tensorflow.org/guide/keras/custom_layers_and_models#building_models)). The line of code producing this exception is\r\n`epsilon = tf.keras.backend.random_normal(shape=(batch, dim))`.\r\n\r\nIt might be a good idea to add a tflite implementation for this function so that people can try a conversion with your existing examples.\r\n", "comments": ["@jaeyoo Do you have any updates regarding the implementation? There seems to be some work in progress for the related issue #34277 [here](https://github.com/orgs/tensorflow/projects/8#card-32750656), but not for the actual implementation. \r\n\r\nIf you have any experimental implementation for testing, please let me know.", "Hi, @DocDriven,\r\n\r\nI could convert the model in tf 2.2.0 with the select_ops\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(vae)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\nCould you try it?\r\n", "Hey @jaeyoo I am able to convert my model using SELECT_TF_OPS, but that greatly increases the size of the libtensorflowlite library.\r\n\r\nI am trying to get my model running on Android and this is the one operator in my way from >100 megabytes decrease in app download size.\r\n\r\nNot only that, but I am trying to use the tensorflowlite c api on android/arm64 and it doesn't seem like android arm compilation is supported when using flex delegate:\r\n\r\n```\r\nERROR: /Users/corey/Workspace/omnivor-io/gst-sync-android/app/src/main/jni/tensorflow/tensorflow/lite/delegates/flex/BUILD:76:1: C++ compilation of rule '//tensorflow/lite/delegates/flex:delegate_only_runtime' failed (Exit 1): clang failed: error executing command \r\n  (cd /private/var/tmp/_bazel_corey/9e892e94b2950203c06bbbfc8119bc78/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=29.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=26 \\\r\n    ANDROID_NDK_HOME=/Users/corey/library/Android/Sdk/ndk-bundle \\\r\n    ANDROID_SDK_API_LEVEL=29 \\\r\n    ANDROID_SDK_HOME=/Users/corey/library/Android/Sdk \\\r\n    PATH=/Users/corey/.pub-cache/bin:/Users/corey/Workspace/depot_tools:/usr/local/opt/llvm/bin:/Users/corey/Downloads/google-cloud-sdk/bin:/Users/corey/.rbenv/shims:/Users/corey/Library/Android/sdk/platform-tools:/Users/corey/Library/Android/sdk/tools:/Users/corey/.fastlane/bin:/Users/corey/.local/bin:/Library/Frameworks/GStreamer.framework/Versions/Current/bin/:/Users/corey/.poetry/bin:/Users/corey/Library/Android/sdk/tools/bin/:/Applications/Postgres.app/Contents/Versions/10/bin:/Users/corey/.pyenv/shims:/usr/local/bin:/Users/corey/local/bin:/Users/corey/.npm-global/node/bin:/Users/corey/.npm-global/bin:/Users/corey/.poetry/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/corey/.ebcli-virtual-env/executables:/Users/corey/.pub-cache/bin:/Users/corey/Workspace/depot_tools:/usr/local/opt/llvm/bin:/Users/corey/Downloads/google-cloud-sdk/bin:/Users/corey/.rbenv/shims:/Users/corey/Library/Android/sdk/platform-tools:/Users/corey/Library/Android/sdk/tools:/Users/corey/.fastlane/bin:/Users/corey/.local/bin:/Library/Frameworks/GStreamer.framework/Versions/Current/bin/:/Users/corey/.poetry/bin:/Users/corey/Library/Android/sdk/tools/bin/:/Applications/Postgres.app/Contents/Versions/10/bin:/Users/corey/.pyenv/shims:/Users/corey/local/bin:/Users/corey/.npm-global/node/bin:/Users/corey/.npm-global/bin:/Users/corey/flutter/bin:/Users/corey/bin:/Users/corey/flutter/bin:/Users/corey/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/Users/corey/.pyenv/versions/3.7.3/bin/python3 \\\r\n    PYTHON_LIB_PATH=/Users/corey/.pyenv/versions/3.7.3/lib/python3.7/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_ENABLE_XLA=1 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=26' -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig '-Werror=return-type' '-Werror=int-to-pointer-cast' '-Werror=pointer-to-int-cast' '-Werror=implicit-function-declaration' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/flex/_objs/delegate_only_runtime/delegate.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/flex/_objs/delegate_only_runtime/delegate.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DSUPPORT_SELECTIVE_REGISTRATION -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm64-v8a-opt/bin/external/local_config_sycl -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/bin/external/double_conversion -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -iquote external/flatbuffers -iquote bazel-out/arm64-v8a-opt/bin/external/flatbuffers -Ibazel-out/arm64-v8a-opt/bin/external/flatbuffers/_virtual_includes/flatbuffers -Ibazel-out/arm64-v8a-opt/bin/external/flatbuffers/src/_virtual_includes/flatbuffers -Ibazel-out/arm64-v8a-opt/bin/external/flatbuffers/_virtual_includes/runtime_cc -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/arm64-v8a-opt/bin/external/double_conversion -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem tensorflow/lite/schema -isystem bazel-out/arm64-v8a-opt/bin/tensorflow/lite/schema -w '-std=c++14' '--sysroot=external/androidndk/ndk/platforms/android-26/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/lite/delegates/flex/delegate.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/flex/_objs/delegate_only_runtime/delegate.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ntensorflow/lite/delegates/flex/delegate.cc:143:44: error: 'TF_AcquireFlexDelegate' has C-linkage specified, but returns user-defined type 'tflite::TfLiteDelegateUniquePtr' (aka 'unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate *)>') which is incompatible with C [-Werror,-Wreturn-type-c-linkage]\r\nextern \"C\" tflite::TfLiteDelegateUniquePtr TF_AcquireFlexDelegate() {\r\n                                           ^\r\n1 error generated.\r\nTarget //tensorflow/lite/c:libtensorflowlite_c.so failed to build\r\n```\r\n\r\nSo for my use case, I believe I need RandomStandardNormal without flex delegate / SELECT_TF_OPS.", "I noticed a commit in `v2.4.0-rc0` that has create a custom prototype for random standard normal:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/11823c6179a5e2ca6ac4f8480aa00c42c772885e\r\n\r\nI've compiled this library but I'm getting the error:\r\n\r\n```\r\n2020-11-04 17:52:41.409 25623-25623/io.kernellabs.sync E/tflite: Encountered unresolved custom op: RandomStandardNormal.\r\n2020-11-04 17:52:41.409 25623-25623/io.kernellabs.sync E/tflite: Node number 1 (RandomStandardNormal) failed to prepare.\r\n```\r\n\r\nAny guidance on how I can register the op using the c api?", "@CoreyCole Hello, Did you manage to find a solution?\r\n", "@henry8th yes, hope this helps: https://github.com/tensorflow/tensorflow/issues/44664#issuecomment-723310060", "Thank you\n\nOn Fri, 19 Feb 2021, 19:12 Corey Cole, <notifications@github.com> wrote:\n\n> @henry8th <https://github.com/henry8th> yes, hope this helps: #44664\n> (comment)\n> <https://github.com/tensorflow/tensorflow/issues/44664#issuecomment-723310060>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33341#issuecomment-782283099>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AP52P6ZVPA7JDAQZXJIA7OTS72ZYTANCNFSM4JAQ3SCA>\n> .\n>\n", "@DocDriven  Is this still an issue for you? If not, Please go ahead and close the issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33341\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33341\">No</a>\n"]}, {"number": 33340, "title": "Significant prediction slowdown after model.compile()", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.4\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n\r\n**Describe the current behavior**\r\nThe prediction speed is slowed down a lot after `model.compile()` call.\r\n\r\n**Describe the expected behavior**\r\nSpeed should not be affected. Predict function is used by users assuming that it will work fast because we use it all the time in production. It should not cause surprise to users.\r\n\r\n**Code to reproduce the issue**\r\nhttps://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true\r\n\r\n![image](https://user-images.githubusercontent.com/15215732/66762282-e3dc0900-eecf-11e9-8d93-82c8bcc5325b.png)\r\n", "comments": ["[Relevant SO](https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58378941#58378941), and another minimal reproducible example:\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\nfrom time import time\r\n\r\ndef timeit(func, arg, iterations):\r\n    t0 = time()\r\n    for _ in range(iterations):\r\n        func(arg)\r\n    print(\"%.4f sec\" % (time() - t0))\r\n\r\nipt   = Input(shape=(4,))\r\nx     = Dense(2, activation='relu')(ipt)\r\nout   = Dense(1, activation='sigmoid')(x)\r\nmodel = Model(ipt, out)\r\n\r\nX = np.random.randn(32,4)\r\n\r\ntimeit(model.predict, X, 1000)\r\nmodel.compile('adam', loss='binary_crossentropy')\r\ntimeit(model.predict, X, 1000)\r\nmodel._make_train_function()  # build optimizer\r\ntimeit(model.predict, X, 1000)\r\n```\r\n\r\n**Outputs**:\r\n\r\n```python\r\n0.9891 sec\r\n29.785 sec\r\n29.521 sec\r\n```\r\n\r\nThat's a **30-fold slowdown**. Worse yet, building the optimizer does not elicit any further slowdowns - so \"graph size\" may not be the main explanation here.", "[Solved](https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58385156#58385156).", "@off99555 ,\r\nCan you confirm if the issue is resolved?Thanks!", "It does not seem to me that it is resolved. It's more like we know how the issue occurs but we don't have a solution, just workaround. I need to compare timing between compiled and non-compiled version and see which is faster. But I don't think a user will be aware of this in general. So we should make a better solution.\r\nIn this case, should I close the issue or keep it open?", "@off99555 I'd agree to request a documentation improvement from TensorFlow to notify users of this, but I doubt any code-level changes will be implemented to address this as it'd require revamping a massive portion of TF graph. It's up to the user to be aware of functionality differences and adjust accordingly - but admittedly, while this isn't the only issue where a workaround is required, other cases are at least documented. ", "@off99555 I cannot reproduce the issue. When I ran it in colab, computation time is little more but not as much as you reported. I have checked some other models also. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d91713318125c34768c5f8841e60b928/untitled582.ipynb). \r\n\r\n@OverLordGoldDragon I could reproduce your issue. The reason is that when the model is very small, it takes more time to predict after compilation. I have checked other similar small models and I noticed it. Thanks!", "> The reason is that when the model is very small, it takes more time to predict after compilation\r\n\r\n@jvishnuvardhan That is the problem, yes, but not its explanation - I've done the latter [here](https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58385156#58385156). Further, it's not the model size, but model size _relative to_ data size. \r\n\r\nAs noted by @off99555 , the solution only works for those aware of the problem and its workaround - this issue, however, isn't documented or mentioned in a docstring. To remedy, some form of mention should be made in documentation or `compile` docstring as a disclaimer.", "As OverLordGoldDragon mentioned, you can disable the `experimental_run_tf_function` flag for a Keras model and force it to the old v1 execution path (I think). It's a bit nicer than disabling eager execution globally. \r\n\r\nThe good news is that you can do this by simply passing the param to the compile() function, so you don't need to make any private calls. Like so: `model.compile(loss=loss, optimizer=optimizer, experimental_run_tf_function=False)`\r\n\r\nThis restores the performance for me (I experienced more than 10x slowdown). Let me second everybody in that I'm surprised this is not a big issue - the parameter is on the public interface of a major release, but it's not even mentioned in the docstring.", "Noone assigned; if it doesn't cause a compile error, doesn't mean it isn't a worthwhile problem. Has anyone made a docstring PR on this yet? If not, I could", "Hi. Let me try to address some of the questions here and see if that helps.\r\n\r\n> Has anyone made a docstring PR on this yet?\r\n\r\n`experimental_run_tf_function` is an implementation detail, and that flag is mostly there as a debug during the transition. We don't plan to document it because it will be removed at some point in the future and the `True` behavior will be the only behavior.\r\n\r\nNow I expect that you may be surprised (or aghast) that it's going to be always on given the discussion in this thread. What `experimental_run_tf_function` does is funnel all calls to `fit`, `evaluate`, and `predict` through a central adapter which creates a Dataset and performs a variety of checks and input validation. This is generally desirable because it makes everything more robust, but there is some overhead to spinning up this machinery which is not amortized by small models with little data.\r\n\r\nCode to profile the step:\r\n```\r\nimport cProfile\r\nimport pstats\r\n\r\nprofiler = cProfile.Profile()\r\nprofiler.enable()\r\nfor _ in range(5):\r\n  model.predict(x)\r\nprofiler.disable()\r\n\r\nstats = pstats.Stats(profiler)\r\nstats.sort_stats(\"cumtime\").print_stats(20)\r\n``` \r\n\r\nIn this case most of the extra time is spent creating the dataset; there is machinery in there which makes sense, but it's surplus to requirements for the degenerate case of a single batch. (@jsimsa in case you want to look into the init time, but it's not obvious that it's unreasonable given the pipeline.) Really, this is not what `model.predict` is for. That endpoint is for predictions on lots of data where the batching and aggregation machinery in that endpoint is necessary. For single batch prediction there is `model.predict_on_batch`, which doesn't invoke all of that machinery and just directly calls into the model function. I tested it, and it is identical in v1 and v2. (And faster than even v1 `model.predict`)\r\n\r\nHowever this seems to be a common pitfall; I see lots of issues around single batch `model.predict`. (@martinwicke increment your counter...) From a documentation standpoint, I think the most valuable contribution would be to document `model.predict_on_batch` in the `model.predict` docstring, and probably also warn in `model.predict` when the batch cardinality is one. @ymodak @jvishnuvardhan  Can you remind me to bring this up at the next triage? And @OverLordGoldDragon if you want to take a crack at a PR that would be great; feel free to tag me and I'll try to provide some assistance.", "Oh, and @goldiegadde for tracking 2.2 performance, of course.", "@robieta Solid response, thanks - I'll look into the PR sometime. As for `predict`, so long as a faster alternative remains (i.e. `_on_batch`) and is noted in a docstring, it'd make an excellent resolution.", "Thanks for the clarification, and for the quick help! `model.predict_on_batch` speeds things up, but is still significantly slower on the v2 path. Here is the cumsum using your profile snippet (using tf2.0.0, calling predict 100 times instead of 5) for a small DQN model on a small batch of data:\r\n\r\n| `experimental_run_tf_function` | `predict()` | `predict_on_batch()` |\r\n| ------------------------------ | ----------- | -------------------- |\r\n| False / v1 | 0.209s | **0.078s** |\r\n| True / v2 (default) | 3.720s | 0.246s |\r\n\r\nSo, anyone with a single batch should switch to `predict_on_batch`, but an even faster option exists (v1 `predict_on_batch`), which is going to be deprecated if I understood correctly.\r\n\r\nIs this use case truly so exotic that we simply should not use the Keras API for it? I understand that we should of course look into batching, but for anyone who just writes quick prototypes it would be nice to have a fast light-weight way of evaluating things. Anyway, warning single-batch users of `predict()` will surely help most users, so that sounds great.\r\n\r\nAlso, here's a detail that may be important to people who migrate their code: When running in v2 mode, `predict_on_batch` will *not* return a numpy array (contrary to the docstring), but an `EagerTensor` instead. The caller may want to wrap the result -- as in `np.array(model.predict_on_batch(...))` -- to guarantee the same behavior for both v1 and v2. `predict` however returns a numpy array in both cases.  If you like, I can make a PR for the docstring.", "@robieta Actually I'll pass on making a PR; currently rather occupied - may try later if noone has done it", "@off99555 @OverLordGoldDragon  I tried to reproduce this with tf nightly but it doesn't seem to occur anymore. Or equivalently even without compile it becomes pretty slow as well.\r\nNow this is more of a tensorflow 1.15 much faster than tensorflow v2 -- can you confirm?", "@tanzhenyu Can confirm for nightly. The v1 path fallback has been removed in https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530#diff-de9b96ac2d81503324cbbbe21732031f , so everything now executes on v2, regardless of model.compile()\r\n\r\nIt's still possible to get the old speed back behavior by using `tf.compat.v1.disable_eager_execution()`. In any case, for most users moving to `predict_on_batch()` should be recommended, and also makes the slowdown less drastic (see numbers above) so the issue is less extreme.\r\n\r\nThat said, sorry for taking so long on the PR for warning single-batch predict() users.\r\n", "@ttbrunner Ah yeah that was the commit, thanks.\r\n\r\nOk so we follow the adapter pattern for convert numpy and dataframes to dataset first, and has a single path for execution. Apparently the speed down is mainly two things: 1) the construction of dataset. 2) creating tf.function for predict. (Check TensorLikeDataAdapter under /python/keras/engine/data_adapter.py if you're interested)\r\n\r\n@off99555 @OverLordGoldDragon @ttbrunner So here's what I would recommend going forward:\r\n1) you can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no \"conversion to dataset\" part, and also it's directly calling a cached tf.function. However be aware that if you have batch norm layers or any other layers that behaves differently between training and inference, make sure to call it with model(x, training=False)\r\n\r\n2) I will make a docstring to recommend users to use model call and explain predict is for large dataset.\r\n\r\nSG?", "@tanzhenyu I haven't benchmarked any of it, but if it is as you say - sounds like an excellent resolution indeed. Do let me know when it's done so I update my SO answer -- thanks for following through with this to the end.", "I have updated the doc, also tested the performance for model(x) in nightly. Closing it for now. Thanks all for reporting and collaborative work!", "@tanzhenyu Great. To clarify, is the speedup for TF 2.1+ only, or also 2.0? (if latter, is 2.1 even faster?)", "> @tanzhenyu Great. To clarify, is the speedup for TF 2.1+ only, or also 2.0? (if latter, is 2.1 even faster?)\r\n\r\nI believe this should be universal to 2.x versions", "Thanks for the docstring update, also for the explanation. I'm always interested!\r\n\r\nCan confirm that `model(x)` has the same runtime as `predict_on_batch(x)`, i.e. the v2 path is still slightly slower. It's OK for my use case though, so thanks again.\r\n\r\nAnother note for users: it's possible to specify `model.run_eagerly = False` before compiling. With this and the `model(x)` call, I am getting almost the same performance as in v1, without globally disabling eager execution.\r\n\r\nP.S.: Sorry for the many edits of this post.", "> Thanks for the docstring update, also for the explanation. I'm always interested!\r\n> \r\n> Can confirm that `model(x)` has the same runtime as `predict_on_batch(x)`, i.e. the v2 path is still slightly slower. It's OK for my use case though, so thanks again.\r\n> \r\n> Another note for users: it's possible to specify `model.run_eagerly = False` before compiling. With this and the `model(x)` call, I am getting almost the same performance as in v1, without globally disabling eager execution.\r\n> \r\n> P.S.: Sorry for the many edits of this post.\r\n\r\nYou can also compile(..., run_eagerly=False)", "@tanzhenyu Side-ish question: what does `run_eagerly=True` do in TF2, which runs in eager by default? From inspecting a fair chunk of the source code, it seems to change only a few execution paths for certain less-usual dataset types. Likewise, any difference with `=False` vs. disabling eager?", "> @ttbrunner Ah yeah that was the commit, thanks.\r\n> \r\n> Ok so we follow the adapter pattern for convert numpy and dataframes to dataset first, and has a single path for execution. Apparently the speed down is mainly two things: 1) the construction of dataset. 2) creating tf.function for predict. (Check TensorLikeDataAdapter under /python/keras/engine/data_adapter.py if you're interested)\r\n> \r\n> @off99555 @OverLordGoldDragon @ttbrunner So here's what I would recommend going forward:\r\n> \r\n> 1. you can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no \"conversion to dataset\" part, and also it's directly calling a cached tf.function. However be aware that if you have batch norm layers or any other layers that behaves differently between training and inference, make sure to call it with model(x, training=False)\r\n> 2. I will make a docstring to recommend users to use model call and explain predict is for large dataset.\r\n> \r\n> SG?\r\n\r\n@tanzhenyu @robieta Could you please clarify what the difference between model.predict_on_batch(x) and model(x) is?\r\nThe call stack and execution times (as captured by statistical profiler pyinstrument) are extremely different.\r\n\r\nHere are the details for `model.predict_on_batch(x)`:\r\n```\r\n0.017 <module>  code1.py:1\r\n\u2514\u2500 0.017 predict_on_batch  tensorflow_core/python/keras/engine/training.py:1220\r\n   \u2514\u2500 0.017 predict_on_batch  tensorflow_core/python/keras/engine/training_v2_utils.py:514\r\n      \u251c\u2500 0.005 _standardize_user_data  tensorflow_core/python/keras/engine/training.py:2247\r\n      \u2502  \u251c\u2500 0.000 run_eagerly  tensorflow_core/python/keras/engine/training.py:508\r\n      \u2502  \u2502  \u2514\u2500 0.000 wrapped  tensorflow_core/python/training/tracking/layer_utils.py:126\r\n      \u2502  \u2514\u2500 0.005 _standardize_tensors  tensorflow_core/python/keras/engine/training.py:2385\r\n      \u2502     \u251c\u2500 0.001 standardize_input_data  tensorflow_core/python/keras/engine/training_utils.py:460\r\n      \u2502     \u2502  \u2514\u2500 0.000 <listcomp>  tensorflow_core/python/keras/engine/training_utils.py:526\r\n      \u2502     \u2502     \u2514\u2500 0.000 standardize_single_array  tensorflow_core/python/keras/engine/training_utils.py:439\r\n      \u2502     \u251c\u2500 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471\r\n      \u2502     \u2502  \u2514\u2500 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\r\n      \u2502     \u2502     \u251c\u2500 0.000 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\r\n      \u2502     \u2502     \u2502  \u2514\u2500 0.000 _yield_value  tensorflow_core/python/util/nest.py:174\r\n      \u2502     \u2502     \u2502     \u2514\u2500 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179\r\n      \u2502     \u2502     \u2514\u2500 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119\r\n      \u2502     \u251c\u2500 0.002 map_structure  tensorflow_core/python/util/nest.py:507\r\n      \u2502     \u2502  \u251c\u2500 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568\r\n      \u2502     \u2502  \u2502  \u2514\u2500 0.001 _type_spec_from_value  tensorflow_core/python/keras/engine/training.py:2431\r\n      \u2502     \u2502  \u2502     \u2514\u2500 0.000 __init__  tensorflow_core/python/framework/tensor_spec.py:39\r\n      \u2502     \u2502  \u2514\u2500 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471\r\n      \u2502     \u2502     \u2514\u2500 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\r\n      \u2502     \u2502        \u251c\u2500 0.001 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\r\n      \u2502     \u2502        \u2502  \u2514\u2500 0.000 _yield_value  tensorflow_core/python/util/nest.py:174\r\n      \u2502     \u2502        \u2502     \u2514\u2500 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179\r\n      \u2502     \u2502        \u2514\u2500 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119\r\n      \u2502     \u2502           \u2514\u2500 0.000 [self]  \r\n      \u2502     \u2514\u2500 0.000 wrapped  tensorflow_core/python/training/tracking/layer_utils.py:126\r\n      \u251c\u2500 0.001 cast_to_model_input_dtypes  tensorflow_core/python/keras/engine/training_utils.py:1363\r\n      \u2502  \u2514\u2500 0.001 map_structure  tensorflow_core/python/util/nest.py:507\r\n      \u2502     \u251c\u2500 0.000 pack_sequence_as  tensorflow_core/python/util/nest.py:471\r\n      \u2502     \u2502  \u2514\u2500 0.000 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\r\n      \u2502     \u2502     \u2514\u2500 0.000 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\r\n      \u2502     \u251c\u2500 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568\r\n      \u2502     \u2502  \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\r\n      \u2502     \u2502     \u2514\u2500 0.001 cast  tensorflow_core/python/ops/math_ops.py:649\r\n      \u2502     \u2502        \u2514\u2500 0.000 cast  tensorflow_core/python/ops/gen_math_ops.py:1944\r\n      \u2502     \u2514\u2500 0.000 pack_sequence_as  tensorflow_core/python/util/nest.py:471\r\n      \u251c\u2500 0.000 _get_or_make_on_batch_function  tensorflow_core/python/keras/engine/training_v2_utils.py:103\r\n      \u2514\u2500 0.010 __call__  tensorflow_core/python/eager/def_function.py:551\r\n         \u2514\u2500 0.010 _call  tensorflow_core/python/eager/def_function.py:590\r\n            \u2514\u2500 0.010 __call__  tensorflow_core/python/eager/function.py:2359\r\n               \u251c\u2500 0.000 _maybe_define_function  tensorflow_core/python/eager/function.py:2637\r\n               \u2502  \u2514\u2500 0.000 _cache_key  tensorflow_core/python/eager/function.py:2496\r\n               \u2514\u2500 0.010 _filtered_call  tensorflow_core/python/eager/function.py:1593\r\n                  \u2514\u2500 0.009 _call_flat  tensorflow_core/python/eager/function.py:1613\r\n                     \u251c\u2500 0.008 call  tensorflow_core/python/eager/function.py:505\r\n                     \u2502  \u2514\u2500 0.008 quick_execute  tensorflow_core/python/eager/execute.py:33\r\n                     \u2502     \u2514\u2500 0.008 [self]  \r\n                     \u2514\u2500 0.001 _build_call_outputs  tensorflow_core/python/eager/function.py:1909\r\n                        \u2514\u2500 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471\r\n                           \u2514\u2500 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\r\n                              \u251c\u2500 0.001 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\r\n                              \u2502  \u2514\u2500 0.000 _yield_value  tensorflow_core/python/util/nest.py:174\r\n                              \u2502     \u2514\u2500 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179\r\n                              \u2514\u2500 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119\r\n```\r\n\r\nAnd here are the details for the much slower `model(x)`:\r\n```\r\n0.041 <module>  code2.py:1\r\n\u2514\u2500 0.041 __call__  tensorflow_core/python/keras/engine/base_layer.py:628\r\n   \u2514\u2500 0.041 call  tensorflow_core/python/keras/engine/network.py:693\r\n      \u2514\u2500 0.041 _run_internal_graph  tensorflow_core/python/keras/engine/network.py:791\r\n         \u2514\u2500 0.041 __call__  tensorflow_core/python/keras/layers/recurrent.py:637\r\n            \u2514\u2500 0.041 __call__  tensorflow_core/python/keras/engine/base_layer.py:628\r\n               \u251c\u2500 0.001 helper  contextlib.py:237\r\n               \u2502  \u2514\u2500 0.001 __init__  contextlib.py:81\r\n               \u251c\u2500 0.039 call  tensorflow_core/python/keras/layers/recurrent.py:2689\r\n               \u2502  \u2514\u2500 0.039 call  tensorflow_core/python/keras/layers/recurrent.py:699\r\n               \u2502     \u251c\u2500 0.001 _process_inputs  tensorflow_core/python/keras/layers/recurrent.py:804\r\n               \u2502     \u2502  \u2514\u2500 0.001 get_initial_state  tensorflow_core/python/keras/layers/recurrent.py:613\r\n               \u2502     \u2502     \u2514\u2500 0.001 get_initial_state  tensorflow_core/python/keras/layers/recurrent.py:2455\r\n               \u2502     \u2502        \u2514\u2500 0.001 _generate_zero_filled_state_for_cell  tensorflow_core/python/keras/layers/recurrent.py:2891\r\n               \u2502     \u2502           \u2514\u2500 0.001 _generate_zero_filled_state  tensorflow_core/python/keras/layers/recurrent.py:2898\r\n               \u2502     \u2502              \u2514\u2500 0.001 map_structure  tensorflow_core/python/util/nest.py:507\r\n               \u2502     \u2502                 \u2514\u2500 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568\r\n               \u2502     \u2502                    \u2514\u2500 0.001 create_zeros  tensorflow_core/python/keras/layers/recurrent.py:2905\r\n               \u2502     \u2502                       \u2514\u2500 0.001 zeros  tensorflow_core/python/ops/array_ops.py:2399\r\n               \u2502     \u2502                          \u2514\u2500 0.001 fill  tensorflow_core/python/ops/array_ops.py:198\r\n               \u2502     \u2502                             \u2514\u2500 0.001 fill  tensorflow_core/python/ops/gen_array_ops.py:3193\r\n               \u2502     \u2514\u2500 0.038 rnn  tensorflow_core/python/keras/backend.py:3808\r\n               \u2502        \u251c\u2500 0.001 <listcomp>  tensorflow_core/python/keras/backend.py:4023\r\n               \u2502        \u2502  \u2514\u2500 0.001 _slice_helper  tensorflow_core/python/ops/array_ops.py:759\r\n               \u2502        \u2502     \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\r\n               \u2502        \u251c\u2500 0.001 [self]  \r\n               \u2502        \u2514\u2500 0.036 while_loop  tensorflow_core/python/ops/control_flow_ops.py:2482\r\n               \u2502           \u251c\u2500 0.018 _step  tensorflow_core/python/keras/backend.py:4096\r\n               \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502           \u2502  \u251c\u2500 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.003 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502           \u2502  \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396\r\n               \u2502           \u2502  \u2502     \u251c\u2500 0.001 bias_add  tensorflow_core/python/keras/backend.py:5508\r\n               \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 bias_add  tensorflow_core/python/ops/nn_ops.py:2699\r\n               \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\r\n               \u2502           \u2502  \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4132\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.001 wrapped  tensorflow_core/python/util/tf_should_use.py:234\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 write  tensorflow_core/python/ops/tensor_array_ops.py:1139\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 write  tensorflow_core/python/ops/tensor_array_ops.py:829\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.001 _write  tensorflow_core/python/ops/tensor_array_ops.py:779\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 numpy  tensorflow_core/python/framework/ops.py:918\r\n               \u2502           \u2502  \u2502                 \u2514\u2500 0.001 _numpy  tensorflow_core/python/framework/ops.py:905\r\n               \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 select_v2  tensorflow_core/python/ops/gen_math_ops.py:8662\r\n               \u2502           \u2502  \u251c\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502           \u2502  \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189\r\n               \u2502           \u2502  \u2502     \u2502        \u2514\u2500 0.001 __eq__  tensorflow_core/python/framework/dtypes.py:261\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\r\n               \u2502           \u2502  \u251c\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396\r\n               \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\r\n               \u2502           \u2502  \u2502                 \u2514\u2500 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263\r\n               \u2502           \u2502  \u2502                    \u2514\u2500 0.001 _autopacking_conversion_function  tensorflow_core/python/ops/array_ops.py:1355\r\n               \u2502           \u2502  \u2502                       \u2514\u2500 0.001 _should_not_autopack  tensorflow_core/python/ops/array_ops.py:1345\r\n               \u2502           \u2502  \u2502                          \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/ops/array_ops.py:1351\r\n               \u2502           \u2502  \u251c\u2500 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502           \u2502  \u2502  \u251c\u2500 0.001 [self]  \r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502           \u2502  \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 sigmoid  tensorflow_core/python/keras/activations.py:246\r\n               \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/math_ops.py:3134\r\n               \u2502           \u2502  \u2502     \u2502        \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/gen_math_ops.py:8720\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\r\n               \u2502           \u2502  \u251c\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 add_v2  tensorflow_core/python/ops/gen_math_ops.py:451\r\n               \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\r\n               \u2502           \u2502  \u2502                 \u2514\u2500 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263\r\n               \u2502           \u2502  \u2514\u2500 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502           \u2502     \u251c\u2500 0.001 [self]  \r\n               \u2502           \u2502     \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502           \u2502        \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502           \u2502        \u2502  \u2514\u2500 0.001 sigmoid  tensorflow_core/python/keras/activations.py:246\r\n               \u2502           \u2502        \u2502     \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/math_ops.py:3134\r\n               \u2502           \u2502        \u2502        \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/gen_math_ops.py:8720\r\n               \u2502           \u2502        \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\r\n               \u2502           \u2502           \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\r\n               \u2502           \u2502              \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\r\n               \u2502           \u2502                 \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\r\n               \u2502           \u251c\u2500 0.001 [self]  \r\n               \u2502           \u251c\u2500 0.004 _step  tensorflow_core/python/keras/backend.py:4096\r\n               \u2502           \u2502  \u251c\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502           \u2502  \u2502  \u2514\u2500 0.002 <genexpr>  tensorflow_core/python/keras/backend.py:4067\r\n               \u2502           \u2502  \u2502     \u2514\u2500 0.002 _expand_mask  tensorflow_core/python/keras/backend.py:3913\r\n               \u2502           \u2502  \u2502        \u2514\u2500 0.002 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\r\n               \u2502           \u2502  \u2502           \u2514\u2500 0.002 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\r\n               \u2502           \u2502  \u2502              \u251c\u2500 0.001 quick_execute  tensorflow_core/python/eager/execute.py:33\r\n               \u2502           \u2502  \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\r\n               \u2502           \u2502  \u2502                 \u2514\u2500 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263\r\n               \u2502           \u2502  \u2502                    \u2514\u2500 0.001 get  tensorflow_core/python/framework/tensor_conversion_registry.py:114\r\n               \u2502           \u2502  \u2514\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502           \u2502     \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502           \u2502        \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502           \u2502        \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502           \u2502        \u2502     \u2514\u2500 0.001 _mul_dispatch  tensorflow_core/python/ops/math_ops.py:1197\r\n               \u2502           \u2502        \u2502        \u2514\u2500 0.001 mul  tensorflow_core/python/ops/gen_math_ops.py:6093\r\n               \u2502           \u2502        \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502           \u251c\u2500 0.001 assert_same_structure  tensorflow_core/python/util/nest.py:293\r\n               \u2502           \u2514\u2500 0.012 _step  tensorflow_core/python/keras/backend.py:4096\r\n               \u2502              \u251c\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502              \u2502  \u2514\u2500 0.002 <genexpr>  tensorflow_core/python/keras/backend.py:4067\r\n               \u2502              \u2502     \u2514\u2500 0.002 _expand_mask  tensorflow_core/python/keras/backend.py:3913\r\n               \u2502              \u2502        \u2514\u2500 0.002 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\r\n               \u2502              \u2502           \u2514\u2500 0.002 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\r\n               \u2502              \u251c\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502              \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502              \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502              \u2502     \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502              \u2502     \u2502     \u2514\u2500 0.001 _mul_dispatch  tensorflow_core/python/ops/math_ops.py:1197\r\n               \u2502              \u2502     \u2502        \u2514\u2500 0.001 mul  tensorflow_core/python/ops/gen_math_ops.py:6093\r\n               \u2502              \u2502     \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\r\n               \u2502              \u2502        \u2514\u2500 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189\r\n               \u2502              \u2502           \u2514\u2500 0.001 add_v2  tensorflow_core/python/ops/gen_math_ops.py:451\r\n               \u2502              \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4108\r\n               \u2502              \u2502  \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:1127\r\n               \u2502              \u2502     \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:746\r\n               \u2502              \u251c\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502              \u2502  \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070\r\n               \u2502              \u2502  \u2502  \u2514\u2500 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889\r\n               \u2502              \u2502  \u2502     \u2514\u2500 0.001 select_v2  tensorflow_core/python/ops/gen_math_ops.py:8662\r\n               \u2502              \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\r\n               \u2502              \u2502     \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\r\n               \u2502              \u2502        \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\r\n               \u2502              \u2502           \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\r\n               \u2502              \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\r\n               \u2502              \u2502                 \u2514\u2500 0.001 <listcomp>  tensorflow_core/python/eager/execute.py:271\r\n               \u2502              \u251c\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\r\n               \u2502              \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\r\n               \u2502              \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\r\n               \u2502              \u2502     \u2502  \u2514\u2500 0.001 tanh  tensorflow_core/python/keras/activations.py:224\r\n               \u2502              \u2502     \u2502     \u2514\u2500 0.001 tanh  tensorflow_core/python/ops/gen_math_ops.py:10284\r\n               \u2502              \u2502     \u2514\u2500 0.001 bias_add  tensorflow_core/python/keras/backend.py:5508\r\n               \u2502              \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4108\r\n               \u2502              \u2502  \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:1127\r\n               \u2502              \u2502     \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:746\r\n               \u2502              \u2514\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\r\n               \u2502                 \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070\r\n               \u2502                 \u2502  \u2514\u2500 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889\r\n               \u2502                 \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\r\n               \u2502                    \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\r\n               \u2502                       \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\r\n               \u2502                          \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\r\n               \u2514\u2500 0.001 _set_mask_metadata  tensorflow_core/python/keras/engine/base_layer.py:1918\r\n                  \u2514\u2500 0.001 flatten  tensorflow_core/python/util/nest.py:242\r\n```\r\n\r\nOut of the two suggestions you guys provided in this thread, the one that ended up being put in the docstring is the slowest one (even slower than `model.predict(x)`) for my simple LSTM encoder being run on Tensorflow 2.1.0 on with GPU support, and other users like me might be confused.\r\n\r\nAlso, can you please clarify why calling `model(x)` runs the model step by step even though `run_eagerly` is set to `False` and, as per tanzhenyu's comment, it should instead call a cached tf.function? \r\n\r\nThanks!"]}, {"number": 33339, "title": "embedding_column converts from variable-length input feature does not work with Distributed Keras MultiWorkerMirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v2.0.0-rc2-26-g64c3d382ca', '2.0.0')\r\n- Python version: 2.7.13\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\n\r\n`embedding_column` converts from `variable-length input feature` does not work with Distributed Keras `MultiWorkerMirroredStrategy`. \r\n**IF:**\r\n\u2460 **With local training(non-distributed), everything is fine . \u2714\ufe0f**\r\n\u2461 **Convert variable-length input feature to indicator_column everything is ok. \u2714\ufe0f**\r\n\u2462 **embedding_column converts from fixed-length input feature works pretty good. \u2714\ufe0f**\r\n\r\nMaybe this problem is caused by SparseTensor ?\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"localhost:12345\", \"localhost:23456\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\n\r\n# generate fake data\r\ndef serialize_example(value):\r\n    feature = {\r\n      'color': tf.train.Feature(bytes_list=tf.train.BytesList(value=value)),\r\n    }\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    return example_proto.SerializeToString()\r\n\r\ntfrecord_writer = tf.io.TFRecordWriter('./color.tfrecord')\r\nfor each in [['G', 'R'], ['B'], ['B', 'G'], ['R']]:\r\n    tfrecord_writer.write(serialize_example(each))\r\ntfrecord_writer.close()\r\n\r\n# build feature column\r\ncolor_column = tf.feature_column.categorical_column_with_vocabulary_list('color', ['R', 'G', 'B'], dtype=tf.string)\r\ncolor_embeding = tf.feature_column.embedding_column(color_column, 4) # tf.feature_column.indicator_column(color_column)\r\n\r\ninputs = {}\r\ninputs['color'] = tf.keras.layers.Input(name='color', shape=(None, ), sparse=True, dtype='string')\r\n\r\n# build model\r\nwith tf.distribute.experimental.MultiWorkerMirroredStrategy().scope():\r\n    dense = tf.keras.layers.DenseFeatures([color_embeding])(inputs)\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\r\n    model = tf.keras.Model(inputs, output)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n# build dataset\r\ndef parse(example_proto):\r\n    feature_description = {\r\n        'color': tf.io.VarLenFeature(tf.string)\r\n    }\r\n    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\r\n    return parsed_features, True\r\n    \r\ndataset = tf.data.TFRecordDataset('./color.tfrecord').map(parse).repeat().batch(1)\r\n\r\nmodel.fit(dataset, epochs=3, steps_per_epoch=1)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2019-10-14 22:35:48.491329: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-14 22:35:48.504154: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd065e01d50 executing computations on platform Host. Devices:\r\n2019-10-14 22:35:48.504171: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-14 22:35:48.506239: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\r\n2019-10-14 22:35:48.506975: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:23456\r\nINFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:XLA_CPU:0']\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = u'worker', task_id = 1, environment = None, rpc_layer = 'grpc'\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = u'worker', task_id = 1, num_workers = 2, local_devices = (u'/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {u'worker': [u'localhost:12345', u'localhost:23456']}, task_type = u'worker', task_id = 1, num_workers = 2, local_devices = (u'/job:worker/task:1',), communication = CollectiveCommunication.AUTO\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n2019-10-14 22:36:00.226012: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\nTrain for 1 steps\r\nEpoch 1/3\r\nINFO:tensorflow:Collective batch_all_reduce: 2 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce for IndexedSlices: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 2 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce for IndexedSlices: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2019-10-14 22:36:01.352450: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.352478: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.352638: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.352657: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353012: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353029: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353213: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353225: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353325: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353338: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352818000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-14 22:36:01.353352: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352818000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-14 22:36:01.353370: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353359: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n\t [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce]]\r\n2019-10-14 22:36:01.353469: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352917000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-14 22:36:01.353451: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352818000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-14 22:36:01.353491: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352917000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-14 22:36:01.353512: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n2019-10-14 22:36:01.353630: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352917000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2019-10-14 22:36:01.353698: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[{{node Adam/allreduce_1/CollectiveGather_1}}]]\",\"grpc_status\":13}\r\n1/1 [==============================] - 1s 1s/step\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-1-b83aa9a3e019> in <module>()\r\n     43\r\n     44 dataset = tf.data.TFRecordDataset('./color.tfrecord').map(parse).repeat().batch(1)\r\n---> 45 model.fit(dataset, epochs=3, steps_per_epoch=1)\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729\r\n    730   def evaluate(self,\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.pyc in fit(self, *args, **kwargs)\r\n    787   def fit(self, *args, **kwargs):\r\n    788     return train_with_multi_worker(self._single_worker_loop.fit)(\r\n--> 789         *args, **kwargs)\r\n    790\r\n    791   def evaluate(self, *args, **kwargs):\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.pyc in wrapper(model, **kwargs)\r\n    774         _worker_fn,\r\n    775         model._distribution_strategy,\r\n--> 776         mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n    777\r\n    778   return wrapper\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.pyc in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)\r\n    851         # All jobs run `worker_fn` if between-graph.\r\n    852         return _run_single_worker(worker_fn, strategy, cluster_spec, task_type,\r\n--> 853                                   task_id, session_config, rpc_layer)\r\n    854       else:\r\n    855         # Only one node runs `worker_fn` if in-graph.\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.pyc in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier, coord)\r\n    358         return worker_fn(strategy)\r\n    359     else:\r\n--> 360       return worker_fn(strategy)\r\n    361\r\n    362\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.pyc in _worker_fn(_)\r\n    769       filtered_callbacks = dist_utils.filter_distributed_callbacks(callbacks)\r\n    770       kwargs['callbacks'] = filtered_callbacks\r\n--> 771       return method(model, **kwargs)\r\n    772\r\n    773     return dc.run_distribute_coordinator(\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.pyc in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.pyc in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.pyc in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87\r\n     88   return execution_function\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.pyc in __call__(self, *args, **kwds)\r\n    455\r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.pyc in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824\r\n   1825   @property\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142\r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/function.pyc in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/eager/execute.pyc in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/Users/felix/Envs/tf2/lib/python2.7/site-packages/six.pyc in raise_from(value, from_value)\r\n    735 else:\r\n    736     def raise_from(value, from_value):\r\n--> 737         raise value\r\n    738\r\n    739\r\n\r\nInternalError:  Inconsistent output shapes, got [4], but expected is [2].\r\n\t [[node Adam/allreduce_1/CollectiveGather_1 (defined at /Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1571063761.352390000\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Inconsistent output shapes, got [4], but expected is [2].\\n\\t [[node Adam/allreduce_1/CollectiveGather_1 (defined at /Users/felix/Envs/tf2/lib/python2.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\",\"grpc_status\":13}\r\n\t [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce]] [Op:__inference_distributed_function_1149]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n\r\nThis problem has been bothering me for a week, any help will be appreciated", "comments": ["@yuefengz ", "I had the same error, have you solved ?.", "@cdj0311 \r\nNo, I have not found a solution yet. I am still waiting for a response.  I can only use the old distributed strategy (Estimator + ParameterServerStrategy) at present .", "The issue should have been fixed in tf-nightly. In your original example you will need to disable dataset autosharding (see https://github.com/tensorflow/tensorflow/issues/35878) as the number of files is less than the number of workers.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33339\">No</a>\n", "> The issue should have been fixed in tf-nightly. In your original example you will need to disable dataset autosharding (see #35878) as the number of files is less than the number of workers.\r\n\r\n@ckkuang  Thanks for the reply, I have tried using TF2.1 and turned off auto shard, but the problem still exists.\r\n\r\ncode based on **TF 2.1.0**\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"localhost:12345\", \"localhost:23456\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\n\r\n# generate dummy dataset\r\ndef serialize_example(value):\r\n    feature = {\r\n      'color': tf.train.Feature(bytes_list=tf.train.BytesList(value=value)),\r\n    }\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    return example_proto.SerializeToString()\r\n\r\ntfrecord_writer = tf.io.TFRecordWriter('./color.tfrecord')\r\nfor each in [['G', 'R'], ['B'], ['B', 'G'], ['R']]:\r\n    tfrecord_writer.write(serialize_example(each))\r\ntfrecord_writer.close()\r\n\r\n# build feature column\r\ncolor_column = tf.feature_column.categorical_column_with_vocabulary_list('color', ['R', 'G', 'B'], dtype=tf.string)\r\ncolor_embeding = tf.feature_column.embedding_column(color_column, 4) # tf.feature_column.indicator_column(color_column)\r\n\r\ninputs = {}\r\ninputs['color'] = tf.keras.layers.Input(name='color', shape=(None, ), sparse=True, dtype='string')\r\n\r\n# build model\r\nwith tf.distribute.experimental.MultiWorkerMirroredStrategy().scope():\r\n    dense = tf.keras.layers.DenseFeatures([color_embeding])(inputs)\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\r\n    model = tf.keras.Model(inputs, output)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n# build dataset\r\ndef parse(example_proto):\r\n    feature_description = {\r\n        'color': tf.io.VarLenFeature(tf.string)\r\n    }\r\n    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\r\n    return parsed_features, True\r\n    \r\ndataset = tf.data.TFRecordDataset('./color.tfrecord').map(parse).batch(1).repeat()\r\n\r\n######### \u2193 \u2193 \u2193 change \u2193 \u2193 \u2193 #######\r\noptions = tf.data.Options()  \r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF  # AutoShardPolicy.DATA  also not works\r\ndataset = dataset.with_options(options) \r\n\r\n\r\nmodel.fit(dataset, epochs=3, steps_per_epoch=1)\r\n```\r\n\r\nERROR LOG:\r\n```\r\n2020-03-06 18:40:44.677135: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-06 18:40:44.689828: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc0109539b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-06 18:40:44.689848: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-03-06 18:40:44.692892: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\r\n2020-03-06 18:40:44.693182: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nTrain for 1 steps\r\nEpoch 1/3\r\n2020-03-06 18:40:46.211341: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: Serialized sparse should have 3 as the last dimension [2]\r\n\t [[{{node DeserializeSparse}}]]\r\n1/1 [==============================] - 1s 652ms/step\r\nTraceback (most recent call last):\r\n  File \"t0.py\", line 54, in <module>\r\n    model.fit(dataset, epochs=3, steps_per_epoch=1)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"/Users/felix/Envs/tf2.1/lib/python2.7/site-packages/six.py\", line 738, in raise_from\r\n    raise value\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Serialized sparse should have 3 as the last dimension [2]\r\n\t [[node DeserializeSparse (defined at t0.py:54) ]] [Op:__inference_distributed_function_1125]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n2020-03-06 18:40:46.277340: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n```", "The fix is currently in tf-nightly, and will be included in future tf 2.2. You may try it using tf-nightly."]}, {"number": 33338, "title": "//tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.py fails with Python2.7", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.0.0\r\n- Python version: Python 2.7.15+\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 8.3.0-6ubuntu1) 8.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nLooks like //tensorflow/python/autograph/pyct/static_analysis:activity_py3_test is newly added in TF 2.0. Following command throws this error \r\n```\r\nbazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test --define=tensorflow_mkldnn_contraction_kernel=0 --host_javabase=\"@local_jdk//:jdk\" //tensorflow/python/autograph/pyct/static_analysis:activity_py3_test\r\n```\r\nError:\r\n```\r\nFile \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/static_analysis/activity_py3_test.py\", line 39\r\n    nonlocal nonlocal_a\r\n                      ^\r\nSyntaxError: invalid syntax\r\n```\r\nI found [here](https://stackoverflow.com/questions/14264313/syntax-error-on-nonlocal-statement-in-python/14264325#14264325) that this issue gets resolved by upgrading Python2 to Python3. Tried the same and test passes. \r\n\r\nIs Python3.x recommended for building Tensorflow 2.0? I didn't see such update in Release notes for Tensorflow 2.x. Also as per [this update on Python](https://pip.pypa.io/en/latest/development/release-process/#python-2-support) Python2.7 will be deprecated in few months, are we planning to completely shift to Python 3.x?\r\n\r\nPlease guide us on this. Thanks.", "comments": ["Adding below tests which show same behavior:\r\n\r\n//tensorflow/python/autograph/impl:api_py3_test\r\n//tensorflow/python/autograph/operators:py_builtins_py3_test\r\n//tensorflow/python/autograph/pyct/static_analysis:liveness_py3_test\r\n//tensorflow/python/autograph/pyct/static_analysis:reaching_definitions_py3_test", "You are trying to run python3 tests using python2 :-/\r\n\r\nWe are working on completely shifting to Python3.x", "Thanks @mihaimaruseac."]}, {"number": 33337, "title": "Dimension reasoning", "body": "**System information**\r\n- TensorFlow version (you are using): Tensorflow2.0\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n**Describe the feature and the current behavior/state.**\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nNo\r\n**Any Other info.**\r\n\r\n\r\n`    \r\n```\r\ndef roi_align_one_layer(feat, bbox, s, align_c):\r\n\r\n            s_y, s_x = s\r\n            num_bbox = tf.shape(bbox)[0]\r\n            feat_shape = tf.shape(feat)\r\n```\r\n# tensorflow know c_i here, for example, [None, None, 256]\r\n            h_i, w_i, c_i = feat_shape[0], feat_shape[1], feat_shape[2]\r\n\r\n            h_f, w_f = tf.cast(h_i, tf.float32), tf.cast(w_i, tf.float32)\r\n            bbox_y1 = bbox[:, 0:1]\r\n            bbox_x1 = bbox[:, 1:2]\r\n            bbox_y2 = bbox[:, 2:3]\r\n            bbox_x2 = bbox[:, 3:4]\r\n            bbox_h = bbox_y2 - bbox_y1\r\n            bbox_w = bbox_x2 - bbox_x1\r\n            if align_c:\r\n                off_y = 0. if s_y > 1 else bbox_h / 2.\r\n                off_x = 0. if s_x > 1 else bbox_w / 2.\r\n                grid_y = tf.linspace(0.0, 1.0, s_y)\r\n                grid_x = tf.linspace(0.0, 1.0, s_x)\r\n            else:\r\n                off_y = bbox_h / (2. * s_y)\r\n                off_x = bbox_w / (2. * s_x)\r\n                grid_y = tf.linspace(0.0, 1.0, s_y + 1)[:-1]\r\n                grid_x = tf.linspace(0.0, 1.0, s_x + 1)[:-1]\r\n            grid_y = tf.expand_dims(tf.matmul(bbox_h, tf.expand_dims(grid_y, axis=0)) + off_y + bbox_y1, axis=-1)\r\n            grid_x = tf.expand_dims(tf.matmul(bbox_w, tf.expand_dims(grid_x, axis=0)) + off_x + bbox_x1, axis=-1)\r\n\r\n            grid_y = tf.where(grid_y < 0., 0., grid_y)\r\n            grid_x = tf.where(grid_x < 0., 0., grid_x)\r\n            grid_y = tf.where(grid_y > h_f-1., h_f-1., grid_y)\r\n            grid_x = tf.where(grid_x > w_f-1., w_f-1., grid_x)\r\n            grid_y = tf.tile(grid_y, [1, 1, s_y])\r\n            grid_x = tf.tile(grid_x, [1, 1, s_x])\r\n            grid_y = tf.reshape(grid_y, [num_bbox, -1])\r\n            grid_x = tf.reshape(grid_x, [num_bbox, -1])\r\n            grid_y1 = tf.math.floor(grid_y)\r\n            grid_y2 = tf.math.floor(grid_y+1.)\r\n            grid_x1 = tf.math.floor(grid_x)\r\n            grid_x2 = tf.math.floor(grid_x+1.)\r\n            wey1 = tf.expand_dims(grid_y - grid_y1, axis=-1)\r\n            wey2 = tf.expand_dims(grid_y2 - grid_y, axis=-1)\r\n            wex1 = tf.expand_dims(grid_x - grid_x1, axis=-1)\r\n            wex2 = tf.expand_dims(grid_x2 - grid_x, axis=-1)\r\n            grid_y2 = tf.where(grid_y2 > h_f - 1., h_f - 1., grid_y2)\r\n            grid_x2 = tf.where(grid_x2 > w_f - 1., w_f - 1., grid_x2)\r\n            grid_y1 = tf.cast(grid_y1, tf.int32)\r\n            grid_y2 = tf.cast(grid_y2, tf.int32)\r\n            grid_x1 = tf.cast(grid_x1, tf.int32)\r\n            grid_x2 = tf.cast(grid_x2, tf.int32)\r\n            grid_11 = grid_y1 * w_i + grid_x1\r\n            grid_12 = grid_y1 * w_i + grid_x2\r\n            grid_21 = grid_y2 * w_i + grid_x1\r\n            grid_22 = grid_y2 * w_i + grid_x2\r\n            feat = tf.reshape(feat, [h_i*w_i, c_i])\r\n            feat_11 = tf.gather(feat, grid_11)\r\n            feat_12 = tf.gather(feat, grid_12)\r\n            feat_21 = tf.gather(feat, grid_21)\r\n            feat_22 = tf.gather(feat, grid_22)\r\n            feat_bilinear = wey2 * (feat_11 * wex2 + feat_12 * wex1) + wey1 * (feat_21 * wex2 + feat_22 * wex1)\r\n\r\n# tensorflow don't know c_i here,   [None, 14, 14, None]\r\n            feat_bilinear = tf.reshape(feat_bilinear, [num_bbox, s_y, s_x, c_i])\r\n\r\n            return feat_bilinear\r\n`\r\n\r\nthe data_format is the 'channels_last'\r\n\r\nwhen enable eager or disable_enger\r\n\r\nwhen I use `y = roi_align_one_layer(feat, bbox, s, align_c)` the tensorflow2.0 know the number of channels of `feat` but don't know the number of channels of `y`,\r\nwhen I use `tf.keras.layers.Conv()(y)`, tensorflow2.0 raise error that tensorflow2.0 don't know the number of input channels\r\n\r\nhow to let tensorflow  know it automatically\r\n\r\n \r\n", "comments": ["@Stick-To ,\r\nThanks for reporting the issue, Can you share standalone code to reproduce the issue?"]}, {"number": 33335, "title": "tf.contrib.image.transform in TF2.0", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn TF 1.X there were nice methods [tf.contrib.image.transform()](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/contrib/image/transform) and [tf.contrib.image.compose_transforms()](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/contrib/image/compose_transforms) that allowed to perform affine transforms with an image. \r\n\r\nIn TF 2.0 whole tf.contrib.* is dropped.\r\n\r\nFor instance, I was using those in my custom augmentation mechanisms to add rotation and shear transforms (along with all zooms, shifts, flips, etc). And in TF 2.0 I see no way to perform those.\r\n\r\nI tried to implement such a functionality with numpy+opencv instead of TF. However when I tried injecting such a functionality to tf.dataset.map() pipeline tensors seemed not to have .nupmy() method yet.\r\n\r\nI also found tf.keras.preprocessing.image.apply_affine_transform() which does perform affine transform on image, but according to [docs](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/preprocessing/image/apply_affine_transform) it works with numpy array only.\r\n\r\n**Will this change the current api? How?**\r\nI guess that would be nice to port those functions form 1.14 to 2.X to: tf.image.compose_transforms() and tf.image.transform() or something. Should not affect existing API I guess.\r\n\r\n**Who will benefit with this feature?**\r\nAnybody who likes to perform affine transforms on image say in data augmentation step of model training.\r\n\r\n**Any Other info.**\r\nIf I miss something and mentioned functionality can be implemented with existing API, please let me know.", "comments": ["`tf.contrib.image.transform` and `tf.contrib.image.compose_transform` are now part of the [`tensorflow_addons.image`](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/image/README.md) module which should implement the functionality you are looking for.", "@chayka As mentioned in the previous comment, you can refer to tensorflow_addons to find the same functions. Closing this issue as it has been resolved.", "Thank you guys a lot! Could be a nice idea to leave a note in tf.contrib.* branch docs that some functionality is not dropped, but could be found somewhere else. Spent quite some time looking for it and still missed it. But anyway, thanks for TF2.0, great job!"]}, {"number": 33334, "title": "replaced builtin `all` by `np.all`", "body": "For reasons that I cannot understand, the builtin function `all` is somehow overridden by `K.all`.\r\n\r\nTo make things even more complicated, it depends on the details of the computation graph. It doesn't happen for all configurations. I tried to create a minimal test case so that others might be able to reproduce it, but I can't... so here are my suggestions to reproduce (in version 1.14 and up):\r\n```\r\n$ git clone git@github.com:KristianHolsheimer/keras-gym.git\r\n$ cd keras-gym/script/atari/\r\n$ python3 ppo.py\r\n```\r\nThis will throw the following error at the line that I edited in the commit:\r\n> TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n\r\nTo have a closer look at what the python interpreter sees, I added some lines just above the changes:\r\n```python\r\ntry:\r\n  bool(all(input_shape[1:]))\r\nexcept:\r\n  import inspect\r\n  print(inspect.getsource(all)\r\n  raise\r\n```\r\nPrints the following code snippet:\r\n```python\r\n@keras_export('keras.backend.all')\r\ndef all(x, axis=None, keepdims=False):\r\n  \"\"\"Bitwise reduction (logical AND).\r\n\r\n  Arguments:\r\n      x: Tensor or variable.\r\n      axis: axis along which to perform the reduction.\r\n      keepdims: whether the drop or broadcast the reduction axes.\r\n\r\n  Returns:\r\n      A uint8 tensor (0s and 1s).\r\n  \"\"\"\r\n  x = math_ops.cast(x, dtypes_module.bool)\r\n  return math_ops.reduce_all(x, axis, keepdims)\r\n```\r\n\r\nVery strange indeed. \r\n\r\nI feel like I've already lost a lot of time on this, so to solve my problem I propose to use the numpy version. It's a only a tiny change.\r\n\r\nIf you'd like to dig deeper into this very strange behavior, let me know.\r\n\r\nEither way, I hope we can get this resolved quickly.\r\n\r\nThanks,\r\nKris", "comments": ["I forgot to mention something important.. this is happening in `tf.keras.models.clone_model()`.\r\n\r\nPerhaps we're deserializing `K.all()` in the global namespace.", "@qlzh727 Would you be able to have a look at this? The changeset is tiny.\r\n\r\nIf you'd like to investigate the unexpected behavior of `tf.keras.models.clone_model()` I can open a new issue for that specifically.", "Sorry for the very late reply.\r\n\r\nI think we will need to understand the root cause, rather than workaround the issue like this. If this is related to save model, we could get more weird issue if the root cause is not fixed.", "Adding Kathy since the issue is somehow related to save model.", "@KristianHolsheimer, on a side note, it will be nice to have a bug tracking this, rather than in a PR.", "Okay, in that case I'll see if I can create a minimal example to reproduce this and file an issue with that.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@KristianHolsheimer Any update on this PR, please.", "Sorry, I haven't had time to create a minimal example to reproduce. The steps to reproduce that I mentioned in my original post should still work.\r\n\r\nI did write a little function that patches tensorflow with the minor change in this PR, because I wanted to move on.\r\n\r\nSo to reproduce, you can simply comment out the call to the patch. The steps are:\r\n```\r\n$ git clone git@github.com:KristianHolsheimer/keras-gym.git\r\n$ python3 -m pip install -e ./keras-gym\r\n```\r\nComment out [line 13 of `keras_gym/base/patches.py`](https://github.com/KristianHolsheimer/keras-gym/blob/master/keras_gym/base/patches.py#L13), the one that calls `_monkey_patch_tensorflow()`.\r\n\r\nAfter that, you can run e.g. the atari/ppo script (which is how I stumbled upon this):\r\n```\r\n$ cd keras-gym/script/atari/\r\n$ python3 ppo.py\r\n```\r\nThis should throw the error mentioned in my initial message.", "Thanks!"]}, {"number": 33333, "title": "Potential error in Codelab : Learning Tensorflow 2 : Computer Vision ", "body": "So I was following this codelab : https://codelabs.developers.google.com/codelabs/tensorflow-lab2-computervision/index.html?index=..%2F..index#4\r\n\r\nOn slide 5 the optimizer is set to be `tf.train.AdamOptimizer()`\r\nand it returns an error of\r\n `module 'tensorflow_core._api.v2.train' has no attribute 'AdamOptimizer'`\r\nI guess it should be \r\n`tf.optimizers.Adam()`\r\n?", "comments": ["Just found another error on the same codelab\r\n`if(logs.get('acc')>0.95):`\r\nin the 8th slide, last chunk of code, in the myCallBack definition. \r\nputting logs.get('acc') on tensorflow 2.0.0 returns an error\r\n`TypeError: '>' not supported between instances of 'NoneType' and 'float'`\r\nchanging it to logs.get('accuracy') works. ", "@Satwato \r\n\r\nHow did you  navigate to this codelab page? Is this the page you are referring,  is from tensorflow official website?.Thanks!", "@ravikyram I got to this codelab by searching for Tensorflow on this website : https://codelabs.developers.google.com/", "@Satwato Please check [this tutorial](https://www.tensorflow.org/tutorials/keras/classification) on TF website which is most updated and very similar to the codelabs tutorial. If you notice any issues the tutorial on TF website, please file bug in TF repository. Regarding the callbacks, you can replace `acc` with `accuracy` and it should work. But for more detailed guide on custom callbacks, please check [here](https://www.tensorflow.org/guide/keras/custom_callback).\r\n\r\nIf you are looking for more tutorials and guides on `tensorflow`, [here](https://www.tensorflow.org/tutorials) is the page for tutorials and [here](https://www.tensorflow.org/guide) is the page for guides. These links have everything to become an expert in TF. Thanks!\r\n\r\nI am closing this as it is resolved. Please feel free to open a new issue when you notice any errors in tutorials on TF website. Thanks! "]}, {"number": 33332, "title": "How can I change variable on the layer ( fine tuning )", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.13.1\r\n- **Python version**:3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0/7.4\r\n- **GPU model and memory**:Nvidia Geforce 840m\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI have a pre-trained model that detects the whole face with 68 key points, I would like to use this pre-trained model to train my dataset for detecting only the eye region ( 40 points ).\r\nSo I need to change the value of units on the logits layer from 136(68 * 2) to 80(40 * 2).\r\nI see an answer on Stackoverflow that I should use the fine-tuning, how can I use this technique to change some variables on the frozen model (.pb) ?\r\n\r\n![mdlrp](https://user-images.githubusercontent.com/19480228/66752207-2f22f700-ee91-11e9-802c-3a40c540078d.PNG)\r\n\r\nHope I get an answer.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "I have posted this question on Stackoverflow and I have tagged #Tensorflow on the question but I didn't get any answer.", "@abdou31 As you mentioned you have a dataset for training, you could load the pre-trained model,  change layer properties (from non-trainable to `trainable`) of last few layers, add another layer if required, update last layer for correct number of outputs and then train the model with your data. Thanks!\r\n\r\n", "How can I change the layer properties of a frozen graph inference model ( .pb )?", "@abdou31 Is this still an issue for you? Do you have access to the pre-trained model? Can you share a standalone code to reproduce the issue?\r\n\r\nIf this was resolved for you, then please feel free to close the issue. Thanks!", "This is not currently possible, at least not easily. You'll need the source graph to properly fine tune the model."]}, {"number": 33331, "title": "[bug/enhancement/minor fixes/bugs] - tensorflow/tensorflow/acnkowledg\u2026", "body": "#### [bug/enhancement/minor fixes/bugs] - tensorflow/tensorflow/acnkowledgements.md - minor fixes in grammar", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah"]}, {"number": 33330, "title": "[docs/enhancement/bug fixes/minor typos] - tensorflow/tensorflow/lice\u2026", "body": "#### [docs/enhancement/bug fixes/minor typos] - tensorflow/tensorflow/license.md - fixing minor issues and bugs", "comments": ["Thanks, but please do not make changes to the license."]}, {"number": 33329, "title": "[docs/enhancement/minor fix] - tensorflow/tensorflow - fixing minor i\u2026", "body": "\u2026ssues\r\n\r\n[docs/enhancement/minor fix] - tensorflow/tensorflow - fixing minor issues", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33329) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33329) for more info**.\n\n<!-- ok -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah"]}, {"number": 33328, "title": "use tf.layers.dense as a layer ups bug", "body": "hi,dear all,\r\nI just wanna use the tf's dense,\r\nand when I set tf.layers.dense as a keras layer,it ups a bug,\r\nhow to solve it ?\r\n\r\ncodes:\r\n```\r\ndef dense(inputs):\r\n    return tf.layers.dense(inputs,units=513,activation='relu')\r\n\r\nx_mixed=tf.keras.Input(shape=(5, 256),dtype=tf.float32, name='input')\r\nfc_out=tf.keras.layers.Lambda(dense)(x_mixed)\r\n\r\nmodel=tf.keras.Model(inputs=x_mixed,outputs=fc_out)\r\nmodel.compile(loss=\"mse\",metrics=['mae'],optimizer='adam')\r\nmodel.summary()\r\n\r\nX=np.random.randn(1000,5,256)\r\ny=np.random.randn(1000,5,513)\r\nmodel.fit(X,y,batch_size=16,epochs=5)\r\n```\r\n\r\nbug\uff1a\r\n`ValueError: Tensor conversion requested dtype float32_ref for Tensor with dtype float32: 'Tensor(\"Adam/dense/kernel/m/Initializer/zeros:0\", shape=(256, 513), dtype=float32)'`\r\nThx\r\n\r\n", "comments": ["@ucasiggcas \r\nCan you please let us know the Tensorflow version you are using?.I have tried on colab with TF version 1.15.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bf3ed878169513cdf1ee4e4fc4269bc5/untitled270.ipynb). Thanks!\r\n\r\n", "@ravikyram \r\nthe tf version is 1.14.0, and installed with\r\n`pip install tensorflow-gpu`", "I have tried on colab with TF version 1.14, 1.15.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2493233a0c605357b9cb39a5d9f1798a/untitled270.ipynb).Thanks!", "How to fix this error?", "May you try [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=nightly) in TF 2.x? As  and `tf.keras.layers.*` uses reference variables by default in TF 2.x, so the compatibility between keras model and `tf.layers.*` is not guaranteed.", "@ucasiggcas Based on @yhliang2018 suggestion, I updated one line in the code and the error is gone. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/30231da16b6ca9455e6cecf2737b4c31/untitled837.ipynb). I have used `TF1.15`. Thanks!\r\n\r\nThe line I change from `return tf.layers.dense(inputs,units=513,activation='relu')` to `return tf.keras.layers.Dense(units=513,activation='relu')(inputs)`. Thanks\r\n\r\nI am closing this issue as it was resolved. Please feel free to open if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33328\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33328\">No</a>\n", "thx"]}, {"number": 33327, "title": "modulenotfounderror no module named '_pywrap_tensorflow_internal'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\nI am using Windows 10, CPU: Intel(R) Core(TM) 2 Duo CPU T6600 @ 2.2GHz (2 CPUs) ~2.2GHz. RAM: 4GB. Video card: ATI Mobility Radeon HD 3400 Series. \r\n\r\n\r\nI installed Python 3.6 and Tensorflow==1.10.0. When I do import tensorflow, I get this error.\r\n\r\n`modulenotfounderror no module named '_pywrap_tensorflow_internal'\r\n`\r\n\r\nI can install whichever Python/Tensorflow version you want. I just want to use Tensorflow. I saw similar issues, but none of them seems to be solving my issue. ", "comments": ["Try using the latest version of Tensorflow i.e., 1.14 or 1.15. If you are encounterning the same error. Please follow this [answer](https://github.com/tensorflow/tensorflow/issues/17386#issuecomment-479668684) and it should solve your issue.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 15 days. Please add additional comments and we can open the issue again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33327\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33327\">No</a>\n"]}, {"number": 33326, "title": "[TF 2.0 API Docs] tf.GradientTape", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/GradientTape\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nClass members and Return values in \"gradient\" function not consistent with actual values in code\r\n\r\n### Correct links\r\nYes\r\n### Parameters defined\r\nYes\r\n### Returns defined\r\nNeeds to be modified\r\ngradient () :\r\n1. Argument - unconnected_gradients: a value which can either hold 'none' or 'zero'\r\n\r\n  Should be NONE or ZERO\r\n\r\n2. Return values:  \r\n\r\nIn addition to the mentioned return values, If none of the provided elements in \"sources\" argument are being watched, the function will return None\r\n\r\n\r\n### Raises listed and defined\r\nYes\r\n\r\n### Usage example\r\nYes\r\n### Submit a pull request?\r\nYes", "comments": ["Hi @prabindh,\r\n \r\nIt looks like the maintainers are only accepting more-significant Docs PRs, to reduce the overhead of tiny changes. Thanks for taking a shot. Please try again when you have more to contribute."]}, {"number": 33325, "title": "Sample pull request", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33325) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 33324, "title": "[Intel MKL] Adding OMP_NUM_THREADS support to mkl tests", "body": "This will speed up our public CI", "comments": []}, {"number": 33323, "title": "ValueError when using AdamOptimizer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA10.0 and cnDNN7.6\r\n\r\n**Describe the current behavior**\r\nI instantiate the Adam in two ways, one works but the another report the ValueError.\r\n\r\n**Describe the expected behavior**\r\nThere should be no difference between two ways to instantiate the optimizers.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.keras import layers, optimizers\r\nimport tensorflow as tf\r\n\r\n# model\r\ninp = layers.Input((None, None, 3))\r\nx = layers.Conv2D(32, 3, padding=\"same\")(inp)\r\nx = layers.Conv2D(3, 3, padding=\"same\")(inp)\r\nmodel = keras.Model(inp, x)\r\n\r\n# compile\r\n# opt = keras.optimizers.Adam() # This method works\r\nopt = optimizers.Adam()  # This method doesn't work\r\nmodel.compile(optimizer=opt, loss=\"mse\")\r\n\r\n# data\r\nx = tf.ones((16, 48, 48, 3))\r\ny = tf.zeros((16, 48, 48, 3))\r\n\r\n# train\r\nmodel.fit(x, y, batch_size=1)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 527, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 437, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 541, in _apply_op_helper\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 437, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"demo.py\", line 21, in <module>\r\n    model.fit(x, y, batch_size=1)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 674, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 189, in model_iteration\r\n    f = _make_execution_function(model, mode)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 565, in _make_execution_function\r\n    return model._make_execution_function(mode)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2184, in _make_execution_function\r\n    self._make_train_function()\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2116, in _make_train_function\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py\", line 476, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py\", line 92, in get_gradients\r\n    if None in grads:\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\", line 1336, in tensor_equals\r\n    return gen_math_ops.equal(self, other)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 3627, in equal\r\n    name=name)\r\n  File \"/home/weitong/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 545, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.\r\n```\r\n", "comments": ["There is another post mentioning the implementation of these two are different. \r\n\r\n[link](https://github.com/tensorflow/tensorflow/issues/32980#issue-501286446)", "Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/ec7b69ab776f7d708a0d8de94900c6c7/33323.ipynb) of colab.Thanks!", "@Mulns Anything under `tf.python.*` is private, intended for development only, rather than public use. Importing from tensorflow.python or any other modules (including import tensorflow_core...) is not supported, and can break unannounced. For full response on similar issue, you can check [here](https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546).\r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33323\">No</a>\n"]}, {"number": 33322, "title": "Can distributed strategy solve VRAM runout isuue?", "body": "I have a complex function with tens of millions parameters and two hundreds of variables. I need to calculate the gradient of this function,but my Nvidia graphic card has not enough VRAM. I tried with two cards but it cannot solve the issue.\nTwo cards only accelerate the calculating speed, cannot solve memory problem, because every video card needs to save the same calculation graph, right?\n\nAnyone can help me?", "comments": ["@Dream7-Kim, Please provide the standalone code and also tensorflow version. Please go through this [link](https://www.tensorflow.org/guide/distributed_training). Thanks!", "@gadagashwini\r\n\r\n```python\r\nimport numpy.random as rng\r\n\r\nNUM_VAR = 10**5\r\nNUM_ARG = 200\r\n\r\nvar = 10 * rng.rand(1, NUM_VAR)\r\narg = rng.rand(NUM_ARG, 10)\r\nvarf = open('variable.txt', 'w')\r\nfor vari in var[0]:\r\n    varf.write(str(vari) + \" \")\r\nvarf.close()\r\n\r\nargf = open('argument.txt', 'w')\r\nfor i in range(10):\r\n    # print(arg[:, i])\r\n    for argi in arg[:, i]:\r\n        argf.write(str(argi) + \" \")\r\n    argf.write(\"\\n\")\r\nargf.close()\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\nimport read_data\r\nimport write_res\r\nimport recordtime\r\n\r\ntf.debugging.set_log_device_placement(True)\r\nprint(\"Num GPUs available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\nvar, arg = read_data.read_data()\r\n\r\ndef func(arg):\r\n    divider = 0 # denominator\r\n    numerator = 0\r\n    temp = tf.matmul(arg, var)\r\n    temp_sin = tf.math.sin(temp)\r\n    temp_cos = tf.math.cos(temp)\r\n    temp = tf.math.add(temp_cos, temp_sin)\r\n\r\n    temp1 = tf.pow(temp, 2)\r\n    divider = tf.reduce_sum(temp1)\r\n    divider = tf.pow(divider, 1/2)\r\n\r\n    temp2 = tf.reduce_sum(temp, 1)\r\n    temp2 = tf.pow(temp2, 2)\r\n    numerator = tf.reduce_sum(temp2)\r\n\r\n    ress = tf.divide(numerator, divider)\r\n\r\n    return tf.math.log(ress)\r\n\r\nrecordtime.record(\"-----------------------------tensorflow execution time--------------------------------\")\r\nprint(\"-----------------------------tensorflow execution time--------------------------------\")\r\nfor i in range(10):\r\n    arg_tf = tf.Variable(initial_value=arg[:, i].reshape(200,1), dtype=tf.float64)\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    \r\n    start = time.time()\r\n    with strategy.scope():\r\n        with tf.GradientTape(persistent=True) as g:\r\n            g.watch(arg_tf)\r\n            fun = func(arg_tf)\r\n        grad = g.gradient(fun, arg_tf)\r\n    print(grad)\r\n    end = time.time()\r\n    recordtime.record(\"\\t\" + str(i+1) + \"th execution time: \" + str(end-start) + \" s ... ... ...\")\r\n    print(\"\\t\" + str(i+1) + \"th execution time: \" + str(end-start) + \" s ... ... ...\")\r\n    write_res.writeres(grad.numpy(), 'tfres.txt')\r\n    del g\r\n```\r\n\r\nThis is my TF version code to run. The first one is to generate random data to test.\r\nI browsed the link you provided. But I cannot find appropriate solution for my problem. MirroredStrategey cannot solve this problem. It needs to use ```model parallelism``` . But support for ```model parallelism``` in TF is specialized for models of deep learning. And as I know it will be distribute layers of neural network to multiple devices, but I need to calculate the gradient of a function and its calculation graph will be very big. Have you any solution for this?\r\nThanks for you reply and help!", "distribution strategy doesn't yet support model parallelism, so you're right that it cannot help with your use case currently. Have you looked at mesh TF (https://github.com/tensorflow/mesh)? It is a general model parallelism solution built on top of tensorflow.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}, {"number": 33321, "title": "\"Skipping rendezvous re-initialization\" when calling collective_ops in a distributed cluster", "body": "We meet a [problem](https://stackoverflow.com/questions/58161090/when-does-skipping-rendezvous-re-initialization-emerge-in-calling-collective) which continuously logs `re-initialization` within the same MonitoredSession on the leader worker. Furthermore, there seems to be an accuracy losing during the gradient descent of the parabolic toy model. We have no idea how to prevent the logging of `re-initialization`. All hints or explanations are welcome.\r\n\r\nIt seems the [`BaseRemoteRendezvous::Initialize`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc#L152) method is called at every train step. The log of the leader worker is attached below.\r\n```console\r\ntask 0: before syncing ...\r\n{'u': 0.1, 'loss': -0.19, 'global_step': 0}\r\ntask 0: running init_ops ...\r\n{'u': 0.1, 'loss': -0.19, 'global_step': 0}\r\ntask 0: obtaining gstep ...\r\ntrain one step at step 0.\r\n2019-09-30 10:15:56.954439: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n{'u': 0.1, 'loss': -0.19, 'global_step': 1}\r\ntrain one step at step 1.\r\n2019-09-30 10:15:56.959006: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n{'u': 0.28, 'loss': -0.4816, 'global_step': 2}\r\ntrain one step at step 2.\r\n2019-09-30 10:15:56.962205: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n{'u': 0.42396685, 'loss': -0.66818583, 'global_step': 3}\r\ntrain one step at step 3.\r\n2019-09-30 10:15:56.965719: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n{'u': 0.53912044, 'loss': -0.78759, 'global_step': 4}\r\ntrain one step at step 4.\r\n2019-09-30 10:15:56.969335: I tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc:159] Skipping rendezvous re-initialization.\r\n{'u': 0.6312327, 'loss': -0.8640107, 'global_step': 5}\r\n{'u': 0.70491827, 'loss': -0.9129268, 'global_step': 5}\r\ntask 0: final syncing ...\r\ntask 0: finalized\r\n```\r\nA related issue #32810 also has the similar `re-initialization` at first. But the logging disappears with the improved code snippet, which patches the `send_tensor`, `recv_tensor` and `ChiefSessionCreator` on both workers.", "comments": ["@whhu \r\nI believe the code provided in Stack overflow is the reproducible code with TF version 1.13. Please, confirm.Thanks!", "I'm sorry the code is not the reproducible one. We come across the `re-initialization` problem in a private project, which makes use of the `collective_ops` in a similar way as the stackoverflow [question](https://stackoverflow.com/questions/58161090/when-does-skipping-rendezvous-re-initialization-emerge-in-calling-collective). We try some examples, but this weird problem occurs only in our private project. That is why we want to find out when TensorFlow logs `Skipping rendezvous re-initialization`.\r\n\r\nWe try [`broadcast_send`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/collective_ops.py#L61)/[`broadcast_recv`](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/collective_ops.py#L108) and find out that two workers hang and the logging is triggered by worker 0 when worker 1 fetches the collective tensor defined on worker 0. I wish this maybe helpful to illustrate the problem.", "@whhu \r\nIs it possible for you to share simple stand alone code to reproduce the issue. It will be easy for localizing the issue faster. Also, let us know which TensorFlow version you are using?", "The evidence looks like irrelevant to the `collective_ops`. In our project, the accessing to global_step triggers the `re-initialization` problem, which is hard to imagine. (-\\_-)!!\r\nMany thanks for the response all the same. I will close this issue and may start a new one when we locate the problem."]}, {"number": 33320, "title": "tf.distribute.MirroredStrategy read data from tfrecord slower!!!!", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES\r\n- OS Platform and Distribution : Redhat 7.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): tensorflow 2.0.0b1\r\n- Python version: 3.5.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory: CRNN for OCR\r\n\r\n**Describe the current behavior**\r\nUsing tf.distribute.MirroredStrategy parallel training, it was found that reading tfrecord was abnormally slow and the final training speed was very slow.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n# tfrecord reader\r\n        eos_idx = self.charset.get_eosid()\r\n        padded_shapes = ([], [self.norm_h, None, 3], [], [None], [], [], [])\r\n        padding_values = ('', 0.0, '', eos_idx, 0, '', 0)\r\n        fileset = tf.data.Dataset.list_files(self.file_list)\r\n        dataset = fileset.apply(\r\n                    tf.data.experimental.parallel_interleave(\r\n                        lambda filename: tf.data.TFRecordDataset(\r\n                            filename, num_parallel_reads=self.num_parallel),\r\n                        cycle_length=32))\r\n        dataset = dataset.map(map_func=self.parse_example, num_parallel_calls=self.num_parallel)\r\n        dataset = dataset.filter(self.filter)\r\n        dataset = dataset.apply(tf.data.experimental.ignore_errors()).shuffle(5000)\r\n        dataset = dataset.padded_batch(self.batch_size, padded_shapes, padding_values)\r\n\r\n        if repeat != 0:\r\n            dataset = dataset.repeat(repeat)\r\n        dataset = dataset.prefetch(buffer_size=6000) \r\n\r\n# tf.function code\r\n    @tf.function\r\n    def distributed_train_step(dataset_inputs):\r\n        per_replica_losses = mirrored_strategy.experimental_run_v2(train_step_multi, args=(dataset_inputs,))\r\n        return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\r\n    @tf.function\r\n    def distributed_test_step(dataset_inputs):\r\n        return mirrored_strategy.experimental_run_v2(test_step_multi, args=(dataset_inputs,))\r\n\r\n# The following code will have an apparent idle state of about 1s after each iteration.\r\n    with mirrored_strategy.scope():\r\n        iter_count = 0\r\n        for epoch in range(train_epoch):\r\n            logger.info(\"run epoch {}\".format(epoch))\r\n            for batch, data in enumerate(train_dataset):\r\n                try:\r\n                    distributed_train_step(data)\r\n                    if iter_count % show_interval == 0:\r\n                        print(\"train_loss:\", train_loss.result().numpy())\r\n                        train_loss.reset_states()\r\n                except Exception as e:\r\n                    print(\"Exception:\", str(e))\r\n\r\n\r\n# The following code is much faster than the above code, gpu will not be obviously idle, but the usage rate of gpu cannot be maintained more than 90%.\r\n    with mirrored_strategy.scope():\r\n        iter_count = 0\r\n        for epoch in range(train_epoch):\r\n            logger.info(\"run epoch {}\".format(epoch))\r\n            data = None\r\n            for batch, data1 in enumerate(train_dataset):\r\n               data  = data1\r\n               break\r\n            for i in range(999999999):\r\n                try:\r\n                    distributed_train_step(data)\r\n                    if iter_count % show_interval == 0:\r\n                        print(\"train_loss:\", train_loss.result().numpy())\r\n                        train_loss.reset_states()\r\n                except Exception as e:\r\n                    print(\"Exception:\", str(e))\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@attendfov, Can you please test against latest tensorflow version 2.0.0 to see whether the issue still persists or not ?. `pip install -U tensorflow`. Thanks!", "# Upgrade tf to version 2.0.0. Run the previous ocr to identify the training program, which is exactly the same as the previous problem. During the running process, there are warnings and errors: the ignore_longer_outputs_than_inputs flag does not see the parameters that need to be passed in the ctc_loss interface of tf2.0. The specific running logs are as follows:\r\n\r\n\r\n2019-10-17 18:29:03.329901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-17 18:29:03.329919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-17 18:29:03.329935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-17 18:29:03.329951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-17 18:29:03.329966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-17 18:29:03.329981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-17 18:29:03.330004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-17 18:29:03.332832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-10-17 18:29:03.332896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-17 18:29:03.332908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-10-17 18:29:03.332915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N \r\n2019-10-17 18:29:03.332920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N \r\n2019-10-17 18:29:03.335789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15190 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\r\n2019-10-17 18:29:03.336565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15190 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\r\nWARNING:tensorflow:From /home/junhuang.hj/code/att_ctc_tf2/ctc/../utils/Dataset.py:672: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\n2019-10-17 18:29:04,553 - deprecation.py - 323 - new_func - From /home/junhuang.hj/code/att_ctc_tf2/ctc/../utils/Dataset.py:672: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\n2019-10-17 18:29:06,858 - TrainMultiDebugIssue.py - 398 - <module> - run epoch 0\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:10,490 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:12,963 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:14.790925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-17 18:29:15.026614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:20,656 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:20,658 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:20,658 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:20,659 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\ntrain_loss: 255.27899\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:21,414 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:25,645 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:25,647 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:25,648 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:25,649 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\ntrain_loss: 197.00568\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:26,416 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:30,101 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2019-10-17 18:29:30,105 - cross_device_ops.py - 427 - reduce_implementation - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\ntrain_loss: 167.84708\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:30,880 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\ntrain_loss: 57.613113\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:35,156 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:5 out of the last 5 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:29:39,098 - def_function.py - 474 - __call__ - 5 out of the last 5 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 54.202675\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:39,943 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:6 out of the last 6 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:29:43,620 - def_function.py - 474 - __call__ - 6 out of the last 6 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 50.241222\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:44,397 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:7 out of the last 7 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:29:48,322 - def_function.py - 474 - __call__ - 7 out of the last 7 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 53.948166\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:49,092 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:8 out of the last 8 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:29:52,523 - def_function.py - 474 - __call__ - 8 out of the last 8 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 53.695297\r\nINFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2019-10-17 18:29:53,317 - cross_device_ops.py - 748 - _do_batch_all_reduce - batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:9 out of the last 9 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:29:56,621 - def_function.py - 474 - __call__ - 9 out of the last 9 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 49.95946\r\nWARNING:tensorflow:10 out of the last 10 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:30:01,105 - def_function.py - 474 - __call__ - 10 out of the last 10 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 50.3804\r\n2019-10-17 18:30:04.528594: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at ctc_loss_op.cc:168 : Invalid argument: Not enough time for target transition sequence (required: 14, available: 9)24You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n2019-10-17 18:30:04.528718: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 14, available: 9)24You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[{{node CTCLoss}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/SGD/SGD/update_0/Const/_49]]\r\n2019-10-17 18:30:04.528767: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 14, available: 9)24You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[{{node CTCLoss}}]]\r\n\t [[replica_1/CTCLoss/_40]]\r\n2019-10-17 18:30:04.528890: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 14, available: 9)24You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[{{node CTCLoss}}]]\r\nException: 2 root error(s) found.\r\n  (0) Invalid argument:  Not enough time for target transition sequence (required: 14, available: 9)24You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[node CTCLoss (defined at /home/junhuang.hj/bin/anaconda3_tf20/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n\t [[GroupCrossDeviceControlEdges_0/SGD/SGD/update_0/Const/_49]]\r\n  (1) Invalid argument:  Not enough time for target transition sequence (required: 14, available: 9)24You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[node CTCLoss (defined at /home/junhuang.hj/bin/anaconda3_tf20/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_distributed_train_step_13289]\r\n\r\nFunction call stack:\r\ndistributed_train_step -> distributed_train_step\r\n\r\nWARNING:tensorflow:11 out of the last 11 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n2019-10-17 18:30:09,254 - def_function.py - 474 - __call__ - 11 out of the last 11 calls to <function distributed_train_step at 0x7f3c403e47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\ntrain_loss: 41.91076\r\n2019-10-17 18:30:12.834006: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at ctc_loss_op.cc:168 : Invalid argument: Not enough time for target transition sequence (required: 24, available: 10)46You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n2019-10-17 18:30:12.834102: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 24, available: 10)46You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[{{node replica_1/CTCLoss}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/SGD/SGD/update_0/Const/_49]]\r\n2019-10-17 18:30:12.834148: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 24, available: 10)46You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[{{node replica_1/CTCLoss}}]]\r\n\t [[replica_1/CTCLoss/_40]]\r\n2019-10-17 18:30:12.834244: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Not enough time for target transition sequence (required: 24, available: 10)46You can turn this error into a warning by using the flag ignore_longer_outputs_than_inputs\r\n\t [[{{node replica_1/CTCLoss}}]]\r\n", "Can you provide profiles so we can take a look at what's slow?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 33319, "title": "Unable to run tensorflow-gpu 2.0.0 on RStudio after update", "body": "The following error pops:\r\n\r\n`> tensorflow::tf_config()`\r\n\r\n_tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library '**cudart64_100.dll**'; dlerror: **cudart64_100.dll** not found_\r\n\r\n- Have CUDA v.10.1 installed.  Folder CUDA\\ v10.1\\bin shows **cudart64_101.dll** not cudart64_100.dll\r\n- Have CUDNN v.7.6.4.38 installed\r\n- Environment variables point to CUDA\\v10.1\\bin and CUDA\\v10.1\\libnvvp\r\n- Have created CUDA_HOME, CUDA_PATH and CUDA_PATH_V10_1 - all pointing to the same folder, CUDA\\v10.1\r\n- Anaconda3 contains 3 environments: _base, r-reticulate and tensorflow_, all having python 3.7.4 (do I really need one python for each environment?)\r\n- Anaconda3 environment _tensorflow_ shows tensorflow-gpu 1.14.0 as I have installed tensorflow 2.0.0 from RStudio:\r\n\r\n```\r\n> require(keras)\r\n> install_keras(tensorflow = 'gpu')\r\n```\r\n\r\nwhich installed tensorflow 2.0.0.\r\n\r\nThis was verified via conda\r\n\r\n`> pip list | grep tensorflow`\r\n\r\n_tensorflow 1.14.0_                     ## installed via Anaconda3 Navigator\r\n_tensorflow-gpu 2.0.0_                ## version 1.14.0 shown in Anaconda3 Navigator\r\n_tensorflow-estimator 2.0.0_       ## version 1.14.0 shown in Anaconda3 Navigator\r\n\r\n- I work with\r\n\r\nOS Windows 10\r\nGPU Nvidia GeForce RTX 2080\r\n\r\nI should mention that keras with tensorflow-gpu v 1.14.0 backend worked on RStudio before my tensorflow update to 2.0.0.\r\n\r\nPlease advise, thank you!\r\n", "comments": ["@drag05, Tensorflow supports CUDA10.0. Please downgrade the CUDA version and check. Thanks!", "@gadagashwini Thank you for your patience!\r\n\r\nHaving reverted to Miniconda3, I kept CUDA v.10.1 on and installed tensorflow-gpu v. 2.0.0 using pip with the following results:\r\n\r\n`reticulate::py_config()`\r\n\r\n_python:         C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\\python.exe\r\nlibpython:      C:/Users/Dragos/Miniconda3/envs/tensorflow/python36.dll\r\npythonhome:     C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\r\nversion:        3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\r\nArchitecture:   64bit\r\nnumpy:          C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\r\nnumpy_version:  1.16.5\r\ntensorflow:     C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.p_\r\n\r\n_python versions found: \r\n C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\\python.exe\r\n C:\\Users\\Dragos\\Miniconda3\\python.exe_\r\n\r\nAlso,\r\n\r\n`> tensorflow::tf_config()`\r\n\r\n_TensorFlow v2.0.0 (C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.p)\r\nPython v3.6 (C:\\Users\\Dragos\\Miniconda3\\envs\\tensorflow\\python.exe)_\r\n\r\nAnd, finally:\r\n\r\n`require(keras)`\r\n\r\n`k <- backend()`\r\n\r\n`sess <- k$get_session()`\r\n\r\n_2019-10-15 10:26:37.865668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-10-15 10:26:37.973538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 with Max-Q Design major: 7 minor: 5 memoryClockRate(GHz): 1.095\r\npciBusID: 0000:01:00.0\r\n2019-10-15 10:26:37.973927: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-10-15 10:26:37.974446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-15 10:26:37.975237: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-10-15 10:26:37.977148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 with Max-Q Design major: 7 minor: 5 memoryClockRate(GHz): 1.095\r\npciBusID: 0000:01:00.0\r\n2019-10-15 10:26:37.977514: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-10-15 10:26:37.978013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-15 10:26:38.574030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-15 10:26:38.574280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-15 10:26:38.574428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-15 10:26:38.575575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6273 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)_\r\n\r\nIt seems the GPU is recognized and listed. \r\n\r\n`> keras:::keras_version()`\r\n[1] \u20182.2.4\u2019\r\n\r\nRunning the Boston Housing example with 4 folds as described in the book \"Deep Learning with R\" I get:\r\n\r\nprocessing fold # 1 \r\n\r\n_2019-10-15 10:40:19.996965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 with Max-Q Design major: 7 minor: 5 memoryClockRate(GHz): 1.095\r\npciBusID: 0000:01:00.0\r\n2019-10-15 10:40:19.997342: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-10-15 10:40:19.997978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-15 10:40:19.998205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-15 10:40:19.998437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-15 10:40:19.998575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-15 10:40:19.999132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6273 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-10-15 10:40:21.989893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll_\r\n\r\nprocessing fold # 2 \r\n\r\nprocessing fold # 3 \r\n\r\nprocessing fold # 4_\r\n\r\n\r\nPlease advise, thank you!", "@gadagashwini: I forgot to mention that now I get the following message although I have CUDA 10.1 installed; for example running the Reuters dataset, I get:\r\n\r\n```\r\n> if(!exists('reuters')) \r\n+   reuters <- dataset_reuters(num_words = 10000)\r\n```\r\n_2019-10-15 15:10:53.468098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll_\r\n\r\nIt seems to be working. I will close this issue but please feel free to comment!", "@drag05, Glad it is working.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33319\">No</a>\n"]}, {"number": 33318, "title": "Update outdated K.eval", "body": "`K.eval` for K per `import tensorflow.keras.backend as K` fails for a `tensorflow.python.framework.ops.Tensor` tensor for operations involving `tensorflow.python.ops.resource_variable_ops.ResourceVariable`. Minimal reproducible example for a custom optimizer below; replacing `K.eval` with `K_eval` fixes the problem. `eval_fn` code taken from [Keras' backend](https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L908)\r\n\r\n(_Another_ problem does remain for `.save()`, but one down)\r\n\r\n<hr>\r\n\r\n**Proposed fix**:\r\n\r\n```python\r\ndef K_eval(x):\r\n    try:\r\n        return K.get_value(K.to_dense(x))\r\n    except:\r\n        eval_fn = K.function([], [x])\r\n        return eval_fn([])[0]\r\n```\r\n<hr>\r\n\r\n**UPDATE**: problem is also reproducible in `tensorflow.keras` optimizers & layers, TensorFlow 2.0.0, when eager execution is disabled - overcoming the following error upon `K.eval`:\r\n\r\n**Minimal reproducible example 2**:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nvar = K.variable([2.], name='var')\r\n\r\ntry:\r\n    print(\"K.get_value\", K.get_value(var))\r\nexcept:\r\n    try:\r\n        print(\"K.eval\", K.eval(var))\r\n    except:\r\n        try:\r\n            print(\"K.eager(K.get_value)\", K.eager(K.get_value)(var))\r\n        except:\r\n            try:\r\n                print(\"K.eager(K.eval)\", K.eager(K.eval)(var))\r\n            except:\r\n                print(\"K_eval\", K_eval(var))\r\n```\r\n```python\r\nK_eval [2.]\r\n```\r\n<hr>\r\n\r\n**Minimal reproducible example**:\r\n\r\n```python\r\nfrom tensorflow.python.keras.optimizers import Optimizer\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Model\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\nipt   = Input(shape=(4,))\r\nout   = Dense(1,  activation='sigmoid')(ipt)\r\nmodel = Model(ipt, out)\r\nmodel.compile(SGD(lr=1e-2), loss='binary_crossentropy')\r\n\r\nX = np.random.randn(32,4)\r\nY = np.random.randint(0,3,(32,1))\r\nmodel.train_on_batch(X,Y)\r\n\r\nmodel.save(\"path.h5\")\r\n```\r\n\r\n<hr>\r\n\r\n**Custom optimizer**:\r\n\r\n```python\r\nclass SGD(Optimizer):\r\n    def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False,\r\n                 total_iterations=100, eta_min=0, eta_max=1,\r\n                 t_cur=0, init_verbose=True, **kwargs):\r\n        eta_t = kwargs.pop('eta_t', 1.)\r\n        super(SGD, self).__init__(**kwargs)\r\n        with K.name_scope(self.__class__.__name__):\r\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\r\n            self.lr = K.variable(lr, name='lr')\r\n            self.momentum = K.variable(momentum, name='momentum')\r\n            self.decay = K.variable(decay, name='decay')\r\n            self.eta_min = K.constant(eta_min, name='eta_min')\r\n            self.eta_max = K.constant(eta_max, name='eta_max')\r\n            self.eta_t = K.variable(eta_t, dtype='float32', name='eta_t')\r\n            self.t_cur = K.variable(t_cur, dtype='int64', name='t_cur')\r\n\r\n        self.initial_decay = decay\r\n        self.nesterov = nesterov\r\n        self.total_iterations = total_iterations\r\n\r\n    def get_updates(self, loss, params):\r\n        grads = self.get_gradients(loss, params)\r\n        self.updates = [state_ops.assign_add(self.iterations, 1)]\r\n        self.updates.append(state_ops.assign_add(self.t_cur, 1))\r\n\r\n        lr = self.lr\r\n\r\n        # momentum\r\n        shapes = [K.int_shape(p) for p in params]\r\n        moments = [K.zeros(shape) for shape in shapes]\r\n        self.weights = [self.iterations] + moments\r\n\r\n        self.eta_t = _compute_eta_t(self)\r\n\r\n        for p, g, m in zip(params, grads, moments):\r\n            v = self.momentum * m - lr * g  # velocity\r\n            self.updates.append(state_ops.assign(m, v))\r\n\r\n            if self.nesterov:\r\n                p_t = p + self.momentum * v - lr * g\r\n            else:\r\n                p_t = p + v\r\n\r\n            new_p = p_t\r\n\r\n            # Apply constraints.\r\n            if getattr(p, 'constraint', None) is not None:\r\n                new_p = p.constraint(new_p)\r\n\r\n            self.updates.append(state_ops.assign(p, new_p))\r\n        return self.updates\r\n\r\n    def get_config(self):\r\n        config = {\r\n            'lr': float(K.get_value(self.lr)),\r\n            'momentum': float(K.get_value(self.momentum)),\r\n            'decay': float(K.get_value(self.decay)),\r\n            'nesterov': self.nesterov,\r\n            'total_iterations': int(self.total_iterations),\r\n            'eta_t': int(K.eval(self.eta_t)),\r\n            't_cur': int(K.get_value(self.t_cur)),\r\n            'eta_min': int(K.get_value(self.eta_min)),\r\n            'eta_max': int(K.get_value(self.eta_max)),\r\n        }\r\n        base_config = super(SGD, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\ndef _compute_eta_t(cls):\r\n    PI = 3.141592653589793\r\n    t_frac = K.cast(cls.t_cur / cls.total_iterations , 'float32')\r\n    eta_t = cls.eta_min + 0.5 * (cls.eta_max - cls.eta_min) * \\\r\n        (1 + K.cos(PI * t_frac))\r\n    return eta_t\r\n```\r\n\r\n<hr>\r\n\r\n**Full error trace**:\r\n\r\n```python\r\n\r\n  File \"<ipython-input-7-7b8a41253feb>\", line 1, in <module>\r\n    model.save(\"path.h5\")\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 975, in save\r\n    signatures, options)\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 112, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\", line 99, in save_model_to_hdf5\r\n    model_metadata = saving_utils.model_metadata(model, include_optimizer)\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saving_utils.py\", line 202, in model_metadata\r\n    'config': model.optimizer.get_config()}\r\n\r\n  File \"<ipython-input-3-5d1359d04ea5>\", line 70, in get_config\r\n    'eta_t': int(K.eval(self.eta_t)),\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 1271, in eval\r\n    return get_value(to_dense(x))\r\n\r\n  File \"D:\\Anaconda\\envs\\tf2_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3267, in get_value\r\n    return x.numpy()\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "comments": ["**UPDATE**: problem is also reproducible in `tensorflow.keras` optimizers & layers, TensorFlow 2.0.0, when eager execution is disabled - overcoming the following error upon `K.eval`:\r\n\r\n```python\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\n```\r\n\r\n<hr>\r\n\r\n**Minimal reproducible example 2**:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nvar = K.variable([2.], name='var')\r\n\r\ntry:\r\n    print(\"K.get_value\", K.get_value(var))\r\nexcept:\r\n    try:\r\n        print(\"K.eval\", K.eval(var))\r\n    except:\r\n        try:\r\n            print(\"K.eager(K.get_value)\", K.eager(K.get_value)(var))\r\n        except:\r\n            try:\r\n                print(\"K.eager(K.eval)\", K.eager(K.eval)(var))\r\n            except:\r\n                print(\"K_eval\", K_eval(var))\r\n```\r\n```python\r\nK_eval [2.]\r\n```", "@fchollet Can you please take a look on this PR? Thanks!", "If you are trying to get the value of the variable, you should use K.get_value(), which should work for both eager and graph context.\r\n\r\nIn future, we will probably remove backend functions since TF is the only backend now. There isn't a level of abstraction needed here.", "@qlzh727 As shown in minimal reproducible example 2, `K.get_value()` fails. In fact, my fix isn't foolproof and fails for some Variables in `tf.keras` optimizers, with very ambiguous errors. Couldn't figure a way around it reasonably, so nothing to add to `K_eval` - but this one is at least more reliable than the current `K.eval` implementation.\r\n\r\nAgreed that development should just proceed with `tf.keras`; retaining backend neutrality is a pain, and rather unnecessary given usage statistics. Though I don't look forward to losing `keras` implementations, as they're significantly more readable than `tf.keras`.", "A new error this change addresses:\r\n\r\n```python\r\nValueError: Operation 'lstm/while/add_27' has been marked as not fetchable.\r\n```\r\n\r\nContext: in [LSTMCell](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L2027), add the following:\r\n\r\n```python\r\n# in __init__\r\nself.step = K.variable(0, dtype='int64', name='step')\r\n\r\n# in call\r\nself.step = self.step + 1\r\n```\r\nThen, trying `K.get_value` or `K.eval` on `lstm_layer.cell.step` will yield the error. Yes, a `K.constant` shouldn't be updated in this manner, but still - however, `K_eval` doesn't fix the same error if using `self.step = self.step.assign_add(1)` instead.", "Sorry for the late reply.\r\n\r\nI wasn't be able to repo the issue with Minimal reproducible example 2 and latest tf nightyly\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\ntf.compat.v1.disable_eager_execution()\r\nprint(tf.version.VERSION)\r\n\r\nvar = K.variable([2.], name='var')\r\n\r\ntry:\r\n  print(\"K.get_value\", K.get_value(var))\r\nexcept:\r\n  try:\r\n    print(\"K.eval\", K.eval(var))\r\n  except:\r\n    try:\r\n      print(\"K.eager(K.get_value)\", K.eager(K.get_value)(var))\r\n    except:\r\n      try:\r\n        print(\"K.eager(K.eval)\", K.eager(K.eval)(var))\r\n      except:\r\n        print(\"K_eval\", K_eval(var))\r\n```\r\n\r\n2.2.0-dev20200227\r\nK.get_value [2.]\r\n", "@OverLordGoldDragon Can you please check qlzh727's comments and keep us posted? Thanks!", "@gbaned Good to see one of my examples is addressed (thanks for sharing @qlzh727), I'll verify another one in the custom optimizer context sometime.", "Pardon the late reply; I don't recall all the failure cases, but some were fixed; no time to dig further now. I'll open an issue on recurrence, as I doubt something like a try-except will be merged anyway.", "@qlzh727 @gbaned Found the culprit; the minrep. example fails under `tf.python.distribute.distribution_strategy_context.in_replica_context() == True`, which is the case for `_resource_apply_dense` invoked from `OptimizerV2._distributed_apply()`; a `K.variable` created within that context will error any `K.eval`-etc attempts. My `K_eval` worked for TF 2.1, but no longer in 2.2; at this point it seems the replica context just doesn't want its tensors evaluated like this, so I won't be hacking away further - this context-fest isn't very custom functionality-friendly."]}]