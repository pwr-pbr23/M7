[{"number": 50563, "title": "Cherry-pick: Disable the cudnn frontend for cudnn 8.1.0", "body": "git cherry-pick 75ee2f3c02559d4a3cc0ffdaf863666fca174627\r\n\r\nDisable the cudnn frontend for cudnn 8.1.0. It causes some failures in conv + relu cases:\r\n\r\n```\r\nerrors_impl.UnknownError:  CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(4358): 'status'\r\n   [[node sequential/conv2d/Relu (...) ]] [Op:__inference_train_function_962]\r\n```\r\n\r\nb/191767626\r\nPiperOrigin-RevId: 382139944\r\nChange-Id: I21d4aeda4f40d900083da37fe57831ff7e549bad", "comments": []}, {"number": 50562, "title": "Tensorflow 2.5/2.4 ptxas and gpu issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 2.4.0/2.4.2/2.5.0 gpu\r\n- Python version: 3.8.0/3.8.10\r\n- Installed using virtualenv? pip? conda?:  pip\r\n- CUDA/cuDNN version: cuda 11.0/11.1/11.2.1/11.2.2, cudnn8.1.1\r\n\r\n\r\n\r\nwhen running CNN, log shows:\r\n\r\n> 2021-07-01 22:39:34.767676: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: \r\n> Relying on driver to perform ptx compilation. \r\n> Modify $PATH to customize ptxas location.\r\n> This message will be only logged once.\r\n\r\nthe log also shows that it is **potentially using CPU and oneDNN**, even though gpu is available (and tf obviously can find it according to log)\r\nthe same message appears for combinations of tf2.4.0/2.4.2/2.5.0 with cuda11.0/11.1/11.2.1/11.2.2 (I tried several of these combinations, and the message always exist)\r\ncurrent combination on my machine: tf2.5.0+cuda11.2.1\r\n\r\n**also tried: adding ptxas.exe path to system PATH**\r\ndoesn't help\r\n\r\nsimilar to issue https://github.com/tensorflow/tensorflow/issues/49824 , but it seems that post is caused by signal handling, which I did not use\r\n\r\ncode:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.random.random((12800, 120, 120, 3,))\r\nlabel = np.array([1 for _ in range(12800)])\r\nprint(data)\r\n\r\nmodel = tf.keras.Sequential(\r\n    [tf.keras.layers.Conv2D(256, kernel_size=5, strides=2, input_shape=(120, 120, 3,), activation=\"relu\"),\r\n     tf.keras.layers.Conv2D(256, kernel_size=5, strides=2, input_shape=(120, 120, 3,), activation=\"relu\"),\r\n     tf.keras.layers.GlobalAvgPool2D(),\r\n     tf.keras.layers.Dense(1, activation=\"sigmoid\")]\r\n)\r\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy())\r\nmodel.fit(x=data, y=label, batch_size=32, epochs=20)\r\n```\r\n**note: this is just for demonstration, the same logging message appears whenever I use Conv2d in model**\r\n\r\nfull log:\r\n\r\n> 2021-07-01 22:39:31.597866: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n> 2021-07-01 22:39:31.623038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5\r\n> coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n> 2021-07-01 22:39:31.623217: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n> 2021-07-01 22:39:31.629267: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n> 2021-07-01 22:39:31.629359: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2021-07-01 22:39:31.632662: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\r\n> 2021-07-01 22:39:31.633830: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\r\n> 2021-07-01 22:39:31.641547: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll\r\n> 2021-07-01 22:39:31.644421: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\r\n> 2021-07-01 22:39:31.645145: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n> 2021-07-01 22:39:31.645317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n> 2021-07-01 22:39:31.645646: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2021-07-01 22:39:31.646131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5\r\n> coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n> 2021-07-01 22:39:31.646377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n> 2021-07-01 22:39:32.111044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2021-07-01 22:39:32.111146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n> 2021-07-01 22:39:32.111202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n> 2021-07-01 22:39:32.111410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3983 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n> 2021-07-01 22:39:33.207902: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n> Epoch 1/20\r\n> 2021-07-01 22:39:33.624659: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n> 2021-07-01 22:39:34.113561: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101\r\n> 2021-07-01 22:39:34.767676: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: \r\n> Relying on driver to perform ptx compilation. \r\n> Modify $PATH to customize ptxas location.\r\n> This message will be only logged once.\r\n> 2021-07-01 22:39:34.848389: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n> 2021-07-01 22:39:35.307315: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n> 400/400 [==============================] - 27s 60ms/step - loss: 0.0016\r\n> Epoch 2/20\r\n> 400/400 [==============================] - 24s 60ms/step - loss: 2.3526e-08\r\n> Epoch 3/20\r\n> 400/400 [==============================] - 24s 60ms/step - loss: 2.3526e-08...\r\n\r\n", "comments": ["@seermer \r\n\r\nI was able to reproduce the issue reported here in tf2.5 with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/fbff8db5d4e1645ef9f63651111f9754/untitled127.ipynb#scrollTo=RlmP-up0p_-V).Thanks", "> @seermer\r\n> \r\n> I was able to reproduce the issue reported here in tf2.5 with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/fbff8db5d4e1645ef9f63651111f9754/untitled127.ipynb#scrollTo=RlmP-up0p_-V).Thanks\r\n\r\nThanks for helping. Just to clarify, I don't think there will be any error showing up, since the message shows up as a part of the TensorFlow logging information. Colab doesn't output the log, so we can't see it.\r\n\r\nI opened this issue because exited with -1 sounds like something problematic, cuda is used 0% when running model according to Task Manager, the log also shows it is using oneDNN, none of these seems to indicate TF-GPU is running correctly, but the solution provided by log (add ptxas to $PATH) does not help at all. So I would like to know how do I solve this problem. And if possible, can the log message show clearer indications on how to solve it? Considering the current message doesn't really tell anything that helps to debug.", "That message itself should be benign, but looping in @cheshire to check if the message can be made more actionable to users not familiar with TF internals.", "> (add ptxas to $PATH) does not help at all\r\n\r\nIt really should (remove the error message, not other issues) at least on Linux-based platforms. Sorry we are really short on Windows expertise. Maybe we are missing the `.exe` postfix when looking for it.\r\n\r\n> I opened this issue because exited with -1 sounds like something problematic, cuda is used 0% when running model according to Task Manager, the log also shows it is using oneDNN, none of these seems to indicate TF-GPU is running correctly\r\n\r\nAs @sanjoy pointed out above these are completely independent issues and not related to the ptxas issue. Closing this issue, feel free to reopen with more detailed information (e.g. if you believe the GPU is not utilized when it should be).", "@sanjoy @cheshire Sorry, could you please reopen the issue? Recently, I noticed that this issue seems not benign.\r\nIt is true that the model still runs without breaking with GPU even though this message appears. However, this error causes ANY jit compiled script to break. for example, running:\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef recompiled_on_launch(a, b):\r\n  return a + b\r\n\r\nrecompiled_on_launch(tf.ones([1, 10]), tf.ones([1, 10]))\r\nrecompiled_on_launch(tf.ones([1, 100]), tf.ones([1, 100]))\r\n```\r\ngives the following error log:\r\n\r\n> 2021-07-28 02:18:43.364806: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n> 2021-07-28 02:18:45.982671: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n> 2021-07-28 02:18:46.012216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5\r\n> coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n> 2021-07-28 02:18:46.012382: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\r\n> 2021-07-28 02:18:46.103818: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\r\n> 2021-07-28 02:18:46.103918: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2021-07-28 02:18:46.142653: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\r\n> 2021-07-28 02:18:46.153116: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\r\n> 2021-07-28 02:18:46.263635: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll\r\n> 2021-07-28 02:18:46.289993: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\r\n> 2021-07-28 02:18:46.294695: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n> 2021-07-28 02:18:46.294846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n> 2021-07-28 02:18:46.295205: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2021-07-28 02:18:46.295917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5\r\n> coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n> 2021-07-28 02:18:46.296217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n> 2021-07-28 02:18:46.806727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2021-07-28 02:18:46.806834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n> 2021-07-28 02:18:46.806894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n> 2021-07-28 02:18:46.807170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3983 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n> 2021-07-28 02:18:46.973764: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x20a9ed051d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2021-07-28 02:18:46.973880: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\r\n> 2021-07-28 02:18:47.040371: F tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:472] ptxas returned an error during compilation of ptx to sass: 'Internal: ptxas exited with non-zero error code -1, output: '  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.\r\n> \r\n> Process finished with exit code -1073740791 (0xC0000409)\r\n\r\nas it shows \"Process finished with exit code -1073740791 (0xC0000409)\" it quits without continue running.\r\nI have again tried reinstalling Anaconda, cuda, cudnn, tensorflow-gpu 2.5.0 according to the exact version specified many times, this problem stays.\r\nSo I would say this is not benign, and the new error message still doesn't help at all.", "To be honest, we really don't have enough Windows expertise to help you debug the issue. It seems ptxas is not found. You could try to add some print statements/logging in asm_compiler.cc to see where it is looking for it. Is your ptxas.exe definitely in your $PATH?", "@cheshire Alright, thanks, maybe I will just keep trying. I am quite sure I have ptxas.exe in path, and also all the important cuda folders. I also have tried logging info, but the error points to compiling a temp file, which disappears instantly. Maybe I will just keep trying, and see if I can somehow get the tempfile.\r\nThanks for helping."]}, {"number": 50561, "title": "BFloat16 ROCM implementation for GEMM.", "body": "", "comments": ["Adding HIP support for bfloat16 GEMM ops.\r\n\r\n/cc @cheshire "]}, {"number": 50560, "title": "Update resource_handle.h", "body": "Changed access modifier to \"private\"\r\n\r\nFix for #50481", "comments": ["> Why?\r\n\r\nHi Mihai,\r\nSubmitted pr on behalf on user as he did not want to contribute them self.\r\nPlease let me know if this is invalid, so i could close this pr and update the same to the user.", "Looked through the history change and indeed it looks like a typo. Also, tests are passing, so let's go ahead with this one.\r\n\r\nThank you for the PR."]}, {"number": 50559, "title": "Fail to use custom trained model on developing mobile app", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\nHave been downloaded the official example of object detection mobile app for android and have been run successfully. But when I replaced it with my custom trained model using SSD MobileNet V2 FPNLite 320x320, the app does not manage to start. I've been following the guide to change the .tflite and labelmap.txt and even set the quantized to false.", "comments": ["Please follow the issue template and provide logs.", "@azziqshmsdn ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50557, "title": "Trying to install tensorflow==2.0.3 with bazel on google app engine wont work ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu Google App Engine\r\n\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.0.3\r\n- Python version: 3x\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): latest from google app engine deploy bazel\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: google app engine\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have followed the exact commands from: https://www.tensorflow.org/install/source\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\r\nERRO[0000] error waiting for container: context canceled \r\n\r\nthe error I am receiving", "I have tried all the other solutions from other github issues, none has worked for me. ", "I have deployed bazel through google click to deploy containers bazel and later followed the instructions here in command shell: https://www.tensorflow.org/install/source\r\n\r\nI am using flex environment and just trying to reproduce a simple app that I have deployed with google cloud deploy but with better performance than on my computer", "@berhanpolat \r\n\r\n We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version,cuda and cudnn versions like that and steps followed before you ran into this error .Could you please fill the [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abuild%2Finstall&template=10-build-installation-issue.md). Thanks", "> @berhanpolat\r\n> \r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version,cuda and cudnn versions like that and steps followed before you ran into this error .Could you please fill the [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abuild%2Finstall&template=10-build-installation-issue.md). Thanks\r\n\r\nI have filled the info. Now I am trying to build tensorflow with CPU instead of GPU and it says 26617 targets configured, it seems that it will take hours. I am doing something wrong, I guess.", "@berhanpolat \r\n\r\nIs there any specific reason to install tf2.0 instead of latest stable Tensoflow(2.5) version.\r\n\r\nalso look at the similar [issue](https://github.com/tensorflow/tensorflow/issues/34707) and let  us know if it helps.Thanks", "I have managed torun my app with the latest tensorflow version in python. However it takes 10 sec for my mac to compute. I want to deploy my app to google app engine with bazel so that it computes sentence encoder multilingual under 1 second. Is there a way to manage this easily?\n\nThank you for your help!\n\nKind regards\n\nBerhan Polat\n\n> On Jul 1, 2021, at 3:49 PM, UsharaniPagadala ***@***.***> wrote:\n> \n> \n> @berhanpolat <https://github.com/berhanpolat>\n> Is there any specific reason to install tf2.0 instead of latest stable Tensoflow(2.5) version.\n> \n> also look at the similar issue <https://github.com/tensorflow/tensorflow/issues/34707> and let us know if it helps.Thanks\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/50557#issuecomment-872263746>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AKXSC5PEUSZJ4JVIMQUB3P3TVRXADANCNFSM47UE7OUQ>.\n> \n\n", "@berhanpolat \r\n\r\nAs per the documentation there is no GPU support for macOS. For more info please refer this [documentation](https://www.tensorflow.org/install/source#install_bazel).\r\nand also refer similar [issue](https://github.com/NVIDIA/nvidia-docker/issues/1034) and [link](https://collabnix.com/introducing-new-docker-cli-api-support-for-nvidia-gpus-under-docker-engine-19-03-0-beta-release/) \r\nThanks", "I have tried this documentation. To install tensorflow with bazel it takes hours to install it locally and gives errors at the end.\n\nI actually want to deploy my app to google engine and build tensorflow with bazel. I have also tried this but does not work. It takes too long and gives errors too.\n\nAny methods to suggest?\n\n> On Jul 9, 2021, at 7:28 PM, UsharaniPagadala ***@***.***> wrote:\n> \n> \n> @berhanpolat <https://github.com/berhanpolat>\n> As per the documentation there is no GPU support for macOS. For more info please refer this documentation <https://www.tensorflow.org/install/source#install_bazel>.Thanks\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/50557#issuecomment-877307540>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AKXSC5MQCVT7XSJEMCFNMBTTW4PT5ANCNFSM47UE7OUQ>.\n> \n\n", "@berhanpolat \r\nPlease refer to [this link](https://stackoverflow.com/questions/34562557/can-i-use-tensorflow-in-a-google-app-engine-module) and let us know.\r\n\r\n#create a docker container with tensorflow (or a prebuilt one available in gcp hub)  and use that image to deploy on app engine,by gcp hub i meant  container registry", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50557\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50557\">No</a>\n", "Hello\n\nSorry for my late answer, I was in vacation. I was trying from the beginning with flexible instances. The issue that is mentioned has been brought up 6 years ago. My problem is still not solved and issue persists. I find it weird that google does not make its own resources ready to use for gcloud applications, although so many people use them. \n\nIt is close to impossible to install tensorflow with bazel in google app engine. This was my issue and the mentioned issue is not similar to mine.\n\nThank you for your help so far. \n\nKind regards\n\nBerhan Polat\n\n\n> On Aug 19, 2021, at 5:53 PM, google-ml-butler[bot] ***@***.***> wrote:\n> \n> \n> Closed #50557 <https://github.com/tensorflow/tensorflow/issues/50557>.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/50557#event-5181219028>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AKXSC5I6BE3UZOR3MSRSQ7LT5ULFHANCNFSM47UE7OUQ>.\n> Triage notifications on the go with GitHub Mobile for iOS <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.\n> \n\n"]}, {"number": 50556, "title": "kotlin.UninitializedPropertyAccessException: lateinit property styleTransferModelExecutor", "body": "\r\nSystem information\r\n\r\nAndroid 9 \r\nMobile device :One plus 8\r\nwhen launcher google style transfer demo,it show Force close issue\r\n\r\n07-01 16:32:53.894  5715  5715 E AndroidRuntime: FATAL EXCEPTION: main\r\n07-01 16:32:53.894  5715  5715 E AndroidRuntime: Process: org.tensorflow.lite.examples.styletransfersample, PID: 5715\r\n07-01 16:32:53.894  5715  5715 E AndroidRuntime: kotlin.UninitializedPropertyAccessException: lateinit property styleTransferModelExecutor has not been initialized\r\n07-01 16:32:53.894  5715  5715 E AndroidRuntime:        at org.tensorflow.lite.examples.styletransfer.MainActivity.startRunningModel(MainActivity.kt:320)", "comments": ["@ouclbc ,\r\n\r\nPlease fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also  in order to expedite the trouble-shooting process, could you please provide complete code and the TensorFlow version you are using.Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50554, "title": "map_fn can't process sparse tensor in graph mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n\r\n**Describe the current behavior**\r\n\r\nthe following customized keras layer can't be executed in graph mode.\r\n\r\n```python\r\n#!/usr/bin/python3\r\n\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\n\r\n# please ignore this function, it is just for generating a sparse tensor\r\ndef Dense2Sparse():\r\n  dense = tf.keras.Input((None, None, None)); # dense.shape = (batch, num_heads, query_length, key_length)\r\n  mask = tf.keras.Input((1, None, None)); # mask.shape = (batch, 1, query_length or 1, key_length)\r\n  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.cond(tf.math.not_equal(tf.shape(x[0])[2], tf.shape(x[1])[2]), lambda: tf.tile(x[0], [1,1,tf.shape(x[1])[2],1,]), lambda: x[0]))([mask, dense]); # mask.shape = (batch, 1, query_length, key_length)\r\n  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.tile(x[0],[1,tf.shape(x[1])[1],1,1]))([reshaped_mask, dense]); # mask.shape = (batch, num_heads, query_length, key_length)\r\n  indices = tf.keras.layers.Lambda(lambda x: tf.where(tf.cast(x, dtype = tf.int32)))(reshaped_mask); # indices.shape = (num non zero values, 4)\r\n  values = tf.keras.layers.Lambda(lambda x: tf.gather_nd(x[0], x[1]))([dense, indices]); # values.shape = (num non zero values)\r\n  sparse = tf.keras.layers.Lambda(lambda x: tf.sparse.SparseTensor(x[0], values = x[1], dense_shape = tf.cast(tf.shape(x[2]), dtype = tf.int64)))([indices, values, dense]);\r\n  return tf.keras.Model(inputs = (dense, mask), outputs = sparse);\r\n\r\n# NOTE: ***this layer cannot be executed in graph mode***\r\nclass SparseDenseMatMul(tf.keras.layers.Layer):\r\n  def __init__(self, **kwargs):\r\n    super(SparseDenseMatMul, self).__init__(**kwargs);\r\n  def call(self, inputs):\r\n    a = inputs[0]; # a.shape = (batch, heads, query_length, key_length)\r\n    b = inputs[1]; # b.shape = (batch, heads, key_length, value_dim)\r\n    reshaped_a = tf.sparse.reshape(a, (-1, tf.shape(a)[-2], tf.shape(a)[-1])); # reshaped_a.shape = (batch * heads, query_length, key_length)\r\n    reshaped_b = tf.reshape(b, (-1, tf.shape(b)[-2], tf.shape(b)[-1])); # reshaped_b.shape = (batch * heads, key_length, value_dim)\r\n    def dot(x):\r\n      a = x[0];\r\n      b = x[1];\r\n      c = tf.sparse.sparse_dense_matmul(a,b);\r\n      return c; # c.shape = (query_length, value_dim)\r\n    results = tf.map_fn(dot, (reshaped_a, reshaped_b), fn_output_signature = tf.TensorSpec((tf.shape(reshaped_a)[-2], tf.shape(reshaped_b)[-1]), dtype = tf.float32));\r\n    results = tf.reshape(results, (tf.shape(a)[0], tf.shape(a)[1], tf.shape(results)[-2], tf.shape(results)[-1]));\r\n    return results;\r\n\r\na = np.random.normal(size = (4,3,10,40));\r\nmask = np.random.randint(low = 0, high = 2, size = (4,1,10,40));\r\na = Dense2Sparse()([a, mask]);\r\nb = np.random.normal(size = (4,3,40,20)).astype(np.float32);\r\n\r\nprint(SparseDenseMatMul()([a,b])); # eager mode is OK\r\n\r\ninputs_a = tf.keras.Input((None, None, None), sparse = True);\r\ninputs_b = tf.keras.Input((None, None, None));\r\nresults_c = SparseDenseMatMul()([inputs_a,inputs_b]);\r\nmodel = tf.keras.Model(inputs = (inputs_a, inputs_b), outputs = results_c);\r\nprint(model([a,b])); # graph mode is failed\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nmap_fn should process sparse tensor in both eager and graph mode.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no \r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n#!/usr/bin/python3\r\n\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\n\r\n# please ignore this function, it is just for generating a sparse tensor\r\ndef Dense2Sparse():\r\n  dense = tf.keras.Input((None, None, None)); # dense.shape = (batch, num_heads, query_length, key_length)\r\n  mask = tf.keras.Input((1, None, None)); # mask.shape = (batch, 1, query_length or 1, key_length)\r\n  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.cond(tf.math.not_equal(tf.shape(x[0])[2], tf.shape(x[1])[2]), lambda: tf.tile(x[0], [1,1,tf.shape(x[1])[2],1,]), lambda: x[0]))([mask, dense]); # mask.shape = (batch, 1, query_length, key_length)\r\n  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.tile(x[0],[1,tf.shape(x[1])[1],1,1]))([reshaped_mask, dense]); # mask.shape = (batch, num_heads, query_length, key_length)\r\n  indices = tf.keras.layers.Lambda(lambda x: tf.where(tf.cast(x, dtype = tf.int32)))(reshaped_mask); # indices.shape = (num non zero values, 4)\r\n  values = tf.keras.layers.Lambda(lambda x: tf.gather_nd(x[0], x[1]))([dense, indices]); # values.shape = (num non zero values)\r\n  sparse = tf.keras.layers.Lambda(lambda x: tf.sparse.SparseTensor(x[0], values = x[1], dense_shape = tf.cast(tf.shape(x[2]), dtype = tf.int64)))([indices, values, dense]);\r\n  return tf.keras.Model(inputs = (dense, mask), outputs = sparse);\r\n\r\n# NOTE: ***this layer cannot be executed in graph mode***\r\nclass SparseDenseMatMul(tf.keras.layers.Layer):\r\n  def __init__(self, **kwargs):\r\n    super(SparseDenseMatMul, self).__init__(**kwargs);\r\n  def call(self, inputs):\r\n    a = inputs[0]; # a.shape = (batch, heads, query_length, key_length)\r\n    b = inputs[1]; # b.shape = (batch, heads, key_length, value_dim)\r\n    reshaped_a = tf.sparse.reshape(a, (-1, tf.shape(a)[-2], tf.shape(a)[-1])); # reshaped_a.shape = (batch * heads, query_length, key_length)\r\n    reshaped_b = tf.reshape(b, (-1, tf.shape(b)[-2], tf.shape(b)[-1])); # reshaped_b.shape = (batch * heads, key_length, value_dim)\r\n    def dot(x):\r\n      a = x[0];\r\n      b = x[1];\r\n      c = tf.sparse.sparse_dense_matmul(a,b);\r\n      return c; # c.shape = (query_length, value_dim)\r\n    results = tf.map_fn(dot, (reshaped_a, reshaped_b), fn_output_signature = tf.TensorSpec((tf.shape(reshaped_a)[-2], tf.shape(reshaped_b)[-1]), dtype = tf.float32));\r\n    results = tf.reshape(results, (tf.shape(a)[0], tf.shape(a)[1], tf.shape(results)[-2], tf.shape(results)[-1]));\r\n    return results;\r\n\r\na = np.random.normal(size = (4,3,10,40));\r\nmask = np.random.randint(low = 0, high = 2, size = (4,1,10,40));\r\na = Dense2Sparse()([a, mask]);\r\nb = np.random.normal(size = (4,3,40,20)).astype(np.float32);\r\n\r\nprint(SparseDenseMatMul()([a,b])); # eager mode is OK\r\n\r\ninputs_a = tf.keras.Input((None, None, None), sparse = True);\r\ninputs_b = tf.keras.Input((None, None, None));\r\nresults_c = SparseDenseMatMul()([inputs_a,inputs_b]);\r\nmodel = tf.keras.Model(inputs = (inputs_a, inputs_b), outputs = results_c);\r\nprint(model([a,b])); # graph mode is failed\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 44, in <module>\r\n    results_c = SparseDenseMatMul()([inputs_a,inputs_b]);\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 969, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1107, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 840, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 880, in _infer_output_signature\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    test.py:32 call  *\r\n        results = tf.map_fn(dot, (reshaped_a, reshaped_b), fn_output_signature = tf.TensorSpec((tf.shape(reshaped_a)[-2], tf.shape(reshaped_b)[-1]), dtype = tf.float32));\r\n    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_spec.py:51 __init__  **\r\n        self._shape = tensor_shape.TensorShape(shape)\r\n    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:765 __init__\r\n        self._dims = [Dimension(d) for d in dims]\r\n    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:765 <listcomp>\r\n        self._dims = [Dimension(d) for d in dims]\r\n    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:206 __init__\r\n        six.raise_from(\r\n    <string>:3 raise_from\r\n        \r\n\r\n    TypeError: Dimension value must be integer or None or have an __index__ method, got value '<tf.Tensor 'sparse_dense_mat_mul_1/strided_slice_4:0' shape=() dtype=int32>' with type '<class 'tensorflow.python.framework.ops.Tensor'>'\r\n\r\n```", "comments": ["@breadbread1984 \r\nSome Arguments are deprecated in map_fn like dtype in future versions,for this refer  the [document](https://www.tensorflow.org/api_docs/python/tf/map_fn),might be that is causing [TypeError](https://www.tensorflow.org/api_docs/python/tf/map_fn#raises).\r\n\r\nLooking at the error log at is looks like this [#169](https://github.com/tensorflow/transform/issues/169) and  [comment](https://stackoverflow.com/questions/62205053/typeerror-dimension-value-must-be-integer-or-none-or-have-an-index-method), let us know if it helps.Thanks", "I am not using dtype of map_fn, the dtype is argument of TensorSpec.", "@Saduf2019 \r\n\r\nI was able to replicate the issue reported.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/4db7aa27dfe54a10038656d0a5dbeb59/-50554.ipynb) here.Thanks", "@breadbread1984 \r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50554\">No</a>\n"]}, {"number": 50553, "title": "Fix configure.py when using bazelisk on Windows", "body": "Bazelisk on Windows has `bazel.cmd` on the path instead of `bazel.exe`, which cannot be called simply with `bazel` when using `subprocess.run` with `shell=False`. Instead, we can simply use the full path obtained by the `which` command.", "comments": []}, {"number": 50552, "title": "[TF-TRT] testTrtGraphConverter_AllowEngineNativeSegmentExecution skipped with TRT8", "body": "@bixia1 for review\r\n\r\nThis PR does 2 things:\r\n\r\n1. Deactivate `testTrtGraphConverter_AllowEngineNativeSegmentExecution` as it does not function anymore with TRT >= 8.\r\nPermanent fix will come in a following PR\r\n\r\n2. Fix a failure mode issue where the environment variable `TF_TRT_ALLOW_ENGINE_NATIVE_SEGMENT_EXECUTION` may not be reset if an AbortedError is not raised in the `converter.build()` action. Fixed using a try/finally block", "comments": ["@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "There are two comments that are not addressed:\r\nhttps://github.com/tensorflow/tensorflow/pull/50552#pullrequestreview-704375467\r\nhttps://github.com/tensorflow/tensorflow/pull/50552#pullrequestreview-701368753\r\n\r\n\r\n", "Can you also address this comment https://github.com/tensorflow/tensorflow/pull/50552#pullrequestreview-701368753\r\ncopied and pasted here:\r\nCan you also revise the PR description to describe the problem and the solution without putting code there?\r\n\r\nThanks,\r\n", "@bixia1 PR description updated. And mention to the NVBug removed. Added a TODO for a following PR."]}, {"number": 50550, "title": "add 11 missing out-of-class definitions", "body": "This fixes a broken debug build.", "comments": []}, {"number": 50549, "title": "`Failed to convert a NumPy array to a Tensor (Unsupported object type dict).`", "body": "`history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps)`\r\n\r\n`def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):`\r\n` X1, X2, y = list(), list(), list()`\r\n` for desc in desc_list:`\r\n`seq = tokenizer.texts_to_sequences([desc])[0]`\r\n` for i in range(1, len(seq)):`\r\n`in_seq, out_seq = seq[:i], seq[i] in_seq = pad_sequences([in_seq], maxlen=max_length)[0]`\r\n` out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]`\r\n` X1.append(photo)`\r\n` X2.append(in_seq)`\r\n` y.append(out_seq)`\r\n` return array(X1), array(X2), array(y)`\r\n\r\n`def data_generator(descriptions, photos, tokenizer, max_length, imgsIds, vocab_size):`\r\n`while 1:`\r\n` for ind in range(len(imgsIds)): `\r\n`photo = photos[ind]`\r\n` key = imgsIds[ind]`\r\n` desc_list = descriptions[str(key)] `\r\n`in_img, in_seq, out_word = create_sequences( tokenizer, max_length, desc_list, photo, vocab_size)`\r\n` yield [in_img, in_seq], out_word`\r\n\r\ni got\r\n\r\n`Failed to convert a NumPy array to a Tensor (Unsupported object type dict).`\r\nif there is anything i should add it please comment \u2026 Thanks\r\n\r\n`Traceback (most recent call last): File \"fit.py\", line 271, in <module> main(sys.argv) File \"fit.py\", line 268, in main fit_model(train, train_descriptions, train_rnn_input, val, val_descriptions, val_rnn_input) File \"fit.py\", line 255, in fit_model history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func return func(*args, **kwargs) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1479, in fit_generator initial_epoch=initial_epoch) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper return method(self, *args, **kwargs) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 872, in fit return_dict=True) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper return method(self, *args, **kwargs) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1057, in evaluate model=self) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1112, in __init__ model=model) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 775, in __init__ peek = _process_tensorlike(peek) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1013, in _process_tensorlike inputs = nest.map_structure(_convert_numpy_and_scipy, inputs) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 617, in map_structure structure[0], [func(*x) for x in entries], File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 617, in <listcomp> structure[0], [func(*x) for x in entries], File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1008, in _convert_numpy_and_scipy return ops.convert_to_tensor(x, dtype=dtype) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1341, in convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function return constant_op.constant(value, dtype, name=name) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 262, in constant allow_broadcast=True) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 270, in _constant_impl t = convert_to_eager_tensor(value, ctx, dtype) File \"/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type dict). 2021-06-27 04:46:22.936001: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. [[{{node PyFunc}}]]`\r\n\r\n", "comments": ["Could you please fill the issue template.Also Please provide the simple standalone code/ colab link to reproduce the issue at our end.Thanks\r\n ", "I can do it using np.asarray(variable_name),np.expand_dims(variable_name,axis)\r\n@saikumarchalla kindly  assign me this \r\n", "@namaannn  We should not assign this issue  but you can comment the answer/ workaround here.Thanks!", "Sorry sir, but can you tell why you can't assign me this issue? ", "thanks for your replying , i solved it . the problem was in my arrays i passed it was in dict form not array structure"]}, {"number": 50548, "title": "Describe the behavior on the false branch.", "body": "Per discussion with @wangpengmit.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50548) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 50547, "title": "Improve BiasAdd GPU performance with more threads", "body": "Originally, the launch config is too conservative and it only uses the number of SMs as the block number when the workload is large enough. This can barely saturate the GPU resources especially for the memory bound kernels, like the BiasAdd. Our tests show `tf.nn.bias_add` might be significantly slower than the `tf.math.add`.\r\n\r\nThis PR changes the launch config by using the `cudaOccupancyMaxPotentialBlockSize` based method to get a more reasonable block/thread config. The new results show `tf.nn.bias_add` can get comparable performance with `tf.math.add` after this change.\r\n\r\n\r\ncc. @nluehr ", "comments": []}, {"number": 50546, "title": "graph simplifications examples", "body": "I saw Tatiana Shpeisman's presentation about TF graph optimization at Stanford.\r\n\r\nI want to simplify a frozen graph (tf 1.x) from pb.file\r\n\r\nDo you have any end-to-end example on how to simplify a TF frozen graph?\r\n\r\nFor example, using tensorflow.python.grappler.tf_optimizer or  tf.config.optimizer.get_experimental_options?\r\n\r\nWhen I contacted to Tatiana Shpeisman(shpeisman@google.com), she encouraged to post this request here.\r\n\r\nThanks\r\n\r\nTaehee\r\n", "comments": ["@TaeheeJeong \r\n\r\nCould you please refer this [document](https://www.tensorflow.org/guide/graph_optimization) and [TFGraphOptimizationsStanford](http://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf),hope it helps.\r\n\r\nPlease upgrade to tf2.x ,as there is no active support for tf1.x. Thanks", "Hello Usharani\n\nI already reviewed the linked files.\n\nHowever, it is not clear for me how to simplify the frozen graph.\n\nBecause of that, I posted the request.\n\nCould you provide end-to-end examples on how to simplify the frozen graph?\n\nThanks\n\nRegards,\n\nTaehee\n\nOn Wed, Jun 30, 2021 at 3:05 PM UsharaniPagadala ***@***.***>\nwrote:\n\n> @TaeheeJeong <https://github.com/TaeheeJeong>\n>\n> Could you please refer this document\n> <https://www.tensorflow.org/guide/graph_optimization> and\n> TFGraphOptimizationsStanford\n> <http://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf>,hope\n> it helps.\n>\n> Please upgrade to tf2.x ,as there is no active support for tf1.x. Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/50546#issuecomment-871757204>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEFAWIWRLRDK5IK26STOFYLTVOIJJANCNFSM47S3N4PA>\n> .\n>\n", "@TaeheeJeong \r\n\r\nCould you please refer this [link1](https://towardsdatascience.com/freezing-a-keras-model-c2e26cb84a38), [link2](https://leimao.github.io/blog/Save-Load-Inference-From-TF2-Frozen-Graph/) hope it helps.Thanks\r\n\r\nLooks like similar [#27614](https://github.com/tensorflow/tensorflow/issues/27614)", "Hello Usharani\n\nThe blog seems interesting.\n\nI will check it in detail.\n\nThanks\n\nTaehee\n\n\n\nOn Wed, Jun 30, 2021 at 3:26 PM UsharaniPagadala ***@***.***>\nwrote:\n\n> @TaeheeJeong <https://github.com/TaeheeJeong>\n>\n> Could you please refer this blog,\n> <https://towardsdatascience.com/freezing-a-keras-model-c2e26cb84a38> hope\n> it helps.Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/50546#issuecomment-871766983>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEFAWIXFV4D7A6CTK6K5I73TVOKYXANCNFSM47S3N4PA>\n> .\n>\n", "@TaeheeJeong \r\n\r\nThank you for your update, glad it is useful for you, kindly move this issue to closed status as it is resolved.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50545, "title": "Reappeared bug: TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.", "body": "The bug https://github.com/tensorflow/tensorflow/issues/42596 which was fixed in Tensorflow 2.4 seems to have reappeared in Tensorflow 2.5 a different way.\r\nI already commented in that bug, but as it is closed and I'm not able to reopen it and the cause seems to be a bit different I opened this new bug report.\r\nSo when running my code on TF 2.4 everything works fine including the ipynb example from https://www.tensorflow.org/tutorials/keras/classification but when I run it with TF 2.5 I get this error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-22-16231aa6b55c> in <module>\r\n----> 1 model = tf.keras.Sequential([\r\n      2     tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n      3     tf.keras.layers.Dense(128, activation='relu'),\r\n      4     tf.keras.layers.Dense(10)\r\n      5 ])\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    520     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    521     try:\r\n--> 522       result = method(self, *args, **kwargs)\r\n    523     finally:\r\n    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\r\n    112     \"\"\"\r\n    113     # Skip the init in FunctionalModel since model doesn't have input/output yet\r\n--> 114     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n    115         name=name, autocast=False)\r\n    116     base_layer.keras_api_gauge.get_cell('Sequential').set(True)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    520     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    521     try:\r\n--> 522       result = method(self, *args, **kwargs)\r\n    523     finally:\r\n    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n    316     self._steps_per_execution = None\r\n    317 \r\n--> 318     self._init_batch_counters()\r\n    319     self._base_model_initialized = True\r\n    320 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    520     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    521     try:\r\n--> 522       result = method(self, *args, **kwargs)\r\n    523     finally:\r\n    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)\r\n    324     # `evaluate`, and `predict`.\r\n    325     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\r\n--> 326     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    327     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    328     self._predict_counter = variables.Variable(\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    260       return cls._variable_v1_call(*args, **kwargs)\r\n    261     elif cls is Variable:\r\n--> 262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n    242     if aggregation is None:\r\n    243       aggregation = VariableAggregation.NONE\r\n--> 244     return previous_getter(\r\n    245         initial_value=initial_value,\r\n    246         trainable=trainable,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)\r\n    235                         shape=None):\r\n    236     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    239       previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n   2660   shape = kwargs.get(\"shape\", None)\r\n   2661 \r\n-> 2662   return resource_variable_ops.ResourceVariable(\r\n   2663       initial_value=initial_value,\r\n   2664       trainable=trainable,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    265 \r\n    266 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1582       self._init_from_proto(variable_def, import_scope=import_scope)\r\n   1583     else:\r\n-> 1584       self._init_from_args(\r\n   1585           initial_value=initial_value,\r\n   1586           trainable=trainable,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1736           else:\r\n   1737             shape = initial_value.shape\r\n-> 1738           handle = eager_safe_variable_handle(\r\n   1739               initial_value=initial_value,\r\n   1740               shape=shape,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in eager_safe_variable_handle(initial_value, shape, shared_name, name, graph_mode)\r\n    235   \"\"\"\r\n    236   dtype = initial_value.dtype.base_dtype\r\n--> 237   return _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name,\r\n    238                                                graph_mode, initial_value)\r\n    239 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, initial_value)\r\n    175     handle_data.is_set = True\r\n    176     handle_data.shape_and_type.append(\r\n--> 177         cpp_shape_inference_pb2.CppShapeInferenceResult.HandleShapeAndType(\r\n    178             shape=shape.as_proto(), dtype=dtype.as_datatype_enum))\r\n    179 \r\n\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.\r\n```", "comments": ["@Sur3 ,\r\n\r\nI was able to execute the code in tf v2.5 without any issues.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4c8ea2462baf883bc635544e24aef422/classification.ipynb).Also could you please create a virtual environment and test your code again. It helps. Thanks!", "I'm able to reproduce this. I suspect it came from this commit https://github.com/tensorflow/tensorflow/commit/f599aed5d8d73d45991f74ba4c62db92bf716eba, as when I reorder the lines\r\n\r\n```\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n```\r\n\r\nto\r\n\r\n\r\n```\r\nfrom tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\nfrom tensorflow.python.eager import context\r\n```\r\n\r\nthe problem goes away.\r\n\r\nWorking on testing this in a docker container for reproducibility.\r\n", "What do you mean with creating a virtual environment and how would that help?\r\nI'm using Gentoo Linux; when tensorflow-2.5 is installed the code doesn't work, but when I downgrade that package to tensorflow-2.4 it works. Therefore there must be something wrong with the tensorflow-2.5 code.", "@tilakrayal \r\n\r\nNeeds this patch to compile with a local protobuf https://github.com/tensorflow/tensorflow/commit/95abf88e4c117f8445308c3174cc42795a6694e6 which I very hackily applied in the Dockerfile below.\r\n\r\n```\r\nFROM ubuntu:20.04\r\n\r\nARG bazel_version=3.7.2\r\nARG protobuf_version=3.12.4\r\nARG tensorflow_commit=v2.5.0\r\nARG compile_jobs=32\r\n\r\nWORKDIR /tensorflow-test\r\n\r\n# Install build dependencies\r\nRUN apt-get update && \\\r\n        DEBIAN_FRONTEND=noninteractive apt-get install -y \\\r\n                build-essential \\\r\n                autoconf \\\r\n                libtool-bin \\\r\n                zlib1g-dev \\\r\n                unzip \\\r\n                wget \\\r\n                git \\\r\n                openjdk-8-jdk-headless \\\r\n                python-is-python3 \\\r\n                python3-all-dev \\\r\n                python3-setuptools \\\r\n                python3-pip \\\r\n                python3-six \\\r\n                python3-numpy && \\\r\n        rm -rf /var/lib/apt/lists/*\r\n\r\n# Install Bazel\r\nRUN wget https://storage.googleapis.com/bazel-apt/pool/jdk1.8/b/bazel-${bazel_version}/bazel-${bazel_version}_${bazel_version}_amd64.deb && \\\r\n        dpkg -i bazel-*.deb\r\n\r\n# Compile and install protobuf\r\nRUN wget -O protobuf-${protobuf_version}.tar.gz \\\r\n                https://github.com/protocolbuffers/protobuf/archive/v${protobuf_version}.tar.gz && \\\r\n        tar xvf protobuf-${protobuf_version}.tar.gz && \\\r\n        cd protobuf-${protobuf_version} && \\\r\n        ./autogen.sh && \\\r\n        ./configure --prefix=/usr && \\\r\n        make -j${compile_jobs} install && \\\r\n        ldconfig && \\\r\n        cd python && \\\r\n        python3 setup.py install --cpp_implementation\r\n\r\n# Install tensorflow python dependencies\r\nRUN pip3 install \\\r\n                absl-py \\\r\n                astunparse \\\r\n                flatbuffers \\\r\n                gast \\\r\n                google-pasta \\\r\n                keras-preprocessing \\\r\n                opt-einsum \\\r\n                wrapt \\\r\n                termcolor\r\n\r\n# Compile and install tensorflow\r\nRUN git clone https://github.com/tensorflow/tensorflow && \\\r\n        cd tensorflow && \\\r\n        git checkout ${tensorflow_commit} && \\\r\n        echo \"build --action_env PYTHON_BIN_PATH=\\\"/usr/bin/python3.8\\\"\" >> .tf_configure.bazelrc && \\\r\n        echo \"build --action_env PYTHON_LIB_PATH=\\\"/usr/lib/python3/dist-packages\\\"\" >> .tf_configure.bazelrc && \\\r\n        echo \"build --python_path=\\\"/usr/bin/python3.8\\\"\" >> .tf_configure.bazelrc && \\\r\n        echo \"build --action_env TF_SYSTEM_LIBS=\\\"com_google_protobuf\\\"\" >> .tf_configure.bazelrc && \\\r\n        sed -i -e '/\\/\\/third_party\\/systemlibs:protobuf.bzl/a\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \"//third_party/systemlibs:protobuf_deps.bzl\": \"protobuf_deps.bzl\",' tensorflow/workspace2.bzl && \\\r\n        echo \"def protobuf_deps():\" > third_party/systemlibs/protobuf_deps.bzl && \\\r\n        echo \"    pass\" >> third_party/systemlibs/protobuf_deps.bzl && \\\r\n        bazel-${bazel_version} build --jobs=${compile_jobs} //tensorflow/tools/pip_package:build_pip_package && \\\r\n        ./bazel-bin/tensorflow/tools/pip_package/build_pip_package .. && \\\r\n        pip3 install --no-deps ../tensorflow-*.whl\r\n\r\n\r\n```\r\n\r\nYou can then observe the error by running\r\n`python3 -c \"import tensorflow; tensorflow.keras.models.Sequential()\"`\r\n\r\nSwap the two lines in `tensorflow/python/__init__.py` as described in https://github.com/tensorflow/tensorflow/issues/50545#issuecomment-872229474 and the error does not occur.", "@Saduf2019  ,\r\nI was able to execute the code without any isssues in tf [v2.4](https://colab.research.google.com/gist/tilakrayal/41207bef2b66eb3f236089521bdbb8c7/2-4-50545.ipynb) and [v2.5](https://colab.research.google.com/gist/tilakrayal/4c8ea2462baf883bc635544e24aef422/classification.ipynb).Please find the gist here.", "Tagging @jzhoulon who re-introduced the bug, and @annarev who fixed this last time. It looks like there was some discussion about this reordering in PR #43610.", "@Sur3 \r\nIs this still an issue.", "@Saduf2019 Yes it is.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50545\">No</a>\n", "> I'm able to reproduce this. I suspect it came from this commit [f599aed](https://github.com/tensorflow/tensorflow/commit/f599aed5d8d73d45991f74ba4c62db92bf716eba), as when I reorder the lines\r\n> \r\n> ```\r\n> from tensorflow.python.eager import context\r\n> from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n> ```\r\n> \r\n> to\r\n> \r\n> ```\r\n> from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n> from tensorflow.python.eager import context\r\n> ```\r\n> \r\n> the problem goes away.\r\n> \r\n> Working on testing this in a docker container for reproducibility.\r\n\r\nI got the same issue as reported on TF 2.6.0 on my Apple M1 when trying executing the code \r\n\r\n`from tensorflow.keras import layers`\r\n\r\n`text_vectorizer = layers.TextVectorization(\r\n    max_tokens=max_vocab_length, # how many different words\r\n    standardize='lower_and_strip_punctuation',\r\n    split='whitespace',\r\n    ngrams=None,\r\n    output_mode='int',\r\n    output_sequence_length=max_length, # none means = padding each sequence to the longest sequence\r\n)`\r\n\r\n`text_vectorizer.adapt(X_train)`\r\n\r\nIndeed switching the two imports in my _tensorflow/python/__init__.py_ as mentioned above solves the issue. \r\nFor conda users: Find that __init__.py file in your virtual environment directory under _/Users/username/miniforge3/envs/venv_nlp/lib/python3.9/site-packages/tensorflow/python/_.\r\n\r\nThanks for the solution."]}, {"number": 50544, "title": "Update setup.py", "body": "Update Keras package version to 2.6.0rc1, which pins the protobuf version back to 3.9.2. It is aligned with the protobuf version used by TF in the workspace.", "comments": ["Closing this since there is already one created in https://github.com/tensorflow/tensorflow/pull/50543"]}, {"number": 50543, "title": "Update setup.py after estimator release", "body": "", "comments": []}, {"number": 50542, "title": "tf.data Service Dispatcher/ Worker behaviour", "body": "## Tensorflow Version\r\nTF v2.5.0\r\n\r\n\r\n## URL(s) with the issue:\r\n\r\n1. https://github.com/tensorflow/community/blob/master/rfcs/20200113-tf-data-service.md\r\n2. https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/distribute\r\n3. https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/from_dataset_id \r\n\r\n## Description\r\n\r\nHello, I'm trying to use tf.data.experimental.service to offload pre-processing into Compute Servers alongside my training jobs. I am also using [Horovod](https://github.com/horovod/horovod) as my distributed ML framework, so I will be referring to `worker_index` as `rank`.\r\n\r\nI'm a little confused with the documentation. I understand that this was originally a contribution from the community so document (1) might not be up-to-date to the implementation supported by Tensorflow.\r\n\r\nMy understanding is as follows:\r\n1. Datasets are registered into a DispatcherServer\r\n2. A job is created by a training process to consume the dataset\r\n3. The dataset is distributed across the Workers based on the distribution strategy\r\n\r\n\r\nMy questions follow:\r\n\r\n1. Are jobs autonomously run on the Workers i.e. batches are autonomously computed, and stored by the Worker in-memory. Or are they only computed per request from dataset.apply(distribute()), which is called per training step/iteration?\r\n\r\n2. When using `register_dataset()` and `from_dataset_id()` instead of `distribute()`, from my understanding we can use the dataset across multiple training processes from the same training job. In this case, one process would be in charge of registering the dataset and the others could simply retrieve elements using from_dataset_id(). In this way each process receives a partition of the dataset through one full sweep. Why is the parameter `processing_mode` in from_dataset_id() instead of register_dataset() in this case? Does this mean that each training process will process the dataset independently, and hence receive all of the dataset?\r\n\r\n3. What is the difference between the `job_name` that links datasets to actual jobs in tf.data service and using `register_dataset()` and `from_dataset_id()`. Implementation wise, what's the difference between these two options in pseudocode:\r\n\r\n### Option 1\r\n``` python\r\n# Input pipeline of ALL training processes\r\ndataset = dataset.map(foo)\r\n                 .batch(mini_batch_size)\r\n                 .apply(tf.data.experimental.service.distribute(\r\n                        service=dispatcher_address,\r\n                        job_name=\"job1\"\r\n                  ))\r\n                 .prefetch()\r\nmodel.fit(dataset)\r\n```\r\n### Option 2\r\n``` python\r\n# Input pipeline of training process with index 0\r\ndataset = dataset.map(foo)\r\n                 .batch(mini_batch_size)\r\ndataset_id = dataset.register_dataset(\r\n                                        service=dispatcher_address\r\n                                        dataset=dataset\r\n                                   )\r\ndataset = tf.data.experimental.service.from_dataset_id(\r\n                           processing_mode=\"parallel_epochs\",\r\n                           service=dispatcher_string,\r\n                           dataset_id=dataset_id,\r\n                           element_spec=dataset.element_spec)\r\ndataset = dataset.prefetch()\r\nmodel.fit(dataset)\r\n                           \r\n# Input pipeline of training process with index 1 to n\r\n# Assume we get dataset_id and element_spec from somewhere\r\ndataset = tf.data.experimental.service.from_dataset_id(\r\n                           processing_mode=\"parallel_epochs\",\r\n                           service=dispatcher_string,\r\n                           dataset_id=dataset_id,\r\n                           element_spec=element_spec)\r\ndataset = dataset.prefetch()\r\nmodel.fit(dataset)\r\n```\r\n\r\n4. What's the path for a data element once it is produced? Worker -> Dispatcher -> training process? Worker -> training process? I am assuming the latter\r\n\r\n5. From document (2), does this imply the memory used on the training process side or the Worker side?\r\n> **max_outstanding_requests**\r\n(Optional.) A limit on how many elements may be requested at the same time. You can use this option to control the amount of memory used, since distribute won't use more than element_size * max_outstanding_requests of memory.\r\n\r\n\r\n6. Is there a way to share pre-processing across simultaneous independent training jobs with tf.data service ?\r\n\r\n", "comments": ["Hi @TerenceHernandez, these are great questions! I have added my responses inline.\r\n\r\n> My understanding is as follows:\r\n> \r\n> 1. Datasets are registered into a DispatcherServer\r\n> 2. A job is created by a training process to consume the dataset\r\n> 3. The dataset is distributed across the Workers based on the distribution strategy\r\n\r\nI would just tweak the above to say the dataset is distributed across the Workers based on the **processing mode**\r\n\r\n> 1. Are jobs autonomously run on the Workers i.e. batches are autonomously computed, and stored by the Worker in-memory. Or are they only computed per request from dataset.apply(distribute()), which is called per training step/iteration?\r\n\r\nWorkers buffer a number of batches asynchronously so that the data is ready as soon as it is requested.\r\n\r\n> 2. When using `register_dataset()` and `from_dataset_id()` instead of `distribute()`, from my understanding we can use the dataset across multiple training processes from the same training job. In this case, one process would be in charge of registering the dataset and the others could simply retrieve elements using from_dataset_id(). In this way each process receives a partition of the dataset through one full sweep. Why is the parameter `processing_mode` in from_dataset_id() instead of register_dataset() in this case? Does this mean that each training process will process the dataset independently, and hence receive all of the dataset?\r\n\r\nThe `register_dataset`/`from_dataset_id` lets you create a dataset once, register it, then consume from the dataset with multiple processes. It is functionally equivalent to constructing the same dataset in every process and then calling `apply(distribute(...))` on that dataset. Using `register_dataset`/`from_dataset_id` makes it more explicit that all the processes are reading from the same dataset.\r\n\r\nThe data that each worker receives is controlled by `job_name` and `processing_mode`. The data produced by a job is determined by the processing_mode. Processes which use the same `job_name` will consume from the same job. To partition a dataset across the training processes, you could set `processing_mode=\"distributed_epoch\"` and set the same `job_name` for all training processes.\r\n\r\n> 3. What is the difference between the `job_name` that links datasets to actual jobs in tf.data service and using `register_dataset()` and `from_dataset_id()`. Implementation wise, what's the difference between these two options in pseudocode:\r\n\r\nThe second option doesn't set a `job_name`, so each process will iterate through the dataset independently. The options would be equivalent if you passed `job_name=\"job1\"` to the calls to `from_dataset_id`.\r\n\r\n> 4. What's the path for a data element once it is produced? Worker -> Dispatcher -> training process? Worker -> training process? I am assuming the latter\r\n\r\nYes, the data goes directly from Worker to training process. Otherwise the dispatcher quickly becomes a bottleneck.\r\n\r\n> 5. From document (2), does this imply the memory used on the training process side or the Worker side?\r\n> \r\n> > **max_outstanding_requests**\r\n> > (Optional.) A limit on how many elements may be requested at the same time. You can use this option to control the amount of memory used, since distribute won't use more than element_size * max_outstanding_requests of memory.\r\n\r\nThis is memory on the training process side.\r\n\r\n> 6. Is there a way to share pre-processing across simultaneous independent training jobs with tf.data service ?\r\n\r\nNot currently, but this is a direction we are exploring!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50542\">No</a>\n"]}, {"number": 50541, "title": "/libmetal_plugin.dylib: Symbol not found: _TF_AssignUpdateVariable", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Big Sur OS X 11.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- Binary\r\n- TensorFlow version (use command below):\r\n-  python -m pip install tensorflow-metal\r\n- Tensorflow Metal - v2.5\r\n- Python version:\r\n- 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nAMD Radeon Pro 5700 XT\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nipython\r\nPython 3.8.5 (default, Sep  4 2020, 02:22:02) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.24.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nIn [2]: quit()\r\n(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 notebooks % otool -hV /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib\r\n/Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib:\r\nMach header\r\n      magic  cputype cpusubtype  caps    filetype ncmds sizeofcmds      flags\r\nMH_MAGIC_64   X86_64        ALL  0x00       DYLIB    22       3160   NOUNDEFS DYLDLINK TWOLEVEL WEAK_DEFINES BINDS_TO_WEAK NO_REEXPORTED_DYLIBS MH_HAS_TLV_DESCRIPTORS\r\n\r\n nm -n  /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib | grep TF_AssignUpdate\r\n                 U _TF_AssignUpdateVariable\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@dbl001 \r\n\r\n Could you please elaborate your issue with complete details. And what are the steps to get the above error.\r\n\r\nAlso refer the similar issues  [#45645](https://github.com/tensorflow/tensorflow/issues/45645), [#42482](https://github.com/tensorflow/tensorflow/issues/42482).Thanks\r\n\r\n", "I am trying to run Tensorflow 2.5 on an iMac 27\" running Big Sur with an AMD Radeon Pro 5700 XT GPU.\r\nI followed these installation instructions:\r\n\r\nhttps://developer.apple.com/metal/tensorflow-plugin/\r\n\r\nThe error occurs when I try to import tensorflow from ipython in my tensorflow-metal virtual environment.\r\n\r\n```\r\n(base) davidlaxer@x86_64-apple-darwin13 ~ % source ~/tensorflow-metal/bin/activate\r\n(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 ~ % conda list tensorflow\r\n# packages in environment at /Users/davidlaxer/anaconda3:\r\n#\r\n# Name                    Version                   Build  Channel\r\nmesh-tensorflow           0.1.19                   pypi_0    pypi\r\ntensorflow                2.5.0                    pypi_0    pypi\r\ntensorflow-datasets       4.3.0                    pypi_0    pypi\r\ntensorflow-estimator      2.5.0                    pypi_0    pypi\r\ntensorflow-hub            0.12.0                   pypi_0    pypi\r\ntensorflow-metadata       1.0.0                    pypi_0    pypi\r\ntensorflow-text           2.5.0rc0                 pypi_0    pypi\r\n(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 ~ % ipython\r\nPython 3.8.5 (default, Sep  4 2020, 02:22:02) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.24.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\n~/tensorflow-metal/lib/python3.8/site-packages/tensorflow/__init__.py in <module>\r\n    447     _plugin_dir = _os.path.join(_s, 'tensorflow-plugins')\r\n    448     if _os.path.exists(_plugin_dir):\r\n--> 449       _ll.load_library(_plugin_dir)\r\n    450       # Load Pluggable Device Library\r\n    451       _ll.load_pluggable_device_library(_plugin_dir)\r\n\r\n~/tensorflow-metal/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)\r\n    152 \r\n    153     for lib in kernel_libraries:\r\n--> 154       py_tf.TF_LoadLibrary(lib)\r\n    155 \r\n    156   else:\r\n\r\nNotFoundError: dlopen(/Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: _TF_AssignUpdateVariable\r\n  Referenced from: /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib\r\n  Expected in: flat namespace\r\n\r\n\r\n```\r\nI am trying to get tensorflow to use the AMD GPU.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50541\">No</a>\n", "@dbl001 I have noticed couple of issues related to metal plugin. Unfortunately, we don't have any control. Please post them in [Apple Developer Forum](https://developer.apple.com/forums/tags/tensorflow-metal). Thanks!\r\n", "There are many issues in building tensor flow (e.g.compiler as well as building wheels, missing components (e.g. tensorflow-text), tensor flow-profile doesn\u2019t work, training models runs into memory issues causing the kernel stops the process (e.g. ru_maxss), I\u2019ve posted on GitHub, StackOverflow as well as Apple Developer Forum, but  haven\u2019t gotten any resolutions.\n\n> On Jul 30, 2021, at 10:42 AM, Vishnuvardhan Janapati ***@***.***> wrote:\n> \n> \n> @dbl001 <https://github.com/dbl001> I have noticed couple of issues related to metal plugin. Unfortunately, we don't have any control. Please post them in Apple Developer Forum <https://developer.apple.com/forums/tags/tensorflow-metal>. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/50541#issuecomment-890051213>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAXWFWYDFWRFNTFVWPD5LZLT2LP7RANCNFSM47SQFXNA>.\n> \n\n", "@dbl001 Sorry to hear that but unfortunately we don't have any control on the plugin and don't have access to that hardware.  As of now there is no support but that can change in future. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50541\">No</a>\n"]}, {"number": 50538, "title": "QNNPACK for TensorFlow Lite", "body": "I am looking for an alternative for QNNPACK to execute quantized DNN effectively on CPU, ARM-based devices. In other words, what is the recommended approach to effectively interpret quantized NN?\r\n\r\nPlease can I have some examples?\r\n\r\nThank you.", "comments": ["@onyx22574 \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow.](https://stackoverflow.com/questions/tagged/tensorflow) or in TF [Forum](https://discuss.tensorflow.org/) There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. \r\nPlease refer this [documentation](https://www.tensorflow.org/lite/guide/hosted_models) hope it helps.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50537, "title": "[MLIR][DISC] convert memref::alloc and memref::dealloc to ral.dispatch according to address space", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\n\r\nThere will be 3 PRs to process memory allocation for dynamic shape. 1) Mark shape calculation Ops by Op attribute and insert D2H and H2D Op in mhlo dialect; 2) Add device type info to memref's address space in lmhlo dialect; 3) Convert for device alloc/dealloc with memref's address space.\r\n\r\nThis PR mainly is to convert memref::alloc and memref::dealloc to ral.dispatch. \r\nThis PR depends on [PR1](https://github.com/tensorflow/tensorflow/pull/50245/files), [PR2](https://github.com/tensorflow/tensorflow/pull/50365), [PR3](https://github.com/tensorflow/tensorflow/pull/50434/files)(legalize disc to llvm)\r\n\r\nRelated discussion is [here](https://llvm.discourse.group/t/placement-representation-in-tensor-world-buffer-world/3502/6).", "comments": ["@azazhu  Can you please check @joker-eph's comments and keep us posted ? Thanks!", "> @azazhu Can you please check @joker-eph's comments and keep us posted ? Thanks!\r\n\r\nSure, Actually I replyed one comment:).  There are 3 PRs about processing memory allocation for dynamic shape. I am now working on refine first one(This is last one). ", "> Can you add tests?\r\n\r\nDone", "I see this crash right now on a memref with a \"gpu\" stringattr memory space (running the test `disc_ral_legalize_to_llvm.mlir`):\r\n```\r\nFATAL: logging.cc:107 assert.h assertion failed at mlir/lib/IR/BuiltinTypes.cpp:629 in unsigned int mlir::detail::getMemorySpaceAsInt(mlir::Attribute): memorySpace.isa<IntegerAttr>() && \"Using `getMemorySpaceInteger` with non-Integer attribute\"\r\n*** Check failure stack trace: ***\r\n    @     0x55794cdecbec  absl::logging_internal::LogMessage::PrepareToDie()\r\n    @     0x55794cdec7e6  absl::logging_internal::LogMessage::SendToLog()\r\n    @     0x55794cdebede  absl::logging_internal::LogMessage::Flush()\r\n    @     0x55794cded049  absl::logging_internal::LogMessageFatal::~LogMessageFatal()\r\n    @     0x55794cdea894  __assert_fail\r\n    @     0x55794cbf4e6c  mlir::detail::getMemorySpaceAsInt()\r\n    @     0x55794cbf4b45  mlir::MemRefType::getMemorySpaceAsInt()\r\n    @     0x55794bca7079  mlir::ConvertToLLVMPattern::getMemRefDescriptorSizes()\r\n    @     0x55794a35f97a  mlir::disc_ral::(anonymous namespace)::ConvertMemRefAllocOpToDispatchOpPattern::matchAndRewrite()\r\n```", "@azazhu  Can you please check @joker-eph's comments and keep us posted ? Thanks!\r\n", "> @azazhu Can you please check @joker-eph's comments and keep us posted ? Thanks!\r\n\r\nSorry for late response, We are working on internal porting(to make sure final porting can e2e run) based on tot TF, which takes more time than we expected. We will continue updating this PR(and rest PR) once internal verification finish."]}, {"number": 50536, "title": "Grammatical Mistakes in the documentation of tf.keras.Model.load_weights", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights\r\n\r\n## Description of issue (what needs changing): \r\n\r\nThe documentation is a bit confusing with the Terms like **`False Weights`**, **`should be the same as when`**, etc.. It can be refined.", "comments": ["Created a CL to fix the mistakes.", "Hi, Is this issue still active? Is anyone working on it? ", "@worldpeaceaspirer \r\nThis issue has been addressed in CL, hence moving this to closed status."]}, {"number": 50535, "title": "OSError: SavedModel file does not exist at: error", "body": "Hello Please Help,\r\n\r\nI am not much of an expert on tensorflow. I have followed the AI GUYS tutorial and created custom my yolov4 weights. I cannot convert it to tensorflow. Please help.\r\n\r\n(yolov4-gpu) C:\\Users\\LENOVO-L3\\yolov4-custom-functions>python detect.py --weights ./checkpoints/custom-416 --size 416 --model yolov4 --images ./data/images/car2.jpg --plate\r\nTraceback (most recent call last):\r\nFile \"detect.py\", line 146, in\r\napp.run(main)\r\nFile \"C:\\Users\\LENOVO-L3\\anaconda3\\envs\\yolov4-gpu\\lib\\site-packages\\absl\\app.py\", line 312, in run\r\n_run_main(main, args)\r\nFile \"C:\\Users\\LENOVO-L3\\anaconda3\\envs\\yolov4-gpu\\lib\\site-packages\\absl\\app.py\", line 258, in _run_main\r\nsys.exit(main(argv))\r\nFile \"detect.py\", line 49, in main\r\nsaved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])\r\nFile \"C:\\Users\\LENOVO-L3\\anaconda3\\envs\\yolov4-gpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 590, in load\r\nreturn load_internal(export_dir, tags, options)\r\nFile \"C:\\Users\\LENOVO-L3\\anaconda3\\envs\\yolov4-gpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 601, in load_internal\r\nloader_impl.parse_saved_model_with_debug_info(export_dir))\r\nFile \"C:\\Users\\LENOVO-L3\\anaconda3\\envs\\yolov4-gpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 56, in parse_saved_model_with_debug_info\r\nsaved_model = _parse_saved_model(export_dir)\r\nFile \"C:\\Users\\LENOVO-L3\\anaconda3\\envs\\yolov4-gpu\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 113, in parse_saved_model\r\nconstants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: ./checkpoints/custom-416/{saved_model.pbtxt|saved_model.pb}\r\n\r\nI have run the below command before running the above. But the weight files did not converted as expected I think.\r\npython save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4\r\nAny help is much appreciated.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50535\">No</a>\n"]}, {"number": 50534, "title": "How to apply a hierarchical mask in Tensorflow2.0 (tf.keras)?", "body": "I am trying to build a hierarchical sequence model for time series classification (refer to the paper: hierarchical attention networks for document classification). But I'm very confused about how to mask the hierarchical sequences.\r\n\r\nMy data is a hierarchical time series. Specifically, each sample is composed of multiple sub-sequences and each sub-sequence is a multiple multivariate time series (just like word--> sentence -->document in NLP). So I need to pad and mask it twice. This is critical as a document will often not have the same number of sentences (or all sentences the same number of words). Finally, I get data as follows:\r\n\r\n\r\n\r\n```\r\narray([[[[0.21799476, 0.26063576],\r\n         [0.2170655 , 0.53772384],\r\n         [0.18505535, 0.30702454],\r\n         [0.22714901, 0.17020395],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ]],\r\n\r\n        [[0.2160176 , 0.23789616],\r\n         [0.2675753 , 0.21807681],\r\n         [0.26932836, 0.21914595],\r\n         [0.26932836, 0.21914595],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ]]],\r\n\r\n       [[[0.03941338, 0.3380829 ],\r\n         [0.04766269, 0.3031088 ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ]],\r\n\r\n        [[0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ],\r\n         [0.        , 0.        ]]]], dtype=float32)\r\n```\r\nThen I build a hierarchical model as follows:\r\n```\r\ninputs = Input(shape=(maxlen_event, maxlen_seq, 2))\r\nx = TimeDistributed(\r\n        Sequential([\r\n            Masking(),\r\n            LSTM(units=8, return_sequences=False)\r\n        ])\r\n    )(inputs)\r\nx = LSTM(units=32, return_sequences=False)(x)\r\nx = Dense(16, activation='relu')(x)\r\noutput = Dense(16, activation='sigmoid')(x)\r\n```\r\n\r\nAs my data is padded in on both dimensions, I don't know how to mask it correctly. I have two questions about it:\r\nQ1: In TimeDistributed, do I use the masking layer correctly to mask the first padding?\r\nQ2: How to mask the second padding?\r\n\r\nThank you.", "comments": ["@Wwwwei \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Tensorflow [Forum.](https://discuss.tensorflow.org/) There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Please move this to closed status.\r\nThanks\r\n", "> @Wwwwei\r\n> \r\n> This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Tensorflow [Forum.](https://discuss.tensorflow.org/) There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Please move this to closed status.\r\n> Thanks\r\n\r\nThanks for your advice. I have closed the issue."]}, {"number": 50533, "title": "shared_embedding_columns are not supported when eager execution is enabled?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwhen i use tf.feature_column.shared_embedding(...)\uff0ccode raises the RuntimeValue about \"shared_embedding_columns are not supported when eager execution is enabled.\"\r\n\r\nThen i see the feature_column_v2.py\uff0cfind \r\n```\r\n  if context.executing_eagerly():\r\n    raise RuntimeError('shared_embedding_columns are not supported when eager '\r\n                       'execution is enabled.')\r\n```\r\n\r\nand i also find the tf.feature_column.embedding_column don't check eagerly\u3002\r\n\r\nSo, my question is why they should be different? and can remove eager check?\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Peterisfar ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the tensorflow version,the complete code and dataset to reproduce the issue reported here.Thanks!", "Also please take a look at this issue with the similar error.It helps.[Link](https://github.com/tensorflow/tensorflow/issues/48638).Thanks", "I met he same problem. When i use disable_eager_execution() to avoid this exception, build a keras model with shared_embeddings seems ok but shared_embeddings layer doesn't have any trainable variables, raise error [table not inicialized] (but other feature_column.embedding_column is ok).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50533\">No</a>\n"]}, {"number": 50532, "title": "Change batching in timeseries_dataset_from_array to be optional", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe ``keras.preprocessing.timeseries_dataset_from_array`` function works well for creating timeseries datasets, but when creating the dataset from multiple independent timeseries (e.g. different locations) one would need to merge the datasets.\r\nAs the current implementation requires batching to be used and to my knowledge batched datasets cannot be concatenated, developers cannot use the generating function in that case.\r\nA simple fix would be to allow the developer to provide ``None`` for the batch size and do not perform batching in that case.\r\nThis would be in line with the other parameters.\r\n\r\n**Will this change the current api? How?**\r\nSmall change, the documentation of ``batch_size`` would need to be updated.\r\n\r\n**Who will benefit with this feature?**\r\nUsers wanting to train more complex timeseries problems.\r\n\r\n**Any Other info.**\r\n", "comments": ["@sebimarkgraf \r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "@Sadulf2019 thank you very much! Definitely going to do this.\r\nThen there's no reason to keep this here open."]}, {"number": 50531, "title": "tensorflow v2.0.0 compatibility with scipy ", "body": "hello ,\r\ni'm developing a neural network to train a  data set , i'm getting this error:\r\nraise ImportError('Image transformations require SciPy. '\r\n\r\nImportError: Image transformations require SciPy. Install SciPy.\r\n\r\ni'm using tensorflow v2.0.0 what version of SciPy is compatible with tensorflow 2.0.0 ? what version of Scipy should i install and how ? I'm working with Nvidia Jetson  nano .\r\nplease help \r\n ", "comments": ["@saida11000 \r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abuild%2Finstall&template=10-build-installation-issue.md) has not been filled, could you please do so as it helps us analyse the issue\r\n\r\nCould you please refer the similar issues [#40884](https://github.com/tensorflow/tensorflow/issues/40884) , [#87](https://github.com/apple/tensorflow_macos/issues/87) and also refer the [doc1](https://forums.developer.nvidia.com/t/official-tensorflow-for-jetson-nano/71770),  [doc2](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform-release-notes/tf-jetson-rel.html) ,let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50531\">No</a>\n"]}, {"number": 50530, "title": "tensorflow v2.0.0 compatibility with scipy ", "body": "Please go to Stack Overflow for help and support:\r\n, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@saida11000 \r\n\r\nLooks like this is duplicate of issue #50517. Can you please close this issue, since it is already being tracked there? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50530\">No</a>\n"]}, {"number": 50528, "title": "Fix -Wunreachable-code-aggressive.", "body": "Bug: chromium:1066980", "comments": ["Thanks, PTAL!"]}]