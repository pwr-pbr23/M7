[{"number": 7026, "title": "It seems that tf.scatter_nd only can run on CPU?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": [" Is there any way that I can run tf.scatter_nd on GPU? or is there an equivalent command?", "Some operations should work. Which operations are you looking for?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L348", "I am looking for tf.scatter_nd. when I am looking at log_device_placemnt it is saying that it is running in CPU. I am using tf.device to run it on GPU. The GPU version of this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L339", "Could you try registering the kernel for GPUs and see what happens? I'm not super familiar with that code and I'm not sure that it would work.", "@rmlarsen might have some input.", "I am not sure how i should register it! ", "Even I couldn't find scatter_nd_op.cc file in my tensorflow directory. Can you give me some hint where is should be located? ", "It should just be in core/kernels/scatter_nd_op.cc. You should run the same `TF_CALL` macro for all types as in the CPU version, but instead register the GPU ones, if that makes sense. If you have repro instructions on how to test I could give it a shot on my end.", "@drpngx Unfortunately, my sensorflow directory is different from github files. I installed it by using pip. it is version 0.12 for python 2.7 in ubuntu 14 64bit. Can you guide me why it is different and I couldn't locate scatter_nd_op.cc file. :( Thanks.   ", "For that, you have to install from source. You can't do it from a pip file unfortunately. You should follow this:\r\nhttps://www.tensorflow.org/get_started/os_setup#installing_from_sources", "@drpngx I have installed tf from source. Did I need to installed it from source again if I make changes to scatter_nd_op.cc file? \r\nAlso,  should I add sth like this to that file:\r\n#define REGISTER_SCATTER_ND_GPU(type) REGISTER_SCATTER_ND(type, GPU);\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_ADD_SUB_GPU);\r\n// TODO(simister): Re-enable all types after binary size is under control.\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_UPDATE_GPU);\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_GPU);\r\n\r\nThanks,\r\nAli", "Yes, please try that. Thanks!", "@drpngx I apply those changes to cc files in clone of tensorflow in my machine and then I try to apply changes in cc files by this codes:\r\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.12.1-py2-none-any.whl\r\n\r\nbut, it seems that those changes are not applied. May I ask am i doing it correctly? \r\nThanks", "Yes, that's the right thing. I guess we have to figure out why the kernels are not registered.", "Following up on #7027, which is similar, until build issues are resolved.", "Can someone point out to the GPU implementation for tf.scatter_nd?\r\nSeems to me there is no any. I found only CPU version, I believe this one:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op_cpu_impl.h#L106\r\ncalled from here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L169", "Yes, we think it should be there. Could you try to follow the instruction on #7027 to print `LogAllRegisteredKernels`?", "`LogAllRegisteredKernels` shows only CPU versions of the `ScatterNd` op being registered.  Based on the comment here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L354, I don't think the ScatterND GPU kernel is supposed to be registered, so that is expected. (Uncommenting those lines just results in errors since those kernels don't exist).  But do you know if that is something that is being actively worked on and we can expect `scatter_nd` GPU support in the somewhat near future, or should we just plan on not using that op if we want to run on the GPU?", "@drpngx , just make to sure I understood your response from #7026 which says:\r\n\r\n```\r\nYeah, we were trying to add it to see what happens but somehow the thread died out. \r\nPlease try it out if you have cycles.\r\n```\r\n\r\nDo you suggest that I add the line `TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_CPU);` to `scatter_nd_op.cc`, recompile tensorflow and see what happens? I definitely can do it, although this would involve learning to compile Tensorflow from source first. Is it what you are asking?\r\n\r\n \r\n", "Note: it looks like 8f1cde386cb8295d2d0ddb2a4af0ba6336bea897 added GPU support for `tf.scatter_update/add/sub` (sparse variable updates), but not `tf.scatter_nd`.", "@rizar, as @drasmuss says some operations have been implemented, but not `ScatterNd`. Browsing through the code I don't see an obvious reason why we couldn't add it.", "Actually I just tested with `force_gpu=True`, and it seems to be on the GPU. Can you confirm?", "``` python\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    y = tf.scatter_nd([[1], [2], [3]], [1., 1., 1.], (5,))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(y)\r\n```\r\ngives error\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _device=\"/device:GPU:0\"](ScatterNd/indices, ScatterNd/updates, ScatterNd/shape)]]\r\n```\r\nThis is on the latest `master` code.", "@drpngx , I can not confirm. I tried and without any changes to the code `scatter_nd` is definitely done on cpu.\r\n\r\nI also tried adding the following diff:\r\n\r\n```cpp\r\n--- a/tensorflow/core/kernels/scatter_nd_op.cc\r\n+++ b/tensorflow/core/kernels/scatter_nd_op.cc\r\n@@ -374,8 +374,12 @@ TF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_CPU);\r\n #define REGISTER_SCATTER_ND_UPDATE_GPU(type) \\\r\n   REGISTER_SCATTER_ND_UPDATE(type, GPU);\r\n \r\n+#define REGISTER_SCATTER_ND_GPU(type) \\\r\n+  REGISTER_SCATTER_ND(type, GPU);\r\n+\r\n TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_ADD_SUB_GPU);\r\n TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_UPDATE_GPU);\r\n+TF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_GPU);\r\n \r\n // Forward declarations of the functor specializations for GPU.\r\n namespace functor {\r\n```\r\n\r\nAs a result, `scatter_nd` is placed on the GPU but eventually my test code crushes. My test code is listed below:\r\n\r\n```python\r\nconfig = config_pb2.ConfigProto(log_device_placement=True)\r\n\r\nwith tf.Graph().as_default() as graph, tf.Session(config=config) as sess:\r\n\r\n    indices = tf.placeholder(tf.int32, shape=(2, 2))\r\n\r\n    z = tf.scatter_nd(indices, tf.ones(2), shape=(10, 100))\r\n    s = tf.reduce_sum(z)\r\n\r\n    for i in range(10):\r\n        print(sess.run(s, feed_dict={indices: [(0, 0), (1, 1)]}))\r\n```", "@drpngx Using the latest ```master```, ```scatter_nd``` is placed to the CPU if ```allow_soft_placement = True```. If I force it to execute on the GPU, tf complains there is no GPU implementation. ", "Right, somehow `force_gpu` didn't do what I thought it would. See @rizar and @drasmuss comments above. Further investigation is needed to figure out the cause of the crash.", "Finally got a chance to take a look. You need `.HostMemory(\"shape\")` in the `REGISTER_OP` [call](https://github.com/tensorflow/tensorflow/issues/7026). That's because `MakeShape` runs on the CPU, and crashes. I'm not an expert on this, so there might be further consequences to this. It might be better if I push this internally first.", "Thank you @drpngx ! I think that supporting efficient subtensor operations such as `tf.gather_nd` and `tf.scatter_nd` is very important, especially for working with language data. I am looking forward to more updates from your side, and please let me know if would like me to try something on my setup. ", "It's pushed internally, will be pushed to GitHub tomorrow or Monday likely\n\nOn Feb 23, 2017 12:48 PM, \"Dzmitry Bahdanau\" <notifications@github.com>\nwrote:\n\n> Thank you @drpngx <https://github.com/drpngx> ! I think that supporting\n> efficient subtensor operations such as tf.gather_nd and tf.scatter_nd is\n> very important, especially for working with language data. I am looking\n> forward to more updates from your side, and please let me know if would\n> like me to try something on my setup.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7026#issuecomment-282115985>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbW_o8sVWDc23NFmftHeWe3MrPnh9ks5rffCWgaJpZM4Lrs5N>\n> .\n>\n", "@drpngx The update did not resolve the issue for me. Did anybody else try it out? @rizar @drasmuss ?", "I wrote a test that worked. Maybe it's placing the op on the CPU for different reasons. Are you using integer types perhaps?\r\n\r\n```\r\n  with tf.device(\"/gpu:0\"):\r\n    y = tf.scatter_nd([[1], [2], [3]], [1., 1., 1.], (5,))\r\n\r\n  session_config = tf.ConfigProto(allow_soft_placement=False,\r\n                                  log_device_placement=True)\r\n\r\n  with tf.Session(config=session_config) as sess:\r\n    print(sess.run(y))\r\n```", "@drpngx Thanks for the test code. Unfortunately it does not work on my system. I get the following error:\r\n\r\n`InvalidArgumentError (see above for traceback): Cannot assign a device to node 'ScatterNd': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _device=\"/device:GPU:0\"](ScatterNd/indices, ScatterNd/updates, ScatterNd/shape)]]\r\n`", "Just to be clear, you sync'd past that CL, rebuilt and ran?", "Seems to work for me with on a simple test, haven't tried it in the context of a real training script yet.\r\n\r\nFrom the log of device placement:\r\n\r\n```\r\nScatterNd/shape: (Const): /job:localhost/replica:0/task:0/gpu:0\r\n2017-03-01 13:51:08.598013: I tensorflow/core/common_runtime/simple_placer.cc:841] ScatterNd/shape: (Const)/job:localhost/replica:0/task:0/gpu:0\r\n```\r\n", "@drpngx Ah, there was a problem with my installation! It works just fine for me as well. Sorry about the confusion. I've tested your code as well as my entire network, which uses multiple scatter_nd's. I got a speed-up of ~5x for the latter. Thanks a lot for the patch!  ", "Yay!\n\nOn Mar 2, 2017 1:48 AM, \"Ali Osman Ulusoy\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> Ah, there was a problem with my\n> installation! It works just fine for me as well. Sorry about the confusion.\n> I've tested your code as well as my entire network, which uses multiple\n> scatter_nd's. I got a speed-up of ~5x for the latter. Thanks a lot for the\n> patch!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7026#issuecomment-283606319>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbeMgsUTwX0tGemexDeBt3hueOo38ks5rhpB5gaJpZM4Lrs5N>\n> .\n>\n"]}, {"number": 7025, "title": "Getting \"Dst tensor is not initialized.\" when really the problem is out of GPU memory", "body": "This is the stack trace we sometimes get when trying to use TensorFlow on a GPU that's occupied by another process. It would help debugging if the error said something about memory.\r\n\r\n@zheng-xq \r\n\r\ntf.version: '0.12.1-1934-g27fca7d-dirty'\r\n(nightly from last week)\r\n\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:04:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 381.44MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"memory_test.py\", line 87, in <module>\r\n    profile_densenet(False)\r\n  File \"memory_test.py\", line 65, in profile_densenet\r\n    sess.run(net.initializer, {net.x_init: trainx[:init_batch_size]})\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'zeros_1266', defined at:\r\n  File \"memory_test.py\", line 87, in <module>\r\n    profile_densenet(False)\r\n  File \"memory_test.py\", line 59, in profile_densenet\r\n    net = densenet_lib.densenet(init_batch_size, batch_size, layers_per_block, filters_per_layer, save_memory=save_memory)\r\n  File \"/home/yaroslav/openai.git/densenet/densenet.py\", line 183, in densenet\r\n    optimizer = nn.adamax_updates(all_params, loss, lr=tf_lr)\r\n  File \"/home/yaroslav/openai.git/densenet/nn.py\", line 41, in adamax_updates\r\n    mg = tf.Variable(tf.zeros(int_shape(p)), p.name + '_adamax_mg')\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1376, in zeros\r\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Dst tensor is not initialized.\r\n\t [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n```", "comments": ["Classifying as \"docs\" but this is really an error message / error propagation issue.\r\n\r\n@yaroslavvb do you by any chance have a PR in mind that you could submit?", "I can change the message to read \"Dst tensor is not initialized (possibly caused by Out of Memory)\" unless @zheng-xq has a better idea. PS, this is not a hypothetical problem, we've had people thinking it's a regression in tensorflow/training scripts because of this error", "My feeling is that this should fail at the place of memory allocation, not\nat the point of memory copy. There needs to be some digging to find out why\nit didn't fail earlier.\n\nOn Mon, Jan 23, 2017 at 4:25 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> I can change the message to read \"Dst tensor is not initialized (possibly\n> caused by Out of Memory)\" unless @zheng-xq <https://github.com/zheng-xq>\n> has a better idea\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7025#issuecomment-274662530>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APAgTtwKSbjLp9qQ0r0te9E38jX4uMb2ks5rVUUVgaJpZM4Lrr-s>\n> .\n>\n", "Hi  @yaroslavvb @zheng-xq \r\n\r\nI'm getting this `Dst Tensor Not Initialized` error. \r\n\r\n(See my comment (the last one)  in this issue elsewhere: https://github.com/aymericdamien/TensorFlow-Examples/issues/38)\r\n\r\nI'm reproducing the stack trace here in case it helps diagnose the issue:\r\n\r\n```\r\n\u25b6 python imagenet_inference.py \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 305.92MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \r\nLimit:                   111063040\r\nInUse:                     9337856\r\nMaxInUse:                  9337856\r\nNumAllocs:                      11\r\nMaxAllocSize:              3538944\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\r\nE tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\r\n\t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\nTraceback (most recent call last):\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\r\n    status, run_metadata)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"imagenet_inference.py\", line 19, in <module>\r\n    sess.run(init)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'Variable_10/initial_value', defined at:\r\n  File \"imagenet_inference.py\", line 16, in <module>\r\n    probs = AlexNet(x, feature_extract=False)\r\n  File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\r\n    fc6W = tf.Variable(net_data[\"fc6\"][0])\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\r\n    initial_value, name=\"initial_value\", dtype=dtype)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Dst tensor is not initialized.\r\n\t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n```\r\n\r\nHere's deviceQuery successfully reporting seeing the GPU:\r\n\r\n```\r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME \r\n/usr/local/cuda\r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\r\n\r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery \r\n./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GT 750M\"\r\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\r\n  CUDA Capability Major/Minor version number:    3.0\r\n  Total amount of global memory:                 2048 MBytes (2147024896 bytes)\r\n  ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\r\n  GPU Max Clock rate:                            926 MHz (0.93 GHz)\r\n  Memory Clock rate:                             2508 Mhz\r\n  Memory Bus Width:                              128-bit\r\n  L2 Cache Size:                                 262144 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\r\nResult = PASS\r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 \r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 \r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 \r\n py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest \r\n[CUDA Bandwidth Test] - Starting...\r\nRunning on...\r\n\r\n Device 0: GeForce GT 750M\r\n Quick Mode\r\n\r\n Host to Device Bandwidth, 1 Device(s)\r\n PINNED Memory Transfers\r\n   Transfer Size (Bytes)\tBandwidth(MB/s)\r\n   33554432\t\t\t3633.5\r\n\r\n Device to Host Bandwidth, 1 Device(s)\r\n PINNED Memory Transfers\r\n   Transfer Size (Bytes)\tBandwidth(MB/s)\r\n   33554432\t\t\t6343.5\r\n\r\n Device to Device Bandwidth, 1 Device(s)\r\n PINNED Memory Transfers\r\n   Transfer Size (Bytes)\tBandwidth(MB/s)\r\n   33554432\t\t\t42554.1\r\n\r\nResult = PASS\r\n\r\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\r\n```\r\n\r\nMy System: \r\n```\r\nMacBook Pro (Retina, 15-inch, Late 2013)\r\n2.3 GHz Intel Core i7\r\n16 GB 1600 MHz DDR3\r\nNVIDIA GeForce GT 750M 2048 MB\r\n\r\n---\r\nfrom System Report > Graphics\r\nNVIDIA GeForce GT 750M:\r\n\r\n  Chipset Model:\tNVIDIA GeForce GT 750M\r\n  Type:\tGPU\r\n  Bus:\tPCIe\r\n  PCIe Lane Width:\tx8\r\n  VRAM (Total):\t2048 MB\r\n  Vendor:\tNVIDIA (0x10de)\r\n  Device ID:\t0x0fe9\r\n  Revision ID:\t0x00a2\r\n  ROM Revision:\t3776\r\n  gMux Version:\t4.0.8 [3.2.8]\r\n  Displays:\r\nColor LCD:\r\n  Display Type:\tRetina LCD\r\n  Resolution:\t2880 x 1800 Retina\r\n  Retina:\tYes\r\n  Pixel Depth:\t32-Bit Color (ARGB8888)\r\n  Main Display:\tYes\r\n  Mirror:\tOff\r\n  Online:\tYes\r\n  Built-In:\tYes\r\n```\r\n", "Sounds like you are running out of GPU memory\n\nOn Jan 26, 2017 10:33 AM, \"Atul Acharya\" <notifications@github.com> wrote:\n\n> Hi @yaroslavvb <https://github.com/yaroslavvb> @zheng-xq\n> <https://github.com/zheng-xq>\n>\n> I'm getting this Dst Tensor Not Initialized error.\n>\n> (See my comment (the last one) in this issue elsewhere:\n> aymericdamien/TensorFlow-Examples#38\n> <https://github.com/aymericdamien/TensorFlow-Examples/issues/38>)\n>\n> I'm reproducing the stack trace here in case it helps diagnose the issue:\n>\n> \u25b6 python imagenet_inference.py\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\n> name: GeForce GT 750M\n> major: 3 minor: 0 memoryClockRate (GHz) 0.9255\n> pciBusID 0000:01:00.0\n> Total memory: 2.00GiB\n> Free memory: 305.92MiB\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State:\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\n> I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\n> I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\n> Limit:                   111063040\n> InUse:                     9337856\n> MaxInUse:                  9337856\n> NumAllocs:                      11\n> MaxAllocSize:              3538944\n>\n> W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\n> W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\n> W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\n> E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n> Traceback (most recent call last):\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n>     return fn(*args)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n>     status, run_metadata)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\n>     next(self.gen)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n>     pywrap_tensorflow.TF_GetCode(status))\n> tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n>   File \"imagenet_inference.py\", line 19, in <module>\n>     sess.run(init)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n>     run_metadata_ptr)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n>     feed_dict_string, options, run_metadata)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n>     target_list, options, run_metadata)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n>     raise type(e)(node_def, op, message)\n> tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n>\n> Caused by op 'Variable_10/initial_value', defined at:\n>   File \"imagenet_inference.py\", line 16, in <module>\n>     probs = AlexNet(x, feature_extract=False)\n>   File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\n>     fc6W = tf.Variable(net_data[\"fc6\"][0])\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n>     expected_shape=expected_shape)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n>     initial_value, name=\"initial_value\", dtype=dtype)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n>     return constant(v, dtype=dtype, name=name)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n>     attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n>     self._traceback = _extract_stack()\n>\n> InternalError (see above for traceback): Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n>\n>\n> Here's deviceQuery successfully reporting seeing the GPU:\n>\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME\n> /usr/local/cuda\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\n>\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery\n> ./deviceQuery Starting...\n>\n>  CUDA Device Query (Runtime API) version (CUDART static linking)\n>\n> Detected 1 CUDA Capable device(s)\n>\n> Device 0: \"GeForce GT 750M\"\n>   CUDA Driver Version / Runtime Version          8.0 / 8.0\n>   CUDA Capability Major/Minor version number:    3.0\n>   Total amount of global memory:                 2048 MBytes (2147024896 bytes)\n>   ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n>   GPU Max Clock rate:                            926 MHz (0.93 GHz)\n>   Memory Clock rate:                             2508 Mhz\n>   Memory Bus Width:                              128-bit\n>   L2 Cache Size:                                 262144 bytes\n>   Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n>   Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n>   Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n>   Total amount of constant memory:               65536 bytes\n>   Total amount of shared memory per block:       49152 bytes\n>   Total number of registers available per block: 65536\n>   Warp size:                                     32\n>   Maximum number of threads per multiprocessor:  2048\n>   Maximum number of threads per block:           1024\n>   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n>   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n>   Maximum memory pitch:                          2147483647 bytes\n>   Texture alignment:                             512 bytes\n>   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n>   Run time limit on kernels:                     Yes\n>   Integrated GPU sharing Host Memory:            No\n>   Support host page-locked memory mapping:       Yes\n>   Alignment requirement for Surfaces:            Yes\n>   Device has ECC support:                        Disabled\n>   Device supports Unified Addressing (UVA):      Yes\n>   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n>   Compute Mode:\n>      < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n>\n> deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\n> Result = PASS\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest\n> [CUDA Bandwidth Test] - Starting...\n> Running on...\n>\n>  Device 0: GeForce GT 750M\n>  Quick Mode\n>\n>  Host to Device Bandwidth, 1 Device(s)\n>  PINNED Memory Transfers\n>    Transfer Size (Bytes)\tBandwidth(MB/s)\n>    33554432\t\t\t3633.5\n>\n>  Device to Host Bandwidth, 1 Device(s)\n>  PINNED Memory Transfers\n>    Transfer Size (Bytes)\tBandwidth(MB/s)\n>    33554432\t\t\t6343.5\n>\n>  Device to Device Bandwidth, 1 Device(s)\n>  PINNED Memory Transfers\n>    Transfer Size (Bytes)\tBandwidth(MB/s)\n>    33554432\t\t\t42554.1\n>\n> Result = PASS\n>\n> NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n>\n> My System:\n>\n> MacBook Pro (Retina, 15-inch, Late 2013)\n> 2.3 GHz Intel Core i7\n> 16 GB 1600 MHz DDR3\n> NVIDIA GeForce GT 750M 2048 MB\n>\n> ---\n> from System Report > Graphics\n> NVIDIA GeForce GT 750M:\n>\n>   Chipset Model:\tNVIDIA GeForce GT 750M\n>   Type:\tGPU\n>   Bus:\tPCIe\n>   PCIe Lane Width:\tx8\n>   VRAM (Total):\t2048 MB\n>   Vendor:\tNVIDIA (0x10de)\n>   Device ID:\t0x0fe9\n>   Revision ID:\t0x00a2\n>   ROM Revision:\t3776\n>   gMux Version:\t4.0.8 [3.2.8]\n>   Displays:\n> Color LCD:\n>   Display Type:\tRetina LCD\n>   Resolution:\t2880 x 1800 Retina\n>   Retina:\tYes\n>   Pixel Depth:\t32-Bit Color (ARGB8888)\n>   Main Display:\tYes\n>   Mirror:\tOff\n>   Online:\tYes\n>   Built-In:\tYes\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7025#issuecomment-275470982>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s>\n> .\n>\n", "I get the same error and pretty sure it is for the same reason. I am on tf `1.0.0` and using keras `2.0.2`. ", "For now, I changed the message to `\"Dst tensor is not initialized (are you out of memory?).\"` (not yet on Github).", "@vrv may have localized the problem to https://github.com/tensorflow/tensorflow/blob/a4b352bfddd518b540c30e456f3bc0027ba9351f/tensorflow/core/common_runtime/gpu/gpu_device.cc#L467, which doesn't check for errors and thus produces an uninitialized tensor.\r\n\r\n@yaroslavvb If you can reproduce the situation, would you mind checking if `Tensor::IsInitialized` at that point detects the problem?", "@vrv Has a CL that will hopefully fix this.", "Yeah, verified I have a CL that fixes this, submitting it internally now.", "I just updated TF to 1.6 and still experience the same issue.\r\n\r\nI am running the smallest possible model, Cart-Pole, on GTX1080 8MB. Is it a TensorFlow bug that can be fixed somehow or we are simply trying to fit too big models (overenthusiastic the batch size probably the main reason for that)?\r\n\r\nsebtac", "Still experiencing this issue with recent version 068fd9c936dbf8c9ace9edae9e7bb9e64256d381.\r\n(I can confirm this is due to an OOM issue.)\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: _arg_q_actions_0_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_339__arg_q_actions_0_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: loss/assert_broadcastable/AssertGuard/Assert/Switch/_33 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_91_loss/assert_broadcastable/AssertGuard/Assert/Switch\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "Getting this in TF 1.9.0 when I increase the size of my test set to a large number of samples (possibly a memory issue).", "I am also getting the same error in TF 1.8.0  also in the latest version. My machine is 3x NVidia Tesla P40 22GB\r\n\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n         [[Node: conv4_22_1x1_increase/Conv2D-0-1-TransposeNCHWToNHWC-LayoutOptimizer/_2025 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3008_conv4_22_1x1_increase/Conv2D-0-1-TransposeNCHWToNHWC-LayoutOptimizer\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n         [[Node: adversarial/Mean/_3087 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_5391_adversarial/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n", "If not solved yet... free memory... previously generated not-used embedding, models etc...\r\n\r\ndel all_embs, (model_names...), (model_input_names)\r\nimport gc; gc.collect()\r\ntime.sleep(10)", "Has anyone solved this?", "Make sure multiple processes are not accessing GPU and see if that solves the problem. It seemed to be the solution for me. Especially if you're running your code in Jupyter notebook, kill any other running notebook kernel."]}, {"number": 7024, "title": "Branch 145339960", "body": "", "comments": ["Fixing failing tests.\r\nWill push after."]}, {"number": 7023, "title": "ImportError: No module named regularizers", "body": "Any idea how to fix this for Keras on TensorFlow?\r\n\r\n```\r\n    from regularizers import EigenvalueRegularizer\r\nImportError: No module named regularizers\r\n```\r\n", "comments": ["@fchollet FYI since this is Keras-related.\r\n\r\nCould you point to the code?\r\n", "If this is the Keras regularizers module that this is about, shouldn't it be `from keras.regularizers import EigenvalueRegularizer`?", "@fchollet  now that I used your line of code, that error is removed, however, I get this new error, can you provide feedback?\r\n```\r\nmona@pascal:~/computer_vision$ python human_activity_recognition.py \r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"human_activity_recognition.py\", line 18, in <module>\r\n    from regularizers import EigenvalueRegularizer\r\nImportError: No module named regularizers\r\nmona@pascal:~/computer_vision$ vi human_activity_recognition.py \r\nmona@pascal:~/computer_vision$ vi human_activity_recognition.py \r\nmona@pascal:~/computer_vision$ python human_activity_recognition.py \r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"human_activity_recognition.py\", line 76, in <module>\r\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 332, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 572, in __call__\r\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 635, in add_inbound_node\r\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 166, in create_node\r\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/pooling.py\", line 160, in call\r\n    dim_ordering=self.dim_ordering)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/pooling.py\", line 210, in _pooling_function\r\n    pool_mode='max')\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2833, in pool2d\r\n    x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\r\n  File \"/home/mona/tensorflow/_python_build/tensorflow/python/ops/nn_ops.py\", line 1393, in max_pool\r\n    name=name)\r\n  File \"/home/mona/tensorflow/_python_build/tensorflow/python/ops/gen_nn_ops.py\", line 1595, in _max_pool\r\n    data_format=data_format, name=name)\r\n  File \"/home/mona/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/mona/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 2390, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/home/mona/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 1785, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/home/mona/tensorflow/_python_build/tensorflow/python/framework/common_shapes.py\", line 596, in call_cpp_shape_fn\r\n    raise ValueError(err.message)\r\nValueError: Negative dimension size caused by subtracting 2 from 1\r\n\r\n```\r\n\r\nThe code is:\r\n'''This script reuses pieces of code from the post:\r\n\"Building powerful image classification models using very little data\"\r\nfrom blog.keras.io\r\nand from:\r\nhttps://www.kaggle.com/tnhabc/state-farm-distracted-driver-detection/keras-sample\r\nThe training data can be downloaded at:\r\nhttps://www.kaggle.com/c/state-farm-distracted-driver-detection/data\r\n'''\r\n\r\n```\r\nimport os\r\nimport h5py\r\nimport numpy as np\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras import optimizers\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense\r\n#from regularizers import EigenvalueRegularizer\r\nfrom keras.regularizers import EigenvalueRegularizer\r\nfrom numpy.random import permutation\r\nfrom keras.optimizers import SGD\r\nimport pandas as pd\r\nimport datetime\r\nimport glob\r\nimport cv2\r\nimport math\r\nimport pickle\r\nfrom collections import OrderedDict\r\nfrom keras import backend as K\r\n\r\n\r\n# Enter here the path to the model weights files:\r\nweights_path = '/data/wd1/activity_recognition/vgg16_weights.h5'\r\n# Enter here the path to the top-model weights files:\r\ntop_model_weights_path = '/data/wd1/activity_recognition/fc_model.h5'\r\n# Enter here the path for storage of the whole model weights (VGG16+top classifier model):\r\nwhole_model_weights_path = '/data/wd1/activity_recognition/whole_model.h5'\r\n# Enter here the name of the folder that contains the folders c0, c1,..., c9, with the training images belonging to classes 0 to 9:\r\ntrain_data_dir = 'train'\r\n# Enter here the name of the folder where is the test images (the data evalueted in the private leaderboard):\r\ntest_data_dir = 'test'\r\n\r\ntest_images_path = 'test/test'\r\n\r\n# Enter here the features of the data set:\r\nimg_width, img_height = 224, 224\r\nnb_train_samples = 22424\r\nnb_test_samples = 79726\r\ncolor_type_global = 3\r\n\r\n# You can set larger values here, according with the memory of your GPU:\r\nbatch_size = 32\r\n\r\n# Enter here the number of training epochs (with 80 epochs the model was positioned among\r\n# the 29% best competitors in the private leaderboard of state-farm-distracted-driver-detection)\r\n# According to our results, this model can achieve a better performance if trained along a larger \r\n# number of epochs, due to the agressive regularization with Eigenvalue Decay that was adopted.\r\nnb_epoch = 80\r\n\r\n#Enter here the path for the whole model (VGG16+top classifier model):\r\nwhole_model_weights_path = '/data/wd1/activity_recognition/whole_model.h5'\r\n\r\n# build the VGG16 network:\r\nmodel = Sequential()\r\nmodel.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\r\n\r\nmodel.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n# loading the weights of the pre-trained VGG16:\r\n\r\nassert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\r\nf = h5py.File(weights_path)\r\nfor k in range(f.attrs['nb_layers']):\r\n    if k >= len(model.layers):\r\n        break\r\n    g = f['layer_{}'.format(k)]\r\n    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\r\n    model.layers[k].set_weights(weights)\r\nf.close()\r\nprint('Model loaded.')\r\n\r\n# building a classifier model on top of the convolutional model:\r\n\r\ntop_model = Sequential()\r\ntop_model.add(Flatten(input_shape=model.output_shape[1:]))\r\ntop_model.add(Dense(64, activation='relu', W_regularizer=EigenvalueRegularizer(10)))\r\ntop_model.add(Dense(10, activation='softmax', W_regularizer=EigenvalueRegularizer(10)))\r\ntop_model.load_weights(top_model_weights_path)\r\n\r\n# add the model on top of the convolutional base\r\nmodel.add(top_model)\r\n\r\n# setting the first 15 layers to non-trainable (the original weights will not be updated)\r\n    \r\nfor layer in model.layers[:15]:\r\n    layer.trainable = False\r\n\r\n# Compiling the model with a SGD/momentum optimizer:\r\n\r\nmodel.compile(loss = \"categorical_crossentropy\",\r\n              optimizer=optimizers.SGD(lr=1e-6, momentum=0.9),\r\n              metrics=['mean_squared_logarithmic_error', 'accuracy'])\r\n\r\n# Data augmentation:\r\n\r\ntrain_datagen = ImageDataGenerator(shear_range=0.3, zoom_range=0.3, rotation_range=0.3)\r\ntest_datagen = ImageDataGenerator()\r\n\r\nprint('trainning')\r\ntrain_generator = train_datagen.flow_from_directory(\r\n        train_data_dir,\r\n        target_size=(img_height, img_width),\r\n        batch_size=32,\r\n        class_mode='categorical')\r\n  \r\n\r\nprint('testing')\r\ntest_generator = test_datagen.flow_from_directory(\r\n        test_data_dir,\r\n        target_size=(img_height, img_width),\r\n        batch_size=32,\r\n        class_mode='categorical',\r\n        shuffle=False)\r\n\r\nclass_dictionary = train_generator.class_indices\r\nsorted_class_dictionary = OrderedDict(sorted(class_dictionary.items()))\r\nsorted_class_dictionary = sorted_class_dictionary.values()\r\nprint(sorted_class_dictionary)\r\n\r\n# Fine-tuning the model:\r\nmodel.fit_generator(\r\n        train_generator,\r\n        samples_per_epoch=nb_train_samples,\r\n        nb_epoch=nb_epoch,\r\n        validation_data=train_generator,\r\n        nb_val_samples=nb_train_samples)\r\n        \r\nmodel.save_weights(whole_model_weights_path)\r\n\r\naux = model.predict_generator(test_generator, nb_test_samples)\r\npredictions = np.zeros((nb_test_samples, 10))\r\n\r\n# Rearranging the predictions:\r\n\r\nord = [5, 0, 6, 2, 7, 9, 1, 4, 8, 3]\r\n\r\nfor n in range(10):\r\n    i = ord[n]\r\n    print(i)\r\n    print(aux[:, i])\r\n    predictions[:, n] = aux[:, i]\r\n\r\n# Trick to improve the multi-class logarithmic loss (the evaluation metric of state-farm-distracted-driver-detection from Keras):\r\n\r\npredictions = 0.985 * predictions + 0.015\r\n\r\ndef get_im(path, img_width, img_height, color_type=1):\r\n    if color_type == 1:\r\n        img = cv2.imread(path, 0)\r\n    elif color_type == 3:\r\n        img = cv2.imread(path)\r\n    # Reduce size\r\n    resized = cv2.resize(img, (img_height, img_width))\r\n    return resized\r\n\r\ndef load_test(img_width, img_height, color_type=1):\r\n    print('Read test images')\r\n    path = os.path.join(test_images_path, '*.jpg')\r\n    files = glob.glob(path)\r\n    X_test = []\r\n    X_test_id = []\r\n    total = 0\r\n    thr = math.floor(len(files)/10)\r\n    for fl in files:\r\n        flbase = os.path.basename(fl)\r\n        img = get_im(fl, img_width, img_height, color_type)\r\n        X_test.append(img)\r\n        X_test_id.append(flbase)\r\n        total += 1\r\n        if total % thr == 0:\r\n            print('Read {} images from {}'.format(total, len(files)))\r\n\r\n    return X_test, X_test_id\r\n\r\nX_test, test_id = load_test(img_width, img_height, color_type_global)\r\n\r\ndef create_submission(predictions, test_id):\r\n    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3',\r\n                                                 'c4', 'c5', 'c6', 'c7',\r\n                                                 'c8', 'c9'])\r\n    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\r\n    now = datetime.datetime.now()\r\n    if not os.path.isdir('subm'):\r\n        os.mkdir('subm')\r\n    suffix = '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\r\n    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\r\n    result1.to_csv(sub_file, index=False)\r\n\r\ncreate_submission(predictions, test_id)\r\n```", "@monajalal it looks like this is not strictly a tensorflow bug. So we're responding as we have bandwidth here, but your best bet is to go on stackoverflow, where we monitor issues with the tag `tensorflow`.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7022, "title": "Remove concat_v2 from r1.0 branch", "body": "Removing concat_v2 and references. tf.concat now has the same signature as tf.concat_v2. So, tf.concat should be used instead of tf.concat_v2.", "comments": ["This is a planned and approved API change that should happen pre 1.0."]}, {"number": 7021, "title": "Remove errorneous squeeze in `tf.contrib.losses.sparse_softmax_cross_entropy_loss`", "body": "The `tf.contrib.losses.sparse_softmax_cross_entropy_loss` has an weights parameter which can be used to weight the individual batch elements. The weights parameter can have various shape, which are all taken care of in `compute_weighted_loss`.\r\n\r\nHowever, the `sparse_softmax_cross_entropy_loss` methods contains an errorneous squeeze of the weights. If some dimension of weights is not known, the number of dimensions after the squeeze is unknown, which causes the `compute_weighted_loss` method to throw an exception. However, if the squeeze in `sparse_softmax_cross_entropy_loss` is not called, `compute_weighted_loss` can deal even with unknown dimensions of weights.\r\n\r\nNote that `tf.contrib.losses.softmax_cross_entropy_loss` does not contain the squeeze of the weights, even if it uses the weights argument in an equal way.\r\n\r\nShould fix #6846.", "comments": ["Can one of the admins verify this patch?", "Could we add a test for that perhaps?", "@foxik could you add a test to your PR, please?", "I implemented the tests, they fail without the fix applied.\r\n\r\nI added the tests also for `contrib.losses.softmax_cross_entropy` -- they are not strictly needed there (there was not the same error in `softmax_cross_entropy`), but the API offers the same guarantees for the `weights` in both calls.", "@tensorflow-jenkins test this please", "Thanks for the fix."]}, {"number": 7020, "title": "ImportError: cannot import name control_flow_ops", "body": "Please suggest fixes. I just recently installed tf for Python2.7 on Ubuntu 14.04 and CUDA8. \r\n```\r\nmona@pascal:~/computer_vision/distracted-drivers-tf$ python main.py \r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\n/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\r\n  \"This module will be removed in 0.20.\", DeprecationWarning)\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 12, in <module>\r\n    from model import Model\r\n  File \"/home/mona/computer_vision/distracted-drivers-tf/model.py\", line 6, in <module>\r\n    from layers import Input\r\n  File \"/home/mona/computer_vision/distracted-drivers-tf/layers.py\", line 4, in <module>\r\n    from tensorflow.python import control_flow_ops\r\nImportError: cannot import name control_flow_ops\r\n\r\n```", "comments": ["You are not supposed to use `control_flow_ops` directly, since this is not part of the public API. Please use `import tensorflow as tf` and then `tf.cond` etc directly.", "Closing due to lack of recent activity.\r\nPlease reopen if you still run into this problem when trying to use `tf.cond`.", "`from tensorflow.python.ops import control_flow_ops`"]}, {"number": 7019, "title": "Windows Cmake RelWithDebInfo not working", "body": "Summary:\r\nI'm having an issue getting TensorFlow to build with debug symbols in Windows using Cmake.\r\n\r\nAbout my system:\r\n\r\nWindows 10\r\nVersion 1607\r\nBuild 14393.693\r\n\r\nCmake version 3.7.1\r\n\r\nVisual Studio Community 2015\r\n\r\nClone tensor flow repository from https://github.com/tensorflow/tensorflow\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n\r\nChange directory to tensorflow\\tensorflow\\contrib\\cmake\r\n\r\ncd gitFolder\\tensorflow\\tensorflow\\contrib\\cmake\r\n\r\n\r\nMake build directory, change to directory\r\n\r\nMkdir build\r\n\r\ncd build\r\n\r\nUse cmake\r\n\r\nCmake .. -A  x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo ^\r\n-DSWIG_EXECUTABLE=C:\\tools\\swigwin-3.0.10\\swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:\\Users\\ian\\Anaconda3\\python.exe ^\r\n-DPYTHON_LIBRARIES=C:\\Users\\ian\\Anaconda3\\python35.lib ^\r\n-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF\r\n\r\nNote:\r\nSwig, python executable directory and python library directory will change based on your system. I tried at with the GRPC support enabled (this is the default) but I got build errors related to GRPC. Because we will be handling remote web requests with our own server application, I didn\u2019t think we needed the GRPC support.\r\n\r\nOpen the solution in Visual Studio.\r\n\r\nChange build mode to RelWithDebInfo x64\r\n\r\nBuild\r\n\r\nResult:\r\n\r\nMost projects build, but sadly not the one that I\u2019m interested in, tf_label_image_example. The errors I received are:\r\nLink1120 1 Unresolved externals pywrap_tensorflow\r\nLink1318  Unexpected PDB error; OK(0) tf_tutorials_example_trainer\r\nLink1318  Unexpected PDB error; OK(0) tf_label_image_example\r\nLNK2019\tunresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewServer(class tensorflow::ServerDef const &,class std::unique_ptr<class tensorflow::ServerInterface,struct std::default_delete<class tensorflow::ServerInterface> > *)\" (?NewServer@tensorflow@@YA?AVStatus@1@AEBVServerDef@1@PEAV?$unique_ptr@VServerInterface@tensorflow@@U?$default_delete@VServerInterface@tensorflow@@@std@@@std@@@Z) referenced in function \"void __cdecl PyServer_New(class tensorflow::ServerDef const &,class std::unique_ptr<class tensorflow::ServerInterface,struct std::default_delete<class tensorflow::ServerInterface> > *,struct TF_Status *)\" (?PyServer_New@@YAXAEBVServerDef@tensorflow@@PEAV?$unique_ptr@VServerInterface@tensorflow@@U?$default_delete@VServerInterface@tensorflow@@@std@@@std@@PEAUTF_Status@@@Z)\tpywrap_tensorflow\tC:\\tesorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.obj\t\r\n\r\n\r\n\r\n", "comments": ["It looks like you need to place the binaries in your `%PATH%`. Can you try that?", "Thanks for the response!\r\nAll binaries from the build? Meaning the ones that appear in the tensorflow\\tensorflow\\contrib\\cmake\\build\\RelWithDebInfo folder? \r\nI added C:\\tesorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\RelWithDebInfo to my path, and I'm rebuilding the solution. I'll reply after it's done building or trying to build.", "Unfortunately, adding the build directory to path did not fix my issue. The build is failing with the same errors.", "Oh wait, how did you change the build to `x86_64`? In visual studio, you have to make sure all projects are building for the right target. They are usually copied from the 32-bit `x86` target, then you have to change all projects in the solution.", "I used x64 as a flag for cmake, and then I changed to x64 in Visual Studio's Solution Platform drop down menu. Is there something else I need to change in the individual project? My solution's configuration properties says that all projects are trying to build as RelWithDebInfo x64.", "Visual studio project/solution files can be very annoying, because propagation of configuration sometimes needs a lot of manual setting on the dropdowns by hand. I can't remember the project view on the top of my head. Can you try the www.dependencywalker.com tool on the pywrap DLL and see if there's anything missing from there?", "I'm not sure that I have a pywrap DLL. I have an obj and a lib. Is this a problem?", "Yes. That's strange, it should have been produced by the build. Would you mind putting the build logs somewhere on pastebin?", "[Build Output.txt](https://github.com/tensorflow/tensorflow/files/725368/Build.Output.txt)\r\n", "Thanks! I'll take a look tomorrow. Ping if I drop this.", "Any progress on this?", "OK, it fails to build the pywrap DLL because it failed to compile the wrapper. And that's because it failed to resolve the C++ function. Could you find a way to get the command-line that's running on this? It should be trying to link against the C++ tensorflow lib. We'd need to check the symbols in the C++ tensorflow lib and find out what symbols are out there. Maybe they're x86 symbols, or they have the wrong linkage.\r\n\r\n```\r\n194>pywrap_tensorflow.obj : error LNK2019: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::NewServer(class tensorflow::ServerDef const &,class std::unique_ptr<class tensorflow::ServerInterface,struct std::default_delete<class tensorflow::ServerInterface> > *)\" (?NewServer@tensorflow@@YA?AVStatus@1@AEBVServerDef@1@PEAV?$unique_ptr@VServerInterface@tensorflow@@U?$default_delete@VServerInterface@tensorflow@@@std@@@std@@@Z) referenced in function \"void __cdecl PyServer_New(class tensorflow::ServerDef const &,class std::unique_ptr<class tensorflow::ServerInterface,struct std::default_delete<class tensorflow::ServerInterface> > *,struct TF_Status *)\" (?PyServer_New@@YAXAEBVServerDef@tensorflow@@PEAV?$unique_ptr@VServerInterface@tensorflow@@U?$default_delete@VServerI\r\n```\r\n\r\nJust above there's \r\n\r\n```\r\n196>utils.obj : fatal error LNK1318: Unexpected PDB error; OK (0) ''\r\n195>utils.obj : fatal error LNK1318: Unexpected PDB error; OK (0) ''\r\n```", "CC @mrry just in case", "I set Visual Studio's MSBuild output level to diagnostic, and I'm rebuilding the solution. I believe that will give the command line that's running underneath, let me know if I misunderstood the request. I'll post an attachment when it finishes running. ", "Hmm, I guess the problem is that we don't test with `-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF` very often, and that option has bit-rotted... in particular, I don't think we've tested the Python support with it disabled.\r\n\r\nI think the correct fix is to ensure that `tensorflow/core/distributed_runtime/server_lib.cc` is compiled and linked in even when `-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF`. That will ensure that the unresolved symbol is defined in any case. Does that sound right? I'm preparing a fix now.", "Sounds good thanks!\n\nOn Jan 25, 2017 1:52 PM, \"iantheconway\" <notifications@github.com> wrote:\n\n> I set Visual Studio's MSBuild output level to diagnostic, and I'm\n> rebuilding the solution. I believe that will give the command line that's\n> running underneath, let me know if I misunderstood the request. I'll post\n> an attachment when it finishes running.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7019#issuecomment-275243912>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbWbdw5bj2GTvNJAxnSs796gjb4bMks5rV8QNgaJpZM4LrjZ8>\n> .\n>\n", "Excellent, thanks! As I mentioned, I got another issue when I tried to build RelWithDebInfo with GRPC support on. Would it help if I tried to build with GRPC support on again and got back to you with info if it still fails?", "That would indeed be useful, thanks! (I typically build with RelWithDebInfo locally, so I'm surprised that it isn't working with gRPC... but it's possible that I have something in my environment that causes it work.)", "Hey, I tried pulling again after the commit was pushed, and the build still failed for me. I noticed my CMake version is 3.7.1 and that the CMake instructions say to use 3.5 up to 3.6, could that be an issue? Everything else on my system seems to be fine. The errors I'm getting are:\r\n\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\ttf_label_image_example\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\ttf_label_image_example\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\ttf_tutorials_example_trainer\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\ttf_tutorials_example_trainer\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\tcompare_graphs\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\tcompare_graphs\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\ttransform_graph\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\ttransform_graph\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\tsummarize_graph\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\tsummarize_graph\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\tpywrap_tensorflow\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\nError\tLNK1318\tUnexpected PDB error; OK (0) ''\tpywrap_tensorflow\tC:\\tensorflow_1_25_17_b\\tensorflow\\tensorflow\\contrib\\cmake\\build\\utils.obj\t1\t\r\n\r\n", "Possibly related to this Stack Overflow question? http://stackoverflow.com/questions/4256524/lnk1318-unexpected-pdb-error-ok-0", "Hmm. It was a fresh clone with a fresh Cmake build, so I don't think deleting project temp files would help, but restarting Windows is always worth a try. I'll let you know if that works. Thanks.", "Restarting did not help for me, my build failed with the same errors. However, it seems that it's failing with or without the gRPC support, so my issue must not be related to the issue you fixed. I think I'll try on a fresh machine to see if it's an issue with my environment.", "A fresh build on a separate machine did not fix my PDB errors.\r\n\r\nOn a whim, I tried enabling Cuda support on the second machine, and I got a different set of errors, starting with: \r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError\tMSB6006\t\"cmd.exe\" exited with code 1.\ttf_core_gpu_kernels\tC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets\t171\t\r\n\r\nAnd followed by a bunch of errors related to tf_core_gpu_kernels not existing. Any ideas about this one?\r\n\r\nAlso, @mrry you said that RelWithDebInfo builds fine for you, right? Do you use Visual Studio or do you use MSBuild like in the instructions? Thanks.", "This is how we run the build:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/cpu/cmake/run_build.bat", "@iantheconway I only ever build using MSBuild from the Command Prompt, like in the instructions. If you're using Visual Studio, it ought to work, but there may be some configuration mismatch. In particular, the build is pretty massive, and Visual Studio's UI doesn't always make it easy to find the root cause of a build failure. It'd be great if you could capture the complete build output when building from a clean repository, as that will probably contain the information we need to fix it.", "Will do, thanks.", "[build_output.txt](https://github.com/tensorflow/tensorflow/files/736097/build_output.txt) This is what Visual Studio is outputting. \r\n", "Could you do this?\r\n\r\n1. Kill the PDB server (better yet, reboot)\r\n2. Remove all PDB files\r\n3. Use the msbuild command as in the instructions\r\n\r\nAnd see if it works. If that works, then we can try to narrow it down to a GUI problem.", "Thanks for sharing the new build output. Is it possible that you're using the 32-bit toolchain? If I remember correctly, the default version of the compiler and linker (even when compiling x64 code) is a 32-bit executable. TensorFlow generates quite a large binary (and debug symbols), which might exhaust the address space of a 32-bit compiler/linker.\r\n\r\nIt's a little tricky to force VS to use the 64-bit toolchain, and I believe that's why I started using the command-line tools instead. This [article on MSDN](https://msdn.microsoft.com/en-us/library/x4d2c09s.aspx) covers how to set the 64-bit toolchain from the command line. I'm not 100% certain (can't find a reference right now) but I *think* that if you run `devenv.exe` from a command prompt configured this way, you will get the 64-bit tools when you compile from within Visual Studio.", "@drpngx I will try this and let you know the results\r\n@mrry That would make sense. After I try the command line build, if I try the vcvarsall command from the article you sent followed by devenv.exe that might work, right?\r\n\r\nThanks.", "That might work indeed! (If not, you'll at least be in the right environment to run `msbuild.exe` :)....)", "I tested it today. Using MSBuild, I was able to build all executables in RelWithDebInfo :). I'll try devenv.exe and hopefully that will allow me to use Visual Studio to build. Thanks for all the help."]}, {"number": 7018, "title": "replace is_real with is_floating, remove is_real", "body": "@martinwicke \r\n\r\nHaven't tested (because of https://github.com/tensorflow/tensorflow/issues/6911 testing time goes 1 min -> 20 mins every time you switch branch)", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please!", "Thank you!", "@caisq, this will fix the API modification in the pull.", "Thanks, @yaroslavvb "]}, {"number": 7017, "title": "Fix TensorBoard projector plugin reading tensor file in python 3.", "body": "Current code throws an error in Python 3, since map() returns a map object and not a list.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.\r\n\r\n(I was an unrelated timeout the last time, but just to be sure.)", "Jenkins, test this please."]}, {"number": 7016, "title": "Enable denormal test in open source", "body": "They seem to work fine; not sure what changed since ages ago.  The test works fine on my linux desktop and my OS X laptop; we'll see how it fares on the rest of the Jenkins machines.", "comments": ["Internal bug link so that I can get to it from here: b/28906384.", "@mrry Suggestions for how to fix the Windows part?  I'm a bit surprised that it doesn't work, since the denormal flag code only depends on SSE3 (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/denormal.cc#L27).  Maybe SSE3 isn't on for the Windows build?", "2 tests in the Python 3 run seem to fail for unrelated reasons, so Windows is the only issue. ", "```\r\n10:14:10 FAIL: testFlushDenormalsCPU (__main__.DenormalTest)\r\n10:14:10 ----------------------------------------------------------------------\r\n10:14:10 Traceback (most recent call last):\r\n10:14:10   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/denormal_test.py\", line 50, in testFlushDenormalsCPU\r\n10:14:10     self._flushDenormalsTest(use_gpu=False, dtypes=(np.float32, np.float64))\r\n10:14:10   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/denormal_test.py\", line 44, in _flushDenormalsTest\r\n10:14:10     self.assertAllEqual(flush.eval(), np.zeros(shape))\r\n10:14:10   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 535, in assertAllEqual\r\n10:14:10     np.testing.assert_array_equal(a, b)\r\n10:14:10   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\testing\\utils.py\", line 871, in assert_array_equal\r\n10:14:10     verbose=verbose, header='Arrays are not equal')\r\n10:14:10   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\testing\\utils.py\", line 796, in assert_array_compare\r\n10:14:10     raise AssertionError(msg)\r\n10:14:10 AssertionError: \r\n10:14:10 Arrays are not equal\r\n10:14:10 \r\n10:14:10 (mismatch 100.0%)\r\n10:14:10  x: array(1.1754946310819804e-39, dtype=float32)\r\n10:14:10  y: array(0.0)\r\n10:14:10 \r\n10:14:10 ======================================================================\r\n10:14:10 FAIL: testFlushDenormalsGPU (__main__.DenormalTest)\r\n10:14:10 ----------------------------------------------------------------------\r\n10:14:10 Traceback (most recent call last):\r\n10:14:10   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/denormal_test.py\", line 54, in testFlushDenormalsGPU\r\n10:14:10     self._flushDenormalsTest(use_gpu=True, dtypes=(np.float32,))\r\n10:14:10   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/denormal_test.py\", line 44, in _flushDenormalsTest\r\n10:14:10     self.assertAllEqual(flush.eval(), np.zeros(shape))\r\n10:14:10   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 535, in assertAllEqual\r\n10:14:10     np.testing.assert_array_equal(a, b)\r\n10:14:10   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\testing\\utils.py\", line 871, in assert_array_equal\r\n10:14:10     verbose=verbose, header='Arrays are not equal')\r\n10:14:10   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\testing\\utils.py\", line 796, in assert_array_compare\r\n10:14:10     raise AssertionError(msg)\r\n10:14:10 AssertionError: \r\n10:14:10 Arrays are not equal\r\n10:14:10 \r\n10:14:10 (mismatch 100.0%)\r\n10:14:10  x: array(1.1754946310819804e-39, dtype=float32)\r\n10:14:10  y: array(0.0)\r\n```\r\n\r\nI suppose we could use `assertAllClose` instead of `assertAllEqual` unless it should be exactly zero.", "Given that we're checking whether denormals are flushed to zero, I'd prefer to stay with `assertAllEqual`.", "@girving From this [similar-sounding bug in Eigen](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=136), it looks like MSVC does build with SSE3 instructions, but does doesn't define `__SSE3__`. Since Eigen seems to have the necessary conditionals for sending whether SSE3 is available, perhaps we could use`#ifdef EIGEN_VECTORIZE_SSE3` instead?", "@mrry Excellent idea.  I've confirmed that `__SSE3__ -> EIGEN_VECTORIZE_SSE3` doesn't break my desktop or laptop, so hopefully Jenkins will be greener after the last commit.", "@mrry Alas, looks like that didn't do it.", "Hmm, looking at the present-day Eigen code, perhaps this is never defined:\r\n\r\nhttps://bitbucket.org/eigen/eigen/src/737f257bb23da9175bcef9102b702b4ab0671a2b/Eigen/Core?at=default&fileviewer=file-view-default#Core-121\r\n\r\nIf we assume that all Windows machines that we care about have SSE3 (I think that's reasonable, right?), maybe adding an explicit `-DEIGEN_VECTORIZE_SSE3` to CMakeLists.txt for the Windows build [here](https://github.com/tensorflow/tensorflow/blob/859d8e30070fec468fa9f8f052eae5b7c086d72b/tensorflow/contrib/cmake/CMakeLists.txt#L62) would be the way to go.", "@mrry I'm okay with an explicit define if you are.  It's a bit sad, but the web isn't showing me any alternatives either.  I'll give it a try.", "Woot.  @mrry: I think it's ready for review.", "This test seems to have failed in windows-cpu cmake build.\r\nhttp://ci.tensorflow.org/job/tf-master-win-gpu-cmake/223/console\r\ntestFlushDenormalsGPU fails with:\r\n01:46:34.321 (mismatch 100.0%)\r\n01:46:34.321  x: array(1.1754946310819804e-39, dtype=float32)\r\n01:46:34.321  y: array(0.0)\r\n\r\nAny ideas?", "I merged when it was all green. We had that error before and @girving / @mrry fixed it on the thread above. It sounded it like fix was deterministic.", "@gunan Yep, the fix should have been deterministic, but let me know if these failures continue.", "Windows GPU tests take hours, so we do not run them during presubmits.\nI will keep an eye on the build and touch back if we still have issues\nafter a few runs.\n\nOn Wed, Jan 25, 2017 at 7:30 AM, Geoffrey Irving <notifications@github.com>\nwrote:\n\n> @gunan <https://github.com/gunan> Yep, the fix should have been\n> deterministic, but let me know if these failures continue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7016#issuecomment-275138738>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOWwn0AmLMlC9bg6CRYaPe-oZzMHSks5rV2qmgaJpZM4LrWk0>\n> .\n>\n", "@gunan Ah, the GPU Windows build probably will break.  To fix it I need to add a flag to cmake's use of nvcc.  Since I can't test it myself, should I prepare a PR and ask you to test?", "Sure, I can quickly start a presubmit run for windows cmake gpu on any pull\nrequests you need.\n\nOn Wed, Jan 25, 2017 at 2:24 PM, Geoffrey Irving <notifications@github.com>\nwrote:\n\n> @gunan <https://github.com/gunan> Ah, the GPU Windows build probably will\n> break. To fix it I need to add a flag to cmake's use of nvcc. Since I can't\n> test it myself, should I prepare a PR and ask you to test?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7016#issuecomment-275252084>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOTVVzXkkMs2g_kViWv_WDXAJ3l-Pks5rV8uEgaJpZM4LrWk0>\n> .\n>\n", "Is it possible this was passing, because we were building everything with sse4.1?\r\nMy change to disable sse4.1 seems to break this test:\r\nhttps://github.com/tensorflow/tensorflow/pull/7349\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/3588/consoleFull", "@gunan Yes, we need at least SSE3 for this test to pass.  Is the plan to not have SSE3 going forwards?"]}, {"number": 7015, "title": "ptb_rnn_lm.py error: InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]", "body": "I am having the same issue that can be found here:\r\nhttps://github.com/tensorflow/tensorflow/issues/6469\r\n\r\nThat issue was closed however. It seems like a bunch of people are still having the problem with the LSTM Penn Tree Bank dataset tutorial online [here](https://www.tensorflow.org/tutorials/recurrent/)\r\n\r\nHas anyone found a fix? The exact error I am getting is: \r\n```\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]\r\n\t [[Node: Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Train/Model/add, Train/Model/sequence_loss_by_example/Reshape)]]\r\n```\r\n\r\nI have made all of the changes to incorporate the model being a newer version. I have TensorFlow 12.1. Can anyone help?! Thank you all so much.", "comments": ["I have the same problem. How did you solve it\uff1f"]}, {"number": 7014, "title": "clip_by_value clips NaN to clip_value_max", "body": "Running \r\n`sess.run(tf.clip_by_value(float('nan'), 0.0, 100.0))`\r\nreturns `100.0`\r\n\r\nI'm not sure if this is expected behavior or convenient for clipping gradients, but I believe it should return `nan` (as `np.clip()` does) or be documented.\r\n\r\n### Environment info\r\nWindows 7\r\nOnly CPU\r\nPython 3.5.2 from Anaconda\r\nTensorflow 0.12.0-rc1 installed from binary pip package", "comments": ["@aselle for numpy compat.", "In general, I think it's probably a good idea to follow `np.clip()`. We might be using `isfinite()` instead of `isnan()`. If you submit a PR we could discuss the issues there.\r\n\r\n@rmlarsen FYI", "This is generally the case... Also tf.clip_by_value should be consistent with the implementation of\r\ntf.minimum and tf.maximum s.t.\r\nclip_by_value(v,a,b) is equivalent to  tf.minimum(tf.maximum(a, v), b)\r\ntf.minimum and maximum have the correct nan behavior. \r\n", "I cannot reproduce this on 0.9, 0.10, 0.11, 0.12 or the last nightly build from master in ubuntu 14 and docker ubuntu 14.  I get nan.  What version and platform are you using? What version of CUDA? The issue template we ask people to complete asks what version and platform you are using, so we can help you on these types of issues better. Please use it in the future. Thank you.\r\n\r\n", "I have filled in the information. I will do so in the future too. Sorry.\r\nI will confirm the precise tensorflow version and test with a newer version as soon as I am able.", "Same results on tensorflow 0.12.1. And I also get the float instead of the `nan` if I call `tf.minimum(float('nan'), 0.0)` or `tf.maximum(float('nan'), 0.0)`", "Just to be clear, @garibarba you're running:\r\n\r\n```\r\nimport tensorflow as tf\r\nprint tf.Session().run(tf.minimum(float('nan'), 0.0))\r\n```\r\n?", "Precisely:\r\n`python -c \"import tensorflow as tf; print(tf.Session().run(tf.minimum(float('nan'), 0.0)))\"`\r\n`python -c \"import tensorflow as tf; print(tf.Session().run(tf.maximum(float('nan'), 0.0)))\"`\r\n`python -c \"import tensorflow as tf; print(tf.Session().run(tf.clip_by_value(float('nan'), 0.0, 100.0)))\"`", "OK, it gives `nan` for me on Linux and `master`. @mrry / @gunan do you happen to have a windows build available where you could repro?", "Trying now.", "Reproduced the issue on windows, both on master and 0.12.1", "Thank you @gunan ! We should definitely add a test for that once fixed.\r\n\r\nAssigning to @aselle . I might take a look later", "It looks like the problem is caused by maximum and minimum.\r\nAnd the implementations of those seem to just use eigen, if I am not mistaken.", "Hopefully it's not a problem with NaN on SIMD.", "I \"fixed\" the underlying NaN problem for SIMD max/min Eigen in \r\n\r\nhttps://bitbucket.org/eigen/eigen/commits/738d7c6becf04a2a8f400d5edd2c13ea44f28428\r\n\r\n(plus a few followup). Please notice that it is a bit subtle since max and min are defined to match the (elementwise) behavior of std::max / std::min. See http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373 for a discussion.\r\n\r\nUnfortunately we have not been able to update TensorFlow to use a recent version of Eigen containing this fix, due to build breakage in Eigen. I'll try to give this another try.", "Nice!", "@rmlarsen I'm unable to clone the Eigen repo because bitbucket's https clone doesn't work.  Has this been fixed yet?", "Fixed a while ago, apparently."]}, {"number": 7013, "title": "ResourceExhaustedError while converting ImageNet dataset to TFRecord", "body": "Determining list of input files and labels from /run/media/root/d9ecddfe-d069-4252-aade-b75fa707b3d2/imagenet-data/raw-data/train.\r\nTraceback (most recent call last):\r\n  File \"/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py\", line 704, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n  File \"/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py\", line 700, in main\r\n  File \"/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py\", line 597, in _process_dataset\r\n  File \"/cgpit/models/inception/bazel-bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py\", line 513, in _find_image_files\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 251, in get_matching_files\r\n  File \"/usr/lib64/python2.7/contextlib.py\", line 24, in __exit__\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\ntensorflow.python.framework.errors.ResourceExhaustedError: /run/media/root/d9ecddfe-d069-4252-aade-b75fa707b3d2/imagenet-data/raw-data/train/n01440764\r\n\r\nI have tried changing different number of threads,different shard size for both training and validation. But still getting error.  There is enough RAM is free while executing this command. \r\n\r\nIs it bug or it required some configuration changes?\r\n\r\nCommand used for converting:\r\nbazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n\r\nwhere $DATA_DIR= /run/media/root/d9ecddfe-d069-4252-aade-b75fa707b3d2/imagenet-data/\r\nEnvironment info:\r\nOperating System:\r\nCentOS 7 64bit\r\n32GB RAM\r\nIntel\u00ae Xeon(R) CPU E5-2630 v2 @ 2.60GHz \u00d7 12 \r\n\r\nCPU only version of tensorflow\r\nInstalled from source\r\n[root@cgpits inception]# bazel version\r\nBuild label: 0.3.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\r\nBuild timestamp: 1469783392\r\nBuild timestamp as int: 1469783392\r\n\r\nhead for tenserflow\r\n\r\n[root@cgpits tensorflow]# git rev-parse HEAD\r\n787ab22d490e79ea8c06511d60d6cddf1b2dd2c2\r\n\r\nhead version for tesorflow/model\r\n[root@cgpits inception]# git rev-parse HEAD\r\n82c219f24d3a7c5c48a9550aef10bde3a031520d\r\n\r\n[root@cgpits inception]# python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n0.11.0rc0\r\n", "comments": ["I wonder what resource is getting exhausted. File Descriptors? Someone got around a similar sounding problem by adding \"time.sleep\" statements to stagger their file opens in https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273946136", "I *think* @yaroslavvb is right. From reading the code, it should be coming from [GetChildren](http://google3/third_party/tensorflow/core/platform/posix/posix_file_system.cc?l=201), which calls `opendir`. The only two that could arise in this situation are too many (global/process) open files, or too many links (unlikely unless you have a loop of sorts). \r\n\r\nCould you try to debug? Reboot, reduce the number of threads, insert `time.sleep`, or trace down descriptor leak. First thing to do would be to put some code in `GetChildren` to see if that's what you're getting.\r\n\r\n```\r\n    case ENOSPC:   // No space left on device\r\n#if !defined(_WIN32)\r\n    case EDQUOT:   // Disk quota exceeded\r\n#endif\r\n    case EMFILE:   // Too many open files\r\n    case EMLINK:   // Too many links\r\n    case ENFILE:   // Too many open files in system\r\n    case ENOBUFS:  // No buffer space available\r\n    case ENODATA:  // No message is available on the STREAM read queue\r\n    case ENOMEM:   // Not enough space\r\n    case ENOSR:    // No STREAM resources\r\n#if !defined(_WIN32)\r\n    case EUSERS:   // Too many users\r\n#endif\r\n```", "can you please provide more details where I can add this statements? \r\nI am not sure what to compare with this case statements.\r\nI have even tried to use different number of threads from 1 to 1024.\r\nMachine is dedicated server with single user and 16TB storage and 32GB RAM.\r\nI am quite sure that error is one of the following case:\r\n    case EMFILE:   // Too many open files\r\n    case EMLINK:   // Too many links\r\n    case ENFILE:   // Too many open files in system\r\n    case ENOBUFS:  // No buffer space available\r\n    case ENODATA:  // No message is available on the STREAM read queue\r\n    case ENOMEM:   // Not enough space\r\n    case ENOSR:    // No STREAM resources\r\n", "Right which one? Can you put a `LOG(INFO)` statement?", "Closing due to lack of response."]}, {"number": 7012, "title": "Add gradient for `placeholder_with_default`", "body": "Patch for #6906 ", "comments": ["Can one of the admins verify this patch?", "Gradient registered in python api only. I found no existing python test for the gradient of `identity`, so I didn't create one for `placeholder_with_default` (manual test seems ok).\r\n\r\nI also tried to register the gradient in c api, but I'm not sure how to do it. My attempt:\r\n\r\n```\r\ndiff --git a/tensorflow/cc/gradients/array_grad_test.cc b/tensorflow/cc/gradients/array_grad_test.cc\r\nindex 2d6fabd..c91166f 100644\r\n--- a/tensorflow/cc/gradients/array_grad_test.cc\r\n+++ b/tensorflow/cc/gradients/array_grad_test.cc\r\n@@ -96,6 +96,13 @@ TEST_F(ArrayGradTest, IdentityGrad) {\r\n   RunTest(x, shape, y, shape);\r\n }\r\n \r\n+TEST_F(ArrayGradTest, PlaceholderWithDefaultGrad) {\r\n+  TensorShape shape({5, 2});\r\n+  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n+  auto y = PlaceholderWithDefault(scope_, x, TensorShape());\r\n+  RunTest(x, shape, y, shape);\r\n+}\r\n+\r\n TEST_F(ArrayGradTest, SplitGrad) {\r\n   TensorShape x_shape({5, 2});\r\n   auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(x_shape));\r\n\r\ndiff --git a/tensorflow/core/ops/array_grad.cc b/tensorflow/core/ops/array_grad.cc\r\nindex 2894e3c..844585b 100644\r\n--- a/tensorflow/core/ops/array_grad.cc\r\n+++ b/tensorflow/core/ops/array_grad.cc\r\n@@ -87,6 +87,7 @@ Status IdentityGrad(const AttrSlice& attrs, FunctionDef* g) {\r\n   return Status::OK();\r\n }\r\n REGISTER_OP_GRADIENT(\"Identity\", IdentityGrad);\r\n+REGISTER_OP_GRADIENT(\"PlaceholderWithDefault\", IdentityGrad);\r\n \r\n Status PackGrad(const AttrSlice& attrs, FunctionDef* g) {\r\n   // clang-format off\r\n```\r\n\r\n`//tensorflow/cc:gradients_array_grad_test` reports the following error:\r\n\r\n```\r\ntensorflow/cc/gradients/array_grad_test.cc:38: Failure\r\nValue of: (ComputeGradientError(scope_, {x}, {x_shape}, {y}, {y_shape}, &max_error))\r\n  Actual: Not found: No gradient defined for op: PlaceholderWithDefault\r\nExpected: ::tensorflow::Status::OK()\r\n```", "I think I saw a changelist fly by that fixed something like that. Could you sync without this and see if it's still required?", "Looks like `placeholder_with_default` still doesn't have a gradient as of current HEAD https://github.com/tensorflow/tensorflow/commit/a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2.", "Jenkins, test this please.", "Please add a gradient test to `python/kernel_tests/constant_op_test.py`, specifically in `PlaceholderWithDefaultTest`.", "@gaohuazuo could you please add the unit test requestd by @girving?", "It is Chinese New Year now. I can add the test next week.", "@gaohuazuo OK, sounds great.", "@gaohuazuo happy new year!  Do you have time to add the tests?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I have added a simple gradient test (the last commit). All other commits come from the tf main repo, but github seems to recognize them as part of the pr. I guess I should open another pr to replace this one?", "I think you added a commit, then merged, then added another commit.  Github then freaked out in the process.  Rebasing on top of HEAD should fix the problem, no need to make a new PR.", "It is possible to fix this (rebase your commits to current HEAD) in this PR, but you might as well send a new PR instead, it might be easier."]}, {"number": 7011, "title": "Is there any method to read HDFS file in a distributed way?", "body": "I'm working on reading hdfs file in between graph mode. \r\nI used `tf.train.string_input_produce()` to read from hdfs and produce the input.\r\n\r\nHowever, I found that all the workers are reading the whole hdfs file every time fetching a batch.\r\n\r\nI'm wondering if it is a known issue or is there any way in TensorFlow that each worker could just read a part of the file like hadoop MapReduce?", "comments": ["I don't think we support splittable files.", "We don't currently support splittable files, but I'm planning to support that in a future release.\r\n\r\nFor now, you should split data at training data generation time.", "Is the feature added in the current distributed Tensorflow?", "Any progress on this? Merging the files in HDFS is a very expensive operation (and has some annoying side effects like adding /user/nobody/ to the path)", "Any progress on this?", "This is still a important feature for us, is there any progress?", "We're discussing switching the file format to Riegeli: https://github.com/google/riegeli , but no concrete plans yet and no ETA."]}, {"number": 7010, "title": "Check None for elapsed_secs in LoggingTensorHook", "body": "Fixed edge case when `elapsed_secs` is `None` that results in the following:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/opt/python/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 861, in emit\r\n    msg = self.format(record)\r\n  File \"/usr/local/opt/python/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 734, in format\r\n    return fmt.format(record)\r\n  File \"/usr/local/opt/python/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 465, in format\r\n    record.message = record.getMessage()\r\n  File \"/usr/local/opt/python/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 329, in getMessage\r\n    msg = msg % self.args\r\nTypeError: float argument required, not NoneType\r\nLogged from file basic_session_run_hooks.py, line 183\r\n```", "comments": ["Just wanted to add a note here: this will be fixed from an internal merge. (you guys are ahead of me)"]}, {"number": 7009, "title": "Corrected first row in boston data", "body": "", "comments": []}, {"number": 7008, "title": "Exported Tensorflow Model not Preserving Placeholder Shape", "body": "I am using exporter from tensorflow.contrib.session_bundle to save out my model:\r\n\r\n```\r\nx = tf.placeholder(tf.float32, (None,) + (100, 200) + (1,))\r\n....\r\nsaver = tf_saver.Saver(sharded=True)\r\nmodel_exporter = exporter.Exporter(saver)\r\nmodel_exporter.init(\r\n    sess.graph.as_graph_def(),\r\n    named_graph_signatures={\r\n        'inputs': exporter.generic_signature({'images': x}),\r\n        'outputs': exporter.generic_signature({'classes': y})})\r\n```\r\nand then I load the model back in (session_bundle from tensorflow.contrib.session_bundle):\r\n\r\n```\r\nsess, meta_graph_def = session_bundle.load_session_bundle_from_path(input)\r\n```\r\nHowever when I inspect the Placeholder tensor corresponding to the input x, I see no shape information:\r\n```\r\n> sess.graph.get_tensor_by_name(input_name)\r\n<tf.Tensor 'Placeholder:0' shape=<unknown> dtype=float32>\r\n```\r\nIs this by design or is there some bug causing the shape to be lost?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nReport from SO: http://stackoverflow.com/questions/40733752/exported-tensorflow-model-not-preserving-placeholder-shape\r\n\r\n### Environment info\r\nOperating System:\r\nUsing `gcr.io/tensorflow/tensorflow:latest-devel-gpu` docker iamge\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n# ls -al /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root    558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root    415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root    775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 1000 users       13 Jul 27 05:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 1000 users       17 Jul 27 05:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxrwxr-x 1 1000 users 79337624 Jul 27 05:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-rw-r-- 1 1000 users 69756172 Jul 27 05:53 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\n4d924e796368163eff11a8151e8505715345f58d\r\n```\r\n2. The output of `bazel version`\r\n```\r\n# bazel version\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nExtracting Bazel installation...\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n```\r\n", "comments": ["cc @nfiedel who worked on session bundle", "If I load the meta file:\r\n```python\r\nmeta_graph_def = meta_graph_pb2.MetaGraphDef()\r\nmeta_graph_def.ParseFromString(file_io.read_file_to_string(...))\r\n```\r\nI do see the shape saved on the `_output_shapes` key on the `graph_def`:\r\n```\r\nnode {\r\n  name: \"Placeholder\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n          dim {\r\n            size: 225\r\n          }\r\n          dim {\r\n            size: 300\r\n          }\r\n          dim {\r\n            size: 1\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nHowever that value is not on the actual shape attribute not in the `meta_info_def`:\r\n```\r\n  op {\r\n    name: \"Placeholder\"\r\n    output_arg {\r\n      name: \"output\"\r\n      type_attr: \"dtype\"\r\n    }\r\n    attr {\r\n      name: \"dtype\"\r\n      type: \"type\"\r\n    }\r\n    attr {\r\n      name: \"shape\"\r\n      type: \"shape\"\r\n      default_value {\r\n        shape {\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\nThe issue seems like it might be in session_bundle.py:\r\n```python\r\n  collection_def = meta_graph_def.collection_def\r\n  graph_def = tf.GraphDef()\r\n  if constants.GRAPH_KEY in collection_def:\r\n    # Use serving graph_def in MetaGraphDef collection_def if exists\r\n    graph_def_any = collection_def[constants.GRAPH_KEY].any_list.value\r\n    if len(graph_def_any) != 1:\r\n      raise RuntimeError(\r\n          \"Expected exactly one serving GraphDef in : %s\" % meta_graph_def)\r\n    else:\r\n      graph_def_any[0].Unpack(graph_def)\r\n      # Replace the graph def in meta graph proto.\r\n      meta_graph_def.graph_def.CopyFrom(graph_def)\r\n```\r\nThe serving graph seems to be missing the `_output_shapes` attr which means this value gets copies over and not used by `importer` to restore the shape.", "So I tracked this issue down to:\r\n```python\r\nsaver = save.Saver(sharded=True)\r\nmodel_exporter = exporter.Exporter(saver)\r\nmodel_exporter.init(\r\n  sess.graph.as_graph_def(),\r\n  ...\r\n)\r\n```\r\nChanging the `as_graph_def()` to: `sess.graph.as_graph_def(add_shapes=True)` solves this problem.\r\nThis line of code seems some what duplicated since a similar line exists in `saver`:\r\n```python\r\n    return export_meta_graph(\r\n        filename=filename,\r\n        graph_def=ops.get_default_graph().as_graph_def(add_shapes=True),\r\n```", "@zheng-xq for any comments about the default exporter behavior. Should we fix this?", "Session_bundle is deprecated. Please use SavedModel for exporting models instead.", "@gunan: https://github.com/tensorflow/tensorflow/issues/7962 \ud83d\ude04 . Migration path is not totally clear."]}, {"number": 7007, "title": "Native TF, Keras (Backend TF) and Lasagne(Theano) performance comparison", "body": "Hey everyone,\r\n\r\nI created the exact same network with native tensorflow, keras(tensorflow) and lasagne(theano) but after many hours of testing using number of different parameters, still couldn't figure out why keras and lasagne outperforms the native tensorflow, do better convergences and produce better(slightly but better) results.\r\n\r\nThe score difference is always like\r\nKeras with Tensorflow: ~0.9830 - 0.9885\r\nLasagne with Theano: ~0.9833 - 0.9885\r\nTensorflow Native ~0.9765 - 0.9830\r\n\r\nIf anyone feels like digging in, my environment is:\r\n\r\n> Python 3.5.2 -Anaconda / Windows 10\r\n> CUDA: 8.0 with cuDNN 5.1 \r\n> Keras 1.2.1\r\n> Tensorflow 0.12.1\r\n> Nvidia Geforce GTX 860M\r\n\r\nand **keras.json** file:\r\n\r\n```\r\n{\r\n    \"image_dim_ordering\": \"tf\", \r\n    \"epsilon\": 1e-07, \r\n    \"floatx\": \"float32\", \r\n    \"backend\": \"tensorflow\"\r\n}\r\n```\r\n\r\nAnd you can copy and execute following files in my repo:\r\n[Keras code](https://github.com/emrahyigit/deep/blob/master/keras_cnn_mnist.py)\r\n[Native Tensorflow code](https://github.com/emrahyigit/deep/blob/master/tf_cnn_mnist.py)\r\n[Mnist](https://github.com/emrahyigit/deep/blob/master/mnist.py)\r\n\r\n", "comments": ["Three full days and I am going to kill myself. \ud83d\udc4d\r\n\r\nThe problem was due to incorrect use of keep_prob parameter of the dropout layer as I should have fed this parameter with different values on train and test process."]}, {"number": 7006, "title": "can not plot embedded data points using tensorboard embedding chart", "body": "I tried to plot original input data points and embedded data points using tensorboard embedding function.\r\nI did basically everything according to the official instruction.\r\nThere are three tensors in my graph, namely input, embedding, and embedded.\r\nand embedded = tf.MatMul(input, embedding)\r\nI failed to plot the embedded tensor, even thought I provide metadata file , tensorboard seems just ignore it.\r\nHowever I did plot the input tensor, tensorboard has no problem locating metadata file, it shows all the points in 3D with labels I provided in the metadata file.\r\nI also succeeded to plot the embedding tensor, So does anyone know how to solve this?", "comments": ["here is a more detailed version, I omitted training calls to make it more readable\r\n\r\n```\r\ngraph = tf.Graph()\r\n        with graph.as_default():\r\n            input = tf.placeholder(tf.float32, shape=[None, n], name='input')\r\n            embedding = tf.Variable(tf.random_normal([n, embedding_size]), name='embedding')\r\n            embedded = tf.matmul(input, embedding, name='embedded')\r\n            weights = tf.Variable(tf.random_normal([128, 1], stddev=0.01, name='random-normal'), name='weights')\r\n            logits = tf.matmul(embedded, weights, name='logits')\r\n            labels_batch = tf.placeholder(tf.float32, name='y-input')\r\n            cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, labels_batch, name='cross_entropy'), name='cross_entropy_mean')\r\n            train_op = tf.train.AdamOptimizer().minimize(cross_entropy)\r\n            tmp_tensor = tf.Variable(valid_data)\r\n            result = tf.matmul(tmp_tensor, embedding, name='res')\r\n        with tf.Session(graph=graph) as sess:\r\n            projector_config = projector.ProjectorConfig()\r\n            add_embedding = projector_config.embeddings.add()\r\n            add_embedding.tensor_name = result.name\r\n            add_embedding.metadata_path = metadata_path\r\n            summary_writer = tf.summary.FileWriter(log_dir + '/projector', sess.graph)\r\n            projector.visualize_embeddings(summary_writer, projector_config)\r\n            sess.run(tf.global_variables_initializer())\r\n            sess.run(tf.local_variables_initializer())\r\n            sess.run(result)\r\n            saver = tf.train.Saver()\r\n            saver.save(sess, log_dir + '/model.ckpt')\r\n            summary_writer.close()\r\n```", "It seems that intermediate results are NOT saved in a checkpoint file.\r\nI double checked to make sure that input tensor and embedded tensor have same number of rows which is the number of rows in metadata file", "@robbine : I'm not sure I fully understand your problem, but just to be clear, [`tf.train.Saver`](https://www.tensorflow.org/api_docs/python/state_ops/saving_and_restoring_variables#Saver) saves only variables to the checkpoint - it does not save the output of all nodes in the graph.\r\n\r\nDoes that answer your question?", "Yes, I managed to save embedded results by copying it to another variable, \r\ncopy_op = result.assign(embedded)\r\nand it solved my problem. Thanks"]}, {"number": 7005, "title": "TypeError: inputs must be a sequence occure in Bidirectional RNN", "body": "Sorry to bother.\r\nI just use the bidirectional rnn now.\r\nHowever, I just encounter some problem.\r\n```terminal\r\nTraceback (most recent call last):\r\n  File \"test1.py\", line 19, in <module>\r\n    sequence_length=length(sequence),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 530, in bidirectional_rnn\r\n    raise TypeError(\"inputs must be a sequence\")\r\nTypeError: inputs must be a sequence\r\n```\r\nI don't know what the problem it is?\r\nThe following show my code.\r\n```python\r\nimport tensorflow as tf\r\n\r\nmax_length = 100\r\nframe_size = 64\r\nnum_hidden = 200\r\n\r\ndef length(sequence):\r\n    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\r\n    length = tf.reduce_sum(used, reduction_indices=1)\r\n    length = tf.cast(length, tf.int32)\r\n    return length\r\n\r\nsequence = tf.placeholder(tf.float32, [None, max_length, frame_size])\r\noutput, state = tf.nn.bidirectional_rnn(\r\n    tf.nn.rnn_cell.GRUCell(num_hidden),\r\n    tf.nn.rnn_cell.GRUCell(num_hidden),\r\n    sequence,\r\n    dtype=tf.float32,\r\n    sequence_length=length(sequence),\r\n)\r\n```\r\n\r\nThe code I refer to the [blog](https://danijar.com/variable-sequence-lengths-in-tensorflow/) that other issue demonstrate and recommond.\r\n\r\n---\r\nI try to figure out why the exception was raise.\r\n```python\r\nimport tensorflow as tf\r\nimport collections\r\n\r\nself.input_sentences = tf.placeholder(tf.float32, [None, sequence_length, embedding_size])\r\nprint \"six: \", isinstance(self.length, collections.Sequence)\r\n```\r\nThis is the part of my code.\r\nHowever, the result is shown below:\r\n```\r\nsix:  False\r\n```\r\nIs somebody encounter such this problem?\r\nor how can I solve this?\r\n\r\nI use CPU only, the OS is ubuntu 16.04\r\nthe version of tensorflow is 0.12.1", "comments": ["This question is probably better posed on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests. Thanks.", "I found the solution at last.\r\nThe following show the revised code:\r\n```python\r\nimport tensorflow as tf\r\n\r\nmax_length = 82\r\nframe_size = 2\r\nnum_hidden = 64\r\n\r\ndef length(sequence):\r\n    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\r\n    length = tf.reduce_sum(used, reduction_indices=1)\r\n    return length\r\n\r\n_input = tf.placeholder(tf.float32, [None, max_length, 100])\r\n_taggg = tf.placeholder(tf.float32, [None, max_length, 2])\r\n\r\noutput, state, _ = tf.nn.bidirectional_rnn(\r\n    tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(num_hidden)]),\r\n    tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(num_hidden)]),\r\n    tf.unpack(tf.transpose(_input, perm=[1, 0, 2])),\r\n    dtype=tf.float32,\r\n    sequence_length=length(_taggg),\r\n)\r\n```\r\nYou should unpack the input as the list, and the list will be transfer to the sequence by tensorflow automatically. \r\nThe last but not least, output of the function is three args. ", "@SunnerLi \r\nWhat is `perm=[1, 0, 2]` ?\r\nThanks!", "it means;\r\ndim0 -> dim1\r\ndim1 -> dim0\r\ndim2 -> dim2"]}, {"number": 7003, "title": "Fix breakages in Python 3.5 tests by using int type in indices", "body": "Previously, float and np.float64 types were used in these places,\r\ncausing a certain version of Python 3.5 + NumPy to error out.", "comments": ["Also TESTED: Experimental Linux CPU Python3.5 PIP build (Jenkins login required to view log):\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-linux-python35-pip/1/console"]}, {"number": 7002, "title": "Replace deprecated api links in deep_cnn tutorial issue #7001", "body": "Replace three deprecated tf.train summary operations by the most close alternatives in the latest TF release found under tf.summary instead.\r\n\r\nI am not sure if there is an automatic tool which should update these links or not, but I just wanted to try to help.", "comments": ["Can one of the admins verify this patch?", "Thank you, @medhat-omr. PR merged.", "Thanks everyone. \n\nBest regards,\nMedhat \n\nSent from my iPhone\n\n> On Jan 21, 2017, at 9:08 PM, Shanqing Cai <notifications@github.com> wrote:\n> \n> Merged #7002.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n"]}, {"number": 7001, "title": "All links to summary operations in Convolution NN tutorial are broken", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/41782812/how-do-i-find-alternative-methods-in-tensorflow-latest-release-to-deprecated-one/41783019#41783019\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nFor example, by following the link at summary_image found under the [Model Inputs](https://www.tensorflow.org/versions/master/tutorials/deep_cnn/#model_inputs) section. You will effectively land at https://www.tensorflow.org/versions/master/api_docs/python/train/ instead of https://www.tensorflow.org/versions/master/api_docs/python/train/#image_summary since image_summary under tf.train module is now deprecated according to [Release 0.12.0 release notes](https://github.com/tensorflow/tensorflow/releases/tag/0.12.0-rc0)\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Done through PR #7002 "]}, {"number": 7000, "title": "hello there, i am trying to instal tensorflow on mac. Even though the installation is successful when I import tensor flow gives me this error:", "body": "ImportError: Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Symbol not found: ___sincos_stret\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\nany idea??\r\n", "comments": ["Hi, could you ask this on Stackoverflow instead? This list is for bugs in TensorFlow and feature requests"]}, {"number": 6999, "title": "Please update Windows TF to support python 3.6", "body": "The current .whl does not support python 3.6.  Please update to support the latest version of python.  Thanks!\r\n\r\nC:\\WINDOWS\\system32>python --version\r\nPython 3.6.0\r\n\r\nC:\\WINDOWS\\system32>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl\r\ntensorflow-0.12.1-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\r\n\r\n", "comments": ["BTW, you can just rename the wheel to say `cp36` instead of `cp35`, that's what official 3.6 release for Mac does", "The compiled TensorFlow Python extension for Python 3.5 depends on `python35.dll`, so I doubt Yaroslav's suggestion will work.\r\n\r\nIt should be possible to build TensorFlow for Python 3.6 manually by following the instructions [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) on a machine that has Python 3.6 installed. We haven't yet set up automated build and testing resources to make an official Windows/Python 3.6 distribution, but hope to do that when time permits.", "@AngledLuffa - could you try the suggestion here: https://github.com/tensorflow/tensorflow/issues/6533#issuecomment-273881004 ?\r\n\r\nAs pointed out, we are working on, but haven't completed testing on the release for 3.6\r\n", "I had to install protobuf and wheel myself, but it was happy installing that version of TF.  However, it gives me the dreaded `No module named \"_pywrap_tensorflow\"` error.  This is on a system where python 3.5 is able to run TF without hitting that error.", "@yifeif It looks like this also works for windows.\r\nCan we update our pypi repository with windows python 3.3 to 3.6 wheel files?", "Testing renaming the wheel, with:\r\n- Windows 10\r\n- Python 3.6 64-bit (only Python installed)\r\n- TensorFlow GPU 0.12r\r\n- Microsoft Visual C++ 2015 Redistributable Update 3 installed\r\n\r\nstill gives the [known error](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#pip-installation-on-windows) referred  by @AngledLuffa related to `MSVCP140.DLL` . At least on my side it's a nonsolution to rename the wheel.\r\n\r\n@asimshankar the pypi installation yields the same error (beginning of the log only)\r\n```\r\nC:\\WINDOWS\\system32>pip install -i https://testpypi.python.org/pypi tensorflow\r\nCollecting tensorflow\r\n  Downloading https://testpypi.python.org/packages/db/d2/876b5eedda1f81d5b5734277a155fa0894d394a7f55efa9946a818ad1190/tensorflow-0.12.1-cp36-cp36m-win_amd64.whl (13.7MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.8MB 50kB/s\r\nRequirement already satisfied: numpy>=1.11.0 in c:\\users\\adriano\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\adriano\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\adriano\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: protobuf>=3.1.0 in c:\\users\\adriano\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: setuptools in c:\\users\\adriano\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from protobuf>=3.1.0->tensorflow)\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-0.12.1\r\n\r\nC:\\WINDOWS\\system32>python\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Adriano\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n```", "Same cluster of issues.\r\nRenaming wheel does not help.\r\n\r\n`tensorflow_gpu-0.12.1-cp36-cp35m-win_amd64.whl is not a supported wheel on this platform.`\r\n\r\nWindows 10 \r\nGPU TF\r\nPython 3.6.0 | Anaconda 4.3.0 (64-bit)\r\n\r\n\r\nTF installed and ran the basic test code\r\n...Then after CUDA install it failed\r\nTried putting C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin in path as suggested here: \r\nhttps://github.com/tensorflow/tensorflow/issues/6235\r\n\r\n(I was previously working on Mac CPU TF 0.1, install was simpler!) \r\n\r\n\r\n\r\n", "As @mrry pointed out, renaming the whl file does not help in windows.\r\nMany of the tools we depend on(virtualenv, pip, bazel, libraries we depend on, etc.) has many many more bugs, and much less support on windows, so getting anything to work on windows takes more time for us.\r\nWe are working to get this to work, but we do not have the built packages yet.\r\n\r\nIn the meantime, if you need the pip package for windows for a python version other than py 3.5, you will need to build from sources. We are sorry for the inconvenience.", "Hi gunan had a go at building from source as per \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\n\r\nJust don't have the time to sort out all of the dependencies, ie SWIGWIN-3.0.12 not 3.0.10 etc.. \r\nCould you please let me know when Python 3.6 support wheel for Windows 10 is updated. \r\nWill wait until then. \r\nThanks in advance.", "If you are using anaconda distribution, you can do the following to use python 3.5 on the new environnement \"tensorflow\": \r\n\r\n```\r\nconda create --name tensorflow python=3.5\r\nactivate tensorflow\r\nconda install jupyter\r\nconda install scipy\r\npip install tensorflow\r\n# or\r\n# pip install tensorflow-gpu\r\n```\r\n\r\nIt is **important** to add `python=3.5` at the end of the first line, because it will **install** Python 3.5.", "Thanks AlexisTM that worked.\r\nTested: https://www.tensorflow.org/get_started/basic_usage\r\nResults:\r\n![p100 tf test](https://cloud.githubusercontent.com/assets/25624072/22764367/635421fa-eebe-11e6-8fbd-53e3bd6679f7.jpg)\r\n\r\n\r\n", "I had the exact same problem yesterday ;)", "I am using python 3.6 (anaconda ) . All the steps worked for me except for the last one - \r\npip install tensorflow\r\nI get the following output on console.\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow. Any idea what should I do to make it work ?", "@tswapnil That's the reason for the `conda create --name tensorflow python=3.5`. Indeed, it installs python 3.5 because tensorflow is **not** compiled for Python 3.6. \r\n\r\nDid you that exact command or did you changed `python=3.5` to `python=3.6`. If you changed it, that's the reason it fails.", "I changed it to python=3.6 . How should I change it back ? Because it took a lot of time installing and I don't wish to do that all again for a new Name . Is there a way to change the prefix ?", "Very simple, check the cheatsheet, delete your environnement and reinstall it. (It can't reuse same packages anyway as those are for 3.6 version and not 3.5.)\r\n\r\nhttps://conda.io/docs/using/cheatsheet.html", "Thanks @AlexisTM  . ", "You're welcome ;)", "Hi AlexisTm.\r\nI had some side issues with other things not working the way I would like and wanted to share with you what I did.\r\nI downloaded the anaconda python 3.5 from here:\r\nhttps://repo.continuum.io/archive/index.html\r\nfor my install I used:\r\nAnaconda3-4.2.0-Windows-x86_64\r\nI then did a Pip upgrade from the cmd window:\r\npip - m pip install --upgrade pip\r\nI was then able to run the command but added to the end of the string \"--ignore-installed\"\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl --ignore-installed\r\nThis worked perfectly (so far).", "Surprisingly, I was able to build (compile) latest TensorFlow 1.0.0rc2 from sources on Windows 10 for Python 3.6 64 bit (Anaconda 4.3). I used free [Visual Studio Build Tools](https://blogs.msdn.microsoft.com/vcblog/2016/11/16/introducing-the-visual-studio-build-tools/), cmake-3.8.0-rc1, and swigwin-3.0.12. \r\nI followed instruction for CMake [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md), technically it was just running two commands - CMake first and MSBuild second.\r\n\r\nTo compile, it required me to make two changes in sources, which I found based on compilation errors.\r\nOne was adding `#include <intrin.h>` to `tensorflow\\core\\platform\\windows\\cpu_info.h` inside of `#ifndef` block because of the following error:\r\n> error C3861: '__cpuidex': identifier not found \r\n\r\nSecond was commenting out procedures `_mm256_extract_epi32` and `_mm256_insert_epi32` in `tensorflow\\core\\platform\\windows\\intrinsics_port.h` due to error\r\n\r\n> error C2169: '_mm256_extract_epi32': intrinsic function, cannot be defined \r\n\r\nAfter that, I was able to successfully run\r\n> MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\nWhich gave me `tensorflow-1.0.0rc2-cp36-cp36m-win_amd64.whl`, that I successfully installed into Python using pip. It seems to be working - I was able to run couple of jupyter notebooks, including udacity assignments.\r\n\r\nBTW, compilation took about 2-3 hours on my week laptop (i5-4200U).\r\nUpdate - I compiled from master branch, so it gave me 1.0.0rc2, while I see there is 1.0.0 release available.", "@pavelbulanov Thanks for digging into this. Can you please open a new issue to track this problem? (@petewarden, it looks like there may be a slight issue with your recent change to `cpu_info.cc`.)", "Thanks Derek! I'll sync offline about the best way to address this, since our Windows cmake test passed with the original change. I'm guessing there's a slightly different setup here?", "@petewarden, Shall I open a new issue?\r\n\r\nThis is indeed a different setup, I'm using \"new\" Visual Studio Build Tools, which are related to Visual Studio 2017. They call it \"Build Tools for Visual Studio 2017 RC\" in the [download](https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017-rc) section. It's free.\r\n\r\nAs for the issue, I see [commit](https://github.com/tensorflow/tensorflow/commit/a672cb166dae93ae955c1d38f3de8903dd242373) made that introduced usage of `__cpuidex` function. And based on [this](https://msdn.microsoft.com/en-us/library/hskdteyh.aspx) MSDN link, I discovered that Header file `<intrin.h>` has to be included.  That's why I put it inside `cpu_info.h` (not sure if it's exactly perfect place).\r\n\r\nThe second issue with `_mm256_extract_epi32` may be connected with the same reason, I don't know. \r\nIn the meantime, `platform/windows/intrinsics_port.h` [references](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/intrinsics_port.h#L22) `immintrin.h`, but I don't see it's actually included anywhere. I assume it may correlate with `instrih.h` inclusion (based on [this](https://msdn.microsoft.com/en-us/library/26td21ds.aspx) link). Meanwhile, `_mm256_extract_epi32 `can be found in `avxintrin.h` which may be is included by `immintrin.h`.\r\n\r\n> // the following avx intrinsics are not defined on windows\r\n> // in immintrin.h so we define them here.", "Thanks for the update @pavelbulanov ! It's helpful to know this is broken with the new build tools, and opening a new issue on this would be useful.", "I see this post here is quite recent. Could you tell me if it is still not possible to install tensorflow with Python 3.6? According to the official documentation 3.5 is required, but I would like to run it with 3.6.", "@tomwaitforitmy  You have to build from source currently.", "Any ETA on a 3.6 version of 1.0?", "1.0 will not have python 3.6 support on windows.\r\nWe will check the adoption numbers for python itself to see if we will add support for TF 1.1", "If we could update tensorflow for python 3.6 that would be great. Good work!\r\n\r\nThanks and good job!", "Updated Conda enviroment and surprise... still no tensorflow for Python 3.6 on Windows not even the latest Tensorflow 1.0.1\r\n\r\ntried \r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.1-cp36-cp36m-win_amd64.whl\r\n\r\nbut the repo that should be for Python 3.6 is missing", "You can still install is on 3.5, with the last version of conda.\n\nLe ven. 10 mars 2017 18:08, Silvio Marano <notifications@github.com> a\n\u00e9crit :\n\n> Updated Conda enviroment and surprise... still no tensorflow for Python\n> 3.6 on Windows not even the latest Tensorflow 1.0.1\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-285726083>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGp06GJ75_un5MO8PkFRnJLUWEG0HX5bks5rkYOfgaJpZM4LqK-s>\n> .\n>\n", "> You can still install is on 3.5, with the last version of conda.\r\n\r\nSure, but isn't a pleasant solution use the older 3.5", "Whatever, you can make your TensorFlow application and run it seamlessly on a Linux machine when you deploy it. No issue (:", "@SMH17 Some folks are requesting Python 3.6 support on Windows but as @gunan [mentioned](https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-283158964) TensorFlow currently does not support it although it may be considered for 1.1r based on user adoption numbers. \r\n\r\nYour option as of now is to build from source. Be sure they are hearing and considering all the requests folks are making so bear with them. If you decide to go for building from source and have questions feel free to ask as any other doubts. ", "@AlexisTM great suggestion for installing python=3.5\r\nOne edit suggestion to your solution would be to add the conda command to remove a previously created ENV with the name tensorflow.\r\n\r\n\"conda env remove --name tensorflow\"", "I had a go at building this myself and succeeded. I used python 3.6, visual studio 2015 update 3 (the update 3 is important otherwise you will get compiler errors), and cmake 3.6.x (not cmake 3.7.x as this introduces a bug with quotes somewhere).\r\n\r\nThere was also this weird bug in `array_ops.cc` where visual studio complains that <code>`</code> cannot be in a raw string which is totally bogus, adding a new line at line 4951 fixes it.\r\n\r\nThe entire build process took 2 hours on an i7 3770k and the directory at the end of the build process was 3gb in size.\r\n\r\nHowever a much simpler way is as @yaroslavvb suggests, that is to simply use the python 3.5 install and convert this to a python 3.6 install. This is actually possible by opening up `_pywrap_tensorflow.pyd` and replacing the python 3.5 import with a python 3.6 import. You can do so via a hex editor. Find `python35` in the dll, and replace the only instance with `python36`. I can import tensorflow through this method.\r\n\r\n", "Did you run the tests when you compiled it yourself/by editing the hex file? Why do you desperately want Python 3.6 when you can have both 3.5 and 3.6 via Anaconda?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/6999#issuecomment-278459224", "@AlexisTM No, but since the mac version is done this way I'm guessing the python version works too. Python 3.6 comes with some nice features although they are not necessary for me right now, but more importantly installing python 3.5 again and migrating all my libraries takes a lot of work, and it is a very annoying thing to downgrade just because of one library.", "@stevenxxiu With Conda, installing the environment is 3 commands and then wait the installation to finish. That's pretty fast. But you wot have those Python features you always dreamed of :/", "not official but http://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow\r\n\r\nalso don't mean to be rude, but\r\n\r\n> 1.0 will not have python 3.6 support on windows.\r\n> We will check the adoption numbers for python itself to see if we will add support for TF 1.1\r\n\r\nthis is unacceptable, guys.", "@imbaczek It's a very bad choice. The adoption of Python 3.6 depends on how the others software will support It, and  not viceversa. I'm with Python 3.5 exactly because of the lack of official support for newer 3.6. ", "please AlexisTM , i have a problem. i have successfully processed the 3 first line after i wrote   \r\npip install tensorflow\r\n# or\r\n# pip install tensorflow-gpu\r\n\r\ni received the following error failed to create process.\r\n\r\nplease help", "Try to execute one or the other ;)", "I have Anaconda 4.3.1 (64bit) with Python 3.6 and couldn't find a  workable version of Tensorflow anywhere. I tried some workarounds and unofficial builds I found here, it just didn't work out for me. Frustrated as I was after struggling for a few days, I decided to build one myself. It turned out the building was very smooth, I didn't hit any issue other than taking a few hours. Here is my build env:\r\n\r\n- tensorflow 1.1.0rc2 with GPU\r\n- Python 3.6/Anaconda 3.4.1\r\n- CUDA Tookit 8.0 and cuDNN 5.1 \r\n- Swigwin 3.0.12\r\n- CMake 3.8.0\r\n- Windows 10 pro.\r\n\r\nThe build works just fine for me. I can give it to anyone who asks.\r\n\r\nThanks,", "Mr. Christoph Gohlke added a Tensorflow wheel a few days ago for Python 3.6:\r\n\r\nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow\r\n\r\nIt's for both GPU and CPU. Check it out.\r\n\r\nedit: Caveat emptor: it wasn't compiled to use SSE (yes, not even the first SSE!), so it might come to be much slower than a standard compile:\r\n\r\n```\r\nW d:\\build\\tensorflow\\tensorflow-r1.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.                                                                \r\nW d:\\build\\tensorflow\\tensorflow-r1.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.                                                               \r\nW d:\\build\\tensorflow\\tensorflow-r1.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.                                                               \r\nW d:\\build\\tensorflow\\tensorflow-r1.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.                                                             \r\nW d:\\build\\tensorflow\\tensorflow-r1.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n```", "interesting, I have solved the problem by @oldmud0 suggestions. But as he or she said, the IDE shows the warning as above. It takes more time to compile", "Still no 3.6 support four months later? :/\r\nIs there anything to help with?\r\n\r\nWill use the linux subsystem for now.", "@black-snow There are multiple simple solutions : \r\n\r\n- https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-278459224 Anaconda to manage environnements\r\n- Recompile yourself\r\n- (hex)Edit the executable to change the link to the Python 3.6 dlls", "I tried installing TensorFlow provided by Christoph Gohlke, but any attempt to import the package produces the following error:\r\n\r\n```\r\n  File \"C:\\Users\\Andy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Andy\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Andy\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\nI tried it with both Python 3.6.0 and 3.6.1 and get the very same error.\r\nDoes anyone know how to solve this?", "@andyatgh Are you using CPU or GPU? Be sure you have [MSVC 2015 x64 redist](https://www.microsoft.com/en-us/download/details.aspx?id=53587) installed as well as Numpy *with* the Intel Math Kernel Library DLLs installed (wheel available on the [same site](http://www.lfd.uci.edu/%7Egohlke/pythonlibs/#numpy)). If it's the GPU version, well I've never gotten it to work, to be honest.", "@andyatgh You are already using Anaconda, just use Python 3.5 to make your program and wait for a solution. \r\n\r\nThe only other solution is to recompile from source. ", "@oldmud0 It's the GPU version that fails, with all dependencies properly installed. the CPU build works well. Not sure what goes wrong there..\r\n@AlexisTM I wanted to combine all machine learning stuff under one environment for simplicity, especially that people also [managed](https://github.com/pytorch/pytorch/issues/494#issuecomment-304439658) to build PyTorch under Windows in Python 3.6.\r\n\r\nBut anyway, it turned out that hex-editing the library file works well. I created a separate environment with Python 3.5, installed tensorflow-gpu there, then copied the content of tensorflow folders from `site-packages` into the environment with Python 3.6.1. I also manually updated all dependencies, just in case. Then I changed `python35` into `python36` in `_pywrap_tensorflow_internal.pyd` and everything worked.", "@andyatgh Good news! We finally have a procedure for people wanting to work with Python 3.6 only ! \\o/\r\n", "I confirm what andyatgh has said. The 3.6 GPU version at this link http://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow isn't compiled correctly but CPU version seems working like the official Tensorflow release. However Tensorflow staff need to support Python 3.6 and Windows out of the box without force the users to manual compilations and workaround.", "@SMH17 Why do they have to invest money in the Python3.6 version? How much do you pay to use TensorFlow?\r\n", "I also got the \"ImportError: DLL load failed: The specified module could not be found.\" error with Python 3.6.1 (CPython, not Anaconda) and Christoph Gohlke's May 3 version of TensorFlow-GPU (tensorflow_gpu\u20111.1.0\u2011cp36\u2011cp36m\u2011win_amd64.whl). Followed advice from a number of places on the Net and tried reinstalling MSVC 2015 x64 redist, etc. without success.\r\nFinally ==> replaced cuDNN 5.1 with 6.0 and everything just magically worked! Give it a try and it might work for you.", "@AlexisTM  'Good news! We finally have a procedure for people wanting to work with Python 3.6 only ! \\o/'\r\n\r\nWhere can I find the solution? Thank you  =)", "@AlexisTM I don't want to be polemic with you but Tensorflow isn't some amateur project, It's reference framework for deeplearning of a big company named Google, and if I invest time to develop my projects on Tensorflow instead of competitor solutions, I would like everything work as It should. It's perfectly legit asking for Python 3.6 official support. I'm not insulting, threatening or holding anyone hostage. So, what's your problem with my(and not only my) request?", "@Moondra Using those binairies https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-298116252 and this fix https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-304495235 gives you **Python3.6 support**.\r\n\r\nThe easy workaround using **Python3.5**, plug'n'play, without breaking any Python installation is using Conda following this https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-278459224\r\n\r\n@SMH17 1140 issues this day. Even if it is Google, there are humans behind. Just a team with the pressure of \"Try not to break anything, lot of people rely on our software\". Those need to be fixed before Python3.6. Only a low percentage of users especially needs Python3.6 under Windows. Big customers are most likely using Linux servers. If I was managing this project, I would probably let this minor issue on the side and focus on major issues. I still agree with you that there are only workaround but **no real solution** for Windows and Python3.6.", "@AlexisTM The fact there are less Windows based companies explain why they have given higher priority to Linux support(that is pretty obvious) but doesn't explain why according to you I shouldn't ask for official support showing at the company staff my interest in Python 3.6 support, giving a feedback about this matter. \r\n", "@SMH17 It is not giving priorities, it is seamless for Linux, while is it DLL dependent on Windows. It is simply working on Linux, that's why it works.", "I can confirm that the hack works to copy the python 3.5-installed tensorflow packages from site-packages, and copy it back into python 3.6 after hex-editing the pywrap_tensorflow_internal.pyd, to change python35->python36 .\r\n\r\nStill it would be great if this package was kept current, a lot of people do want to use Windows platform but it's very hard to make a cohesive environment when different ML packages are only working under pythonX.X or pythonY.Y", "After installing it on win7 x64 via ```https://testpypi.python.org/packages/db/d2/876b5eedda1f81d5b5734277a155fa0894d394a7f55efa9946a818ad1190/tensorflow-0.12.1-cp36-cp36m-win_amd64.whl```\r\nIt seems to finish installation successfully, however anytime I try to import tensorflow in my python code I see the following error:\r\n```\r\nimport tensorflow as tf\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```", "@pmunin Check the message above by @Carmezim, that package doesnt work since it was just a renamed 3.5 package. I'd recommend going through the hack that was described earlier in the thread, of installing on 3.5, upgrading to 3.6 and hex editing one .pyd file.\r\n\r\nedit: Hex editing what you have installed now will probably work. Not sure.", "Exciting news folks, Python 3.6 support will be soon available from release 1.2 forward so bear with the TensorFlowers just a bit more https://github.com/tensorflow/tensorflow/pull/10356\r\n\r\n@pmunin @emulvihill is correct in his assessment. Simply renaming the wheel won't yield any result. If you can wait I strongly suggest the official release as soon it's available as stated above. TensorFlow can only provide support for official builds. If you need only CPU support you can download the [nightly build](https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows,PY=36/194/). ", "Excellent, thank you!\n\nOn Thu, Jun 1, 2017 at 11:13 AM, gunan <notifications@github.com> wrote:\n\n> Closed #6999 <https://github.com/tensorflow/tensorflow/issues/6999> via\n> #10356 <https://github.com/tensorflow/tensorflow/pull/10356>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6999#event-1106552123>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADQMWVzJ0yqzf9noPjVqOD8uELBcQ8IBks5r_v9OgaJpZM4LqK-s>\n> .\n>\n", "As @Carmezim said, the CPU nightly pip package is ready.\r\nGPU nightly pip package should be ready later today.\r\n\r\nAs for pypi, the binaries wont be there until our next release, which should be sometime next week.", "Just found Tensorflow 1.1 working on python 3.6 for windows: [http://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow](http://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow). Unofficial apparently, but worked for me when I import tensorflow or tflearn in my code. They have scipy windows package there too.", "How exactly would I install the tf 1.2rc? I would like to play around with tf before the official version is released. I'm used to installing everything via the official python way. Thank you. ", "You can either use `--pre` flag with your pip command, or you can just set the version like this: `tensorflow==1.2.0rc2`", "@gunan  Thank you so much; If I want the GPU version I'm assuming I should add `gpu` as such:\r\n\r\n`tensorflow-gpu==1.2.0rc2`", "The latest successful build (followed by many days of failures!) of the win64, Python 3.6, GPU version (https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/) appears to have been on June 7th. Has anyone tried that one? Thanks!", "@PaulDufort As nobody talks since 10 days, I guess they all were able to install TensorFlow with Python3.6.", "Something went awry the first pip attempt, but I was able to get it going off pip for my Anaconda 4.4 environment running Python 3.6 (needed for the enum Flag functionality my application depends on). I'll share my console output...\r\n\r\n```\r\nC:\\>conda install tensorflow\r\nFetching package metadata .............\r\nSolving package specifications: .\r\n\r\nUnsatisfiableError: The following specifications were found to be in conflict:\r\n  - python 3.6*\r\n  - tensorflow -> python 3.5*\r\nUse \"conda info <package>\" to see the dependencies for each package.\r\n\r\n\r\nC:\\>python -m pip install tensorflow   --proxy=\"XXXXXXXXX\"\r\nCollecting tensorflow\r\n  Using cached tensorflow-1.2.0-cp36-cp36m-win_amd64.whl\r\nCollecting backports.weakref==1.0rc1 (from tensorflow)\r\n  Using cached backports.weakref-1.0rc1-py3-none-any.whl\r\nRequirement already satisfied: six>=1.10.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: bleach==1.5.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: werkzeug>=0.11.10 in c:\\anaconda3\\lib\\site-packages (from tensorflow)\r\nCollecting protobuf>=3.2.0 (from tensorflow)\r\nCollecting html5lib==0.9999999 (from tensorflow)\r\nCollecting markdown==2.2.0 (from tensorflow)\r\nRequirement already satisfied: numpy>=1.11.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: wheel>=0.26 in c:\\anaconda3\\lib\\site-packages (from tensorflow)\r\nRequirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages\\setuptools-27.2.0-py3.6.egg (from protobuf>=3.2.0->tensorflow)\r\nInstalling collected packages: backports.weakref, protobuf, html5lib, markdown, tensorflow\r\n  Found existing installation: html5lib 0.999\r\n    DEPRECATION: Uninstalling a distutils installed project (html5lib) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially u\r\nninstall the project.\r\n    Uninstalling html5lib-0.999:\r\n      Successfully uninstalled html5lib-0.999\r\nSuccessfully installed backports.weakref-1.0rc1 html5lib-0.9999999 markdown-2.2.0 protobuf-3.3.0 tensorflow-1.2.0\r\n\r\nC:\\>python\r\nPython 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2017-06-20 13:54:32.656253: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are\r\navailable on your machine and could speed up CPU computations.\r\n2017-06-20 13:54:32.656253: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are\r\n available on your machine and could speed up CPU computations.\r\n2017-06-20 13:54:32.660254: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are\r\n available on your machine and could speed up CPU computations.\r\n2017-06-20 13:54:32.665254: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these a\r\nre available on your machine and could speed up CPU computations.\r\n2017-06-20 13:54:32.670255: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows\\py\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these a\r\nre available on your machine and could speed up CPU computations.\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n>>>\r\n```", "We do not maintain the conda-forge copy of tensorflow.\r\nonly the packages we upload to pypi, and the packages we share from our website are officially supported.", "Nothing new to install the official Tensor Flow for py36 on a Windows 10 win32 machine ???", "@yaroslavvb comment from Jan 21 below helped me and it's working perfect now!  Thanks!\r\n\r\n\"BTW, you can just rename the wheel to say\u00a0cp36\u00a0instead of\u00a0cp35, that's what official 3.6 release for Mac does\"\r\n--\r\n\r\n\r\n @yaroslavvb ", "Could someone help me on this please?\r\n\r\nInstalled tensorflow gpu in Anaconda3 on Windows 10. It created the tensorflow ENV with no issues. And when I run this following I see no issues as well:\r\n```\r\n\r\nC:\\Users\\XXXX>activate tensorflow\r\n(tensorflow) C:\\Users\\XXXX>python\r\nPython 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\n>>> numpy.version.version\r\n'1.13.1'\r\n>>> import numpy\r\n>>> numpy.version.version\r\n'1.13.1'\r\n```\r\nHowever, when I run the same like this using python 3.6.1, I get the following error:\r\n\r\n```\r\nC:\\Users\\xxxx>python\r\nPython 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\numpy\\core\\__init__.py\", line 16, in <module>\r\n    from . import multiarray\r\nImportError: cannot import name 'multiarray'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\numpy\\__init__.py\", line 142, in <module>\r\n    from . import add_newdocs\r\n  File \"D:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\numpy\\add_newdocs.py\", line 13, in <module>\r\n    from numpy.lib import add_newdoc\r\n  File \"D:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\numpy\\lib\\__init__.py\", line 8, in <module>\r\n    from .type_check import *\r\n  File \"D:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\numpy\\lib\\type_check.py\", line 11, in <module>\r\n    import numpy.core.numeric as _nx\r\n  File \"D:\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\numpy\\core\\__init__.py\", line 26, in <module>\r\n    raise ImportError(msg)\r\nImportError:\r\nImporting the multiarray numpy extension module failed.  Most\r\nlikely you are trying to import a failed build of numpy.\r\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\r\nfiles not under version control).  Otherwise reinstall numpy.\r\n\r\nOriginal error was: cannot import name 'multiarray'\r\n```\r\nI had removed and reinstalled numpy also by the way:\r\n\r\n```\r\nC:\\Users\\xxxx>conda remove numpy\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for package removal in environment D:\\Anaconda3:\r\n\r\nThe following packages will be REMOVED:\r\n\r\n    astropy:      1.3.2-np112py36_0\r\n    blaze:        0.10.1-py36_0\r\n    bokeh:        0.12.5-py36_1\r\n    bottleneck:   1.2.1-np112py36_0\r\n    dask:         0.14.3-py36_1\r\n    datashape:    0.5.4-py36_0\r\n    distributed:  1.16.3-py36_0\r\n    h5py:         2.7.0-np112py36_0\r\n    matplotlib:   2.0.2-np112py36_0\r\n    numba:        0.33.0-np112py36_0\r\n    numexpr:      2.6.2-np112py36_0\r\n    numpy:        1.12.1-py36_0\r\n    odo:          0.5.0-py36_1\r\n    pandas:       0.20.1-np112py36_0\r\n    patsy:        0.4.1-py36_0\r\n    pytables:     3.2.2-np112py36_4\r\n    pywavelets:   0.5.2-np112py36_0\r\n    scikit-image: 0.13.0-np112py36_0\r\n    scikit-learn: 0.18.1-np112py36_1\r\n    scipy:        0.19.0-np112py36_0\r\n    seaborn:      0.7.1-py36_0\r\n    statsmodels:  0.8.0-np112py36_0\r\n\r\nProceed ([y]/n)? y\r\n\r\n\r\nC:\\Users\\xxxx>conda install numpy\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for installation in environment D:\\Anaconda3:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    numpy: 1.13.1-py36_0\r\n\r\nProceed ([y]/n)? y\r\n\r\nnumpy-1.13.1-p 100% |###############################| Time: 0:00:00   5.28 MB/s\r\n\r\n```\r\nOther information about the system:\r\n\r\n```\r\nC:\\Users\\xxx>conda list python\r\n# packages in environment at D:\\Anaconda3:\r\n#\r\nipython                   5.3.0                    py36_0\r\nipython_genutils          0.2.0                    py36_0\r\nmsgpack-python            0.4.8                    py36_0\r\npython                    3.6.1                         2\r\npython-dateutil           2.6.0                    py36_0\r\n\r\nC:\\Users\\xxx>conda list numpy\r\n# packages in environment at D:\\Anaconda3:\r\n#\r\nnumpy                     1.13.1                   py36_0\r\nnumpydoc                  0.6.0                    py36_0\r\n\r\nC:\\Users\\xxx>conda info\r\nCurrent conda install:\r\n\r\n               platform : win-64\r\n          conda version : 4.3.24\r\n       conda is private : False\r\n      conda-env version : 4.3.24\r\n    conda-build version : not installed\r\n         python version : 3.6.1.final.0\r\n       requests version : 2.14.2\r\n       root environment : D:\\Anaconda3  (writable)\r\n    default environment : D:\\Anaconda3\r\n       envs directories : D:\\Anaconda3\\envs\r\n                          C:\\Users\\xxx\\AppData\\Local\\conda\\conda\\envs\r\n                          C:\\Users\\xxx\\.conda\\envs\r\n          package cache : D:\\Anaconda3\\pkgs\r\n                          C:\\Users\\xxx\\AppData\\Local\\conda\\conda\\pkgs\r\n           channel URLs : https://repo.continuum.io/pkgs/free/win-64\r\n                          https://repo.continuum.io/pkgs/free/noarch\r\n                          https://repo.continuum.io/pkgs/r/win-64\r\n                          https://repo.continuum.io/pkgs/r/noarch\r\n                          https://repo.continuum.io/pkgs/pro/win-64\r\n                          https://repo.continuum.io/pkgs/pro/noarch\r\n                          https://repo.continuum.io/pkgs/msys2/win-64\r\n                          https://repo.continuum.io/pkgs/msys2/noarch\r\n            config file : C:\\Users\\xxx\\.condarc\r\n             netrc file : None\r\n           offline mode : False\r\n             user-agent : conda/4.3.24 requests/2.14.2 CPython/3.6.1 Windows/10 Windows/10.0.15063\r\n          administrator : False\r\n```\r\n\r\nRequest some help on this issue please.\r\nThanks.\r\n\r\n\r\n****SOLVED****\r\nI think I solved the issue by installing tensorflow using conda and after deleting the env/tensorflow folder that I built while building the tensorflow packages:\r\n\r\n`conda install tensorflow`\r\n\r\nThanks!", "Hi I am using Anaconda3 4.4.0 64-bit, Windows 7, Python 3.6.1.\r\nI just installed tensorflow successfully with the command\r\n `conda install tensorflow`\r\nSeems like the issue is resolved. So no need of creating new conda environment and installing the 3.5 version of python there and activating that environment.", "Congrats! Is it for tf-cpu? If so, wondering if you have the same experience with tf-gpu? Thanks.", "What about Python 3.7 ???", "What features do you need? Are all your other libraries already migrated? \n\nJust use Virtualenv, pipenv, conda,... ", "@AlexisTM I prefer to rely on newer and improved software stacks gradually migrating the projects as newer versions are released. This makes the code better to be maintained compared to rely for years on old\\legacy software and then try to migrate the projects changing a lot of code to be compatible with all the changes accumulated in years of different releases. For this reason I prefer to see official Tensorflow releases following also the newer development lines as well, rather than following the philosophy to stay only with the older ones because still work, ignoring the new stuff.", "I mean: Give the time to the devs to update. \r\nIt cost both time and money.", "@SMH17 I think this is where the big divide between a data scientist and a software engineer's needs comes in. If one is using Pytorch in isolation, then it can be argued that infrequent platform updates do not matter.  But for someone building a large system with Pytorch as one small component, then it is not acceptable to be precluded from steady migration forward to new platform versions.  Not migrating in small increments can lead to millions of dollars of investment becoming stranded in obsolesence, because the cost of large migration becomes too large to stomach.   I'm an engineer who just tinkers with DL, so I am generally in the second camp.\r\n\r\nI guess Pytorch and all other DL frameworks will have to decide who their target audience is, going forward.", "Does anyone know where I can access the tensorflow_gpu-1.0.0-cp35-cp35m-win_x86_64.whl file? Need to install on a computer without internet, I'm new to python/programming and haven't had any luck finding it.", "So basically the aptitude here is to ignore windows users right ? I get it, 'we should be using Linux if we want to use Tensorflow' . I understand the latest releases  would be on Linux first, but its insulting to suggest that Windows users should not make requests for robust and clear Tensorflow 1.5 and Keras support on Windows ad Python 3.6.   I mean its only been like a year or so.  "]}, {"number": 6998, "title": "Back-propagating gradients through a sparse tensor?", "body": "I have a normal feed-forward network that produces a vector v. The elements of v are then used as the non-zero entries of a sparse matrix M (assume the coordinates are predefined). The sparse matrix is then multiplied by a dense vector and a loss is defined on the resulting scalar. I want to back-propagate the loss w.r.t. the weights of the network, which entails going through the sparse matrix. \r\n\r\nThis seems like a perfectly reasonable use-case for a sparse matrix, but it appears that such functionality is not supported. Indeed, even calling tf.gradients(M,[v]) produces an error:\r\n\r\n> AttributeError: 'SparseTensor' object has no attribute 'value_index'\r\n\r\nAm I doing something wrong or am I correct in presuming that this functionality doesn't (yet?) exist? If the latter, then is there a work-around for this particular use-case short of rewriting all of the sparse tensor operations with gradients defined? ", "comments": ["cc @martinwicke since he recently answered some q's about state of sparse tensor support", "This is a great question for StackOverflow. It looks to me that there is no bug, but that the error message could be improved. I'll leave this open while I improve this error.", "Wait a second -- the error message wasnt the primary concern here. The question was about whether or not my use case is currently  (or planned to be) supported by tensorflow.", "Not sure what's the status of sparse gradients is, I see a recent commit referencing gradients and sparse together -- ef489fb3", "If you just pass `M.values` instead, I bet your code will just work. I think I'll just fix this though, just have to add tests.", "Yup, that solved it -- thanks!", "> If you just pass `M.values` instead, I bet your code will just work. I think I'll just fix this though, just have to add tests.\r\n\r\nhi, sir,  I meet the problem too,  if I need  get the gradients wrt the sparse tensor's zero elements, what should I do?", "Use tf.sparse.to_dense first. Since you'll be expanding the tensor for the\nbackward pass anyway, there's little reason to keep it sparse.\n\nOn Thu, Dec 27, 2018, 04:36 \u738b\u5409\u5b8f <notifications@github.com wrote:\n\n> If you just pass M.values instead, I bet your code will just work. I\n> think I'll just fix this though, just have to add tests.\n>\n> hi, sir, I meet the problem too, if I need get the gradients wrt the\n> sparse tensor's zero elements, what should I do?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6998#issuecomment-450142625>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_VQASehTbpSJeZzZOM1Ke7iamRzNks5u9L6-gaJpZM4LqKtC>\n> .\n>\n", "@martinwicke There is a bunch of reasons to keep it sparse. Especially in graph neural networks.", "I can't picture how. \r\n\r\nNote I'm not saying make everything dense. I'm only saying that if you require the gradients of zero elements of a sparse tensor S, compute the gradients of tf.sparse.to_dense(S) instead.", "Oh sorry, I misunderstand what you said. So is there a way that can elegantly activate gradient for the non-zero elements of a sparse tensor?", "Yes, just compute the gradient on the values only, see https://github.com/tensorflow/tensorflow/issues/6998#issuecomment-277367733", "Thank you for your reply! But I still need help because I don't know how to **backpropagate the gradients on the SparseTensor** after getting a list of gradients. Is there any example?", "The SparseTensor is backed by its tensors (in particular, .values). So if you just a regular TensorFlow optimizer, everything should work as expected if you only want gradients of the non-zero elements.", "Is the gradient only undefined in 2.0? That would be very surprising. I\nfind it more likely that we never had this gradient. It would be great to\nadd it -- an issue with this feature request would be a good start.\n\nOn Tue, Sep 3, 2019 at 2:51 AM L\u00e1szl\u00f3 M\u00e9r\u0151 <notifications@github.com> wrote:\n\n> On TF 2.0rc this does not work, the gradient for the sparse_dense_matmul\n> op is not defined.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6998?email_source=notifications&email_token=AAEM57JSXUFX6JEVEALA3UTQHYXSPA5CNFSM4C5IVNBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5XUMBQ#issuecomment-527386118>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEM57JCALE2R2XNQXMUE2DQHYXSPANCNFSM4C5IVNBA>\n> .\n>\n"]}, {"number": 6997, "title": "Add note to new op and source build doc (#6473)", "body": "As seen in #6473 issue topic I want to suggest this two little changes to the documentation. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@keveman does this look OK to you?", "@MircoT unfortunately our docs are undergoing a reorganization -- can I ask you to wait until the new docs are available so you can make the same changes there?  Thanks!\r\n\r\nWill mark this as stalled until then.", "@vrv no problem for me. I'll take this as a note to do the changes later. \r\n\r\nSorry but do you mean with the new version of the library, or before that?", "I think the docs are being moved around and such, so these edits would get lost in the translation (which got snapshotted last week), so I've been asked to tell people to hold off until the new files are checked in, and then you can make these changes in the new locations.", "@vrv thanks for the response. I'll do again the pull request adjusting it after the changes of the documentation.", "Could you please update your PR to reflect our move from g3doc/ to doc_src/?\r\nhttps://github.com/tensorflow/tensorflow/commit/1c707ac780313f48a6733dc3beedf4b8a2b3df77\r\n\r\nIn particular, there are some new instructions here:\r\nhttps://github.com/tensorflow/tensorflow/blob/1c707ac780313f48a6733dc3beedf4b8a2b3df77/tensorflow/g3doc/README.txt\r\n\r\nThanks!", "@tensorflow-jenkins test this please"]}, {"number": 6996, "title": "Documentation for seq2seq", "body": "Hello\r\n\r\nI want to use the Tensorflow seq2seq library (I'm doing the Udacity Deep Learning course, Assignment 6).  I can't find anything that documents what functions are available, and the details of all the arguments that can be used for each function (ie what is documented for the Tensorflow API).\r\n\r\nThe library is referenced in the Sequence to Sequence Models tutorial - a tantalising amount of detail is given, but not enough to work with.\r\n\r\nI've googled the subject and searched in stack-overflow - a few posts reference the library but they don't give documentation.\r\n\r\nI found this:\r\nhttps://github.com/tensorflow/tensorflow/blob/63409bd23facad471973b110df998782c0e19c06/tensorflow/models/rnn/translate/translate.py#L132\r\n\r\n - which, I think, gives the code but this is a lower level than I was hoping for!\r\n\r\nI also found the link below on Tensorflow site - maybe the most promising - but none of the links work:\r\n\r\nhttps://www.tensorflow.org/versions/r1.0/api_docs/python/contrib.legacy_seq2seq/#sequence_loss\r\n\r\nCan anyone point me in the right direction?  Apologies if I've come to the wrong place (I've read the Guidelines) - the Support options on the Tensorflow site linked to Github or Stack overflow.\r\nThanks\r\nKathryn\r\n\r\n", "comments": ["This helped me understand the seq2seq api: [tutorial](https://github.com/hans/ipython-notebooks/blob/master/tf/TF%20tutorial.ipynb). What took me a while to realize for whatever reason was that the input to the seq2seq stuff isn't a tensor but a list of size sequence length of tensors of size batch_size by whatever where each element in the list of tensors is just the data for that timestep. ", "`contrib.legacy_seq2seq` is deprecated, along with `translate` example, and it's not worth investing your learning time. New dynamic seq2seq was recently added to the master and will probably be out with 1.0.0 release. I've written a [small example](https://github.com/ematvey/tensorflow-seq2seq-tutorials/blob/master/3-seq2seq-native-new.ipynb) while learning how to use it, however you should know that API will probably change between now and 1.0.0 release.", "Hello both\r\nThank you both for your replies - both really helpful in different ways.  The tutorial is very instructive - gives lots to work with.   The insight into current state of play on seq2seq helps to inform my next steps - I think I will come back to this assignment when it is released.  \r\nThanks again.", "Thanks for the reply @tallen11 and @ematvey .\r\n@Karmits glad to hear you found some help for now.\r\n\r\n@ebrevdo @xiejw : FYI (especially since it seems that the legacy_seq2seq docs do not yet work, so wanted to check in regarding what seq2seq support is likely to be in 1.0)", "There's active work in contrib.seq2seq.  we're not sure yet if the final\nAPI will be functional or object oriented.  Will know more in about a week.\n\nOn Jan 22, 2017 10:05 AM, \"Asim Shankar\" <notifications@github.com> wrote:\n\n> Thanks for the reply @tallen11 <https://github.com/tallen11> and @ematvey\n> <https://github.com/ematvey> .\n> @Karmits <https://github.com/Karmits> glad to hear you found some help\n> for now.\n>\n> @ebrevdo <https://github.com/ebrevdo> @xiejw <https://github.com/xiejw> :\n> FYI (especially since it seems that the legacy_seq2seq docs do not yet\n> work, so wanted to check in regarding what seq2seq support is likely to be\n> in 1.0)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-274347376>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7sYxLzUWEBfxDarJq476kl4Epwuks5rU5phgaJpZM4LqHHc>\n> .\n>\n", "@ebrevdo is there any news?", "Hi all\r\nAlso I'd like to note that [seq2seq tutorial](https://www.tensorflow.org/tutorials/seq2seq/) reference to some files like `models/rnn/translate/seq2seq_model.py` that does not exists now in the repo. There no any `models` folder at all. I think links should be pointed to outer repo *tensorflow/models*, to [this folder](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate).\r\nI've decided not open new issue, because this seems related to my info. But if I'm not right - please fix me.", "The new API is available; we're working on adding attention and beam search\nover the next several weeks.  Some examples of how to use it are here\n<https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/kernel_tests/decoder_test.py#L132>\n.\n\nWe also hope to have a new tutorial in the next few weeks based on this API.\n\nOn Wed, Feb 8, 2017 at 3:45 AM, Illarion <notifications@github.com> wrote:\n\n> Hi all\n> Also I'd like to note that seq2seq tutorial\n> <https://www.tensorflow.org/tutorials/seq2seq/> reference to some files\n> like models/rnn/translate/seq2seq_model.py that does not exists now in\n> the repo. There no any models folder at all. I think links should be\n> pointed to outer repo *tensorflow/models*, to this folder\n> <https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate>\n> .\n> I've decided not open new issue, because this seems related to my info.\n> But if I'm not right - please fix me.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-278305820>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7D2uPSeTQ_ariqJh1GIUaDBQGLXks5raaq-gaJpZM4LqHHc>\n> .\n>\n", "Hey, @ebrevdo \r\n\r\nDo you have any time estimate on when this new tutorial might be published?", "Which stable version of Tensorflow will this new API land on? Any time estimate for release?", "Guys, I think it was finally published:\r\nhttps://google.github.io/seq2seq/\r\nhttps://github.com/google/seq2seq", "This is a slightly different library and tutorial. They use a similar code\nbase, but we are also targeting a library in tf.contrib.seq2seq which is\nmore general.\n\nOn Mar 15, 2017 5:52 AM, \"Konstantin Glushak\" <notifications@github.com>\nwrote:\n\n> Guys, I think it was finally published:\n> https://google.github.io/seq2seq/\n> https://github.com/google/seq2seq\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-286731903>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyrlHNgQd8obbJ9wIQz-6U9DcfWfks5rl973gaJpZM4LqHHc>\n> .\n>\n", "@ebrevdo hi, have you finished beam search part code in the tf.contrib.seq2seq package?\r\nI'm really confused about how to realize it..", "We're close to having it done.  Initial version should be pushed tomorrow,\nand cleanups and API changes in the next week.\n\nOn Wed, Apr 19, 2017 at 12:15 AM, DaoD <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> hi, have you finished beam search\n> part code in the tf.contrib.seq2seq package?\n> I'm really confused about how to realize it..\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-295134545>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0aO2Ql1pCaWHjTxec1pKB6Lt8Hdks5rxbSkgaJpZM4LqHHc>\n> .\n>\n", "@ebrevdo Thanks for your kindly reply. Looking forward to it~", "@ebrevdo \r\nWhen I use the new seq2seq module in the master branch, got this error:\r\nseq2seq\\python\\ops\\beam_search_ops.py, line 20, in <module> from seq2seq.ops import gen_beam_search_ops\r\nImportError: cannot import name 'gen_beam_search_ops'\r\nIs the seq2seq package haven't finished yet?\r\nThanks", "@ebrevdo hi, i am also waiting for the new seq2seq api. will it be released soon or did i miss it?", "Hi, the new API in tf.contrib.seq2seq in the tf nighties should be mostly\nfeature complete and working.\n\nOn May 11, 2017 2:04 AM, \"PJ\" <notifications@github.com> wrote:\n\n@ebrevdo <https://github.com/ebrevdo> hi, i am also waiting for the new\nseq2seq api. will it be released soon or did i miss it?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-300729224>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABtim_jW-BpORkWqtxfqZI42MuFnNfTSks5r4s8PgaJpZM4LqHHc>\n.\n", "Hi, \n\nHave tutorials on the new seq2seq API been released yet? \n\nThanks! ", "No, we are preparing them and it's likely they will be released around the\nsame time as tf 1.2 final, since they rely on it.\n\nOn Jun 12, 2017 5:15 AM, \"Simon ESPIGOL\u00c9\" <notifications@github.com> wrote:\n\n> Hi,\n>\n> Have tutorials on the new seq2seq API been released yet?\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-307772105>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim56UG3ne8llAa6x38UdwarijCnCVks5sDSvxgaJpZM4LqHHc>\n> .\n>\n", "`v1.2` released. Any update on this?", "@jihopark Also looking a tour on tf1.2 seq2seq API.", "I believe this is the announcement of this tutorial: https://research.googleblog.com/2017/07/building-your-own-neural-machine.html\r\n\r\nThank you @ebrevdo and team for all the hard work!", "Great!  We welcome all feedback as you play with the new tutorial.\n\nOn Jul 12, 2017 11:57 AM, \"Konstantin Glushak\" <notifications@github.com>\nwrote:\n\nI believe this is the announcement of this tutorial:\nhttps://research.googleblog.com/2017/07/building-your-own-\nneural-machine.html\n\nThank you @ebrevdo <https://github.com/ebrevdo> and team for all the hard\nwork!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-314864211>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABtim9K_dKm5RI74CKSbJZ90flBrX9RMks5sNRcOgaJpZM4LqHHc>\n.\n", "@ebrevdo Sorry for bother you. A small question that if this seq2seq module provide a way to avoid generating unknown words when infer from new input?\r\nJust like, when _unk is the word with highest probability, simply choose the second one to replace it.\r\nOr some other methods to avoid generating unknown words?\r\nThanks so much.", "![image](https://user-images.githubusercontent.com/28672917/33743150-9c345430-dbd1-11e7-949a-537fea76f918.png)\r\ngetting this error how to solve please help", "@satyajit123A That is deprecated. For more information, see https://github.com/tensorflow/nmt for an up-to-date explanation.", "DaoD, try using word pieces instead. The vocabulary is smaller and unks\nmuch more rare, possibly no longer even necessary.\n\nOn Thu, Dec 7, 2017, 3:25 PM nave01314 <notifications@github.com> wrote:\n\n> @satyajit123A <https://github.com/satyajit123a> That is deprecated. For\n> more information, see https://github.com/tensorflow/nmt for an up-to-date\n> explanation.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6996#issuecomment-350125832>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_1D9Npz_d2JiiVSdp6MBq3zGqqyks5s-HPkgaJpZM4LqHHc>\n> .\n>\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@tensorflowbutler  i am extremely sorry for not replying actually i got the problem solved by  using \r\nimport tensorflow.contrib.seq2seq as seq2seq. thanks to  https://github.com/tensorflow/nmt and @nave01314 ", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}]