[{"number": 27027, "title": "tf.dataset failed to correctly load image data", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDebian GNU/Linux 8 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNone\r\n- TensorFlow installed from (source or binary):\r\nconda install \r\n- TensorFlow version (use command below):\r\ntensorflow-gpu1.12.0rc0\r\n- Python version:\r\npython 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda 9.2 with cdunn7.3.1\r\n- GPU model and memory:\r\nnvidia1080ti, 11gb\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ni tried to use tf.dataset api to build a custom dataloader, all the images are in .jpg format and rgb color space with exact 3 channels (i double checked this using opencv to load images and numpy to print image shape). then i got this error:\r\n\r\nInvalidArgumentError (see above for traceback): Cannot batch tensors with different shapes in component 0. First element had shape [270,270,3] and element 92 had shape [270,270,1].\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef load_function(image_path, label_path, crop_size=256):\r\n        image = tf.read_file(image_path)\r\n        image = tf.image.decode_jpeg(image)\r\n        image = tf.image.resize_images(image, [270, 270])\r\n        image = tf.image.random_crop(image, [crop_size, crop_size, 3])\r\n        label = tf.read_file(label_path)\r\n        label = tf.image.decode_png(label)\r\n        label = tf.image.resize_images(label, [270, 270])\r\n        label = tf.image.random_crop(label, [crop_size, crop_size, 1])\r\n        return image, label\r\n```\r\n\r\n", "comments": ["What the error indicates is that the 92nd image has different shape ([270, 270, 3] vs. [270, 270, 1]) then the previous 91 images. \r\n\r\nMy recommendation would be to remove `batch` from your input pipeline and then iterate through your input pipeline one by one, printing out the shape and image name to identify the problematic image.\r\n\r\nThis is problem with your data, not with tf.data. (If you believe it is a problem with tf.data, please provide a reproducible example).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27027\">No</a>\n"]}, {"number": 27026, "title": "dilated convolution does not convergence with data format NHWC", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4.1708\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): conda installation\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA Version 9.0.176\uff0c \r\n- GPU model and memory: V100, 16G\r\n\r\n\r\n**Describe the current behavior**\r\nI implemented WaveGlow model, in the model it contains dilated convolution, so in early time I used tf.nn.conv2d to implement the dilated convolution model- -- WaveNet, I used the default data format NHWC, but after many experiments, I found the model does not convergence. Even after 700K steps with a small learning rate, it did not convergence. I have tried many hyper-parameters settings.\r\n\r\nSo I changed the dilated convolution to a native implementation from [tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet). Then the WaveGlow model convergence quickly after 50k-80k steps.\r\n\r\nThen, I want to understand what's wrong with my usage of tf.nn.conv2d, but no clue. Then I tried data format NCHW, the model convergences quickly after 70K steps. \r\n\r\nSo I doubt there may be a bug in Tensorflow's implementation of dilated convolution with data format NHWC.\r\n\r\n**Describe the expected behavior**\r\ntf.nn.conv2d with data format NHWC should convergence as expected, it should behave the same as data format NCHW or naive dilated convolution implementation.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nmy code: https://github.com/weixsong/WaveGlow\r\nmaster branch is the naive implementation of dilated conv1d\r\n\r\nmodel samples are also in the repo.\r\n\r\nbranch **tf_dilated_conv** is the implementation by tf.nn.conv2d with data format NHWC : https://github.com/weixsong/WaveGlow/tree/tf_dilated_conv\r\n\r\nbranch **tf_dilated_conv_channel_first** is the implementation by tf.nn.conv2d with data format NCHW  , https://github.com/weixsong/WaveGlow/tree/tf_dilated_conv_channel_first\r\n\r\n**Other info / logs**\r\nN/A\r\n", "comments": ["Could you provide us a MCVE (See https://stackoverflow.com/help/mcve) that involves only a single TensorFlow operation and the results have large difference with data format NHWC and NCHW with a particular input?\r\n\r\nAlso, could you give it a try after setting 'TF_CUDNN_USE_AUTOTUNE' to '0' with NHWC data format to see if that helps? Winograd convolution that is used in case of NHWC is known to have higher numerical error. See section 4 in https://openreview.net/pdf?id=H1ZaRZVKg", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27026\">No</a>\n"]}, {"number": 27025, "title": "TfLite squeeze_test extend test case added", "body": "Additional data type support test case added.", "comments": ["Corrupt PR is replaced with #27413."]}, {"number": 27024, "title": "Reset mem allocation type after TensorDataFree", "body": "Reset mem allocation type after TfLiteTensorDataFree", "comments": ["@karimnosseir @aselle please help to review this PR. TIA", "Can one of the admins verify this patch?", "@siju-samuel  Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 27023, "title": "TF 1.x: remove the \"deprecated\" warning messages", "body": "I know the functional APIs, such as tf.layers.dense, will disappear in TF 2.0. However, their alternatives, tf.keras.layers, are not compatible with other components of TF 1.x, for example, they even do not support variable scope (#27016). So I will stick on the deprecated APIs in TF 1.x. \r\nWould you please remove the disgusting \"deprecated\" warning messages like this: `xxx (from tensorflow.python.layers.core) is deprecated and will be removed in a future version. Use keras.layers.xxx instead.`", "comments": ["You can use this:-\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\n\r\nIn detail:-\r\n0 = all messages are logged (default behavior)\r\n1 = INFO messages are not printed\r\n2 = INFO and WARNING messages are not printed\r\n3 = INFO, WARNING, and ERROR messages are not printed", "this is what i have been using for years\r\nit cannot stop the deprecation warning", "By the way, before deprecating old components, would you please complete the current functionality? See #27042. It seems that TF is very good at deprecating widely used functions, but poor at improving its weak points", "Deprecation messages are needed to inform people who want to use an actual version of TensorFlow what is going to happen in the future, and give them time to adapt.\r\n\r\nAs such, we won't be able to remove these messages. You can disable them for yourself using this private API\r\n\r\n```python\r\nimport tensorflow.python.util.deprecation as deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n```", "Thanks\r\nSee #27045, any suggestions?", "This code gave me an error:\r\n\r\n```python\r\nimport tensorflow.python.util.deprecation as deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n```\r\n\r\n```\r\nAttributeError: module 'tensorflow' has no attribute 'python'\r\n```\r\n\r\nBut changing the import line made it work (and suppresses the deprecation warnings as desired):\r\n```python\r\nfrom tensorflow.python.util import deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\n```\r\n\r\nThanks\r\n", "@skylogic004 is this from a recent nightly/build from source from recent master?", "@mihaimaruseac Oh, I'm using tf 1.13 from anaconda (I'm not familiar with what branch & build it would have come from originally, sorry, but maybe this is enough info for you?).\r\n\r\n```\r\n$ python\r\nPython 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 11:48:23) [MSC v.1900 64 bit (AMD64)] on win32  \r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.                                          \r\n>>> import tensorflow as tf                                                                                     \r\n>>> tf.__version__                                                                                              \r\n'1.13.1'                                                                                                        \r\n\r\n$ conda list | grep tensor\r\ntensorboard               1.13.1           py36h33f27b4_0\r\ntensorflow                1.13.1          gpu_py36h1635174_0\r\ntensorflow-base           1.13.1          gpu_py36h0fff12a_0\r\ntensorflow-estimator      1.13.0                     py_0\r\ntensorflow-gpu            1.13.1               h0d30ee6_0\r\n```\r\n\r\nLink to anaconda package: https://anaconda.org/anaconda/tensorflow-gpu\r\n\r\n", "Then it's not something I should worry about (full context: over the past week there have been some failures on imports due to some changes I made; but those are only on master code, which doesn't seem to be the case)", "Deprecation warnings are important for developers. They may not be all that useful to users. Therefore there are situations where it is desirable to switch them off.\r\n\r\nPython has `sys.warnoptions` which could be for example set via `PYTHONWARNINGS`, e.g.:\r\n`PYTHONWARNINGS=ignore`.\r\n\r\nTensorFlow has it's own `TF_CPP_MIN_LOG_LEVEL` option as pointed out earlier.\r\n\r\nAnd there is also the TensorFlow Python logging API (which I believed worked before):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n```\r\n\r\nThat none of the options above are preventing the warning messages from being printed seem to be a bug.\r\n\r\nThe `_PRINT_DEPRECATION_WARNINGS` flag seems to be internal and may be removed any time.\r\n\r\nEDIT: Apologies, I was confusing the [numpy FuturreWarning](30427)s for tensorflow warnings. The last option to set the logging level should generally still work.", "On `1.14.0` only this works:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n```", "For r1.14 and onward, try this:\r\n```python\r\ntry:\r\n    from tensorflow.python.util import module_wrapper as deprecation\r\nexcept ImportError:\r\n    from tensorflow.python.util import deprecation_wrapper as deprecation\r\ndeprecation._PER_MODULE_WARNING_LIMIT = 0\r\n```\r\nAnyway, it is internal and subjects to change.", "Or switch to TF2.0 where deprecation warnings are removed (as not needed).", "Unless you use a good selection of IDE's :p \r\nOnly thing that made me roll back was the autocomplete issue so these suppression requests are valid imo. ", "Autocompletion has been mostly solved", "Tried all of what have been suggested:\r\n\r\n```python\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nimport tensorflow as tf\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nfrom tensorflow.python.util import deprecation\r\ndeprecation._PRINT_DEPRECATION_WARNINGS = False\r\ntry:\r\n    from tensorflow.python.util import module_wrapper as deprecation\r\nexcept ImportError:\r\n    from tensorflow.python.util import deprecation_wrapper as deprecation\r\ndeprecation._PER_MODULE_WARNING_LIMIT = 0\r\n```\r\n\r\nStill, have a pile of warnings.\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nTF version 1.14.0\r\n```\r\nUbuntu 16.04\r\nPython 3.6.8\r\n\r\n", "@pyotr777 For python builtin warnings, you should use following to suppress:\r\n\r\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\r\n    warnings.filterwarnings('ignore', category=FutureWarning)\r\n", "@pyotr777 those come from numpy. You are using a numpy version that is slightly incompatible with the TF version you're using.\r\n\r\nAt the risk of getting more downvotes, please switch to 2.0", "> FutureWarning\r\n\r\n```\r\nimport warnings\r\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\r\nwarnings.filterwarnings('ignore', category=FutureWarning)\r\n```\r\n did not supress \r\n```\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n```", "For me this worked for t2.0 warning \r\n```\r\nimport tensorflow as tf\r\ntf.get_logger().warning('test')\r\n# WARNING:tensorflow:test\r\ntf.get_logger().setLevel('ERROR')\r\ntf.get_logger().warning('test')\r\n```\r\nFYI Logging in TensorFlow changed as TF_CPP_MIN_LOG_LEVEL is not working Anymore ", "Still, have a pile of warnings.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n\r\n\r\n**Please tried  one at a time to change the 1 to 0 eg:  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])  to  _np_qint8 = np.dtype([(\"qint8\", np.int8, 0)]) that works for me**", "@TiwaPhil https://github.com/tensorflow/tensorflow/issues/27023#issuecomment-545991801", "![solve warnings](https://user-images.githubusercontent.com/59890569/72373177-bfade500-36d5-11ea-835b-f73e52ba888a.JPG)\r\n", "> You are using a numpy version that is slightly incompatible with the TF version you're using.\r\n\r\nFor me worked putting \r\n```\r\nimport warnings\r\nwarnings.filterwarnings('ignore',category=FutureWarning)\r\n```\r\nbefore \r\n```\r\nimport tensorflow as tf\r\n```\r\nand all other imports.\r\n\r\nI have TF 1.14.0 and numpy 1.18.1.\r\n", "before import tensorflow, you should implement:\r\nimport warnings\r\nwarnings.filterwarnings('ignore',category=FutureWarning)\r\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\r\nit works for tensorflow==1.12.0, numpy==1.18.1, python 3.6", "I tried this:\r\n```\r\nimport warnings\r\nwarnings.filterwarnings('ignore',category=FutureWarning)\r\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\r\n```\r\nAnd it did **not** work for me when I ran python non-interactively `tensorflow==1.14.0`, `numpy==1.18.1`, `python==3.6.10`, but it did work in an interactive session. ", "I found that the following worked when I run python both interactively and non-interactively. When using tensorflow 2.X, you can use:\r\n```\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n```\r\nAnd when using tensorflow 1.X (worked on 1.14 for me):\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n```", "I'm using tf 1.13.1\r\nThis works:\r\n```python\r\nimport tensorflow as tf\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n```\r\n\r\n`.compat.v1` is not required.", "I tried all methods, but none of them worked in jupyter notebook...\r\n\r\n`\r\nwarnings.filterwarnings('ignore',category=FutureWarning)\r\n`\r\nThis can ignore warnings when importing tensorflow, but there still exists many many many WARN if i load model...", "This work for me on python 3.6 and TensorFlow 2.2. Inspired from @bhushanbrb, thanks.\r\n```\r\nimport tensorflow as tf\r\ntf.get_logger().setLevel('ERROR')\r\n```", "> > You are using a numpy version that is slightly incompatible with the TF version you're using.\r\n> \r\n> For me worked putting\r\n> \r\n> ```\r\n> import warnings\r\n> warnings.filterwarnings('ignore',category=FutureWarning)\r\n> ```\r\n> \r\n> before\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> ```\r\n> \r\n> and all other imports.\r\n> \r\n> I have TF 1.14.0 and numpy 1.18.1.\r\n\r\nThis indeed works. I tried every other suggestions provided above but none of them worked for me. But, this worked like a charm"]}, {"number": 27022, "title": "tf.data.experimental.ignore_errors", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**May have been addressed by:** [https://github.com/tensorflow/tensorflow/issues/25700](url)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. Stock code with additional operations\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu 1.12.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: 10.0/7.4.1\r\n- GPU model and memory: TitanXP 12196MiB\r\n\r\n\r\n**Describe the current behavior**\r\ntf.data.experimental.ignore_errors() causes tensorflow to hang when applied after a \"dataset.batch(N)\" operation.\r\n**Describe the expected behavior**\r\nThrow an error, suggest that the user ignore errors prior to batching to prevent uneven batches.\r\n**Code to reproduce the issue**\r\n`import tensorflow as tf\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\r\n\r\ndataset = dataset.map(lambda x: tf.check_numerics(1. / x, \"error\"))\r\n\r\ndataset = dataset.repeat().batch(6)\r\n\r\ndataset = dataset.apply(tf.data.experimental.ignore_errors()) \r\n\r\nit = dataset.make_one_shot_iterator()\r\nnex = it.get_next()\r\nsess = tf.Session()\r\nprint(sess.run(nex))`\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSee above.\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @dan-garvey ,\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade code base to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27022\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27022\">No</a>\n"]}, {"number": 27021, "title": "Bugfix: avoid casting a type 'type'", "body": "Hi! This is my first fix.\r\n\r\nThe problem is in `HParams` class in `contrib`. Minimal example:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import HParams\r\n\r\nhp = HParams()\r\nhp.add_hparam('param', tf.Tensor)\r\nprint(hp.param)\r\nhp.set_hparam('param', tf.Tensor)\r\nprint(hp.param)\r\n```\r\n\r\nExpected output: \r\n```\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n```\r\nActual output: \r\n```\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\n<class 'type'>\r\n```\r\n\r\nI.e., while the function add_hparam works correctly, the function set_hparam destroys the value passed to it if the value is a class.\r\n\r\nThe problem seems to in `def _cast_to_type_if_compatible` function. The proposed fix is to not apply the parameter conversion if the param is a type i.e. if the type of the param is `type`.", "comments": ["I am not sure if this is relevant now that the contrib is being phased out, but this is a problem in my project so would be great to fix this. I am not sure where the class is in TF2.0", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "I couldn't see any comments on the commit code. Can you point me to them? This is my official first GitHub fix."]}, {"number": 27020, "title": "tensorflow-gpu 2.0.0-alpha0 on win10 ImportError: DLL load failed:", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  win10 Pro x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA: v9.0, cuDnn: v7.5\r\n- GPU model and memory: 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nerrors occur when `import tensorflow as tf`\r\n \r\nI have read some similar issues and [this](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) but those do not work on my case.\r\n\r\nAnd I reinstall tensorflow 1.13.1, it works well.\r\n\r\n**Any other info / logs**\r\n\r\nI'm sure you've seen many similar logs like below:\r\n\r\n>Traceback (most recent call last):\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found\r\n\r\n>During handling of the above exception, another exception occurred:\r\n\r\n>Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\JoTang507\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n>ImportError: DLL load failed: The specified module could not be found", "comments": ["You should install CUDA toolkit v10 just like this [guide](https://www.tensorflow.org/install/gpu#software_requirements) said.\r\nOr you can install on conda follow this [guide](https://medium.com/@shaolinkhoa/install-tensorflow-gpu-2-0-alpha-on-anaconda-for-windows-10-ubuntu-ced099010b21?fbclid=IwAR17F1Z7Xd6yk0UYnnLhYYuHcAr6mBn61xdxbg1fLy-swd10vprfrbc__GM)", "I suspect you must have installed TF 1.13.1 CPU version earlier since TF 1.13.1 GPU version supports cuda 10.0 too. In addition to cuda 10.0 installation please also take a look at [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup) to add cuda path. Thanks!\r\n", "@YAOYI626 Where do you want to install Tensorflow-gpu 2.0? local or on anaconda?\r\n**If you want to install on local:**\r\nI had been this error before while I install tensorflow-gpu 2.0 on local (without Anaconda). The reason are:\r\n+ Wrong Nvidia GPU version => Solution: Install Nvidia GPU latest ( >=410.x)\r\n+ Wrong CUDA toolkit version =>  Solution: Install CUDA toolkit 10.0\r\n+ Wrong cuDNN version  =>  Solution: Download cuDNN 7.5 and follow this [guide](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows)\r\n\r\n**If you want to install on anaconda:**\r\nFollow this [guide](https://medium.com/@shaolinkhoa/install-tensorflow-gpu-2-0-alpha-on-anaconda-for-windows-10-ubuntu-ced099010b21?fbclid=IwAR17F1Z7Xd6yk0UYnnLhYYuHcAr6mBn61xdxbg1fLy-swd10vprfrbc__GM):\r\nThe reason is Tensorflow 2.0 require cuDNN version >= 7.4.1 but Anaconda hasn\u2019t supported cuDNN version >= 7.4.1 yet. So we have to download cuDNN 7.5 and manually copy cuDNN 7.5 to Anaconda environment.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27020\">No</a>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "The [gpu TF requirements guide](https://www.tensorflow.org/install/gpu#software_requirements) should link straight to the [cuda-10.0-download-archive](https://developer.nvidia.com/cuda-10.0-download-archive) rather than [cuda-zone](https://developer.nvidia.com/cuda-zone). It is all too easy for newbies to then install 10.1 and loose at least an hour figuring out this ImportError issue. Not a good first impression of TF2. ", "I installed  \"cuda_10.1.168_425.25_win10\" and \"cudnn-10.1-windows10-x64-v7.6.2.24\".But I have the same problem"]}, {"number": 27019, "title": "Lite: Kernel_util refactored", "body": "CalculateActivationRangeUint8(), CalculateActivationRangeInt8() remove to keep the logic in one place.", "comments": ["Over to @jianlijianli or @suharshs for review.", "@suharshs : Thanks for review, your comment is handled now, would you please check and approve, Thanks!", "@suharshs , @jianlijianli : Gentle reminder!", "@suharshs , @jianlijianli : Gentle reminder! Please this need to be merge at earliest as new PRs might use these APIs which are removed as part of this PR, Thanks!", "Can one of the admins verify this patch?", "@ANSHUMAN87 can you please fix build failures ?", "@rthadur : The failure is not related to this PR, when i checked last time, however, can you please rerun and check, if it still fails. Please share the latest logs. Thanks!", "@ANSHUMAN87 we can try to pull this again , can you please check build failures and resolve conflicts ", "@rthadur : The conflict is resolved now, please check, Thanks!", "@ANSHUMAN87 thank you so much for resolving conflicts  , can you please check build failures ?", "> @ANSHUMAN87 thank you so much for resolving conflicts , can you please check build failures ?\r\n\r\n@rthadur : I have pushed some changes related to the failures. Thanks!", "@suharshs @jianlijianli can you please review latest changes ?", "@ANSHUMAN87 Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "conflict resolved!", "@jianlijianli , @suharshs : Gentle Reminder for review!", "> Looks like there are some additional lite/micro/kernels that need to be updated? At the very least, mul.cc? Can you take a look? Thanks!\r\n\r\nIts done!"]}, {"number": 27018, "title": "Add scalar graph test for Lite NEG", "body": "", "comments": []}, {"number": 27017, "title": "[lite/micro] fix various typos.", "body": "", "comments": ["@csukuangfj could you please resolve the conflicts? Thanks!", "@gbaned \r\nIt takes so long to get a reply that I deleted my repository some time ago.\r\n\r\nI will close this pullrequest and reopen it at #28593 with all conflicts solved."]}, {"number": 27016, "title": "tf.keras.layers (in TF 1.13.1): variable_scope does not work", "body": "Run the following code:\r\n```\r\nimport tensorflow as tf\r\n\r\nfor index in range(2):\r\n    with tf.variable_scope(name_or_scope=tf.get_variable_scope(), reuse=True if index > 0 else None):\r\n        outputs = tf.keras.layers.Dense(10).apply(tf.ones([10, 10]))\r\n\r\nprint(tf.trainable_variables())\r\n```\r\nThere will be 4 variables:\r\n```\r\n[<tf.Variable 'dense/kernel:0' shape=(10, 10) dtype=float32>,\r\n <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32>,\r\n <tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32>,\r\n <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32>]\r\n```\r\n\r\nBut if you change tf.keras.layers to tf.layers:\r\n```\r\nimport tensorflow as tf\r\n\r\nfor index in range(2):\r\n    with tf.variable_scope(name_or_scope=tf.get_variable_scope(), reuse=True if index > 0 else None):\r\n        outputs = tf.layers.Dense(10).apply(tf.ones([10, 10]))\r\n\r\nprint(tf.trainable_variables())\r\n```\r\nThere will be only 2 variables (the correct result):\r\n```\r\n[<tf.Variable 'dense/kernel:0' shape=(10, 10) dtype=float32_ref>,\r\n <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>]\r\n```", "comments": ["There is [no variable scope in TensorFlow 2.0](https://github.com/tensorflow/tensorflow/issues/26854#issuecomment-475495847) but this seems like a bug if the code is not on TF2.0", "> There is [no variable scope in TensorFlow 2.0](https://github.com/tensorflow/tensorflow/issues/26854#issuecomment-475495847) but this seems like a bug if the code is not on TF2.0\r\n\r\nThe above codes are written with TF 1.13.1", "If this issue is not solved in TF 1.X, please do not mislead users to replace tf.layers to tf.keras.layers\r\nBy the way, I think the change after TF 1.12 is a failure.", "The replacement is meant to happen in TF2.0, not before.", "But your \"deprecated\" warning message (in TF 1.13.1) is like this:\r\n```\r\nWARNING:tensorflow:From <stdin>:1: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dense instead.\r\n```", "Deprecated warning means \"X is deprecated in this version, will go away in version A and at that time you should use Y instead\"", "according to the confusing warning message, i spent a whole day to modify the funtional tf.layers to the OOP tf.keras.layers, but finally found that they do not support variable scope...shit", "When will you deprecate keras?\r\nWhen will you deprecate Tensorflow?\r\nWhen will you deprecate Google?", "When you use `tf.keras.layers.Dense()`, you actually do not need to bother using variable scope. The layer object itself allows you to reuse variables, as those variables-to-be-shared are created in the constructor.\r\n\r\nHowever, I also agree that the expected behavior is just what is described in the issue (on 1.X), as it sounds more consistent. I know variable_scope will be depcreated, but since that was also the way in 1.x Keras layers should respect variable scopes as well and this should be fixed.", "`variable_scope` is the most important foundation of the previous TF, and `tf.layers.xxx` is also a very popular feature of TF. Deprecating them is really a stupid decision. \r\nI wonder if TF still an open source software? \r\nAnd:\r\nWhen will you deprecate keras?\r\nWhen will you deprecate Tensorflow?\r\nWhen will you deprecate Google?", "> When you use `tf.keras.layers.Dense()`, you actually do not need to bother using variable scope. The layer object itself allows you to reuse variables, as those variables-to-be-shared are created in the constructor.\r\n> \r\n> However, I also agree that the expected behavior is just what is described in the issue (on 1.X), as it sounds more consistent. I know variable_scope will be depcreated, but since that was also the way in 1.x Keras layers should respect variable scopes as well and this should be fixed.\r\n\r\nPlease do not assume that everyone use OOP. I prefer `tf.layers.dense`.", "I also think using simpler functional-style (like `dense()`) makes a sense (actually I like that as well as OOP styles) when it's more handy. My point was that the keras layers should work well being compatible with variable_scope. Please refer to the discussion in #20842 as well.\r\n\r\nAnd please be respectful, logical, but not aggressive. Nobody will listen to you if you say like this.", "`graph` and `variable_scope` used to be your basic mechanism, and now you are depreciating it.\r\n`tf.layers.xxx` used to be your major selling point, and now you are depreciating it.\r\n\r\nJust tell me: is there anything that you will not depreciate? \r\n\r\nAnd:\r\nWhen will you deprecate keras? (it is only a third-party package)\r\nWhen will you deprecate Tensorflow?\r\nWhen will you deprecate Google?\r\n\r\nI am very respectful, logical, and not aggressive. Thank you.", "Personally, I consider it very aggressive to keep repeating the same questions and spamming over several issues. Please understand that all APIs evolve and change. There was a time to discuss these API changes but it has been many months ago. Now the ship has sailed and these will stay deprecated unless valid reasoning asks for their reintroduction.\r\n\r\nClosing, as https://github.com/tensorflow/tensorflow/issues/27016#issuecomment-480621768 suggests that this is not a real issue.", "Yes, you are very correct: all things evolve and change.\r\nSo I really want to know:\r\n\r\n**When will you deprecate keras?\r\nWhen will you deprecate Tensorflow?\r\nWhen will you deprecate Google?**\r\n\r\nKnowing this in advance is very important to our users, but why do you deem this as \"aggressive\"?\r\n\r\nAnyway, closing this issue is the best way if you cannot answer these questions.", "Today I noticed TF2.0 will deprecate variable scope (tf.contrib.seq2seq need a Layer object, I need check if it support scope because I did not see a scope variable in the API). Although I think it is good news, there will be legacy problems of our code like tf.contrib.seq2seq.\r\nTo me I need spend 5 times effort to use TF develop models than keras before google bought keras.\r\nI even develop my own OOP style Layers(Dense, RNN, CNN and so on), all are based on variable scope. I think it is time to use tf.keras.layers.\r\n", "Maybe we could have a warning when instantiating tf.keras layers inside a reuse scope? That would have saved me a lot of debugging.", "> When you use `tf.keras.layers.Dense()`, you actually do not need to bother using variable scope. The layer object itself allows you to reuse variables, as those variables-to-be-shared are created in the constructor.\r\n\r\nSo how would a minimal workaround with tf.keras.layers look like?", "I understand that this issue is closed, but there are scenarios where these features can be useful(I think,at least). All the GAN examples that I have looked at (including the ones from the official tensorflow repo) use tf.layers rather than tf.keras.layers when doing custom training(eg. using estimators etc) and its very useful to have the reuse= True/Auto option", "I think those tutorials on the tensorflow site need to be updated. Can you please open issues for those?", "@mihaimaruseac  They need to be updated for sure, but there are a whole bunch of GAN models implemented in tensorflow ,either official tutorials or just someone trying to recreate a research paper   that follow the convention used in the official tensorflow TPU GAN below. Basically, create a generator/discriminator by stacking a bunch of layers under a variable scope with reuse =tf.auto_reuse.  Now, when we just keep everything else the same and replace tf.layers with the tf.keras class, everything runs as expected except that the layers are not getting reused and there are typically no warnings/errors, nothing to indicate that something is wrong except that the loss diverges. \r\n https://github.com/tensorflow/tpu/tree/master/models/experimental/dcgan\r\nWhat changes would you recommend to transition to the keras API if creating a Keras Model and using its methods are not suitable(TPU distribute strategy does not support Keras GAN models currently as far as I can tell) Thanks\r\n", "@mihaimaruseac I think it would be more clear in the deprecating msg to state explicitly something like `do not try to replace tf.layers with tf.keras.layers until you are on TF version > 2.0`."]}, {"number": 27015, "title": "Lite: Fully_connected Op resize check removed", "body": "This PR depends on changes from #26885 .\r\nOnce above PR is merged this PR should be merged.", "comments": []}, {"number": 27014, "title": "Add summary, args and exceptions raised for zeros_initializer docstring", "body": "Closing multiple issues listed in #26532 one by one in this pull request.\r\n\r\nAdded summary, args and exceptions to the ``tf.zeros_initializer``  #26538 docs.\r\n\r\nTasks to be done:\r\n - tf.unstack #26534\r\n - tf.unique #26535\r\n - tf.zeros #26537", "comments": ["Could anyone help me as to where to make changes for code generated docs. I read the ``api_def_Unique.pbtxt`` in ``api_def_Unique.pbtxt``, but couldn't find the complete docstring visible on the ``tf.unique`` API page", "I am not a good reviewer for this change. Please redirect.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 27013, "title": "DEBUG_LD -> LD_DEBUG", "body": "DEBUG_LD has no effect. I assume it is meant to be LD_DEBUG?\r\n\r\nhttp://www.bnikolic.co.uk/blog/linux-ld-debug.html", "comments": []}, {"number": 27012, "title": "tflite depthwise_conv2d produces different(wrong) results ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: CUDA 9 cuDNN 7.1\r\n- GPU model and memory:GTX 1080\r\n\r\nSo I was trying to use tflite converter to convert a pretrained model. When I use the same input to do inference by both tflite interpreter and native tensorflow I observed vastly different outputs . The difference is caused by the tfLite depthwise2d operator converted from the original  `tf.nn.conv2d` operator.  So I constructed a mini example to explain the problem:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n#construct a fake weight coeffecients\r\nnp.random.seed(0)\r\nweight_array= np.random.uniform(-1,1, size=(1,3,1,9)).astype(\"float32\")\r\n\r\n#construct a simple graph involving conv2d\r\ntf.reset_default_graph()\r\ninput_1 = tf.placeholder(dtype=tf.float32, shape=[1,1,600,1])\r\nweights = tf.constant(weight_array)\r\nweights = tf.quantization.fake_quant_with_min_max_args(weights, np.min(weight_array), np.max(weight_array))\r\nout = tf.nn.conv2d(input_1, weights, [1,2,2,1], \"SAME\")\r\nout_1 = tf.nn.relu6(out)\r\nq_out = tf.quantization.fake_quant_with_min_max_args(out_1, 0, 6)\r\n\r\n#convert the graph to tflite in unit8 quantized inference mode\r\nwith tf.Session() as sess:\r\n    converter = tf.contrib.lite.TFLiteConverter.from_session(sess, [input_1], [q_out])\r\n    converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\n    converter.inference_input_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\n    input_arrays = converter.get_input_arrays()\r\n    converter.quantized_input_stats = {input_arrays[0] : (128, 1)} \r\n    tflite_model = converter.convert()\r\n\r\n#define a dummy input\r\ninput_sig = np.ones([1,1,600,1])\r\ninput_data = input_sig.astype(\"uint8\")\r\n\r\n#inference using tflite interpreter\r\ninterpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\ninput_index = input_details[0]['index']\r\noutput_index = output_details[0]['index']\r\n\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_index, input_data)\r\ninterpreter.invoke()\r\n#get tflite int8 output\r\ntf_lite_output = interpreter.get_tensor(output_index)\r\n\r\nscale = output_details[0]['quantization'][0] #scale factor for quantized output\r\nweight_scale =  interpreter._get_tensor_details(0)['quantization'][0] #scale factor for quantized weights\r\nzero_point = output_details[0]['quantization'][1] #the point for quantized output maps to float 0 \r\n\r\n#get tensorflow float32 output from fake quantized node\r\nwith tf.Session() as sess:\r\n    tf_output = sess.run(q_out,  {input_1: input_sig})\r\n\r\n#compare, they should be at least close, but nope\r\nprint tf_lite_output[0][0][0] \r\nprint tf_output[0][0][0]/ scale + zero_point #converted to uint8 output using the quantization information in the output_details\r\n```\r\n**Describe the current behavior**\r\ntflite output gives:\r\n`[  0,   0,   0,   0,   0,   0, 255,   0,   0]`\r\nafter converting to uint8 by sc, tensorflow model gives:\r\n`[18. 74. 52. 35. 27.  0.  0.  4. 37.]`\r\n**Describe the expected behavior**\r\nI believe those two results should give a very similar result when fed with identical inputs. I also checked that the quantized weight matches with the output from fake quantization node after scaling using the `weight_scale`. I also manually calculated the operation defined in the graph and produced a result close to what tensorflow model gives. Is there a reason that tflite computation behaves completely different than the native tensorflow ?\r\n", "comments": ["Actually found the solution. Changed `converter.quantized_input_stats = {input_arrays[0] : (128, 1)} ` to `converter.quantized_input_stats = {input_arrays[0] : (0, 1)} ` and then tf and tflite gave almost same outputs.  Although I am pretty suprised that the output is that sensitive input stats.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27012\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27012\">No</a>\n", "hi shenyichen,\r\n     Why do you feed same input to Tf and Tflite ?\r\nI expected Tflite to get quantized input. But i see you are feeding the same input. \r\nAre you simulating same float models for both ?\r\n"]}, {"number": 27011, "title": "Add missing args in description for LSTM and GRU", "body": "Added time_major description for LSTM and GRU.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27011) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27011) for more info**.\n\n<!-- ok -->"]}, {"number": 27010, "title": "tf.keras.layers.Softmax does not support masking?", "body": "```\r\nimport tensorflow as tf\r\noutputs = tf.keras.layers.Softmax().apply(\r\n  tf.keras.layers.Masking().apply(\r\n    tf.zeros([3,5,7])\r\n  )\r\n)\r\n```\r\nSince the default mask value of Masking is zero, Softmax should skip all values in the above case, and its behavior should be like sparse softmax. Therefore, I suppose the output should be all zeros, but that is not the case.", "comments": ["@chwang85 can you elaborate a bit because the output is correct i guess,  which is  Tensor of shape `[3,5,7]` of value = `0.14285715` which is correct as shown here [wiki](https://en.wikipedia.org/wiki/Softmax_function)", "I think my question is clear enough...\r\nMaybe you need to know what is masking first...\r\n", "Sir, according to my understanding mask just skip those value which are equal to mask value.", "The default mask value is zero, so the Softmax should skip all values in my given case", "@chwang85 masking replace the value with 0 actually for example.\r\nt = tf.fill([2,2],5.0)\r\nm = tf.keras.layers.Masking(5)\r\nprint(m.apply(t))\r\n\"\"\" output \r\ntf.Tensor(\r\n[[0. 0.]\r\n [0. 0.]], shape=(2, 2), dtype=float32)\r\n\"\"\"\r\n\r\nt = tf.fill([2,2],5.0)\r\nm = tf.keras.layers.Masking() # default value 0.0\r\nprint(m.apply(t))\r\n\r\n\"\"\" output \r\n[[5. 5.]\r\n [5. 5.]], shape=(2, 2), dtype=float32)\r\n\"\"\"", "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\r\n\r\nMasks a sequence by using a mask value to skip timesteps.\r\n\r\nFor each timestep in the input tensor (dimension # 1 in the tensor), if all values in the input tensor at that timestep are equal to mask_value, then **the timestep will be masked (skipped) in all downstream layers** (as long as they support masking).", "No one can explain why?\r\nDoes Softmax support masking?\r\nIf so, why the masked values are not skipped in Softmax (the \"downstream\" layer of Masking)?", "@erikchwang: My notes here might help you understand masking better https://github.com/keras-team/keras/issues/3086#issuecomment-559690497", "So, can you explain the following question? \r\n```\r\nimport tensorflow as tf\r\noutputs = tf.keras.layers.Softmax().apply(\r\n  tf.keras.layers.Masking().apply(\r\n    tf.zeros([3,5,7])\r\n  )\r\n)\r\n```\r\nSince the default mask value of Masking is zero, Softmax should skip all values in the above case, and its behavior should be like sparse softmax. Therefore, I suppose the output should be all zeros, but that is not the case.", "@erikchwang: If you look at my second, third and fourth bullets in my comment, you will understand this. Yes - the output is not supposed to be zero all the time.\r\n\r\n\"- Masking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence.\r\n    Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\r\n - When we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\r\n  - I thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\r\n\r\n0 0 0 1 1 0 0 0\r\n\r\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\"", "I did not find this relevant to my question. Please just explain why the outputs of Softmax are not full zeros when all the inputs are masked? \r\n```\r\nimport tensorflow as tf\r\noutputs = tf.keras.layers.Softmax().apply(\r\n  tf.keras.layers.Masking().apply(\r\n    tf.zeros([3,5,7])\r\n  )\r\n)\r\n```\r\n", "@erikchwang: Even if you mask, the softmax layer still treats everything as usual. For instance if you put this: [3.,1.,2.,2.,0.,0.] into the softmax, regardless of whether you do masking or not, the output is always:\r\narray([[ 0.50744212,  0.06867483,  0.18667753,  0.18667753,  0.02526405, 0.02526405]])\r\n\r\nWhat masking does is that it notifies the loss computing that do not take into account the \"neuron\" that is masked, and that is it, no more no less. This is extremely useful, of course because we do padding all the time.\r\nAlso, it is very useful for LSTM as it skips inputs that have zeros (i.e. missing inputs - see my picture for the example why we need that). Note also that in case of LSTM it is a bit different in the sense that if you have a sequence of, say, 0 0 0 1 1 0 0 0, the output of the first three zeros is actually 0. But the output of the last three zeros is not 0.\r\n\r\nIn summary, dont' expect the output of masking is zeros, except LSTMs but in just a specific case like I shown.\r\n\r\n", "You made too many assumptions. I do not use LSTM, neither do I calculate loss, I just want to verify if Softmax support masking. Now it seems that the answer is NO.", "Sometimes we need more flexibility than just stacking keras layers...\r\nThe graph-style tf.layers is much more flexible than the dynamic tf.keras.layers, but it has been DEPRECATED...", "I think this is a perfectly valid question, which needs to be addressed. Can we reopen? Added on SO https://stackoverflow.com/questions/65745053/tensorflow-softmax-does-not-ignore-masking-value", "I like the answer here\r\nhttps://stackoverflow.com/questions/65745053/tensorflow-softmax-does-not-ignore-masking-value#65745327"]}, {"number": 27008, "title": "Cannot use load_model for a model with a DenseFeatures layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (https://www.tensorflow.org/alpha/tutorials/keras/feature_columns + model.save + load_model)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0.130/7.3.1\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\nloading a saved .h5 model which includes a DenseFeatures Layer fails:\r\n`ValueError: Unknown layer: DenseFeatures`\r\n\r\n**Describe the expected behavior**\r\nmodel is loading\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport pandas as pd\r\n\r\n# pip install -q tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nfeature_columns = []\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy')\r\n\r\nmodel.fit(train_ds, \r\n          validation_data=val_ds, \r\n          epochs=5)\r\n\r\n\r\nmodel.save('my_model.h5')\r\nfrom tensorflow import keras\r\nnew_model = keras.models.load_model('my_model.h5')\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"error.py\", line 67, in <module>\r\n    new_model = keras.models.load_model('my_model.h5')\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\", line 215, in load_model\r\n    custom_objects=custom_objects)\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 95, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 192, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 351, in from_config\r\n    custom_objects=custom_objects)\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 95, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 181, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"C:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 166, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: DenseFeatures\r\n```\r\n", "comments": ["I have a similar problem. I am trying to load tensorflow_hub module into tf.keras model.\r\n```python\r\n    model = tf.keras.models.Sequential()\r\n    model.add(hub.KerasLayer(\r\n        \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\",\r\n        trainable=True,\r\n        output_shape=[128],  # Outputs a tensor with shape [batch_size, 20].\r\n        input_shape=[],     # Expects a tensor of shape [batch_size] as input.\r\n        dtype=tf.string))\r\n```\r\nIt saves fine but when loading it gives this error:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/mnt/dev_files/FeatureExtractor/training/text_class_training/nlp_training.py\", line 103, in <module>\r\n    train_nlp(config, data)\r\n  File \"/mnt/dev_files/FeatureExtractor/training/text_class_training/nlp_training.py\", line 84, in train_nlp\r\n    new_model = tf.keras.models.load_model('training/text_class_training/test1.h5')\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 215, in load_model\r\n    custom_objects=custom_objects)\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 95, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 192, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 351, in from_config\r\n    custom_objects=custom_objects)\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 95, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 181, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/home/ali1234/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 166, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: KerasLayer\r\n```\r\n\r\nAny help here would be really appreciated.", "duplicate #26835\r\nClosing since we have open thread above.", "I am having the same problem too. When I am trying to load my model with a feature layer it gives me:\r\nValueError: Unknown layer: DenseFeatures", "same problem", "ValueError: Unknown layer: DenseFeatures\r\n", "Same problem", "same problem", "is there any solutions now ? I am also facing the same problem ", "same problem"]}, {"number": 27007, "title": "[INTEL MKL] Added support for quantized depthwise conv2D per channel.", "body": "", "comments": ["@penpornk Can you please take a look at this PR? Thank you.", "I'm not familiar with this code, so I'll unassign myself and leave the review to @penpornk", "@penpornk Except for combining all quantized depthwise ops into one fused op (which I'll be changing later), I think I have addressed all your review comments. Thanks", "Thanks for the review, @penpornk! Is it okay if we implement the fused op later (after the next TF release) so that this PR gets done now?", "@bhavani-subramanian I'm okay with that (unless API folks don't like it). I'll tag API review then.", "Adding ready to pull to trigger internal changelist creation so this shows up in the tf api review process.", "@rthadur Friendly ping for pulling.\r\nEdited: Thank you @hgadig for helping pull this in!"]}, {"number": 27006, "title": "Removed references to _ref types in documentation.", "body": "See #27005.", "comments": ["I am not a good reviewer for this change. Please redirect.", "I'm also not a good reviewer for this. Maybe @alextp or @superbobry "]}, {"number": 27005, "title": "[TF 2.0] Documentation still mentions _ref types", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:2.0.0-alpha\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/dtypes/DType\r\n\r\n\r\n**Describe the documentation issue**\r\nThe documentation of TF 2.0 still mentions ```_ref``` types even though it is no longer a thing.\r\nRelated issue #26941.\r\n\r\n\r\n \r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes", "comments": ["Removing:\r\n\r\n> In addition, variants of these types with the _ref suffix are defined for reference-typed tensors.\r\n\r\nAs well as:\r\n\r\n```python\r\nDType(T)       .is_compatible_with(DType(T).as_ref) == True\r\nDType(T).as_ref.is_compatible_with(DType(T))        == False\r\nDType(T).as_ref.is_compatible_with(DType(T).as_ref) == True\r\n```\r\n\r\nfrom the reference documentation. Thanks for alerting us to this issue!"]}, {"number": 27004, "title": "WARNING: Entity <method-wrapper '__call__' of weakref object...", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): one line modification to official [colab checkpoints.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/checkpoints.ipynb)\r\n- Tensorflow version: 2.0.0-alpha0\r\n\r\n**Describe the current behavior**\r\nAfter the model has been loaded from a checkpoint, the following messages appear:\r\n```\r\nW0321 20:29:37.423420 139748586436480 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7f197ec58278> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\nW0321 20:29:37.434415 139748586436480 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x7f197ebfad68> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\nWARNING: Entity <method-wrapper '__call__' of weakref object at 0x7f197ec58278> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\nWARNING: Entity <method-wrapper '__call__' of weakref object at 0x7f197ebfad68> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo warning messages.\r\n\r\n**Code to reproduce the issue**\r\nDecorate `def call` with `@tf.function` in the second cell of the notebook:\r\n```\r\nclass Net(tf.keras.Model):\r\n  \"\"\"A simple linear model.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(Net, self).__init__()\r\n    self.l1 = tf.keras.layers.Dense(5)\r\n\r\n  @tf.function\r\n  def call(self, x):\r\n    return self.l1(x)\r\n```\r\n", "comments": ["I met the same problem with the following code running second time. It can be fixed by uncomment the line \"fc(x)\". So I think it is related to the restoration of Checkpoint.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python import keras\r\n\r\n@tf.function\r\ndef forward(net, x):\r\n    return net(x)\r\n\r\nx = tf.random.normal([10, 10])\r\nfc = keras.layers.Dense(100)\r\n\r\ncp = tf.train.Checkpoint(fc=fc)\r\ncp.restore(tf.train.latest_checkpoint('log/test_cp'))\r\n\r\n# fc(x)\r\ny = forward(fc, x)\r\n\r\ncp.save('log/test_cp/haha')\r\n```", "Yes, it happens when restoring from check points, although the root cause is unrelated.\r\n\r\nThe issue should be fixed in the nightly build by https://github.com/mark0725/tensorflow/pull/1003/commits/7463cae99f784336aed55f6cf5c46d202141bdd9.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27004\">No</a>\n", "I have meet the same trouble, why? @mdanatg ", "@xiaohu2015 The alpha preview is fairly outdated. Are you seeing the issue even if you `pip install tf-nightly-2.0-preview`?", "> @xiaohu2015 The alpha preview is fairly outdated. Are you seeing the issue even if you `pip install tf-nightly-2.0-preview`?\r\n\r\nI built tensorflow-2.0 from source, maybe I need try the new.", "It's worth checking the git revision of your repo. If the source you are building from is at head, then this might be a regression."]}, {"number": 27003, "title": "TF2 has tf.floor but not tf.ceil", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):n/a\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tf-nightly-gpu-2.0-preview-2.0.0.dev20190319\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.floor)\r\nprint(tf.ceil)\r\n```\r\nprints:\r\n```\r\n<function floor at 0x7fad8ad08950>\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 6, in <module>\r\n    print(tf.ceil)\r\nAttributeError: module 'tensorflow' has no attribute 'ceil'\r\n```\r\n`tf.ceil` is only accessible through `tf.math.ceil` now. But `tf.floor` still exists.\r\n\r\nI expect the two functions to either both exist or both not exist.", "comments": ["i am not getting the link , where i can contribute as in documentation it is written that tf.math.ceil() is in tensorflow/python/ops/gen_math_ops.py but there is no such file in tensorflow codebase. ", "The gen_math_ops.py file is missing in repo of Tensorflow 2.0.0-alpha0, but the functions of this file are working :confused: I think it is deprecated!\r\n\r\nThere is [already](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/ceil) ```tf.math.ceil``` function present so no need to be added.", "Added a PR #27068 for the fix to deprecate tf.floor in TF 2.0.", "@ppwwyyxx most of the math ops are moved under tf.math.*. Both the ceil and floor are in TF as `tf.math.ceil` and `tf.math.floor`. As @alextp mentioned in the related PR https://github.com/tensorflow/tensorflow/pull/27068 that `TF2 API frozen, so we can't remove it anymore` in the current TF.2.0rc0. \r\n\r\nI am closing this issue. Feel free to reopen if you have any concerns. Thanks!\r\n\r\n\r\n"]}, {"number": 27002, "title": "sigdev error when training model with image data generator", "body": "Hi \r\n\r\nsee also [SO question](https://stackoverflow.com/questions/55283194/tensorflow-gpu-sigdev-error-when-training-on-image-data-generator) regarding this issue \r\n\r\nI am using ubuntu with conda, python 3.6 and tf gpu 1.12, and using image data generator to load images, and then fit generator. \r\n\r\nI am using this code \r\n\r\n    i = gen.flow_from_dataframe(dataframe=df, x_col=\"new_filename\",\r\n                                y_col=\"label\", class_mode=\"sparse\",\r\n                                directory=links_dir, target_size=(224, 224), batch_size=32)\r\n    \r\n    model.fit_generator(i, epochs=1, workers=1,\r\n                        use_multiprocessing=False, max_queue_size=1,\r\n                        verbose=1)\r\n\r\nand getting this error, without any batches even running \r\n\r\n> Process finished with exit code 139 (interrupted by signal 11:\r\n> SIGSEGV)\r\n\r\nI validated that the generator is loading the images using \r\n\r\nI used nvidia smi and made sure the model is created on GPU and the gpu memory is used.\r\n\r\nThe model is a simple, small conv net, nothing this GPU can't handle. \r\n\r\nprint(gen[0]) and got data and everything went fine.\r\nSo I assume that the image data is fine.\r\n\r\nWhat can be the issue here? could this be happening because I have two GPU's in the machine and I am only using one?\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@ymodak \r\nI suspect this is a bug, given it does not work as expected (I used the same code many times without any issues in the past)", "@thebeancounter Before I reopen the issue, can you please tell me what changes you made to your configuration that might have caused your code to crash since you were able to execute it successfully in the past ? Thanks!"]}, {"number": 27001, "title": "[INTEL MKL] Fix a bug related to quantized  MklConvOp ", "body": "1. Fix a bug which is related to the quantized MklConvOp. This bug prevents the execution of INT8 inference on the inception-resnet-v2 model.\r\n2. Fix clang coding style issues (source file: mkl_conv_ops.cc) - Background: a recent merged \"Per-Channel\" PR by Intel should have the fix. but somehow it was not merged into public master. ", "comments": []}, {"number": 27000, "title": "estimator built with keras.estimator.model_to_estimator fails with CollectiveAllReduceStrategy", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS 7 on a HPC system\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary (from anaconda)\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\n3.6.8\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen converting a keras model to an estimator, it can be trained in a non-distributed environment,\r\nbut it fails with CollectiveAllReduceStrategy on multiple machines. However, when building an estimator from scratch (without keras), CollectiveAllReduceStrategy works.\r\n**Describe the expected behavior**\r\nestimator created from keras models should be trainable with CollectiveAllReduceStrategy.\r\n\r\n**Code to reproduce the issue**\r\n\r\nthe following script fails when run in a distributed way in a slurm-cluster:\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\nimport os\r\nimport sys\r\nimport json\r\n\r\n\r\n\r\ntrain_number = \"test\"\r\nworker_ips = sys.argv[1]\r\nport = sys.argv[2]\r\nworker_index = int(sys.argv[3])\r\nmodeldir = sys.argv[4]\r\nworkers = [ip+':'+port for ip in worker_ips.split(',')]\r\n\r\nos.environ[\"TF_CONFIG\"] = json.dumps({\r\n    \"cluster\": {\r\n        \"worker\": workers\r\n    },\r\n   \"task\": {\"type\": \"worker\", \"index\": worker_index}\r\n})\r\n\r\ndistribution = tf.contrib.distribute.CollectiveAllReduceStrategy(\r\n    num_gpus_per_worker=0)\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution, model_dir=modeldir)\r\n\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001),\r\nmetrics=['accuracy'])\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model, config=config)\r\n\r\ninput_name = model.input_names[0]\r\n\r\n\r\ndef gen():\r\n    for x,y in zip(x_train,y_train):\r\n        yield x, y\r\n\r\n\r\ndef train_input_fn():\r\n    dataset = tf.data.Dataset.from_generator(\r\n        gen, (tf.float32, tf.float32), output_shapes=(x_train.shape[1:],y_train.shape[1:]))\r\n\r\n    return dataset.shuffle(1).repeat(epochs).batch(batch_size)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\r\neval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn)\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nthe multi-machine job is started with the following script\r\n```\r\n#! /bin/bash\r\n\r\n# N must be >=2\r\n\r\n#SBATCH -N2\r\n#SBATCH -A snic2018-1-13\r\n#SBATCH --time=00:30:00\r\n\r\nnodelists=$(scontrol show hostname $SLURM_JOB_NODELIST | paste -d, -s)\r\nIFS=',' read -r -a nodearray <<< \"$nodelists\"\r\n\r\n\r\nsource activate tf-dist-env\r\n\r\nport=2222\r\n\r\nmodeldir=$(mktemp -d --tmpdir='.')\r\nrm -r $modeldir\r\nmkdir $modeldir\r\n\r\nfor i in $(seq 1 $SLURM_JOB_NUM_NODES); do\r\n# convert to 0-based index\r\n    i=$((i-1))\r\n    echo starting job $i\r\n    srun --nodelist=\"${nodearray[i]}\"  -N1 -n1 python test-tf-estimators-from-keras_distributed.py ${nodelists} ${port} ${i} ${modeldir} &\r\ndone\r\nwait\r\n```\r\n\r\n\r\nwhich produces one of the following error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b70cc944a40, 0x2b70cc944ec0] backing_tensor bounds = [0x2b70cc01e000, 0x2b70cc4b1c28]\r\n\t [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=\"scoped_allocator_1\", shape=[1199882], _device=\"/job:worker/replica:0/task:1/device:CPU:0\"](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test-tf-estimators-from-keras_distributed.py\", line 100, in <module>\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 462, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 279, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 792, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 344, in _run_single_worker\r\n    worker_fn(strategy)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 246, in _worker_fn\r\n    hooks=hooks)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1352, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1471, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run\r\n    run_metadata=run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1312, in run\r\n    run_metadata=run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b70cc944a40, 0x2b70cc944ec0] backing_tensor bounds = [0x2b70cc01e000, 0x2b70cc4b1c28]\r\n\t [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=\"scoped_allocator_1\", shape=[1199882], _device=\"/job:worker/replica:0/task:1/device:CPU:0\"](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]\r\n2019-03-21 17:18:47.019340: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b7b2409eb00, 0x2b7b2409ef80] backing_tensor bounds = [0x2b7a76ae2040, 0x2b7a76f75c68]\r\n\t [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=\"scoped_allocator_1\", shape=[1199882], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\nTraceback (most recent call last):\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b7b2409eb00, 0x2b7b2409ef80] backing_tensor bounds = [0x2b7a76ae2040, 0x2b7a76f75c68]\r\n\t [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=\"scoped_allocator_1\", shape=[1199882], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test-tf-estimators-from-keras_distributed.py\", line 100, in <module>\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 462, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 279, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 792, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 344, in _run_single_worker\r\n    worker_fn(strategy)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 246, in _worker_fn\r\n    hooks=hooks)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1352, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1471, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run\r\n    run_metadata=run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1312, in run\r\n    run_metadata=run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/proj/bolinc/users/x_sebsc/anaconda3/envs/tf-dist-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Upper bound check fail for input 1 from node training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1 input bounds = [0x2b7b2409eb00, 0x2b7b2409ef80] backing_tensor bounds = [0x2b7a76ae2040, 0x2b7a76f75c68]\r\n\t [[{{node scoped_allocator_concat_1}} = _ScopedAllocatorConcat[N=8, T=DT_FLOAT, id=1, reshape=false, sa_name=\"scoped_allocator_1\", shape=[1199882], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](scoped_allocator_1, training/TFOptimizer/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter, training/TFOptimizer/gradients/conv2d_1/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense/BiasAdd_grad/BiasAddGrad, training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul_1, training/TFOptimizer/gradients/dense_1/BiasAdd_grad/BiasAddGrad, ^training/TFOptimizer/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInput, ^training/TFOptimizer/gradients/dense/MatMul_grad/MatMul, ^training/TFOptimizer/gradients/dense_1/MatMul_grad/MatMul)]]\r\n```\r\n\r\n\r\n\r\nWhen instead running the following script on a single machine, it works (same as the python script for the distributed job, but without the configuration for distribution):\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\nimport os\r\nimport sys\r\nimport json\r\n\r\n\r\n\r\n\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001),\r\nmetrics=['accuracy'])\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n\r\ninput_name = model.input_names[0]\r\n\r\n\r\ndef gen():\r\n    for x,y in zip(x_train,y_train):\r\n        yield x, y\r\n\r\n\r\ndef train_input_fn():\r\n    dataset = tf.data.Dataset.from_generator(\r\n        gen, (tf.float32, tf.float32), output_shapes=(x_train.shape[1:],y_train.shape[1:]))\r\n\r\n    return dataset.shuffle(1).repeat(epochs).batch(batch_size)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)\r\neval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn)\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nThe following minimum example using an estimator not created with keras works with CollectiveAllReduceStrategy\r\n\r\n\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\n\r\nimport sys\r\nworker_ips = sys.argv[1]\r\nport = sys.argv[2]\r\nworker_index = int(sys.argv[3])\r\nmodeldir = sys.argv[4]\r\nworkers = [ip+':'+port for ip in worker_ips.split(',')]\r\n\r\nos.environ[\"TF_CONFIG\"] = json.dumps({\r\n    \"cluster\": {\r\n        \"worker\": workers\r\n    },\r\n   \"task\": {\"type\": \"worker\", \"index\": worker_index}\r\n})\r\n\r\ndistribution = tf.contrib.distribute.CollectiveAllReduceStrategy(\r\n    num_gpus_per_worker=0)\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution, model_dir=modeldir)\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    layer = tf.layers.Dense(1)\r\n    logits = layer(features)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\"logits\": logits}\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.mean_squared_error(\r\n        labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\r\ndef input_fn():\r\n    print('debug: calling input_fn')\r\n    # check whether input_fn can see info about the worker is bein\r\n    # executed it\r\n    # print(os.environ[\"TF_CONFIG\"]) # this does not work...\r\n    print(os.environ)\r\n    print('dir():', dir())\r\n    print('globals():', globals())\r\n    print('locals():', locals())\r\n    features = tf.data.Dataset.from_tensors([[1.]]).repeat(100000)\r\n    labels = tf.data.Dataset.from_tensors(1.).repeat(100000)\r\n    return tf.data.Dataset.zip((features, labels))\r\n\r\n\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=10000)\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n```", "comments": ["problem solved with upgrading to 1.13.1", "just found out that actually the problem was not the tensorflow version, but the fact that i used 1.12.0 from conda. So it works both with 1.12.0 and 1.13.1 from pip. "]}, {"number": 26999, "title": "Fix TensorFlow incompatibility with --incompatible_no_transitive_loads", "body": "See https://github.com/bazelbuild/bazel/issues/5636\r\n\r\nFYI @laurentlb ", "comments": ["OK, we'll need to wait for upgrading Bazel to 0.23.2, because the new rules_closure doesn't work with old Bazel version anymore."]}, {"number": 26998, "title": "2.0 Reference Models: MobileNetV2 (1 GPU, 8 GPU with dist strat and Keras)", "body": "**MobileNetV2** is a significant improvement over [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) and pushes the state of the art for mobile visual recognition including classification, object detection and semantic segmentation. It builds upon the ideas from MobileNetV1, using depthwise separable convolution as efficient building blocks. However, V2 introduces two new features to the architecture: \r\n\r\n1. Linear bottlenecks between the layers, and \r\n2. Shortcut connections between the bottlenecks1.\r\n\r\nThe research team's academic paper describes MobileNetV2 in detail: https://arxiv.org/abs/1801.04381.\r\n\r\nAn example of MobileNetV2 implemented with Slim can be found [here](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["@dynamicwebpaige I'ld like to work on this. can you please assign this to me?\r\n", "@dynamicwebpaige Can You give me some pointers how to start? I went through the repository to find something where the model is implemented. However all I found is that there's no implementation of mobiilenet_v2 in tensorflow. The model is being imported from keras_applications and the appropriate object is returned when tf.keras.applications.MobileNetV2 is invoked.\r\n**REF:**\r\nhttps://github.com/tensorflow/tensorflow/blob/f665d092254845812ed0e89ccda57c5873a34dd1/tensorflow/python/keras/applications/mobilenet_v2.py#L22\r\nhttps://github.com/tensorflow/tensorflow/blob/f665d092254845812ed0e89ccda57c5873a34dd1/tensorflow/python/keras/applications/mobilenet_v2.py#L31-L32", "@dynamicwebpaige Look like you are looking for the following feature [link1]([url](url)) , [link2](https://keras.io/api/applications/mobilenet/)\r\n\r\nIf you think the above is not the feature you are looking, please feel free to open a PR in [keras-team/keras](https://github.com/keras-team/keras/issues) repository.\r\nPlease note that Keras development moved to [keras-team/keras](https://github.com/keras-team/keras/issues) repository to focus on only keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 26997, "title": "2.0 Reference Models: MobileNetv2 (TPU with dist strat and Keras)", "body": "**MobileNetV2** is a significant improvement over [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) and pushes the state of the art for mobile visual recognition including classification, object detection and semantic segmentation. It builds upon the ideas from MobileNetV1, using depthwise separable convolution as efficient building blocks. However, V2 introduces two new features to the architecture: \r\n\r\n1. Linear bottlenecks between the layers, and \r\n2. Shortcut connections between the bottlenecks1.\r\n\r\nThe research team's academic paper describes MobileNetV2 in detail: https://arxiv.org/abs/1801.04381.\r\n\r\nAn example of MobileNetV2 implemented with Slim can be found [here](https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["@dynamicwebpaige I'd like to contribute this.", "@dynamicwebpaige Could you please have a look on the [link1](https://github.com/tensorflow/models/blob/master/research/object_detection/models/keras_models/mobilenet_v2.py) , [link2](https://colab.research.google.com/github/rok/models/blob/mobilenet_v2_tpu_with_dist_strat_and_keras/research/object_detection/mobilenet_v2_example.ipynb) , [link3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/MobileNetV2) and let us know if it helps?  \r\nIf you still need help on this issue please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues),To know more see; [https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]