[{"number": 46987, "title": "how to find what tensorflow/keras release contains a specific bug fix", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nThis is related to a known bug (see below in italics), but I'm trying to figure out how to install a version of tf/keras which has the fix.\r\n**System information**\r\n- OS Platform and Distribution -> Ubuntu 20.0.4\r\n- TensorFlow installed from -> docker tensorflow-gpu \r\n- TensorFlow version: 2.5.0-dev20210204 from docker tensorflow/tensorflow:nightly-gpu\r\n- Python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \\n[GCC 8.4.0]\r\n- Installed usinga  Dockerfile to add some required Python libraries to tensorflow-gpu image\r\n- CUDA/cuDNN version:Cuda compilation tools, release 11.0, V11.0.221\r\nBuild cuda_11.0_bu.TC445_37.28845127_0\r\n- GPU model and memory: Nvidia RTX 2080 Ti\r\n\r\nI'm training a large (millions of parameters) NN to analyze audio files (speech vs. music).  After 28/50 epochs I hit a known bug which is (to my knowledge) fixed already.  I'm not sure how to find out what tf/keras release the fix is in or if my docker image has the fix.  I'm using a docker image since it was recommended as the easiest way to get version compatibility between tf/keras and CUDA. \r\n\r\nThis is the bug description and reference to the fix:\r\n\r\n_Tensorflow 2.1 Error \u201cwhen finalizing GeneratorDataset iterator\u201d - a memory leak? #37515\r\n\r\nURL: https://github.com/tensorflow/tensorflow/commit/e918c6e6fab5d0005fcde83d57e92b70343d3553\r\nFixing a memory leak in Keras.\r\nFixes: #37515\r\nPiperOrigin-RevId: 302568217\r\nChange-Id: I28d0eaf3602fea0461901680df24899f135ce649_\r\n\r\n", "comments": ["@jmm5491,\r\nThe error mentioned in [#37515](https://github.com/tensorflow/tensorflow/issues/37515) seems to have been fixed in TensorFlow v2.2, so TensorFlow v2.5 should also work in this case.\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here and the dataset you are using. Thanks!", "I hit this problem when running someone else's github project, so I'm not very familiar with the code.  I'll look into it some more and see what I can do.  It uses a large dataset. FYI, it is this project: https://github.com/qlemaire22/speech-music-detection.\r\n", "I can provide a tgz file including the project code and one dataset with which what I run to reproduce the problem.  It's 4Gb compressed.  With slightly more steps (using sox app to convert mp3 files to a series of .wav files, plus .npy files), I can reduce the file size quite a bit.  I'm not sure the best way to get it to you.  I  could upload it to my Google Drive and make it available that way, for example.", "> I could upload it to my Google Drive and make it available that way, for example.\r\n\r\n@jmm5491,\r\nYes please, you can share the code and the dataset from your Google Drive.\r\n\r\nAlternatively, you can also run your code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I tried to reproduce this on Colab using the complete 4Gb dataset, but rather than seeing the symptom described in my original post, the python process hung.  Because of this and the fact that other users don't seem to have this problem, I'm going to assume the problem is in the code I'm using.  This issue can be closed now.  Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46987\">No</a>\n"]}, {"number": 46986, "title": "Run MIRNet and get Segment Fault on Raspberry Pi Model 3B", "body": "Run [MIRNet](https://github.com/sayakpaul/MIRNet-TFLite-TRT) and get Segment Fault on Raspberry Pi Model 3B.\r\nThe tflite model I used is from [dynamic_shape](https://github.com/sayakpaul/MIRNet-TFLite/releases/download/v0.1.0/dynamic_shape.zip) and named mirnet_dr.tflite.\r\nFollowing is the error info from my binary built from [our repo](https://github.com/Duan-JM/edge-brain):\r\n```\r\n#0  0x003648dc in ruy::TrMul(ruy::Ctx*, ruy::TrMulParams*) ()\r\n#1  0x0035fd84 in ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*) ()\r\n#2  0x00095c84 in void ruy::MulFrontEnd<(ruy::Path)17, signed char, signed char, int, int>(ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::MulParams<int, int> const&, ruy::Ctx*, ruy::Mat<int>*) ()\r\n#3  0x00091918 in void ruy::Mul<(ruy::Path)17, signed char, signed char, int, int>(ruy::Matrix<signed char> const&, ruy::Matrix<signed char> const&, ruy::MulParams<int, int> const&, ruy::Context*, ruy::Matrix<int>*) ()\r\n#4  0x0033ba00 in tflite::tensor_utils::NeonCpuBackendGemm(signed char const*, int const*, signed char const*, int, int, int, int, int*, tflite::CpuBackendContext*) ()\r\n#5  0x0033ba88 in tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, int*, float*, tflite::CpuBackendContext*) ()\r\n#6  0x000f0c74 in tflite::optimized_ops::HybridConv(tflite::ConvParams const&, float*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, int*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*) ()\r\n#7  0x001088c4 in TfLiteStatus tflite::ops::builtin::conv::EvalHybrid<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) ()\r\n#8  0x000fe188 in TfLiteStatus tflite::ops::builtin::conv::EvalImpl<(tflite::ops::builtin::conv::KernelType)2, (TfLiteType)1>(TfLiteContext*, TfLiteNode*) ()\r\n#9  0x000f68dc in TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*) ()\r\n#10 0x00391c6c in tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*) ()\r\n#11 0x0038e940 in tflite::Subgraph::Invoke() ()\r\n#12 0x0039e354 in tflite::Interpreter::Invoke() ()\r\n#13 0x0001ff24 in mirnet::MIRNet::RunInference(cv::Mat const&, cv::Mat&) ()\r\n#14 0x00020188 in mirnet::MIRNet::EnhanceImage(cv::Mat const&, cv::Mat&) ()\r\n#15 0x0001c5d0 in main ()\r\n```\r\nBTW, it also was crash with minimal.cc examples from tflite repo, please take a check. \r\nThanks.", "comments": ["@multiverse-tf @wangtz @renjie-liu, Hi all, anyone can give me a hand on it?", "The version of [our repo](https://github.com/Duan-JM/edge-brain) got crash is commit 08cb572f383044dbf96e1cdd657b5c1229a67e34.", "@SunAriesCN \r\nPlease share a simple stand alone code for us to replicate the issue reported, or share a colab gist.", "@Saduf2019 \r\nHi, Saduf\r\nYou can just go to our repo I mentioned, that is with link, and run the `build_hello_world.sh`. \r\nYou will get a binary on `bazel-out/armhf-fastbuild/bin/examples/hello_world` directory named `hello_world`.\r\nAnd you just place it on a RPI platform, and run it with the following instructions:\r\n```\r\n$ ./hello_wolrd mirnet_dr.tflite\r\n```\r\nThen you may get a segment fault, and when you use `gdb` on RPI to check it, the above logs will come out.\r\nThe `mirnet_dr.tflite` is in the [zip](https://github.com/sayakpaul/MIRNet-TFLite/releases/download/v0.1.0/dynamic_shape.zip) here, it's easy to download and find.\r\n\r\nThanks for your reply.", "> @Saduf2019\r\n> Hi, Saduf\r\n> You can just go to our repo I mentioned, that is with link, and run the `build_hello_world.sh`.\r\n> You will get a binary on `bazel-out/armhf-fastbuild/bin/examples/hello_world` directory named `hello_world`.\r\n> And you just place it on a RPI platform, and run it with the following instructions:\r\n> \r\n> ```\r\n> $ ./hello_wolrd mirnet_dr.tflite\r\n> ```\r\n> \r\n> Then you may get a segment fault, and when you use `gdb` on RPI to check it, the above logs will come out.\r\n> The `mirnet_dr.tflite` is in the [zip](https://github.com/sayakpaul/MIRNet-TFLite/releases/download/v0.1.0/dynamic_shape.zip) here, it's easy to download and find.\r\n> \r\n> Thanks for your reply.\r\n\r\nBTW, the `hello_wolrd` we build with, are just the same codes from [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) in TFLite. I think it's a little bit easy for one to reproduce this crash.", "> > @Saduf2019\r\n> > Hi, Saduf\r\n> > You can just go to our repo I mentioned, that is with link, and run the `build_hello_world.sh`.\r\n> > You will get a binary on `bazel-out/armhf-fastbuild/bin/examples/hello_world` directory named `hello_world`.\r\n> > And you just place it on a RPI platform, and run it with the following instructions:\r\n> > ```\r\n> > $ ./hello_wolrd mirnet_dr.tflite\r\n> > ```\r\n> > \r\n> > \r\n> > Then you may get a segment fault, and when you use `gdb` on RPI to check it, the above logs will come out.\r\n> > The `mirnet_dr.tflite` is in the [zip](https://github.com/sayakpaul/MIRNet-TFLite/releases/download/v0.1.0/dynamic_shape.zip) here, it's easy to download and find.\r\n> > Thanks for your reply.\r\n> \r\n> BTW, the `hello_wolrd` we build with, are just the same codes from [minimal.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc) in TFLite. I think it's a little bit easy for one to reproduce this crash.\r\n\r\nAnd I'm really sorry about our English [README_EN.md](https://github.com/Duan-JM/edge-brain/blob/main/README_en.md) is not obvious enough that may confused you, here is our [README_EN.md](https://github.com/Duan-JM/edge-brain/blob/main/README_en.md) in English version.", "@SunAriesCN\r\nPlease update the tf version used.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46986\">No</a>\n", "> Run [MIRNet](https://github.com/sayakpaul/MIRNet-TFLite-TRT) and get Segment Fault on Raspberry Pi Model 3B.\r\n> The tflite model I used is from [dynamic_shape](https://github.com/sayakpaul/MIRNet-TFLite/releases/download/v0.1.0/dynamic_shape.zip) and named mirnet_dr.tflite.\r\n> Following is the error info from my binary built from [our repo](https://github.com/Duan-JM/edge-brain):\r\n> \r\n> ```\r\n> #0  0x003648dc in ruy::TrMul(ruy::Ctx*, ruy::TrMulParams*) ()\r\n> #1  0x0035fd84 in ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*) ()\r\n> #2  0x00095c84 in void ruy::MulFrontEnd<(ruy::Path)17, signed char, signed char, int, int>(ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::MulParams<int, int> const&, ruy::Ctx*, ruy::Mat<int>*) ()\r\n> #3  0x00091918 in void ruy::Mul<(ruy::Path)17, signed char, signed char, int, int>(ruy::Matrix<signed char> const&, ruy::Matrix<signed char> const&, ruy::MulParams<int, int> const&, ruy::Context*, ruy::Matrix<int>*) ()\r\n> #4  0x0033ba00 in tflite::tensor_utils::NeonCpuBackendGemm(signed char const*, int const*, signed char const*, int, int, int, int, int*, tflite::CpuBackendContext*) ()\r\n> #5  0x0033ba88 in tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(signed char const*, int, int, signed char const*, float const*, int, int*, float*, tflite::CpuBackendContext*) ()\r\n> #6  0x000f0c74 in tflite::optimized_ops::HybridConv(tflite::ConvParams const&, float*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, int*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*) ()\r\n> #7  0x001088c4 in TfLiteStatus tflite::ops::builtin::conv::EvalHybrid<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) ()\r\n> #8  0x000fe188 in TfLiteStatus tflite::ops::builtin::conv::EvalImpl<(tflite::ops::builtin::conv::KernelType)2, (TfLiteType)1>(TfLiteContext*, TfLiteNode*) ()\r\n> #9  0x000f68dc in TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*) ()\r\n> #10 0x00391c6c in tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*) ()\r\n> #11 0x0038e940 in tflite::Subgraph::Invoke() ()\r\n> #12 0x0039e354 in tflite::Interpreter::Invoke() ()\r\n> #13 0x0001ff24 in mirnet::MIRNet::RunInference(cv::Mat const&, cv::Mat&) ()\r\n> #14 0x00020188 in mirnet::MIRNet::EnhanceImage(cv::Mat const&, cv::Mat&) ()\r\n> #15 0x0001c5d0 in main ()\r\n> ```\r\n> \r\n> BTW, it also was crash with minimal.cc examples from tflite repo, please take a check.\r\n> Thanks.\r\n\r\nSorry for the late reply. I also had a segfault issue recently with a very similar stack trace. And it turned out to be caused by an int overflow when handling very large tensors in the RUY library. A fix is WIP. After it's pushed, pls patch it and see if this happened again or not. ", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46986\">No</a>\n", "Hi all, any good news for it?", "The fix I mentioned earlier was pushed 3 days ago in the RUY repo (https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59). Could you try to patch it and see whether it fixes the crash or not? Thx!", "Well cool, I think TensorFlow([tensorflow/tensorflow@f779f42](https://github.com/tensorflow/tensorflow/commit/f779f42004d1873784cb2b4cb351d4b342ad0816)) could be easier for me to check, isn't it?\r\n\r\n> The fix I mentioned earlier was pushed 3 days ago in the RUY repo ([google/ruy@54774a7](https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59)). Could you try to patch it and see whether it fixes the crash or not? Thx!\r\n\r\n", "> Well cool, I think TensorFlow([tensorflow/tensorflow@f779f42](https://github.com/tensorflow/tensorflow/commit/f779f42004d1873784cb2b4cb351d4b342ad0816)) could be easier for me to check, isn't it?\r\n\r\nIndeed!\r\n\r\n> \r\n> > The fix I mentioned earlier was pushed 3 days ago in the RUY repo ([google/ruy@54774a7](https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59)). Could you try to patch it and see whether it fixes the crash or not? Thx!\r\n\r\n", "Btw, as the model has dynamic shape, what's the proper input layer shape to be specified when running the inference? I am just wondering if the fix could be verified by the benchmark model tool which supports specifying input layer name and its shape.\r\n\r\n> > Well cool, I think TensorFlow([tensorflow/tensorflow@f779f42](https://github.com/tensorflow/tensorflow/commit/f779f42004d1873784cb2b4cb351d4b342ad0816)) could be easier for me to check, isn't it?\r\n> \r\n> Indeed!\r\n> \r\n> > > The fix I mentioned earlier was pushed 3 days ago in the RUY repo ([google/ruy@54774a7](https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59)). Could you try to patch it and see whether it fixes the crash or not? Thx!\r\n\r\n", "Emmmm....I think this fix can be checked by the [hello world example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc), because I have ever got the similar crash with that tool.\r\n> Btw, as the model has dynamic shape, what's the proper input layer shape to be specified when running the inference? I am just wondering if the fix could be verified by the benchmark model tool which supports specifying input layer name and its shape.\r\n> \r\n> > > Well cool, I think TensorFlow([tensorflow/tensorflow@f779f42](https://github.com/tensorflow/tensorflow/commit/f779f42004d1873784cb2b4cb351d4b342ad0816)) could be easier for me to check, isn't it?\r\n> > \r\n> > \r\n> > Indeed!\r\n> > > > The fix I mentioned earlier was pushed 3 days ago in the RUY repo ([google/ruy@54774a7](https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59)). Could you try to patch it and see whether it fixes the crash or not? Thx!\r\n\r\n", "> Emmmm....I think this fix can be checked by the [hello world example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc), because I have ever got the similar crash with that tool.\r\n\r\nI just checked the mirnet_dr.tflite model with the minimal binary w/ the patch. The binary still crashed with segfault. But the stack trace is quite different as shown below:\r\n\r\n#0  tflite::tensor_utils::NeonSymmetricQuantizeFloats (values=0x0, size=0, quantized_values=0x0, min=0x7fffffffd458, max=0x7fffffffd454, scaling_factor=0x0)\r\n    at tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:2293\r\n#1  0x00005555559324f6 in tflite::ops::builtin::conv::EvalHybrid<(tflite::ops::builtin::conv::KernelType)2> (context=0x57a83fc887c8, node=0x57a83fcdcce0, \r\n    params=0x57a83fc014c0, data=0x57a83fd0e9c0, input=0x57a83fd3d680, filter=0x57a83fd27e30, bias=0x57a83fd359d0, im2col=0x0, accum_scratch=0x57a83fd6f020, \r\n    output=0x57a83fd3d6f0) at tensorflow/lite/kernels/conv.cc:949\r\n#2  0x00005555559315ae in tflite::ops::builtin::conv::EvalImpl<(tflite::ops::builtin::conv::KernelType)2, (TfLiteType)1> (context=0x57a83fc887c8, node=0x57a83fcdcce0)\r\n    at tensorflow/lite/kernels/conv.cc:1027\r\n#3  0x00005555558b01f9 in tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2> (context=0x57a83fc887c8, node=0x57a83fcdcce0)\r\n    at tensorflow/lite/kernels/conv.cc:1064\r\n#4  0x0000555555bfc4ed in tflite::Subgraph::OpInvoke (this=0x57a83fc887a0, op_reg=..., node=0x57a83fcdcce0) at /tensorflow/lite/core/subgraph.h:427\r\n#5  0x0000555555bf98a3 in tflite::Subgraph::Invoke (this=0x57a83fc887a0) at tensorflow/lite/core/subgraph.cc:1070\r\n#6  0x0000555555c062d9 in tflite::Interpreter::Invoke (this=0x57a83fc86380) at tensorflow/lite/interpreter.cc:284\r\n#7  0x00005555557f2b21 in main (argc=2, argv=0x7fffffffd9b8) at tensorflow/lite/examples/minimal/minimal.cc:70\r\n\r\nI guess we probably need to specify proper input shape to make the model inference run, and this is why I suggested using the benchmark model earlier.\r\n> \r\n> > Btw, as the model has dynamic shape, what's the proper input layer shape to be specified when running the inference? I am just wondering if the fix could be verified by the benchmark model tool which supports specifying input layer name and its shape.\r\n> > > > Well cool, I think TensorFlow([tensorflow/tensorflow@f779f42](https://github.com/tensorflow/tensorflow/commit/f779f42004d1873784cb2b4cb351d4b342ad0816)) could be easier for me to check, isn't it?\r\n> > > \r\n> > > \r\n> > > Indeed!\r\n> > > > > The fix I mentioned earlier was pushed 3 days ago in the RUY repo ([google/ruy@54774a7](https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59)). Could you try to patch it and see whether it fixes the crash or not? Thx!\r\n\r\n", "( T ^ T )  Ohhhh God, BIT ME.....It's a BAD NEWS....however, it's really thankful for your new reply, I think we have to reopen this issue again.\r\n\r\n> > Emmmm....I think this fix can be checked by the [hello world example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc), because I have ever got the similar crash with that tool.\r\n> \r\n> I just checked the mirnet_dr.tflite model with the minimal binary w/ the patch. The binary still crashed with segfault. But the stack trace is quite different as shown below:\r\n> \r\n> #0 tflite::tensor_utils::NeonSymmetricQuantizeFloats (values=0x0, size=0, quantized_values=0x0, min=0x7fffffffd458, max=0x7fffffffd454, scaling_factor=0x0)\r\n> at tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc:2293\r\n> #1 0x00005555559324f6 in tflite::ops::builtin::conv::EvalHybrid<(tflite::ops::builtin::conv::KernelType)2> (context=0x57a83fc887c8, node=0x57a83fcdcce0,\r\n> params=0x57a83fc014c0, data=0x57a83fd0e9c0, input=0x57a83fd3d680, filter=0x57a83fd27e30, bias=0x57a83fd359d0, im2col=0x0, accum_scratch=0x57a83fd6f020,\r\n> output=0x57a83fd3d6f0) at tensorflow/lite/kernels/conv.cc:949\r\n> #2 0x00005555559315ae in tflite::ops::builtin::conv::EvalImpl<(tflite::ops::builtin::conv::KernelType)2, (TfLiteType)1> (context=0x57a83fc887c8, node=0x57a83fcdcce0)\r\n> at tensorflow/lite/kernels/conv.cc:1027\r\n> #3 0x00005555558b01f9 in tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2> (context=0x57a83fc887c8, node=0x57a83fcdcce0)\r\n> at tensorflow/lite/kernels/conv.cc:1064\r\n> #4 0x0000555555bfc4ed in tflite::Subgraph::OpInvoke (this=0x57a83fc887a0, op_reg=..., node=0x57a83fcdcce0) at /tensorflow/lite/core/subgraph.h:427\r\n> #5 0x0000555555bf98a3 in tflite::Subgraph::Invoke (this=0x57a83fc887a0) at tensorflow/lite/core/subgraph.cc:1070\r\n> #6 0x0000555555c062d9 in tflite::Interpreter::Invoke (this=0x57a83fc86380) at tensorflow/lite/interpreter.cc:284\r\n> #7 0x00005555557f2b21 in main (argc=2, argv=0x7fffffffd9b8) at tensorflow/lite/examples/minimal/minimal.cc:70\r\n> \r\n> I guess we probably need to specify proper input shape to make the model inference run, and this is why I suggested using the benchmark model earlier.\r\n> \r\n> > > Btw, as the model has dynamic shape, what's the proper input layer shape to be specified when running the inference? I am just wondering if the fix could be verified by the benchmark model tool which supports specifying input layer name and its shape.\r\n> > > > > Well cool, I think TensorFlow([tensorflow/tensorflow@f779f42](https://github.com/tensorflow/tensorflow/commit/f779f42004d1873784cb2b4cb351d4b342ad0816)) could be easier for me to check, isn't it?\r\n> > > > \r\n> > > > \r\n> > > > Indeed!\r\n> > > > > > The fix I mentioned earlier was pushed 3 days ago in the RUY repo ([google/ruy@54774a7](https://github.com/google/ruy/commit/54774a7a2cf85963777289193629d4bd42de4a59)). Could you try to patch it and see whether it fixes the crash or not? Thx!\r\n\r\n"]}, {"number": 46984, "title": "STM32F746NG Hello World LCD Display not working correctly", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OSX10.15.7\r\n- TensorFlow installed with homebrew\r\n- Tensorflow version 2.4.1\r\n- Target platform Arm Mbed OS\r\n\r\n**Describe the problem**\r\nAfter the hello world project has compiled, when I copy the mbed.bin file (177Kb) to the STM32F746NG  the LCD screen just goes white. The program is clearly running because if I run screen /dev/tty.usbmodem403 9600 is can see all the correct values scrolling down my terminal window. Also, the hardware is working correctly as if I install the MBED LCD demo https://ide.mbed.com/compiler/#nav:/DISCO-F746NG_LCDTS_demo it runs fine and both the screen display and the capacitative touch work correctly.\r\n\r\n\r\n", "comments": ["Apologies for the delay in response. Is this still an issue?", "Hi Yasir,\r\n\r\nYes, this is still an issue. I\u2019ve not been able to resolve the problem.\r\n\r\nBest Regards,\r\nDino\r\n\r\n\r\n\r\nOn 4 Mar 2021, at 20:21, Yasir Modak <notifications@github.com> wrote:\r\n\r\n\ufeff\r\n\r\nApologies for the delay in response. Is this still an issue?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F46984%23issuecomment-790913373&data=04%7C01%7C%7Cb90180eb00f0422ad8cf08d8df4b1acb%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637504860949226267%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NsIRaFlbIBFPXO%2Fu7HJMmpUVqWZL45bXorMCC%2B5aqis%3D&reserved=0>, or unsubscribe<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAI7ULE4F4QXBIJJN5FW5EXLTB7TUZANCNFSM4XHQ2G3A&data=04%7C01%7C%7Cb90180eb00f0422ad8cf08d8df4b1acb%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637504860949236262%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ViSbAmeuYFrBa7DRDN22dIUXuXr80cngCP81sb4faBA%3D&reserved=0>.\r\n", "Since it appears to be hardware issue, Have you tried posting this question on [ST community](https://community.st.com/s/)?\r\nI found a similar [thread](https://community.st.com/s/question/0D50X00009XkXSgSAN/stm32f429-white-screen-flashing-com-led)", "Hi Yasir,\r\n\r\nThe problem doesn\u2019t seem to be hardware related, the display works fine and examples from the ST website (which use a different display library) work fine.\r\n\r\nBest Regards,\r\nDino\r\n\r\n\r\nOn 4 Mar 2021, at 20:27, Yasir Modak <notifications@github.com> wrote:\r\n\r\n\ufeff\r\n\r\nSince it appears to be hardware issue, Have you tried posting this question on ST community<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcommunity.st.com%2Fs%2F&data=04%7C01%7C%7C4c4f7a0ed3224577c14508d8df4be250%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637504864311693216%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=PD%2FMnQiXauWNHdZaiCO0YVC2NTGJGltsfvBQ%2Bgg2ceQ%3D&reserved=0>?\r\nI found a similar thread<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcommunity.st.com%2Fs%2Fquestion%2F0D50X00009XkXSgSAN%2Fstm32f429-white-screen-flashing-com-led&data=04%7C01%7C%7C4c4f7a0ed3224577c14508d8df4be250%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637504864311693216%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=4CRyMuXgB5UnQt51lIAEJOXsnPTkbKKRGyUJaoanYRk%3D&reserved=0>\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F46984%23issuecomment-790917156&data=04%7C01%7C%7C4c4f7a0ed3224577c14508d8df4be250%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637504864311703210%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=PlwKPD%2BT6JGKgTDmSSX8VfxDH1FakxeCUqLF7QVgmXA%3D&reserved=0>, or unsubscribe<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAI7ULE7WCMKKZ45K74PIWS3TB7UJZANCNFSM4XHQ2G3A&data=04%7C01%7C%7C4c4f7a0ed3224577c14508d8df4be250%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637504864311703210%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=YNXRJgb6SZ9cMRAhJUo%2BYWDjTIjfarABd%2FBERNd00fA%3D&reserved=0>.\r\n", "@mdinoulis there have been some recent updates to mbed generation for the hello world example. Is this still an issue with these recently merged changes?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46983, "title": "Hard to use a lstm tflite model.", "body": "I'm interested in lstm models, and I really need one tflite model(other framework like onnx or saved model for me is not acceptable now) include lstm node. But for days, I can ony find one float32 model from https://storage.googleapis.com/download.tensorflow.org/models/tflite/speech_asr_am_model.tflite and it's uint8 model from https://storage.googleapis.com/download.tensorflow.org/models/tflite/speech_asr_am_model_int8.tflite. Other old official models like tts and speakerid models are deprecated. But for my work, the shape info between layers are missing, and for me, it's impossible to infer these shapes before real work. Are there any tools to edit or infer the shape info and regenerate a new tflite model? Or is it just a wrong model? I have taken look at tfhub, tensorflow official website, and tracked with lots of discussions for lstm tflite using google search, but no more models found and not much useful information. And I found that others are also confused with lstm tflite like model convert issues. So what I needs are: 1. usable models. 2. tools like tflite editor or shape inferencer. 3. any update for lstm. 4. is lstm tflite supported or not. Really need help. Thanks.", "comments": ["Hi,\r\n\r\nTensorFlow Lite currently supports limited usage of RNN & LSTM. \r\nHave you checked the [TensorFlow RNN conversion to TensorFlow Lite](https://www.tensorflow.org/lite/convert/rnn) guide yet?\r\nPlease take a look and let us know if you have any further questions. ", "@aaltonenzhang Could you please update as per the above comment and let us know if this issue still persists ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46983\">No</a>\n"]}, {"number": 46982, "title": "Unexpected TFDS Shuffling behavior", "body": "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\r\ndataset = dataset.shuffle(buffer_size=256)\r\ndataset = dataset.take(1)\r\nprint(list(dataset.as_numpy_iterator()))\r\ndataset = dataset.repeat(5)\r\nprint(list(dataset.as_numpy_iterator()))\r\n\r\n>>> [2]\r\n>>> [1, 2, 3, 3, 2]\r\n\r\nIs this on purpose? I was expecting [2, 2, 2, 2, 2], as I would expect the dataset in memory to be replaced with the subset that I took in take(1), rather than now having (I assume) two datasets in memory, the subset dataset + the original dataset.\r\n\r\nThanks =) ", "comments": ["@Muennighoff \r\nThis is because `repeat` will repeat the processing pipeline before it instead of repeat the output. Therefore each `repeat` will still do the shuffling. You could set the `reshuffle_each_iteration=False` in the shuffle or use `cache` to cache the result.", "> @Muennighoff\r\n> This is because `repeat` will repeat the processing pipeline before it instead of repeat the output. Therefore each `repeat` will still do the shuffling. You could set the `reshuffle_each_iteration=False` in the shuffle or use `cache` to cache the result.\r\n\r\nOh I see, so the dataset doesn't execute the processing until I actually call it, but just stores the processing operations to be applied?", "> Oh I see, so the dataset doesn't execute the processing until I actually call it, but just stores the processing operations to be applied?\r\n\r\nExactly. If you have experience with tf1, the dataset functions are just building the graph and the iterators are the ones to run the ops.", "> > Oh I see, so the dataset doesn't execute the processing until I actually call it, but just stores the processing operations to be applied?\r\n> \r\n> Exactly. If you have experience with tf1, the dataset functions are just building the graph and the iterators are the ones to run the ops.\r\n\r\nThat makes sense. Thanks a lot for helping my understanding. Closing this now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46982\">No</a>\n"]}, {"number": 46980, "title": "Documentation about convolution padding computation is missing", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/convolution?version=nightly\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation refers to an invalid link, https://tensorflow.org/api_guides/python/nn#Convolution, and the reference to that link is recently deleted in this commit:  https://github.com/tensorflow/tensorflow/commit/271df9ce498f630b6c417087f18b32f2f7446a83 (cc @MarkDaoust ). \r\n\r\nHowever, I think **the correct action to take is to make an alternative to the invalid link (rather than deleting it)**.\r\n\r\nWhen the link existed in the past, it contains the formula of how padding is computed for convolution, which is obviously a necessary piece of information for users to know and must be in the documentation.\r\n\r\n\r\nAs a reference, I found some [incomplete third-party website](https://tensorflow.juejin.im/api_guides/python/nn.html) that contain the old version of this now-missing page:\r\n\r\n\r\n![2021-02-06_23-50](https://user-images.githubusercontent.com/1381301/107140355-37990f80-68d6-11eb-9f2f-6c06d3503102.png)\r\n", "comments": ["Hey @jvishnuvardhan , is anyone working on this?? Can you assign this to me??", "Done. Give it a shot.\r\n\r\nThose docs are coming from:\r\n\r\n1. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1138-L1161\r\n2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L998-L1135\r\n\r\n", "Hey @MarkDaoust , thanks for replying. Soo do I only need to attach the link of the third-party website in the links to the docs provided by you?? Like just edit the docs with the given link?? Where should I provide the link to the website?? Any specified location?? Please reply thanks.", "The page has been updated with the tf 2.7 launch.\r\nThe link now points here, it's no longer a 404:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn?version=nightly#notes_on_padding_2"]}, {"number": 46979, "title": "why tensorflow has so much ugly designed api?  be pythonic please...", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["It is optimized for performance, not for ease of usage. Don't worry, you'll get the hang of it! \ud83d\ude04 ", "optimized for performance???? Ohhhh ... be pythonic please. ", "@pengyuan \r\nPlease elaborate the issue faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46978, "title": "Confiuguration error ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18 with qemu and docker aarch64 image for jetson platform\r\n\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3\r\n \r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2 8\r\n- GPU model and memory: gtx 1070\r\n\r\n\r\nHi everyone I have a problem in the configuration phase. When asking for Cuda and cudnn library I get this error:\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 497, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 489, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 468, in find_cuda_config\r\n    result.update(_find_cudnn_config(cudnn_paths, cudnn_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 346, in _find_cudnn_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 235, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 224, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 342, in get_header_version\r\n    return \".\".join(version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 341, in <genexpr>\r\n    for name in (\"CUDNN_MAJOR\", \"CUDNN_MINOR\", \"CUDNN_PATCHLEVEL\"))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 123, in _get_header_version\r\n    for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\n  File \"/usr/lib/python3.6/codecs.py\", line 321, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb7 in position 18: invalid start byte\r\nAsking for detailed CUDA configuration...\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /folder_shared/lib64,/usr/local/cuda-10.2/targets/aarch64-linux/include/,/usr/local/cuda-10.2/bin/,/usr/local/cuda-10.2/nvvm/libdevice,/usr/local/cuda-10.2/nvvmx/libdevice,/usr/local/cuda-10.2/nvvmx/libdevice/libdevice.10.bc,/usr/local/cuda-10.2,/usr/include,/folder_shared,/folder_shared/cudnn8/\r\n\r\n", "comments": ["@robertorovella91 \r\n\r\nPlease, see tested build configuration from [here](https://www.tensorflow.org/install/source#gpu). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46978\">No</a>\n"]}, {"number": 46976, "title": "Build with clang support is failing", "body": "To reproduce, configure with clang enabled with cuda, i.e.\r\n\r\n```\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: y\r\nClang will be downloaded and used to compile tensorflow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n```\r\n\r\nThen star building\r\n```\r\n$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nERROR: Config value 'download_clang' is not defined in any .rc file\r\n```\r\n\r\nThe generated `.tf_configure.bazelrc` contains:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/local/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3.9/site-packages\"\r\nbuild --python_path=\"/usr/local/bin/python3\"\r\nbuild --config=xla\r\nbuild --config=download_clang\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-Wno-sign-compare\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nI didn't find `download_clang` defined as a build config in .bazelrc, guess it might be the reason.\r\n\r\nThis happens on the master branch with latest commit 57bbc5e0d4b93483b8ae853352173516f1c08018", "comments": ["Probably need something like this in `.bazelrc`:\r\n\r\n```\r\nbuild:download_clang --action_env=TF_DOWNLOAD_CLANG=1\r\n```", "@mzhaom \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version]", "@Saduf2019, this happens with the latest  commit 57bbc5e", "@mzhaom \r\nCan you try updating to the latest stable version 2.6.0/2.7 and let us know if the issue still persists? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46976\">No</a>\n"]}, {"number": 46974, "title": "Fix crash of tf.strings.substr when pos and len have different shapes", "body": "This PR tries to address the issue raised in #46900 where\r\ntf.strings.substr will crash when pos and len have different shapes.\r\nAccording to the documentation of tf.strings.substr, ValueError\r\nshould be raised instead when pos and len does not have the same shape.\r\n\r\nThis PR add shape check in kernel to allows grace error throw (instead of crash).\r\n\r\nThis PR fixes #46900.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 46973, "title": "Fix crash with tf.transpose when a is complex and conjugate is True", "body": "This PR tries to address the issue raised in #46891 where\r\ntf.transpose will crash when a is complex and conjugate is True.\r\nThe issue comes from:\r\nhttps://github.com/tensorflow/tensorflow/blob/57bbc5e0d4b93483b8ae853352173516f1c08018/tensorflow/core/kernels/transpose_functor.h#L169\r\n\r\nHowever, as ndims < 2 has already been handled properly:\r\nhttps://github.com/tensorflow/tensorflow/blob/57bbc5e0d4b93483b8ae853352173516f1c08018/tensorflow/core/kernels/transpose_functor_cpu.cc#L103-L105\r\nThe check could be removed.\r\n\r\nThis PR fixes #46891.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@rohan100jain @cantonios thanks for the review. The PR has been updated. Please take a look.", "Thanks @cantonios  for the help. The PR has been updated. Please take a look and let me know if there are any issues.", "Thanks @cantonios for the help. The PR has been updated. Please let me know if there are any additional issues."]}, {"number": 46971, "title": "Tensorflow implementation in Julia", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.x\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, there is no Tensorflow implementation in Julia. Since Julia is the next major upcoming scientific computation language and it is much faster than Python while maintaining similar intuitiveness, I believe that we should have a Julia-version of TF.\r\n\r\n**Will this change the current api? How?**\r\nThis will not change existing features but will port them over to Julia as well.\r\n\r\n**Who will benefit with this feature?**\r\nThe entire scientific community will be benefitted as more and more people will start shifting to Julia in the near future, as more and more packages are being developed for it.\r\n**Any Other info.**\r\n", "comments": ["@atreyamaj Thanks for opening the issue. Features like these are managed by the special interest groups (SIGs). So, Please feel free to open this issue [here](https://github.com/tensorflow/community/issues). \r\n\r\nThis repository is only for Bug and performance related to TF (python). So, I am closing this issue here. Thanks!"]}, {"number": 46970, "title": "ValueError: ssd_mobilenet_v2 is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow", "body": "Hi Folks\r\n\r\nFrom last few days I'm working on generating model file for Object Detection using Tensorflow 2.x. I have gone through so many blogs and github repos etc. etc.\r\n\r\nWith help of them I am able to generate xml -> csv file then test.record / train.record files.\r\n\r\nLinks which I had gone through are mentioned below\r\nhttps://teyou21.medium.com/training-your-object-detection-model-on-tensorflow-part-2-e9e12714bdf\r\nhttps://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/\r\nhttps://medium.com/@marklabinski/installing-tensorflow-object-detection-api-on-windows-10-7a4eb83e1e7b\r\n\r\nNow I am at that step where I have to run command to train the record files using label text file.\r\npython train.py --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_quantized_300x300_coco.config\r\n\r\nBut after running this command in PyCharm I am getting error\r\n\r\n![ssd_mobilenet_v2](https://user-images.githubusercontent.com/34931349/107122123-d3695200-68bb-11eb-9ea4-80356801a16f.jpeg)\r\n\r\nI searched a lot to solve this issue but not able to find any solution\r\n\r\nOperating System: Windows 10 64 bit\r\nPython version: 3.7\r\nTensorflow version : 2.4.1\r\n\r\nPlease help me if anyone has any solution for this.\r\n", "comments": ["I don't know how to resolve this error, but just one side note. I highly recommend using [TF Hub](https://tfhub.dev/s?module-type=image-object-detection) for pretrained / pre-made model architectures. It makes the process way easier, and ypu can run it locally, on Colab or on Kaggle. ", "@Neeraj1108Yadav \r\n\r\nJust to verify did you download the model from this [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).Also, refer similar issue [#4056 ](https://github.com/tensorflow/models/issues/4056) and see if it helps you. \r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!\r\n\r\nThanks!", "@ravikyram \r\n\r\nI didn't download from your mentioned link\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\n\r\nBut after opening it, I'm confused now which one to download.", "@Neeraj1108Yadav \r\n\r\nYou can use your desired model from the list.Eg:(`SSD MobileNet v2 320x320`).Thanks!", "@ravikyram \r\n\r\nI need help here and am so close to generate model file, but I don't know what wrong I am doing.\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\n\r\nSo I am referring above link to train my model and able to generate all files **.pbtxt, test.record, train.record**.  I am at the step where I have to generate a **config file**. As per your suggestions I downloaded the model but the files and folders inside folder  is not matching as per required in mentioned below images\r\n\r\nStructure of folder\r\n>  +--ssd_mobilenet_v2_320x320_coco17_tpu-8\r\n   |   pipeline.config\r\n   |   \r\n   +---checkpoint\r\n   |       checkpoint\r\n   |       ckpt-0.data-00000-of-00001\r\n   |       ckpt-0.index\r\n   |       \r\n   \\---saved_model\r\n       |   saved_model.pb\r\n       |   \r\n       \\---variables\r\n               variables.data-00000-of-00001\r\n               variables.index\r\n\r\nPlease refer images from the above links\r\n\r\n![train_config_one](https://user-images.githubusercontent.com/34931349/107186560-821ba880-6a0a-11eb-8814-75355aefc228.JPG)\r\n\r\n![train_config_two](https://user-images.githubusercontent.com/34931349/107186579-8942b680-6a0a-11eb-99ec-ad5d835ebbcf.JPG)\r\n\r\n![train_config_three](https://user-images.githubusercontent.com/34931349/107186591-8f389780-6a0a-11eb-8203-e07eba38f725.JPG)\r\n\r\nPlease help me on this", "@Neeraj1108Yadav \r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!", "@Neeraj1108Yadav \r\n\r\nOne of the issues can be you are to train the model using train.py which is a legacy file. Try using model_main_tf2.py if you are using tf2. I had a similar issue when using the train.py file.  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46970\">No</a>\n", "Hi, @Neeraj1108Yadav can you share your pipeline config file, I think that you set \"ssd_mobilenet_v2\" as type of your feature_extractor. It happened with me, set \"ssd_mobilenet_v2_keras\" and try again.\r\n```\r\nmodel {\r\n    ssd {\r\n          ...\r\n          feature_extractor {\r\n                type: 'ssd_mobilenet_v2_keras'\r\n                 ...\r\n           }\r\n          ...\r\n    }\r\n...\r\n}\r\n```\r\n"]}, {"number": 46969, "title": "implement changes necessary to support new bconv2d API in lib_nn", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46969) for more info**.\n\n<!-- need_author_cla -->", "Sry, accidentally opened PR to the wrong branch."]}, {"number": 46968, "title": "XLA: Allow devices to indicate whether the executable can be run async", "body": "This allow an XLA device/backend to indicate if executables can be executed asynchronously.", "comments": []}, {"number": 46967, "title": "Optimised Images", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46967) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46967) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46967) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46967) for more info**.\n\n<!-- need_author_consent -->", "@agarwalutkarsh554  Can you please sign CLA. Thanks! ", "I have already signed it please check", "@agarwalutkarsh554 Can you please make sure to use same GitHub username and email-id associated with it.", "What is the purpose of this PR?", "@agarwalutkarsh554 Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!"]}, {"number": 46966, "title": "Add CAST and ZEROS_LIKE tests in lite/micro/kernels/BUILD", "body": "This is a fix for micro ops CAST and ZEROS_LIKE (issues #45608 and #46049).", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46965, "title": "Integrate CUDNN v8 frontend API for convolution", "body": "This PR integrates the new cuDNN Frontend API (https://github.com/NVIDIA/cudnn-frontend) for the convolution backends.\r\n\r\n\r\nFYI. @nluehr ", "comments": ["I have split the PR into multiple commits ranging from backend (configs, stream executor, autotuner utils) towards frontend (conv2d, conv3d). PTAL. @timshen91 ", "@kaixih Can you please check @sanjoy's comments and keep us posted ? Thanks!", "I think all the above comments are addressed. PTAL. @timshen91 ", "Just addressed the new comments. PTAL. @timshen91", "Just made a couple of new changes. PTAL @timshen91 ", "Thanks for the reviews. @timshen91 @sanjoy \r\n\r\nI think only this `Windows Bazel GPU` test fails. The log indicates it fails to find the `third_party/gpus/cudnn/cudnn_backend.h`. I have a feeling that this error is related to the CI settings and the windows tests may not take this branch https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L1103 to copy all the headers for cudnn. Can you help check it?", "Hi Kaixi, you are absolutely right, that line should say `if cuda_config.cudnn_version.rsplit(\"_\", 1)[-1] >= \"8\":`.\r\n\r\nI will submit a fix shortly.", "Retest this please.", "Thanks Christian!"]}, {"number": 46964, "title": "Move Tensorflow Lite packages off of Bintray / JCenter due to deprecation of Bintray", "body": "Some Tensorflow packages are on Bintray at https://bintray.com/google/tensorflow, but according to https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/, Bintray (and JCenter) will be unavailable on May 1, 2021. Does this affect Tensorflow's usage of Bintray?\r\n\r\nLooking at older issues, I saw a comment https://github.com/tensorflow/tensorflow/issues/30348#issuecomment-533239346 that says that Tensorflow is \"no longer releasing TensorFlow Mobile Android artifacts on Maven\". If the deprecation of Bintray affects Tensorflow, then it would be great if the packages were made available on Maven Central ", "comments": ["Thanks for the notice. We are aware of the issue, and planning to host the TFLite Android binaries on a different maven repository somewhere other than JCenter before the actual shutdown.", "Thanks for the remainder. Yes we're investigating this issue and will action later.", "We've already pushed artifacts to Maven Central. Closing it now. For any issues related, Feel free to reopen.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46964\">No</a>\n"]}, {"number": 46963, "title": "[INTEL MKL] Adding padding_list in mkl quantized conv2d per channel op", "body": "Adding padding_list in mkl quantized conv2d per channel op in the attribute. It is needed to do pad fusion with quantized conv2d per channel.", "comments": []}, {"number": 46962, "title": "Fix kernel_exp_test with the Xtensa toolchain.", "body": "Underlying issue is a bug in the EXPECT_NEAR macro, as described in #46960\r\n\r\nAlso,\r\n  * added an exp_test rule to the BUILD file.\r\n  * changed the golden value computation to make use of std::exp instead of hard-coded values. This is closer to the TfLite test as well.\r\n\r\nManually confirmed that the following command passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test -j8\r\n```\r\n\r\nFixes #46960\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46960, "title": "TF_LITE_MICRO_EXPECT_NEAR(inf, inf) gives incorrect result for some platforms.", "body": "micro/kernels/exp_test.cc checks that two inf values are near eachother:\r\nhttps://github.com/tensorflow/tensorflow/blob/ed22f400428a669c1c6e4553cd7f4900abeaf954/tensorflow/lite/micro/kernels/exp_test.cc#L67-L72\r\n\r\nThis works ok for all the CI targets, but broke the xtensa build:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_exp_test -j8\r\n```\r\nfails with:\r\n```\r\nTesting SingleDim\r\nexpected_output_data[i] (Inf) near output_data[i] (Inf) failed at tensorflow/lite/micro/kernels/exp_test.cc:54\r\n0/1 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\n\r\nThe underlying issue that the EXPECT_NEAR_MACRO is taking a difference of two infinities which at least with the xtensa toolchain can give a `nan`, which in turn results in the check failing, even though inf==inf is true:\r\nhttps://github.com/tensorflow/tensorflow/blob/ed22f400428a669c1c6e4553cd7f4900abeaf954/tensorflow/lite/micro/testing/micro_test.h#L153-L165\r\n\r\n", "comments": []}, {"number": 46959, "title": "[INTEL MKL] Adding padding list in quantized conv2d per channel", "body": "Supporting padding list in attribute in Quantized Conv2D per Channel op both in block format and Native format. Changed pb file for api compatibility.", "comments": ["Need to change the PR. Now closing and will re-open after the fix."]}, {"number": 46958, "title": "model train InvalidArgumentError:Input is empty", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4(latest)\r\n- Python version:latest\r\n\r\n\r\n**Describe the current behavior**\r\nI am about to train the model using the model_main_tf2.py and it seems to show this error and stop. I have tried it on centernet_resnet50_v1_fpn_512x512 and ssd_mobilenet_v2_fpnlite_320x320 and it is the same for both. when I ran this on AWS it did not show any errors but it did not seem to generate any checkpoints either.\r\n**Describe the expected behavior**\r\nExpect it to run without showing any errors\r\n\r\nI guess the error is in the tfrecord file but all the paths and the values have been verified.\r\nthe code for generating tfrecord\r\nwriter = tf.io.TFRecordWriter(outputPath)\r\n        total = 0\r\n        for k in keys:\r\n\r\n            encoded = np.asarray(Image.open(k))\r\n            h,w,ch=encoded.shape\r\n            filename = k.split(os.path.sep)[-1]\r\n            encoding = filename[filename.rfind(\".\") + 1:]\r\n\r\n            image = encoded.tostring() \r\n            xMins,xMaxs,yMins,yMaxs,textLabels,classes=[],[],[],[],[],[]\r\n            for (label,(startX, startY, endX, endY)) in D[k]:\r\n                endX,endY=endX-startX,endY-startY\r\n                xMin = (startX+endX/2) / w\r\n                xMax = endX / w\r\n                yMin = (startY+endY/2) / h\r\n                yMax = endY / h\r\n                if xMin<0 or xMax<0 or yMin<0 or yMax<0 or xMin>1 or xMax>1 or yMin>1 or yMax>1:\r\n                    continue\r\n                xMins.append(xMin)\r\n                xMaxs.append(xMax)\r\n                yMins.append(yMin)\r\n                yMaxs.append(yMax)\r\n                textLabels.append(label.encode(\"utf-8\"))\r\n                classes.append(CLASSES[label])\r\n\r\n            total += 1\r\n\r\n            classes=np.array(classes)            \r\n            example = tf.train.Example(features=tf.train.Features(feature={\r\n           'image/height': int64_feature(h),\r\n           'image/width': int64_feature(w),\r\n           'image/filename': bytes_feature(filename.encode('utf-8')),\r\n           'image/source_id': bytes_feature(filename.encode('utf-8')),\r\n           'image/image_raw': bytes_feature(image),\r\n           'image/format': bytes_feature(encoding.encode('utf-8')),\r\n           'image/object/bbox/xmin': float_list_feature(xMins),\r\n           'image/object/bbox/xmax': float_list_feature(xMaxs),\r\n           'image/object/bbox/ymin': float_list_feature(yMins),\r\n           'image/object/bbox/ymax': float_list_feature(yMaxs),\r\n           'image/object/class/text': bytes_list_feature(textLabels),\r\n           'image/object/class/label': int64_list_feature(classes),\r\n }))\r\n\r\n            writer.write(example.SerializeToString())\r\n\r\n        writer.close()\r\n\r\n**Other info / logs** \r\n2021-02-05 23:08:11.056603: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-02-05 23:08:11.056634: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2021-02-05 23:08:13.273009: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-05 23:08:13.273186: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-02-05 23:08:13.273204: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-02-05 23:08:13.273234: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kingsman-desktop): /proc/driver/nvidia/version does not exist\r\n2021-02-05 23:08:13.274261: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nW0205 23:08:13.275029 140202867857216 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nI0205 23:08:13.275206 140202867857216 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0205 23:08:13.295096 140202867857216 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0205 23:08:13.295231 140202867857216 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0205 23:08:13.363639 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py:523: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nINFO:tensorflow:Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\nI0205 23:08:13.371121 140202867857216 dataset_builder.py:163] Reading unweighted datasets: ['/home/kingsman/deepl/records/train.tfrecord']\r\nINFO:tensorflow:Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\nI0205 23:08:13.371364 140202867857216 dataset_builder.py:80] Reading record datasets for input file: ['/home/kingsman/deepl/records/train.tfrecord']\r\nINFO:tensorflow:Number of filenames to read: 1\r\nI0205 23:08:13.371473 140202867857216 dataset_builder.py:81] Number of filenames to read: 1\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0205 23:08:13.371576 140202867857216 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0205 23:08:13.373951 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0205 23:08:13.397681 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nW0205 23:08:20.996112 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nW0205 23:08:24.354368 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nWARNING:tensorflow:From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0205 23:08:26.214364 140202867857216 deprecation.py:339] From /home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/inputs.py:281: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\n2021-02-05 23:08:28.723569: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-05 23:08:28.745519: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3400075000 Hz\r\nTraceback (most recent call last):\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\r\n    yield\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 566, in train_loop\r\n    unpad_groundtruth_tensors)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 344, in load_fine_tune_checkpoint\r\n    features, labels = iter(input_dataset).next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 628, in next\r\n    return self.__next__()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\r\n    return self.get_next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\r\n    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\r\n    return self._format_data_list_with_options(self._iterator.get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\r\n    result.append(self._device_iterators[i].get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\r\n    return self._next_internal()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\r\n    executor_new.wait()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input is empty.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\r\n", "comments": ["@2vin2vin  I think you can take one of your TFRecords and decode it and check whether things are getting written in TFRecords. Other thing is that TFRecords suffer data loss for large sizes (>2 GB or so). Thus, it is a  better idea to shard the data in multiple TFRecords. \r\nHope this helps.", "i converted the data to shards as you said and when I try to train I get this error\r\n\r\nI also converted the images to jpeg and tried it but the same error shows up\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\r\n    yield\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unknown image file format. One of JPEG, PNG, GIF, BMP required.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 574, in train_loop\r\n    unpad_groundtruth_tensors)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 352, in load_fine_tune_checkpoint\r\n    features, labels = iter(input_dataset).next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 628, in next\r\n    return self.__next__()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\r\n    return self.get_next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\r\n    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\r\n    return self._format_data_list_with_options(self._iterator.get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\r\n    result.append(self._device_iterators[i].get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\r\n    return self._next_internal()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\r\n    executor_new.wait()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unknown image file format. One of JPEG, PNG, GIF, BMP required.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]", "Two things:\r\n1. Could you please share the source code and driver code in full? Also could you please share the directory structure of source of images and TFRecords directory?\r\n2. Please use \" ` \" (the key below esc) to mark code. \ud83d\ude05  It becomes more readable.  Use \"  ``` \" to start and end large code blocks.\r\n", "images_dir=/home/kingsman/tfod/visdr/final_images/        #  it contains both jpg and jpeg images\r\nrecords_dir=/home/kingsman/tfod/visdr/records/                 #   train and test records along with label_map.pbtxt\r\nconfig=/home/kingsman/models/research/centernet_resnet50_v1_fpn_512x512_coco17_tpu-8/pipeline.config\r\n\r\nI am using the model_main_tf2.py in models/research/object_detection to train\r\nI am actually new to this so I am not sure what driver code is", "Sorry but TBH, I have no experience with TensorFlow Model Garden  object detection API. I usually use [TF Hub](https://tfhub.dev/tensorflow/collections/object_detection/1) for my purposes. Please visit it. It is easy to use and you can use it on locally, on Colab or on Kaggle. ", "I have to train the model on a local system that does not have connection to the internet and I have been looking through the TF HUB but i didn't find resources on training detection models using it. If possible could you help me out with the error.\r\nThanks @AdityaKane2001 ", "I'll try my best. \r\nCan you please share the code used to make the TFRecords ?\r\nAlso, I had a question. Is the model accessible in TF Models or just the API ? What I mean is that whether a .h5 file is available to be used directly?", "The tensorflow model is .pb file that is available in the tensorflow model zoo\r\n\r\nthe tfrecord code is as follows\r\n\r\nthe k in keys is the imagefile from home directory\r\nexample:  /home/kingsman/tfod/visdr/final_images/9999998_00119_d_0000097.jpeg\r\n\r\n```\r\nfor (dType, keys, outputPath) in datasets:\r\n        print(\"[UPDATE] processing '{}'...\".format(dType))\r\n        ntotal=0\r\n        nt=1\r\n        outputPath='./records/'+dType+'_%.5d-of-%.5d.record'%(count,nt)                          #change this\r\n        writer = tf.io.TFRecordWriter(outputPath)\r\n        total, ntotal = 0 , 0\r\n        for k in keys:\r\n\r\n            encoded = Image.open(k)#.split(\"/\")[-1], \"rb\").read()\r\n            pilImage = encoded\r\n            encoded = (np.array(encoded)).tostring()\r\n            \r\n            (w, h) = pilImage.size[:2]\r\n            \r\n            filename = k.split(os.path.sep)[-1]\r\n            encoding = filename[filename.rfind(\".\") + 1:]\r\n            \r\n            image = bytes_feature(encoded)\r\n            encoding = bytes_feature(encoding.encode('utf-8'))\r\n            filename = bytes_feature(filename.encode('utf-8'))\r\n            filename1 = bytes_feature(k.encode('utf-8'))\r\n            width = int64_feature(w)\r\n            height = int64_feature(h)\r\n            ntotal+=1\r\n            xMins,xMaxs,yMins,yMaxs,textLabels,classes,cl=[],[],[],[],[],[],[]\r\n            if ntotal==20:\r\n                writer.close()\r\n                ntotal=0\r\n                count+=1\r\n                print(count)\r\n            #    if count==2:\r\n             #       count=1\r\n              #      break\r\n                outputPath='./records/'+dType+'_%.5d-of-%.5d.record'%(count,nt)                      #change this\r\n                writer = tf.io.TFRecordWriter(outputPath)\r\n            for (label, (startX, startY, endX, endY)) in D[k]:\r\n                # if image sizes are different w1 and h1 are data dimensions\r\n #               startX,startY,endX,endY=startX/w1*w,startY/h1*h,endX/w1*w,endY/h1*h\r\n                xMin = startX / w\r\n                xMax = endX / w\r\n                yMin = startY / h\r\n                yMax = endY / h\r\n\r\n                xMins.append(xMin)\r\n                xMaxs.append(xMax)\r\n                yMins.append(yMin)\r\n                yMaxs.append(yMax)\r\n                textLabels.append(label.encode(\"utf-8\"))\r\n                classes.append(CLASSES[label])\r\n#                difficult.append(0)\r\n\r\n                total += 1\r\n            #,\r\n #           \"image/object/difficult\": difficult}\r\n            #print(classes)\r\n            features = tf.train.Features(feature = {\r\n            \"image/height\": height,\r\n            \"image/width\": width,\r\n            \"image/filename\": filename,\r\n            \"image/source_id\": filename1,\r\n            \"image/encoded\": image,\r\n            \"image/format\": encoding,\r\n            \"image/object/bbox/xmin\": float_list_feature(xMins),\r\n            \"image/object/bbox/xmax\": float_list_feature(xMaxs),\r\n            \"image/object/bbox/ymin\": float_list_feature(yMins),\r\n            \"image/object/bbox/ymax\": float_list_feature(yMaxs),\r\n            \"image/object/class/text\": bytes_list_feature(textLabels),\r\n            \"image/object/class/label\": int64_list_feature(classes)})\r\n            example = tf.train.Example(features=features)\r\n\r\n            writer.write(example.SerializeToString())\r\n\r\n        writer.close()\r\n        print(\"[UPDATE] {} examples saved for '{}'\".format(total, dType))\r\n\r\n```\r\n\r\nThe error is\r\n```\r\nCPU Frequency: 3400275000 Hz\r\n2021-02-07 14:59:31.089045: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 776 of 2048\r\n2021-02-07 14:59:40.984812: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 968 of 2048\r\n2021-02-07 14:59:51.070973: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1070 of 2048\r\n2021-02-07 15:00:01.086571: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1137 of 2048\r\n2021-02-07 15:00:11.203637: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1239 of 2048\r\n2021-02-07 15:00:20.888056: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1327 of 2048\r\n2021-02-07 15:00:31.026801: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1415 of 2048\r\n2021-02-07 15:00:40.996405: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 1713 of 2048\r\n2021-02-07 15:00:47.608265: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:230] Shuffle buffer filled.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\r\n    yield\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unknown image file format. One of JPEG, PNG, GIF, BMP required.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 574, in train_loop\r\n    unpad_groundtruth_tensors)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 352, in load_fine_tune_checkpoint\r\n    features, labels = iter(input_dataset).next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 628, in next\r\n    return self.__next__()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\r\n    return self.get_next()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\r\n    self._iterators[i].get_next_as_list_static_shapes(new_name))\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\r\n    return self._format_data_list_with_options(self._iterator.get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\r\n    result.append(self._device_iterators[i].get_next())\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\r\n    return self._next_internal()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\r\n    executor_new.wait()\r\n  File \"/home/kingsman/.virtualenvs/deepl/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unknown image file format. One of JPEG, PNG, GIF, BMP required.\r\n\t [[{{node case/cond/else/_10/case/cond/cond_jpeg/else/_105/case/cond/cond_jpeg/decode_image/DecodeImage}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\r\n```\r\nThe shuffle buffer does not kill the program even if it is filled but the buffer issue can be solved by changing its value.\r\nIt says unknown image format but I already saved the images to .jpeg from .jpg in the same folder.\r\nI do not understand the last few lines after the UNKNOWN IMAGE FORMAT ERROR.\r\nHope this helps.\r\n", "@2vin2vin.\r\nIn order to reproduce the issue reported here, could you please share the Python script/notebook you are running and also the dataset you are using. Thanks!", "I am using the Visdrone dataset, performing augmentation, generating tf records and running the training like mentioned in the below link\r\nhttps://gilberttanner.com/blog/tensorflow-object-detection-with-tensorflow-2-creating-a-custom-model", "@2vin2vin,\r\nThe code in the tutorial provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. \r\n\r\nYou can also run your code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!", "@amahendrakar,\r\nit started working for me when I changed the tfrecord code of the image.\r\nearlier: encoded = Image.open(k)\r\n            image=bytes_feature(encoded)\r\n\r\nnow: image=bytes_feature(open('filename','rb').read())\r\n\r\nalso in the config file I changed the fine tune checkpoint to none rather than pointing it to the checkpoint", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46958\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46958\">No</a>\n"]}, {"number": 46957, "title": "No registered kernels for SobolSample.", "body": "I tested this on two similar machines (with slightly different versions, apparently)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- TensorFlow installed from (source or binary): pip3 binary\r\n- TensorFlow version (use command below): 2.3.0 and 2.3.1\r\n- Python version: 3.6.8 and 3.8.0\r\n- CUDA/cuDNN version: not relevant\r\n- GPU model and memory: not relevant; but GTX1660Ti and RTX2080Ti\r\n\r\nv2.3.0-54-gfcc4b966f1\r\nv2.3.0-rc2-23-gb36436b087\r\n\r\n**Describe the current behavior**\r\n`tf.math.sobol_sample(4, 10)` produces following error:\r\n```py\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\", line 4846, in sobol_sample\r\n    return gen_math_ops.sobol_sample(dim, num_results, skip, dtype=dtype)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 9280, in sobol_sample\r\n    return sobol_sample_eager_fallback(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 9312, in sobol_sample_eager_fallback\r\n    _result = _execute.execute(b\"SobolSample\", 1, inputs=_inputs_flat,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node SobolSample}} = SobolSample[dtype=DT_FLOAT]\r\nAll kernels registered for op SobolSample:\r\n  <no registered kernels>\r\n [Op:SobolSample]\r\n```\r\n\r\n**Describe the expected behavior**\r\nI don't know why this kernel is not registered. It seems the code is there (although I can't validate the correctness of it):\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/core/kernels/sobol_op.cc#L175\r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport tensorflow as tf\r\ntf.math.sobol_sample(4, 10)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mcourteaux \r\nI ran the code shared and this has been fixed in tf 2.4 and nightly, please refer tot he [gist here](https://colab.research.google.com/gist/Saduf2019/9befe80921f9d512d5f18946d260cae0/untitled526.ipynb).", "Oooh great news! I dared to update and after getting rid of the old version (which got was getting selected for loading so libraries for some reason), sobol now indeed works! Thank you very much! Did you fix this because of this issue, or was this already fixed in 2.4 before I opened this issue?", "@mcourteaux \r\nIt was fixed when i checked it on rf 2.4, kindly move this issue to closed status as resolved.", "Ok cool!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46957\">No</a>\n"]}, {"number": 46956, "title": "TensorFlow custom loss function error- ValueError: No gradients provided for any variable", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version (use command below): 2.2 gpu\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.6.5\r\n- GPU model and memory: NVIDIA Tesla v100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have written custom loss function which is Loss(pred,label)= { 0.0 if pred\u2212label<=0.1, 1.0 elsewhere.\r\n\r\nHere is my code: \r\n'''\r\ncomparing_tensor = tf.convert_to_tensor([0.1, 0.1, 0.1])\r\ndef custom_loss(y_pred, y_true):\r\n    loss_tensor = tf.raw_ops.Abs(x=y_pred - y_true) # get the abs diff between y_true and y_pred\r\n    boolean_tensor = tf.raw_ops.Greater(x=loss_tensor, y=comparing_tensor) # get a boolean tensor based on Greater operation. Example: [True, False, True] \r\n    binary_tensor = tf.raw_ops.Cast(x=boolean_tensor, DstT=tf.float32) # convert boolean to bianry tensor Example: [1.0, 0.0, 1.0]\r\n    mean_tensor= tf.raw_ops.Mean(input=binary_tensor, axis=-1) # get mean of binary tensor, 2/3=0.66 \r\n    loss = tf.raw_ops.Reshape(tensor=mean_tensor, shape=(1,1), name=None) # reshape mean tensor to get desired shape\r\n    return loss\r\n'''\r\n\r\n\r\nThe error I am getting is :\r\n\r\nValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'dense/kernel:0', 'dense/bias:0', 'x/kernel:0', 'x/bias:0'].\r\n**Describe the expected behavior**\r\nThe error should not be there\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@vedanshthakkar \r\n\r\nCan you please share colab link or simple standalone code with proper indentation  and supporting files to reproduce the issue in our environment .It helps us in localiizng the issue faster.\r\n\r\nAlso, can you try with latest TF stable version 2.4 and nightly version and see if the issue still persists. There are lot of performance improvements in the latest versions. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46956\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46956\">No</a>\n"]}, {"number": 46955, "title": "Deprecate legacy hdfs file system", "body": "This PR is part of the effort to switch to modular file system support\r\nby deprecates hdfs file system and direct user to use modular file system\r\nfrom tensorflow-io instead.\r\n\r\nA `TF_ENABLE_LEGACY_FILESYSTEM=1` will allow the usage of legacy hdfs file\r\nsystem the same way as before (a warning will be displayed).\r\n\r\n/cc @mihaimaruseac  @vnvo2409 @tensorflow/sig-io-maintainers @burgerkingeater \r\n\r\n**Update**: See related changes in https://github.com/tensorflow/io/pull/1309 where hdfs is enabled in tensorflow/io.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang thanks for the PR. does it mean after tf 2.5 or 2.6, HDFS access from TF is officially supported from tensorflow IO?", "@burgerkingeater Yes that is the plan. Refers to SIG IO's monthly meeting notes:\r\nhttps://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit#heading=h.xjrh565dirib\r\n"]}, {"number": 46954, "title": "Makefile fails when setting CO_PROCESSOR=ethos-u", "body": "@tensorflow/micro\r\n\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/ext_libs/ethos-u.inc'.  Stop.\r\n\r\n**System information**\r\n\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS \r\n- Tensorflow from most recent commit\r\n\r\n**Describe the problem**\r\n\r\n```\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/ext_libs/ethos-u.inc'.  Stop.\r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n ```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis-nn CO_PROCESSOR=ethos-u microlite\r\n```", "comments": ["Hi!\r\n\r\nTry using ethos_u and cmsis_nn instead of ethos-u/cmsis-nn:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis_nn CO_PROCESSOR=ethos_u microlite`", "> Hi!\r\n> \r\n> Try using ethos_u and cmsis_nn instead of ethos-u/cmsis-nn:\r\n> \r\n> `make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis_nn CO_PROCESSOR=ethos_u microlite`\r\n\r\nThat's fixed it thank-you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46954\">No</a>\n", "> > Hi!\r\n> > Try using ethos_u and cmsis_nn instead of ethos-u/cmsis-nn:\r\n> > `make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m55 OPTIMIZED_KERNEL_DIR=cmsis_nn CO_PROCESSOR=ethos_u microlite`\r\n> \r\n> That's fixed it thank-you\r\n\r\nNo problem!"]}, {"number": 46953, "title": "Problems with tf.keras.metrics.CategoricalAccuracy?", "body": "Hi\r\n\r\nAs per the documentation in https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy , I am to understand that this metric is expecting y_true to be one hot encoded. Now, taking an example from the docs:\r\n\r\n```\r\nm = tf.keras.metrics.CategoricalAccuracy()\r\nm.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],[0.05, 0.95, 0]])\r\nm.result().numpy() \r\n```\r\nThis gives me the desired result.\r\nBut when I DO NOT one hot encode the y_true data and just use the labels(something for which SparseCategoricalAccuracy is there).\r\nCode shown below:\r\n```\r\nm = tf.keras.metrics.CategoricalAccuracy()\r\nm.update_state([[2], [1]], [[0.1, 0.9, 0.8],[0.05, 0.95, 0]]) \r\nm.result().numpy()\r\n```\r\nInstead of raising an exception, it gives me a result.\r\nWas this designed intentionally?\r\nIf no, can we do something about it cause it'd be really helpful to intimate the user about a possible miss(not one hot encoding the y_true labels) on their part.", "comments": ["I am not sure whether this answer is correct, but please hear me out.\r\n\r\n1. If you go to [source](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/metrics.py#L733) of CategoricalAccuracy, you will see that it is just an inherited class which has `categorical_accuracy()` function defined on line 3250 ([here](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/metrics.py#L3250)). That is a straightforward peice of code, which will work exactly as expected. \r\n2.  In the ` MeanMetricWrapper` class (line [572](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/metrics.py#L572)),  you will see that in `update_state()` :\r\n ` y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\r\n        y_pred, y_true)`\r\n3. Now if you go to [`squeeze_or_expand_dimensions()` ](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/utils/losses_utils.py#L99), you will see that the function checks if the rank of both tensors differs by exactly 1, and if yes, it expands or squeezes the tensors. Due to this, `[[2],[1]]` becomes a valid input, and thus CategoricalAccuracy returns 0. Please find gist [here](https://colab.research.google.com/gist/AdityaKane2001/9bce96510c0b6e5e237af1b80fcb13e9/46953.ipynb).\r\n\r\nHope this helps.", "@AdityaKane2001 Thanks for your comment. I saw the source code and I agree with you as to why it is the way it is. What I want to know is , is this the way it is supposed to be **functionally**. ", "Following is the categorical accuracy function:\r\n\r\n```\r\ndef categorical_accuracy(y_true, y_pred):\r\n  \"\"\"Calculates how often predictions matches one-hot labels.\r\n  Standalone usage:\r\n              ...\r\n  Args:\r\n              ...\r\n  Returns:\r\n             ...\r\n  \"\"\"\r\n  return math_ops.cast(\r\n      math_ops.equal(\r\n          math_ops.argmax(y_true, axis=-1), math_ops.argmax(y_pred, axis=-1)),\r\n      K.floatx())\r\n\r\n```\r\n\r\nI think the way it works is not intentional, and the function should ideally check the y_pred Tensor to be of one hot encoded before calculating the accuracy. ", "Requesting to close this issue, as it has been solved in [#48000 ](https://github.com/tensorflow/tensorflow/pull/48000)", "Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46953\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46953\">No</a>\n"]}, {"number": 46951, "title": "Help understand outputs of Mask RCNN tflite model", "body": "Hi, I could really do with some help in understanding the outputs of the tflite model of Mask RCNN.\r\n\r\nI have converted the model to tflite using Tensorflow 2.3.\r\n\r\nThe model takes 3 inputs - the resized image, image meta and anchors\r\n\r\nI have written the android equivalent code for the resized image - included padding and subtracted pixel means, as well as the image meta and anchors. The model is running but I can't seem to understand which output stands for what.\r\n\r\nThere are 7 outputs of the tflite model which have dimensions as follows:\r\n\r\n[1][1][1]\r\n[1][1000][81][4]\r\n[1][1000][81]\r\n[1][100][6]\r\n[1][100][28][28][81]\r\n[1][1][4]\r\n[1][1][2]\r\n\r\nI assume [1][100][28][28][81] is for the masks and I thought [1][100][6] would be the class ID, probability score and bounding boxes. However that doesn't seem to be consistent with the results. I can share the tflite model and the first few results of the outputs if that helps. Any help would be very much appreciated!", "comments": ["We've run the model many times but we keep getting the same values repeated for the 3rd output. For example:\r\n\r\n2021-02-08 15:08:16.206 23908-23908/com.objdetector E/Edg = output3: 0.0011121973 _ I \r\n2021-02-08 15:08:16.207 23908-23908/com.objdetector E/Edg = output3: 0.24362154 _ I \r\n2021-02-08 15:08:16.207 23908-23908/com.objdetector E/Edg = output3: 0.0011121973 _ I \r\n2021-02-08 15:08:16.207 23908-23908/com.objdetector E/Edg = output3: 0.27135313 _ I \r\n2021-02-08 15:08:16.207 23908-23908/com.objdetector E/Edg = output3: 3.0 _ I \r\n2021-02-08 15:08:16.207 23908-23908/com.objdetector E/Edg = output3: 0.8469641 \r\n\r\nThis is repeated each time for each detection. The second and third outputs don't seem to have any pattern and for the fifth output the values keep getting repeated again. Would really appreciate any help here.\r\n", "@MeghaGhosh,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal codes snippet to reproduce the issue reported here and also the dataset you are using. Thanks!", "Sure, this is the code:\r\n\r\n**Gradle build**\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\ncompile files ('libs/tensorflow-lite-select-tf-ops.aar')\r\ncompile files ('libs/tensorflow-lite.aar')\r\n\r\n**Populate input  float arrays:**\r\n\r\ngetAnchor( 800, 1024, 0);\r\n\r\nprivate void getAnchor(double min_dim, double max_dim, double min_scale) {    \r\n\r\nint[][] backbone_shapes = new int[5][2];    \r\nint stride[] = new int[5];    \r\ndouble init_scales[] = {8, 16, 32, 64, 128};    \r\ndouble ratios[] = {0.5, 1, 2};    \r\nint feature_strides[] = {4, 8, 16, 32, 64};    \r\ndouble scales[] = new double[3];    \r\ndouble heights[] = new double[3];    \r\ndouble widths[] = new double[3];    \r\ndouble anchors[][] = new double[261888][4];    \r\nfor (int i = 0; i < 5; i++) {        \r\nstride[i] = 2 * (int) (Math.pow(2, i + 1));        \r\nfor (int k = 0; k < 2; k++) {            \r\nbackbone_shapes[i][k] = (int) (Math.ceil(1024 / stride[i]));        }        \r\nint shape_0 = backbone_shapes[i][0];        \r\ndouble shift_x[] = new double[shape_0];        \r\ndouble shift_y[] = new double[shape_0];        \r\nfor (int k = 0; k < 3; k++) {            \r\nscales[k] = init_scales[i];            \r\nheights[k] = scales[k] / Math.sqrt(ratios[k]);            \r\nwidths[k] = scales[k] * Math.sqrt(ratios[k]);        }        \r\nfor (int a = 0; a < shape_0; a++) {            \r\nshift_x[a] = a * feature_strides[i];        }        \r\ndouble box_widths[][] = new double[shape_0 * shape_0][3];        \r\ndouble box_centers_x[][] = new double[shape_0 * shape_0][3];        \r\ndouble box_heights[][] = new double[shape_0 * shape_0][3];        \r\ndouble box_centers_y[][] = new double[shape_0 * shape_0][3];        \r\nfor (int x = 0; x < shape_0 * shape_0; x++) {            \r\nfor (int y = 0; y < 3; y++) {                \r\nbox_widths[x][y] = widths[y];                \r\nbox_centers_x[x][y] = shift_x[x % shift_x.length];                \r\nbox_heights[x][y] = heights[y];                \r\nbox_centers_y[x][y] = shift_x[x / shift_x.length];            }        }        \r\ndouble flattened_widths[] = new double[shape_0 * shape_0 * 3];        \r\ndouble flattened_centers_x[] = new double[shape_0 * shape_0 * 3];        \r\ndouble flattened_heights[] = new double[shape_0 * shape_0 * 3];        \r\ndouble flattened_centers_y[] = new double[shape_0 * shape_0 * 3];        \r\nint s = 0;        \r\nfor (int k = 0; k < shape_0 * shape_0; k++)            \r\nfor (int j = 0; j < 3; j++) {                \r\nflattened_widths[s] = box_widths[k][j];                \r\ns++;            }        \r\ns = 0;        \r\nfor (int k = 0; k < shape_0 * shape_0; k++)            \r\nfor (int j = 0; j < 3; j++) {                \r\nflattened_centers_x[s] = box_centers_x[k][j];                \r\ns++;            }        \r\ns = 0;        \r\nfor (int k = 0; k < shape_0 * shape_0; k++)            \r\nfor (int j = 0; j < 3; j++) {                \r\nflattened_heights[s] = box_heights[k][j];                \r\ns++;            }        \r\ns = 0;        \r\nfor (int k = 0; k < shape_0 * shape_0; k++)            \r\nfor (int j = 0; j < 3; j++) {                \r\nflattened_centers_y[s] = box_centers_y[k][j];                \r\ns++;            }        \r\ndouble box_sizes[][] = new double[shape_0 * shape_0 * 3][2];        \r\ndouble box_centers[][] = new double[shape_0 * shape_0 * 3][2];        \r\nfor (int m = 0; m < shape_0 * shape_0 * 3; m++) {            \r\nbox_sizes[m][0] = flattened_heights[m];            \r\nbox_sizes[m][1] = flattened_widths[m];            \r\nbox_centers[m][0] = flattened_centers_y[m];            \r\nbox_centers[m][1] = flattened_centers_x[m];        }        \r\ndouble boxes[][] = new double[shape_0 * shape_0 * 3][4];        \r\nfor (int n = 0; n < shape_0 * shape_0 * 3; n++) {            \r\nboxes[n][0] = box_centers[n][0] - 0.5 * box_sizes[n][0];            \r\nboxes[n][1] = box_centers[n][1] - 0.5 * box_sizes[n][1];            \r\nboxes[n][2] = box_centers[n][0] + 0.5 * box_sizes[n][0];            \r\nboxes[n][3] = box_centers[n][1] + 0.5 * box_sizes[n][1];        }        \r\nif (i == 0) {            \r\nfor (int y = 0; y < shape_0 * shape_0 * 3; y++) {                \r\nfor (int z = 0; z < 4; z++) {                    \r\nanchors[y][z] = boxes[y][z];                }            }        } \r\nelse if (i == 1) {            \r\nfor (int y = 0; y < shape_0 * shape_0 * 3; y++) {                \r\nfor (int z = 0; z < 4; z++) {                    \r\nanchors[y + 196608][z] = boxes[y][z];                }            }        } \r\nelse if (i == 2) {            \r\nfor (int y = 0; y < shape_0 * shape_0 * 3; y++) {                \r\nfor (int z = 0; z < 4; z++) {                    \r\nanchors[y + 196608 + 49152][z] = boxes[y][z];                }            }        } \r\nelse if (i == 3) {            \r\nfor (int y = 0; y < shape_0 * shape_0 * 3; y++) {                \r\nfor (int z = 0; z < 4; z++) {                    \r\nanchors[y + 196608 + 49152 + 12288][z] = boxes[y][z];                }            }        } \r\nelse {            for (int y = 0; y < shape_0 * shape_0 * 3; y++) {                \r\nfor (int z = 0; z < 4; z++) {                    \r\nanchors[y + 196608 + 49152 + 12288 + 3072][z] = boxes[y][z];                }            }        }    }    \r\nfor (int z = 0; z < 261888; z++) {        \r\nanchors[z][0] = anchors[z][0] - 0;        \r\nanchors[z][1] = anchors[z][1] - 0;        \r\nanchors[z][2] = anchors[z][2] - 1;        \r\nanchors[z][3] = anchors[z][3] - 1;    }    \r\nfor (int z = 0; z < 261888; z++) {        \r\nfor (int c = 0; c < 4; c++) {            \r\nanchors[z][c] = anchors[z][c] / 1023;        }    }    \r\nfloat final_anchors[][][] = new float[1][261888][4];    \r\nfor (int z = 0; z < 261888; z++) {        \r\nfor (int c = 0; c < 4; c++) {           \r\n// final_anchors[0][z][c] = anchors[z][c];            \r\nfinal_anchors[0][z][c] = (float)anchors[z][c];        }    }    \r\n\r\nBitmapFactory.Options options = new BitmapFactory.Options();    \r\noptions.inJustDecodeBounds = true;    \r\nBitmapFactory.decodeResource(getResources(), R.drawable.b1, options);    \r\n\r\ndouble w = options.outWidth;    \r\ndouble h = options.outHeight;    \r\ndouble height = h;    \r\ndouble width =w;    \r\ndouble scale = 1;    \r\ndouble[] window ={0,0,w,h};    \r\nscale = Math.max(1, min_dim / Math.min(h, w));    \r\nif(scale < min_scale){        \r\nscale = min_scale;    }    \r\ndouble image_max = Math.max(h, w);    \r\nif (round(image_max * scale) > max_dim) {        \r\nDouble ab = max_dim;        \r\ndouble ac = image_max;        \r\nscale = ab / ac;    }    \r\nif(scale != 1){        \r\nh= round(h*scale);        \r\nw = round(w*scale);    }    \r\ndouble  top_pad = (max_dim - h) / 2;    \r\ndouble  left_pad = (max_dim - w) / 2;    \r\nwindow[0] = top_pad;    window[1] = left_pad;    \r\nwindow[2] = h + top_pad;    \r\nwindow[3] = w + left_pad;    \r\nfloat [][] meta_value = new float[1][18];    \r\nmeta_value[0][0]=0;    \r\nmeta_value[0][1] =(float)height;    \r\nmeta_value[0][2] =(float)width;    \r\nmeta_value[0][3] =3;    \r\nmeta_value[0][4] =1024;    \r\nmeta_value[0][5] =1024;    \r\nmeta_value[0][6] =3;    \r\nmeta_value[0][7] =(float)scale;    \r\nmeta_value[0][8] =(float)window[0];    \r\nmeta_value[0][9] =(float)window[1];    \r\nmeta_value[0][10] =(float)window[2];    \r\nmeta_value[0][11] =(float)window[3];    \r\nmeta_value[0][12] =0;    \r\nmeta_value[0][13] = 0;    \r\nmeta_value[0][14] =0;    \r\nmeta_value[0][15] = 0;    \r\nmeta_value[0][16] =0;    \r\nmeta_value[0][17] = 0;   \r\n \r\nint h_h = (int)h;    \r\nint w_w = (int)w;    \r\n\r\nsendvalue(meta_value, final_anchors,h_h,w_w);}\r\n\r\nprivate void sendvalue(float[][] meta_value, float[][][] final_anchors, int h, int w) {    \r\nimageBitmapForModel= BitmapFactory.decodeResource(getResources(), R.drawable.b1);\r\nint[] pix = new int[1024*1024];\r\nimageBitmapForModel.getPixels(pix, 0, 1024, 0, 0, 1024, 1024);\r\nfloat input[][][][] = new float[1][1024][1024][3];\r\nfor (int y = 0; y < 1024; y++){    \r\nfor (int x = 0; x < 1024; x++)    {        \r\nint index = y * 1024 + x;        \r\ninput[0][x][y][0]  = (((pix[index]  >> 16) & 0xff) -123.7f);     \r\ninput[0][x][y][1]= (((pix[index] >> 8) & 0xff) -116.8f);        \r\ninput[0][x][y][2] = ((pix[index] & 0xff)-103.9f);             \r\n//   pix[index] = 0xff000000 \\| (red << 16) \\| (green << 8) \\| blue;    }} \r\nfinal List<DetectionResult> results = newObjectDector.detectObjects(imageBitmapForModel,meta_value,final_anchors,input);}\r\n\r\n**Run model**\r\n\r\npublic List<DetectionResult> detectObjects(Bitmap bitmap, float[][] meta_value, float [][][] final_anchors, float input[][][][]) {\r\n\r\n    Object[] inputs = new Object[] {input, meta_value, final_anchors};\r\n\r\n   Map<Integer, Object> outputs = new HashMap<>();\r\n    outputs.put(0, new float[1][1][1]);\r\n    outputs.put(1, new float[1][1000][6][4]);\r\n    outputs.put(2, new float[1][1000][6]);\r\n    outputs.put(3, new float[1][100][6]);\r\n    outputs.put(4, new float[1][100][28][28][6]);\r\n    outputs.put(5, new float[1][1][4]);\r\n    outputs.put(6, new float[1][1][2]);\r\ntry {\r\n    Date date = Calendar.getInstance().getTime();\r\n    DateFormat formatter = new SimpleDateFormat(\"dd/MM/yyyy-mm-ss\");\r\n    String today = formatter.format(date);\r\n    tfLite.runForMultipleInputsOutputs(inputs, outputs);\r\n        float[][][] output0 = (float[][][])outputs.get(0);\r\n\r\n                float[][][][] output1 = (float[][][][])outputs.get(1)\r\nfloat[][][] output2 = (float[][][])outputs.get(2);\r\nfloat[][][] output3 = (float[][][])outputs.get(3);\r\nfor (int i = 0; i < 20;i++)\r\n{for (int k =0; k<6;k++)\r\n{\r\n\r\n    Log.e(\"output3 \",\"\"+output3[0][i][k] + \" _ I - \" + i);\r\n\r\n}}\r\nfloat[][][][][] output4 = (float[][][][][])outputs.get(4);\r\nfloat[][][] output5 = (float[][][])outputs.get(5);\r\nfloat[][][] output6 = (float[][][])outputs.get(6);\r\nDate dates = Calendar.getInstance().getTime();\r\nDateFormat formatterdd = new SimpleDateFormat(\"dd/MM/yyyy-mm-ss\");\r\nString todays = formatterdd.format(dates);\r\n} catch (Exception e){\r\n    e.printStackTrace();\r\n}\r\n\r\n\r\nThis is the google drive link to our tflite model:\r\n\r\nhttps://drive.google.com/file/d/1S6T5GUHa9WrcK2WZa0DrHpD68w9jHnTX/view?usp=sharing\r\n\r\nThis is the image we are passing to the model:\r\n\r\n![b1](https://user-images.githubusercontent.com/29752250/107219126-a42b2000-6a36-11eb-8fe2-6d294779db77.png)\r\n\r\n\r\n\r\n\r\n", "Update: \r\n\r\nConverted the model with 2.4.0 and updated gradle build as follows:\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.4.0'\r\ncompile files ('libs/tensorflow-lite-select-tf-ops.aar')\r\ncompile files ('libs/tensorflow-lite.aar')\r\n\r\nThis is the link to the model:\r\n\r\nhttps://drive.google.com/file/d/1uA05SPwJCzVq1Si2_LnitP7w-qUjLEm6/view?usp=sharing\r\n\r\nThis is the link to the colab notebook to convert the model:\r\n\r\nhttps://colab.research.google.com/drive/1Ei4-KTdyf0C6ifnxtnuDm8pwCEwxipY8?usp=sharing\r\n\r\nGetting zeros in output3 now\r\n\r\n. I suspect it has something to do with the anchor scales. Please check the models and let me know. Any help would be appreciated.", "Hi @amahendrakar any update? A bit tight on time, and we have tried passing the input image in various ways and we keep getting the same values repeated in the output. I'm pretty sure the third output tensor [1][100][6] should return the bounding boxes, class ID and score, but the values aren't right and keep getting repeated. We have compared the inputs with python and they match. ", "Hey I got the outputs correctly.. the image meta values had gotten reversed, also I used the tflite support library for image processing. Getting correct values now. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46951\">No</a>\n"]}]