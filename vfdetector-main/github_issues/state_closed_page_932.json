[{"number": 25487, "title": "    from utils import visualization_utils as vis_util ImportError: cannot import name 'visualization_utils'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Are you getting the error during installation or during running some code. It would be great if you can provide a small code to reproduce the error. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 25486, "title": "[Bug, Quantization, MKL] Rounding error of fake quantization not corresponds to rounding error of real quantization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): C++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nThere is considering quantized_conv2d() operation. If kernel type is quint8 than simple kernel will be performed. But if kernel type is qint8 than mkl kernel will be performed.\r\nIf I use not mkl kernel for quantized_conv2d() then rounding errors correspond both for fake quantization and for real quantization:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.random_normal([100, 30, 30, 1])\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nfqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)\r\nfqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)\r\nfqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')\r\n\r\nqA = tf.quantize(A, min_A, max_A, tf.quint8, mode='MIN_FIRST')\r\nqW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')\r\nqAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\nqAW = tf.dequantize(*qAW, mode='MIN_FIRST')\r\nresult = tf.reduce_mean(tf.abs(fqAW - qAW))\r\n\r\nprint('Result :', tf.Session().run(result))\r\n```\r\n```\r\nResult : 2.017457e-07\r\n```\r\nBut if I use mkl kernel for quantized_conv2d() than for all possible quantization modes combinations rounding errors aren't corresponding:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.random_normal([100, 30, 30, 1])\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nfqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)\r\nfqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)\r\nfqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')\r\n\r\nmodes = ['MIN_COMBINED', 'MIN_FIRST', 'SCALED']\r\nmodes_variants = [[i, j, k] for i in modes for j in modes for k in modes]\r\nfor variant in modes_variants:\r\n    qA = tf.quantize(A, min_A, max_A, tf.quint8, mode=variant[0])\r\n    qW = tf.quantize(W, min_W, max_W, tf.qint8, mode=variant[1])\r\n    qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\n    qAW = tf.dequantize(*qAW, mode=variant[2])\r\n    result = tf.reduce_mean(tf.abs(fqAW - qAW))\r\n\r\n    print('{:<50} - {:.4}'.format(str(variant), tf.Session().run(result)))\r\n```\r\n```\r\n['MIN_COMBINED', 'MIN_COMBINED', 'MIN_COMBINED']   - 6.242\r\n['MIN_COMBINED', 'MIN_COMBINED', 'MIN_FIRST']      - 6.926\r\n['MIN_COMBINED', 'MIN_COMBINED', 'SCALED']         - 6.526\r\n['MIN_COMBINED', 'MIN_FIRST', 'MIN_COMBINED']      - 6.111\r\n['MIN_COMBINED', 'MIN_FIRST', 'MIN_FIRST']         - 6.051\r\n['MIN_COMBINED', 'MIN_FIRST', 'SCALED']            - 6.575\r\n['MIN_COMBINED', 'SCALED', 'MIN_COMBINED']         - 5.819\r\n['MIN_COMBINED', 'SCALED', 'MIN_FIRST']            - 5.704\r\n['MIN_COMBINED', 'SCALED', 'SCALED']               - 7.089\r\n['MIN_FIRST', 'MIN_COMBINED', 'MIN_COMBINED']      - 6.403\r\n['MIN_FIRST', 'MIN_COMBINED', 'MIN_FIRST']         - 6.296\r\n['MIN_FIRST', 'MIN_COMBINED', 'SCALED']            - 5.376\r\n['MIN_FIRST', 'MIN_FIRST', 'MIN_COMBINED']         - 5.7\r\n['MIN_FIRST', 'MIN_FIRST', 'MIN_FIRST']            - 6.233\r\n['MIN_FIRST', 'MIN_FIRST', 'SCALED']               - 6.769\r\n['MIN_FIRST', 'SCALED', 'MIN_COMBINED']            - 6.188\r\n['MIN_FIRST', 'SCALED', 'MIN_FIRST']               - 6.119\r\n['MIN_FIRST', 'SCALED', 'SCALED']                  - 5.869\r\n['SCALED', 'MIN_COMBINED', 'MIN_COMBINED']         - 1.38\r\n['SCALED', 'MIN_COMBINED', 'MIN_FIRST']            - 1.395\r\n['SCALED', 'MIN_COMBINED', 'SCALED']               - 1.383\r\n['SCALED', 'MIN_FIRST', 'MIN_COMBINED']            - 1.389\r\n['SCALED', 'MIN_FIRST', 'MIN_FIRST']               - 1.395\r\n['SCALED', 'MIN_FIRST', 'SCALED']                  - 1.405\r\n['SCALED', 'SCALED', 'MIN_COMBINED']               - 1.352\r\n['SCALED', 'SCALED', 'MIN_FIRST']                  - 1.355\r\n['SCALED', 'SCALED', 'SCALED']                     - 1.357\r\n```\r\n**Other info / logs**\r\nTensorflow is build by command \r\n```\r\nbazel build --config=mkl --config=opt --copt=-DINTEL_MKL_QUANTIZED -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nStackOverflow link:\r\n```\r\nhttps://stackoverflow.com/questions/54518253/rounding-error-of-fake-quantization-not-corresponds-to-rounding-error-of-real-qu\r\n```", "comments": ["@Vooblin: MKL quantization related works only support `'SCALED'` mode which means `fp32 zero` maps to `int8 zero`. Current mkl version of `quantized_conv2d`, only supports `uint8` input and `int8` filter. So you need to make sure that original (fp32) input to the convolution is indeed non-negative, which is a typical scenario in popular CNNs, due to `Relu`. As a practical workflow, while converting a `fp32 graph` to a `quantized graph`, we keep a `conv2d` node in `fp32` if its input contains negative values.\r\nWhen you perform quantization with `tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')`, by the design of TensorFlow it clips all negative values to zero. So you are doing a `relu + quantize` operation internally. A Non-MKL (Eigen) build for the code below has the following result.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.random_normal([100, 30, 30, 1])\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nfqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)\r\nfqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)\r\nfqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')\r\n\r\nqA = tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')\r\nqW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')\r\nqAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\nqAW = tf.dequantize(*qAW, mode='MIN_FIRST')\r\nresult = tf.reduce_mean(tf.abs(fqAW - qAW))\r\n\r\nprint('Result :', tf.Session().run(result))\r\n```\r\n\r\n`('Result :', 1.6090124)`\r\n\r\nA slight modification gives the following result\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.nn.relu(tf.random_normal([100, 30, 30, 1]))\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nfqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)\r\nfqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)\r\nfqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')\r\n\r\nqA = tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')\r\nqW = tf.quantize(W, min_W, max_W, tf.quint8, mode='MIN_FIRST')\r\nqAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\nqAW = tf.dequantize(*qAW, mode='MIN_FIRST')\r\nresult = tf.reduce_mean(tf.abs(fqAW - qAW))\r\n\r\nprint('Result :', tf.Session().run(result))\r\n```\r\n\r\n`('Result :', 1.6503674e-07)`\r\n\r\nNow the above code with MKL build gives the following result\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.nn.relu(tf.random_normal([100, 30, 30, 1]))\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.constant(np.random.normal(size=[3, 3, 1, 4]).astype(np.float32))\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nfqA = tf.fake_quant_with_min_max_vars(A, min_A, max_A)\r\nfqW = tf.fake_quant_with_min_max_vars(W, min_W, max_W)\r\nfqAW = tf.nn.conv2d(fqA, fqW, [1, 1, 1, 1], 'SAME')\r\n\r\nqA = tf.quantize(A, min_A, max_A, tf.quint8, mode='SCALED')\r\nqW = tf.quantize(W, min_W, max_W, tf.qint8, mode='SCALED')\r\nqAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\nqAW = tf.dequantize(*qAW, mode='MIN_FIRST')\r\nresult = tf.reduce_mean(tf.abs(fqAW - qAW))\r\n\r\nprint('Result :', tf.Session().run(result))\r\n```\r\n`('Result :', 0.009543991)`\r\n\r\nThe reason for slightly reduced precision with MKL is due to `SCALED` mode implementation of `quantized_conv2d` where you will have less resolution if min-max are not symmetric.", "@mdfaijul Thanks for you response!"]}, {"number": 25485, "title": "Added missing condition in the Test case", "body": "Added the missing condition in the existing testcase", "comments": ["@aselle can you pls review the PR and let me know your comments", "@pragyaak & @aselle , this PR is approved but cannot see the label ready to pull, can you please replace the label.", "@rthadur, i have checked the failures, it has nothing to do with this changes, can you pls check and merge this PR.", "@rthadur , i have checked the failure is because of some other reason and not because of the PR, kindly check and merge.", "@rthadur , this PR is approved quiet some time , but not still merge, i have checked the error and these are not related to this PR, can you please help.", "@rthadur , all checks have passed for this PR, kindly help to merge this one.", "@rthadur , can you pls help to get this merged, as this is approved for long with all checks passed and i am afraid that if it stays longer again some merge conflicts might come.\r\n\r\nRegards\r\nAmit", "@aselle and @rthadur , can you pls help to merge this PR.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "Can you please describe why the test got changed? Is there a bug to link against?", "Closing as it stalled. Please reopen if there is still a need for the PR"]}, {"number": 25484, "title": "initialize variables from other tensor slow in tf1.12.0: comparison tf0.12 vs tf1.12.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow version (use command below): 1.12.0 and 0.12\r\n- Python version: 3.5 (for tf1.12.0) and 2.7 (for tf0.12)\r\n\r\n**Describe the current behavior**\r\nI am trying to use weight normalization with data-dependent initialization as reported in Salimans Kingma 2016 https://arxiv.org/pdf/1602.07868.pdf\r\nI use two approaches: 1 sharing the variables between initialization and convolution, 2 creating initializers that will be used in convolution to create the variables accordingly.\r\n\r\nIt seems there are substantial differences in tf0.12 and tf1.12.0 in terms of initialize a variable throught the value of another tensor.\r\nFor example, the following code (with 6 layers) is very slow in the newer tf1.12.0.\r\napprox 2 sec in tf.0.12\r\napprox 70 sec in tf1.12.0\r\nThe creation of the graph in tf.1.12.0 is progressively getting slower for each extra added layer..\r\n\r\n**Describe the expected behavior**\r\nI would expect both version to be equally fast, or the newer version to be faster. In any case I would not expect the creation of the graph to slow down at each extra added layer. Also I cannot understand while sharing variables should be slower than instantiating from initializer with constant tensor...\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# In[1]:\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom pprint import pprint\r\nimport time\r\n\r\n\r\nnetwork_architecture = {\r\n    \"channels\" : 10,  # Size of z variables.\r\n    \"num_layers\" : 6,  # Number of resnet blocks for each downsampling layer.\r\n}\r\n\r\ndef initialize_conv2dwn_vars(x, kernel_shape, output_channels, stride, padding, init_scale=1.0, mask=None):\r\n\r\n    input_shape = x.get_shape()\r\n    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]\r\n    stride_shape = [1, stride[0], stride[1], 1]\r\n\r\n    v_inizializer = tf.random_normal_initializer(0, 0.05)\r\n    v = tf.get_variable(\"v\", filter_shape, tf.float32, v_inizializer)\r\n#     see https://www.tensorflow.org/api_docs/python/tf/Variable#initialized_value\r\n    v_aux = v.initialized_value()\r\n    \r\n    if mask is not None:  # used for auto-regressive convolutions.\r\n        v_aux = mask * v_masked\r\n    \r\n    v_norm = tf.nn.l2_normalize(v_aux, [0, 1, 2])\r\n    x_init = tf.nn.conv2d(x, v_norm, strides=stride_shape, padding=padding) # ***\r\n    m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])\r\n    scale_init = init_scale / tf.sqrt(v_init + 1e-10)\r\n\r\n    h_aux = tf.reshape(scale_init, [1, 1, 1, -1]) * (x_init - tf.reshape(m_init, [1, 1, 1, -1]))\r\n\r\n    g = tf.get_variable(\"g\", initializer=tf.log(scale_init) / 3.0)\r\n    b = tf.get_variable(\"b\", initializer=-m_init * scale_init)\r\n            \r\n    return h_aux\r\n\r\ndef initializers_for_conv2dwn_vars(x, kernel_shape, output_channels, stride, padding, init_scale=1.0, mask=None):\r\n\r\n    input_shape = x.get_shape()\r\n    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]\r\n    stride_shape = [1, stride[0], stride[1], 1]\r\n    \r\n    v_aux = tf.constant(np.random.normal(loc=0, scale=0.05, size=filter_shape), dtype=tf.float32, name=\"v_aux\")\r\n\r\n    if mask is not None:  # used for auto-regressive convolutions.\r\n        v_aux = mask * v_masked\r\n    \r\n    v_norm = tf.nn.l2_normalize(v_aux, [0, 1, 2])\r\n    x_init = tf.nn.conv2d(x, v_norm, strides=stride_shape, padding=padding) # ***\r\n    m_init, v_init = tf.nn.moments(x_init, [0, 1, 2])\r\n    scale_init = init_scale / tf.sqrt(v_init + 1e-10)\r\n\r\n    def g_inizializer(*args, **kwargs):\r\n        return tf.log(scale_init) / 3.0\r\n    \r\n    def b_inizializer(*args, **kwargs):\r\n        return -m_init * scale_init\r\n\r\n    def v_inizializer(*args, **kwargs):\r\n        return v_aux\r\n    \r\n    h_aux = tf.reshape(scale_init, [1, 1, 1, -1]) * (x_init - tf.reshape(m_init, [1, 1, 1, -1]))\r\n            \r\n    return {'v' : v_inizializer, 'g' : g_inizializer, 'b' : b_inizializer}, h_aux\r\n\r\n\r\ndef conv2dwn_reuse_vars(inputs, kernel_shape, output_channels, stride, padding, mask):\r\n\r\n    input_shape = inputs.get_shape()\r\n    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]\r\n    stride_shape = [1, stride[0], stride[1], 1]\r\n    print(\"...ready v\")\r\n    v = tf.get_variable(\"v\", shape=filter_shape)\r\n    print(\"...ready g\")\r\n    g = tf.get_variable(\"g\", shape=[output_channels]) #initializer=initializers['g'],\r\n    print(\"...ready b\")\r\n    b = tf.get_variable(\"b\", shape=[output_channels]) # initializer=initializers['b'],\r\n    print(\"...done vars\")\r\n    if mask is not None:\r\n        v = mask * v\r\n\r\n    # use weight normalization (Salimans & Kingma, 2016)\r\n    w = tf.reshape(tf.exp(g), [1, 1, 1, output_channels]) * tf.nn.l2_normalize(v, [0, 1, 2])\r\n\r\n    # calculate convolutional layer output\r\n    b = tf.reshape(b, [1, 1, 1, -1])\r\n    \r\n    print(\"...ready\")\r\n    r = tf.nn.conv2d(inputs, w, stride_shape, padding) + b\r\n    print(\"...done\")\r\n        \r\n    return r\r\n\r\ndef conv2dwn_create_vars(inputs, initializers, kernel_shape, output_channels, stride, padding, mask):\r\n\r\n    input_shape = inputs.get_shape()\r\n    filter_shape = [kernel_shape[0], kernel_shape[1], int(input_shape[-1]), output_channels]\r\n    stride_shape = [1, stride[0], stride[1], 1]\r\n    print(\"...ready v\")\r\n    v = tf.get_variable(\"v\", shape=filter_shape, initializer=initializers['v'])\r\n    print(\"...ready g\")\r\n    g = tf.get_variable(\"g\", shape=[output_channels], initializer=initializers['g'])\r\n    print(\"...ready b\")\r\n    b = tf.get_variable(\"b\", shape=[output_channels], initializer=initializers['b'])\r\n    print(\"...done vars\")\r\n    if mask is not None:\r\n        v = mask * v\r\n\r\n    # use weight normalization (Salimans & Kingma, 2016)\r\n    w = tf.reshape(tf.exp(g), [1, 1, 1, output_channels]) * tf.nn.l2_normalize(v, [0, 1, 2])\r\n\r\n    # calculate convolutional layer output\r\n    b = tf.reshape(b, [1, 1, 1, -1])\r\n    \r\n    print(\"...ready\")\r\n    r = tf.nn.conv2d(inputs, w, stride_shape, padding) + b\r\n    print(\"...done\")\r\n        \r\n    return r\r\n\r\n\r\n# In[8]:\r\n\r\n\r\n# REUSE VARIABLES\r\n\r\ndef conv2d_weightnorm_layer(name, inputs, inputs_aux, n_channels, kernel_shape=(3,3), stride=(1,1), init_scale=1.0, mask=None):\r\n    \r\n    conv2dwn_kwargs = {\"kernel_shape\" : kernel_shape,\r\n                       \"stride\" : stride,\r\n                       \"padding\" : 'SAME',\r\n                       \"mask\" : mask\r\n    }\r\n\r\n    print(\"creating layer \" + name)\r\n    \r\n    with tf.variable_scope(name, reuse=None):#, reuse=tf.AUTO_REUSE):\r\n        h_aux = initialize_conv2dwn_vars(inputs_aux,\r\n                                      output_channels = n_channels,\r\n                                      init_scale = init_scale,\r\n                                      **conv2dwn_kwargs)\r\n\r\n    print(\"middle\")\r\n\r\n    with tf.variable_scope(name, reuse=True):#, reuse=tf.AUTO_REUSE):\r\n        h = conv2dwn_reuse_vars(inputs, output_channels = n_channels, **conv2dwn_kwargs)\r\n        \r\n    print(\"done\")\r\n    print(h, h_aux)\r\n        \r\n    return h, h_aux\r\n\r\n\r\nclass Network:\r\n    \r\n    def __init__(self, network_architecture):\r\n        self._num_layers = network_architecture[\"num_layers\"]\r\n        self._channels = network_architecture[\"channels\"]\r\n    \r\n    def _build_net(self, x):\r\n        \r\n        h, h_aux = conv2d_weightnorm_layer(\"first_layer_conv\",\r\n                                        x,\r\n                                        x,\r\n                                        self._channels,\r\n                                        kernel_shape = (5,5),\r\n                                        stride = (2,2)\r\n                                       )\r\n\r\n        print(\"start loop\")\r\n        for i in range(self._num_layers):\r\n            print(\"\\n layer %d\"%i)\r\n            h, h_aux = conv2d_weightnorm_layer(\"layer%d\"%i,\r\n                                    h,\r\n                                    h_aux,\r\n                                    self._channels,\r\n                                    kernel_shape = (5,5),\r\n                                    stride = (1,1)\r\n                                   )\r\n            print(\"DONE layer %d \\n\"%i)\r\n                \r\n        return h, h_aux\r\n    \r\n\r\n\r\n# In[9]:\r\n\r\ntf.reset_default_graph()\r\n\r\nprint(\"\\n\\nGRAPH CREATION WITH WEIGHT SHARING...\\n\\n\")\r\nt_i = time.time()\r\n\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\r\nnet = Network(network_architecture)\r\noutput = net._build_net(x)\r\n\r\nt_f = time.time()\r\nprint(\"\\nEND OF GRAPH CREATION WITH WEIGHT SHARING\")\r\nprint(\"time : %g s\\n\\n\"%(t_f - t_i))\r\n\r\n# In[ ]:\r\n\r\n\r\ng_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\npprint([str(var.name)+\" \"+str(var.get_shape().as_list()) for var in g_vars])\r\nprint(len(g_vars))\r\n\r\n\r\n# In[ ]:\r\n\r\n\r\n# PASS INITIALIZERS\r\n\r\ndef conv2d_weightnorm_layer(name, inputs, inputs_aux, n_channels, kernel_shape=(3,3), stride=(1,1), init_scale=1.0, mask=None):\r\n    \r\n    conv2dwn_kwargs = {\"kernel_shape\" : kernel_shape,\r\n                       \"stride\" : stride,\r\n                       \"padding\" : 'SAME',\r\n                       \"mask\" : mask\r\n    }\r\n\r\n    print(\"creating layer \" + name)\r\n    \r\n    with tf.variable_scope(name, reuse=None):#, reuse=tf.AUTO_REUSE):\r\n        initializers, h_aux = initializers_for_conv2dwn_vars(inputs_aux,\r\n                                                  output_channels = n_channels,\r\n                                                  init_scale = init_scale,\r\n                                                  **conv2dwn_kwargs)\r\n    \r\n    print(\"middle\")\r\n\r\n    with tf.variable_scope(name, reuse=None):#, reuse=tf.AUTO_REUSE):\r\n        h = conv2dwn_create_vars(inputs,\r\n                                initializers = initializers,\r\n                                output_channels = n_channels,\r\n                                **conv2dwn_kwargs)\r\n        \r\n    print(\"done\")\r\n    print(h, h_aux)\r\n    \r\n    return h, h_aux\r\n\r\n\r\nclass Network:\r\n    \r\n    def __init__(self, network_architecture):\r\n        self._num_layers = network_architecture[\"num_layers\"]\r\n        self._channels = network_architecture[\"channels\"]\r\n    \r\n    def _build_net(self, x):\r\n        \r\n        h, h_aux = conv2d_weightnorm_layer(\"first_layer_conv\",\r\n                                        x,\r\n                                        x,\r\n                                        self._channels,\r\n                                        kernel_shape = (5,5),\r\n                                        stride = (2,2)\r\n                                       )\r\n\r\n        print(\"start loop\")\r\n        for i in range(self._num_layers):\r\n            print(\"\\n layer %d\"%i)\r\n            h, h_aux = conv2d_weightnorm_layer(\"layer%d\"%i,\r\n                                    h,\r\n                                    h_aux,\r\n                                    self._channels,\r\n                                    kernel_shape = (5,5),\r\n                                    stride = (1,1)\r\n                                   )\r\n            print(\"DONE layer %d \\n\"%i)\r\n                \r\n        return h, h_aux\r\n    \r\n\r\n\r\n# In[ ]:\r\n\r\n\r\ntf.reset_default_graph()\r\n\r\nprint(\"\\n\\nGRAPH CREATION WITH INITIALIZERS FROM OTHER TENSORS...\\n\\n\")\r\nt_i = time.time()\r\n\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\r\nnet = Network(network_architecture)\r\noutput = net._build_net(x)\r\n\r\nt_f = time.time()\r\nprint(\"\\nEND OF GRAPH CREATION WITH INITIALIZERS FROM OTHER TENSORS\")\r\nprint(\"time : %g s\\n\\n\"%(t_f - t_i))\r\n\r\n# In[ ]:\r\ng_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\npprint([str(var.name)+\" \"+str(var.get_shape().as_list()) for var in g_vars])\r\nprint(len(g_vars))\r\n\r\n```\r\n\r\n\r\n**Other info**\r\nAlso plotting the graph for the two tf versions give completely different results (see graphs attached). It seems in the newer version tf1.12.0 th layer will directly depend on all the previous layers, does this means that more connections will be created in the newer version?\r\nIs this expected? Could you help me understand the mechanism behind and if there is a workaround to have a fast custom data-dependent initialization from a tensor?\r\n\r\nIN TF0.12\r\n![wn_tf0 12](https://user-images.githubusercontent.com/9975354/52204247-4e867080-287c-11e9-911f-af81c6ecddb9.png)\r\n\r\nIN TF1.12.0\r\n![wn_tf1 12 0](https://user-images.githubusercontent.com/9975354/52204250-50e8ca80-287c-11e9-8685-d3388a151364.png)\r\n", "comments": ["cc @alextp @akshaym @martinwicke ", "I believe these are just cosmetic differences because tf.layers changed its implementation of name scopes.", "That explains the visualization, but not the runtime though", "I believe it is not only a cosmetic difference. There are two methods in the code that I uploaded:\r\n1. variable initialization (data-dependent) and weight sharing,\r\n2. initialization from custom initializers (custom tensors data-dependent).\r\n\r\nI am not sure exactly what is the difference in the low-level implementation of these two but the times are very different for them.\r\n\r\nAlso: the graph of tf1.12.0 seems to have much more direct connections, is this really only a visualization issue?\r\nWhen I set to initialize from a tensor, in my understanding, it should evaluate the value of that tensor during  the variables initialization operator and assign the value to my variable. Is this correct? So what are the extra connections to all the previous layers for...\r\nI would say the graph of tf0.12 makes more sense to me... but maybe I don't know exactly what is behind?\r\n", "Any news on this bug?\r\nThanks!", "I don't know how to debug this because 0.12 and 1.12 are very very many\nreleases apart from each other, so many things could have changed. Can you\ntry to find a small reproducible example of the behavior you don't like?\n\nOn Tue, Feb 26, 2019 at 7:49 AM Riccardo Volpi <notifications@github.com>\nwrote:\n\n> Any news on this bug?\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25484#issuecomment-467490220>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVmmd9CMmwycj5jVCmzjOWNOkkCgks5vRVekgaJpZM4ag9rY>\n> .\n>\n\n\n-- \n - Alex\n", "Certainly!\r\nI think the small code I posted is exactly the example you mention, I am simply creating some convolutional layers and I try to initialize them in data dependent way.\r\nIf anything specific is not clear please let me know. Thanks\r\n\r\nEdit: it seems to me the graph is excessively connected in the tf 1.12, if you look at the graph all layers are directly connected to all the previous ones. This might be the reason why the slow down is growing so much with the number of layers... in the 0.12 graph each layer is connected only to the previous one, as one would expect I believe,...", "@ricvo, I tried on colab with Tensorflow version 1.15.0. Looks like the issue is fixed in Tf1.15. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/97f468883cddce3ccf19b2596de7c2ba/untitled205.ipynb) and let us know if issue still persists. Thanks!", "@ricvo, Is this still an issue!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25484\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25484\">No</a>\n"]}, {"number": 25483, "title": "Fixed the indentation convention problem", "body": "Fixed indentation convention problem in two places", "comments": ["@aselle can you pls review the PR and let me know your comments", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 60 days with no activity and the `awaiting review` label has been applied.", "@impjdi, thanks for the review, i am closing this PR "]}, {"number": 25482, "title": "Fixed a misspelled var name in hlo_cost_analysis.cc comment.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 25481, "title": "Here is a list of operators for which you will need custom implementations: AddN.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.2\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source):  \r\ntensorflow==1.12.0  \r\ntf-nightly==1.13.0.dev20190126\r\n\r\n**Describe the current behavior**\r\nWhen I convert my custom model to TensorFlow Lite model format, I got the following error message and cannot convert it. My question is that what does \"AddN\" in this message mean. Another question is that why does this model include NEG and RELU. I don't use these layers in original model I create.\r\n\r\n**Provide the text output from tflite_convert**\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. \r\nIf those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). \r\nOtherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). \r\nHere is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MUL, NEG, PRELU, RELU, RESIZE_BILINEAR, RESIZE_NEAREST_NEIGHBOR, SUB, TRANSPOSE_CONV. \r\nHere is a list of operators for which you will need custom implementations: AddN.\r\n```\r\n\r\nThis file is the target file that I tried to convert.  \r\n[savedmodel.zip](https://github.com/tensorflow/tensorflow/files/2826784/savedmodel.zip)\r\n\r\n**Any other info / logs**\r\nThis is the code of file conversion I tried.\r\n```\r\ntflite_convert --output_file tensorflow_lite/model.tflite --saved_model_dir savedmodel\r\n```\r\n", "comments": ["This issue was solved.  \r\nI found that the Add layer causes this problem.  \r\nBy removing the Add layer, an error above was not occured.  \r\nThanks."]}, {"number": 25480, "title": "C- How to draw bounding-box using tensorflow c_api", "body": "### In python \r\nI did object detection, Its working fine. Same thing I want to do in C\r\n\r\nHere is the code to detect the object object in python\r\n\r\nCalling boxes, like following:\r\n\r\n```\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n```\r\nsimilarly for scores, and classes.\r\n\r\nThen just called them in session run.\r\n\r\n```\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n```\r\nNow Drawing the results of the detection\r\n\r\n```\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=8,\r\n    min_score_thresh=0.80)\r\n```\r\nNow I can able to view the image and detect the object(face).\r\n\r\n`cv2.imshow('Object detector', image)`\r\n\r\nQuestion :\r\n\r\n**C language**\r\n\r\nNow my question is how can I Feed the image data's into graph and detect the object in that image as well as draw the rectangle box using C Programming.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "Above I had explained everything????\r\n\r\nI got the graph data from the trained graph.\r\n\r\nThen I passed both image data and graph data into **TF_SessionRun** . After done passing data's, When I view ` cvShowImage(\"Face Detection\", img);`. It's not detecting the face.\r\n\r\nAbove I posted my full code please check.\r\n\r\nAtleast, If any one will give any idea!!!, That will be great help.\r\n\r\nThanks", "TF_Tensor * imgTensor = TF_NewTensor(TF_UINT8, dims, 4, img->imageData, img->imageSize, &deallocator, NULL);\r\ntry \"img->imageData\"  ", "@tika64208  It's not working....", "Have you solved your problem\uff1f\r\n\r\n a similar problem  puzzle me \r\n_\r\n`#include <stdio.h>\r\n#include <stdlib.h>\r\n#include <string.h>\r\n#include \"c_api.h\"\r\n\r\n\r\n#define STB_IMAGE_IMPLEMENTATION\r\n#include \"stb_image.h\"\r\n\r\nTF_Buffer* read_file(const char* file);  \r\nvoid free_buffer(void* data, size_t length) { free(data); }\r\nvoid deallocator(void* ptr, size_t len, void* arg) { free((void*)ptr); }\r\n\r\n\r\nTF_Buffer* read_file(const char* file) {     \r\n    printf(\"debug %s  %d \\n\",file,__LINE__);                                             \r\n  FILE *f = fopen(file, \"rb\");\r\n  \r\n  \r\n    printf(\"debug %p  %d \\n\",f,__LINE__);\r\n  fseek(f, 0, SEEK_END);\r\n  long fsize = ftell(f);                                                                  \r\n  fseek(f, 0, SEEK_SET);  //same as rewind(f);                                            \r\n\r\n    printf(\"debug  %d \\n\",__LINE__);\r\n  void* data = malloc(fsize);                                                             \r\n  fread(data, fsize, 1, f);\r\n  fclose(f);\r\n\r\n    printf(\"debug  %d \\n\",__LINE__);\r\n  TF_Buffer* buf = TF_NewBuffer();                                                        \r\n  buf->data = data;\r\n  buf->length = fsize;                                                                    \r\n  buf->data_deallocator = free_buffer;                                                    \r\n  return buf;\r\n}\t\r\n\r\nint testgettensor(char* filepath)\r\n{\r\n\t\r\n    printf(\"debug  %d \\n\",__LINE__);\r\n\tTF_Buffer* graph_def = read_file(\"./model.pb\");   \r\n  printf(\"graph_def %p \\n\",graph_def);                   \r\n  TF_Graph* graph = TF_NewGraph();\r\n  TF_Status* status = TF_NewStatus();                                                     \r\n  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();                        \r\n  TF_GraphImportGraphDef(graph, graph_def, opts, status);\r\n  TF_DeleteImportGraphDefOptions(opts);\r\n  if (TF_GetCode(status) != TF_OK) {\r\n          fprintf(stderr, \"ERROR: Unable to import graph %s\\n\", TF_Message(status));        \r\n          return 1;\r\n  }\r\n  \r\n  fprintf(stdout, \"Successfully imported graph\\n\");                                          \r\n  TF_SessionOptions* opt = TF_NewSessionOptions();\r\n  TF_Session* sess = TF_NewSession(graph, opt, status);\r\n  //TF_DeleteSessionOptions(opt);\r\n  if (TF_GetCode(status) != TF_OK) {\r\n    fprintf(stderr, \"ERROR: Unable to create session %s\\n\", TF_Message(status));\r\n    return 1;\r\n  }\r\n  fprintf(stdout, \"Successfully session created\\n\");\r\n\r\n\r\n\r\n\tint w, h, n;\r\n\tfloat *data = stbi_loadf(filepath, &w, &h, &n, 0);\r\n\t if(!data) {\r\n    printf(\"Failed to laodiamge\");\r\n  }\r\n  \r\n  \r\n    printf(\"debug  %d \\n\",__LINE__);\r\n  int64_t dims[] = {1, w, h, n };\r\n  float tmpdata[100*100*3]={0};\r\n  \r\n\tTF_Tensor * imgTensor = TF_NewTensor(TF_FLOAT, dims, 4, data, w*h*n*sizeof(float), &deallocator, NULL);\r\n\r\n\r\n    printf(\"debug  %d \\n\",__LINE__);\r\n  \r\n  TF_Operation* input_image_op = TF_GraphOperationByName(graph, \"x\");\r\n  if(!input_image_op) {\r\n    printf(\"Failed to find Op '%s'\", \"image_tensor\");\r\n    return 1;\r\n  }\r\n  \r\n    printf(\"debug  %d \\n\",__LINE__);\r\n  TF_Output image_input;\r\n  image_input.oper = input_image_op;\r\n  image_input.index = 0;\r\n  TF_Output inputs[1] = {image_input};\r\n  \r\n  \r\n  \r\n  TF_Operation* logits_eval_op = TF_GraphOperationByName(graph, \"logits_eval\");\r\n  if(!logits_eval_op) {\r\n    printf(\"Failed to find Op '%s'\", \"num_detections\");\r\n    return 1;\r\n  }\r\n  TF_Output num_detection_output;\r\n  num_detection_output.oper = logits_eval_op;\r\n  num_detection_output.index = 0;\r\n  TF_Output outputs[1] = {num_detection_output};\r\n  TF_Tensor* output_tensors[1];\r\n  \r\n  TF_SessionRun(sess, NULL, \r\n                &image_input, &imgTensor, 1,\r\n                outputs, output_tensors, 1,\r\n                NULL, 0,\r\n                NULL, status);\r\n                \r\n               \r\n\tprintf(\"TF_SessionRun: %s\\n\", TF_Message(status));\r\n\t  if(TF_GetCode(status) != TF_OK) {\r\n    printf(\"It's coming here\");\r\n    printf(\"%s\\n\", TF_Message(status));\r\n  }\r\n  else {\r\n\r\n    printf(\"Ran successfully\\n\");\r\n    float* f = (float*)TF_TensorData(output_tensors[0]);\r\n    printf(\"TF data %f\\n\", f[0]);\r\n    printf(\"TF data %f\\n\", f[1]);\r\n    printf(\"TF data %f\\n\", f[2]);\r\n    printf(\"TF data %f\\n\", f[3]);\r\n    printf(\"TF data %f\\n\", f[4]);\r\n    \r\n    printf(\"TF data %d\\n\", TF_TensorType(output_tensors[0]));\r\n\r\n  \r\n  \r\n  return 0;\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n    printf(\"Hello from TensorFlow C library version\\n\");\r\n    \r\n    printf(\"Failed to laodiamge\\n\");\r\n    testgettensor(argv[1]);\r\n   // testLoadModel();\r\n    return 0;\r\n}\r\n`_\r\nthis code can run ,but wrong result \r\n\r\n", "@Ashokcharu ,\r\n\r\nFrom your codes, I think you were using `TF 1.X` version and its officially considered as end of life, can you update to recent stable version `2.6.0` and let us know if the problem still persists. Here's a [documentation](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/draw-bounding-boxes) of drawing bounding boxes using C++ API. Let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25480\">No</a>\n"]}, {"number": 25479, "title": "tensorflow build 1.13 build fails", "body": "I need tensorflow 1.13 (1.12 or below doesn't fit my need.)\r\nBut for now I can only build from source.\r\ncommand:\r\n` bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\nreturned:\r\n> **ERROR: /Users/cindy951357/Documents/pythonworkspace/sphinxenv/tensorflow-1.13.0-rc0/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\n> Traceback (most recent call last):\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n>     from tensorflow.python.tools.api.generator import doc_srcs\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 72, in <module>\r\n>     from tensorflow.python.ops.standard_ops import *\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py\", line 25, in <module>\r\n>     from tensorflow.python import autograph\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/__init__.py\", line 37, in <module>\r\n>     from tensorflow.python.autograph.core.converter import ConversionOptions\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/core/converter.py\", line 74, in <module>\r\n>     from tensorflow.python.autograph.pyct import cfg\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/cfg.py\", line 41, in <module>\r\n>     from tensorflow.python.autograph.pyct import compiler\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/compiler.py\", line 30, in <module>\r\n>     import astor\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/__init__.py\", line 14, in <module>\r\n>     from .code_gen import to_source  # NOQA\r\n>   File \"/private/var/tmp/_bazel_cindy951357/121f1294318945434311585d2ce28ccd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/code_gen.py\", line 311\r\n>     def visit_FunctionDef(self, node, async=False):\r\n>                                           ^\r\n> SyntaxError: invalid syntax\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 8612.039s, Critical Path: 400.52s\r\n> INFO: 9224 processes: 9224 local.\r\n> FAILED: Build did NOT complete successfully**\r\n\r\n[this discussion](https://github.com/tensorflow/tensorflow/issues/25151) says that we can wait for **rc2**.\r\nBut when will rc2 release?\r\nOr can I use python3.6 to build tensorflow 1.13? \r\n", "comments": ["@cindy951357 Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thank", "Thank you for your response. I am not quite understanding what you mean by **update them if they are relevant in your case, or leave them as N/A**. \r\nFor your information, from my experience( after some testing), tensorflow 1.13 can be installed with python 3.6.5 but **NOT** python 3.7.", "@cindy951357 We want the system information in order to provide faster resolution. For example, if you don't have GPU, then you will write N/A for \"GPU model and memory:\". Regarding TF 1.13, python 3.7 was not listed on the download [page](https://pypi.org/project/tf-nightly/). Please try Python 3.6.5. Thanks!\r\n", "Alex, Mark -- I believe you are users of astor in TF. The message above indicates that astor is not compatible with Py 3.7 (use of reserved word). This means that we must replace astor, or fix it. \r\n\r\nCan it be fixed?", "@aselle @goldiegadde this would prevent 3.7 binaries for 1.13, right?", "@gunan @av8ramit ", "This looks like it's fixed in the latest `astor` release:\r\n\r\nhttps://github.com/berkerpeksag/astor/blob/0.7.1/astor/code_gen.py#L312\r\n\r\nAnd it looks like tensorflow is using 0.71.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L287\r\n\r\n@cindy951357 what version of astor do you have installed. It looks like 0.6.* or before is broken.\r\n\r\nI don't see how it would make a difference, AFAIK the builds are supposed to run from the workspace file, but try:\r\n\r\n```\r\npip install -U astor\r\n```\r\nMaybe something in your local installation is masking the 0.7.1 version.", "@martinwicke yes unfortunately I'm running into this issue on the r1.13 branch. It seems to be fixed in master. I think we need to cherrypick https://github.com/tensorflow/tensorflow/commit/868a11a8de788134caeadc895625583e8b44b660", "FYI @mdanatg ", "PR is out: https://github.com/tensorflow/tensorflow/pull/25503", "It has been merged. @cindy951357 is this still an issue?", "I'm going to close this issue for now. Please comment if this is still an issue."]}, {"number": 25478, "title": "TF 1.11.0 Updating non-Variables?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes https://gist.github.com/dvisztempacct/43e738e1651ecf61323ae92cae41c94c\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu Bionic Derivative (System76 PopOS)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n- TensorFlow installed from (source or binary):\r\n```\r\npip3 install tensorflow==1.11.0\r\n```\r\n- TensorFlow version (use command below):\r\n```\r\n$ python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.4) doesn't match a supported version!\r\n  RequestsDependencyWarning)\r\nv1.11.0-0-gc19e29306c 1.11.0\r\n```\r\n- Python version:\r\n```\r\n$ python3 --version\r\nPython 3.6.7\r\n```\r\n- Bazel version (if compiling from source):\r\nn/a\r\n- GCC/Compiler version (if compiling from source):\r\nn/a\r\n- CUDA/cuDNN version:\r\n```\r\n$ dpkg -l | grep -iE 'cuda|cudnn'\r\nii  cuda-repo-ubuntu1604                             9.0.176-1                           amd64        cuda repository configuration files\r\nii  system76-cuda                                    0pop2                               amd64        NVIDIA CUDA Compiler / Libraries / Toolkit Metapackage\r\nii  system76-cuda-9.0                                0pop3                               amd64        NVIDIA CUDA 9.0 Compiler / Libraries / Toolkit\r\nii  system76-cuda-9.2                                0pop3                               amd64        NVIDIA CUDA 9.2 Compiler / Libraries / Toolkit\r\nii  system76-cudnn-9.0                               7.1.4~0pop1                         amd64        NVIDIA CUDA Deep Neural Network library (cuDNN) for CUDA 9.0\r\n```\r\n- GPU model and memory:\r\n```\r\n$ lspci | grep VGA\r\n00:02.0 VGA compatible controller: Intel Corporation Device 3e9b\r\n01:00.0 VGA compatible controller: NVIDIA Corporation GP104M [GeForce GTX 1070 Mobile] (rev a1)\r\n$ nvidia-smi\r\nSun Feb  3 20:54:44 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   52C    P5    16W /  N/A |    682MiB /  8119MiB |     29%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nNon-variables seem to be being updated.\r\n\r\n**Describe the expected behavior**\r\nNon-variables should be initialized when running `tf.global_variables_initializer` and otherwise not updated.\r\n\r\n**Code to reproduce the issue**\r\nhttps://gist.github.com/dvisztempacct/43e738e1651ecf61323ae92cae41c94c\r\n\r\n**Other info / logs**\r\n![screenshot from 2019-02-03 20-51-44](https://user-images.githubusercontent.com/37460069/52190947-d2961500-27f6-11e9-9585-27ad0a23432c.png)\r\n\r\nI may simply be misunderstanding how `tf.random_normal` is supposed to work, but I had thought if I didn't initialize a `tf.Variable` using the tensor returned by `tf.random_normal` (as I do on line 12) that it would only be initialized by `tf.global_variables_initializer` or similar, and wouldn't be updated or reinitialized upon subsequent runs unless I included such an initializer.\r\n\r\nThanks!", "comments": ["tf.random_normal is a stateful operation. This means every time it's executed its output tensor might have a different value (or that it might have side effects). So I think the behavior you're observing is by design.\r\n\r\nIf you want a constant random normal vector use tf.constant(np.random.normal(...)), say.", "Hi @alex-petrenko I did try `tf.constant`:\r\n\r\n```\r\nimport tensorflow as tf\r\nfoo = tf.random_normal(\r\n    shape=[],\r\n    mean=0.0,\r\n    stddev=1.0,\r\n    name='train_x'\r\n)\r\ntrain_x = tf.constant(foo, shape=foo.shape)\r\n```\r\n\r\nThe output:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 8, in <module>\r\n    train_x = tf.constant(foo, shape=foo.shape)\r\n  File \"/home/hdon/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 207, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/home/hdon/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 442, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/home/hdon/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 350, in _AssertCompatible\r\n    raise TypeError(\"List of Tensors when single Tensor expected\")\r\nTypeError: List of Tensors when single Tensor expected\r\n```\r\n\r\nThe only thing that did work for me was a `tf.Variable` with `trainable=False`. Which leads me to wonder: what is the difference between a `tf.constant` and `tf.Variable` with `trainable=False`?\r\n\r\nThanks!\r\n\r\nEdit: Thanks also for the note about `random_normal` being stateful. I had assumed that once initialized, its value would not change when the graph is reevaluated.", "tf.constant takes a numpy array as an argument, not a tensor\n\nOn Tue, Feb 5, 2019 at 10:25 AM Don Viszneki <notifications@github.com>\nwrote:\n\n> Hi @alex-petrenko <https://github.com/alex-petrenko> I did try tf.constant\n> :\n>\n> import tensorflow as tf\n> foo = tf.random_normal(\n>     shape=[],\n>     mean=0.0,\n>     stddev=1.0,\n>     name='train_x'\n> )\n> train_x = tf.constant(foo, shape=foo.shape)\n>\n> The output:\n>\n> Traceback (most recent call last):\n>   File \"bug.py\", line 8, in <module>\n>     train_x = tf.constant(foo, shape=foo.shape)\n>   File \"/home/hdon/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 207, in constant\n>     value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n>   File \"/home/hdon/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 442, in make_tensor_proto\n>     _AssertCompatible(values, dtype)\n>   File \"/home/hdon/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 350, in _AssertCompatible\n>     raise TypeError(\"List of Tensors when single Tensor expected\")\n> TypeError: List of Tensors when single Tensor expected\n>\n> The only thing that did work for me was a tf.Variable with trainable=False.\n> Which leads me to wonder: what is the difference between a tf.constant\n> and tf.Variable with trainable=False?\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25478#issuecomment-460746426>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfJn3T_RL7PGXi51IFyEyaTXWkwhks5vKcx7gaJpZM4agjd4>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 25477, "title": "[tools]add transform graph(fold_moments,fold_batch_norms_algebraic)", "body": "1. fold moments and batch norms when using instance_norm\r\n2. add test case", "comments": ["@czy2014hust can you please resolve conflicts ?", "Can one of the admins verify this patch?"]}, {"number": 25476, "title": "Gradient calculation in eager mode", "body": "Hi, \r\nI would like to know what exactly tape.gradient does in eager mode when a batch is passed to it. \r\nDoes it aggregate all gradients? and if so, does it sum them up or average them or what?\r\n\r\n\r\nThanks", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this support issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). Thanks!"]}, {"number": 25475, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. \\n Failed to load the native TensorFlow runtime.", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.8.0\r\n- Python version: 3.6.8\r\n- Installed using pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\nI am trying to run an extractor on my pc. Keep getting error no matter what I try. I tried using python 3.7 and 3.6, 64 bit and 32 bit, using tensorflow 1.8 and 1.12, and always get the same error. Has anyone else seen this? \r\n![untitled](https://user-images.githubusercontent.com/46904845/52184742-108a3d80-27e5-11e9-8a44-ac1afc00342a.png)\r\n\r\n*extra context: the program works on my regular laptop. I wanted to try it on my pc and let it do the hard ML stuff and I use my laptop for other stuff. This pc didnt have any software stuff, so i installed python, pycharm and all the packages.*\r\n\r\n\r\nI'm a noob so it might be a silly mistake. But I spent 3 days doing everything I can and still nothing...\r\n\r\n\r\n", "comments": ["@mickey740 Did you follow the instructions listed on [TensorFlow website](https://www.tensorflow.org/install/source_windows). Please also check [tested build configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations). Did you try creating Virtual Machine and installing all the required modules? Please also provide any other details to find root-cause of the issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 25473, "title": "tf.distribution.Normal memory leak on calls to sample() ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\npip3 installation\r\n- TensorFlow version (use command below):\r\n12.0\r\n- Python version:\r\n3.6.7\r\n\r\n**Describe the current behavior**\r\nType in the code below and watch memory usage increase.\r\n\r\n**Describe the expected behavior**\r\nType in the code below and see no memory usage increases.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n\r\n    dist=tf.distributions.Normal(loc=0., scale=1.)\r\n\r\n    for i in range(1_000_000):\r\n        a=dist.sample()\r\n", "comments": ["Eager execution is still not the default in TF 1.x, so that code is appending to the default TensorFlow graph repeatedly and not doing any real computation.\r\n\r\nAdding `tf.enable_eager_execution()` after the TF import should fix the memory usage.", "I thought it was a good repro for a problem in a 300 lines of code program, but seems like I'll just have to search for another way to give a reasonable reproduction.", "You might call [tf.get_default_graph().finalize()](https://www.tensorflow.org/api_docs/python/tf/Graph#finalize) before the training loop to rule out the graph itself getting larger. Or if that's a difficult refactor because the graph is expanding every training iteration, enabling eager execution is a good option.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25472, "title": "tf2.0 - tf.keras.optimizers.Optimizer.get_updates() does not work", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190203\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen keras optimizer's get_updates() function is called, it raises error \"RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\"\r\n\r\n**Describe the expected behavior**\r\nI expect it returns correct operators that can be used to build keras function that changes weights.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nfrom tensorflow import keras\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(1, input_shape=(1,)))\r\nmodel.compile(optimizer='adam', loss='mse')\r\nloss = keras.losses.mse(model.input, model.output)\r\nupdates = model.optimizer.get_updates(params=model.trainable_weights, loss=loss)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-f4bb57df0d91> in <module>()\r\n      5 loss = keras.losses.mse(model.input, model.output)\r\n      6 model.optimizer.get_updates(\r\n----> 7         params=model.trainable_weights, loss=loss)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in get_updates(self, loss, params)\r\n    440 \r\n    441   def get_updates(self, loss, params):\r\n--> 442     grads = self.get_gradients(loss, params)\r\n    443     grads_and_vars = list(zip(grads, params))\r\n    444     self._assert_valid_dtypes([\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in get_gradients(self, loss, params)\r\n    353     \"\"\"\r\n    354     loss = self._scale_loss(loss)\r\n--> 355     grads = gradients.gradients(loss, params)\r\n    356     if None in grads:\r\n    357       raise ValueError(\"An operation has `None` for gradient. \"\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\r\n    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n    157         gate_gradients, aggregation_method, stop_gradients,\r\n--> 158         unconnected_gradients)\r\n    159   # pylint: enable=protected-access\r\n    160 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    545   \"\"\"Implementation of gradients().\"\"\"\r\n    546   if context.executing_eagerly():\r\n--> 547     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\r\n    548                        \"is enabled. Use tf.GradientTape instead.\")\r\n    549   if src_graph is None:\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n```", "comments": ["This is fixed TF 2.0 nightly '2.0.0-dev20190718' \r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25472\">No</a>\n"]}, {"number": 25471, "title": "Additional  FreeBSD compatibility ", "body": "In this commit some additional changes to increase compatibility with FreeBSD.\r\nAlso i have some little patches for gcs & ignite, but I do not know if they should be included in the master branch.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I've signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@rthadur I'm not familiar with the changed files, please help to find other appropriate reviews.", "For some reason GitHub doesn't let me to add comments inline, so I'll ask here.\r\n\r\nWhat's the reason for the change in `tensorflow/lite/toco/BUILD`?", "> What's the reason for the change in `tensorflow/lite/toco/BUILD`?\r\nBuild failed when package neon is installed in system. I think this is version\\patches conflict. This is dependency solves build error independent of system packages.", "Are you talking about `www/neon`?\r\n\r\nIf so, the change seems right, but it shouldn't be constained to FreeBSD only, as any Linux can have `neon` installed from system package manager. Any TensorFlow dev to comment on that?", "> Are you talking about `www/neon`?\r\n\r\nYes, but now i read description this package and this should not influence. I think it's FreeBSD specific dependency resolve error, on Linux building are ok without this changes.", "For me, `bazel build //tensorflow:libtensorflow.so` succeceeds wihtout that change, and `//tensorflow/python:python` fails with unrelated error.\r\n\r\nWhat error do you get without this change?", "> What error do you get without this change?\r\n\r\nERROR: /usr/home/xeon/tensorflow/tensorflow/lite/toco/BUILD:425:1: Linking of rule '//tensorflow/lite/toco:toco' failed (Exit 1)\r\nld: error: undefined symbol: tflite::tensor_utils::NeonSymmetricQuantizeFloats(float const*, int, signed char*, float*, float*, float*)\r\n>>> referenced by tensor_utils.cc\r\n>>>               tensor_utils.o:(tflite::tensor_utils::SymmetricQuantizeFloats(float const*, int, signed char*, float*, float*, float*)) in archive bazel-out/host/bin/tensorflow/lite/kernels/internal/libtensor_utils.a\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n", "The `//tensorflow/lite/toco:toco` built fine for me. Maybe you need to rebase your changes onto recent HEAD?\r\n\r\nOr try out my branch with other FreeBSD fixes added: https://github.com/tensorflow/tensorflow/pull/28689", "> The `//tensorflow/lite/toco:toco` built fine for me. Maybe you need to rebase your changes onto recent HEAD?\r\n> \r\n> Or try out my branch with other FreeBSD fixes added: #28689\r\n\r\nyour changes same :)\r\nhttps://github.com/tensorflow/tensorflow/pull/28689/commits/663d93c92593fa256e039abdc8cb0307c1a87164\r\n\r\nthis affects to the whole tf-lite module, and not to the specific problematic toco-module", "Closing in favor of #28689"]}, {"number": 25470, "title": "Scoping bug with custom layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\n-\r\n- GCC/Compiler version (if compiling from source):\r\n-\r\n- CUDA/cuDNN version:\r\nUsing Cpu\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to implement a noisy linear layer in tensorflow, inheriting from tf.keras.layers.Layer . Everything works fine except for reusing variables. This seems to stem from some issue with the scoping: Whenever i use the add_weight function from the superclass and a weight with the same name already existing, it seems to ignore the given reuse-flag in the scope and creates a new variable instead. Interestingly, it does not add a 1 to the variable name in the end as usual in similar cases, but rather adds the 1 to the scope name.\r\nThe Code below prints the following variables:\r\n\r\nscope/noisy_dense/noisy_kernel:0\r\nscope_1/noisy_dense/noisy_kernel:0\r\nscope/my_variable:0\r\n\r\n**Describe the expected behavior**\r\nInstead, i'd expect the weights to be reused. The print would then only be \r\n\r\nscope/noisy_dense/noisy_kernel:0\r\nscope/my_variable:0\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass NoisyDense(tf.keras.layers.Layer):\r\n    def __init__(self,output_dim):\r\n        self.output_dim=output_dim\r\n\r\n        super(NoisyDense, self).__init__()\r\n\r\n    def build(self, input_shape):\r\n        self.input_dim = input_shape.as_list()[1]\r\n        self.noisy_kernel = self.add_weight(name='noisy_kernel',shape=  (self.input_dim,self.output_dim))\r\n\r\ndef noisydense(inputs, units):\r\n\r\n    layer = NoisyDense(units)\r\n\r\n    return layer.apply(inputs)\r\n\r\ninputs = tf.placeholder(tf.float32, shape=(1, 10),name=\"inputs\")\r\n\r\n\r\n\r\nscope=\"scope\"\r\nwith tf.variable_scope(scope):\r\n    inputs3 = noisydense(inputs,\r\n           1)\r\n    my_variable = tf.get_variable(\"my_variable\", [1, 2, 3],trainable=True)\r\n\r\n\r\nwith tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\r\n    inputs2 = noisydense(inputs,\r\n           1)\r\n    my_variable = tf.get_variable(\"my_variable\", [1, 2, 3],trainable=True)\r\n\r\ntvars = tf.trainable_variables()\r\n\r\n\r\n\r\ninit=tf.global_variables_initializer()    \r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    tvars_vals = sess.run(tvars)\r\n\r\nfor var, val in zip(tvars, tvars_vals):\r\n    print(var.name, val)\r\n```\r\n\r\n\r\n", "comments": ["Hi @flodorner , tf.keras layers do not respect variable_scopes when creating their variables, as variable_scopes are going away.\r\n\r\nInstead, share variables by calling your layer multiple times:\r\n\r\n```python\r\nlayer = NoisyDense(units)\r\ny = layer(x)\r\ny2 = layer(x2) # will share variables\r\n```"]}, {"number": 25469, "title": "No converter defined for Switch", "body": "This is a similar issue like https://github.com/tensorflow/tensorflow/issues/23397\r\n```\r\nfrom tensorflow.python.framework import function\r\nfrom tensorflow.python.ops.parallel_for.gradients import jacobian as tf_jacobian\r\nimport numpy as np\r\nimport numpy.matlib\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nmy_graph = tf.Graph()\r\nwith my_graph.as_default():\r\n    x = tf.placeholder(tf.float32, shape=(None,9))\r\n    tf_is_training_ph = tf.placeholder(tf.bool, shape=())\r\n    \r\n    def computeFunc1D(x_1D, is_training):\r\n        x = tf.reshape(x_1D, [1, 9])\r\n        with tf.variable_scope('Test', reuse=tf.AUTO_REUSE):\r\n            dense_out = tf.layers.dense(x, 3, name='dense')\r\n            bn_out = tf.layers.batch_normalization(dense_out, training=is_training, name='bn')\r\n        return tf.reshape(bn_out, [3])\r\n    \r\n    def tensor_jacobian(X, is_training):\r\n        fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)\r\n        J = tf.map_fn(fn_compute_jacobian_1datapt, X)\r\n        return J\r\n    \r\n#    my_tf_jacobian = tensor_jacobian(x, True) # this is working\r\n    my_tf_jacobian = tensor_jacobian(x, tf_is_training_ph) # this is NOT working\r\n\r\nwith tf.Session(graph=my_graph) as sess:\r\n    tf.global_variables_initializer().run()\r\n    \r\n    x_ = 0.5 * np.random.random((2,9))\r\n    \r\n    [my_tf_jacobian_val\r\n     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: True})\r\n    \r\n    print \"x_ = \", x_\r\n    print \"my_tf_jacobian_val = \", my_tf_jacobian_val\r\n    print \"my_tf_jacobian_val.shape = \", my_tf_jacobian_val.shape\r\n```\r\n\r\nThis will result in:\r\n```\r\n1.12.0\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-b20d7560d318>\", line 1, in <module>\r\n    runfile('/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py', wdir='/home/amdgsutanto/Desktop')\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/spyder/utils/site/sitecustomize.py\", line 94, in execfile\r\n    builtins.execfile(filename, *where)\r\n\r\n  File \"/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py\", line 27, in <module>\r\n    my_tf_jacobian = tensor_jacobian(x, tf_is_training_ph) # this is NOT working\r\n\r\n  File \"/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py\", line 23, in tensor_jacobian\r\n    J = tf.map_fn(fn_compute_jacobian_1datapt, X)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py\", line 494, in map_fn\r\n    maximum_iterations=n)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3291, in while_loop\r\n    return_same_structure)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3004, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2939, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3260, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py\", line 483, in compute\r\n    packed_fn_values = fn(packed_values)\r\n\r\n  File \"/home/amdgsutanto/Desktop/test_tf_compute_jacobian_w_batch_norm.py\", line 22, in <lambda>\r\n    fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/gradients.py\", line 59, in jacobian\r\n    pfor_outputs = control_flow_ops.pfor(loop_fn, output_size)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 129, in pfor\r\n    outputs.append(converter.convert(loop_fn_output))\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1077, in convert\r\n    output = self._convert_helper(y)\r\n\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1223, in _convert_helper\r\n    \"which may run slower\" % (y_op.type, y_op, converted_inputs))\r\n\r\nValueError: No converter defined for Switch\r\nname: \"map/while/loop_body/gradients/map/while/Test/bn/cond/Merge_grad/cond_grad\"\r\nop: \"Switch\"\r\ninput: \"map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul\"\r\ninput: \"map/while/Test/bn/cond/pred_id\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul\"\r\n    }\r\n  }\r\n}\r\n\r\ninputs: [WrappedTensor(t=<tf.Tensor 'map/while/loop_body/gradients/map/while/Test/bn/batchnorm/mul_2_grad/Mul/pfor/Mul:0' shape=(3, 3) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'map/while/Test/bn/cond/pred_id:0' shape=() dtype=bool>, is_stacked=False, is_sparse_stacked=False)]. \r\nEither add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n```\r\nWould you mind fixing this, similar like https://github.com/tensorflow/tensorflow/issues/23397 ?\r\n\r\nP.S.: If the fix made it to nightly build, how to update it on my side? Is it just re-installing (e.g. with pip)? Thanks a lot!", "comments": ["So, I came along a simple hack to get around this issue (which sort of works):\r\n```\r\nfrom tensorflow.python.framework import function\r\nfrom tensorflow.python.ops.parallel_for.gradients import jacobian as tf_jacobian\r\nimport numpy as np\r\nimport numpy.matlib\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nmy_graph = tf.Graph()\r\nwith my_graph.as_default():\r\n    x = tf.placeholder(tf.float32, shape=(None,9))\r\n    tf_is_training_ph = tf.placeholder(tf.bool, shape=())\r\n    \r\n    def computeFunc1D(x_1D, is_training):\r\n        x = tf.reshape(x_1D, [1, 9])\r\n        with tf.variable_scope('Test', reuse=tf.AUTO_REUSE):\r\n            dense_out = tf.layers.dense(x, 3, name='dense')\r\n            bn_out = tf.layers.batch_normalization(dense_out, training=is_training, name='bn')\r\n        return tf.reshape(bn_out, [3])\r\n    \r\n    def computeTensorJacobian(X, is_training):\r\n        fn_compute_jacobian_1datapt = lambda x: tf_jacobian(computeFunc1D(x, is_training), x)\r\n        J = tf.map_fn(fn_compute_jacobian_1datapt, X)\r\n        return J\r\n    \r\n    def computeTensorJacobianOnTraining(): return computeTensorJacobian(x, is_training=True)\r\n    def computeTensorJacobianOffTraining(): return computeTensorJacobian(x, is_training=False)\r\n    my_tf_jacobian = tf.cond(tf_is_training_ph, \r\n                             computeTensorJacobianOnTraining, \r\n                             computeTensorJacobianOffTraining)\r\n\r\nwith tf.Session(graph=my_graph) as sess:\r\n    tf.global_variables_initializer().run()\r\n    \r\n    x_ = 0.5 * np.random.random((2,9))\r\n    print \"x_ = \", x_\r\n    \r\n    [my_tf_jacobian_val\r\n     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: True})\r\n    \r\n    print \"my_tf_jacobian_val = \", my_tf_jacobian_val\r\n    \r\n    [my_tf_jacobian_val\r\n     ] = sess.run([my_tf_jacobian], feed_dict={x: x_, tf_is_training_ph: False})\r\n    \r\n    print \"my_tf_jacobian_val = \", my_tf_jacobian_val\r\n```\r\n\r\nBut then I got another issue:\r\n```\r\n1.12.0\r\nx_ =  [[0.24 0.49 0.47 0.19 0.33 0.03 0.22 0.03 0.39]\r\n [0.28 0.24 0.33 0.34 0.34 0.24 0.47 0.09 0.13]]\r\nmy_tf_jacobian_val =  [[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n\r\n [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\r\nmy_tf_jacobian_val =  [[[-0.59 -0.13  0.57 -0.39 -0.54 -0.22 -0.32  0.05 -0.34]\r\n  [ 0.62  0.16 -0.14 -0.18 -0.19 -0.66  0.51  0.19 -0.09]\r\n  [-0.25 -0.23 -0.39 -0.63 -0.14  0.01 -0.5   0.22  0.61]]\r\n\r\n [[-0.59 -0.13  0.57 -0.39 -0.54 -0.22 -0.32  0.05 -0.34]\r\n  [ 0.62  0.16 -0.14 -0.18 -0.19 -0.66  0.51  0.19 -0.09]\r\n  [-0.25 -0.23 -0.39 -0.63 -0.14  0.01 -0.5   0.22  0.61]]]\r\n```\r\ni.e. the computed Jacobian is always 0 during training phase when using batch normalization layer.\r\nObviously I also want to optimize parameters in the batch norm layer (my real case is I have a loss which is a function of the Jacobian of a neural net with batch norm layer).\r\nAny clue or ideas?\r\n\r\nThanks so much for your time!", "Hi, just checking: is there any update on this? How long usually this will get resolved? I am working on a paper deadline around the end of next week that depends on this, and I would deeply appreciate if this issue could be fixed soon...", "Has this been resolved?", "@agarwal-ashish ptal.", "Looks like you are applying batch normalization on input of size 1 with training=True. So the output of that layer will be normalized to a constant value which means the jacobian should be 0. You can print the output of the batch normalization layer to verify.\r\n\r\nAlso looks like you are doing a map_fn with jacobian computation. You could have used batch_jacobian instead which will be more efficient. However in this case, since each output depends on each input due to batch normalization, so you may actually may need the jacobian of the full output wrt the full input.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25469\">No</a>\n", "@agarwal-ashish Thank you for response. But I still fail to understand, what is the issue here. I followed all your fix suggestion, but I am getting similar problem. I believe the issue is with the tf.layers.batch_normalization() function.\r\n\r\nCan you please look at my code and tell me what's wrong.\r\n\r\n```\r\nfrom __future__ import print_function\r\nfrom __future__ import division\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\nfrom tensorflow.python.ops.parallel_for.gradients import batch_jacobian, jacobian\r\n\r\ndef CNN(input, is_training=True, output_channels=3):\r\n    with tf.variable_scope('block1'):\r\n        output = tf.layers.conv2d(input, 64, 3, padding='same', activation=tf.nn.relu)\r\n    with tf.variable_scope('block2'):\r\n        output = tf.layers.conv2d(output, 64, 3, padding='same', name='conv2', use_bias=False)\r\n        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training, name='bn2'))\r\n        #output = tf.nn.relu(output)\r\n    with tf.variable_scope('block3'):\r\n        output = tf.layers.conv2d(output, output_channels, 3, padding='same')\r\n    return input-output\r\n\r\ndef main(_):\r\n    X = tf.placeholder(tf.float32, [None, None, None, 3]) # shape = [batch_size, W, H, 3]\r\n\r\n    is_training = tf.placeholder(tf.bool, name='is_training') #for batchnorm\r\n\r\n    with tf.variable_scope('my_CNN'):\r\n        output = CNN(X, is_training=is_training)\r\n\r\n    scalar = tf.reduce_sum(output, axis=[1,2,3]) # shape becomes [batch_size]\r\n    loss = tf.reduce_sum(batch_jacobian(scalar, X))\r\n\r\n    sess = tf.Session()\r\n\r\n    #optimizer\r\n    optimizer = tf.train.AdamOptimizer(0.001, name='AdamOptimizer')\r\n\r\n    #for batchnorm\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n        train_op = optimizer.minimize(loss)\r\n\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    sess.run(train_op, feed_dict={X: np.random.normal(size = (100, 28, 28, 3)), is_training: True})\r\n\r\n    my_loss = sess.run(loss, feed_dict={X: np.random.normal(size = (100, 28, 28, 3)), is_training: False})\r\n\r\n    print(\"Loss: \", my_loss)\r\n\r\n\r\nif __name__ == '__main__':\r\n    os.environ['CUDA_VISIBLE_DEVICES']='1'\r\n    tf.app.run()\r\n\r\n```\r\nThis results in the following error:\r\n\r\n```\r\n  File \"github_example.py\", line 53, in <module>\r\n    tf.app.run()\r\n  File \"/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"github_example.py\", line 29, in main\r\n    loss = tf.reduce_sum(batch_jacobian(scalar, X))\r\n  File \"/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/gradients.py\", line 118, in batch_jacobian\r\n    pfor_output = control_flow_ops.pfor(loop_fn, output_row_size)\r\n  File \"/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\", line 122, in pfor\r\n    outputs.append(converter.convert(loop_fn_output))\r\n  File \"/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1075, in convert\r\n    output = self._convert_helper(y)\r\n  File \"/home/shaka/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/parallel_for/pfor.py\", line 1221, in _convert_helper\r\n    \"which may run slower\" % (y_op.type, y_op, converted_inputs))\r\nValueError: No converter defined for Switch\r\nname: \"loop_body/gradients/my_CNN/block2/bn2/cond/Merge_grad/cond_grad\"\r\nop: \"Switch\"\r\ninput: \"loop_body/gradients/my_CNN/block2/Relu_grad/ReluGrad\"\r\ninput: \"my_CNN/block2/bn2/cond/pred_id\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@loop_body/gradients/my_CNN/block2/Relu_grad/ReluGrad\"\r\n    }\r\n  }\r\n}\r\n\r\ninputs: [WrappedTensor(t=<tf.Tensor 'loop_body/gradients/my_CNN/block2/Relu_grad/ReluGrad/pfor/ReluGrad:0' shape=(?, ?, ?, ?, 64) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'my_CNN/block2/bn2/cond/pred_id:0' shape=<unknown> dtype=bool>, is_stacked=False, is_sparse_stacked=False)]. \r\nEither add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n```", "The earlier suggestion was to address jacobian being 0 for the second version of the code. Regarding error about missing converter, this is happening because jacobian does not currently support converting tf.cond inside the vectorization logic. When is_training is set to True instead of a placeholder, that tf.cond will not be present and the code should work. Implementing tf.cond converter is a bit involved and may need some time. I'd suggest avoiding the placeholder for is_training if possible. ", "Note that tf.cond is now supported in vectorization. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25469\">No</a>\n"]}, {"number": 25468, "title": "ImportError: DLL load failed with error code -1073741795", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "(venv) C:\\>python\r\nPython 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD6\r\n4)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", l\r\nine 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\__init__\r\n.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Administrator\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "I hava encountered exactly the same problem!!!!!", "I use Windows 7(64 bit), choose tensorflow for CPU only,  follow the instructions on  https://tensorflow.google.cn/install/pip   step by step \r\nI create a virtual environment  and use  virtualenv install  ", "of course, when  I  run  the command:\r\npython -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\nit went wrong!!!\r\n", "The versions are as follows:\r\n\r\n(venv) C:\\>python3\r\n'python3' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n\r\n(venv) C:\\>python --version\r\nPython 3.6.7\r\n\r\n(venv) C:\\>pip3 --version\r\npip 19.0.3 from c:\\users\\administrator\\venv\\lib\\site-packages\\pip (python 3.6)\r\n\r\n(venv) C:\\>virtualenv --version\r\n16.4.3", "@SmirnovKol Could you post entire commands you used?\r\nHere are the commands that worked for me on Windows10\r\n\r\nCreating Virtual Environment on your system\r\n$ virtualenv -p python venv-tf\r\n$ source venv-tf/bin/activate\r\n$ pip install tensorflow-gpu\r\n$ python\r\n$ import tensorflow as tf\r\n$ print(tf.__version__)\r\n$ deactivate venv-tf\r\n\r\n\r\nPlease let me know how it progresses. Thanks!\r\n"]}, {"number": 25467, "title": "Add CropAndResize gradients for C++ image gradient operators", "body": "Add gradient and test for `CropAndResize`, and just refer to the implementation of `image_grad.py`.\r\n\r\nBut something still goes wrong, when I run the uint test, it will notice me that `Not found: No gradient defined for op: CropAndResize`.\r\n\r\nI hope I could get some reference in the thread #21019, but no response there, so I come up with the PR and hope someone could give me some reference.\r\n\r\nThanks.", "comments": ["You need to use the REGISTER_GRADIENT_OP macro. There are examples of it in image_grad.cc. It should be something like `REGISTER_GRADIENT_OP(\"CropAndResize\", CropAndResizeGradHelper);`", "@skye,  Thanks for your guidance, I have updated it now and pass the test. \r\n\r\nPlease take a look at it. Thanks.", "Looks good, except I think there are some minor formatting errors. Can you run `clang-format -style=google` over the files?", "@skye , I have formatted the code as you say, and thanks for your review.", "@skye, Thanks for your help. It seems some CI build  failed, when I see the details, some irrelevant test failed info occured.  Just like some info within `Ubuntu Python3`:\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/python/kernel_tests/linalg/BUILD:201:1: Couldn't build file tensorflow/python/kernel_tests/linalg/linear_operator_inversion_test/shard_1_of_5/test.log:  failed (Exit 34). Note: Remote connection/protocol failed with: execution failed Call failed with not retriable error: io.grpc.StatusRuntimeException: PERMISSION_DENIED: The caller does not have permission: \r\n```\r\n\r\nHow could I fix these?", "Let's try rerunning the tests..."]}, {"number": 25466, "title": "Add CropAndResize gradients for C++ image gradient operators", "body": "xxx", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 25465, "title": "Add CropAndResize gradient for C++ image gradient operators", "body": "Add gradient and test for `CropAndResize`, and just refer to the implementation of `image_grad.py`.\r\n\r\nBut something still goes wrong, when I run the uint test, it will notice me that `Not found: No gradient defined for op: CropAndResize`.\r\n\r\nI hope I could get some reference in the thread #21019, but no response there, so I come up with the PR and hope someone could give me some reference.\r\n\r\nThanks.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 25464, "title": "Unnecessary logging noise for non-authenticated GCS access", "body": "**System information**\r\n\r\n* Ubuntu 16.04\r\n* Python 3.6\r\n* TF 1.12\r\n\r\n**Describe the current behavior**\r\n\r\nUnnecessary access to GCP metadata endpoint, 10 retries, and lots of logging. \r\n\r\n**Describe the expected behavior**\r\n\r\nShould not be trying to access GCP metadata endpoint.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n$ python\r\n>> tf.gfile.Exists(\"gs://tfds-data\")  # this is a public bucket, doesn\u2019t need any auth\r\n2019-02-03 02:51:58.175696: I tensorflow/core/platform/cloud/retrying_utils.cc:73] The operation failed and will be automatically retried in 0.12376 seconds (attempt 1 out of 10), caused by: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\r\n\r\n...(more of the same)\r\n\r\n2019-02-03 02:52:05.330143: I tensorflow/core/platform/cloud/retrying_utils.cc:73] The operation failed and will be automatically retried in 1.29527 seconds (attempt 10 out of 10), caused by: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\r\n2019-02-03 02:52:06.626410: W tensorflow/core/platform/cloud/google_auth_provider.cc:157] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\".\r\n```\r\n\r\n(Linking issue tensorflow/datasets#38)", "comments": ["My fix in a252fe8 adds a hook to avoid the \"unnecessary access\" part of this bug; we still get noisier logs than I'd like. I'm going to repurpose this issue for \"make logs less chatty for expected retries\".", "@rsepassi Can you please confirm if the issue is being solved with the above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25464\">No</a>\n"]}, {"number": 25463, "title": "TensorFlow GCS access does not work from colab ", "body": "**System information**\r\nUsing colab.research.google.com\r\n\r\n**Describe the current behavior**\r\n\r\nHangs.\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.io.gfile.exists(\u201cgs://tfds-data\u201d)  # which is a public GCS bucket\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nShould not hang.", "comments": ["Linked tensorflow/datasets#36", "Yes, this one's a known bug: the problem is that it's trying an authenticated request, with some number of timeouts; IIRC, it'll eventually complete after ~10m. (!!!)\r\n\r\nQuick workaround: first, do\r\n\r\n    from google.colab import auth\r\n    auth.authenticate_user()\r\n\r\nand it'll work.\r\n\r\nThis was intended to be fixed upstream, but it looks like that didn't work (at least, it still hangs for me). I'll take a look.", "Thanks @craigcitro. Why is it trying to make an authenticated request? In the particular case I'm running into, it's a publicly accessible GCS bucket, so no auth necessary. Where is the \"authenticated request\" logic happening? I'm guessing it's a TF thing, not a colab thing, right? Should we update it to first check if credentials exist? (if so, make authenticated request, if not don't)\r\n\r\nIn my particular case, TFDS is storing some files on a public GCS bucket and is trying to load them. The user shouldn't know anything about the GCS bucket. Calling `auth.authenticate_user()` forces the user to go to an oauth link, copy a verification code, and paste it back in. This isn't something we want to force users to do. In the short term TFDS will probably use the GCS HTTP API instead of tf.io.gfile.", "I've added a `$NO_GCE_CHECK` environment variable that allows a user to completely sidestep the GCE metadata checks; anyone who's hitting the original issue (slow timeouts + retries attempting to fetch a public GCS resource) should be able to use this to get unblocked.\r\n\r\nFor Colab in particular, I'm going to add this patch and enable it in our runtimes (where GCE metadata is never available).", "Thanks!\n\nOn Wed, Feb 6, 2019 at 10:24 PM Craig Citro <notifications@github.com>\nwrote:\n\n> I've added a $NO_GCE_CHECK environment variable that allows a user to\n> completely sidestep the GCE metadata checks; anyone who's hitting the\n> original issue (slow timeouts + retries attempting to fetch a public GCS\n> resource) should be able to use this to get unblocked.\n>\n> For Colab in particular, I'm going to add this patch and enable it in our\n> runtimes (where GCE metadata is never available).\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25463#issuecomment-461301381>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABEGWxqzc1Y2eueoxJ6Cux6txS1tKe5Dks5vK8aIgaJpZM4af5x9>\n> .\n>\n"]}, {"number": 25462, "title": "tflite outputs don't match with tensorflow outputs for conv2d_transpose", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0 (using python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\")\r\nPython version: Python 3.5.4 |Anaconda custom (64-bit)| (default, Nov 20 2017, 18:44:38)\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): [GCC 7.2.0] on linux\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\ntflite outputs don't match with tensorflow outputs\r\n**Describe the expected behavior**\r\ntflite outputs is expected to match tensorflow outputs\r\n**Code to reproduce the issue**\r\n<pre>\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\ntf.random.set_random_seed(1234)\r\n\r\ndef trans_conv1d(x,\r\n                 num_filters,\r\n                 filter_length,\r\n                 stride):\r\n    batch_size, length, num_input_channels = x.get_shape().as_list()\r\n    x = tf.reshape(x, [batch_size, 1, length, num_input_channels])\r\n\r\n    weights = tf.get_variable('W', shape=(1, filter_length, num_filters, num_input_channels))\r\n    biases = tf.get_variable('b', shape=(num_filters,))\r\n\r\n    y = tf.nn.conv2d_transpose(\r\n        x,\r\n        filter=weights,\r\n        output_shape=(batch_size, 1, stride * length, num_filters),\r\n        strides=(1, 1, stride, 1),\r\n        padding='SAME',\r\n        data_format='NHWC',\r\n        name=\"cnn2d\")\r\n    y = tf.nn.bias_add(y, biases)\r\n    return y\r\n\r\nnum_filters = 4\r\nfilter_length = 40\r\nstride = 8\r\nx = tf.placeholder(dtype = tf.float32, shape = [1, 96, 2], name = \"input\")\r\ny = trans_conv1d(x, num_filters, filter_length, stride)\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ninput_data = np.array(np.random.rand(1, 96, 2), dtype=np.float32)\r\noutput_data_tf = sess.run(y, feed_dict={x:input_data})\r\nconverter = tf.contrib.lite.TFLiteConverter.from_session(sess, [x], [y])\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\nsess.close()\r\n\r\n# tflite test\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on the same input data.\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data_tflite = interpreter.get_tensor(output_details[0]['index'])\r\nprint(np.array_equal(output_data_tf, output_data_tflite))\r\n</pre>\r\n**Other info / logs**\r\nFalse", "comments": ["Just wondering if there is any update on this? Thanks", "@haozha111 Could you take a look since you're recently looking into TransposeConv?", "Sure.", "Patched your code and run with tf-nightly. Here is the result:\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\noutput tf: [[[[ 0.18149409  0.12595409  0.25195837 -0.7155486 ]\r\n   [ 0.15551439  0.15711296  0.42898354 -0.4680799 ]\r\n   [-0.09389246 -0.14556983  0.5282279  -0.2589781 ]\r\n   ...\r\n   [ 0.16891086 -0.02126111  0.17114447 -0.08986548]\r\n   [ 0.02455523  0.10942102  0.41191384 -0.33445707]\r\n   [ 0.07544918  0.02863858  0.36540815 -0.33818465]]]]\r\noutput tflite: [[[[ 0.18149409  0.12595409  0.25195837 -0.7155486 ]\r\n   [ 0.15551439  0.15711296  0.42898354 -0.46807992]\r\n   [-0.09389246 -0.14556983  0.5282279  -0.2589781 ]\r\n   ...\r\n   [ 0.16891086 -0.02126111  0.17114449 -0.08986548]\r\n   [ 0.02455523  0.10942102  0.41191384 -0.33445707]\r\n   [ 0.07544918  0.02863858  0.36540815 -0.33818465]]]]\r\nFalse\r\ndiff: [[[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  2.9802322e-08]\r\n   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n   ...\r\n   [ 0.0000000e+00  0.0000000e+00 -1.4901161e-08  0.0000000e+00]\r\n   [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n   [ 0.0000000e+00 -3.7252903e-09  0.0000000e+00  0.0000000e+00]]]]\r\n\r\nAs you can see, most of the output values agree, with only a few differences. However the difference is pretty small(1e-8).\r\nIf you try diff by: print(np.allclose(output_data_tf, output_data_tflite, atol=1e-8))\r\nThe result will agree. I will investigate more on this.", "Numerical errors are almost unavoidable in floating point calculation. \r\nIn many of our test cases, we test with 1e-6 threshold. \r\n1e-8 is pretty acceptable, so I'd say this is intended behavior. \r\n\r\nI'd suggest to always test float results with a threshold, not exact matching. ", "Agreed. I will close for now."]}, {"number": 25461, "title": "Feature: Update the environment capture script for system information.", "body": "The current [**environment capture script for TensorFlow**](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh) does not provide all of the system information required for filing bug or performance issues:\r\n\r\n- **OS Platform and Distribution** (e.g., Linux Ubuntu 16.04)\r\n- **Mobile device** (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device\r\n- TensorFlow installed from (**source or binary**)\r\n- **TensorFlow version** (use command below)\r\n- **Python** version\r\n- **Bazel** version (if compiling from source)\r\n- **GCC/Compiler** version (if compiling from source)\r\n- **CUDA/cuDNN** version\r\n- **GPU model** and memory\r\n\r\nThe purpose of this issue would be to modify the environment capture script to print as much of the information above as possible to an end user's console, or to create an extension of the TensorFlow Python API (similar to fastai's [`collect_env.py`](https://github.com/fastai/fastai/blob/110d29d7a407409399c77916b2453e7408a3a450/fastai/utils/collect_env.py) that accomplishes the same goal.", "comments": ["@dynamicwebpaige  is this appropriate for a beginner to try for the first contribution?\r\n\r\nI'm interested in this, hope I could get some advice and tackle this issue.", "@a6802739 - thanks for the offer to upgrade the environment capture script! \ud83d\ude0a\r\n\r\nAdding information for one or more of the bullet-points above (or even comments describing what the current upgrade script does, line by line!) would be a great first issue, and I'd be happy to chat about next steps. Please feel free to send me an email ([webpaige@google.com](mailto:webpaige@google.com)). ", "@dynamicwebpaige , Thank you very much, I will try to add one system information for the current `environment capture script` first.\r\n And I will ask for your help if I met some problems, hope it wouldn't bother you too much.\r\nThank you again.", "Hi @a6802739 - can I give you some help on this issue ?", "@scouvreur, sure, Thanks.\r\nHow about I add some information for the first few item, and then you help to finish the left?", "Sure ! I will work on the GCC/Compiler, CUDA/cuDNN versions and the GPU model and memory", "I made a pr for \r\n* TensorFlow installed from (**source or binary**)\r\n* **Python** version\r\n* **Bazel** version (if compiling from source)\r\nhttps://github.com/tensorflow/tensorflow/pull/25673\r\n\r\ntensorflow version seemed already part of output,\r\nwasn't sure how to confirm source or binary so I added location install info,\r\nif it's installed under python/site-packages it's a good sign tf was installed from binary\r\n\r\ndidn't add unit tests\r\npls let me know if I need to correct anything, first contrib too :p ", "Hi @dynamicwebpaige, I just submitted a pull request #25679 with the features I mentioned above", "Thanks so much for your contributions! We appreciate it - and including this information in the environment capture script will certainly help folks who are filing issues. \ud83d\ude4f \r\n\r\n(cc: @martinwicke @alextp)", "@dynamicwebpaige, I have submitted a pull request #25833, please have a look at that. Thanks!", "@dynamicwebpaige @a6802739 @scouvreur \r\nIs there anything left to do for this issue?", "Closing this issue since the PR has merged. Thanks!"]}, {"number": 25460, "title": "usage of Dataset.window causes TypeError when creating iterator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\n\r\nFollowing code is a snippet from [Dataset.window documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window)\r\n```\r\nimport tensorflow as tf\r\ndataset = tf.data.Dataset.range(7).window(2)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\n\r\nCalling `iterator.get_next()` throws following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/execute.py\", line 123, in make_type\r\n    v = dtypes.as_dtype(v).base_dtype\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py\", line 712, in as_dtype\r\n    raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" % type_value)\r\nTypeError: Cannot convert value <tensorflow.python.data.ops.dataset_ops._NestedDatasetComponent object at 0x7fdf882319b0> to a TensorFlow DType.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-29-624c3010a782>\", line 1, in <module>\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\nnext_element = iterator.get_next()\r\n```\r\nshould create iterator object.\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n\r\n", "comments": ["@tomzo I could reproduce the issue with `tensorflow==1.12.0` but not with `tf-nightly`. Looks like the issue has been fixed in the master branch. Could you try with `tf-nightly`?", "Tested in docker image `tensorflow/tensorflow:nightly`  \r\n```\r\nimport tensorflow as tf\r\ndataset = tf.data.Dataset.range(7).window(2)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n```\r\nDoes not throw errors anymore.\r\n\r\nThanks."]}, {"number": 25459, "title": "Converting a keras Sequential CNN to estimator and training it removes all layers, then crashes for want of layers", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n- Python version: 2.7.15rc1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: Not installed\r\n- GPU model and memory: GeForce GTX 1050 Ti, 4GB\r\n\r\n**Describe the current behavior**\r\n\r\nThis issue was also mentioned in #21778, but I believe this warrants an issue of its own.\r\n\r\nI ran into this problem trying tho make a CNN using the `fashion_mnist` dataset. Instead of training the model, invoking `train` on the model seems to drop all the layers of the original model, and then crash because there are no layers. \r\n\r\nThe example below has this output:\r\n\r\n````\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_l6v51cf\r\n[<tensorflow.python.keras.layers.core.Reshape object at 0x7fcb25244a90>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fcb07f670f0>, <tensorflow.python.keras.layers.core.Flatten object at 0x7fcb07f7fa20>, <tensorflow.python.keras.layers.core.Dense object at 0x7fcb07f7fb00>]\r\nWARNING:tensorflow:From /home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\n[]\r\n```` \r\n\r\n**Describe the expected behavior**\r\nI would expect `train` to train the model without emptying the `layers`-array. Also, the deprecation warning probably shouldn't be there either.\r\n\r\n**Code to reproduce the issue**\r\n````\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nnetwork = keras.Sequential([\r\n            keras.layers.Reshape((10, 10, 1)),\r\n            keras.layers.Conv2D(filters=8, kernel_size=(4, 4), activation=tf.nn.elu),\r\n            keras.layers.Flatten(),\r\n            keras.layers.Dense(1, activation=tf.nn.relu)\r\n        ])\r\n\r\nnetwork.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\r\n                loss='categorical_crossentropy',\r\n                metric='accuracy')\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=network)\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x=np.random.rand(10, 100),\r\n        y=np.random.rand(10, 1),\r\n        num_epochs=1,\r\n        shuffle=False)\r\n\r\nprint(network.layers)\r\n\r\ntry:\r\n    estimator.train(input_fn=train_input_fn, steps=10)\r\nexcept:\r\n    print(network.layers)\r\n\r\n````\r\n\r\n**Other info / logs**\r\nWithout the try/except, the error thrown is \r\n\r\n````\r\nTraceback (most recent call last):\r\n  File \"/home/heikki/Koodaus/python_sketches/Estimator.py\", line 26, in <module>\r\n    estimator.train(input_fn=train_input_fn, steps=10)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1237, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 278, in model_fn\r\n    labels)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 201, in _clone_and_build_model\r\n    optimizer_iterations=global_step)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/keras/models.py\", line 448, in clone_and_build_model\r\n    _in_place_subclassed_model_reset(clone)\r\n  File \"/home/heikki/Koodaus/python_sketches/venv/lib/python3.6/site-packages/tensorflow/python/keras/models.py\", line 318, in _in_place_subclassed_model_reset\r\n    name = layers_to_names[layer]\r\nKeyError: <tensorflow.python.keras.layers.core.Reshape object at 0x7fde28e5ca90>\r\n````\r\n", "comments": ["I was able to reproduce your error message in TF 1.12.0\r\nHowever using TF 1.13.rc0 gave more clear error message as below:\r\n```ERROR:tensorflow:Model diverged with loss = NaN.```\r\nBy solving this error message I was able to use ```train``` to train the model.", "How does one solve that error? I tried changing the loss function to `tf.losses.mean_squared_error`, but that didn't change the result on 1.12.0.", "The error arises due to the assignment of ```x, y``` args to ```tf.estimator.inputs.numpy_input_fn``` . If you rerun the code snippet as is multiple times it should pass at next try. I see that currently it is random assignment to x, y which leads to model diverging in some runs.", "I see. I tried to change the code to this:\r\n\r\n````\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n\r\ninput_height = test_images.shape[1]\r\ninput_width = test_images.shape[2]\r\nnetwork = keras.Sequential([\r\n    keras.layers.Reshape((input_height, input_width, 1)),\r\n    keras.layers.Conv2D(filters=8, kernel_size=(4, 4), activation=tf.nn.elu),\r\n            keras.layers.Flatten(),\r\n            keras.layers.Dense(10, activation=tf.nn.relu)\r\n        ])\r\n\r\nnetwork.compile(optimizer=tf.keras.optimizers.SGD(lr=0.00001, momentum=0.9),\r\n                loss=tf.losses.mean_squared_error,\r\n                metric='accuracy')\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=network)\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x=test_images,\r\n        y=test_labels,\r\n        num_epochs=1,\r\n        shuffle=False)\r\n\r\nprint(network.layers)\r\n\r\ntry:\r\n    estimator.train(input_fn=train_input_fn, steps=10)\r\nexcept:\r\n    print(network.layers)\r\n````\r\n\r\nBut the behavior repeats, including emptying the network's layers.", "Apologies for the delay in response. The new version of code snippet you provided throws a different error in TF 1.13.1 Can you please confirm? Thanks!", "The error I'm getting now is this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"Estimator.py\", line 31, in <module>\r\n    estimator.train(input_fn=train_input_fn, steps=10)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 278, in model_fn\r\n    labels)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 201, in _clone_and_build_model\r\n    optimizer_iterations=global_step)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow/python/keras/models.py\", line 511, in clone_and_build_model\r\n    target_tensors=target_tensors)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow/python/training/checkpointable/base.py\", line 442, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 449, in compile\r\n    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 647, in weighted\r\n    score_array = fn(y_true, y_pred)\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py\", line 669, in mean_squared_error\r\n    predictions.get_shape().assert_is_compatible_with(labels.get_shape())\r\n  File \"/home/heikki/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1023, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (?, 10) and (?,) are incompatible\r\n```\r\n\r\nThe layers, however, seem to be intact now, so I'm willing to call this fixed.", "Closing this issue since the original error has now been resolved. Feel free to post a new issue if have further problem. Thanks!"]}, {"number": 25458, "title": "TF1.13.0rc0 cuda10: ImportError: libcublas.so.9.0: cannot open shared object file", "body": "### System information\r\n- **OS Platform and Distribution:  Linux Ubuntu 16.04 \r\n- **TensorFlow installed from: Docker Tag 1.13.0rc0-gpu-py3\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: cuda 10.0\r\n- **GPU model and memory**: 2 * RTX 2080 TI and 2 * GTX 1080\r\n\r\n### Describe the problem\r\n\r\nWith version 1.13.0rc0 (for gpu, py3) from dockerhub an error occurs on:\r\n\r\n`import tensorflow as tf`\r\n\r\nWith version 1.13.0-dev20181228 (for gpu, py3) from dockerhub everything works well.\r\n\r\nEnvironment variables seem to be ok:\r\nLD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64 \r\n\r\n### Source code / logs\r\n---------------------------------------------------------------------------\r\n```\r\nImportError                               Traceback (most recent call last)\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n/usr/lib/python3.5/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n/usr/lib/python3.5/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-59571c70a10d> in <module>\r\n      8 get_ipython().run_line_magic('autoreload', '2')\r\n      9 \r\n---> 10 import tensorflow as tf\r\n     11 print(tf.__version__)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["@aselle this shouldn't try to load CUDA 9, right?", "I'm facing this same issue with the `latest-gpu-py3` tag on dockerhub.\r\n@Tassfighter which tag did you pulled to get a working version (which refers to `1.13.0-dev20181228`) ?", "@Walkoss I've pulled version 1.13.0-dev20181228 some days ago from a nightly build. So I assume it's not there anymore, sorry :(", "A bug (which has been fixed) in our Docker release CI led to the `latest-` tagged images containing CUDA 10 libraries but TF 1.12 (which needs CUDA 9). The images have been updated so that they use CUDA 10 and TF 1.13rc1 (which needs CUDA 10). Sorry about the confusion! This should be resolved now, but please comment again if not."]}, {"number": 25457, "title": "[XLA] Check shape has layout", "body": "`LayoutUtil::HasLayout` Ignores tokens etc, so need to check first that the shape has a layout before trying to access it.", "comments": ["Hi Justin, this is a problem we have recently encountered after adding infeeds. Calling MinorToMajor checks that the shape is a dense array which isn\u2019t the case for tokens etc. and that causes the program to terminate.\r\nI\u2019ve mainly made this PR because I\u2019m not sure what the best solution here is so I\u2019m open to suggestions!", "> Calling MinorToMajor checks that the shape is a dense array which isn\u2019t the case for tokens etc. and that causes the program to terminate.\r\n\r\nAah, I see.\r\n\r\nIt seems pretty weird that we return true for LayoutUtil::HasLayout but false shape.has_layout(), but that's not your problem. This change makes sense to me, but would like a test and a comment explaining it if possible.", "Hi Justin, resolved those - I was just following the style in that file (let me know if you want me to change for other tests as well)"]}]