[{"number": 11574, "title": "The same code for tensorboard test generate .mm file which should be .user file", "body": "I use my classmate's file for tensorboard test. But, this .py file generate .mm file not a .user file.\r\n\r\nCodes are as follows:\r\nimport tensorflow as tf\r\nimport math\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\ndef conv_layer(input,channels_in,channels_out,strides, name=\"conv\",):\r\n    with tf.name_scope(name):\r\n        w = tf.Variable(tf.truncated_normal([5,5,channels_in,channels_out],stddev=0.1),name=\"W\")\r\n        b = tf.Variable(tf.ones([channels_out])/10,name=\"B\")\r\n        conv = tf.nn.conv2d(input=input,filter=w,strides=[1,strides,strides,1],padding=\"SAME\")\r\n        # print conv\r\n        act = tf.nn.relu(conv + b)\r\n        tf.summary.histogram(\"weight\",w)\r\n        tf.summary.histogram(\"bias\",b)\r\n        tf.summary.histogram(\"activations\",act)\r\n        # print act\r\n        return act\r\n\r\ndef fc_layer(input,channels_in,channels_out, name=\"fc\"):\r\n    with tf.name_scope(name):\r\n        w = tf.Variable(tf.truncated_normal([channels_in,channels_out],stddev=0.1),name=\"W\")\r\n        b = tf.Variable(tf.ones([channels_out])/10,name=\"B\")\r\n        print input\r\n        act = tf.nn.relu(tf.matmul(input, w) + b)\r\n        # act = tf.matmul(input, w) + b\r\n        return act\r\n\r\ndef compatible_convolutional_noise_shape(Y):\r\n    noiseshape = tf.shape(Y)\r\n    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\r\n    return noiseshape\r\n\r\nx = tf.placeholder(dtype=tf.float32,shape=[None,784],name=\"x\")\r\ny = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"lables\")\r\n\r\n# learning rate\r\nlr = tf.placeholder(tf.float32)\r\n# dropout\r\npkeep = tf.placeholder(tf.float32)\r\npkeep_conv = tf.placeholder(tf.float32)\r\n# batch_normal\r\ntst = tf.placeholder(tf.bool)\r\niter = tf.placeholder(tf.float32)\r\n\r\n\r\nx_image = tf.reshape(x,shape=[-1,28,28,1])\r\ntf.summary.image(\"input\",x_image,6)\r\n\r\n# creat net\r\nconv1 = conv_layer(x_image,1,4,1,\"conv_1\")\r\n# conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))\r\n# pool1 = tf.nn.max_pool(value=conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\r\n\r\nconv2 = conv_layer(conv1,4,8,2,\"conv_2\")\r\n# conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))\r\n# pool2 = tf.nn.max_pool(value=conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\r\nconv3 = conv_layer(conv2,8,16,2,\"conv_3\")\r\n\r\nflattened = tf.reshape(conv3,[-1, 7 * 7 * 16])\r\nfc1 = fc_layer(flattened,7 * 7 * 16, 200,\"fc1\")\r\n# fc1_dr = tf.nn.dropout(fc1,pkeep)\r\n\r\nw5 = tf.Variable(tf.truncated_normal([200,10],stddev=0.1))\r\nb5 = tf.Variable(tf.ones([10])/10)\r\nlogits = tf.matmul(fc1,w5)+b5\r\n\r\nwith tf.name_scope(\"loss\"):\r\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))\r\n    tf.summary.scalar(\"loss\",cross_entropy)\r\nwith tf.name_scope(\"BP\"):\r\n    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\r\n\r\nwith tf.name_scope(\"accuracy\"):\r\n    correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\r\n    tf.summary.scalar(\"accuracy\",accuracy)\r\n\r\n# writer = tf.summary.FileWriter(\"~/workspace/TensorFlow_Test/1\")\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    merged_summary = tf.summary.merge_all()\r\n    writer = tf.summary.FileWriter(\"/tmp/mnist_demo/\")\r\n    writer.add_graph(sess.graph)\r\n\r\n    # learning rate decay\r\n    max_learning_rate = 0.02\r\n    min_learning_rate = 0.001\r\n    decay_speed = 2000\r\n    for i in range(2001):\r\n        batch = mnist.train.next_batch(100)\r\n\r\n        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-1/decay_speed)\r\n        if i % 5 == 0:\r\n            s = sess.run(merged_summary,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\r\n            writer.add_summary(s,i)\r\n\r\n        if i  % 100 == 0:\r\n            [train_accuracy] = sess.run([accuracy], feed_dict={x: mnist.test.images, y: mnist.test.labels,lr:learning_rate,pkeep:1.0,pkeep_conv:1.0})\r\n            print(\"step %d, test accuracy is %g\" %(i,train_accuracy))\r\n        sess.run(train_step,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\r\n\r\nThis file runs well on my classmate's machine.........", "comments": []}, {"number": 11573, "title": "Weight normalization for RNN Cells.", "body": "The current RNN implementation executes a user defined\r\nfunction (the call() method of subclasses of RNNCells) inside a\r\ntf.while() loop. Weight normalisation requires a one-time\r\n normalization of the transition matrices prior to\r\nentering the while loop. The following 2 edits have been made in\r\ntensorflow/python/ops to enable this functionality:\r\n\r\n - RNNCell now has a prepare() method. It does nothing as implemented\r\nin the base class\r\n - A call to cell.prepare() has been added just before entering\r\n _dynamic_rnn_loop()\r\n\r\nSubclasses of RNNCell may implement normalization in the cell's\r\nprepare() method. One implementation with BasicLSTMCell\r\nand associated tests have been added to contrib. Note that any\r\nwrappers to be used with a weight-normalized cell need to be\r\nappropriately subclassed, as illustrated with the\r\nPrepareableMultiRNNCell example in contrib.", "comments": ["Can one of the admins verify this patch?", "Hi @ebrevdo, the NLP team from Winton will be at the ACL conference next week. We will be happy to meet with someone from the tensorflow team if they are attending. We thought it might be more efficient seeing as this PR proposes an update in core tensorflow and might require further discussion/alternate implementation ideas.", "IIRC @ebrevdo just went on vacation for 2 weeks, and he's the best person to look at this change.  Is this urgent, or can it wait for him to get back?  Adding an (optional) API to something as core as the RNN interface probably deserves some careful thought, which I know you've already spent some time thinking about!", "The problem is one of optimizing ops out of the while_loop context. I have\na change I'm working on to move ops obviously constant w.r.t the while loop\nout of it automatically.  In the meantime you can use:\n\n```\nwith tf.control_dependencies(None):\n  Add ops that should be executed outside the loop (be very careful)\n```\n\nFor example to do the normalization.\n\nOn Jul 25, 2017 9:37 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> IIRC @ebrevdo <https://github.com/ebrevdo> just went on vacation for 2\n> weeks, and he's the best person to look at this change. Is this urgent, or\n> can it wait for him to get back? Adding an (optional) API to something as\n> core as the RNN interface probably deserves some careful thought, which I\n> know you've already spent some time thinking about!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11573#issuecomment-317945170>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5niCLzqFNNmgzqWYaQe-nYRtTxuks5sRsKigaJpZM4ObNVj>\n> .\n>\n", "@ebrevdo Thanks for pointing out the use of control_dependencies. I had a slightly different reading of semantics of the *None* argument in there - meaning the ops would just run normally as if tf.control_dependencies wasn't there at all. \r\nNot quite sure how the tf.while skips these ops during the loop. But it solved the problem!", "Hi @ebrevdo , Hope you've had a nice vacation when you see this.\r\n\r\nI've updated the PR with more commits (squashed) to include documentation and formatting, mostly following the pylint template and existing code in contrib. I should mention here that most of the code is adapted from `rnn_cell_impl.LSTMCell` with an additional `norm` argument, which if `True`, does the normalization, else returns the same outputs as `rnn_cell_impl.LSTMCell`. I've included this information in the docstrings.\r\n\r\nHopefully these changes are in the right direction. Looking forward to your comments.", "@ebrevdo could you please take another look?", "Back Thursday\n\nOn Aug 8, 2017 9:29 AM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> could you please take another look?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11573#issuecomment-321057363>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4OttaDpbt8m80sOyfX2FdipD542ks5sWLcUgaJpZM4ObNVj>\n> .\n>\n", "@tensorflow-jenkins  test this please", "Hi @ebrevdo , did you get a chance to look at the code? I triggered a Jenkins build earlier which seems to be okay.", "Hi @rmlarsen @ebrevdo , any news on this?", "@ebrevdo any chance to take a look?", "@ebrevdo please take a look", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Looks like rnn_cell_test is failing due to this PR:\r\nhttps://source.cloud.google.com/results/invocation/52c608cb-76cc-4f18-9c65-39e70a9740ce/targets/%2F%2Ftensorflow%2Fcontrib%2Frnn:rnn_cell_test?page=log\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/botexec/bazel-out/k8-py3-opt/bin/tensorflow/contrib/rnn/rnn_cell_test.runfiles/org_tensorflow/tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py\", line 40, in <module>\r\n    from tensorflow.python.ops.losses import losses_impl\r\nModuleNotFoundError: No module named 'tensorflow.python.ops.losses'\r\n```\r\n\r\nMaybe missing BUILD dependencies?", "Hi @gunan , what would be the preferred way to do this. Should I just add another commit to this PR? Or a new PR/review process will be required?\r\n\r\nThanks\r\nAshwini", "Please add a new commit to this PR. Looks like you will also need to rebase your changes on top of master, as you can see there are some merge conflicts now.", "Jenkins, test this please", "Jenkins, test this please", "Hi @gunan , I've added new commits fixing the merge breaks. \r\n\r\nThe python tests `rnn_cell_test.py` and `rnn_test.py` are running OK with a fresh pip install of tensorflow 1.4. It's a bit strange since the build on your side was failing on `tensorflow.python.ops.losses` which seems to be fine with the pip install version. Not sure why this difference.\r\n\r\nCould you may be try again? \r\n\r\nThanks\r\nAshwini", "@discoveredcheck thanks for the updates.\r\nThe tests that are run on the pull request build the pip package from scratch and run the tests on them. So the pip tests do not run on any prebuilt packages.\r\n\r\nJenkins, test this please.", "Jenkins, test this please", "Hi @gunan , I was able to reproduce the test failure locally with Bazel, it should be fixed after my last commit. I ran the Jenkins build last night but it still seems to be \"Waiting for status\". All rnn tests should now pass.\r\n\r\nCheers\r\nAshwini\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Thanks for running the build @gunan . Looks all good for rnn?", "Removed lines of commented out code. Have left in pylint and explanatory comments.", "Update your PR message, it's outdated.", "Thanks for your comments @ebrevdo . I've update the PR message and code accordingly. Squashed the commits as well.\r\n\r\nJenkins, test this please.", "@discoveredcheck quick ping on this.", "Hi @ebrevdo, I have pushed in commits covering your latest review comments (multiline lambdas, PR message and commented-out lines of code). Is there anything else that needs change?", "@discoveredcheck the change has been approved, but you need to pull rebase and push again. Then we'll run the tests.", "@discoveredcheck can you please resolve conflicts before we test again and merge the PR?", "@discoveredcheck is on vacation until the 8th. Does this need to be resolved before the 8th? If so, I can have a look.", "@rorywaite @caisq @drpngx I will have a look today. It shouldnt need much since a rebase was done with the last round of changes.", "Hi @caisq , I've made the necessary changes. RNN tests pass locally. Please let us know if any further work is required on this.\r\n\r\nJenkins, test this please.", "@tensorflow-jenkins test this please", "Hey, could you give us a heads-up a day or two before you plan to merge? We want to ensure that we're available to fix merge issues as they crop up.", "@rorywaite we are ready to merge this PR sometime today or tomorrow.", "@rorywaite ok to merge?", "@drpngx , yes, please go ahead with the merge."]}, {"number": 11572, "title": "How to install tensorflow c#", "body": "Hi,\r\nI am trying to build email spam filtering .I have already done when email has text only but the problem when deal with images\r\n\r\n\r\n\r\nAny help how to install tensorflow and deal with images.\u00a0\r\n\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11571, "title": "No Module Named '_pywrap_tensorflow_internal' (still without working solution)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\tYes, it is one general import command (see below).\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\tWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n\tFrom source (nightly build  tensorflow_gpu-1.2.1-cp35-cp35m-win_amd64.whl from 2017/07/13).\r\n- **TensorFlow version (use command below)**:\r\n\t1.2.1 (command does not work, it includes the failed command)\r\n- **Python version**: \r\n\t3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n\tNot compiled from source.\r\n- **CUDA/cuDNN version**:\r\n\tCUDA 8.0, cudnn64_5.dll (Windows file description is: NVIDIA CUDA CUDNN Library. Version 8.0.54)\r\n- **GPU model and memory**:\r\n\t GPU NVidia Geforce 1050\r\n- **Exact command to reproduce**:\r\n\timport tensorflow\r\n\r\n### Describe the problem\r\nImporting TensorFlow causes an error:\r\n```\r\n$ ipython\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             return importlib.import_module(mname)\r\n     19         except ImportError:\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 41   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     42   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 21     _pywrap_tensorflow_internal = swig_import_helper()\r\n     22     del swig_import_helper\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     21     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127\r\n\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22\r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48\r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50\r\n     51 # Protocol buffers\r\n\r\nc:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     50 for some common reasons and solutions.  Include the entire stack trace\r\n     51 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 52   raise ImportError(msg)\r\n     53\r\n     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\users\\steph\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n\r\n### Source code / logs\r\ncheck_tensorflow.py produces:\r\n\r\n> $ python check_tensorflow.py\r\n> ERROR: Failed to import the TensorFlow module.\r\n> \r\n> - Python version is 3.5.\r\n> \r\n> - TensorFlow is installed at: C:\\Users\\Steph\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\r\n> \r\n> - All required DLLs are present. Please open an issue on the\r\n>   TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\n\r\nThe issue was already reported to Stackoverflow:\r\n[Stackoverflow issue](https://stackoverflow.com/questions/45090440/no-module-named-pywrap-tensorflow-internal-still-without-working-solution/45091193#45091193)\r\n", "comments": ["I'm getting the same error, with a different environment (Tensorflow r1.2, on a MacBook with OSX 10.12.5 and no GPU support). The installation works, but I'm getting errors, when I try to import TF in python.\r\n\r\n    Python 2.7.10 (default, Feb  7 2017, 00:08:15)\r\n    [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n    >>> import tensorflow as tf\r\n      Traceback (most recent call last):\r\n      File \"<stdin>\", line 1, in <module>\r\n      File \"tensorflow/__init__.py\", line 24, in <module>\r\n        from tensorflow.python import *\r\n      File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n        from tensorflow.python import pywrap_tensorflow\r\n      File \"tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n        raise ImportError(msg)\r\n      ImportError: Traceback (most recent call last):\r\n      File \"tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n        from tensorflow.python.pywrap_tensorflow_internal import *\r\n      ImportError: No module named pywrap_tensorflow_internal\r\n    \r\n    \r\n    Failed to load the native TensorFlow runtime.\r\n\r\nI'm working in a `virtualenv` (using `virtualenvwrapper`) with up-tp-date packages\r\n - virtualenv 15.1.0\r\n - python 2.7.10\r\n - pip 9.0.1\r\n - numpy 1.13.1\r\n - wheel 0.29.0\r\n - six 1.10.0\r\n\r\nI tried to install the default pip package using `pip install tensorflow` and tried to install tensorflow from source following the [installation tutorial][1], without CUDA support. Building and installing reported no errors, but in both cases I got the import error.\r\n\r\nRelated issues/questions (Github/Stackoverflow) and their answers pointed out problems with CUDA or missing Windows dll files, which do not seem to fit here.\r\n\r\n  [1]: https://www.tensorflow.org/install/install_sources", "@StephanH84 This is a common error that's usually related to missing DLLs and your system Path not being configured correctly. \r\nI see you are trying to install on Windows, @mrry has a great installation troubleshooter script that can help pinpoint issues available here: [mrry's Self-check Script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c). It will help pinpoint the missing DLLs.\r\n\r\nAlso in your issue filing you say that you've installed from source but then list a nightly binary and say you've _not_ compiled from source. Can I assume that you installed using a binary via pip? If so may I suggest uninstalling and installing straight from Pypi `pip install tensorflow`. Try the CPU version first in a separate environment. This can help troubleshooting to ensure it's nothing to do with CUDA! \r\n\r\nLet us know how you get on. Good luck!\r\n\r\n@pjaehrling It might be best that you file a Stack Overflow issue as your issue appears to be separate (although similar!) due to the different operating system, different version of tensorflow. I have no experience with the Mac version so hopefully someone else will come and help you!", "@mrry should we check in the script?", "@jubjamie: I already posted my problem on stackoverflow, unfortunately I got no response yet.\r\n\r\nhttps://stackoverflow.com/questions/45149129/tesorflow-on-osx-failed-to-load-the-native-tensorflow-runtime-no-module-named", "For me it was possibly solved by adding Visual Studio directories to Windows' path variable (_C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\14.0\\VC\\bin\\amd64, C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\14.0\\VC\\bin_) and then reinstalling tensorflow-gpu via pip (but here it took a version from cache):\r\n```\r\n2017-07-30 00:17:56.950667: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1050\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.34GiB\r\n2017-07-30 00:17:56.950921: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:961] DMA: 0 \r\n2017-07-30 00:17:56.951039: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   Y \r\n2017-07-30 00:17:56.951192: I c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\n\r\n```\r\n There were some issues at the start as comments:\r\n```\r\n2017-07-30 00:17:55.078289: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.078709: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.079111: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.079352: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.079600: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.079830: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.080102: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-30 00:17:55.080397: W c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n\r\n```\r\n\r\nApart from that at least for my issue \"this issue\" can be closed.", "Those are just warnings to do with possible ways to improve performance if compiling from source. They are normal and can be ignored. If you're happy the issue is solved you can close it yourself! ", "@jubjamie should I open another issue? I couldn't find a way to make it work nor got a response to my Stack Overflow issue so far. I also tried to install tensorflow outside of the virtualenv and tried to build a pip wheel from source for the latest release version 1.3, but got the same error.\r\n\r\nWhen I build the `.whl` file, I get warnings, which may seem to point to the problem?\r\n\r\n```\r\n\u25b6 bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nWed Aug 2 17:05:01 CEST 2017 : === Using tmpdir: /var/folders\r\n...\r\nWed Aug 2 17:05:07 CEST 2017 : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/external'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\n```", "You're best off opening a separate issue to @StephanH84 if you have a different problem. In the new issue be sure to fill out the full template and also link and Stack Overflow questions you're already filed.", "I just wrestled with this for a couple hours (Windows 10, Anaconda 4.4.0 64 bit). I had been trying to install tensorflow-gpu using pip with no success. I followed all the guides I could find, tried using Python 3.5.3, installed cudnn 5.1 and 6.0, tried different combinations using conda environments, and still ran into the same error.\r\n\r\nI found that installing via conda rather than pip worked the first time. Conda installed cudatoolkit, cudnn, libprotobuf, and protobuf and updated itself and vs2015_runtime.\r\n\r\nApparently conda is better at putting all this stuff in the right place than I am. Praise conda.", "Had the same problem. Resolved using CUDA 9.0 and tf-nightly-gpu as suggested above. I have put up instructions here: https://github.com/rohit-patel/Install_Instructions-Win10-Deeplearning-Keras-Tensorflow", "I meet this issue too. I installed anaconda on my Windows10 system. I tried to install TF through conda channel, it was OK at that time, however, the version of TF was 1.2.1, which I thought it was too old, besides, the instructions on tensorflow.org said TF installed via conda channel is not supported officially. So I switched to the official installation via the command: pip install tensorflow, however, the issue raised up. \r\nI am just wondering why I still encounter this issue when followed the instructions to install TF.", "Windows 10, conda env,\r\nresolved using wheel file guided here: https://www.tensorflow.org/install/pip\r\n`pip install https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl`"]}, {"number": 11570, "title": "LSTM with Projection issue", "body": "I wanna build a multi-layer LSTM CTC network with projection in LSTM.\r\n\r\n        cell = tf.nn.rnn_cell.LSTMCell(FLAGS.n_hidden, \r\n                                       initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), \r\n                                       num_proj = FLAGS.n_hidden/2, \r\n                                       state_is_tuple=True)\r\n        init_stat = cell.zero_state(FLAGS.batch_size, dtype=tf.float32)\r\n        _inputs = output_conv\r\n        for layer in range(FLAGS.recur_layer):\r\n            with tf.name_scope('LSTM_{}'.format(layer+1)) as scope: \r\n                outputs, _ = tf.nn.dynamic_rnn(cell, _inputs, seq_len, initial_state=init_stat, dtype=tf.float32)\r\n                _inputs = tf.nn.relu(outputs)\r\n                tf.summary.histogram('LSTM_{}'.format(layer+1), _inputs)\r\n                _inputs = tf.contrib.layers.batch_norm(_inputs, center=True, scale=True, is_training=is_training, updates_collections=None)\r\n                tf.summary.histogram('LSTM_{}_bn'.format(layer+1), _inputs)\r\n        outputs = _inputs\r\n\r\nthe _inputs size is 1024, FLAGS.n_hidden=1024,\r\nbut when I run it, there is an error:\r\n\r\n outputs, _ = tf.nn.dynamic_rnn(cell, _inputs, seq_len, initial_state=init_stat, dtype=tf.float32)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 574, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 737, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2770, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2599, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2549, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 720, in _time_step\r\n    skip_conditionals=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 206, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 708, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 542, in call\r\n    lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1017, in _linear\r\n    initializer=kernel_initializer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 669, in _get_single_variable\r\n    found_var.get_shape()))\r\nValueError: Trying to share variable rnn/lstm_cell/kernel, but specified shape (1024, 4096) and found shape (1536, 4096).\r\n\r\nHow can I fix it?  Using the num_proj, is there any example of it?\r\n\r\nMany thanks\r\nXin.q.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11569, "title": "Why use memory_layer in all cases?", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2\r\n\r\nIn current AttentionWrapper, both [BahdanauAttention](https://github.com/tensorflow/tensorflow/blob/c996c7b381a8eb54f9c7d7b298b24b1715645b68/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L416-L417) and [LuongAttention](https://github.com/tensorflow/tensorflow/blob/c996c7b381a8eb54f9c7d7b298b24b1715645b68/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L295-L296) enforce to use a memory_layer. According to my understanding, it is needed only when the depth of memory is not matched with that of query_layer. Is it intended to be in this manner?\r\n\r\n@ebrevdo , would you mind having a look at this?", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 11568, "title": "Add mark_flag_as_required functions to make the APIs compatible with \u2026", "body": "Add mark_flag_as_required functions to make the APIs compatible with python-gflags.(#11195)", "comments": ["Can one of the admins verify this patch?", "Also, @yilei in case he wants to double check anything here.", "Overall looks good to me, left one comment.", "@vrv @yilei Thanks for the reviews.", "@vrv @yilei I have changed the code, please review it if you have time.", "@tensorflow-jenkins test this please", "This looks good to me, since this is adding to the API I've invoked the API review committee.", "Thanks.", "There seems some errors occurred at flag test. @vrv could you help to have a look at it?", "The error is listed below, though I am not sure why it's happening, since you imported logging already.\r\n\r\n```\r\nINFO: From Testing //tensorflow/python:flags_test:\r\n==================== Test output for //tensorflow/python:flags_test:\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/python/flags_test.runfiles/org_tensorflow/tensorflow/python/platform/flags_test.py\", line 117, in <module>\r\n    flags.mark_flag_as_required('string_foo_required')\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/python/flags_test.runfiles/org_tensorflow/tensorflow/python/platform/flags.py\", line 173, in mark_flag_as_required\r\n    logging.warning(\r\nNameError: global name 'logging' is not defined\r\n```\r\n", "Tried one thing, let me test.\r\n\r\n@tensorflow-jenkins test this please", "That's wired.", "remove_undocumented was likely removing all public symbols, including 'logging', so on a later use, the symbol isn't found.  By hiding it as _logging, the symbol is not removed by remove_undocumented.", "I see, thanks for your works.", "LGTM", "@yilei @vrv Thanks.", "@vrv when this PR could be merged? Everything looks fine.", "This PR will be merged soon after the API review committee validates that this change is safe to go in, so hopefully worst case in a few days.  ", "Looks good for API change."]}, {"number": 11567, "title": "Add mark_flag_as_required functions to be compatible with python-gfla\u2026", "body": "Add mark_flag_as_required functions to make the APIs compatible with python-gflags.(#11195)", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->"]}, {"number": 11566, "title": "Fix typos", "body": "This PR fixes some typos: `tranposed`, `observaiton`, `the the`, `implementaton`, and `concurently`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11565, "title": "update:tf_inspect.py", "body": "DeprecationWarning: inspect.getargspec() is deprecated, use\ninspect.signature() or inspect.getfullargspec()\nif d.decorator_argspec is not None), _inspect.getargspec(target))", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution! We don't use inspect.signature because it isn't available in Python 2."]}, {"number": 11564, "title": "XLA bugs on training accuracy", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.2.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: NVIDIA TITAN Xp 12GB\r\n- **Exact command to reproduce**:\r\n\r\n(At the tensorflow/model/inception directory)\r\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=/tmp/imagenet_train --data_dir=/tmp/imagenet_data\r\nbazel-bin/inception/imagenet_eval --checkpoint_dir=/tmp/imagenet_train --eval_dir=/tmp/imagenet_eval\r\n\r\n== cat /etc/issue ===============================================\r\nLinux Ares 4.8.0-58-generic #63~16.04.1-Ubuntu SMP Mon Jun 26 18:08:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux Ares 4.8.0-58-generic #63~16.04.1-Ubuntu SMP Mon Jun 26 18:08:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\ntensorflow-tensorboard (0.1.2)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.1-2-gc996c7b\r\ntf.COMPILER_VERSION = v1.2.1-2-gc996c7b\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Jul 18 09:26:11 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 381.09                 Driver Version: 381.09                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN Xp            Off  | 0000:01:00.0     Off |                  N/A |\r\n| 48%   75C    P2   287W / 250W |  11771MiB / 12189MiB |     54%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1005    G   /usr/lib/xorg/Xorg                              18MiB |\r\n|    0     30938    C   /usr/bin/python                              11737MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n\r\n\r\n### Describe the problem\r\nI activated XLA and trained the Inception model.\r\nWhen I verify the training results, the accuracy is always 0.001.\r\n(It is considered that the training is not performed normally.)\r\n\r\nWhen XLA is disabled, normal accuracy is achieved.\r\n\r\n### Source code / logs\r\nI added some codes of model/inception/inception/inception_train.py to enable XLA.\r\nI attached the file.\r\n\r\n[inception_train.zip](https://github.com/tensorflow/tensorflow/files/1154355/inception_train.zip)\r\n\r\n\r\n", "comments": ["Can you reproduce this on a simpler example than full inception training? I.e. does a simple example work for you with XLA?", "I ran the simple example. (tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py)\r\n\r\nThis example seems to work correctly even with XLA.\r\n\r\nTo ensure that xla optimization is enabled, the --xla_generate_hlo_graph option is enabled.\r\n\r\n<Without XLA>\r\n\r\n```\r\nnurlonn@Ares:~/workspace/mnist$ ls\r\nmnist_softmax_xla.py\r\nnurlonn@Ares:~/workspace/mnist$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py --xla=''\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-07-26 13:04:01.942110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-07-26 13:04:01.942576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: TITAN Xp\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.72GiB\r\n2017-07-26 13:04:01.942595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-07-26 13:04:01.942601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-07-26 13:04:01.942609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0)\r\n2017-07-26 13:04:01.989717: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-07-26 13:04:01.989757: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices\r\n2017-07-26 13:04:01.990281: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x3f5d970 executing computations on platform Host. Devices:\r\n2017-07-26 13:04:01.990308: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-07-26 13:04:01.990549: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-07-26 13:04:01.990571: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices\r\n2017-07-26 13:04:01.990868: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x3f6bd50 executing computations on platform CUDA. Devices:\r\n2017-07-26 13:04:01.990884: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2017-07-26 13:04:03.225400: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n0.9192\r\n```\r\n\r\n\r\n<With XLA>\r\n\r\n```\r\nnurlonn@Ares:~/workspace/mnist$ TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-07-26 13:10:04.847645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-07-26 13:10:04.848130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: TITAN Xp\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.72GiB\r\n2017-07-26 13:10:04.848148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-07-26 13:10:04.848154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-07-26 13:10:04.848163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0)\r\n2017-07-26 13:10:04.895322: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-07-26 13:10:04.895369: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices\r\n2017-07-26 13:10:04.895798: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x55020d0 executing computations on platform Host. Devices:\r\n2017-07-26 13:10:04.895819: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-07-26 13:10:04.895980: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-07-26 13:10:04.895992: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 4 visible devices\r\n2017-07-26 13:10:04.896399: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x55384d0 executing computations on platform CUDA. Devices:\r\n2017-07-26 13:10:04.896415: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2017-07-26 13:10:05.202342: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [optimization: pipeline start, before simplification]: /tmp/hlo_graph_0.DUlZcS.dot\r\n2017-07-26 13:10:05.202706: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [simplification: pipeline start, before algsimp]: /tmp/hlo_graph_1.7hTRMk.dot\r\n2017-07-26 13:10:05.203085: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [simplification: after algsimp, before reshape-motion]: /tmp/hlo_graph_2.46FPmN.dot\r\n2017-07-26 13:10:05.203403: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v82 [simplification: after reshape-motion, before constant_folding]: /tmp/hlo_graph_3.jbcSWf.dot\r\n\r\n...\r\n<omitted>\r\n...\r\n\r\n2017-07-26 13:10:06.055764: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_1[_XlaCompiledKernel=true,_XlaNumConstantArgs=0,_XlaNumResourceArgs=0].v11 [GPU-ir-emit-prepare: after dce, before flatten-call-graph]: /tmp/hlo_graph_91.569aPU.dot\r\n2017-07-26 13:10:06.055924: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:573] computation cluster_1[_XlaCompiledKernel=true,_XlaNumConstantArgs=0,_XlaNumResourceArgs=0].v11 [GPU-ir-emit-prepare: after flatten-call-graph, pipeline end]: /tmp/hlo_graph_92.jcgozq.dot\r\n0.9212\r\n```\r\n", "FYI, I attach execution logs of mnist and inception with \"--tf_xla_parallel_checking=true --parallel_check_failfast=false\".\r\n\r\nMNIST examples shows correct results as I mentioned before.\r\n\r\nBut inception shows that optimized subgraphs generate different output from original subgraphs.\r\n\r\nCheck attached log for more information.\r\n[inception_parallel_check_logs.txt](https://github.com/tensorflow/tensorflow/files/1175763/inception_parallel_check_logs.txt)\r\n[mnist_parallel_check_logs.txt](https://github.com/tensorflow/tensorflow/files/1175762/mnist_parallel_check_logs.txt)\r\n\r\n", "@tatatodd, are you aware if anyone has successfully used the models/inception training on imagenet with xla? I don't really know how to start debugging this.\r\n", "I tried adjusting some HLO optimization options to find the problems.\r\n\r\nI found that reshape-motion in HLO optimizer optimization is a problem.\r\n(tensorflow/compiler/xla/service/reshape_mover.cc)\r\n(I cannot be sure this optimization is always a problem. In some cases this seems to work well.)\r\n\r\nI used the parallel check option to compare the results of turning the reshape-motion optimization option on and off.\r\n\r\nDefault - All HLO optimization is enabled\r\nCommand\r\n`TF_XLA_FLAGS=\"--tf_xla_parallel_checking=true --parallel_check_failfast=false\" bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=${TRAIN_DIR} --data_dir=${DATA_DIR} --max_steps=40040` \r\n\r\nThe results are recorded in the attached file.\r\n[inception_enable_all.txt](https://github.com/tensorflow/tensorflow/files/1196029/inception_enable_all.txt)\r\n\r\nTurn off reshape-motion optimization\r\nCommand\r\n`TF_XLA_FLAGS=\"--tf_xla_parallel_checking=true --parallel_check_failfast=false --xla_disable_hlo_passes=reshape-motion\" bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=${TRAIN_DIR} --data_dir=${DATA_DIR} --max_steps=40040` \r\n\r\nThe results are also recorded in the attached file.\r\n[inception_disable_reshape.txt](https://github.com/tensorflow/tensorflow/files/1196028/inception_disable_reshape.txt)\r\n\r\nYou can see that the result is much closer to the original value.\r\n\r\nI checked the inception for 1epoch using the imagenet input, and confirmed that normal accuracy was obtained.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Ping @tatatodd.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@jlebar knows more about XLA GPU related issues, and might have some thoughts here.", "If this still reproduces, this is quite bad.  @hpucha, do you have cycles to take a look? ", "Will take a look.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@hpucha any luck?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, I know this has sat for way too long; I'm sorry about that.\r\n\r\nI just tried this again, and it seems to work.  I'm using the latest version of inception from tensorflow/models, and I hacked it like you did to allow enabling XLA.  (I'll try to push a change upstream to add this flag.)  I ran locally with a P100 and CUDA 8.\r\n\r\nWithout XLA:\r\n```step 2950, loss = 12.21 (122.6 examples/sec; 0.261 sec/batch)```\r\n\r\nWith XLA:\r\n```step 2950, loss = 12.51 (122.7 examples/sec; 0.261 sec/batch)```\r\n\r\nI think the 12.21 vs 12.51 is noise; I let the XLA one run a bit longer and by step 3360 the loss was down to 11.96.\r\n\r\nI'm going to close this because...it seems to be working?  But please don't hesitate to reopen this if it's not working for you.  We have more folks working on this, so I think / hope we'll be able to have a turnaround time significantly shorter than 6 months.  :)\r\n\r\nThanks again for your patience, and sorry this dragged on so long.", "Awesome, thanks for checking!", "Sorry, forgot to mention, I had to patch in our fix for #17090, which should be upstream soon.\r\n\r\nJust to be sure I'm going to leave this XLA one going overnight.", "The XLA run got down to\r\n```step 125000, loss = 7.53 (113.8 examples/sec; 0.281 sec/batch)```\r\nbefore my hard drive ran out of space for storing checkpoints.  :)"]}, {"number": 11563, "title": "[DO NOT MERGE] Remove RTLD_GLOBAL when importing pywrap_tensorflow", "body": "Just making a pull request to run tests.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Thanks Alex. Can I test this myself, or do I need someone with write access to say the magic words?\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Marking this as do not merge since it's testing only. Feel free to change the title if you intend to merge.", "Yes, do not merge is good for now. Thanks!\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Hi Asim and Jonathan,\r\n\r\nI'm adding you two as initial reviewers. I'm off next week, but will address comments when I get back. Once you're satisfied, I will add a few more people.", "As discussed, please move this to an internal change so there are fewer conflicts that the sync rotation has to resolve. Thanks!"]}, {"number": 11562, "title": "https://github.com/tensorflow/tensorflow/issues/9481", "body": "Descrition:\r\nFix the bug that Tensorflow on Windows running into issues when there's UTF8 encoded characters in the file path.\r\nSolution:\r\nSwitch to use WideChar API calls for Windows, like the CreateFileW,FindFirstFileW,FindNextFileW, LoadLibraryExW.\r\nTest:\r\nInstall Tensorflow on Windows with path have Chinese characers in it. No issue.\r\nRun command \"from tensorflow.contrib.rnn.python.ops.gru_ops import *\" No issue.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please!", "@tensorflow-jenkins test this please.", "@HectorSVC mind checking the test failures?", "looking", "@jhseu Could you run the test again? I passed these test locally. No clue how it relate to my changes.", "Jenkins, test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "found more place need to be changed", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please.", "@mrry, Is there something wrong with Jenkins?", "Jenkins seems to be working, but I think the code is broken for non-Windows platforms:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blame/dbb9040ca31e25d81482559130e68ca928b19a6c/tensorflow/core/platform/env.cc#L35\r\n\r\nThis line seems to be including Windows-specific FS code unconditionally into cross-platform code. You probably need to wrap that `#include` in an `#if defined(PLATFORM_WINDOWS)`/`#endif` pair. ", "@tensorflow-jenkins test this please.", "tf.gfile.FastGFile(r'F:\\train\\Rubus_pedatifolius_Gen\u00e9v\\4000.jpg', 'rb').read() failed...", "This correction has been made in which version of tensorflow ?"]}, {"number": 11561, "title": "Fix contrib learn testcases", "body": "Fix contrib learn regex in testcases for Windows build", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11560, "title": "Fixes 'window_size' may be used uninitialized", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 11559, "title": "Add the Constant operator class", "body": "Create a custom operator class to create constants in the Graph,\r\nand introduce the Operator marker annotation to identify\r\noperator classes.\r\n\r\nPlease see #7149 for the master tracking issue.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 11558, "title": "Add 'axis' option for 'tf.boolean_mask()'", "body": "This fix tries to address #9721 where it was not possible to pass an\r\n'axis' option for 'tf.boolean_mask()'.\r\n\r\nThis fix fixes #9721.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@josh11b Thanks for the review. The PR has been updated. Please take a look.", "@yongtang Any updates? Thanks!", "@jhseu @josh11b @langmore Sorry for missing the GitHub notification and late reply. I have rebased and updated PR with the docstrings addressed based on the comments.\r\n\r\nPlease take a look and let me know if there are any issues.", "@langmore @josh11b any additional thoughts?\r\n\r\nJenkins, test this please.", "@langmore @josh11b any additional thoughts?", "This is good for api review.", "Thanks @josh11b for the review. The API goldens have been updated so that `//tensorflow/tools/api/tests:api_compatibility_test` could be passed.", "Jenkins, test this please."]}, {"number": 11557, "title": "TF/Keras RNN initial_state with stateful = false", "body": "I'm trying to put different initial hidden state into recurrent layers. I saw many issues about initializing states using keras and most of the solutions were using layer.reset_state(state). But in my application, I have to set stateful = False and I cannot use reset_state. As it is written in the document, I tried initial_state argument in calling layers. \r\n\r\nUsing the code below, I compared the outputs and the resulting outputs were always the same. I tried different initializers and different initial states but the both outputs were the same.\r\n\r\n**When the stateful is False, is there other possible way to set initial hidden state???**\r\n\r\nfor size in dimension[1:]:\r\n    initial_ = tf.ones(shape=[batchnum, size])\r\n    recurrent_layer = GRU(units=size, return_sequences=True, kernel_initializer=one,\r\n                                 recurrent_initializer=one)\r\n    recurrent_layertest = GRU(units=size, return_sequences=True, kernel_initializer=one,\r\n                                 recurrent_initializer=one)\r\n\r\n    x = recurrent_layer(inputs=x)\r\n    xtest = recurrent_layertest(inputs=xtest, initial_state = initial_)\r\n\r\nIn my last issue https://github.com/tensorflow/tensorflow/issues/11553, there was a suggestion to put inputs as [state, initial state] and It didn't work saying GRU has no attribute 'states'. I think this happens because the stateful is False.", "comments": ["@fchollet any idea? Is that a documentation issue?", "> When the stateful is False, is there other possible way to set initial hidden state???\r\n\r\nYou can use `reset_states([state1, state2, ...])` to set states *by value* (e.g. by specifying numpy arrays). To specify *symbolic* states, use the `initial_state` argument when calling the layer instance on new inputs.", " @fchollet   I tried as you suggested, and it didn't work.\r\nThe code I used is as below. I tested several times with different inputs.\r\nI think the problem is that the function ignores initial_state argument in   **xtest = net.layers[idx+1](inputs = xtest, initial_state = initial_)**  I tried putting weird shaped initial states (like [1, 1000000]) and there was no error.\r\nCould you check this code please?? Please let me know if I'm using it wrong.\r\n\r\nThanks\r\n\r\n\r\n      self.input = tf.placeholder(dtype, shape=[None, None, dimension[0]],\r\n                                    name=fn.input_names[0])  # Batch, timelen, dim\r\n        testinput = tf.placeholder(dtype, shape=[None, None, dimension[0]],\r\n                                   name='testin')\r\n        # Network\r\n        input_layer = layers.InputLayer(input_tensor=self.input, dtype=dtype)\r\n        input_layertest = layers.InputLayer(input_tensor=testinput, dtype=dtype)\r\n\r\n        x = input_layer.output\r\n        xtest = input_layertest.output\r\n\r\n        num_rnn = 0\r\n\r\n        batchnum = tf.shape(self.input)[0]\r\n        for size in dimension[1:]:\r\n            recurrent_layer = layers.GRU(units=size, return_sequences=True, trainable = True)\r\n            x = recurrent_layer(inputs=x)\r\n            num_rnn += 1\r\n\r\n        output = x\r\n        net = models.Model(inputs=self.input, outputs=output)\r\n\r\n       # Initial_state \r\n\r\n        for idx,size in enumerate(dimension[1:]):\r\n            initial_ = tf.random_uniform(shape=[batchnum, size*100000])\r\n            xtest = net.layers[idx+1](inputs = xtest, initial_state = initial_)\r\n\r\n        outputtest = xtest\r\n        testnet = models.Model(inputs=testinput, outputs=outputtest)\r\n\r\n"]}, {"number": 11556, "title": "Correct learning rate in code snippet.", "body": "Correct the learning rate in code snippet, such that it reflects the expected outcome, ~92%, instead of ~90%.\r\n\r\nIf this discrepancy is intentional, feel free to close this request. It's such a small fix I figured I'd submit the request just in case the change was accidental. \r\n\r\nI would note that the change is fixed in the master brach, however the problem is currently reflected in the production docs, which is confusing to beginners, like myself. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the contribution! We're not currently updating 1.2 anymore.", "@jhseu Awesome, thanks for the heads up!"]}, {"number": 11555, "title": "[OpenCL] Fixes SYCL profiler tests (#141)", "body": "The profiler relies heavily on the canonical device being listed in the\r\nTFProf nodes, which is only set for those devices which return True from\r\nCountAsCPUTime, so we need this to return True for SYCL device nodes\r\ntoo. The check for whether the node will run on an Accelerator comes\r\nfrom IsPlacedOnAccelerator.", "comments": ["Can one of the admins verify this patch?", "@benoitsteiner LGTM", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (26/25 (104%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext1.iad.github.net ([192.30.252.192]:34841 helo=github-smtp2a-ext-cp1-prd.iad.github.net)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.89)\n\t(envelope-from <noreply@github.com>)\n\tid 1dcDY9-0004Po-FB\n\tfor mazecreator@mazecreator.com; Mon, 31 Jul 2017 11:28:39 -0500\nDate: Mon, 31 Jul 2017 09:38:43 -0700\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1501519123;\n\tbh=PJ4WudkansD53RTJVjJp8AhvppDisAfC7yFbGki2IkE=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=SoPmvK+tWpG0zqUF0x/5Vt8t3mJ9DVIwpVd6zVpaA7jEZUOalVWDNRebgGiq1YTZx\n\t YFmmKzVy91HZMXbbrjCLu/wpzXjnFEAseHXA+hiBuyr6ZwicaxMqI9UhdZ+3dWDaq0\n\t zcrVaxYJOk7hBvdDkFiFEE7apyoZE4TLmnEvp3sU=\nFrom: Xin Pan <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/11555/c319126189@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/11555@github.com>\nReferences: <tensorflow/tensorflow/pull/11555@github.com>\nSubject: Re: [tensorflow/tensorflow] [OpenCL] Fixes SYCL profiler tests (#141)\n (#11555)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_597f5d13d93cf_215b23f85c48adc3c32327a\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: panyx0718\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a05d67f12ac648ea821591454086395c81ff1b45d692cf0000000115971f1392a169ce0e82d2ac@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoN6Up8ihJ0woadu9MyHh29TgSg3uks5sTgMTgaJpZM4OaOky>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n\n----==_mimepart_597f5d13d93cf_215b23f85c48adc3c32327a\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@benoitsteiner LGTM\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/11555#issuecomment-319126189\n----==_mimepart_597f5d13d93cf_215b23f85c48adc3c32327a\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://github.com/benoitsteiner\" class=\"user-mention\">@benoitsteiner</a> LGTM</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/11555#issuecomment-319126189\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoNhX6QPsTCv23yZzjH5A5aUBU2hOks5sTgMTgaJpZM4OaOky\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoLSvUryPp-0DGkGH16HIQT5k0HcKks5sTgMTgaJpZM4OaOky.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/11555#issuecomment-319126189\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@panyx0718 in #11555: @benoitsteiner LGTM\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/11555#issuecomment-319126189\"}}}</script>\n----==_mimepart_597f5d13d93cf_215b23f85c48adc3c32327a--\n", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (28/25 (112%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o4.sgmail.github.com ([192.254.112.99]:44543)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1dcDbO-0006XV-Ll\n\tfor mazecreator@mazecreator.com; Mon, 31 Jul 2017 11:32:00 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=k5InsnD84d4TDbt3GQAr31vUNp8=; b=iPOE3ATHSAdmudxz\n\t+E7APPGq6v471I1vZ1RZEkEii5Hb/ZsThGN9NxVa8vlg0161xdIUAOCF4PZAJcA2\n\t9OP032Ydx+8f+Zr2dol1HtgQnDNOtqz8lc+5JDZmm+Len1aLWBEjdQV6fkh2Q7vT\n\tqXJ3hnCmi5bJdL+qfElO+dvkw5g=\nReceived: by filter0447p1mdw1.sendgrid.net with SMTP id filter0447p1mdw1-17529-597F5DDC-2E\n        2017-07-31 16:42:04.335997794 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0029p1mdw1.sendgrid.net (SG) with ESMTP id hJmCmLGHRGa-ylm02mpzqg\n\tfor <mazecreator@mazecreator.com>; Mon, 31 Jul 2017 16:42:04.283 +0000 (UTC)\nDate: Mon, 31 Jul 2017 16:42:04 +0000 (UTC)\nFrom: Xin Pan <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Greg Peatfield <mazecreator@mazecreator.com>, \n Comment <comment@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/11555/review/53273083@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/11555@github.com>\nReferences: <tensorflow/tensorflow/pull/11555@github.com>\nSubject: Re: [tensorflow/tensorflow] [OpenCL] Fixes SYCL profiler tests (#141)\n (#11555)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_597f5ddbf0da8_269e3fef6bff7c344212fc\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: panyx0718\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: comment\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0ab01b75ae3f0c060edb5c479d1cdb991a837d93692cf0000000115971fdb92a169ce0e82d2ac@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoDY09Da7RS1ilA9XikVaNe-Kd3Hvks5sTgPbgaJpZM4OaOky>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqxOAdefJfha7kEnDXLmQh/zc2OdFRj5WU2C0C\n iz4fZaYymnpbfA/2neSZv2RAyXQi81qDMFThwg1Qah2nKvnCOkaZch7v3WjM4bUJ+6gDrxGhYQH8l4\n XjC3D0EnjvnPEnOvycZABwdf/j+JyMxWFrhFg8/+CyKUHzv5Ap3ww7CP1R9ezZgTyPhhhir9mz4moN\n o=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_597f5ddbf0da8_269e3fef6bff7c344212fc\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\npanyx0718 approved this pull request.\n\n\n\n\n\n-- \nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/11555#pullrequestreview-53273083\n----==_mimepart_597f5ddbf0da8_269e3fef6bff7c344212fc\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><b>@panyx0718</b> approved this pull request.</p>\n\n\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you commented.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/11555#pullrequestreview-53273083\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoGPF3d9uQ_5hV9UeBP6c8NSt2Ih8ks5sTgPbgaJpZM4OaOky\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoEMxGUgiW-Jski4tmAfj9-Bvw8Ocks5sTgPbgaJpZM4OaOky.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/11555#pullrequestreview-53273083\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@panyx0718 approved #11555\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/11555#pullrequestreview-53273083\"}}}</script>\n----==_mimepart_597f5ddbf0da8_269e3fef6bff7c344212fc--\n", "@jhseu @benoitsteiner LGTM. But I'm not authorized to merge this pull request", "Jenkins, test this please."]}, {"number": 11554, "title": "Feature request: GPU support for tf.bincount", "body": "Hi. In my opinion, `tf.bincount` is a very useful and simple function to build weighted histograms like in the example below, but recently i learned that unfortunately does not have GPU support. It could be very useful to run it on GPU, since i can't figure any simple way to do a weighted histogram efficiently. Thanks.\r\n\r\n```\r\nsess = tf.Session()\r\n\r\nvalues = tf.random_uniform((1,50),10,20,dtype = tf.int32)\r\nweights = tf.random_uniform((1,50),0,1,dtype = tf.float32)\r\n\r\ncounts = tf.bincount(values, weights = weights)\r\n\r\nhistogram = sess.run(counts)\r\nprint(histogram)\r\n```\r\n", "comments": ["I don't believe anyone is actively working on this at this point, but it seems like a fine thing to pursue.\r\nContributions are welcome.\r\n\r\nFYI @ringw @ekelsen ", "This should be pretty easy to do now that CUB is part of the TF release, I'll look into it.", "Thank you. Anyone can imagine another way to build a weighted histogram with tf functions on GPU? Maybe with sparse matrices? I know that when you build a sparse matrix values with the same coordinates are summed.", "It isn't as easy as I thought to add the CUB version, the cub histogram doesn't support weights.\r\n\r\nFor now, it would be pretty easy to use unsorted_segment_sum to mimic a (possibly weighted) histogram.  data=weights (could be all ones) and segment_ids=keys.", "Thank you so much! Thanks to you i can complete my thesis :)\r\nI didn't know this function, only the sorted version. It returns exactly the same values, with the bonus that i can choose the number of bins with the third argument. It runs quite fast, albeit goes on CPU a bit slower than `bincount`. It could still be very interesting to know if, with `bincount` on GPU, i can improve performance of my code. Unfortunately i have to do weighted histograms with a big amount of data.", "How many bins?", "Around 10^4 bins for arrays around 2.5 x 10^6 elements. Here is a simplified example of my program.\r\n\r\n```\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\n#first i build a matrix of some x positions vs time datas in a sparse format\r\nmatrix = numpy.random.randint(2, size = 100).astype(float).reshape(10,10)\r\n\r\nx = numpy.nonzero(matrix)[0]\r\ntimes = numpy.nonzero(matrix)[1]\r\nweights = numpy.random.rand(x.size)\r\n\r\n#then i define an array of y positions\r\nnStepsY = 5\r\ny = numpy.arange(1,nStepsY+1)\r\n\r\n#this is only an example, in my work these arrays are the input\r\n#from a file or a preceding function\r\n\r\nnRows = nStepsY\r\nnColumns = 80\r\n\r\nx = tf.constant(x, dtype = tf.float32)\r\ntimes = tf.constant(times, dtype = tf.float32)\r\nweights = tf.constant(weights, dtype = tf.float32)\r\ny = tf.constant(y, dtype = tf.float32)\r\n\r\n\r\ndef itermatrix(ithStep):\r\n    yTimed = tf.multiply(y[ithStep],times)\r\n\r\n    positions = tf.round((x-yTimed)+50)\r\n    positions = tf.cast(positions, dtype=tf.int32)\r\n\r\n    values = tf.unsorted_segment_sum(weights, positions, nColumns)\r\n\r\n    return values\r\n\r\nimageMapped = tf.map_fn(itermatrix, tf.range(0,nRows), dtype=tf.float32)\r\n\r\nsess = tf.Session()\r\n\r\nimage = sess.run(imageMapped)\r\n```\r\n\r\n\r\nThe problem is that i have to do these histograms for around 100 y steps, building and storing `image` for thousands of input matrices. The `position` transformation could be vectorized, but the binning part of each row is independent so i used `tf.map_fn` to manage the single input matrix. Now that thanks to you the histogram problem is solved, I'm working on the big data problem.", "If the number of bins fit into the shared memory of the GPU (48kb / 4bytes ~ 12,000 bins max) then you might be able to write a faster version of unsorted_segment_sum that does atomics in shared memory and then to global memory.  I think right now everything goes straight to global memory.", "At the moment I'm not quite good both with GPU programming and Tensorflow to do this, but i hope i will improve my experience. However, i checked my data and I'm afraid the number of bins is 10^5 and not 10^4.", "Added a PR #13813 for the GPU kernel of bincount. Didn't use CUB as the weight issue mentioned. Please take a look."]}, {"number": 11553, "title": "initial_state in keras rnn", "body": " I'm trying to put different initial hidden state into recurrent layers. I saw many issues about initializing states using keras and most of the solutions were using **layer.reset_state(state)**. But in my application, I have to set stateful = False and I cannot use reset_state.\r\nAs it is written in the document, I tried **initial_state** argument in calling layers.\r\n\r\nUsing the code below, I compared the outputs and the resulting outputs were always the same.\r\nI tried different initializers and different initial states but the both outputs were the same.\r\n\r\n**When the stateful is False, is there other possible way to set initial hidden state???**\r\n\r\n        for size in dimension[1:]:\r\n            initial_ = tf.ones(shape=[batchnum, size])\r\n            recurrent_layer = GRU(units=size, return_sequences=True, kernel_initializer=one,\r\n                                         recurrent_initializer=one)\r\n            recurrent_layertest = GRU(units=size, return_sequences=True, kernel_initializer=one,\r\n                                         recurrent_initializer=one)\r\n\r\n            x = recurrent_layer(inputs=x)\r\n            xtest = recurrent_layertest(inputs=xtest, **initial_state = initial_**)", "comments": ["I think the call() function is ignoring the initial_state. ", "Yes, the first element in `input` is taken as the initial state in [`call`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/keras/python/keras/layers/recurrent.py#L327)."]}, {"number": 11552, "title": "DecodeCSV outputs an unexpectedly result", "body": "I recently feel confused while using `DecodeCSV` operator. The meaning of `record_defaults` is different from that described in the documentation.\r\n\r\nHere is the description in documentation:\r\n```\r\nrecord_defaults: One tensor per column of the input record, with either a\r\n  scalar default value for that column or empty if the column is required.\r\n```\r\n\r\nProblem 1 : The size of `record_defaults` limit the size of output\r\n```python\r\n>>> value=[\"a,b,c\"]\r\n>>> parts_val = parsing_ops.decode_csv(value, field_delim=\",\", record_defaults = [[\"\"]])\r\n>>> parts_val\r\n[<tf.Tensor 'DecodeCSV_10:0' shape=(1,) dtype=string>]\r\n```\r\n\r\nProblem 2 : If `record_defaults` contains empty element, the type of output is always float32. In the same time, it failed to check the existence of value.\r\n```python\r\n>>> value=[\"a,b,c\"]\r\n>>> parts_val = parsing_ops.decode_csv(value, field_delim=\",\", record_defaults = [[\"\"], [], []])\r\n>>> parts_val\r\n[<tf.Tensor 'DecodeCSV_13:0' shape=(1,) dtype=string>, <tf.Tensor 'DecodeCSV_13:1' shape=(1,) dtype=float32>, <tf.Tensor 'DecodeCSV_13:2' shape=(1,) dtype=float32>]\r\n```\r\n\r\nMaybe I understand it in a wrong way. Any comment is appreciated. Thanks.", "comments": ["@keveman Any idea? It doesn't look like there's bounds checking on the `records_default`, so that might be a problem.", "I think the documentation is correct, but any suggestions to improve it will be appreciated.\r\n\r\nRE: Problem 1: The documentation suggests that `record_defaults` must contain `One tensor per column of the input record`. So, it needs to match with the number of columns in the input. If it doesn't the op will fail. For example:\r\n\r\n```python\r\nvalue = tf.placeholder(tf.string)\r\nparsed = tf.decode_csv(value, record_defaults=[[\"\"]])\r\nrun = tf.Session().make_callable(parsed, [value])\r\n\r\nprint(run(\"a\")) # Correctly \"decodes\" this single column CSV into string tensor\r\n\r\nprint(run(\"a,b,c\")) # Will throw an exception saying  \"Expect 1 fields but have 2 in record 0\"\r\n```\r\n\r\nRE: Problem 2: An \"empty element\" means an empty tensor, i.e., a zero-element tensor with the expected type of the value of the column. Without any additional context information, TensorFlow interprets `[]` as a float32 tensor, just like say numpy does (well numpy typically uses float64):\r\n\r\n```python\r\ntf.constant([]).dtype # This will be tf.float32\r\nnp.array([]).dtype    # This will be np.float64\r\n```\r\n\r\nTo explicitly provide a zero-element tensor of the correct type, e.g., int32, you could use `tf.constant([], tf.int32)` (and similarly in numpy `np.array([], np.int32)`).\r\n\r\nSo, for your example, I'd do something like:\r\n\r\n```python\r\n>>> value = [\"a,b,c\"]\r\n>>> empty = tf.constant([], tf.string)  # Or np.array([], np.string) \r\n>>> parts_val = tf.decode_csv(value, field_delim=\",\", record_defaults = [[\"\"], empty])\r\n>>> parts_val\r\n[<tf.Tensor 'DecodeCSV:0' shape=(1,) dtype=string>,\r\n <tf.Tensor 'DecodeCSV:1' shape=(1,) dtype=string>]\r\n```\r\n\r\nHope that helps. Since this is not a bug, I'm going to close this out. \r\nIf you'd like to contribute suggestions to improve the documentation, we'd be more that happy to hear (or even better, feel free to send a PR editing the documentation [here](https://github.com/tensorflow/tensorflow/blob/e85d3df/tensorflow/core/ops/parsing_ops.cc#L338))\r\n\r\nHope that helps!", "@asimshankar Thank you for your explanation. \r\n\r\nActually, i don't know the number of columns in the input at the first time. What i hope is `record_defaults` do not has to match with the number of columns in the input.\r\n\r\nIf the size of `record_defaults` is greater than the number of columns, fill in output with the defaults.\r\n\r\nIf the size of `record_defaults` is less than the number of columns, throw exception to show that column is missing and required.\r\n\r\nPerhaps that is more reasonable, isn't it?  :)", "@uuleon : `tf.decode_csv` expects that the number of columns is known before being invoked, so `record_defaults` has to match exactly. I'm not sure of the genesis of this, but I suspect this is to adhere to [RFC 4180](https://tools.ietf.org/html/rfc4180) in its strict sense and discourage subtle bugs from invalid input data (e.g., differing number of columns per line or unexpectedly missing columns in the file - see Section 2 of the RFC)."]}, {"number": 11551, "title": "Fixes ArgMax test deprecation warnings", "body": "The recent commit cbe1ef0 marks the `dimension` argument of argmax and argmin as deprecated, which are used in the corresponding tests and so this brings up warnings when the tests are run. The tests should be using the `axis` argument instead.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 11550, "title": "Segmentation fault occured when I install tensorflow r1.2  with bazel", "body": "------------------------\r\nHi, when I builded tensorflow r1.2 I got following error:\r\n`tensorflow-1.2--mkl/tensorflow/contrib/framework/BUILD:108:1: Executing genrule //tensorflow/contrib/framework:gen_variable_ops_pygenrule failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 11.\r\n/bin/bash: line 1: 133735 Segmentation fault      (core dumped) bazel-out/host/bin/tensorflow/contrib/framework/gen_gen_variable_ops_py_wrappers_cc 0 > bazel-out/local-opt/genfiles/tensorflow/contrib/framework/python/ops/gen_variable_ops.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps`\r\n### System information\r\n- System: Red Hat 4.8.5\r\n- Python: 2.7.5\r\n- bazel: 0.5.2\r\n- Tensorflow: r1.2\r\n\r\n### Commands:\r\n```\r\nwget https://github.com/bazelbuild/bazel/releases/download/0.5.2/bazel-0.5.2-installer-linux-x86_64.sh\r\nchmod +x bazel-0.5.2-installer-linux-x86_64.sh\r\nsudo sh ./zel-0.5.2-installer-linux-x86_64.sh\r\n\r\ngit clone https://github.com/tensorflow/tensorflow/\r\ncd tensorflow\r\ngit checkout r1.2\r\n./configure\r\n \r\nPlease specify the location of python. [Default is /vir_tensorflow/bin/python]: \r\nFound possible Python library paths:\r\n  /vir_tensorflow/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/vir_tensorflow/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /vir_tensorflow/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] y\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] \r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] \r\nNo CUDA support will be enabled for TensorFlow\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\nbazel build --config=opt --config=mkl --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nAre there any mistakes ? ", "comments": ["Could you run with `--verbose_failures`. That flag (mentioned in the error message above) should help narrow down precisely which of the sub-commands initiated by bazel is failing. Something like:\r\n\r\n```\r\nbazel build --config=opt --config=mkl --copt=\"-DEIGEN_USE_VML\" --verbose_failures \\\r\n   //tensorflow/tools/pip_package:build_pip_package\r\n```", "@asimshankar I used the following command you said,\r\n`bazel build --config=opt --config=mkl --copt=\"-DEIGEN_USE_VML\" --verbose_failures \\\r\n   //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nand I got the following error messages,\r\n\r\n```\r\n/export/lumiao/tensorflow-1.2-mkl/tensorflow/cc/BUILD:326:1: Executing genrule //tensorflow/cc:math_ops_genrule failed: bash failed: error executing command \r\n  (cd /export/lumiao/home/lumiao/.cache/bazel/_bazel_lumiao/29603fe0fae46f041d7c5d3a0957b1d1/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/cc/ops/math_ops_gen_cc bazel-out/local-opt/genfiles/tensorflow/cc/ops/math_ops.h bazel-out/local-opt/genfiles/tensorflow/cc/ops/math_ops.cc tensorflow/cc/ops/op_gen_overrides.pbtxt 0'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 11.\r\n/bin/bash: line 1: 27671 Segmentation fault      (core dumped) bazel-out/host/bin/tensorflow/cc/ops/math_ops_gen_cc bazel-out/local-opt/genfiles/tensorflow/cc/ops/math_ops.h bazel-out/local-opt/genfiles/tensorflow/cc/ops/math_ops.cc tensorflow/cc/ops/op_gen_overrides.pbtxt 0\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "It appears that this is failing because the program to generate the header and source files for the math operations is failing. It's hard to say why this is happening, but it appears you're running an old Linux distribution (I believe RedHat 4.8 is from 2009), which may have various shared libraries (such as `libc`) or tools (`gcc` versions) that are not compatible with the source tree.\r\n\r\nPerhaps you can debug the failing command:\r\n\r\n```\r\nbazel-out/host/bin/tensorflow/cc/ops/math_ops_gen_cc \\\r\n  bazel-out/local-opt/genfiles/tensorflow/cc/ops/math_ops.h \\\r\n  bazel-out/local-opt/genfiles/tensorflow/cc/ops/math_ops.cc \\\r\n  tensorflow/cc/ops/op_gen_overrides.pbtxt 0\r\n```\r\n\r\nto figure out what is causing it to segfault. \r\n\r\nUnfortunately, we don't have the bandwidth to debug installation on the Linux distribution you're having trouble with. Officially, only Ubuntu is supported as per https://www.tensorflow.org/install/install_sources and we rely on community support for other distributions.", "@asimshankar  Thank you very much\uff0c I compiled Tensorflowr1.2 on Ubuntu16.04 successfully."]}, {"number": 11549, "title": "Running Model on tensorflow Distribution can't  save model for tensorflow serving", "body": "### System information\r\n- **OS Platform CentOS 7.1**:\r\n- **TensorFlow installed from binary)**:\r\n- **TensorFlow version 1.2.1**:\r\n- **Python version 2.7**: \r\n- **Bazel version 0.4.5**:\r\n\r\n### Describe the problem\r\nSituation One: I add saving model for tensorflow serving based on mnist model distribution version.\r\nwhen i run this model on the same machine and start one ps server and two workers. saving model can work well.\r\nSituation Two: but if the model runs on three different machines(eg: A, B, C). I start ps server on A machine, and B, C machine runs worker, there is something wrong on saving model code.\r\n\r\nSituation Three: then I try another situation, run ps server and one worker on A, another worker on B, the worker running on A machine saves model. it can work well again.\r\n\r\nSituation Four: and run ps server on A, the other worker on B, it can also work.\r\n\r\nI think it is an issue of tensorflow distribution on saving model using saved_model_builder.\r\n\r\n### Source code / logs\r\nmy mnist distribution code as bellow:\r\n```python\r\n#!/usr/bin/env python2.7\r\n\"\"\"Train and export a simple Softmax Regression TensorFlow model.\r\nThe model is from the TensorFlow \"MNIST For ML Beginner\" tutorial. This program\r\nsimply follows all its training instructions, and uses TensorFlow SavedModel to\r\nexport the trained model with proper signatures that can be loaded by standard\r\ntensorflow_model_server.\r\nUsage: mnist_export.py [--training_iteration=x] [--model_version=y] export_dir\r\n\"\"\"\r\n\r\nimport os\r\nimport sys\r\n\r\n# This is a placeholder for a Google-internal import.\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import variables\r\nfrom tensorflow.core.protobuf import saver_pb2\r\n\r\nfrom tensorflow.python.saved_model import builder as saved_model_builder\r\nfrom tensorflow.python.saved_model import signature_constants\r\nfrom tensorflow.python.saved_model import signature_def_utils\r\nfrom tensorflow.python.saved_model import tag_constants\r\nfrom tensorflow.python.saved_model import utils\r\nfrom tensorflow.python.util import compat\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nfrom six.moves import xrange\r\n\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\n\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 100, \"Index of task within the job\")\r\n\r\ntf.app.flags.DEFINE_integer('training_iteration', 2,\r\n                            'number of training iterations.')\r\ntf.app.flags.DEFINE_integer('model_version', 1, 'version number of the model.')\r\ntf.app.flags.DEFINE_string('work_dir', 'model/', 'Working directory.')\r\ntf.app.flags.DEFINE_string('train_dir', 'MNIST_data/', 'Working directory.')\r\n\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\ndef main(_):\r\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n\r\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n        train(server, cluster)\r\n\r\n\r\ndef train(server, cluster):\r\n    # Train model\r\n    print('Training model...')\r\n    with tf.device(\r\n            tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster)):\r\n        mnist = input_data.read_data_sets(FLAGS.train_dir, one_hot=True)\r\n        serialized_tf_example = tf.placeholder(tf.string, name='tf_example')\r\n        feature_configs = {'x': tf.FixedLenFeature(shape=[784], dtype=tf.float32), }\r\n        tf_example = tf.parse_example(serialized_tf_example, feature_configs)\r\n\r\n        x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name\r\n        y_ = tf.placeholder('float', shape=[None, 10])\r\n\r\n        w = tf.Variable(tf.zeros([784, 10]))\r\n        b = tf.Variable(tf.zeros([10]))\r\n\r\n        y = tf.nn.softmax(tf.matmul(x, w) + b, name='y')\r\n        cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\r\n\r\n        global_step = tf.Variable(0)\r\n        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy, global_step=global_step)\r\n        values, indices = tf.nn.top_k(y, 10)\r\n\r\n        prediction_classes = tf.contrib.lookup.index_to_string(\r\n            tf.to_int64(indices), mapping=tf.constant([str(i) for i in xrange(10)]))\r\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\r\n\r\n        summary_op = tf.summary.merge_all()\r\n        init_op = tf.global_variables_initializer()\r\n        saver = tf.train.Saver()\r\n\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=\"train_logs\", init_op=init_op,\r\n                                 summary_op=summary_op, saver=saver, global_step=global_step, save_model_secs=600)\r\n\r\n        with sv.managed_session(server.target) as sess:\r\n            step = 0\r\n\r\n            while not sv.should_stop() and step < 1000:\r\n                batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\r\n                train_feed = {x: batch_xs, y_: batch_ys}\r\n\r\n                _, step = sess.run([train_step, global_step], feed_dict=train_feed)\r\n\r\n                if step % 1000 == 0:\r\n                    print(\"global step: {} , accuracy:{}\".format(step, sess.run(accuracy,\r\n                                                                                feed_dict=train_feed)))\r\n\r\n            print('training accuracy %g' % sess.run(\r\n                accuracy, feed_dict={x: mnist.test.images,\r\n                                     y_: mnist.test.labels}))\r\n            print('Done training!')\r\n            if sv.is_chief:\r\n                # Export model\r\n                # WARNING(break-tutorial-inline-code): The following code snippet is\r\n                # in-lined in tutorials, please update tutorial documents accordingly\r\n                # whenever code changes.\r\n                sess.graph._unsafe_unfinalize()\r\n                export_path_base = FLAGS.work_dir\r\n                export_path = os.path.join(\r\n                    compat.as_bytes(export_path_base),\r\n                    compat.as_bytes(str(FLAGS.model_version)))\r\n                print('Exporting trained model to', export_path)\r\n                builder = saved_model_builder.SavedModelBuilder(export_path)\r\n\r\n                # Build the signature_def_map.\r\n                classification_inputs = utils.build_tensor_info(serialized_tf_example)\r\n                classification_outputs_classes = utils.build_tensor_info(prediction_classes)\r\n                classification_outputs_scores = utils.build_tensor_info(values)\r\n\r\n                classification_signature = signature_def_utils.build_signature_def(\r\n                    inputs={signature_constants.CLASSIFY_INPUTS: classification_inputs},\r\n                    outputs={\r\n                        signature_constants.CLASSIFY_OUTPUT_CLASSES:\r\n                            classification_outputs_classes,\r\n                        signature_constants.CLASSIFY_OUTPUT_SCORES:\r\n                            classification_outputs_scores\r\n                    },\r\n                    method_name=signature_constants.CLASSIFY_METHOD_NAME)\r\n\r\n                tensor_info_x = utils.build_tensor_info(x)\r\n                tensor_info_y = utils.build_tensor_info(y)\r\n\r\n                prediction_signature = signature_def_utils.build_signature_def(\r\n                    inputs={'images': tensor_info_x},\r\n                    outputs={'scores': tensor_info_y},\r\n                    method_name=signature_constants.PREDICT_METHOD_NAME)\r\n\r\n                legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\r\n\r\n                builder.add_meta_graph_and_variables(\r\n                    sess, [tag_constants.SERVING],\r\n                    signature_def_map={\r\n                        'predict_images':\r\n                            prediction_signature,\r\n                        signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\r\n                            classification_signature,\r\n                    },\r\n                    legacy_init_op=legacy_init_op)\r\n\r\n                builder.save()\r\n\r\n                print('Done exporting!')\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n\r\n```\r\n### Error Log\r\n```\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From distribute_mnist_serving_model.py:85: index_to_string (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\r\nInstructions for updating:\r\nThis op will be removed after the deprecation date. Please switch to index_to_string_table_from_tensor and call the lookup method of the returned table.\r\n('Exporting trained model to', 'model/1')\r\n2017-07-17 17:30:10.703382: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session e5aa2c66bff69f11 with config:\r\nglobal step: 0 , accuracy:0.469999998808\r\ntraining accuracy 0.5701\r\nDone training!\r\nTraceback (most recent call last):\r\n  File \"distribute_mnist_serving_model.py\", line 176, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"distribute_mnist_serving_model.py\", line 58, in main\r\n    train(server, cluster)\r\n  File \"distribute_mnist_serving_model.py\", line 159, in train\r\n    legacy_init_op=legacy_init_op)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/builder_impl.py\", line 362, in add_meta_graph_and_variables\r\n    saver.save(sess, variables_path, write_meta_graph=False, write_state=False)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1488, in save\r\n    raise exc\r\ntensorflow.python.framework.errors_impl.NotFoundError: model/1/variables/variables_temp_962a99f708244380a378c7e2218c6865\r\n         [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_INT32], _device=\"/job:ps/replica:0/task:0/cpu:0\"](save_1/ShardedFilename, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Variable, Variable_1, Variable_2)]]\r\nCaused by op u'save_1/SaveV2', defined at:\r\n  File \"distribute_mnist_serving_model.py\", line 176, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"distribute_mnist_serving_model.py\", line 58, in main\r\n    train(server, cluster)\r\n  File \"distribute_mnist_serving_model.py\", line 159, in train\r\n    legacy_init_op=legacy_init_op)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/saved_model/builder_impl.py\", line 356, in add_meta_graph_and_variables\r\n    allow_empty=True)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\r\n    self.build()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 685, in build\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 361, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 335, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 276, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 219, in save_op\r\n    tensors)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 745, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\nNotFoundError (see above for traceback): model/1/variables/variables_temp_962a99f708244380a378c7e2218c6865\r\n         [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_INT32], _device=\"/job:ps/replica:0/task:0/cpu:0\"](save_1/ShardedFilename, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Variable, Variable_1, Variable_2)]]\r\n```\r\n\r\n### The Correct Log\r\n```\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From distribute_mnist_serving_model.py:83: index_to_string (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\r\nInstructions for updating:\r\nThis op will be removed after the deprecation date. Please switch to index_to_string_table_from_tensor and call the lookup method of the returned table.\r\n('Exporting trained model to', 'model/1')\r\n2017-07-17 17:38:46.714706: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 2618b797d2ef99b4 with config:\r\nglobal step: 0 , accuracy:0.479999989271\r\ntraining accuracy 0.5513\r\nDone training!\r\nDone exporting!\r\n```\r\n\r\n### Exact command to reproduce\r\n\r\nSituation One: work well\r\n```\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,A:2224 --job_name=ps --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,A:2224 --job_name=worker --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,A:2224 --job_name=worker --task_index=1\r\n```\r\n\r\nSituation Two: can't work\r\n```\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2222,C:2222 --job_name=ps --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2222,C:2222 --job_name=worker --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2222,C:2222 --job_name=worker --task_index=1\r\n```\r\n\r\nSituation Three: work well\r\n```\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,B:2222 --job_name=ps --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,B:2222 --job_name=worker --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=A:2223,B:2222 --job_name=worker --task_index=1\r\n```\r\n\r\nSituation Four: work well\r\n```\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2223,B:2222 --job_name=ps --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2223,B:2222 --job_name=worker --task_index=0\r\n\r\npython distribute_mnist_serving_model.py --ps_hosts=A:2222 --worker_hosts=B:2223,B:2222 --job_name=worker --task_index=1\r\n```\r\n\r\nI Don't know what cause this problem. how can i fix this issue? thanks a lot.", "comments": ["Could you run with the latest binary? It's a little easier to align the line numbers from the stack trace.", "@drpngx thanks for your reply,  the binary tensorflow version is 1.2.1. is there any higher version than 1.2.1? BTW, I wonder why the supervisor should freeze the graph, if this, how can we use save_model_builder to save the final model? just like what i writen, explicitly call `sess.graph._unsafe_unfinalize()`? if don't unfreeze the graph, when I involke `  builder.add_meta_graph_and_variables`, the program will raise the exception about \" the graph is finalized, and can't be modified\"! thanks for your kindly help", "Yes, that code seems funny but basically there is no good way to do that.\r\n\r\nLet me look into the code, I was unsure whether the error was where I thought it was.", "@drpngx sometimes the Situation Four will also raise the same exception. in other words, if ps server and the saving model worker run on the same machine. SavedModelBuilder works well,  situation 4 perform unstable. situation 2 doesn't work everytime", "OK, same exception?", "@drpngx yeah. ", "Hello, @drpngx is there any  message about this issue\uff1f", "Sorry I'm out this week. Will take a look later\n\nOn Jul 25, 2017 10:26 AM, \"terry.King\" <notifications@github.com> wrote:\n\n> Hello, @drpngx <https://github.com/drpngx> is there any message about\n> this issue\uff1f\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11549#issuecomment-317661800>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbZmvNwf6dEA-5hlEuwylPdsUBgMdks5sRaIpgaJpZM4OZx8V>\n> .\n>\n", "Maybe @mrry has a hint\n\nOn Jul 25, 2017 11:17 AM, \"Patrick Nguyen\" <drpng@google.com> wrote:\n\n> Sorry I'm out this week. Will take a look later\n>\n> On Jul 25, 2017 10:26 AM, \"terry.King\" <notifications@github.com> wrote:\n>\n>> Hello, @drpngx <https://github.com/drpngx> is there any message about\n>> this issue\uff1f\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/11549#issuecomment-317661800>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AT_SbZmvNwf6dEA-5hlEuwylPdsUBgMdks5sRaIpgaJpZM4OZx8V>\n>> .\n>>\n>\n", "@drpngx excuse me, how about this issue going on\uff1fwaiting for your solution, thanks sir", "I am also confused about Supervisor and SavedModelBuilder. \r\nIt seems we can not call add_meta_graph_and_variables() after Supervisor is created unless we unfreeze the graph.\r\nI have also checked MonitoredTrainingSession, it behaves almost the same, and we can not use SessionRunHook for SavedModel either. \r\nadd_meta_graph_and_variables() need \"session\" as a parameter, but both these two api will finalize the graph after the session is created. \r\nAnyone can give some hint on that?", "@hongjic  there is no good way to do that. except using `sess.graph._unsafe_unfinalize()` ", "@terryKing1992  looks like you are right... but this still makes me feel strange. I think there should be some workaround otherwise it could be a good feature request.", "I met the same problem, can anyone solve this except _unsafe_unfinalize()? \r\n", "I think I encountered the same issue and found a workaround.\r\nThe following errors out with a `NotFoundError` in the `SaverV2` code:\r\n```\r\nsaver.save(session, 'model')\r\n```\r\nHowever using a path (even a relative one) works:\r\n```\r\nsaver.save(session, './model')\r\n```", "@wedesoft thanks for your solution, but it does not work for me.  ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@terryKing1992 do you still have the problem? What is the error message?", "I am running into the same problem. I'm running the ps on one machine and have workers running on  4 other machines.\r\n\r\nCheckpoints seem to be saved without a problem, but if I try to use SavedModelBuilder at the end, then I get errors.  Also, I am worried about using the `sess.graph._unsafe_unfinalize()` trick because it says \"unsafe\" in the method name and looks like it is a private method.\r\n\r\nIs there any update to this? Maybe an example of the TF-approved approach to saving models in distributed TF?\r\n\r\nThanks.", "@drpngx  yeah, i still have the problem. the error message is always the same, like ```NotFoundError (see above for traceback): model/1/variables/variables_temp_962a99f708244380a378c7e2218c6865\r\n         [[Node: save_1/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_INT32], _device=\"/job:ps/replica:0/task:0/cpu:0\"](save_1/ShardedFilename, save_1/SaveV2/tensor_names, save_1/SaveV2/shape_and_slices, Variable, Variable_1, Variable_2)]]```", "Yes. I think this is due to the issue of the parameter server and chief worker being on different nodes. Apparently they need to save their checkpoints and model files to the same common directory (e.g. NFS or cloud bucket). Alternatively, you can run the distributed setup without a parameter server (in which case the chief node acts like the ps). However, I've found that this is much slower and basically negates the speed up you get by going distributed. \r\n\r\nIt'd be great if TensorFlow added a function to allow us to consolidate the graph at the end onto the chief node in order to save the trained model. ", "Oh yeah, that's a known gotcha. @mas-dse-greina right, that might be a good idea. Maybe file a separate issue for tracking the feature request.", "I meet the same error in tensorflow-gpu 1.6", "hi,have you solved the problem?", "same problem here. It seems that when using distributed training, ps and worker should see the same places where model is saved? @drpngx do you confirm this?", "Are you loading checkpoints from the worker?", "[Save in distributed mode from stackoverflow](https://stackoverflow.com/questions/37593611/tensorflow-distributed-master-worker-save-fails-silently-the-checkpoint-file-is)\r\n\r\nMrry gave the answer.", "Hello all, just follow the below video and export your own model with in a 10 seconds\r\n\r\nhttps://youtu.be/w0Ebsbz7HYA"]}, {"number": 11548, "title": "Different behavior of tf.extract_image_patches and tf.nn.conv2d for certain padding/stride/filter size combinations", "body": "Hi!\r\n\r\nI am trying to implement something using ``tf.extract_image_patches`` and ran into some troubles that made clear ``tf.extract_image_patches`` handles some combinations of padding, filter size and stride differently than ``tf.nn.conv2d``. Since ``tf.extract_image_patches`` is conceptually a \"part\" of a convolution operation, I think this might be unintended behavior. \r\n\r\nSpecifically, I implemented a \"manual\" version of a convolution operation using ``tf.extract_image_patches``\r\n\r\n```python\r\ndef manual_conv(input, filter, strides, padding):\r\n  h_f, w_f, c_in, c_out = filter.get_shape().as_list()\r\n  input_patches = tf.extract_image_patches(input, ksizes=[1, h_f, w_f, 1 ], strides=strides, rates=[1, 1, 1, 1], padding=padding)\r\n  filters_flat = tf.reshape(filter, shape=[h_f*w_f*c_in, c_out])\r\n  return tf.einsum(\"ijkl,lm->ijkm\", input_patches, filters_flat)\r\n```\r\n\r\nand tested it like this\r\n\r\n```python\r\nimport unittest\r\nimport tensorflow as tf\r\n\r\nclass TestManualConvToyData(unittest.TestCase):\r\n\r\n  def runTest(self):\r\n    m = 32\r\n    c_in = 3\r\n    c_out = 16\r\n\r\n    image_sizes = [127, 64]\r\n    filter_sizes = [1, 2, 3, 5, 11]\r\n    strides = [1, 3, 4, 30]\r\n    paddings = [\"VALID\", \"SAME\"]\r\n\r\n    for fs in filter_sizes:\r\n      for stri in strides:\r\n        for imsize in image_sizes: \r\n          for pad in paddings:\r\n            h = w = imsize\r\n            h_f = w_f = fs\r\n            print \"Testing for\", imsize, fs, stri, pad\r\n\r\n            tf.reset_default_graph()\r\n            X = tf.constant(1.0+np.random.rand(m, h, w, c_in), tf.float32)\r\n            W = tf.truncated_normal([h_f, w_f, c_in, c_out])\r\n\r\n            Z = tf.nn.conv2d(X, W, strides=[1, stri, stri, 1], padding=pad)\r\n            Z_manual = manual_conv(X, W, strides=[1, stri, stri, 1], padding=pad)\r\n\r\n            sess = tf.Session()\r\n            sess.run(tf.global_variables_initializer())\r\n            Z_, Z_manual_ = sess.run([Z, Z_manual])\r\n            self.assertEqual(Z_.shape, Z_manual_.shape)\r\n            self.assertTrue(np.allclose(Z_, Z_manual_, rtol=1e-05))\r\n            sess.close()\r\n ```\r\n\r\nThis test fails for some combinations of padding, filter size and stride. I think it has to do with the fact that ``tf.extract_image_patches`` tries to center patches if possible, as discussed in [this][1] stackoverflow question.\r\n\r\n[1]: https://stackoverflow.com/questions/40731433/understanding-tf-extract-image-patches-for-extracting-patches-from-an-image\r\n\r\n------------------------\r\n\r\n### System information\r\n- Ubuntu 16.04.\r\n- Python 2.7.12\r\n- tensorflow version 1.2.1 installed via pip (CPU only)", "comments": ["Yangzihao, can you verify this is fixed? The reason for the failure might also be due precision issues instead of padding issues.\r\n", "Nagging Assignee @reedwm: It has been 149 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing because I assume this is fixed by a change Yangzihao made. Please reopen if that is not the case."]}, {"number": 11547, "title": "Tensorflow installation issue", "body": "Hi Team,\r\n\r\n    I have anaconda with python 2.7 in my windows 7 machine, i tried to install tensorflow with the below command.\r\n   c:\\> conda create -n tensorflow python=2.7\r\n   it has installed successfully.\r\nactivate tensorflow\r\n  it changed the prompt to\r\n(tensorflow) C:\\>\r\n\r\n  I have visual studio 2015, and selected the environment as python 2.7 and ran a sample program, it is giving the below error, please suggest.\r\n\r\n    from keras.models import Sequential\r\n  File \"D:\\Anaconda2\\Lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"D:\\Anaconda2\\Lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <mod\r\nule>\r\n    from . import conv_utils\r\n  File \"D:\\Anaconda2\\Lib\\site-packages\\keras\\utils\\conv_utils.py\", line 3, in <m\r\nodule>\r\n    from .. import backend as K\r\n  File \"D:\\Anaconda2\\Lib\\site-packages\\keras\\backend\\__init__.py\", line 83, in <\r\nmodule>\r\n    from .tensorflow_backend import *\r\n  File \"D:\\Anaconda2\\Lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", lin\r\ne 1, in <module>\r\n    import tensorflow as tf\r\nImportError: No module named tensorflow\r\nPress any key to continue . . .\r\n\r\n", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\nPlease note that from the [Installation Page for Windows](https://www.tensorflow.org/install/install_windows), Tensorflow is **NOT** supported on Python 2.7 but only **3.5** (or **3.6** in the latest release). Please try again on a newer version of Python.", "Hello\r\nAfter activating tensor flow u have to install tensor flow inside it using pip or conda.\r\nPlease refer to the following link and proceed.\r\nhttps://www.tensorflow.org/versions/r0.12/get_started/os_setup#pip_installation_on_windows\r\n", "@shreyneil You should avoid linking users to old versions of the installation instructions. The latest installation page for Windows on the Tensroflow website is available here:  [Installation Page for Windows](https://www.tensorflow.org/install/install_windows)\r\n\r\nPlease see my above comment on Python versions and let me know how you get on. For further installation issues you may find @mrry 's installation troubleshooter helper here: https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c", "As @jubjamie has stated:\r\n```\r\nPlease note that from the Installation Page for Windows,\r\n Tensorflow is NOT supported on Python 2.7 but only 3.5\r\n (or 3.6 in the latest release). Please try again on a newer\r\n version of Python.\r\n```\r\n\r\nClosing issue.", "Hey guys, I am trying to install tensorflow but found this message: \r\n\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n\r\nany suggestion?", "Best off re-filing on Stack Overflow. Chances are you are trying to install an invalid binary or aren't suing a supported configuration.\r\n\r\nIf you decide to move this issue to Stack Overflow, please post the link here so that myself (or others that stumble upon this issue) can find the S/O thread."]}, {"number": 11546, "title": "PyImport_Import crash", "body": "I recently ran into a problem, while using bazel to build a project.  This project is compiled as a dynamic linking library, using PyImport_Import to import python module. when there is \"import tensorflow as tf\" in the python file , application who calls the dynamic linking library crashed everytime, but when it\u2018s not there ,everything works just fine. where is the problem?\r\nmy tensorflow version is 1.0.0,python 2.7.0,bazel 0.4.3\r\n\r\n\r\nhere is the console information when the application crashes:\r\n\r\n> F tensorflow/core/framework/function.cc:1015] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for Softmax\r\n\r\n\r\nhere is the test python file looks like: \r\n`from __future__ import print_function\r\nimport tensorflow as tf\r\nimport os\r\nimport time\r\nfrom itertools import izip\r\nimport numpy as np\r\nimport wrapt\r\nimport cv2\r\ndef get_int( ):\r\n    a = 10\r\n    b = 20\r\n    return a + b\r\n\r\ndef get_str( s1, s2 ):\r\n    #return s1 + s2  \r\n    #return 'Hello , TY'  \r\n    return ('Hello, World', 10, 20)`\r\n\r\nhere is the source code of .so file:\r\n`   Py_Initialize();\r\n    if ( !Py_IsInitialized() ) {\r\n        return -1;\r\n    }\r\n    PyEval_InitThreads();\r\n    PyThreadState *mainThreadState = NULL;\r\n    //  save a pointer to the main PyThreadState object \r\n    mainThreadState = PyThreadState_Get();\r\n    //  release the lock \r\n    PyEval_ReleaseLock();\r\n    char* mockargv[1]={(char*)\"\"};\r\n    PySys_SetArgv(1,mockargv);\r\n    PyRun_SimpleString(\"import sys\");\r\n    PyRun_SimpleString(\"sys.path.append('./')\");\r\n    pName_ = PyString_FromString(\"test_py\");\r\n    displayPyObject(pName_);\r\n    if(pName_ == NULL){\r\n        return -1;\r\n    }\r\n    pModule_ = PyImport_Import(pName_);\r\n    displayPyObject(pModule_);`\r\n\r\nand here is the dynamic linking library part of my BUILD file\r\n`cc_binary(\r\n    name = \"test.so\",\r\n    linkshared = 1,\r\n    deps = [\r\n        \":test_lib\",\r\n    ],\r\n)\r\ncc_library(\r\n    name = \"test_lib\",\r\n    visibility = [\"//visibility:__subpackages__\"],\r\n    srcs = glob([\"test.cpp\"\r\n             ],\r\n        ),\r\n    includes=[\"test.h\"],\r\n    linkopts = [\r\n\"-lm -lpthread -L/usr/lib/python2.7 -lpython2.7 -lopencv_core -lopencv_imgproc -lopencv_highgui -lopencv_ml -lfreeimage\"\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:framework_internal\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n`\r\n\r\n\r\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/7403\r\ntry these.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) and provide all the information it asks. Thank you.\r\n\r\nThat said, based on what you've suggested, I would suspect the segfault is because of dual initialization of some global state. As @shreyneil pointed out, if you're adding an op, please follow the documentation at https://www.tensorflow.org/extend/, which will result in your custom library extension linking against `pywrap_tensorflow.so` and avoiding any global state conflicts.\r\n\r\nIf that doesn't seem to be the case, please feel free to file a new issue with all requested details and most importantly detailed instructions to reproduce the problem. Hope that helps, thanks!"]}, {"number": 11545, "title": "add nodouble option for all cwise ops", "body": "this is the following patch of ac98d1184008e4 to support nodouble\r\noption for all cwise ops. The macros REGISTER* defined within\r\n__ANDROID_TYPES_SLIM__ should be changed to empty, and it impacts\r\nall the cwise ops, so, all the changes for the cwise ops have to be\r\nin a single patch.\r\n\r\nthere will be more patches for other ops to support nodouble option.", "comments": ["Can one of the admins verify this patch?", "thanks @vrv pointing my mistake, i'll rewrite the code to add new macros such as REGISTER_SYCL, and REGISTER6_SYCL. In the new code, the change could be separated to be one patch for one op, it could make each patch simple and easy to be reviewed.  So, my plan is to close this PR, and send out new PRs one by one (send out the second patch only after the first one is accepted).", "I'd be fine with all the changes in the same PR actually, since they are logically all the same, and can be tested all at once (less load on our test infrastructure). They are mostly mechanical changes so it's easier to review in a single batch :)", "got it, i'll continue this PR with new patches.", "@vrv Could you take another look, please?", "I think @benoitsteiner probably wants to look at this since it's SYCL related.", "@tensorflow-jenkins test this please", "hello, i checked the failure log but do not understand how it relative to my change, could anyone help? thanks.\r\n\r\nthe last few lines of the log:\r\nPASS: //tensorflow/python:session_benchmark\r\nPASS: //tensorflow/python:matmul_benchmark_test\r\nBuild was aborted\r\nAborted by unknown\r\n[Set GitHub commit status (universal)] PENDING on repos [] (sha:ef4e31e) with context:tensorflow-pull-requests-gpu\r\nUnable to get pull request builder trigger!!\r\nSetting status of 12276a750d6240f93878f84fa125f6be48566e7d to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/6269/ and message: 'FAILURE\r\n '\r\nUsing context: Linux GPU\r\nFinished: ABORTED", "@guoyejun The failure was unrelated to your change.", "@tensorflow-jenkins test this please", "@benoitsteiner Can you take a look at the replies?", "@benoitsteiner what do you think?", "@benoitsteiner ping?", "@benoitsteiner any luck with this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "just close it since no feedback for a long time, and code base change results into conflicts"]}]