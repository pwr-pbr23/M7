[{"number": 34919, "title": "Enable preventing engine build at runtime", "body": "Adds a new API argument `allow_build_at_runtime` which allows users to prevent building TRT engines at runtime if desired. The existing behavior can be achieved by setting this argument to `True` which is also the default value.\r\n\r\nThis argument is useful for users of the `build()` method that try to build all the engines offline before doing any inference, and then want to avoid any optimization during inference, keeping low latency. In such cases, if there is no engine found in the cache, then native TF is used instead.", "comments": ["I just fixed the api compatibility test by adding the new argument to the golden api.", "Hi @pooyadavoodi, could you help to resolved the conflicts? Thanks.", "This PR has to wait until https://github.com/tensorflow/tensorflow/pull/35198 gets merged.", "@sanjoy @aaroey \r\n\r\nWhen I run any c++ test that executes native TF function instead of TRT, then I get this error: `Executor failed to create kernel. Not found: No registered '_Arg' OpKernel for 'GPU' devices compatible with node`.\r\n\r\nI don't think it's related to `_Arg` because I replaced it with other nodes and still fails.\r\n\r\nIt also seems unrelated to this PR because I can repro the error without the PR if I change the test to force running native TF functions. For example, you can substitute `ops::Add` by `ops::Less` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op_test.cc#L60 to repro the issue. We currently don't have any tests that execute native TF functions and that's why we never caught this.\r\n\r\nThis is how I run the test: `bazel test -c opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --noincompatible_do_not_split_linking_cmdline //tensorflow/compiler/tf2tensorrt:trt_engine_op_test`\r\nLet me know if the problem is how I run the test.\r\n\r\nI am kind of stuck in root causing this.", "@pooyadavoodi It could be that you're not linking in the correct kernel registrations.  For instance, the GPU implementation of the `_Arg` op is [defined](https://github.com/tensorflow/tensorflow/blob/45e1e4598d3ebab316bf87df9160656f524af36c/tensorflow/core/kernels/function_ops.cc#L105) in function_ops.cc and you need to link that in to be able to execute `_Arg` ops.\r\n\r\nCan you try linking in `//tensorflow/core/kernels:function_ops` and `//tensorflow/core/kernels:array` to the test binary and see if that fixes or at least changes the failure?", "> @pooyadavoodi It could be that you're not linking in the correct kernel registrations. For instance, the GPU implementation of the `_Arg` op is [defined](https://github.com/tensorflow/tensorflow/blob/45e1e4598d3ebab316bf87df9160656f524af36c/tensorflow/core/kernels/function_ops.cc#L105) in function_ops.cc and you need to link that in to be able to execute `_Arg` ops.\r\n> \r\n> Can you try linking in `//tensorflow/core/kernels:function_ops` and `//tensorflow/core/kernels:array` to the test binary and see if that fixes or at least changes the failure?\r\n\r\nThanks @sanjoy. Linking changed the error. Now I am looking at another problem.", "> Thanks @sanjoy. Linking changed the error. Now I am looking at another problem.\r\n\r\nLooks like this isn't ready for review so removing the tag.  @pooyadavoodi please LMK if this is indeed ready for review.", "@sanjoy I am seeing the following error when running `trt_engine_op_test`:\r\n\r\n```\r\nunknown file: Failure\r\nC++ exception with description \"bad_function_call\" thrown in the test body.\r\n```\r\nThe error happens when TF-TRT decides to execute the native TF function instead of the TRT engine. Looks like the TF function is not available?\r\n\r\nWe register the graph in the function library here: https://github.com/pooyadavoodi/tensorflow/blob/allow_build_at_runtime/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op_test.cc#L69-L71\r\n\r\nAny idea what could be the reason?\r\n", "https://en.cppreference.com/w/cpp/utility/functional/bad_function_call says \"`std::bad_function_call` is the type of the exception thrown by `std::function::operator()` if the function wrapper has no target.\".  So probably we have an uninitialized `std::function` somewhere.  Can you get the stack trace from where the exception is thrown?", "> https://en.cppreference.com/w/cpp/utility/functional/bad_function_call says \"`std::bad_function_call` is the type of the exception thrown by `std::function::operator()` if the function wrapper has no target.\". So probably we have an uninitialized `std::function` somewhere. Can you get the stack trace from where the exception is thrown?\r\n\r\n`bazel test` is not showing the stack trace. Do you know how I can force it to show the stack trace?", "> `bazel test` is not showing the stack trace. Do you know how I can force it to show the stack trace?\r\n\r\nYou could try running the program under GDB and put a breakpoint on the constructor of `std::bad_function_call` to see where it is constructed.", "I still see the \"bad_function_call\" error when running the test that you added in the PR, here is the stack:\r\n*** SIGABRT received by PID 2361 (TID 2361) from PID 2361; stack trace: ***\r\nPC: @     0x7f1fd1132602  (unknown)  raise\r\n    @     0x55b930fbd67b       1728  FailureSignalHandler()\r\n    @     0x7f1fd12b09a0  1950672896  (unknown)\r\n    @     0x55b92b3e2109         16  std::__u::__throw_bad_function_call()\r\n    @     0x55b92f737cc9         16  std::__u::__function::__policy_invoker<>::__call_empty()\r\n    @     0x55b92f730c0b        512  tensorflow::(anonymous namespace)::ExecutorImpl::RunAsync()\r\n    @     0x55b92f74452d        560  tensorflow::FunctionLibraryRuntimeImpl::Run()\r\n    @     0x55b92b3eccf3        336  tensorflow::tensorrt::TRTEngineOp::ExecuteNativeSegment()\r\n    @     0x55b92b3ee2cd        192  tensorflow::tensorrt::TRTEngineOp::ComputeAsync()\r\n    @     0x55b92fb4703d         80  tensorflow::AsyncOpKernel::Compute()\r\n    @     0x55b92f6bab00        400  tensorflow::BaseGPUDevice::Compute()\r\n    @     0x55b92b3e0814        208  tensorflow::OpsTestBase::RunOpKernel()\r\n    @     0x55b92b3e09d8        160  tensorflow::tensorrt::TRTEngineOpTestBase_AllowBuildAtRuntime_Test::TestBody()\r\n    @     0x55b92cda00c2         48  testing::Test::Run()\r\n    @     0x55b92cda0d8c         96  testing::TestInfo::Run()\r\n    @     0x55b92cda16c7         80  testing::TestSuite::Run()\r\n    @     0x55b92cdaeb47        208  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x55b92cdae16f         64  testing::UnitTest::Run()\r\n    @     0x55b92ca08f6e         32  main\r\n    @     0x7f1fd111ebbd        208  __libc_start_main\r\n", "I managed to make a debug build of TF that shows stack trace when it crashes but still doesn't show any backtrace in gdb. It would be good to document how to make a debug build of TF.\r\n\r\nHere is the full log of the test: https://gist.github.com/pooyadavoodi/3addaeff4cca5f2de6ecfa8d8fdd00bd\r\n\r\nMy guess is that the failure happens because of `\"Not found: No attr named` such as:\r\n```\r\n2020-01-31 18:36:40.777739: I tensorflow/core/platform/status.cc:95] Generated non-OK status: \"Not found: No attr named '_scoped_allocator' in NodeDef:\". *** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\ttensorflow::Status::Status(tensorflow::error::Code, absl::string_view)\r\n\ttensorflow::AttrSlice::Find(absl::string_view, tensorflow::AttrValue const**) const\r\n\ttensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::string_view, std::vector<int, std::allocator<int> >*)\r\n\t\r\n\ttensorflow::NewLocalExecutor(tensorflow::LocalExecutorParams const&, tensorflow::Graph const&, tensorflow::Executor**)\r\n\t\r\n\ttensorflow::NewExecutor(std::string const&, tensorflow::LocalExecutorParams const&, tensorflow::Graph const&, std::unique_ptr<tensorflow::Executor, std::default_delete<tensorflow::Executor> >*)\r\n\ttensorflow::FunctionLibraryRuntimeImpl::CreateItem(tensorflow::FunctionLibraryRuntimeImpl::Item**)\r\n\ttensorflow::FunctionLibraryRuntimeImpl::GetOrCreateItem(unsigned long long, tensorflow::FunctionLibraryRuntimeImpl::Item**)\r\n\ttensorflow::FunctionLibraryRuntimeImpl::Run(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long long, absl::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, std::function<void (tensorflow::Status const&)>)\r\n\ttensorflow::tensorrt::TRTEngineOp::ExecuteNativeSegment(tensorflow::OpKernelContext*, tensorflow::tensorrt::AsyncHelper*)\r\n\ttensorflow::tensorrt::TRTEngineOp::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\r\n\ttensorflow::AsyncOpKernel::Compute(tensorflow::OpKernelContext*)\r\n\ttensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n\t\r\n\tvoid testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)\r\n\ttesting::Test::Run()\r\n\ttesting::TestInfo::Run()\r\n\ttesting::TestSuite::Run()\r\n\ttesting::internal::UnitTestImpl::RunAllTests()\r\n\tbool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*)\r\n\ttesting::UnitTest::Run()\r\n\tmain\r\n\t__libc_start_main\r\n\t\r\n*** End stack trace ***\r\n```\r\n", "We know that TF-TRT source code can successfully execute native segments (TF functions added to graph as a function library). The problem is that our c++ tests can'r run the native segments.\r\n\r\nTF-TRT Source code: \r\n- Functions are added to the graph here:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc#L672\r\n- Function libraries are instantiated here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L279\r\n \r\nTF-TRT c++ unit tests (e.g. trt_engine_op_test):\r\n- Functions are added to the graph here:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op_test.cc#L69\r\n- Function libraries are not instantiated in the same place as TF-TRT source code because the test creates a new instance of TRTEngineOp which triggers its constructor which is where the function libraries get instantiated.\r\n\r\n\r\n", "I triaged the problem down to `OpsTestBase`.  With this [change](https://gist.github.com/sanjoy/2f355116c7ca88bfcd8665ba988a1bbf) patched in I now get a different [failure](https://gist.github.com/sanjoy/ba102ae122d2eaaa2073e5afe0ee0de2) from the test.", "@nluehr @cliffwoolley", "@sanjoy I can see the following error in the log you linked:\r\n\r\n```\r\n2020-02-01 06:22:10.737479: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n```\r\n\r\nThis usually happens when TRT can't run at all. The reason could vary from bad GPU or OOM.\r\nIt could be also because of having multiple GPUs (due to a known issue in TF-TRT). Could you rerun the test with 1 gpu?", "> This usually happens when TRT can't run at all. The reason could vary from bad GPU or OOM.\r\n> It could be also because of having multiple GPUs (due to a known issue in TF-TRT). Could you rerun the test with 1 gpu?\r\n\r\nMy machine does have two GPUs so that's probably it, but I haven't checked.", "> My machine does have two GPUs so that's probably it, but I haven't checked.\r\n\r\nJust to clarify, the next step here is: someone ( @DEKHTIARJonathan ?) needs to commandeer this PR and apply the [patch](https://github.com/tensorflow/tensorflow/pull/34919#issuecomment-580999679) from the previous comment.  Then the tests should start passing and we can continue reviewing the PR.", "I tried on single GPU, the convert step works very well but calibrate step costs all of the GPU memory (6GB) and shows the following error:\r\n\r\nE tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n\r\nI think it is because tftrt use out of memory. So, is there some method to limit the memory cost during calibrate step? ", "I have applied the patch from @sanjoy and rebased to latest master. The error in TRTEngineOpTestBase.AllowBuildAtRuntime was actially an error in defining the test: I have corrected the check for empty engine creation.  I have not encountered any errors during calibration. \r\n\r\nTo fix CLA issues (original PR author has left the company), I have pushed the corrected branch to my repository, and opened a new PR #36852. This PR can be closed.", "Thanks @tfeher for the confirmation. \r\nClosing this PR, as its change has been moved to [#36852](https://github.com/tensorflow/tensorflow/pull/36852)"]}, {"number": 34918, "title": "[r2.1 Cherrypick]:Override EIGEN strong inline for release builds as well.", "body": "PiperOrigin-RevId: 284261705\nChange-Id: I882c786169fb2d51716c884c9b1c91b59ae2df4e", "comments": []}, {"number": 34917, "title": "[r2.1 Cherrypick]: Group variable initialization when calling lift_to_graph.", "body": "When initializing variables defined inside a @tf.function which are lifted to the outer graph, group the variables together and call lift_to_graph once.  lift_to_graph supports passing in multiple tensors and the graph to lift to is the same for all of the variable initialization.  This improves setup time.\n\nPiperOrigin-RevId: 284263511\nChange-Id: I4cfcdb0394198df8f890a98295cc2fcb77b75413", "comments": []}, {"number": 34916, "title": "Exclude TFLite GPU kernel tests from non-GPU builds", "body": "Typing\r\n```\r\nbazel test //tensorflow/lite/...\r\n```\r\non a system without a GPU results in multiple test failures as the build attempts to compile and test several types of GPU kernels.\r\n\r\nThis pull request adds the \"gpu\" tag to the TFLite tests that require a GPU to run. The default build configuration created by `configure.py` disables tests with this tag if the build is configured without GPU support.", "comments": ["@frreiss Can you please resolve conflicts? Thanks!", "Conflicts resolved."]}, {"number": 34915, "title": "[r2.1 Cherrypick] Eigen update for fixing the Windows builds", "body": "", "comments": []}, {"number": 34914, "title": "Update version numbers for TensorFlow 2.1.0-rc1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 1\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.1.0-rc0\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.1.0rc0\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 34913, "title": "Apply clang-format to all of TF-TRT", "body": "This is what I did: \r\n\r\n```\r\nfind . -name \"*.cc\" -exec clang-format -i {} --style=google \\;\r\nfind . -name \"*.h\" -exec clang-format -i {} --style=google \\;\r\n```\r\n\r\nI have also put all the changes of https://github.com/tensorflow/tensorflow/pull/34293 otherwise this will get conflicts after merging https://github.com/tensorflow/tensorflow/pull/34293. So, we should make sure this gets merged after https://github.com/tensorflow/tensorflow/pull/34293.", "comments": ["@pooyadavoodi thanks for looking into this. In fact I don't think we need to run clang-format manually, each time before the merge it'll be run automatically. So I think it's already in the right format (which is a bit different than the output from your clang-format and I believed that's because we did some code transformations internally).", "> @pooyadavoodi thanks for looking into this. In fact I don't think we need to run clang-format manually, each time before the merge it'll be run automatically. So I think it's already in the right format (which is a bit different than the output from your clang-format and I believed that's because we did some code transformations internally).\r\n\r\nI see, that's great. \r\n\r\nIt would be good to remove clang-format from coding style then, or at least mention that it's done automatically and users don't need to run it manually?"]}, {"number": 34912, "title": "AssertionError: Could not compute output Tensor", "body": "I am trying to use functional API to build an encoder-decoder model for grammatical error correction. Given incorrect sentences (x) and correct sentences (y):\r\n\r\n```\r\nx = tf.random.uniform(shape=(1000,8), minval=2, maxval=voc_len, dtype=tf.dtypes.int32)\r\ny = tf.random.uniform(shape=(1000,6), minval=2, maxval=voc_len, dtype=tf.dtypes.int32)\r\n\r\nx = np.array(tf.concat([tf.ones([1000,1], tf.int32), x, tf.zeros([1000,1], tf.int32)], 1))\r\ny = np.array(tf.concat([tf.ones([1000,1], tf.int32), y, tf.zeros([1000,1], tf.int32)], 1))\r\n\r\ninput_x = Input(shape=(None,), name='inc_sent')\r\ninput_y = Input(shape=(None,), name='cor_sent')\r\n\r\nemb_enc = Embedding(input_dim=voc_len+1, output_dim=emb_size, name='inc_emb')(input_x)\r\nemb_dec = Embedding(input_dim=voc_len+1, output_dim=emb_size, name='cor_emb')(input_y)\r\n\r\ngru_enc_l1 = GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform', name='inc_gru_l1')\r\ngru_dec_l1 = GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform', name='cor_gru_l1')\r\n\r\no_enc1, h_enc1 = gru_enc_l1(emb_enc, initial_state = tf.zeros([1000,100]))\r\no_dec1, h_dec1 = gru_dec_l1(emb_dec, initial_state = h_enc1)\r\n\r\n# I would like to inspect o_dec1:\r\nmodel = Model(inputs = [input_x, input_y], outputs = o_dec1)\r\n\r\nmodel(x,y)\r\n```\r\nThe error shows `AssertionError: Could not compute output Tensor(\"cor_gru_l1_28/Identity:0\", shape=(1000, None, 100), dtype=float32)`\r\n\r\nHow can I fix it? Please let me know. Thank you in advance", "comments": ["@Supachan,\r\nTried reproducing your code but got the error `NotImplementedError: Cannot convert a symbolic Tensor (concat:0) to a numpy array. `. Please find the attached [Gist](https://colab.sandbox.google.com/gist/amahendrakar/f28189f8b7f88dd779743fda5c4968f3/34912.ipynb#scrollTo=lVyyh8qXVu09) here.\r\nCan you please help us to reproduce the issue. Also please provide Tensorflow version, platform details etc. Thanks!", "Any update on this issue? Thanks!", "To amahendrakar,\r\n\r\nI am sorry to reply you late. The code was figured out by typing the last line: `model = ([x,y])`. Just inserted `[ ]` into `x,y`.\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport matplotlib.pylab as plt\r\n\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Flatten, Layer, Dense, LayerNormalization, Embedding, Dropout, Concatenate, Input, GRU, LSTM\r\nfrom tensorflow.keras.models import Sequential, Model\r\n\r\nvoc_len = 30\r\nemb_size = 10\r\nunits = 100\r\n\r\nx = tf.random.uniform(shape=(1000,8), minval=2, maxval=10, dtype=tf.dtypes.int32)\r\ny = tf.random.uniform(shape=(1000,6), minval=2, maxval=10, dtype=tf.dtypes.int32)\r\n\r\nx = np.array(tf.concat([tf.ones([1000,1], tf.int32), x, tf.zeros([1000,1], tf.int32)], 1))\r\ny = np.array(tf.concat([tf.ones([1000,1], tf.int32), y, tf.zeros([1000,1], tf.int32)], 1))\r\n\r\ninput_x = Input(shape=(None,), name='inc_sent')\r\ninput_y = Input(shape=(None,), name='cor_sent')\r\n\r\nemb_enc = Embedding(input_dim=voc_len+1, output_dim=emb_size, name='inc_emb')(input_x)\r\nemb_dec = Embedding(input_dim=voc_len+1, output_dim=emb_size, name='cor_emb')(input_y)\r\n\r\ngru_enc_l1 = GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform', name='inc_gru_l1')\r\ngru_dec_l1 = GRU(units, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform', name='cor_gru_l1')\r\n\r\no_enc1, h_enc1 = gru_enc_l1(emb_enc, initial_state = tf.zeros([1000,100]))\r\no_dec1, h_dec1 = gru_dec_l1(emb_dec, initial_state = h_enc1)\r\n\r\n# I would like to inspect o_dec1:\r\nmodel = Model(inputs = [input_x, input_y], outputs = o_dec1)\r\n\r\nmodel([x,y])\r\n\r\n```\r\n\r\nHope this help you. Let me close this issue.", "To amahendrakar,\r\n\r\nI forget one thing to answer your question. The code was run on Colab and TF2.0.\r\nThanks.\r\n\r\nSupachan", "Hello all!! I have met the same problem. Could you please share your solution for this. Thank you so much.", "@TamMinhVo,\r\n> I am sorry to reply you late. The code was figured out by typing the last line: `model = ([x,y])`. Just inserted `[ ]` into `x,y`.\r\n\r\nThe above change seems to help. Please check if it works. if not submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose), so that we can track the issue there. Thanks!", "This feature is super misleading!  It wasted me half a day to figure out the problem.\r\n\r\nIt should be emphasized in the documentation.", "I had the same issue here using tf.data.Datasets and, for me, the problem was related with the inputs and outputs.\r\n\r\nI solved by naming each input layer and latter by creating a TF Dataset with the inputs as a dict and the output as a single value. Something like the following:\r\n\r\n```\r\n# Example code\r\n\r\nx1 = tf.keras.layer.Input(..., name='input_1')\r\nx2 = tf.keras.layer.Input(..., name='input_2')\r\n......\r\nconcat_layer = tf.keras.layers.Concatenate([x1, x2])\r\ny = tf.keras.layer.Dense(1)(concat_layer)\r\n\r\nmodel = Model([x1, x2], y)\r\n\r\ndataset = tf.data.Dataset(....) # suppose each sample in dataset is a triple (2-features and 1 label)\r\n\r\ndef input_solver(sample):\r\n    return {'input_1': sample[0], 'input_2': sample[1]}, sample[2]\r\n\r\ndataset.map(input_solver) # this will map the first and the second feature in this triple-sample to the inputs.\r\nmodel.fit(dataset, epochs=5)\r\n```\r\n\r\nThis solved for me with datasets with multiple inputs per sample\r\n\r\n", "Hello, I also have the same problem. When I use keras bert , it throws out an issue about inputs and outputs. Can anyone help me solve this problem.\r\n![image](https://user-images.githubusercontent.com/32276346/92403591-58f9a200-f164-11ea-9637-d5f866aeeb00.png)\r\nAbove is my code.\r\nAnd following is my code problem \r\n![image](https://user-images.githubusercontent.com/32276346/92403672-81819c00-f164-11ea-8220-1c504ab0f699.png)\r\n", "> Hello, I also have the same problem. When I use keras bert , it throws out an issue about inputs and outputs. Can anyone help me solve this problem.\r\n\r\n@Supachan,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "> return {'input_1': sample[0], 'input_2': sample[1]}, sample[2]\r\n\r\n\r\nThis is what fixed it for me. \r\n"]}, {"number": 34911, "title": "Fix dynamic display for PyCharm", "body": "### Summary\r\nFixes `self._dynamic_display` not being set to true for printing out the verbose training updates. Previously in PyCharm, it would print a new line every update to the progress bar and with this change it works as expected, clearing each previous update to the line and removing the annoying bug of many, many fast printing lines to the console.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34911) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34911) for more info**.\n\n<!-- ok -->"]}, {"number": 34910, "title": "Add Estimator release notes.", "body": "Just one note.", "comments": []}, {"number": 34909, "title": "Non-deterministic access to Random Number Generator in tf.data.Dataset.map with num_parallel_calls > 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80\r\n\r\n**Describe the current behavior**\r\nCurrently, when a `tf.random.experimental.Generator` is passed to an operator used in `tf.data.Dataset.map` with `num_parallel_calls` > 1, this random number generator is accessed in a non-deterministic order, which makes that the output of the dataset is non-reproducible (see MVCE below).\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is that even when parallelized inside `.map`, Random Number Generators are called in a deterministic way, so that the `data.Dataset` overall pipeline gets fully reproducible.\r\n\r\nI do not know whether this is a bug, a desired behavior, or an unavoidable side-effect of parallelizing the operator passed to `.map`, but the overall consequence is that an operator which leverages random operations (like data augmentation, typically) can not be parallelized over `tf.data.dataset` pipelines if one wants to keep its experiments reproducible. \r\n\r\nI am interested in any trick / workaround which would authorize me to keep both high - performance pipelines and experiments reproducibility.\r\n\r\n**Code to reproduce the issue**\r\nSee below a simple MVCE: It is self explanatory. \r\n\r\nThe `test_sequential` creates a dummy `dataset` of length 10, maps an op which pulls 1 number from a RNG  (not parallelized). The 10 resulting values are concatenated into a 10-length vector. It then repeats the overall process and compares the vector from the first draft to the one of the second draft.\r\nThe `test_parallel` does exactly the same. However, the op pulling numbers from the RNG is parallelized over 4 threads. When comparing the two 10-length vectors, they are non equal often.\r\n\r\n**`test_parallel` is run 10 times which should be enough to highlight some discrepancies (shuffled values) between the two 10-length generated vectors.**\r\nThe `test_sequential` shows that the RNG is indeed reproducible when accessed to in a sequential way.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint('TF Version', tf.__version__)\r\nimport numpy as np\r\n\r\ndef draw_samples(num_parallel_calls):\r\n\r\n    def mapper(x, rng):\r\n        return rng.uniform(shape=(), minval=0, maxval=10, dtype=tf.int32)\r\n\r\n    seed = 12345\r\n    algo = 1\r\n    state = tf.random.experimental.create_rng_state(seed, 1)\r\n    rng = tf.random.experimental.Generator(state=state, alg=algo)\r\n\r\n    ds = tf.data.Dataset.from_tensor_slices(tf.range(10)).map(lambda x: mapper(x, rng), num_parallel_calls=num_parallel_calls).batch(10)\r\n\r\n    return next(iter(ds)).numpy()\r\n\r\ndef test_answer_sequential():\r\n    r = [draw_samples(num_parallel_calls=1) for _ in range(2)]\r\n    print('\\nSequential')\r\n    print('\\n', r[0].tolist(), '\\n', r[1].tolist())\r\n\r\n\r\ndef test_answer_parallel():\r\n    r = [draw_samples(num_parallel_calls=4) for _ in range(2)]\r\n    if not np.allclose(*r):\r\n      print('\\nNon deterministic access when spread on 4 threads:')\r\n      print('\\tFirst draw ', r[0].tolist(), '\\n\\tSecond draw', r[1].tolist())\r\n    else:\r\n      print('Ok by chance')\r\n\r\ntest_answer_sequential()\r\nfor _ in range(10):\r\n  test_answer_parallel()\r\n```\r\n", "comments": ["@remydubois \r\nI have tried on colab with TF version 2.0 . Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f78cf4e81aa1e4719d5c6d098147897d/untitled450.ipynb). Is this the expected behavior?. Thanks!", "Hello, \r\n\r\nThanks for looking into this. The notebook you linked indeed showed expected behavior, by chance I guess. The extract I linked below tuns 10 times the `test_parallel` and should highlight discrepancies between the two vectors output when the `num_parallel_calls` is set to 4 \r\n\r\n```\r\nimport tensorflow as tf\r\nprint('TF Version', tf.__version__)\r\nimport numpy as np\r\n\r\ndef draw_samples(num_parallel_calls):\r\n\r\n    def mapper(x, rng):\r\n        return rng.uniform(shape=(), minval=0, maxval=10, dtype=tf.int32)\r\n\r\n    seed = 12345\r\n    algo = 1\r\n    state = tf.random.experimental.create_rng_state(seed, 1)\r\n    rng = tf.random.experimental.Generator(state=state, alg=algo)\r\n\r\n    ds = tf.data.Dataset.from_tensor_slices(tf.range(10)).map(lambda x: mapper(x, rng), num_parallel_calls=num_parallel_calls).batch(10)\r\n\r\n    return next(iter(ds)).numpy()\r\n\r\ndef test_answer_sequential():\r\n    r = [draw_samples(num_parallel_calls=1) for _ in range(2)]\r\n    print('\\nSequential')\r\n    print('\\n', r[0].tolist(), '\\n', r[1].tolist())\r\n\r\n\r\ndef test_answer_parallel():\r\n    r = [draw_samples(num_parallel_calls=4) for _ in range(2)]\r\n    if not np.allclose(*r):\r\n      print('\\nNon deterministic access when spread on 4 threads:')\r\n      print('\\tFirst draw ', r[0].tolist(), '\\n\\tSecond draw', r[1].tolist())\r\n    else:\r\n      print('Ok by chance')\r\n\r\ntest_answer_sequential()\r\nfor _ in range(10):\r\n  test_answer_parallel()\r\n\r\n```\r\n", "I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e5cf82bc5c057b7a9419d8a9b033dbb3/untitled459.ipynb). Thanks!\r\n", "Actually this issue is closely related to [#13932](https://github.com/tensorflow/tensorflow/issues/13932).\r\n\r\nMrry answered clearly: https://github.com/tensorflow/tensorflow/issues/13932#issuecomment-341263301\r\nBasically this is not a bug but an unavoidable behavior, which can not be fixed.\r\n\r\nClosing the issue thereof,\r\n\r\nThanks for your time", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34909\">No</a>\n"]}, {"number": 34907, "title": "[TF 2.1rc0] The `{predict,train,test}_on_batch` trace functions with fixed batch size", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.1.0rc0\r\n- Python version: 3.5.3\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the updated `predict_on_batch` method wrapped now in `tf.function`, the traced calls have a fixed batch size. Therefore, if batch size changes regularly, the function is retraced all the time. For example, the following\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(1, input_shape=[1])\r\n])\r\nmodel.compile(optimizer=tf.optimizers.Adam(),\r\n              loss=tf.losses.MeanSquaredError())\r\n\r\nfor i in range(1, 300):\r\n    model.predict_on_batch(np.ones([i, 1]))\r\n```\r\n\r\ngives\r\n\r\n```\r\nWARNING:tensorflow:5 out of the last 5 calls to <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f2185a37630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nWARNING:tensorflow:6 out of the last 6 calls to <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f2185a37630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n...\r\n```\r\n\r\nThe same issue happend with `train_on_batch` and `test_on_batch`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `{predict,train,test}_on_batch` methods are traced with undefined batch size.\r\n\r\nAlternatively, the behaviour could be configurable. Each of `{predict,train,test}_on_batch` could take `undefined_batch_size=True/False` argument; personally I vote for a default value of `undefined_batch_size=True`.\r\n\r\n**And I just realized:** if `*_on_batch` methods are used with sequences (for example sentences in NLP), then the retracing would happen on every call. So instead, it would be better to allow passing `experimental_relax_shapes` to the `*_on_batch` or even to `.compile` call.\r\n\r\n**Code to reproduce the issue**\r\nAvailable above.\r\n\r\n**Other info / logs**", "comments": ["Issue replicating for the given code for TF version 2.1rc0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/1e2b044a36f464442c0c5d41437de4fc/34907.ipynb) of colab.Thanks!", "That's not good. \r\n\r\nThanks for providing such a simple and clean repro of the issue. You are correct; passing `experimental_relax_shapes` when we make the function (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_v2_utils.py#L122) resolves the issue. Would you like to make the fix PR? (Otherwise I'm happy to do it.)", "I have created the PR -- however it is just a \"raw\" change. Maybe some tests are also required? But I do not know to do them.\r\n\r\nAlso, the PR is against the master, but maybe it should be backported for TF2.1? Otherwise the final TF2.1 will have suboptimal _on_batch methods.", "PR, for reference: https://github.com/tensorflow/tensorflow/pull/35019\r\n\r\n@goldiegadde @martinwicke For getting it into 2.1\r\n@yhliang2018 @bitfort We should be able to test this in microbenchmarks. (`experimental_relax_shapes` doesn't change semantics, only performance.)", "@foxik once the PR is merged into master, please request a CP to r2.1 branch. \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34907\">No</a>\n", "@goldiegadde The PR has been merged into master as  f835a4a, so I am asking for a CP to r2.1. Is it enough to ask here or should I do something else?", "I can handle the cherrypick. (I have another cherrypick request anyway, so adding one more is not any more work.) Thanks for the report and fix; this will greatly improve 2.1.", "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0rc2\r\n- Python version: 3.7.4\r\n\r\nWith the code above, I get the retracing warnings again in TF2.2.0rc2:\r\nWARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbc0061f5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n", "@robieta Unfortunately, it again does not work with TF 2.2.0rc3", "@foxik I'm no longer part of TensorFlow, but I'm sure @omalleyt12 can help you out. (This seems to have been lost when https://github.com/tensorflow/tensorflow/commit/10666c59dd4858645d1b03ce01f4450da80710ec dropped `training_v2_utils.py`, from the cursory skim that I did.)", "Any update on the issue?"]}, {"number": 34906, "title": "tensorflow gpu inference  only one thread is busy", "body": " PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\r\n  **5708 root      20   0 21.518g 1.820g 794224 S 82.0  0.7   6:38.31 server**\r\n  5722 root      20   0 21.518g 1.820g 794224 S  8.0  0.7   0:36.46 server\r\n  5747 root      20   0 21.518g 1.820g 794224 S  8.0  0.7   0:36.61 server\r\n  5712 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.35 server\r\n  5714 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.49 server\r\n  5716 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.57 server\r\n  5718 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.36 server\r\n  5721 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.52 server\r\n  5723 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.42 server\r\n  5724 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.47 server\r\n  5725 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.60 server\r\n  5726 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.48 server\r\n  5728 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.35 server\r\n  5730 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.46 server\r\n  5731 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.37 server\r\n  5734 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.42 server\r\n  5736 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.43 server\r\n  5737 root      20   0 21.518g 1.820g 794224 S  7.7  0.7   0:36.35 server\r\n  5741 root      20   0 21.518g 1.820g 794224 R  7.7  0.7   0:36.58 server\r\n\r\ntensorflow gpu inference  only one thread is busy, and the inference is so slow ,and the gpu useage is low, something bug?\r\n\r\ntensorflow gpu version is 1.13.1", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "![image](https://user-images.githubusercontent.com/7404433/70619013-18a9d980-1c4f-11ea-9ffe-3f5c9a048b4a.png)\r\n![image](https://user-images.githubusercontent.com/7404433/70619031-22cbd800-1c4f-11ea-9fe3-b360db148978.png)\r\n\r\ninstall a binary  from here, (also try the version 1.15.0, same performance)\r\n![image](https://user-images.githubusercontent.com/7404433/70619174-658db000-1c4f-11ea-99e7-9c8d6412be60.png)\r\nhttps://www.tensorflow.org/install/lang_c\r\n\r\noperating system is Ubuntu VERSION=\"16.04.6 LTS (Xenial Xerus)\"\r\n\r\n\r\nis  single thread tensorflow communicating with GPU  ?\r\n", "@gao8954 Can you please follow the `gpu` usage guide [here](https://www.tensorflow.org/guide/gpu) and let us know whether it resolved your issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34905, "title": "`experimental_relax_shapes` argument of `tf.function` does not work on instance methods", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0, 2.1.0rc0\r\n- Python version: 3.5.3\r\n\r\n**Describe the current behavior**\r\nConsider the following source:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass A:\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def f(self, data):\r\n        return tf.reduce_sum(data)\r\n\r\n@tf.function(experimental_relax_shapes=True)\r\ndef f(data):\r\n    return tf.reduce_sum(data)\r\n\r\na = A()\r\nfor i in range(100):\r\n    print(a.f(np.ones(i)))\r\n```\r\n\r\nThen all calls of `a.f` cause retracing of A.f, even with `experimental_relax_shapes=True`.\r\n\r\nNote that if\r\n```python\r\n    print(f(np.ones(i)))\r\n```\r\nis used instead of `a.f`, the `experimental_relax_shapes` kicks in and the retracing stops after third call to `f`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `experimental_relax_shapes` should work also on instance method.\r\n\r\n**Code to reproduce the issue**\r\n\r\nGiven above.\r\n\r\n**Other info / logs**\r\n\r\nThe problem is that when creating the instance method wrapper in\r\n  https://github.com/tensorflow/tensorflow/blob/746e018d181f52b04f77811b1fbdf9bccdbd1d83/tensorflow/python/eager/function.py#L3179\r\nseveral parameters including experimental_relax_shapes are not copied, see\r\n  https://github.com/tensorflow/tensorflow/blob/746e018d181f52b04f77811b1fbdf9bccdbd1d83/tensorflow/python/eager/function.py#L3221-L3225", "comments": ["I have also created the trivial pull request which passes `experimental_relas_shapes` to the wrapped bound methods.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34905\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34905\">No</a>\n"]}, {"number": 34904, "title": "TF Dataset - group_by_window without iterating the dataset first (input_window_size)", "body": "I have described this in StackOverflow, but may be I have to post my question also here:\r\nhttps://stackoverflow.com/questions/59216451/tf-data-group-by-window-without-iterating-the-complete-dataset-first\r\n\r\n`tf.data.experimental.group_by_window` seems to always iterate the complete Dataset before outputing something. Is this an implementation issue, or am I doing something wrong?\r\n\r\nI'm trying to reduce consecutive frames of fixed size into a single tensor grouped by the example they belong to, i.e. having an input dataset like:\r\n```\r\nid | feature_frame\r\n------------------\r\n 0 | [0, 1, 2, 3]\r\n 0 | [4, 3, 2, 1]\r\n 1 | [3, 1, 0, 0]\r\n 2 | [7, 7, 1, 2]\r\n 2 | [2, 7, 1, 2]\r\n 2 | [4, 7, 1, 3]\r\n```\r\n\r\nhow to turn it into (without iterating to the end of the dataset):\r\n```\r\nid| features_batch\r\n------------------\r\n0 | [[0,1,2,3],[4,3,2,1]]\r\n1 | [[3,1,0,0]]\r\n2 | [[7,7,1,2],[2,7,1,2],[4,7,1,3]]\r\n```\r\n\r\n@jsimsa  - you have answer similar questions (#30585) in the past, It would be great if you can help here!\r\n\r\nSource code to reproduce (change the `max_len=100` in `ReducerTestCase.test_reducer()` to make it work as expected):\r\n```python\r\n\r\n\r\nclass FramesDS:\r\n\r\n    def example_frames_ds(self, max_len=None):\r\n        def gen():\r\n            example_count = 0\r\n            while True:\r\n                example_id = np.random.randint(low=np.iinfo(np.int64).min,\r\n                                               high=np.iinfo(np.int64).max, dtype='int64')\r\n                frame_count = np.random.randint(low=2, high=10)\r\n                print(\"{:4d}| {}: frame_count:{}\".format(example_count, example_id, frame_count))\r\n                example_count += 1\r\n\r\n                max_seq_len = 8\r\n                frames  = np.random.randint(low=0, high=9, size=(frame_count, max_seq_len))\r\n                example_ids = [example_id] * frame_count\r\n                yield example_ids, frames\r\n        ds = tf.data.Dataset.from_generator(gen,\r\n                                            output_types=(tf.int64, tf.int32),\r\n                                            output_shapes=(tf.TensorShape([None, ]), tf.TensorShape([None, None])))\r\n        if max_len is not None:\r\n            ds = ds.take(max_len)\r\n        return ds\r\n\r\n\r\nclass ReducerTestCase(unittest.TestCase):\r\n    def test_reducer(self):\r\n\r\n        max_len = None   # set to 100 to make it work\r\n\r\n        ds = FramesDS().example_frames_ds(max_len)\r\n\r\n        def key_fn(example_id, frame):\r\n            return example_id\r\n\r\n        def init_fn(example_id):\r\n            return example_id, tf.zeros([0,], dtype=tf.int32)\r\n\r\n        def reduce_fn(state, rinput):\r\n            state_eid, frames = state\r\n            example_id, frame = rinput\r\n            tf.assert_equal(state_eid, example_id)\r\n            frames = tf.concat([tf.reshape(frames, (tf.shape(frames)[0],\r\n                                                    tf.shape(frame)[-1])),\r\n                                tf.expand_dims(frame, axis=0)], axis=0)\r\n            return example_id, frames\r\n\r\n        def fin_fn(example_id, frames):\r\n            return example_id, frames\r\n\r\n        reducer = tf.data.experimental.Reducer(init_func=init_fn,\r\n                                               reduce_func=reduce_fn,\r\n                                               finalize_func=fin_fn)\r\n\r\n        ds = ds.unbatch().batch(8)\r\n        ds = ds.unbatch()\r\n\r\n        def window_reduce_fn(key, ds):\r\n            ds = ds.apply(tf.data.experimental.group_by_reducer(key_func=key_fn, reducer=reducer))\r\n            return ds\r\n\r\n        ds = ds.apply(tf.data.experimental.group_by_window(key_func=key_fn,\r\n                                                           reduce_func=window_reduce_fn,\r\n                                                           window_size=20))\r\n\r\n        for example_id, frames in tqdm(ds):\r\n            print(\"{}: {}\".format(example_id, frames.shape))\r\n\r\n```", "comments": ["`group_by_window` does not iterate through the entire dataset before outputing something. It iterates through enough of the dataset for one of the windows to reach `window_size`. The following example illustrates that:\r\n\r\n```\r\nimport itertools\r\nimport tensorflow.compat.v2 as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\ndef gen():\r\n  for i in itertools.count(1):\r\n    print(\"generate: \", i)\r\n    yield i\r\n\r\nds = tf.data.Dataset.from_generator(gen, (tf.int64), (tf.TensorShape([])))\r\n\r\ndef key_fn(x):\r\n  return x % 2\r\n\r\ndef reduce_fn(key, ds):\r\n  return ds.batch(batch_size=2)\r\n\r\nds = ds.apply(tf.data.experimental.group_by_window(key_fn, reduce_fn, window_size=2))\r\n\r\nfor value in ds.take(4):\r\n  print(\"output: \", value.numpy())\r\n```\r\n\r\nproduces the following output:\r\n\r\n```\r\n('generate: ', 1)\r\n('generate: ', 2)\r\n('generate: ', 3)\r\n('output: ', array([1, 3]))\r\n('generate: ', 4)\r\n('output: ', array([2, 4]))\r\n('generate: ', 5)\r\n('generate: ', 6)\r\n('generate: ', 7)\r\n('output: ', array([5, 7]))\r\n('generate: ', 8)\r\n('output: ', array([6, 8]))\r\n```", "@jsimsa - thank you, for the clarification! In a sense the `window_size`  refers to the size of the output.\r\nIn my case above the `window_size` is bigger than any of the grouped features, therefore `group_by_window` would wait until the end of the iterator before outputting an incomplete window (i.e. in a sense all outputs are the last window which is of size less then `window_size`, and could be returned only at the end of the iterator).\r\n\r\nIs there some way, to output a reduction once a window of a given size in the input has been processed? \r\nSomething similar to the `group_by_window`, but with `window_size` referring to the input (i.e. output a reduction once a key has not been seen for `window_size` inputs).\r\n\r\nOr as in my example above, how to convert a dataset of `(id, feature_frame)` with consecutive frames for each id (i.e. `[1,1,0,4,4,4,4,5,9,9,9,...]`) into a `(id, all_id_features_batch)` (i.e, ids would be `[1,0,4,5,9...]`)?\r\n", "Assuming your ids are consecutive, you could do this with `scan` as follows:\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport tensorflow.compat.v2 as tf\r\n\r\ntf.enable_v2_behavior()\r\n\r\nds = tf.data.Dataset.from_tensor_slices(([0, 0, 0, 1, 1, 2, 2, 2, 2, -1], [1, 2, 3, 1, 2, 1, 2, 3, 4, 1]))\r\n\r\nempty_batch = tf.constant([], tf.int32, shape=[0,])\r\ninitial_state = (-1, empty_batch)\r\n\r\ndef scan_func(old_state, input_element):\r\n  current_id, accumulated_batch = old_state\r\n  id, feature = input_element\r\n\r\n  def _accumulate():\r\n    new_accumulated_batch = tf.concat([accumulated_batch, [feature]], 0)\r\n    new_state = (id, new_accumulated_batch)\r\n    return new_state, (current_id, empty_batch)\r\n  \r\n  def _accumulate_and_emit():\r\n    new_state = (id, tf.concat([empty_batch, [feature]], 0))\r\n    return new_state, (current_id, accumulated_batch)\r\n  \r\n  return tf.cond(tf.math.logical_or(current_id == id, current_id == -1), _accumulate, _accumulate_and_emit)\r\n\r\nds = ds.apply(tf.data.experimental.scan(initial_state, scan_func))\r\nds = ds.filter(lambda id, batch: tf.shape(batch)[0] > 0)\r\n\r\nfor elem in ds:\r\n  id, batch = elem\r\n  print(id.numpy(), batch.numpy())\r\n```\r\n\r\nwhich produces:\r\n\r\n```\r\n0 [1 2 3]\r\n1 [1 2]\r\n2 [1 2 3 4]\r\n```\r\n\r\nNote that you need to add a \"sentinel\" value to the end of your dataset to make sure the last batch is emitted.", "@jsimsa - wow, you are the one! I very much appreciate your help! This seems to be what I'm looking for! You saved my day, really! Thank you!"]}, {"number": 34903, "title": "TransposeConv with Bias", "body": "### Description of issue:\r\nFor generating .tflite file with TFLiteConverter, when model contains\r\nConv2DTranspose layers, bias cannot fold into Operator TRANSPOSECONV.\r\nIt will result with extra Op ADD following Op TRANSPOSECONV.\r\nBut with other CONV-like layers (Conv2D, DepthwiseConv2D),\r\nbias will be fold into CONV layer.\r\n(check detailed TF issue: https://github.com/tensorflow/tensorflow/issues/34622)\r\n\r\n### PR try to resolve it:\r\nSo we try to resolve this issue by enable TransposeConv with bias for TFLite:\r\n  \r\n- Update TFLite graph_transform features with:\r\n      fill TransposeConv bias with zero if there is no bias\r\n      fuse bias add into preceding TransposeConv(TEST added)\r\n\r\n- Update TransposeConv with bias:\r\n      add bias input to TransposeConv\r\n      add optional bias to TransposeConv kernels\r\n\r\n### example of the results:\r\n  TRANSPOSE_CONV inputs:\r\n\r\n1. output_shape\r\n2. weights\r\n3. activation\r\n4. bias\r\n\r\n![fused_transposeconv](https://user-images.githubusercontent.com/55463253/70334128-bc088200-183c-11ea-9f94-a803cc80df99.png)\r\n\r\n### Need to discuss:\r\n~~currently this PR only update reference kernel for transpose_conv, optimised kernal is commented out.~~\r\n~~several TEST need to be added as well, but~~ further suggestions are needed for adding additional test.", "comments": ["Hi, thanks for the proposed CL. Until we have an optimized version of the fused bias, I'm not sure this is worth it. Whenever we change op semantics like this (e.g., adding a new attribute, or a new fused input), it impacts all delegates which support a given op. So we would also need to update our nnapi_delegate, gpu_delegate, etc... instances to handle the fused bias.\r\n\r\nThat may not be a bad thing, but I wonder how much that will really buy us in terms of performance? Have you measured the difference? transpose_conv isn't as common as those other operators, so extracting the absolutely maximal performance (like fusing the bias) isn't quite as critical.", "@psunn Could you please check jdduke's comments and keep us posted. Thanks!", "@jdduke thanks for quick replying.\r\n@gbaned thanks for remind me to reply.\r\nI was in a week-long conference, sorry for late reply.\r\n\r\n>Until we have an optimized version of the fused bias, I'm not sure this is worth it.\r\n\r\nI'll update code patch, as soon as I can.\r\n\r\n>That may not be a bad thing, but I wonder how much that will really buy us in terms of performance? Have you measured the difference?\r\n\r\nI meet TransposeConv with Bias when I playing with [FSRCNN](https://arxiv.org/pdf/1608.00367.pdf), so I will try to measuring regarding this model.", "> Until we have an optimized version of the fused bias, I'm not sure this is worth it.\r\n\r\nI've updated the patch with optimized Ops. Please have a review.\r\n\r\n> So we would also need to update our nnapi_delegate, gpu_delegate, etc... instances to handle the fused bias.\r\n\r\nLooks like TransposeConv for nnapi [[1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L2436)] and gpu_delegate [[1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl/kernels/transpose_conv.cc#L83)] has bias inside, I think it will be a minor update.", "> >So we would also need to update our nnapi_delegate, gpu_delegate, etc... instances to handle the fused bias.\r\n\r\n>Looks like TransposeConv for nnapi [1] and gpu_delegate [1] has bias inside, I think it will be a minor update.\r\n\r\nI think it's much easier from your side to push the changes regarding delegates, do you think so?\r\n\r\nIf it's must be pushed from us, shall we resolve it in a follow-up&separate PR, what do you think?", "I've update the patch with new test, please have a review.", "Thanks for review!\r\n\r\n>     2\\. add an entry in lite/tools/versioning/op_version.cc for the new op version, and set it to kPendingReleaseOpVersion\r\n\r\nI guess you mean lite/toco/versioning/tflite/op_version.cc?\r\n\r\nIn lite/toco/versioning/tflite/op_version.cc, shall I add as:\r\n```cpp\r\n{{OperatorType::kTransposeConv, 1}, \"1.9.0\"},\r\n{{OperatorType::kTransposeConv, 3}, kPendingReleaseOpVersion}\r\n```\r\nor  {{OperatorType::kTransposeConv, 2}, \"x.x.x\"} is also needed?", "Good catch, we should add `{{OperatorType::kTransposeConv, 2}, kPendingReleaseOpVersion}` as well, thanks!", "I've update the patch, please have a review.", "@jdduke Can you have a review please?", "I'll be OOO for a few days, will review upon my return early next week (or I'll find somebody else to help review). Thanks again for your patience.", "@jdduke sorry to bother you, I'm wondering is there any chance for some progress regarding this PR this week?", "@suharshs can you validate the quantization-specific changes? Thanks.", "I've rebased on master, so that pip-package can build without error.", "@suharshs sorry to bother you, can you have a look this PR.\r\n\r\nIf nothing wrong, shall we try to merge it soon?", "Can we merge this? Will add test for post-training quantization tooling in separate PR.", "Hi, any chance to merge this PR?", "Sorry for the delay, there are some internal conflicts that we have to resolve manually, hoping to resolve in the next day or two.", "Hi @jdduke @suharshs , can you please review and hopefully re-approve please. I rebased this PR on latest master and resolved conflict. Thanks.", "Hi peng, this is messing with some internal tests that I am cleaning up.\nShould be merged once that is done. Thanks!\n\nOn Fri, Feb 21, 2020, 3:37 AM Peng Sun <notifications@github.com> wrote:\n\n> @psunn <https://github.com/psunn> requested your review on: #34903\n> <https://github.com/tensorflow/tensorflow/pull/34903> TransposeConv with\n> Bias.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34903?email_source=notifications&email_token=AALCE5VSSOH4AFOXDJZZOFTRD64I3A5CNFSM4JW2PHR2YY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOWZTNKEA#event-3060192528>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALCE5WUZTP5HU3MRPYEVXTRD64I3ANCNFSM4JW2PHRQ>\n> .\n>\n", "Hi @jdduke @suharshs ,\r\nIs it ok to re-approval this PR?\r\nHopefully merged soon, if no issues.", "I am still working through some internal tests that are failing. Will merge\nthe pr once those are resolved.\n\nOn Wed, Mar 18, 2020 at 3:49 PM Peng Sun <notifications@github.com> wrote:\n\n> Hi @jdduke <https://github.com/jdduke> @suharshs\n> <https://github.com/suharshs> ,\n> Is it ok to re-approval this PR?\n> Hopefully merged soon, if no issues.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34903#issuecomment-600897142>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALCE5R57S2KDJJSB4USZGDRIFFWFANCNFSM4JW2PHRQ>\n> .\n>\n", "Just an update here: there is an issue i am running into with flatbuffer import in the mlir code path. I am trying to get that fixed, but the optional tensor support has trickled into a few things, will update here when that is resolved. thanks for your patience.", "No that's ok, was just wondering. Thanks\n\nOn Fri, Apr 3, 2020, 1:43 PM Peng Sun <notifications@github.com> wrote:\n\n> *@psunn* commented on this pull request.\n> ------------------------------\n>\n> In\n> tensorflow/lite/toco/graph_transformations/fuse_binary_into_preceding_affine.cc\n> <https://github.com/tensorflow/tensorflow/pull/34903#discussion_r403320402>\n> :\n>\n> >      AddMessageF(\n>          \"Not fusing %s because the preceding %s is not of one of the supported \"\n>          \"types\",\n>          LogName(*binary_op), LogName(*preceding_op));\n>      return ::tensorflow::Status::OK();\n>    }\n>\n> +  if (preceding_op->type == OperatorType::kTransposeConv &&\n>\n> I was intend to fused kAdd to kTransposeConv.\n> Remove if statement, kMul, kSub, kDiv may also be fused to kTransposeConv,\n> which is also good for graph optimise.\n>\n> Shall I remove it?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34903#discussion_r403320402>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALCE5WP7QBYYUXWYQCPRZLRKZC6VANCNFSM4JW2PHRQ>\n> .\n>\n", "@psunn Could you please resolve the conflicts? Thanks!", "> @psunn Could you please resolve the conflicts? Thanks!\r\n\r\n@gbaned done, thanks for the reminder.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 34902, "title": "Micro: Add the missing TF_LITE_STATIC_MEMORY-macro to mbed_app.json.tpl.", "body": "Compiling Tensorflow Lite Micro with Arm Mbed OS fails due to a missing compile flag. This PR adds that compile flag to the mbed_app.json template file.", "comments": ["Thanks Jens.\r\n\r\nWe have an internal change under review that will enable building without the TF_LITE_STATIC_MEMORY macro. I will update this conversation once that lands.", "With commit 55446fa the macro should no longer be needed. I am closing this PR, please let us know if you run into any issues.", "Great, thanks for the quick response! I'll let you know if we see any problems."]}, {"number": 34901, "title": "keras saved model cannot be loaded (no custom layer)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.2\r\n- CUDA/cuDNN version: 10.0.130 / 7.6.1\r\n- GPU model and memory: GTX 1060 MAX-Q 6GB\r\n\r\n**Describe the current behavior**\r\n\r\nA keras `Sequential` model trained and saved cannot be re-loaded by `tf.keras.models.load_model`.\r\nThe complete error message:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    540     try:\r\n--> 541       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    542     except TypeError:\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py in <listcomp>(.0)\r\n    540     try:\r\n--> 541       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    542     except TypeError:\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\util\\compat.py in as_bytes(bytes_or_text, encoding)\r\n     70     raise TypeError('Expected binary or unicode string, got %r' %\r\n---> 71                     (bytes_or_text,))\r\n     72\r\n\r\nTypeError: Expected binary or unicode string, got -1\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-65-6d5b848d58f1> in <module>\r\n----> 1 tf.keras.models.load_model(\"model.h5\")\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py in load_model(filepath, custom_objects, compile)\r\n    144   if (h5py is not None and (\r\n    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\r\n--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n    147\r\n    148   if isinstance(filepath, six.string_types):\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    166     model_config = json.loads(model_config.decode('utf-8'))\r\n    167     model = model_config_lib.model_from_config(model_config,\r\n--> 168                                                custom_objects=custom_objects)\r\n    169\r\n    170     # set weights\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py in model_from_config(config, custom_objects)\r\n     53                     '`Sequential.from_config(config)`?')\r\n     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n---> 55   return deserialize(config, custom_objects=custom_objects)\r\n     56\r\n     57\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py in deserialize(config, custom_objects)\r\n    100       module_objects=globs,\r\n    101       custom_objects=custom_objects,\r\n--> 102       printable_module_name='layer')\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    189             custom_objects=dict(\r\n    190                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 191                 list(custom_objects.items())))\r\n    192       with CustomObjectScope(custom_objects):\r\n    193         return cls.from_config(cls_config)\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py in from_config(cls, config, custom_objects)\r\n    368       layer = layer_module.deserialize(layer_config,\r\n    369                                        custom_objects=custom_objects)\r\n--> 370       model.add(layer)\r\n    371     if not model.inputs and build_input_shape:\r\n    372       model.build(build_input_shape)\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py in add(self, layer)\r\n    194       # If the model is being built continuously on top of an input layer:\r\n    195       # refresh its output.\r\n--> 196       output_tensor = layer(self.outputs[0])\r\n    197       if len(nest.flatten(output_tensor)) != 1:\r\n    198         raise TypeError('All layers in a Sequential model '\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    840                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    841                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 842                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    843                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    844                     # circular dependencies.\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\pooling.py in call(self, inputs, mask)\r\n    641       input_shape = inputs.shape.as_list()\r\n    642       broadcast_shape = [-1, input_shape[steps_axis], 1]\r\n--> 643       mask = array_ops.reshape(mask, broadcast_shape)\r\n    644       inputs *= mask\r\n    645       return backend.sum(inputs, axis=steps_axis) / math_ops.reduce_sum(\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py in reshape(tensor, shape, name)\r\n    129     A `Tensor`. Has the same type as `tensor`.\r\n    130   \"\"\"\r\n--> 131   result = gen_array_ops.reshape(tensor, shape, name)\r\n    132   tensor_util.maybe_set_static_shape(result, shape)\r\n    133   return result\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py in reshape(tensor, shape, name)\r\n   8115   # Add nodes to the TensorFlow graph.\r\n   8116   _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 8117         \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n   8118   _result = _op.outputs[:]\r\n   8119   _inputs_flat = _op.inputs\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    528           except TypeError as err:\r\n    529             if dtype is None:\r\n--> 530               raise err\r\n    531             else:\r\n    532               raise TypeError(\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    525                 dtype=dtype,\r\n    526                 as_ref=input_arg.is_ref,\r\n--> 527                 preferred_dtype=default_dtype)\r\n    528           except TypeError as err:\r\n    529             if dtype is None:\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\r\n   1294\r\n   1295     if ret is None:\r\n-> 1296       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1297\r\n   1298     if ret is NotImplemented:\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    284                                          as_ref=False):\r\n    285   _ = as_ref\r\n--> 286   return constant(v, dtype=dtype, name=name)\r\n    287\r\n    288\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    225   \"\"\"\r\n    226   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 227                         allow_broadcast=True)\r\n    228\r\n    229\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    263       tensor_util.make_tensor_proto(\r\n    264           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 265           allow_broadcast=allow_broadcast))\r\n    266   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    267   const_tensor = g.create_op(\r\n\r\nc:\\users\\kylec\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    543       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n    544                       \"Contents: %s. Consider casting elements to a \"\r\n--> 545                       \"supported type.\" % (type(values), values))\r\n    546     tensor_proto.string_val.extend(str_values)\r\n    547     return tensor_proto\r\n\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [-1, None, 1]. Consider casting elements to a supported type.\r\n```\r\n\r\nAlso if I switch to use functional API the result remains the same.\r\n\r\n**Describe the expected behavior**\r\n\r\nA saved model should be able to re-loaded without error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\r\n                                                      num_words=None,\r\n                                                      skip_top=0,\r\n                                                      maxlen=None,\r\n                                                      seed=113,\r\n                                                      start_char=1,\r\n                                                      oov_char=2,\r\n                                                      index_from=3)\r\n\r\nx_train = pad_sequences(x_train, padding=\"post\")\r\nmaxlen = x_train.shape[1]\r\nvocab_size = x_train.max() + 1\r\n\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Input(shape=(maxlen,), name=\"sequence\"),\r\n  tf.keras.layers.Embedding(vocab_size, 32, mask_zero=True, name=\"word_embedding\"),\r\n  tf.keras.layers.GlobalAveragePooling1D(name=\"doc_embedding\"),\r\n  tf.keras.layers.Dense(16, activation=\"relu\", name=\"relu\"),\r\n  tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")\r\n], name=\"nn_classifier\")\r\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\r\nmetrics = model.fit(x=x_train, y=y_train, batch_size=256, epochs=1)\r\nmodel.save(\"model.h5\")\r\n\r\ntf.keras.models.load_model(\"model.h5\")  # Failed.\r\n```\r\n\r\n", "comments": ["@everdark, \r\nIf you perform the below mentioned changes, it works without any error.  Here is the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/f1a5f088176869349e43c7813f7e6ad0/34901.ipynb#scrollTo=mlBe79JB_hnm)\r\n\r\n1. Save the model using `model.save(\"model\")` instead of `model.save(\"model.h5\")` and \r\n\r\n2.  Load the Model using `tf.keras.models.load_model(\"model\")` instead of `tf.keras.models.load_model(\"model.h5\")`. \r\n\r\nThanks!", "@amahendrakar,\r\nThanks for the workaround.\r\nBut essentially this will save the model in `tf` format. If instead I'd like to save it as a single file `.h5` format it won't work. I prefer `h5` over `tf` format simply because it is neater. :)", "I have figured out the root cause of the error.\r\nIt is the first `Input` layer.\r\nIf I change the model def to be:\r\n\r\n```python\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Embedding(vocab_size, 32, input_length=maxlen, mask_zero=True, name=\"word_embedding\"),\r\n  tf.keras.layers.GlobalAveragePooling1D(name=\"doc_embedding\"),\r\n  tf.keras.layers.Dense(16, activation=\"relu\", name=\"relu\"),\r\n  tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"sigmoid\")\r\n], name=\"nn_classifier\")\r\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\r\nmetrics = model.fit(x=x_train, y=y_train, batch_size=256, epochs=1)\r\nmodel.save(\"model.h5\")\r\n\r\nmodel2 = tf.keras.models.load_model(\"model.h5\")\r\nmodel2.predict(x_train[:10]) # OK.\r\n```\r\n\r\nI think there is something wrong when a model's first layer is `Input`.\r\nAlso in the original model definition even if I save it as `tf` format and successfully load it back, the model is not usable since it will throw error when using either `fit` or `predict` method.\r\nAnd I believe the root cause is the same `Input` presence as the first layer.\r\n\r\nNot sure what is the reason why `Input` is no longer supported as the first layer?", "@everdark This is fixed with TF 2.1.0-rc0 . You can successfully save and load models in ```h5``` format.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34901\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34901\">No</a>\n"]}, {"number": 34900, "title": "[TF2.0] can't model.save(save_format='h5') NotImplementedError", "body": "I cant use ```model.save()``` like this link\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\r\npython=3.7.5\r\ntensorflow-gpu=2.0.0\r\ntransformers=2.2.1\r\n\r\n\r\n```\r\ninput_layer = Input(shape = (512,), dtype='int64') \r\nbert = TFBertModel.from_pretrained('bert-base-chinese')(input_layer)\r\nbert = bert[0]   \r\ndropout = Dropout(0.1)(bert)\r\nflat = Flatten()(dropout)\r\nclassifier = Dense(units=5, activation=\"softmax\")(flat)               \r\nmodel = Model(inputs=input_layer, outputs=classifier)\r\nmodel.summary()\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-6, epsilon=1e-08, clipnorm=1.0)\r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n```\r\n```\r\n>\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 512)]             0         \r\n_________________________________________________________________\r\ntf_bert_model (TFBertModel)  ((None, 512, 768), (None, 102267648 \r\n_________________________________________________________________\r\ndropout_37 (Dropout)         (None, 512, 768)          0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 393216)            0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 5)                 1966085   \r\n=================================================================\r\nTotal params: 104,233,733\r\nTrainable params: 104,233,733\r\nNon-trainable params: 0\r\n```\r\n\r\n```\r\nmodel.save('model/my_model.h5')\r\n```\r\n```\r\n>\r\nNotImplementedError\r\n```\r\nBut I did have InputLayer\r\n\r\n\r\n\r\n\r\n\r\nI have no choice to use ```tf.keras.models.save_model()```\r\n```\r\ntf.keras.models.save_model(\r\n    model,\r\n    \"model/model_bert_eland_softmax_2\",\r\n    overwrite=True,\r\n    include_optimizer=True,\r\n)\r\n```\r\n\r\n\r\n\r\nBut when I loaded it,  I have to add an inputlayer and I losed my model layers structures.\r\n```\r\ninput_layer = Input(shape = (512,), dtype='int64')  \r\nload_model = tf.keras.models.load_model('model/model_bert_eland_softmax_2')(input_layer)\r\nnew_model = Model(inputs=input_layer, outputs=load_model)\r\n\r\n# Show the model architecture\r\nnew_model.summary()\r\n```\r\n```\r\n>\r\nModel: \"model_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 512)]             0         \r\n_________________________________________________________________\r\nmodel (Model)                (None, 5)                 104233733 \r\n=================================================================\r\nTotal params: 104,233,733\r\nTrainable params: 104,233,733\r\nNon-trainable params: 0\r\n```\r\n\r\nIs it a bug?\r\n\r\nIs it possible to use ```tf.keras.models.save_model()``` or ```model.save()```\r\nand still keep all layers of my model like this link?\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model", "comments": ["@roccqqck I created your model and loaded again without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d067175a157d12ac60e4f542909fe9ee/untitled696.ipynb). I have used `tf-nightly` and saved model in \"tf\" format. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "> @roccqqck I created your model and loaded again without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d067175a157d12ac60e4f542909fe9ee/untitled696.ipynb). I have used `tf-nightly` and saved model in \"tf\" format. Thanks!\r\n\r\nif i use tensorflow 2.0.0, it showed error.\r\nbut if i use tfnightly like u did, it worked fine.\r\n\r\nhowever, h5 format still do not work in tfnightly.\r\n\r\n\r\nshould i close it?\r\n\r\n\r\ndoes it mean h5 wont be supported anymore?", "@roccqqck we still support `h5` format. But, default model saving format is `tf` and this format works well with most of the new functionality. Thanks!", "@jvishnuvardhan \r\nI just found out another issue\r\nThe loaded model won't show output shape\r\n\r\n\r\nThe BERTmodel I defined above.\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 512)]             0         \r\n_________________________________________________________________\r\ntf_bert_model (TFBertModel)  ((None, 512, 768), (None, 102267648 \r\n_________________________________________________________________\r\ndropout_37 (Dropout)         (None, 512, 768)          0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 393216)            0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 5)                 1966085   \r\n=================================================================\r\nTotal params: 104,233,733\r\nTrainable params: 104,233,733\r\nNon-trainable params: 0\r\n```\r\n\r\n```\r\nmodel.save(\"my_model\",save_format='tf')\r\nloaded_model = tf.keras.models.load_model(\"my_model\")\r\nloaded_model.summary()\r\n```\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 512)]             0         \r\n_________________________________________________________________\r\ntf_bert_model (TFBertModel)  multiple                  102267648 \r\n_________________________________________________________________\r\ndropout_37 (Dropout)         multiple                  0         \r\n_________________________________________________________________\r\nflatten (Flatten)            multiple                  0         \r\n_________________________________________________________________\r\ndense (Dense)                multiple                  1966085   \r\n=================================================================\r\nTotal params: 104,233,733\r\nTrainable params: 104,233,733\r\nNon-trainable params: 0\r\n```\r\n\r\n\r\nBut in another case, it showed the same output shape\r\n\r\n```\r\n    model = Sequential()\r\n    model.add(Dense(units=64, input_dim=23, kernel_initializer='uniform', activation='relu'))\r\n    model.add(Dense(units=32, activation='relu'))\r\n    model.add(Dropout(0.2))\r\n    model.add(Dense(units=2, activation='softmax'))\r\n    model.summary()\r\n```\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_1 (Dense)              (None, 64)                1536      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 32)                2080      \r\n_________________________________________________________________\r\ndropout_38 (Dropout)         (None, 32)                0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 2)                 66        \r\n=================================================================\r\nTotal params: 3,682\r\nTrainable params: 3,682\r\nNon-trainable params: 0\r\n```\r\n```\r\nmodel.save(\"my_model\",save_format='tf')\r\nloaded_model = tf.keras.models.load_model(\"my_model\")\r\nloaded_model.summary()\r\n```\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_1 (Dense)              (None, 64)                1536      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 32)                2080      \r\n_________________________________________________________________\r\ndropout_38 (Dropout)         (None, 32)                0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 2)                 66        \r\n=================================================================\r\nTotal params: 3,682\r\nTrainable params: 3,682\r\nNon-trainable params: 0\r\n```", "@roccqqck That `multiple` is expected. Please check the detailed explanation [here](https://github.com/tensorflow/tensorflow/issues/29132#issuecomment-497421178). That link also has alternate workarounds to infer shape. Thanks!", "@roccqqck BERT model is a custom model, and doesn't have a config defined. I think you need to add a custom_object as a config to BERT model? \r\n\r\nThis is more related to transformers repository. Please post the issue [here](https://github.com/huggingface/transformers/issues). Thank!\r\n\r\nClosing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}, {"number": 34899, "title": "Fit of Undefined in train TensorFlowJS - Transfer learning audio recognizer", "body": "In the example of 'Transfer learning audio recognizer' there is an error on lines.\r\n**Uncaught (in promise) TypeError: Cannot read property 'fit' of undefined**\r\n\r\nthere is an error highlighted like shown :\r\n\r\n![Ekran Resmi 2019-12-06 17 09 11](https://user-images.githubusercontent.com/25597808/70328972-566fc200-184b-11ea-9add-c31964511b35.png)\r\n", "comments": ["@burakberber ,\r\nPlease raise the issue related to TF.js in this [repo](https://github.com/tensorflow/tfjs/issues/new).Thanks!"]}, {"number": 34897, "title": "TFLITE C++ ", "body": "Hi,\r\n\r\nWe are able to run TFLITE CPP application(label_image) on ARM processor which uses CPU.\r\nCan we run same TFLITE CPP application on any GPU like NVIDIA, POWERVR.\r\n\r\nThanks,\r\nRam.", "comments": ["Can you please go through the [link](https://www.tensorflow.org/lite/performance/gpu) and see if it helps you.\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "@thotaram \r\n\r\nAny update on this issue please. Thanks!", "TFLITE C++ application, to run it on GPU(PowerVR) need to use GPU delegate.\r\nWhen i refer web links regarding GPU delegate it supports Android & IOS.\r\nI would like to know GPU delegate supports for ARM64 platforms or not?\r\nPlease suggest any links or inputs to use GPU delegate for ARM.\r\n\r\nThanks,\r\nRam. ", "TFLite GPU team officially only supports Android & iOS.  However, Android & iOS are both arm64 architectures.  So if you can get it to compile, it's possible that it runs out of the box.  Of course, your GPU needs to be support OpenGL ES 3.1"]}, {"number": 34896, "title": "Tensorflow Lite Micro fails to build for Arm Mbed OS", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1c8c3f2015\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n**Describe the problem**\r\nBuidling Tensorflow Lite Micro for Arm Mbed OS results in the following compile error:\r\n`[Fatal Error] scoped_profiling_label_wrapper.h@38,39: profiling/instrumentation.h: No such file or directory\r\n[ERROR] In file included from ./tensorflow/lite/kernels/internal/reference/integer_ops/mul.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/mul.cc:20:\r\n./tensorflow/lite/kernels/internal/scoped_profiling_label_wrapper.h:38:39: fatal error: profiling/instrumentation.h: No such file or directory\r\n #include \"profiling/instrumentation.h\"\r\n                                       ^\r\ncompilation terminated.`\r\n\r\nIt's related to this [commit](https://github.com/tensorflow/tensorflow/commit/27aac0fe54bf121abed62757891ae4580eb12fc5#diff-90548b139ee2b479dbf2f409aef40ff3) that adds some profiling code.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n$ make -f tensorflow/lite/experimental/micro/tools/make/Make generate_network_tester_test_mbed_project\r\n$ cd tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/prj/network_tester_test/mbed\r\n$ mbed config root .\r\n$ mbed deploy\r\n$ mbed compile -m DISCO_F746NG -t GCC_ARM\r\n", "comments": ["The issue is fixed if the TF_LITE_STATIC_MEMORY flag is defined when compiling, like this:\r\n$ mbed compile -m DISCO_F746NG -t GCC_ARM **-D TF_LITE_STATIC_MEMORY**"]}, {"number": 34895, "title": "SavedModel format for tf.estimator class in Tensorflow 2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):  2.0.0\r\n- Python version: 2.7.10 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI'm moving to TF 2.0 with its very nice dataset functionalities, but I got stuck when I want to save the model in the SavedModel format.\r\n\r\nI'm using the estimator class to do a linear regression, and after training this is how I'd set up the export in TF 1:\r\n```\r\ncolumns = [('hour', tf.int64),\r\n           ('domain', tf.string),\r\n           ('device_type', tf.string)]\r\nfeature_placeholders = {\r\n name: tf.placeholder(dtype, [1], name=name + \"_placeholder\")\r\n for name, dtype in columns\r\n}\r\n```\r\nI have three features with different datatypes, and I use the placeholder method to concatenate them into a dict that is then served using the tf.estimator.export.build_raw_serving_input_receiver_fn() method, and finally exported using the estimator.export_saved_model to my model directory:\r\n\r\n```\r\nexport_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\r\n    feature_placeholders)\r\nestimator.export_saved_model(model_dir, export_input_fn)\r\n```\r\nAll tutorials online uses this series of steps, but tf.placeholder() doesn't exist in TF 2.0, so how can I do this?\r\n", "comments": ["In 2.0, you can use tf.compat.v1.placeholder. We have [moved to Keras as the primary high level API](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a), and would encourage you to write your models using Keras if possible. Estimator is fundamentally tied to v1/Sessions, so if you must stay in the Estimator ecosystem, you will likely have to use some amount of compat.v1 functionality.", "Hey Karmel\r\n\r\nThanks for answering!\r\n\r\nCurrently I'm training in TF 2.0 and then serializing in TF 1.x. Seems to be working..", "@jnsgrnborg  have you tried reverse ? training in TF 1.x and serializing in TF 2.0 ? "]}, {"number": 34894, "title": "TFLu: Pointwise subtraction Int8", "body": "", "comments": ["@giuseros Could you please resolve the conflicts? Thanks!", "@petewarden Can you please take a look at this PR? Thanks!", "@giuseros Could you please resolve the conflicts? Thanks!", "Hi @petewarden @njeffrie @freddan80,\r\nI tried to apply your fixes: could you review this please?\r\n", "Hi @petewarden @njeffrie @freddan80,\r\nAny update on this?", "@giuseros Can you please check build failures? Thanks!", "@giuseros Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 34893, "title": "load_weights dont work in tf.keras: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open \"mobilenet_model\": Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.14, 1.15, 2.0, 2.1rc0\r\n- Keras version: 2.2.4-tf and 2.2.4\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0 and 10.1 / 7.6.5\r\n- GPU model and memory: RTX 2060 6GB\r\n\r\nWhen I use the **tf.keras** and try to load the weights from mobilenet modeI get an error\"\r\n`W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open \"mobilenet_model\": Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?`\r\n\r\nWhen use only **keras** it's ok.\r\n\r\n**Simple code to reproduce the issue**\r\n```\r\nimport os\r\n\r\nuse_tfkeras = True\r\n\r\nif use_tfkeras:\r\n    from tensorflow.compat.v1.keras.applications.mobilenet import MobileNet\r\nelse:\r\n    from keras.applications.mobilenet import MobileNet\r\n\r\n\r\ndef save_mobilenet_weights(alpha, filename):\r\n    mobilenet = MobileNet(alpha=alpha, input_tensor=None, include_top=False, weights='imagenet', pooling=None)\r\n    if use_tfkeras:\r\n        mobilenet.save_weights(filepath=os.path.abspath(filename), overwrite=True, save_format='h5')\r\n    else:\r\n        mobilenet.save_weights(filepath=os.path.abspath(filename), overwrite=True)\r\n\r\ndef load_mobilenet_weights(alpha, filename):\r\n    mobilenet = MobileNet(alpha=alpha, input_tensor=None, include_top=False, weights=None, pooling=None)\r\n    mobilenet.load_weights(os.path.abspath(filename))\r\n\r\n\r\nalpha = 0.75\r\nfilename = 'mobilenet_model'\r\n\r\nsave_mobilenet_weights(alpha, filename)\r\nload_mobilenet_weights(alpha, filename)\r\n```\r\n", "comments": ["Related topics:\r\nhttps://github.com/tensorflow/tensorflow/pull/22998\r\nhttps://github.com/tensorflow/models/issues/2676\r\nhttps://github.com/tensorflow/tensorflow/issues/7696\r\nhttps://github.com/tensorflow/serving/issues/1441\r\nhttps://stackoverflow.com/questions/41048819/how-to-restore-a-model-by-filename-in-tensorflow-r12\r\nhttps://github.com/tensorflow/models/issues/2675\r\n\r\n", "@kiflowb777 ,\r\nWhen i set `use_tfkeras = True` was able to run the code successfully on TF-2.1rc0,2.0,1.14, please find the [gist](https://colab.sandbox.google.com/gist/oanush/acbfe415f78f6b3e746eefb11a3c27dc/34893.ipynb) of colab for your reference.Kindly provide gist of colab if the issue faced.Thanks!", "Please run \"Runtime\" -> \"view runtime logs\", the warning can be found there.", "Issue replicating for given code.Thanks!", "The problem seems solved when set the `save_weights_only=True` , like this:\r\n`model_checkpoint = ModelCheckpoint(\r\n            'best_model.ckpt',\r\n            monitor='val_loss',\r\n            save_best_only=True,\r\n            save_weights_only=True)`", "@kiflowb777  Could you please check with latest TF version and  let us know if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34893\">No</a>\n", "Similar problems:\r\nhttps://github.com/tensorflow/models/issues/2676\r\nhttps://github.com/tensorflow/models/issues/2675\r\nhttps://github.com/tensorflow/tensorflow/issues/7696\r\nhttps://stackoverflow.com/questions/48438122/import-meta-graph-fails-with-data-loss-not-an-sstable-bad-magic-number"]}, {"number": 34892, "title": "Sort layers by name to avoid conficts in h5 saving.", "body": "There is a bug while saving .h5 models. In a model, if a layer name is prefix of a previously defined layer name, then h5py will fail with error: \"ValueError: Unable to create group (name already exists)\".\r\n\r\nThis PR, like this one (https://github.com/keras-team/keras/pull/13477) in `keras`, solve this problem.", "comments": ["Bump?", "@fchollet Can you please take a look at this PR? Thanks!", "@vcarpani Can you please resolve conflicts? Thanks!", "This already merged PR resolves this:\r\nhttps://github.com/tensorflow/tensorflow/commit/0e884391beabe2fadb6398b1fc5f48a9662c333c"]}, {"number": 34891, "title": "Cannot execute the substraction op with the broadcast mechanism", "body": "```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nx = tf.constant(np.random.random((500, 6)))\r\ny = x - x[:,0]\r\n```\r\nThis code raises the issue of not supporting [500,6] - [500] in the `sub` OP.\r\nI suppose that it should compute correctly, using the broadcast automatically?\r\nIs it a bug? \r\nversion: tf2.0", "comments": ["Could reproduce the issue with TF version 2.0. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/4c280e1f0c990947b4397089a389ce91/34891.ipynb) here.\r\nThanks!", "Same problem in div op.\r\nPlease have a check on it, too.", "@jvishnuvardhan any progress here?", "could reproduce the issue with TF version 2.2.0-dev20200319. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bebc538e6b1827e4fd2031f2768ec363/untitled739.ipynb#scrollTo=8w0YFR4JHdg6). Thanks!", "@TomorrowIsAnOtherDay I updated part of your code. You could use the following to access just a column and subtract it from the whole tensor. \r\n\r\n```\r\nimport numpy as np\r\nx = tf.constant(np.random.random((500, 6)))\r\nindices = [0]\r\ny = x - tf.gather(x, indices, axis=1)\r\n```\r\n \r\nPlease close the issue if this was resolved for you. Thanks!", "This is an ugly solution. I just want to write the code in a pythonic way, but the solution rolls me back into tf1.0.", "This is consistent with the broadcast semantics of numpy:\r\n\r\n```\r\n>>> x=np.random.random((500, 6))\r\n>>> x-x[:,0]\r\nValueError: operands could not be broadcast together with shapes (500,6) (500,) \r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34891\">No</a>\n"]}, {"number": 34890, "title": "tf.keras.models.Sequential does not support run_eagarly", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-16986-g6c32a22 2.1.0-dev20191029\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n`tf.keras.models.Sequential` doesn't support `run_eagarly` as mentioned in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#run_eagerly).\r\n\r\n**Describe the expected behaviour**\r\nEither Sequential model accepts `run_eagarly` as a param and changes its behaviour, or we modify the docs. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    layers=[tf.keras.layers.Dense(input_shape=(3, ), units=1)], \r\n    run_eagerly=True)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tst.py\", line 5, in <module>\r\n    run_eagerly=True)\r\n  File \"/home/squadrick/.local/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'run_eagerly'\r\n```", "comments": ["@Squadrick,\r\nHello, Please try giving `run_eagerly=True` as a parameter to compile it should work fine, kindly refer the [link](https://www.tensorflow.org/guide/keras/overview#train_and_evaluate).\r\nAlso find the [gist](https://colab.sandbox.google.com/gist/oanush/e1262a3a58d8bf0e6bb4419b361e78d7/34890.ipynb) of colab. Let us know if it helped.Thanks!", "@oanush My issue is more to do with the incorrect documentation. I eventually figured out how to run the model, but the documentation for `tf.keras.Sequential` is completely wrong. And for anyone wanting the same thing, they're more likely to go to the page of `tf.keras.Sequential` than the link you posted.", "@karmel Any update on this?", "Hi @Squadrick, what part of the doc is incorrect? It doesn't say it's an argument to init. But could clarify in the doc string that it should be called in compile. It would be useful to add the param to the .compile method.\r\nCan you send a PR for https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=nightly ?\r\nThe pages have a link \"View source\" to edit the docstring.", "My bad. I figured `Properties` meant things that could be passed to `__init__`. The docs are correct. I'll close this issue, thanks for the help. ", "I'll create the PR soon."]}, {"number": 34889, "title": "documentation", "body": "Porting the original website from bootstrap3  to bootstrap4\r\n\r\nSection to change:\r\n- [ ] [_alumni.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_alumni.html)\r\n- [ ] [_events-participate.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_events-participate.html)\r\n- [ ] [_events.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_events.html)\r\n- [ ] [_home.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_home.html)\r\n- [ ] [_intro.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_intro.html)\r\n- [ ] [_open-source.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_open-source.html)\r\n- [ ] [_team.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_team.html)\r\n- [ ] [_webinars.html](https://github.com/asetalias/asetalias.github.io/blob/master/templates/home-sections/_webinars.html)\r\n\r\n\r\n", "comments": []}, {"number": 34888, "title": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize", "body": "\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/3930650/error_log.txt)\r\nGetting \"Failed to get Convolution aglorithm\" error \r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04**\r\n- TensorFlow installed from (source or binary): **Binary [https://www.tensorflow.org/install/gpu](url)**\r\n- TensorFlow version: **tensorboard==2.0.2, tensorflow-datasets==1.3.0, tensorflow-estimator==2.0.1, tensorflow-gpu==2.0.0, tensorflow-metadata==0.15.1**\r\n- Python version: **Python 3.6.9**\r\n- Installed using virtualenv? pip? conda?: **pip**\r\n- Bazel version (if compiling from source): **Build label: 1.2.1**\r\n- GCC/Compiler version (if compiling from source): **gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0** \r\n- CUDA/cuDNN version: \r\n**`nvidia-smi` command gives me CUDA version as 10.2 and `nvcc --version` command gives me Cuda compilation tools, release 9.1, V9.1.85**\r\n- GPU model and memory: **GeForce GTX 1660 Ti/PCIe/SSE2  and Memory 15.6 GiB**\r\n\r\n**Problem**\r\n`tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\n\r\nEarlier my GPU was not detected and had to go back and forth on drivers and CUDA library versions. I have followed the steps for Ubuntu 18.04 command set in Tensor Flow GPU installation site. I could detect the Graphics card, but having above issue. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI have run one of the image classification example give in TensorFlow tutorials and was getting the above error. I have attached the complete log. \r\n\r\n**Any other info / logs**\r\nIncluded the logs and the code \r\n[image_classifier.txt](https://github.com/tensorflow/tensorflow/files/3930678/image_classifier.txt)\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/3930680/error_log.txt)\r\n\r\n\r\nStuck with this error and not able use the GPU for Tensor Flow computations with above system configuration. Please help me in this regard.\r\n\r\nThanks\r\nViswanath.B", "comments": ["You may try limiting your gpu memory usage with ```set_memory_growth``` option.\r\nSee https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\nPut the following snippet on top of your code and execute again\r\n```python\r\n import tensorflow as tf\r\n gpus= tf.config.experimental.list_physical_devices('GPU')\r\n tf.config.experimental.set_memory_growth(gpus[0], True)\r\n```", "I had this same issue. For me it was a mismatch in the cudnn toolkit version. If you scroll to the top above the traceback you might see something like `Loaded runtime CuDNN library: 7.5.1 but source was compiled with: 7.6.0`. \r\n\r\nSo you need to make sure your CuDNN install is >7.6 which must be what TF2.0 needs. \r\nThis gets you the PPA's and apt install commands to get 7.6: https://www.tensorflow.org/install/gpu\r\n\r\nThen I found this script helpful to manage my cuda/cudnn installs now that I had multiple versions: \r\nhttps://github.com/phohenecker/switch-cuda\r\n`source switch_cuda.sh 10.0`\r\n\r\nI never found `set_memory_growth` to fix this issue, but once I had CUDA10 + CuDNN 7.6 and made sure my PATH and LD_LIBRARY_PATH were all correct with the switcher script it finally worked. Here is a very minimal fashion mnist that I used to verify the problem and fix. \r\n\r\nNote: Don't run this from a notebook at first. Notebooks can spin up their own path/environment. Make sure it runs as a standalone script from the command line first, then figure out the notebook. \r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nfrom tensorflow import keras\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\ntrain_images = np.expand_dims(train_images, axis=3)\r\ntest_images = np.expand_dims(test_images, axis=3)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=2))\r\nmodel.add(tf.keras.layers.Dropout(0.3))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\r\nmodel.summary()\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\nmodel.fit(train_images, train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels))", "Hi Modak & Irick, \r\n\r\nThank you for providing quick help on the issue I am facing. \r\n\r\n@ymodak , I tried to use the `set_memory_growth` but the issue is still persisting. \r\n\r\n@cirick , I have checked the cuda versions with the help of `source switch-cuda.sh` and `source switch-cuda.sh 9.0` commands. \r\n\r\nError is not solved but changed though. \r\n\r\nResults for `source switch-cuda.sh` is below \r\n`$ source switch-cuda.sh`\r\n`The following CUDA installations have been found (in '/usr/local'):`\r\n`* cuda-9.0`\r\n`* cuda-10.0`\r\n`* cuda-10.1`\r\n`* cuda-10.2`\r\n`* cuda-9.0`\r\n\r\nI have switched to latest version as 10.2 and Now the error turns to different one \r\n\r\n`2019-12-08 15:57:07.034647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0`\r\n`2019-12-08 15:57:07.195071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7`\r\n**`2019-12-08 15:57:07.492080: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`**\r\n**`2019-12-08 15:57:07.512555: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`**\r\n`2019-12-08 15:57:07.512627: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\n`\t [[{{node sequential/conv2d/Conv2D}}]]`\r\n`   64/60000 [..............................] - ETA: 13:29Traceback (most recent call last):`\r\n`  File \"2_test.py\", line 27, in <module>\r\n    model.fit(train_images, train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels))`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit`\r\n`    use_multiprocessing=use_multiprocessing)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__`\r\n`    result = self._call(*args, **kwds)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call`\r\n`    return self._stateless_fn(*args, **kwds)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__`\r\n`    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call`\r\n`    self.captured_inputs)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat`\r\n`    ctx, args, cancellation_manager=cancellation_manager)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 511, in call`\r\n`    ctx=ctx)`\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute`\r\n`    six.raise_from(core._status_to_exception(e.code, message), None)`\r\n`  File \"<string>\", line 3, in raise_from`\r\n`tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.`\r\n`\t [[node sequential/conv2d/Conv2D (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_983]`\r\n\r\n`Function call stack:\r\ndistributed_function`\r\n\r\nAttaching the log for your quick reference.\r\nIs 9.0 suggested for by any case. \r\n[error1_log.txt](https://github.com/tensorflow/tensorflow/files/3936114/error1_log.txt)\r\n[cuda_version.txt](https://github.com/tensorflow/tensorflow/files/3936115/cuda_version.txt)\r\n", "Hi Guys,\r\n\r\nAlso posting details of the Nvidia driver and CUDA version details \r\n`vissu@vissu-Z390-M:~/graphics_card$ nvidia-smi`\r\n`Sun Dec  8 16:15:58 2019       `\r\n`+-----------------------------------------------------------------------------+`\r\n`| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |`\r\n`|-------------------------------+----------------------+----------------------+`\r\n`| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |`\r\n`| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |`\r\n`|===============================+======================+======================|`\r\n`|   0  GeForce GTX 166...  On   | 00000000:01:00.0  On |                  N/A |`\r\n`|  0%   46C    P8    14W / 120W |   1097MiB /  5941MiB |      2%      Default |`\r\n`+-------------------------------+----------------------+----------------------+`\r\n                                                                   \r\n`+-----------------------------------------------------------------------------+`\r\n\r\n\r\n`| Processes:                                                       GPU Memory |`\r\n`|  GPU       PID   Type   Process name                             Usage      |`\r\n`|=============================================================================|`\r\n`|    0      1023      G   /usr/lib/xorg/Xorg                            18MiB |`\r\n`|    0      1212      G   /usr/bin/gnome-shell                          48MiB |`\r\n`|    0      1423      G   /usr/lib/xorg/Xorg                           246MiB |`\r\n`|    0      1568      G   /usr/bin/gnome-shell                         286MiB |`\r\n`|    0      2068      G   ...quest-channel-token=4727398288044008870   493MiB |`\r\n`+-----------------------------------------------------------------------------+`\r\n\r\nAbove command show CUDA is 10.2 but when I check with other commands\r\n\r\n` nvcc --version | grep \"release\" | awk '{print $6}' | cut -c2- `\r\ngives below results \r\n`9.0.176`\r\nAlso result for this command `cat /usr/local/cuda/version.txt` is `CUDA Version 9.0.176`\r\nThis is when I have choosen 9.0 using the above `source switch-cuda.sh` command \r\nalso results of `dpkg -l | grep cuda` is below\r\n\r\n`ii  cuda                                                        10.2.89-1                                       amd64        CUDA meta-package`\r\n`ii  cuda-10-0                                                   10.0.130-1                                      amd64        CUDA 10.0 meta-package`\r\n`ii  cuda-10-2                                                   10.2.89-1                                       amd64        CUDA 10.2 meta-package`\r\n`ii  cuda-command-line-tools-10-0                                10.0.130-1                                      amd64        CUDA command-line tools`\r\n`ii  cuda-command-line-tools-10-2                                10.2.89-1                                       amd64        CUDA command-line tools`\r\n`ii  cuda-compat-10-1                                            418.87.01-1                                     amd64        CUDA Compatibility Platform`\r\n`ii  cuda-compiler-10-0                                          10.0.130-1                                      amd64        CUDA compiler`\r\n`ii  cuda-compiler-10-2                                          10.2.89-1                                       amd64        CUDA compiler`\r\n`ii  cuda-cublas-10-0                                            10.0.130-1                                      amd64        CUBLAS native runtime libraries`\r\n`ii  cuda-cublas-dev-10-0                                        10.0.130-1                                      amd64        CUBLAS native dev links, headers`\r\n`ii  cuda-cudart-10-0                                            10.0.130-1                                      amd64        CUDA Runtime native Libraries`\r\n`ii  cuda-cudart-10-2                                            10.2.89-1                                       amd64        CUDA Runtime native Libraries`\r\n`ii  cuda-cudart-dev-10-0                                        10.0.130-1                                      amd64        CUDA Runtime native dev links, headers`\r\n`ii  cuda-cudart-dev-10-2                                        10.2.89-1                                       amd64        CUDA Runtime native dev links, headers`\r\n`ii  cuda-cufft-10-0                                             10.0.130-1                                      amd64        CUFFT native runtime libraries`\r\n`ii  cuda-cufft-10-2                                             10.2.89-1                                       amd64        CUFFT native runtime libraries`\r\n`ii  cuda-cufft-dev-10-0                                         10.0.130-1                                      amd64        CUFFT native dev links, headers`\r\n`ii  cuda-cufft-dev-10-2                                         10.2.89-1                                       amd64        CUFFT native dev links, headers`\r\n`ii  cuda-cuobjdump-10-0                                         10.0.130-1                                      amd64        CUDA cuobjdump`\r\n`ii  cuda-cuobjdump-10-2                                         10.2.89-1                                       amd64        CUDA cuobjdump`\r\n`ii  cuda-cupti-10-0                                             10.0.130-1                                      amd64        CUDA profiling tools interface.`\r\n`ii  cuda-cupti-10-2                                             10.2.89-1                                       amd64        CUDA profiling tools runtime libs.`\r\n`ii  cuda-cupti-dev-10-2                                         10.2.89-1                                       amd64        CUDA profiling tools interface.`\r\n`ii  cuda-curand-10-0                                            10.0.130-1                                      amd64        CURAND native runtime libraries`\r\n`ii  cuda-curand-10-2                                            10.2.89-1                                       amd64        CURAND native runtime libraries`\r\n`ii  cuda-curand-dev-10-0                                        10.0.130-1                                      amd64        CURAND native dev links, headers`\r\n`ii  cuda-curand-dev-10-2                                        10.2.89-1                                       amd64        CURAND native dev links, headers`\r\n`ii  cuda-cusolver-10-0                                          10.0.130-1                                      amd64        CUDA solver native runtime libraries`\r\n`ii  cuda-cusolver-10-2                                          10.2.89-1                                       amd64        CUDA solver native runtime libraries`\r\n`ii  cuda-cusolver-dev-10-0                                      10.0.130-1                                      amd64        CUDA solver native dev links, headers`\r\n`ii  cuda-cusolver-dev-10-2                                      10.2.89-1                                       amd64        CUDA solver native dev links, headers`\r\n`ii  cuda-cusparse-10-0                                          10.0.130-1                                      amd64        CUSPARSE native runtime libraries`\r\n`ii  cuda-cusparse-10-2                                          10.2.89-1                                       amd64        CUSPARSE native runtime libraries`\r\n`ii  cuda-cusparse-dev-10-0                                      10.0.130-1                                      amd64        CUSPARSE native dev links, headers`\r\n`ii  cuda-cusparse-dev-10-2                                      10.2.89-1                                       amd64        CUSPARSE native dev links, headers`\r\n`ii  cuda-demo-suite-10-0                                        10.0.130-1                                      amd64        Demo suite for CUDA`\r\n`ii  cuda-demo-suite-10-2                                        10.2.89-1                                       amd64        Demo suite for CUDA`\r\n`ii  cuda-documentation-10-0                                     10.0.130-1                                      amd64        CUDA documentation`\r\n`ii  cuda-documentation-10-2                                     10.2.89-1                                       amd64        CUDA documentation`\r\n`ii  cuda-driver-dev-10-0                                        10.0.130-1                                      amd64        CUDA Driver native dev stub library`\r\n`ii  cuda-driver-dev-10-2                                        10.2.89-1                                       amd64        CUDA Driver native dev stub library`\r\n`ii  cuda-drivers                                                440.33.01-1                                     amd64        CUDA Driver meta-package`\r\n`ii  cuda-gdb-10-0                                               10.0.130-1                                      amd64        CUDA-GDB`\r\n`ii  cuda-gdb-10-2                                               10.2.89-1                                       amd64        CUDA-GDB`\r\n`ii  cuda-gpu-library-advisor-10-0                               10.0.130-1                                      amd64        CUDA GPU Library Advisor.`\r\n`ii  cuda-libraries-10-0                                         10.0.130-1                                      amd64        CUDA Libraries 10.0 meta-package`\r\n`ii  cuda-libraries-10-2                                         10.2.89-1                                       amd64        CUDA Libraries 10.2 meta-package`\r\n`ii  cuda-libraries-dev-10-0                                     10.0.130-1                                      amd64        CUDA Libraries 10.0 development meta-package`\r\n`ii  cuda-libraries-dev-10-2                                     10.2.89-1                                       amd64        CUDA Libraries 10.2 development meta-package`\r\n`ii  cuda-license-10-0                                           10.0.130-1                                      amd64        CUDA licenses`\r\n`ii  cuda-license-10-2                                           10.2.89-1                                       amd64        CUDA licenses`\r\n`ii  cuda-memcheck-10-0                                          10.0.130-1                                      amd64        CUDA-MEMCHECK`\r\n`ii  cuda-memcheck-10-2                                          10.2.89-1                                       amd64        CUDA-MEMCHECK`\r\n`ii  cuda-misc-headers-10-0                                      10.0.130-1                                      amd64        CUDA miscellaneous headers`\r\n`ii  cuda-misc-headers-10-2                                      10.2.89-1                                       amd64        CUDA miscellaneous headers`\r\n`ii  cuda-npp-10-0                                               10.0.130-1                                      amd64        NPP native runtime libraries`\r\n`ii  cuda-npp-10-2                                               10.2.89-1                                       amd64        NPP native runtime libraries`\r\n`ii  cuda-npp-dev-10-0                                           10.0.130-1                                      amd64        NPP native dev links, headers`\r\n`ii  cuda-npp-dev-10-2                                           10.2.89-1                                       amd64        NPP native dev links, headers`\r\n`ii  cuda-nsight-10-0                                            10.0.130-1                                      amd64        CUDA nsight`\r\n`ii  cuda-nsight-10-2                                            10.2.89-1                                       amd64        CUDA nsight`\r\n`ii  cuda-nsight-compute-10-0                                    10.0.130-1                                      amd64        NVIDIA Nsight Compute`\r\n`ii  cuda-nsight-compute-10-2                                    10.2.89-1                                       amd64        NVIDIA Nsight Compute`\r\n`ii  cuda-nsight-systems-10-2                                    10.2.89-1                                       amd64        NVIDIA Nsight Systems`\r\n`ii  cuda-nvcc-10-0                                              10.0.130-1                                      amd64        CUDA nvcc`\r\n`ii  cuda-nvcc-10-2                                              10.2.89-1                                       amd64        CUDA nvcc`\r\n`ii  cuda-nvdisasm-10-0                                          10.0.130-1                                      amd64        CUDA disassembler`\r\n`ii  cuda-nvdisasm-10-2                                          10.2.89-1                                       amd64        CUDA disassembler`\r\n`ii  cuda-nvgraph-10-0                                           10.0.130-1                                      amd64        NVGRAPH native runtime libraries`\r\n`ii  cuda-nvgraph-10-2                                           10.2.89-1                                       amd64        NVGRAPH native runtime libraries`\r\n`ii  cuda-nvgraph-dev-10-0                                       10.0.130-1                                      amd64        NVGRAPH native dev links, headers`\r\n`ii  cuda-nvgraph-dev-10-2                                       10.2.89-1                                       amd64        NVGRAPH native dev links, headers`\r\n`ii  cuda-nvjpeg-10-0                                            10.0.130.1-1                                    amd64        NVJPEG native runtime libraries`\r\n`ii  cuda-nvjpeg-10-2                                            10.2.89-1                                       amd64        NVJPEG native runtime libraries`\r\n`ii  cuda-nvjpeg-dev-10-0                                        10.0.130.1-1                                    amd64        NVJPEG native dev links, headers`\r\n`ii  cuda-nvjpeg-dev-10-2                                        10.2.89-1                                       amd64        NVJPEG native dev links, headers`\r\n`ii  cuda-nvml-dev-10-0                                          10.0.130-1                                      amd64        NVML native dev links, headers`\r\n`ii  cuda-nvml-dev-10-2                                          10.2.89-1                                       amd64        NVML native dev links, headers`\r\n`ii  cuda-nvprof-10-0                                            10.0.130-1                                      amd64        CUDA Profiler tools`\r\n`ii  cuda-nvprof-10-2                                            10.2.89-1                                       amd64        CUDA Profiler tools`\r\n`ii  cuda-nvprune-10-0                                           10.0.130-1                                      amd64        CUDA nvprune`\r\n`ii  cuda-nvprune-10-2                                           10.2.89-1                                       amd64        CUDA nvprune`\r\n`ii  cuda-nvrtc-10-0                                             10.0.130-1                                      amd64        NVRTC native runtime libraries`\r\n`ii  cuda-nvrtc-10-2                                             10.2.89-1                                       amd64        NVRTC native runtime libraries`\r\n`ii  cuda-nvrtc-dev-10-0                                         10.0.130-1                                      amd64        NVRTC native dev links, headers`\r\n`ii  cuda-nvrtc-dev-10-2                                         10.2.89-1                                       amd64        NVRTC native dev links, headers`\r\n`ii  cuda-nvtx-10-0                                              10.0.130-1                                      amd64        NVIDIA Tools Extension`\r\n`ii  cuda-nvtx-10-2                                              10.2.89-1                                       amd64        NVIDIA Tools Extension`\r\n`ii  cuda-nvvp-10-0                                              10.0.130-1                                      amd64        CUDA nvvp`\r\n`ii  cuda-nvvp-10-2                                              10.2.89-1                                       amd64        CUDA nvvp`\r\n`ii  cuda-repo-ubuntu1804                                        10.0.130-1                                      amd64        cuda repository configuration files`\r\n`ii  cuda-runtime-10-0                                           10.0.130-1                                      amd64        CUDA Runtime 10.0 meta-package`\r\n`ii  cuda-runtime-10-2                                           10.2.89-1                                       amd64        CUDA Runtime 10.2 meta-package`\r\n`ii  cuda-samples-10-0                                           10.0.130-1                                      amd64        CUDA example applications`\r\n`ii  cuda-samples-10-2                                           10.2.89-1                                       amd64        CUDA example applications`\r\n`ii  cuda-sanitizer-api-10-2                                     10.2.89-1                                       amd64        CUDA Sanitizer API`\r\n`ii  cuda-toolkit-10-0                                           10.0.130-1                                      amd64        CUDA Toolkit 10.0 meta-package`\r\n`ii  cuda-toolkit-10-2                                           10.2.89-1                                       amd64        CUDA Toolkit 10.2 meta-package`\r\n`ii  cuda-tools-10-0                                             10.0.130-1                                      amd64        CUDA Tools meta-package`\r\n`ii  cuda-tools-10-2                                             10.2.89-1                                       amd64        CUDA Tools meta-package`\r\n`ii  cuda-visual-tools-10-0                                      10.0.130-1                                      amd64        CUDA visual tools`\r\n`ii  cuda-visual-tools-10-2                                      10.2.89-1                                       amd64        CUDA visual tools`\r\n`iU  libcudart9.1:amd64                                          9.1.85-3ubuntu1                                 amd64        NVIDIA CUDA Runtime Library`\r\n`ii  libcudnn7                                                   7.4.1.5-1+cuda9.0                               amd64        cuDNN runtime libraries`\r\n`ii  libnvinfer5                                                 5.1.5-1+cuda10.0                                amd64        TensorRT runtime libraries`\r\n`ii  libnvinfer6                                                 6.0.1-1+cuda10.2                                amd64        TensorRT runtime libraries`\r\n`ii  nv-tensorrt-repo-ubuntu1804-cuda10.2-trt6.0.1.8-ga-20191108 1-1                                             amd64        nv-tensorrt repository configuration files`\r\n`iU  nvidia-cuda-doc                                             9.1.85-3ubuntu1                                 all          NVIDIA CUDA and OpenCL documentation`\r\n`iU  nvidia-cuda-gdb                                             9.1.85-3ubuntu1                                 amd64        NVIDIA CUDA Debugger (GDB)`\r\n`iU  nvidia-cuda-toolkit                                         9.1.85-3ubuntu1                                 amd64        NVIDIA CUDA development toolkit`\r\n\r\n\r\n\r\n\r\n", "Hi Group, any work around here, stuck with this issue for a while. ", "You need cuda 10.0 for TF 2.0 and your cuDNN version should be ```>=7.4.1```\r\nSee https://www.tensorflow.org/install/gpu#software_requirements", "Hi Modak,\r\nI have ensured that versions are matching as you mentioned above but still I have facing issue.\r\n\r\n`2019-12-11 14:59:40.298600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1`\r\n`2019-12-11 14:59:40.456044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:40.456342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77`\r\n`pciBusID: 0000:01:00.0`\r\n`2019-12-11 14:59:40.489175: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0`\r\n`2019-12-11 14:59:40.657288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0`\r\n`2019-12-11 14:59:40.728803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0`\r\n`2019-12-11 14:59:40.757395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0`\r\n`2019-12-11 14:59:40.943656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0`\r\n`2019-12-11 14:59:41.060736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0`\r\n`2019-12-11 14:59:41.553896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7`\r\n`2019-12-11 14:59:41.554229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.555542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.556574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0`\r\n`2019-12-11 14:59:41.585149: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA`\r\n`2019-12-11 14:59:41.739272: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz`\r\n`2019-12-11 14:59:41.746819: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c79c00 executing computations on platform Host. Devices:`\r\n`2019-12-11 14:59:41.746892: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version`\r\n`2019-12-11 14:59:41.895480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.895800: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c1ee50 executing computations on platform CUDA. Devices:`\r\n`2019-12-11 14:59:41.895815: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1660 Ti, Compute Capability 7.5`\r\n`2019-12-11 14:59:41.895935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.896165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77`\r\n`pciBusID: 0000:01:00.0`\r\n`2019-12-11 14:59:41.896204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0`\r\n`2019-12-11 14:59:41.896217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0`\r\n`2019-12-11 14:59:41.896227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0`\r\n`2019-12-11 14:59:41.896238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0`\r\n`2019-12-11 14:59:41.896248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0`\r\n`2019-12-11 14:59:41.896257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0`\r\n`2019-12-11 14:59:41.896269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7`\r\n`2019-12-11 14:59:41.896303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.896543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.896757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0`\r\n`2019-12-11 14:59:41.916107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0`\r\n`2019-12-11 14:59:41.938696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:`\r\n`2019-12-11 14:59:41.938728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 `\r\n`2019-12-11 14:59:41.938737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N `\r\n`2019-12-11 14:59:41.970067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.971719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2019-12-11 14:59:41.972849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5269 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)`\r\n`2019-12-11 14:59:50.411307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0`\r\n`2019-12-11 14:59:51.671797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7`\r\n`2019-12-11 14:59:54.084124: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`\r\n`2019-12-11 14:59:54.118009: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR`\r\n`2019-12-11 14:59:54.118048: F tensorflow/core/kernels/conv_grad_input_ops.cc:1102] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()), &algorithms) `\r\n\r\nWill going back to TF 1.x verisons help. If I use tensorflow 2.0 wihtout gpu version things are working. But I wanted to leverage graphics card, but having issues in doing so. Let me know which TF 1.x version should be best to use along with above versions. ", "Got the issue fixed by installing the nightly build for tensorflow 2.0. \r\n`pip install tf-nightly`\r\n\r\nLog shows up error related to conv2d fixed, possibly\r\n`tf.keras`\r\n`Export depthwise_conv2d in tf.keras.backend.`\r\n\r\nThanks for support.\r\nViswanath.B", "Glad it worked for you! Thanks for the update. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34888\">No</a>\n", "> You may try limiting your gpu memory usage with `set_memory_growth` option.\r\n> See https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\n> Put the following snippet on top of your code and execute again\r\n> \r\n> ```python\r\n>  import tensorflow as tf\r\n>  gpus= tf.config.experimental.list_physical_devices('GPU')\r\n>  tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> ```\r\n\r\nSomething finally worked after two days of trying. Thank :)", "> > You may try limiting your gpu memory usage with `set_memory_growth` option.\r\n> > See https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\n> > Put the following snippet on top of your code and execute again\r\n> > ```python\r\n> >  import tensorflow as tf\r\n> >  gpus= tf.config.experimental.list_physical_devices('GPU')\r\n> >  tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> > ```\r\n> \r\n> Something finally worked after two days of trying. Thank :)\r\n\r\nThank you, it fixed it for me as well"]}]