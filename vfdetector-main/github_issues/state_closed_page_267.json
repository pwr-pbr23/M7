[{"number": 46397, "title": "Dynamic shape for block3 to support input_shape (None, None, 3)", "body": "This PR is a part of #37146, the error I have addressed at [this comment](https://github.com/tensorflow/tensorflow/pull/37146#issuecomment-759351768)\r\n\r\nFor ResNext models which use block3 function to build up. When the input shape is `(None, None, 3)`, the error will occur.\r\nI changed the function to get dynamic shape and reshape with None dimension.\r\n\r\n```python\r\nResNeXt50(include_top=False, input_shape=(224, 224,3)).summary()\r\n```\r\n![image](https://user-images.githubusercontent.com/36766404/104455554-0af51f00-55da-11eb-8ed1-8c705fd236f9.png)\r\n\r\n```python\r\nResNeXt50(include_top=False, input_shape=(None, None,3)).summary()\r\n```\r\n![image](https://user-images.githubusercontent.com/36766404/104455487-ebf68d00-55d9-11eb-83b2-74c05d54a48b.png)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46397) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 46396, "title": "Converting Tensorflow to TFLite error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXP, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PAD, RESHAPE, RESIZE_BILINEAR, SPLIT_V, STRIDED_SLICE, TANH. Here is a list of operators for which you will need custom implementations: Softplus.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nimport tensorflow as tf\r\nfrom absl import app, flags, logging\r\nfrom absl.flags import FLAGS\r\nimport numpy as np\r\nimport cv2\r\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\r\nimport core.utils as utils\r\nimport os\r\nfrom core.config import cfg\r\n\r\nflags.DEFINE_string('weights', './checkpoints/yolov4-416', 'path to weights file')\r\nflags.DEFINE_string('output', './checkpoints/yolov4-416-fp32.tflite', 'path to output')\r\nflags.DEFINE_integer('input_size', 416, 'path to output')\r\nflags.DEFINE_string('quantize_mode', 'float32', 'quantize mode (int8, float16, float32)')\r\nflags.DEFINE_string('dataset', \"/Volumes/Elements/data/coco_dataset/coco/5k.txt\", 'path to dataset')\r\n\r\ndef representative_data_gen():\r\n  fimage = open(FLAGS.dataset).read().split()\r\n  for input_value in range(10):\r\n    if os.path.exists(fimage[input_value]):\r\n      original_image=cv2.imread(fimage[input_value])\r\n      original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\r\n      image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])\r\n      img_in = image_data[np.newaxis, ...].astype(np.float32)\r\n      print(\"calibration image {}\".format(fimage[input_value]))\r\n      yield [img_in]\r\n    else:\r\n      continue\r\n\r\ndef save_tflite():\r\n  converter = tf.lite.TFLiteConverter.from_saved_model(FLAGS.weights)\r\n\r\n  if FLAGS.quantize_mode == 'float16':\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.allow_custom_ops = True\r\n  elif FLAGS.quantize_mode == 'int8':\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.allow_custom_ops = True\r\n    converter.representative_dataset = representative_data_gen\r\n\r\n  tflite_model = converter.convert()\r\n  open(FLAGS.output, 'wb').write(tflite_model)\r\n\r\n  logging.info(\"model saved to: {}\".format(FLAGS.output))\r\n\r\ndef demo():\r\n  interpreter = tf.lite.Interpreter(model_path=FLAGS.output)\r\n  interpreter.allocate_tensors()\r\n  logging.info('tflite model loaded')\r\n\r\n  input_details = interpreter.get_input_details()\r\n  print(input_details)\r\n  output_details = interpreter.get_output_details()\r\n  print(output_details)\r\n\r\n  input_shape = input_details[0]['shape']\r\n\r\n  input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n\r\n  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n  interpreter.invoke()\r\n  output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\r\n\r\n  print(output_data)\r\n\r\ndef main(_argv):\r\n  save_tflite()\r\n  demo()\r\n\r\nif __name__ == '__main__':\r\n    try:\r\n        app.run(main)\r\n    except SystemExit:\r\n        pass\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@DanC6312 \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using?\r\n\r\nRequest you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46396\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46396\">No</a>\n"]}, {"number": 46395, "title": "YOLOv4 not completely quantized -> Edge TPU compilation fails", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef representative_dataset_gen():\r\n\ti = 1\r\n\tfor image in range(100):\r\n\t\timage = np.random.random((1, 416, 416, 3)).astype('float32')\r\n\t\tprint(i, image.shape)\r\n\t\ti += 1\r\n\t\tyield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/MyDrive/YOLO/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter._experimental_new_quantizer = True\r\nconverter.allow_custom_ops = True\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\nwith open('/content/drive/MyDrive/YOLO/saved_model/yolov4_full_integer_quant.tflite', 'wb') as w:\r\n\tw.write(tflite_quant_model)\r\nprint(\"Full Integer Quantization complete! - yolov4_full_integer_quant.tflite\")\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nThere is no issue here\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nThe link contains the Saved_Model and the quantized TFLite model.\r\nhttps://drive.google.com/drive/folders/1m4KOjdWHmtBpHPau8Oxu7bC5efkdCRy_?usp=sharing\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Despite using the target spec BUILTINS_INT8, the model is not quantized completely (exp and log operations)\r\n- This then causes an issue when using the EdgeTPU compiler, with an error\r\n```\r\nEdge TPU Compiler version 15.0.340273435\r\nInvalid model: /content/drive/MyDrive/Clutterbot/YOLO/saved_model/yolov4_full_integer_quant.tflite\r\nModel not quantized\r\n```\r\n\r\n@abattery @jingpu Can you please help me out here? I thought Exp and Log operations were common enough to have been quantized, or am I missing something here?\r\n", "comments": ["@liufengdb Can you please take a look at this issue?", "Running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/889d9720c511ac07a2358d56b759a7ee/46395-2-3.ipynb) throws an error stating `RuntimeError: Failed to quantize: <unknown>:0: error: loc(\"functional_1/lambda/Exp;StatefulPartitionedCall/functional_1/lambda/Exp\"): 'tfl.exp' op requires the same type for all operands and results`.\r\n\r\nWas able to reproduce the `Model not quantized` issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/c54d3783ad033b67f99d6a551cc96200/46395.ipynb#scrollTo=wcinmBwAiglX).  \r\n\r\nRunning the code with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3a06e155a169b62379ce80860f015ca4/46395.ipynb), throws an error stating `RuntimeError: Quantization not yet supported for op: 'EXP'. Quantization not yet supported for op: 'LOG'.`\r\n\r\nPlease check the linked gist for reference. Thanks!", "@amahendrakar Do you need any more information from me?", "@jingpu @liufengdb @daverim could you take a look at this?", "Any updates on this?", "Hi @arsenal-2004 ,\r\n\r\n[Log](https://github.com/tensorflow/tensorflow/blob/ffe67626083d356629326fec99ad032c51a7d904/tensorflow/lite/kernels/elementwise.cc#L53) and [Exp](https://github.com/tensorflow/tensorflow/blob/ffe67626083d356629326fec99ad032c51a7d904/tensorflow/lite/kernels/exp.cc#L65) are not currently supported for quantized inference, so you'd want to restructure the model or find alternatives without log/exp.\r\n\r\nWill close this as the referenced one is also closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46395\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46395\">No</a>\n", "@arsenal-2004 If you still want `Log` and `Exp` quantized operators in TFLite, you can file a separate issue for the feature request."]}, {"number": 46394, "title": "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).", "body": "Im getting this valueError in colab gpu env with keras tensorflow ver 2.\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). on running the following code :\r\n\r\ndef ctc_lambda_func(args):\r\n    y_pred, labels, input_length, label_length = args\r\n    # the 2 is critical here since the first couple outputs of the RNN\r\n    # tend to be garbage:\r\n    # print \"y_pred_shape: \", y_pred.shape\r\n    y_pred = y_pred[:, 2:, :]\r\n    # print \"y_pred_shape: \", y_pred.shape\r\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\r\n\r\n# ---- CTC ----\r\n    \r\nlabels = Input(name='the_labels', shape=[None], dtype=dtype)       # transcription data (batch_size * y_seq_size)\r\ninput_length = Input(name='input_length', shape=X_train_length.shape, dtype=dtype)  # unpadded len of all x_sequences in batch\r\nlabel_length = Input(name='label_length', shape=label_length.shape, dtype=dtype)  # unpadded len of all y_sequences in batch\r\n\r\n\r\n# Lambda layer with ctc_loss function due to Keras not supporting CTC layers\r\nloss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))([y_pred, np.float32, Y_train, np.float32, X_train_length, np.float32, label_length, np.float32])\r\nnetwork_model = Model(inputs=[input_data, labels, input_length, label_length], outputs=y_pred)    \r\n\r\n\r\n", "comments": ["@neenaloysius,\r\nOn running the given code snippet with TF v2.4, I am facing an error stating `NameError: name 'dtype' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/404e7bf1a78ec3e83af5735df794a840/46394.ipynb).\r\n\r\nIn order to reproduce the issue reported here, could you please provide the exact TensorFlow version, the complete code and the dataset you are using. Alternatively, you can also share the link of the Colab notebook you are running. Thanks!", "PFA the colab file. The dataset folder I have shared with you. Please let me\u00a0 know if you need more details\nThanks,\nNeena Aloysius\nResearch Scholar\nDept. of CSE,\nAmrita School of Engineering\nPh: +91-9061414992.\n \n\n    On Wednesday, January 13, 2021, 09:30:59 PM GMT+5:30, Abhilash Mahendrakar <notifications@github.com> wrote:  \n \n \n\n\n@neenaloysius,\nOn running the given code snippet with TF v2.4, I am facing an error stating NameError: name 'dtype' is not defined. Please find the gist of it here.\n\nIn order to reproduce the issue reported here, could you please provide the exact TensorFlow version, the complete code and the dataset you are using. Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "Could you please update the progress. Also, please confirm if you were able to access the dataset folder.\nThanks,\nNeena Aloysius\nResearch Scholar\nDept. of CSE,\nAmrita School of Engineering\nPh: +91-9061414992.\n \n\n    On Friday, January 15, 2021, 02:52:39 PM GMT+5:30, Neena Aloysius <neenaloysius@ymail.com> wrote:  \n \n PFA the colab file. The dataset folder I have shared with you. Please let me\u00a0 know if you need more details\nThanks,\nNeena Aloysius\nResearch Scholar\nDept. of CSE,\nAmrita School of Engineering\nPh: +91-9061414992.\n \n\n    On Wednesday, January 13, 2021, 09:30:59 PM GMT+5:30, Abhilash Mahendrakar <notifications@github.com> wrote:  \n \n \n\n\n@neenaloysius,\nOn running the given code snippet with TF v2.4, I am facing an error stating NameError: name 'dtype' is not defined. Please find the gist of it here.\n\nIn order to reproduce the issue reported here, could you please provide the exact TensorFlow version, the complete code and the dataset you are using. Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n    ", "> PFA the colab file. The dataset folder I have shared with you. Please let me\u00a0 know if you need more details\r\n\r\n@neenaloysius,\r\nI do not see a Colab notebook or a dataset attached. Could you please share the links of it here? Thanks!", "The colab file in zipped format:\r\n[CSLR (1).zip](https://github.com/tensorflow/tensorflow/files/5846749/CSLR.1.zip)\r\nThis is the link for dataset:\r\nhttps://drive.google.com/drive/folders/1kus7GxIYTkHFb1HJBLd_ALQGIGlQUSw5?usp=sharing", "@neenaloysius,\r\nI did not face the `ValueError: Failed to convert a NumPy array to a Tensor` on running the code with the TensorFlow v2.4.\r\n\r\nHowever, I am facing a different error stating `ValueError: Can not squeeze dim[1], expected a dimension of 1, got 3 for '{{node ctc/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Placeholder_1)' with input shapes: [?,3].`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/58f50b495e0e9eb67097c72fdd662d82/46394.ipynb). Thanks!", "Yeah, I slightly changed the code and this is the issue im facing now. Not sure which variable is causing the issue.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46394\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46394\">No</a>\n"]}, {"number": 46393, "title": "Checksum error with GCC download", "body": "@tensorflow/micro\r\n\r\n**Short description**\r\n\r\n```\r\ntensorflow/lite/micro/tools/make/targets/apollo3evb_makefile.inc:14: *** Something went wrong with the GCC download: Bad checksum. Expected: bc8ae26d7c429f30d583a605a4bcf9bc, Got: e588d21be5a0cc9caa60938d2422b058.  Stop.\r\n```\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina\r\n- TensorFlow installed from (source or binary): n/a\r\n- Tensorflow version (commit SHA if source): latest from git clone b860885da901ed641015bc75b9ac06fc4a1c5a57\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): SparkFun_edge\r\n\r\n**Reproduction instructions**\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin\r\n```\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Potential duplicate of https://github.com/tensorflow/tensorflow/issues/45033", "This works fine when I use docker running ubuntu 18.04, just on macOS.", "This is very likely an incorrect checksum as part of https://github.com/tensorflow/tensorflow/pull/46287.\r\n\r\nWe do not have appropriate CI for MacOS and many contributors (including myself) only test on Linux.\r\n\r\nIf you could send a PR with an updated checksum that is tested on MacOS, that would be very useful.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46393\">No</a>\n"]}, {"number": 46392, "title": "Training loss not decreasing if both mixed-precision and XLA/JIT is enabled (with deep model)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Linux Ubuntu 18.04:\r\n- TensorFlow installed from binary\r\n- TensorFlow 2.4.0\r\n- Python 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA 11/cuDNN 8:\r\n- GPU model and memory Titan RTX/2080Ti:\r\n\r\n**Describe the current behavior**\r\n\r\nTraining a deep model, e.g., ResNet50, with both mixed-precision and XLA/JIT simultaneously enabled results in the training loss not decreasing.\r\nMixed-precision alone or XLA/JIT with float32 works both fine.\r\n\r\n**Describe the expected behavior**\r\n\r\nEnabling XLA/JIT and mixed-precision training should behave the same as if only mixed-precision is enabled.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Minimal Colab notebook to reproduce issue](https://colab.research.google.com/drive/1FLRd52ZejrD-jSc3fHVYrK4L-QE-73JA?usp=sharing)\r\n\r\n**Notes**\r\nNote that this bug does not occur for small models like the ConvNet in the mixed-precision training guide.\r\nAlso, the fact that training with mixed-precision alone works fine might point to where there could be an issue here.", "comments": ["@mlech26l,\r\nI do not have access to the Colab notebook you have linked. Could you please provide the required permissions to view the files. Thanks!", "Hi, @amahendrakar \r\nSorry, the following [link](https://colab.research.google.com/drive/1FLRd52ZejrD-jSc3fHVYrK4L-QE-73JA?usp=sharing) should work.\r\n\r\nThanks", "Hi @mlech26l, try using a Learning Rate scheduler", "Facing an error stating `AttributeError: module 'tensorflow.keras.mixed_precision' has no attribute 'Policy'` on running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/cded5ab4b8f8225acc70099fb18ad83f/46392-2-3.ipynb#scrollTo=oQSmky8hrZ4X).\r\n\r\nWas able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/299a1ac5dd53648c3ad1b65dbe07a5c5/46392.ipynb).\r\n\r\nWhereas with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8acda2b2bddd514491898af0c2d77a03/46392-tf-nightly.ipynb), `model.fit` runs indefinitely. Please check the linked gist for reference. Thanks!", "Ran into the same problem when training an [EfficientDet](https://github.com/google/automl/tree/master/efficientdet) model after upgrading from TF 2.3.1 to 2.4.1.\r\nAfter updating to tf-nightly 2.5.0-dev20210127 the loss is decreasing again as expected.\r\n\r\n@amahendrakar Code to enable mixed precision is slightly different in TF 2.3:\r\n\r\n```python\r\npolicy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n```", "Hi @juliangrosshauser and @jvishnuvardhan \r\n\r\nI can confirm that the issue seems to be fixed in the latest tf-nightly. ", "@mlech26l Thanks for updating us. Closing the issue since the issue was fixed in Nightly version.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46392\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46392\">No</a>\n"]}, {"number": 46391, "title": "merge", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@Qingqingniu,\r\nClosing the issue as no details are filled. Please re-open and fill in the issue template if mistaken. Thanks!"]}, {"number": 46389, "title": "Support for LSTM and GRU", "body": "Hello,\r\n\r\nI wonder if there is support for LSTM and GRU within TensorFlow Lite for Microcontrollers (audio NN models) or any plan to have these operations? \r\n\r\nThank you.\r\n\r\nBest regards,\r\nPeter", "comments": ["@peter197321,\r\nClosing this issue as it is a duplicate of [#46390](https://github.com/tensorflow/tensorflow/issues/46390). Please feel free to re-open if mistaken. Thanks!"]}, {"number": 46388, "title": "Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors", "body": "**Description**\r\n\r\nWe converted the Matterpot Mask RCNN object detection model (using hdf5 weight file) into tflite format using tensorflow (version 2.3.0).  We are able to get the prediction in android using the version 'org.tensorflow:tensorflow-lite:2.4.0'. Inference time in android devices takes around 40 seconds using CPU (kirin 655 ). So we aiming to use the GPU support to improve inference time for our model in Android devices but the following error is raised \r\n`Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors`. \r\nWe tried using batch size as 1 but the same error is raised.\r\n\r\nWe have included below link where we have shared `H5 format model, converted tflite model and text file containing keras model summary`\r\nLINK : https://drive.google.com/drive/folders/1nTzifGDX7wXxYNXnHFw4a3Sz0p0_GdBe?usp=sharing\r\n\r\nIs there tflite GPU support for graph with dynamic sized tensors or how can we mitigate this issue?\r\n\r\n**We have included two images of graph visualized using graphviz**\r\n", "comments": ["Hi Sudharshan,\r\n\r\nI'm facing the same issue as I'm trying to use Mask RCNN in a TPU (RuntimeError: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors). Have you been able to find a fix?\r\n\r\nThanks in advance\r\n\r\n> **Description**\r\n> \r\n> We converted the Matterpot Mask RCNN object detection model (using hdf5 weight file) into tflite format using tensorflow (version 2.3.0). We are able to get the prediction in android using the version 'org.tensorflow:tensorflow-lite:2.4.0'. Inference time in android devices takes around 40 seconds using CPU (kirin 655 ). So we aiming to use the GPU support to improve inference time for our model in Android devices but the following error is raised\r\n> `Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors`.\r\n> We tried using batch size as 1 but the same error is raised.\r\n> \r\n> We have included below link where we have shared `H5 format model, converted tflite model and text file containing keras model summary`\r\n> LINK : https://drive.google.com/drive/folders/1nTzifGDX7wXxYNXnHFw4a3Sz0p0_GdBe?usp=sharing\r\n> \r\n> Is there tflite GPU support for graph with dynamic sized tensors or how can we mitigate this issue?\r\n> \r\n> **We have included two images of graph visualized using graphviz**\r\n\r\n", "I took a look at the model.\r\nUnfortunately, atleast for now, our GPU backend doesn't support graphs with dynamic tensors. This is because the GPU delegate may not work well So I don't exactly have a 'perfect' solution to this issue, but I can share a few pointers :-)\r\n\r\n1. Did you try multi-threaded inference? That usually speeds things up on the CPU. If you are using our Java bindings, [this option](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L180) can be set to 4.\r\n\r\n2. Even then, an input size of 1024x1024 is pretty large for on-device inference. Is there any way for you to retrain it to a smaller dimension? Even bringing it down to 640-512 will help *significantly*.\r\n\r\n3. If we were to look into removing the 'dynamic' parts of the graph (and thus enable GPU acceleration), it looks like the [NMS call](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L319) in the MaskRCNN model is causing the issues. One thing that some advanced clients have done in the past, is split a graph into the 1) Backbone/Feature extraction (all the convolution/fully-connected/math layers which have static shapes) & 2) Post-processing (which contains tricky bits like NMS)\r\n\r\nWill this be feasible for you? I appreciate that this answer isn't the most convenient, but the size of MaskRCNN causes these issues :-). Out of curiosity, are you using the segmentation mask outputs from MaskRCNN?", "@srjoglekar246 \r\nWe are using the multi-threading inference with threads set to 4. Reducing image size is not convenient for our model accuracy.also we are not using the segmentation mask output.\r\n\r\nIs there any other known model with high accuracy similar to MaskRCNN, where we can get the object detection boxes prediction and able to get the GPU support when the model is converted to tflite?\r\n, ", "Yes. If you only need bounding boxes, look at some of the SSD ResNet models [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). You can read [these docs](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) to figure out how to convert the models, and [this notebook](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) for pointers on how to pass inputs & outputs to the model.", "@srjoglekar246 Thank you for your response", "> I took a look at the model.\r\n> Unfortunately, atleast for now, our GPU backend doesn't support graphs with dynamic tensors. This is because the GPU delegate may not work well So I don't exactly have a 'perfect' solution to this issue, but I can share a few pointers :-)\r\n> \r\n> 1. Did you try multi-threaded inference? That usually speeds things up on the CPU. If you are using our Java bindings, [this option](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L180) can be set to 4.\r\n> 2. Even then, an input size of 1024x1024 is pretty large for on-device inference. Is there any way for you to retrain it to a smaller dimension? Even bringing it down to 640-512 will help _significantly_.\r\n> 3. If we were to look into removing the 'dynamic' parts of the graph (and thus enable GPU acceleration), it looks like the [NMS call](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L319) in the MaskRCNN model is causing the issues. One thing that some advanced clients have done in the past, is split a graph into the 1) Backbone/Feature extraction (all the convolution/fully-connected/math layers which have static shapes) & 2) Post-processing (which contains tricky bits like NMS)\r\n> \r\n> Will this be feasible for you? I appreciate that this answer isn't the most convenient, but the size of MaskRCNN causes these issues :-). Out of curiosity, are you using the segmentation mask outputs from MaskRCNN?\r\n\r\n@srjoglekar246 \r\nI have the same error when I use DB(DifferentiableBinarizationNet) to do text detection. The output shape of DB is (1, 640, 640, 1), just like segmentation mask outputs of MaskRCNN.\r\n\r\nSo, did you bring up the segmentation issue separately because Tensorflow GPU backend doesn't support segmentation output yet ?", "@dagongji10 Not sure if your graph has dynamic tensors. If not, then the TFLite GPU delegate should support it. Do you see any error?", "> @dagongji10 Not sure if your graph has dynamic tensors. If not, then the TFLite GPU delegate should support it. Do you see any error?\r\n\r\n@srjoglekar246 Thanks! I have sloved the problem.\r\nIn fact, I got the same error : \r\n```Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.```\r\nAnd the reason is I forgot to set batch_size for input image tensor. Now although the output of DB is segmentation mask, the `.tflite` can run normally with GPU delegate!", "@Sudhan97,\r\nWith respect to [this comment](https://github.com/tensorflow/tensorflow/issues/46388#issuecomment-871074251), can you please let us know if your issue is resolved so that we can close this it? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46388\">No</a>\n"]}, {"number": 46387, "title": "TF2.4 ParameterServerStrategy tf.range in step_fn is slower than np.arange", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**TF on yarn**(running machine system is CentOS 7)\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.4.0\r\n- Python version:3.6\r\n- CUDA/cuDNN version: no gpu\r\n- GPU model and memory: no gpu\r\n- CPU info: please see it below\r\n- Training config: chief:  8cpus,25G memory; worker: 30 instances, 8 cpus per instance, 25G memory per instance;ps: 8 instances, 12 cpus per instance, 25G memory per instance\r\n\r\n**Describe the current behavior**\r\n\r\nI built a model using ParameterServerStrategy, and I used tf.range(steps_per_invocation) in step_fn to execute more steps. But I found there was a about 0.4 second between one loop end and next loop start, but when I changed to np.arnage(steps_per_invocation), the gap can be ignored.This is very similar to https://github.com/tensorflow/tensorflow/issues/40708.\r\n\r\n```python\r\n@tf.function\r\ndef train_fn(iterator):\r\n    print(\"Flag_A_retrace!!!\", time.time())\r\n    tf.print(\"Flag_A\", tf.timestamp())\r\n    losses = 0.0\r\n    for inner_index in tf.range(FLAGS.steps_per_invocation):\r\n        tf.print(\"Flag_G\", tf.timestamp(), inner_index)\r\n        parsed_feature_dict, label_dict, weight_dict = next(iterator)\r\n        tf.print(\"Flag_H\", tf.timestamp(), inner_index)\r\n        replica_fn = train_step(model, metrics, optimizer)\r\n        tf.print(\"Flag_I\", tf.timestamp(), inner_index)\r\n        losses = strategy.run(replica_fn, args=(parsed_feature_dict, label_dict, weight_dict)) # we just need last loss\r\n        tf.print(\"Flag_J\", tf.timestamp(), inner_index)\r\n    tf.print(\"Flag_B\", tf.timestamp())\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n```\r\n\r\nThe time between Flag_J and next Flag_G is 0.4 second\r\n\r\n```python\r\n@tf.function\r\ndef train_fn(iterator):\r\n    print(\"Flag_A_retrace!!!\", time.time())\r\n    tf.print(\"Flag_A\", tf.timestamp())\r\n    losses = 0.0\r\n    for inner_index in np.arange(FLAGS.steps_per_invocation):\r\n        tf.print(\"Flag_G\", tf.timestamp(), inner_index)\r\n        parsed_feature_dict, label_dict, weight_dict = next(iterator)\r\n        tf.print(\"Flag_H\", tf.timestamp(), inner_index)\r\n        replica_fn = train_step(model, metrics, optimizer)\r\n        tf.print(\"Flag_I\", tf.timestamp(), inner_index)\r\n        losses = strategy.run(replica_fn, args=(parsed_feature_dict, label_dict, weight_dict)) # we just need last loss\r\n        tf.print(\"Flag_J\", tf.timestamp(), inner_index)\r\n    tf.print(\"Flag_B\", tf.timestamp())\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n```\r\nThe time between Flag_J and next Flag_G can be ignored.\r\n\r\n**Describe the expected behavior**\r\n1.  tf.range performance should be the same as np.arange\r\n2. 0.4 second in just one step is very slow because it accouts for abount 94% of one step execute time.\r\n\r\n\r\n", "comments": ["@wuxianxingkong \r\n\r\nLooks like code is incomplete.\r\n\r\nCan you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46386, "title": "Tensorflow Lite Build issue on RK3399 Ubuntu 18.04 aarch64", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04 aarch64\r\n- TensorFlow installed from (source or binary):  source\r\n- Tensorflow version (commit SHA if source):  098231f\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  RK3399 Ubuntu 18.04 aarch64\r\n\r\n**Describe the problem**\r\nI was going to build the tensorflow lite with CMake on RK3399 Ubuntu 18.04 aarch64 device.\r\n\r\nOS: Ubuntu 18.04 aarch64\r\nCMake: 3.18.0\r\ngcc(g++): 10.1.0\r\nGPU: Mali T860\r\nOpenCL: 1.2\r\n\r\nI cloned the TensorFlow and was going to build tflite supporting ARM GPU (OpenCL) with CMake.\r\n\r\n_git clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\nmkdir tflite_build\r\ncd tflite_build\r\ncmake ../tensorflow/lite -DTFLITE_ENABLE_GPU=ON_\r\n\r\nThe configuration was successful.\r\nI started to build the command \"cmake --build . -j\" but I met the following error.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104396628-00109f00-5586-11eb-9fcb-0f57eee17347.png)\r\n\r\n\r\nCould u please let me know how to fix it?\r\nThanks\r\n\r\n", "comments": ["Did you disabled TFLITE_ENABLE_XNNPACK? I've confirmed the error without enabling TFLITE_ENABLE_XNNPACK.\r\nI'll submit a patch soon. ", "Yes, in fact, I made TFLITE_ENABLE_XNNPACK disabled.\r\nIf TFLITE_ENABLE_XNNPACK is enabled, even when compiling the XNNPACK module an error appears.\r\nFor XNNPACK optimization, I think that the TFLite module source itself should contain the XNNPACK module because the XNNPACK module is being upgraded continuously, and the optimization flag by XNNPACK is NOT useful so the XNNPACK optimization always should be enabled when building the TFLite module because a user wants a well-optimized TFLite module as possible.\r\nI hope that the compile issue will be also fixed when using XNNPACK optimization.", "It should be fixed by the PR. Please reopen if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46386\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46386\">No</a>\n"]}, {"number": 46385, "title": "Wrong warning when a saved model (from TF2.4) is loaded using tf-nightly ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): TF2.4, tf-nightly\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nUsing TF2.4, a Keras model was saved using `model.save`.  Later, when I tried to load the saved model using  `tf-nightly`, code throws unexpected error as shown below.\r\n\r\n```\r\n2.5.0-dev20210112\r\nWARNING:tensorflow:SavedModel saved prior to TF 2.4 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\n```\r\n \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe following warning should be revised as the initial model was saved using `TF 2.4` but warning complains that the model was saved prior to TF 2.4.\r\n\r\n`WARNING:tensorflow:SavedModel saved prior to TF 2.4 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.`\r\n\r\nThe above warning was noticed only when the model was saved in `tf` format. The above warning doesn't appear when the model was saved in `h5` format.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/e3ec0d7b92034affc4b7644a89dc7ebc/untitled483.ipynb).\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThis issue was originally opened earlier [here](https://github.com/tensorflow/tensorflow/issues/45288) ", "comments": ["@jvishnuvardhan \r\n\r\nI have tried in colab with saving the model in TF 2.4 and loading the model in TF nightly version(`2.5.0-dev20210113`)  was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d5f00f44515e0bbd1faf3dd2f5c51ae5/untitled612.ipynb).\r\n\r\nHowever If i save and load the model in TF nightly version (`2.5.0-dev20210113`) gist [here](https://colab.research.google.com/gist/ravikyram/fde17b37206baf586914a488d847c946/untitled613.ipynb) or If i ave the model and load the model in TF 2.4 version gist [here](https://colab.research.google.com/gist/ravikyram/a68d1374495aeaad9c64a51b1b22e44f/untitled614.ipynb) then i am not seeing the below error message .\r\n`WARNING:tensorflow:SavedModel saved prior to TF 2.4 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.`\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is intended behavior and the warning is to encourage users to use `model.save`  instead of `tf.saved_model.save` when saving keras models. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/53116f96ec3127dab87c3b109d355846/untitled94.ipynb) is a gist where model was trained and saved using `TF2.4`. Then, loaded the model using `tf-nightly` and trained again on the same data. The accuracy with `TF2.4` and `tf-nightly` are similar which shows that there is no problem loading the model using `tf-nightly`. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46385\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46385\">No</a>\n", "Hi @jvishnuvardhan , there is no `keras_metadata.pb` in tf2.4 saved_model dir. But in tf2.5, I found `keras_metadata.pb`.\r\n\r\nDo you know what's the point of `keras_metadata.pb`?\r\nIs it compatible with prior tf version that has no `keras_metadata.pb`?", "In TF 2.5.0 `model.save(\"model_dir\")` creates one additional file in the model folder - `keras_metadata.pb`.\r\nThe difference btw prev versions is that now we can load the model back and get fully-featured Keras model.\r\n\r\n```\r\nmodel=tf.keras.models.load_model(\"model_dir\")\r\ntype(model)\r\n# tensorflow.python.keras.engine.sequential.Sequential\r\n\r\nmodel._is_graph_network\r\n# True\r\n\r\nmodel.summary() # Works!\r\n```\r\nWe can even re-fit the model!\r\nWe can save it in h5 format (some tools, such as Netron, support only h5 format)", "> In TF 2.5.0 `model.save(\"model_dir\")` creates one additional file in the model folder - `keras_metadata.pb`.\r\n> The difference btw prev versions is that now we can load the model back and get fully-featured Keras model.\r\n> \r\n> ```\r\n> model=tf.keras.models.load_model(\"model_dir\")\r\n> type(model)\r\n> # tensorflow.python.keras.engine.sequential.Sequential\r\n> \r\n> model._is_graph_network\r\n> # True\r\n> \r\n> model.summary() # Works!\r\n> ```\r\n> \r\n> We can even re-fit the model!\r\n> We can save it in h5 format (some tools, such as Netron, support only h5 format)\r\n\r\n@apivovarov Hi, thanks for the reply.\r\n\r\nSo if we use `load_model()` in tf2.5, the states of optimizer will be restored automatically.\r\nHowever, we need to use `custom_objects` in tf2.4 if we have used some custom layers.\r\n\r\nAnd if we want to deploy the saved model in tf2.5, we can simply delete the `keras_metadata.pb` to reduce the file size, and everything will remain the same as tf2.4.\r\n\r\nPlease correct me if there is any mistakes!", "Suppose I am past the point of no return and I cannot follow the \"encouragement\" of this warning. How do I suppress it? \r\n\r\nEDIT:\r\n``` python\r\nimport tensorflow as tf\r\nimport logging\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\nos.environ['AUTOGRAPH_VERBOSITY'] = '0'\r\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\r\n```"]}, {"number": 46384, "title": "tensorflow 2.4.0  the predict function outputs an error, when the same code works on another TF versions. ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform : Windows 10\r\n- TensorFlow installed from ): 2.4.0, using instal_tensorflow(version = \"2.4.0\" )\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0 / 8.0.2\r\n- GPU model and memory: RTX 3070\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI can run this code https://blogs.rstudio.com/ai/posts/2019-08-23-unet/ on the kaggles notebook (which runs TF 2.3.0),\r\nbut on my PC, intel + nvidia 3070, TF 2.4, the code can run and train.  BUT the PREDICT function doesnt work. \r\nThe exact same code the predict function works as expected on Kaggles\r\n\r\n\r\n**Describe the expected behavior**\r\nThe predicitions functions results in predictions, not errors\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://www.kaggle.com/rpsantosakaggle/unet-carvana\r\n\r\n**Other info / logs** \r\n predictions <- predict(model, batch)\r\nError in py_call_impl(callable, dots$args, dots$keywords) : \r\n  ValueError: in user code:\r\n\r\n    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\r\n        return step_function(self, iterator)\r\n    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    C:\\Users\\rpsan\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_re\r\n\r\n\r\n\r\n\r\n\r\n*have run many other codes that worked seamlesly, but with this one, it bugs. \r\n tf_config()\r\nTensorFlow v2.4.0 ()\r\nPython v3.6 (C:/Users/rpsan/AppData/Local/r-miniconda/envs/r-reticulate/python.exe)", "comments": ["Please provide a minimum code snippet to reproduce the reported issue this helps us reduce time on trouble shooting significantly. On a side note: From the stack trace you may want to refer this [thread](https://github.com/rstudio/keras/issues/1063).", "I just manage to solve the issue.  Something changed in the behavior of prediction function from TF 2.3.0 to TF 2.4.0. \r\nWhat worked:\r\n Instead of predict(model, batch) ( what works fine on TF 2.3.0)\r\nI wrote : predict(model, batch[[1]])  ( this owrks on TF 2.4.0) ( like, selecting only the train data, what makes sense)\r\nThankyou for the support and replay ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46384\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46384\">No</a>\n"]}, {"number": 46383, "title": "Port micro operator EXP and its test code", "body": "Combined PR4 and PR5 for issue #45415. This PR, if merged, should finish the issue.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "also, for clang-format errors you can use format_code.py:\r\nhttps://github.com/tensorflow/tensorflow/blob/b0fcce8080f94b44195c429929361a420e9255cc/tensorflow/lite/micro/tools/ci_build/test_code_style.sh#L77-L84\r\n\r\nand pass in --fix for an in-place fix.\r\n\r\nYou will need a very recent version of clang-format. This is what we use as part of CI:\r\nhttps://github.com/tensorflow/tensorflow/blob/b0fcce8080f94b44195c429929361a420e9255cc/tensorflow/tools/ci_build/Dockerfile.micro#L8-L9\r\n\r\n\r\nIdeally we would install git commit hooks, but we need a bit more setup for that.", "@advaitjain : thanks for the detailed review & feedback. I have addressed all issues but one. Still working on the clang-format errors.  ", "@advaitjain clang-format errors have been fixed; all checks have passed", "copybara failed, going to trigger again."]}, {"number": 46382, "title": "TF-TRT Test ConvertConcat in dynamic shape mode", "body": "This PR adds explicit batch and dynamic shape mode tests to ConvertConcat.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n\r\nTracker: #45481", "comments": ["Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 46381, "title": "TF-TRT improve documentation of execution context handling", "body": "This PR improves the description of the execution context handling by TF-TRT.\r\n\r\nThe synchronization related to execution context management was discussed in issue #36959. Relevant parts from that discussion are added as comments to the EngineContext class.\r\n\r\nTagging @bixia1 for review.", "comments": []}, {"number": 46379, "title": "Model Subclassing 'NoneType' object has no attribute 'shape'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nclass MyVGG(tf.keras.Model):\r\n   I don't understand why is the problem please help!\r\n \r\n    def __init__(self, num_classes):\r\n        super(MyVGG, self).__init__()\r\n        \r\n        #self.input_l = tf.keras.layers.Conv2D(filters=64,kernel_size=3, activation='relu', input_shape=(150,150,3))\r\n        #create blocks of Vgg \r\n        self.block_a = Block(filters=64 ,  kernel_size=3, repetitions= 2) \r\n        self.block_b = Block(filters=128 , kernel_size=3 , repetitions= 2) \r\n        self.block_c = Block(filters=256 , kernel_size=3, repetitions= 3) \r\n        self.block_d = Block(filters=512 , kernel_size=3 , repetitions= 3) \r\n        self.block_e = Block(filters=512 , kernel_size=3 , repetitions= 3) \r\n        \r\n        # Define a Flatten layer\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        # Define a Dense Layer \r\n        self.dense = tf.keras.layers.Dense(256, activation='relu')\r\n        # Add  the softmax classifier using a Dense layer\r\n        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\r\n    \r\n    def call(self, inputs):\r\n    \r\n        #x = self.input_l(inputs)\r\n        #Chain all the layers \r\n        x = self.block_a(inputs) \r\n        x = self.block_b(x) \r\n        x = self.block_c(x) \r\n        x = self.block_d(x) \r\n        x = self.block_e(x) \r\n        x = self.flatten(x) \r\n        x = self.dense(x) \r\n        x = self.classifier(x) \r\n        return x\r\nmodel_vgg = MyVGG(num_classes=2)\r\n\r\nmodel_vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\nhistory = model_vgg.fit_generator(train_generator, \r\n                                  epochs=10, \r\n                                  verbose=1, \r\n                                  validation_data = validation_generator)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   2616     if not self.built:\r\n   2617       input_spec.assert_input_compatibility(\r\n-> 2618           self.input_spec, inputs, self.name)\r\n   2619       input_list = nest.flatten(inputs)\r\n   2620       if input_list and self._dtype_policy.compute_dtype is None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py in assert_input_compatibility(input_spec, inputs, layer_name)\r\n    164         spec.min_ndim is not None or\r\n    165         spec.max_ndim is not None):\r\n--> 166       if x.shape.ndims is None:\r\n    167         raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\n    168                          layer_name + ' is incompatible with the layer: '\r\n\r\nAttributeError: 'NoneType' object has no attribute 'shape'\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@kutayk33,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'Block' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d31f86b2fbc0612652c09f6a40963a66/46379.ipynb).\r\n\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46379\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46379\">No</a>\n"]}, {"number": 46378, "title": "Speedup ResizeNearestNeighborGrad.", "body": "Speedup `ResizeNearestNeighborGrad` on CPU. Thread over batch and vectorization over channel. Because there is a non-negligible overhead for non-vectorized sizes within `Eigen::Map<Eigen::Vector>`, vectorization is only enabled when `channels > kPacketSize`. See the benchmark on  `(1, 250, 250, 1) | (499, 499)` and `(1, 250, 250, 3) | (499, 499)`.\r\n\r\nOn **Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz**\r\n\r\n| Input shape `(N, IH, IW, C)` | Output shape `(OW, OH)` | Original | Thread | Vectorization | Thread + Vectorization | This PR (Mixin vectorization) |\r\n| --------------------------------| ---------------------------- | ---   | -------| --------------- | --------------------------|  ----|\r\n| (1, 125, 125, 1)                       | (250, 250)                          | 138        |  148         |  142                      |      122                                 | 148 |\r\n| (1, 125, 125, 3)                       | (250, 250)                          | 318        |  339         |   344                     |       278                                | 318 |\r\n| (1, 125, 125, 16)                       | (250, 250)                          | 456        | 445          |  574                      |     505                                  | 509 |\r\n| (1, 125, 125, 21)                       | (250, 250)                          | 403        |  431         |  494                      |     457                                  | 497 |\r\n| (10, 125, 125, 1)                       | (250, 250)                          | 288        |   438        |  226                      |     415                                  | 468 |\r\n| (10, 125, 125, 3)                       | (250, 250)                          | 279        |   405        |  397                      |     406                                  | 356 |\r\n| (10, 125, 125, 16)                       | (250, 250)                          | 355        |   488        | 466                       |   460                                    | 462 |\r\n| (10, 125, 125, 21)                       | (250, 250)                          | 387        |   491        |  480                      |   474                                    | 451 |\r\n| (1, 499, 499, 1)                       | (250, 250)                          | 310        |  344         |    299                    |       254                                | 346 |\r\n| (1, 499, 499, 3)                       | (250, 250)                          | 669        |  738         |   797                     |       578                                | 739 |\r\n| (1, 499, 499, 16)                       | (250, 250)                          | 1006        |  1083         |  2424                     |   1914                                    | 1870 |\r\n| (1, 499, 499, 21)                       | (250, 250)                          |  962       |   1168        |   2014                     |    1520                                   | 1509 |\r\n| (10, 499, 499, 1)                       | (250, 250)                          |  495       |   1475        |  329                      |      1136                                 | 1586 |\r\n| (10, 499, 499, 3)                       | (250, 250)                          |  885       |  2321         |   802                     |      2139                                 | 2557 |\r\n| (10, 499, 499, 16)                       | (250, 250)                          |  954       |  2202         |  2004                      |    2532                                   | 2603 |\r\n| (10, 499, 499, 21)                       | (250, 250)                          | 993        |   2119        |  1814                      |      2607                                 | 2262 |\r\n\r\nThe number is computed as `time / (iterations * N * IH * IW * C)`, the higher the better.", "comments": ["@WindQAQ  Can you please resolve conflicts? Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@WindQAQ Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 46377, "title": "Problem with pypi package installation in linux debian(parrot os)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Parrot Os 4.10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pypi\r\n- TensorFlow version:\r\n- Python version:3.9.1\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen i run the following commands either with pip3 or just pip basically, i run into an issue like :\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\n`\r\nI even read the article from this website\r\n[https://tutorialforlinux.com/2020/10/02/step-by-step-tensorflow-parrot-linux-installation-guide/](tutorialforlinux)\r\nI have run the various commands yet to no avail\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow\r\npip install tensorflow-cpu\r\npip install tf-nightly\r\npip install tf-nightly-gpu\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@MusheAbdulHakim \r\n\r\nWhich version of TF you are using?\r\n\r\nTF 2.4 supports python from `3.6-3.8` . Please, refer the [document](https://www.tensorflow.org/install/source#cpu).\r\n\r\nTensorflow only supports the 64-bit version of Python\r\n\r\nCan you downgrade to python 3.8 version and see if the issue still persists. Thanks!", "i have created a new virtual environment with python 3.8 installed and used `pip install tensorflow` and is installing now.Thanks", "@MusheAbdulHakim \r\n\r\nPlease, close this thread if your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46377\">No</a>\n"]}, {"number": 46376, "title": "[TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertResize & Various TF2TRT Node Convert Unittest Improvements", "body": "@bixia1 @tfeher for review\r\n<!--\r\n- add Dynamic Shape Testing for ConvertResize.\r\n- add `DebugString(const TrtPrecisionMode)` in file `tensorflow/compiler/tf2tensorrt/convert/utils.[h/cc]`\r\n- add `DebugString(const DataType)` in file `tensorflow/compiler/tf2tensorrt/convert/utils.[h/cc]`\r\n- add `DebugString(const TrtTestMode)` in file `tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc`\r\n- add LOG(INFO) data for each GTest executed from the base class `ParameterizedOpConverterTestBase`\r\n  - `tf_type_`\r\n  - `trt_mode_`\r\n  - `converter_precision_`\r\n- add C++ public getters in class `ParameterizedOpConverterTestBase` for the attributes:\r\n  - `tf_type_`\r\n  - `trt_mode_`\r\n  - `converter_precision_`\r\n  -->\r\nConvertResize now requires a static shape for the tensor representing the resized size to workaround the fact that we can't support shape tensors as inputs yet.\r\nAdd dynamic shape test cases for ConvertResize.\r\n\r\n\r\nFeature Tracker: #45481", "comments": ["```bash\r\nINFO: Elapsed time: 198.196s, Critical Path: 190.09s\r\nINFO: 78 processes: 78 local.\r\nINFO: Build completed successfully, 80 total actions\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test                     PASSED in 14.9s\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                 PASSED in 14.4s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test                     PASSED in 82.8s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                 PASSED in 82.1s\r\n//tensorflow/compiler/tf2tensorrt:segment_test                           PASSED in 0.2s\r\n//tensorflow/compiler/tf2tensorrt:segment_test_gpu                       PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                       PASSED in 12.7s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                   PASSED in 12.8s\r\n//tensorflow/compiler/tf2tensorrt:trt_allocator_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test                     PASSED in 12.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                 PASSED in 12.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test           PASSED in 12.2s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu       PASSED in 12.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test   PASSED in 12.8s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu PASSED in 12.5s\r\n\r\nExecuted 16 out of 16 tests: 16 tests pass.\r\nINFO: Build completed successfully, 80 total actions\r\n****\r\n```", "@bixia1 there seems to be an issue with DynamicShape & `IResizeLayer`. For now the unittest basically does not test this case, we are investigating. Though this PR can be merged as-is and we will later fix the test when it's fully working", "@bixia1 : we are good to go. All the modifications needed requested are merged and the PR has been squashed.\r\nPlease review ;) ", "@bixia1 I have applied the changes you requested.\r\n\r\nI have also updated PR Title and PR Description. If you would prefer smthg else, please give me the exact content you want me to copy/paste. (Or you can also directly edit if allowed by Github).", "@bixia1 I updated as you asked"]}, {"number": 46375, "title": "[Cherrypick:2.4] Fix an exception when profiler is stopped twice", "body": null, "comments": []}, {"number": 46374, "title": "Fix", "body": null, "comments": []}, {"number": 46373, "title": "Failure to train distributed with strategy mirrored, custom training, functions supplied in class", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Buster with ROCm 3.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-rocm 2.3.0 -  v2.3.0-rc1-2358-gc0826c7973 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Radeon VII\r\n\r\n**Describe the current behavior**\r\n\r\nTraining works on single GPU, fails on dual GPU\r\n\r\n**Describe the expected behavior**\r\n\r\nShould also work on dual GPU\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI'm trying to run YOLOV3 tensorflow from https://github.com/ethanyanjiali/deep-vision.git with VOC_2007 data set\r\n\r\n**Other info / logs**\r\n\r\nI compared the code to example code for MirroredStrategy and could not find any ad hoc visible mistake. Only major difference is that the training is realized in a separate trainer class.\r\n\r\nBy running in standard eager execution I get the following error output: [log.txt](https://github.com/tensorflow/tensorflow/files/5804098/log.txt)\r\n\r\nWhen switching off eager execution I get the following output (which might indicate the problem, if the change from eager to non-eager does not create systematic difference): [log_no_eager.txt](https://github.com/tensorflow/tensorflow/files/5804099/log_no_eager.txt)\r\n\r\nI moved the generation of the optimizer into the mirrored strategy scope in main and passed it as parameter to the constructor of the trainer class, but that did not solve the problem.\r\n\r\nThe error message seems similar to [that issue](https://github.com/tensorflow/tensorflow/issues/41768), but there was no reproducible solution mentioned. \r\n\r\n", "comments": ["> Training works on single GPU, fails on dual GPU\r\n\r\n@ThWuensche,\r\nCould you please provide the make and model of both the GPUs on your machine. \r\n\r\nAlso, please run the below code snippet and share the output with us. \r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\nThanks!", "As mentioned the GPUs are AMD Radeon VII, running with ROCm 3.7.0, tensorflow-rocm 2.3.0\r\n\r\nHere is the output from running the code:\r\n>>> import tensorflow as tf\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2021-01-13 19:03:44.609744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libamdhip64.so\r\n2021-01-13 19:03:48.603715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:0a:00.0 name: Device 66af     ROCm AMD GPU ISA: gfx906\r\ncoreClock: 1.801GHz coreCount: 60 deviceMemorySize: 15.98GiB deviceMemoryBandwidth: 0B/s\r\n2021-01-13 19:03:48.603802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 1 with properties: \r\npciBusID: 0000:0d:00.0 name: Device 66af     ROCm AMD GPU ISA: gfx906\r\ncoreClock: 1.801GHz coreCount: 60 deviceMemorySize: 15.98GiB deviceMemoryBandwidth: 0B/s\r\n2021-01-13 19:03:48.650101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so\r\n2021-01-13 19:03:48.652980: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so\r\n2021-01-13 19:03:48.673788: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so\r\n2021-01-13 19:03:48.675584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so\r\n2021-01-13 19:03:48.675730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\nNum GPUs Available:  2\r\n>>> \r\n", "I have a second machine with four of these GPUs, there the training should finally run", "@ThWuensche,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46373\">No</a>\n", "The issue is not solved yet. Unfortunately the feedback in form of e-mail reply to the notifications did not make it into the issue communication.\r\n\r\nCould the issue please be reopened, as far as I understand I can not do that if it was closed by a repo maintainer?\r\n\r\nI have in the meantime installed rocm tensorflow 2.4 in a docker container (docker pull rocm/tensorflow:rocm4.0.1-tf2.4-dev from hub.docker.com), but the problem persists.", "Looks like the code triggers some problems in tensorflow or the rocm implementation. With the above mentioned version of tensorflow it works in single GPU configuration, but if I run the same code in rocm/tensorflow:latest, the validate step delivers NaNs.\r\n\r\nMulti-GPU works in none of the two versions.\r\n\r\nIt's not my code, but I couldn't spot any reasons for problems. Would be interesting to see whether it behaves differently on NVidia, however I have only AMD GPUs.", "Thanks for reopening! Is there anything I can do to help analyze and fix the issue? The YOLO package used is easy to install, but needs an extra step to prepare the dataset. Should I prepare a tarball of the dataset?", "Hi @ThWuensche, in #41768 the problem was the use of `tf.compat.v1.disable_eager_execution()`. Disabling eager execution is not recommended in TF2, and can cause issues when using a distribution strategy.\r\n\r\nHowever, since you're facing problems even when eager execution is not disabled, it seems that we need to do some more investigation to figure out what's going on here.\r\n- Can you share what happens if you replace MirroredStrategy with `tf.distribute.get_strategy()`?\r\n- This is a lot of code for us to parse through. Can you try and trim it down (ideally something we can even run in colab)?\r\n- Did you write this code? If not I think this issue is probably better filed in the repo you got the code from.", "Hi @nikitamaia, thanks for your reply. I just returned from a trip and have to get things set up again, I will try your proposal.\r\n\r\nI did not write that code, but used and modified it for a plant detection system for agriculture. So I would be interested to get that solved. Sure it's a lot of code, but I had done investigation before opening the issue and think the code pretty much resembles the examples for distributed execution, the major difference being that in this code it is packed in classes, while in the examples no encapsulation in classes is used. Maybe that changes some memory layout?\r\n\r\nBasically I think if an application works non-distributed, it should also work distributed, having the distribution handled transparently by tensorflow. If some property of the application prevents that, ideally tensorflow should point out what's the problem and how it can be fixed. That's why I opened the issue here. So far I don't see any indication it's the applications fault.\r\n\r\nI'm running with ROCm on Radeon VIIs, don't have NVidia GPUs to test. ", "Hi, I definitely agree that if an application works in the non-distributed case, it should also work in a distributed environment! However, I am not sure what code you running. The link you provided in your original post is for a repo with many models in it, and doesn't seem to have the distributed code in it at all. Without that it's very difficult for us to diagnose and determine if there was an issue when setting up the distribution strategy, or if the error is something in the tf.distribute.Strategy API. We get lots and lots of issues filed here, so the more narrowed down the code is, the quicker we can help figure out the problem. Thanks!", "Dear @nikitamaia, I'm running the YOLOV3 model in that repository.\r\n\r\nThe relevant code seems to be in \"run\" method in:\r\nhttps://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/train.py\r\n\r\nThere are two tf.functions:\r\n\r\nAt line 125:\r\n        @tf.function\r\n        def distributed_train_epoch(dataset, train_summary_writer,\r\n                                    total_steps):\r\n \r\nand at line 182:\r\n        @tf.function\r\n        def distributed_val_epoch(dataset):\r\n\r\nHope that information is helpful in nailing down the problem.\r\n\r\nOnly thing I had to change to run the code is \"self.strategy.experimental_run_v2\" to just \"self.strategy.run\"", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for the bot trying to close this issue! I forgot to remove the awaiting response label :)\r\n\r\nTo clarify: [you ran the code in this file](https://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/train.py) as is with no changes, except for changing `experimental_run_v2` to `run`? Also, I'm assuming [you followed the instructions here](https://github.com/ethanyanjiali/deep-vision/tree/master/YOLO/tensorflow) to download the data and run the training script? If so my suggestion would be to also file an issue with the person who wrote this code since it seems they have a tutorial that is not working.\r\n\r\nIt would also be useful to understand what happens if you replace MirroredStrategy with `tf.distribute.get_strategy()`.\r\n", "Dear Nikita,\r\n\r\nyes, I ran that code without further changes (besides the mentioned one). As mentioned in the beginning of the issue I installed and worked with the VOC 2007 dataset (not MSCOCO). The dataset installed for me without big problems according to the instructions, only thing I had to provide was \"ray\".\r\n\r\nI wanted to test again on the system I have at hand now with tf.distribute.get_strategy(), but in the moment have hassles to get tf 2.4 working on that system. I remember when I switched to tf 2.4 according to your colleagues proposal, I had to rework most of the basic install of my (other) system. The dependencies AMD is creating with its tensorflow-rocm port are not very nice, close constraints regarding as well python as ROCm version. Most of the time at least one of that constraints is hard to fulfill on Debian standard system setup. So I need to spend some time to create a setup to run the test.\r\n\r\nJust to give some feedback to highlight I'm still very interested in the solution.\r\n\r\nBest regards, Thomas", "Dear Nikita,\r\n\r\nfinally I got around to try with `tf.distribute.get_strategy()`. It throws no error, but I seem to have load only on the second GPU, the first seems to be idle. So looks like it would use only one GPU in that setup, though it lists both as active in the output.\r\n\r\nThat is the output:\r\n[tf_log_deepvision_getstrategy.txt](https://github.com/tensorflow/tensorflow/files/6579745/tf_log_deepvision_getstrategy.txt)\r\n\r\nIf I run with MirroredStrategy, I get some warning about nccl and it breaks:\r\n[tf_log_deepvision_unchanged.txt](https://github.com/tensorflow/tensorflow/files/6579863/tf_log_deepvision_unchanged.txt)\r\n\r\nThe NCCL error seems like a system/rocm issue, will look on that issue further. The behavior seems changed. I was running this time with ROCM 4.0.1.", "@deven-amd any ideas what could be going wrong?", "@ThWuensche,\r\n\r\nCan you try updating TF to latest stable version i.e 2.7.0 and let us know if the issue still persists in newer version? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46373\">No</a>\n"]}, {"number": 46372, "title": "Change from model.predict_proba to  using model.predict", "body": "model.predict_proba is deprecated in the Keras code base, and should be replaced by model.predict\r\nThis replaces the usage of the deprecated call in the scikit_learn wrapper", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46372) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 46371, "title": "Exporting a TF Estimator with a `hub.text_embedding_column_v2` does not include the pretrained model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): AI Platform runtime 2.3 or Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nThis issue has been observed with the estimator API only, not Keras. I'm not entirely sure if it's a problem with estimator, export, or a tf hub issue.\r\n\r\n**Describe the current behavior**\r\n\r\nWhen exporting a TF Estimator which uses a `hub.text_embedding_column_v2` (the new version of tf-hub's `text_embedding_column_v2` that supports pretrained models in TF 2 SavedModel format), the export directory does not include the pretrained model asset files. Instead, the model seems to reference the local path the model was trained with.\r\nWhen `text_embedding_column_v2` is fed with a url (i.e. `tfhub.dev/../my-model`), the model references the cached local path.\r\n\r\nWhen the local path doesn't exist anymore, or when using the exported model in a different machine altogether, calling `tf.saved_model.load` fails with a `FileNotFound` exception:\r\n\r\nWhen defining the embedding column with a local path to the pretrained model:\r\n```\r\n ...\r\n hub.text_embedding_column_v2('/home/.../my-local-model')\r\n ...\r\n estimator = ...\r\n ...\r\n tf.estimator.export_saved_model(...)\r\n\r\n...\r\n # on another machine or when removing /home/.../my-local-model:\r\n...\r\ntf.saved_model.load(...)\r\n\r\nNotFoundError:  /content/local_nnlm50_2/assets/tokens.txt; No such file or directory\r\n\t [[{{node dnn/input_from_feature_columns/input_layer/text_feature0_hub_module_embedding/StatefulPartitionedCall_1/StatefulPartitionedCall/text_file_init/InitializeTableFromTextFileV2}}]] [Op:__inference_pruned_10883]\r\n```\r\n\r\nWhen defining the embedding column with a cached pretrained model from a `tfhub.dev` url (not reproducible in Colab since tf hub cache doesn't kick in there):\r\n```\r\n ...\r\n hub.text_embedding_column_v2('https://tfhub.dev/google/nnlm-en-dim50/2')\r\n ...\r\n estimator = ...\r\n ...\r\n tf.estimator.export_saved_model(...)\r\n\r\n...\r\n # on another machine or when removing cache dir /tmp/tfhub_modules:\r\n...\r\ntf.saved_model.load(...)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError:  /tmp/tfhub_modules/74a841d6eb84e8d93d913d716fb5440d020cc291/assets/tokens.txt; No such file or directory\r\n         [[{{node dnn/input_from_feature_columns/input_layer/topClickQuery_1st_hub_module_embedding/StatefulPartitionedCall_1/StatefulPartitionedCall/text_file_init/InitializeTableFromTextFileV2}}]] [Op:__inference_pruned_1658]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is for the export directory to include the pretrained model's assets, like it does when using `hub.text_embedding_column` (v1) with TF 1 format modules. And more importantly: loading the exported estimators should not fail even when loaded in a different machine, as well as not when the original local path to the pretrained model doesn't exist anymore.\r\nThe entire exported estimator should be contained within the export directory. \r\n\r\n**Standalone code to reproduce the issue**\r\nHere's a Colab replicating the issue:\r\nhttps://colab.research.google.com/drive/1Tr5M0m_EVRd5sFdgDX-EbVsk47UokPxD?usp=sharing\r\n\r\nOverview of Colab:\r\n- Download both versions of nnlm50 to local (TF 1 module, TF 2 SavedModel)\r\n- Define two estimators: one with `text_embedding_column` (v1) and the local TF 1 format pretrained model, and the other with `text_embedding_column_v2` and the local TF 2 format pretrained model\r\n- Train and export the estimators to SavedModels\r\n- Print contents of export dir: **TF 1 format has the nnlm50 assets, TF 2 format does not**\r\n- Try to load both estimators from disk: **success**\r\n- Delete both local nnlm50 directories used for training the estimators\r\n- Try to load the v1 estimator from disk again: **success**\r\n- Try to load the v2 estimator from disk again: **fails**\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@edend10,\r\nTensorFlow Hub issues are tracked in tensorflow/hub repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/hub/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "@amahendrakar \r\nThanks for looking into this. I'll gladly post on tf/hub, but just noting I'm not 100% sure this is a tf hub issue and not an estimator issue (I trust your intuition though). I posted here because the issue manifests when using the estimator API, but not when using the Keras API.", "@edend10,\r\nI was able to reproduce the error with TF v2.3 and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/9b3027743ad936469e62f289f327c949/46371-2-4.ipynb). However, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/82de81ae4a654f9928c7d3136ba69fa8/46371-tf-nightly.ipynb). \r\n\r\nI was able to run the code without any errors. Please checked the linked gist for reference. Thanks!", "Looks good to me @amahendrakar !\r\n\r\nPretty sure [this is the commit](https://github.com/tensorflow/tensorflow/commit/6d8366afc435eb0f78d5abff457db26b68f0183d#diff-85d1ba3be0fd0e98ce033a07237e1f76c61058ce93e0f1e12e58a3c61b9291d5) that fixed it.\r\n\r\n`saved_model/load.py` is being called by `hub.KerasLayer`. So it was a tensorflow issue after all :)\r\n\r\nThanks for looking into it! Closing", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46371\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46371\">No</a>\n"]}, {"number": 46370, "title": "[INTEL MKL] DNN 0.x final code cleanup", "body": "This final DNN 0.x code cleanup removes the header file which defines MACRO's that are not usable any more\r\n   1. delete util/mkl_types.h\r\n   2. update util/mkl_util.h, to un-include mkl_types.h\r\n   3. update util/BUILD\r\n\r\nIt also addresses previous merge problems (code cleanup was accidently overwritten, or somehow missed).  with the following source files:\r\n\r\n   1. kernels/mkl/mkl_matmul_ops_common.h \r\n   2. kernels/mkl/mkl_dequantize_op_test.cc\r\n   3. util/mkl_util_test.cc", "comments": ["I would appreciate that you can review and approve this PR, as it is the final one for DNN 0.x code cleanup and touch quite a few common files (MKL related). Thanks!", "> I'm so sorry for the delay and thank you for the PR! Hooray for the final clean-up! :)\r\n\r\nThank you!"]}, {"number": 46369, "title": "Add missing space in error message", "body": "", "comments": []}, {"number": 46368, "title": "[ROCm] Fix for breakage in ROCm support - 210112", "body": "The following commit introduces a new unit-test `//tensorflow/compiler/xla/service/gpu/tests:mlir_sorting_test`, which fails on the ROCm platform.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/70502be4a5941b6ffa2b8c33bb549657b33976da\r\n\r\nThe test fails on the ROCm platform because the underlying code for it is CUDA-centric. This PR/commit updates the test to make it work on the ROCm platform as well.\r\n\r\n----------------------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": ["`Linux GPU` CI job is failing, but I cannot seem to access the logs.....I get the following error when I click on the \"Details\" link\r\n\r\n![Screenshot from 2021-01-12 13-19-43](https://user-images.githubusercontent.com/36858332/104355660-fc730e80-54d8-11eb-9c1e-32d46f49349b.png)\r\n", "@cheshire gentle ping"]}, {"number": 46367, "title": "MultiWorkerMirroredStrategy documents old communication=tf.distribute.experimental.CollectiveCommunication.NCCL", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe page says\r\n> To override the automatic choice, specify a valid value to the communication parameter of MultiWorkerMirroredStrategy's constructor, e.g. communication=tf.distribute.experimental.CollectiveCommunication.NCCL.\r\n\r\nHowever `MultiWorkerMirroredStrategy`s ctor has changed to expect a `communication_options` parameter instead.\r\n\r\nIt should be described on this page how to actually change the distribution implementation with current TF 2.4\r\n\r\nNote that the deprecated `tf.distribute.experimental.MultiWorkerMirroredStrategy()` still has that parameter but TF warns about usage of this class and the tutorial page doesn't mention that (old) class.", "comments": ["Hi @Flamefire, thanks for pointing this out. It seems a change has been made to the docs already. Can you take a look again and let me know if you still see any issues?", "Yes, fixed. Thanks!"]}, {"number": 46366, "title": "LayerNormalization crashes on empty inputs when run on CPU", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.layers.LayerNormalization` crashes when the input is empty and the layer is executed on CPU.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe layer should not crash but return a tensor with the same shape.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\nimport tensorflow as tf\r\nlayer = tf.keras.layers.LayerNormalization()\r\nlayer(tf.zeros([1, 0, 10]))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe code above exits with this error:\r\n\r\n```text\r\nFloating point exception (core dumped)\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Colab session crashes on running the code, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a17885e619f64587f0946ef115003ff9/46366.ipynb#scrollTo=L8qDirj_Zx4v). Thanks!", "@guillaumekln , @amahendrakar , @jvishnuvardhan a tensor of shape [1,0,10] would return a tensor\r\n `tf.Tensor([], shape=(1, 0, 10), dtype=float32) TensorShape([1, 0, 10])` , Any tensor of value [] would cause the runtime to crash. I will send a pull request to raise a valid error regarding the faulty input tensor.", "Can we close this?", "The crash is not fixed. I believe the `FusedBatchNorm` CPU kernel should check if the input is empty. I tried to fix the issue but eventually moved to something else.", "The PRs at the python level was rejected. Do you think that it will be accepted at cpp level?", "The GPU kernel [is checking for empty inputs](https://github.com/tensorflow/tensorflow/blob/v2.5.0-rc1/tensorflow/core/kernels/fused_batch_norm_op.cc#L818), but not the CPU kernel. So I think a code change would make sense here.", "@nikitamaia I think that we could remove the Keras label here as this is a c++ contribution.", "Yes, the issue is more specifically related to `tf.compat.v1.nn.fused_batch_norm` that is called by `tf.keras.layers.LayerNormalization`.", "Still an issue in TF 2.6 Nightly as well.Thanks!", "I believe the issue has been resolved by commit 4b4bc60. Also verified with latest tf-nightly:\r\n```\r\n# python3\r\nPython 3.8.10 (default, Jun  2 2021, 10:49:15) \r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import os\r\n>>> os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n>>> import tensorflow as tf\r\n2021-09-02 15:23:41.438425: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-09-02 15:23:41.438474: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> layer = tf.keras.layers.LayerNormalization()\r\n>>> layer(tf.zeros([1, 0, 10]))\r\n2021-09-02 15:23:42.905740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-09-02 15:23:42.905782: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-09-02 15:23:42.905806: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-87-192): /proc/driver/nvidia/version does not exist\r\n2021-09-02 15:23:42.906125: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n<tf.Tensor: shape=(1, 0, 10), dtype=float32, numpy=array([], shape=(1, 0, 10), dtype=float32)>\r\n```\r\n\r\nI will close this issue for now, but feel free to re-open if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46366\">No</a>\n"]}]