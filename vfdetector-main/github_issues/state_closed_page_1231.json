[{"number": 16232, "title": "Singleton S3Client", "body": "Fixes #16230 .\r\n\r\nThis drastically speeds up performance of interactions with S3, and eliminates a lot of spurious log warnings when interacting with S3 files.\r\n\r\nThe filesystem unit tests went from taking ~40 seconds to taking ~4 seconds with this change, a 10X performance improvement.\r\n\r\nSome items of note:\r\n- I updated the delete test to work on a bucket that had tests run previously. Without this change, a manual wipe of the file in quest was required after each run.\r\n- I moved the request timeout to a central location, instead of being local to the `Sync` operation.\r\n- I eliminated the increased connection timeout for `Sync`, which shouldn't be needed.\r\n- Configuration is no longer a static variable protected by a mutex, but instead created as-needed. This should be non-functional, given that config is only created once during normal operation now.", "comments": ["@drpngx can you take another look, please?", "Sounds reasonable.", "It looks like the failure is legit; there's a test that's asserting that stderr matches a specific string, and it's failing because of items printed out by the AWS client initialization.\r\n\r\nThis exposes a problem with greedy loading of the client: We'll see errors printed out in environments without valid credentials on startup. I'll make the client lazily-initialized, which should fix the test, and keep the errors from showing up unless you're trying to use S3.", "I'm not sure we want these messages spilled out every time we start tensorflow.", "Were these introduced in this PR?", "Formerly, the client was being initialized with each call to the S3 filesystem - so the messages were being printed iff someone accessed S3, but were also printed *each time* someone accessed S3.\r\n\r\nI updated this to initialize lazily, once. The python tests should pass now, but I couldn't verify locally (I was getting an error about an undefined `enum` module trying to run them).", "@drpngx do you approve this?"]}, {"number": 16231, "title": "x86_64 compilation failed", "body": "### System information\r\n\r\n- **MacOS High Sierra 10.13.2**:\r\n- **Python 3.6.3**:\r\n- **TensorFlow Latest Pull from 1/17/18**:\r\n\r\n### Describe the problem\r\nI am following Pete Warden's TensorFlow for Mobile Poets guide and seem to have a found an error. When I run \"tensorflow/contrib/makefile/build_all_ios.sh\" after about 20 minutes it returns an error. \r\n\r\nI have tried running lipo -info /Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a\r\n\r\nand this returns: \r\n\r\nArchitectures in the fat file:\r\n/Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a are: i386 \r\n\r\nI have the entire error script here:\r\nhttps://drive.google.com/file/d/1JovTMGBJKbqzRPBzXy3cIQ-hbz76n0ab/view?usp=sharing\r\n\r\n### Source code / logs\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see \r\ninvocation)\r\nmake: *** [/Users/ryan/Desktop/tensorflow-\r\nmaster/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'x86_64 compilation failed.'\r\nx86_64 compilation failed.\r\n+ exit 1\r\n", "comments": ["@petewarden , could you take a quick look?", "This is a duplicate of #12904, which I believe I've just fixed."]}, {"number": 16230, "title": "A new S3Client is created with all file operations.", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: commit 4595f1cff635ce024e875f0f3d480172731b0b22\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.5.4-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n### Describe the problem\r\n\r\nThe [S3 filesystem](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc) creates a new `Aws::S3::S3Client` object with all interactions with S3. This is a heavyweight object, and takes relatively large amount of time to create and destroy.\r\n\r\nThis should be a singleton associated with the filesystem object.\r\n\r\nFix shortly.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "* Exact command to reproduce: N/A (any s3 file access is slow)", "@yongtang ", "The original poster has replied to this issue after the stat:awaiting response label was applied."]}, {"number": 16229, "title": "Docker patch 14", "body": "", "comments": []}, {"number": 16228, "title": "tf.contrib.rnn.LSTMCell()  dtype not defined. Error when creating initializer for bias variable. ", "body": "- **OS Platform and Distribution**: Mac OSX 10.10.5\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.5.0rc\r\n- **Python version**:   3.6\r\n- **Have I written custom code**: NA\r\n- **Bazel version**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**:NA\r\n\r\n- **Exact command to reproduce**:\r\n\r\nimport tensorflow as tf\r\nlstm = tf.contrib.rnn.LSTMCell(10)\r\ninput_tensor = tf.ones([10,50])\r\nlstm.build(input_tensor.get_shape())\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydevd_bundle/pydevd_exec2.py\", line 3, in Exec\r\n    exec(exp, global_vars, local_vars)\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 719, in build\r\n    initializer=init_ops.zeros_initializer(dtype=self.dtype))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 88, in __init__\r\n    self.dtype = dtypes.as_dtype(dtype)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py\", line 627, in as_dtype\r\n    \"Cannot convert value %r to a TensorFlow DType.\" % type_value)\r\nTypeError: Cannot convert value None to a TensorFlow DType.\r\n\r\n### Describe the problem\r\nIt seems that LSTMCell does get the dtype in __init__ and does not pass to parent object. Then lstm._dtype is always None. \r\nA workearound is to add:\r\nlstm._dtype = 'float32'\r\nbefore:\r\nlstm.build(input_tensor.get_shape())\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Done", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@ebrevdo can correct me, but I think we should probably use the cell that's in core tensorflow instead: `tf.nn.rnn_cell.LSTMCell`.", "@drpngx the contrib cell is an alias.  this is indeed an issue.  we don't currently accept or propagate a dtype argument from the LSTM constructor.", "@arbellea PRs to add this are welcome.", "Added a PR #18178 for the fix."]}, {"number": 16227, "title": "tf.contrib.factorization.KMeansClustering cannot save model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow version**: 1.4\r\n- **Python version**: 2.7.6\r\n- **TensorFlow installed from**: N/A\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nIn [tf.contrib.factorization.KMeansClustering](https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering#model_fn ), the TensorFlow 1.4 version of the KMeans Estimator ([previous version](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/KMeansClustering)), the [export_savedmodel](https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering#export_savedmodel) function throws an error: \r\n\r\n`ValueError: export_outputs must be a dict and not<type 'NoneType'>`\r\n\r\nAs far as I can tell, the older version used the function [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/learn/python/learn/estimators/model_fn.py#L236) to populate export_outputs from the prediction values. The newer version does not do this, rendering it impossible to create a saved model. Instead the [model_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/kmeans.py#L210) returns an EstimatorSpec with no export_outputs. \r\n\r\nTo replicate the issue, create and train a simple contrib.factorization.KMeansClustering Estimator and try to save it.\r\n\r\n`kmeans = tf.contrib.factorization.KMeansClustering(num_clusters = num_clusters)`\r\n`kmeans.train(input_fn = inputFn)`\r\n`kmeans.export_savedmodel(export_dir, exportFn)`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Please upgrade your tensorflow version to 1.6.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@silentttone Did you try 1.6 as @sandeepkumar8713 suggested? What you reported is a tf.contrib.factorization.KMeansClustering bug in versions 1.5 and earlier. Please try 1.6 or later.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/17002\r\n", "Sorry, yes, the issue is fixed in 1.6. Thanks! ", "AttributeError: module 'tensorflow.contrib.factorization' has no attribute 'KMeansClustering'\r\n\r\ntensorflow-gp==1.9.0 but i got an error?\r\nmycode:\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndef kmeansCluster(X,numClusters):\r\n    get_inputs=lambda: tf.train.limit_epochs(tf.convert_to_tensor(X.as_matrix(), dtype=tf.float32), num_epochs=1)\r\n    cluster = tf.contrib.factorization.KMeansClustering(num_clusters=numClusters,use_mini_batch=False)\r\n    cluster.train(input_fn=get_inputs,steps=2000)\r\n    y_pred=cluster.predict_cluster_index(input_fn=get_inputs)\r\n    y_pred=np.asarray(list(y_pred))\r\n    return y_pred\r\n\r\nk=2000\r\npoints=pd.read_csv(\"data/source_data.csv\",usecols=[14,15,16,17,18])\r\n\r\ny= kmeansCluster(points,k)\r\nprint(y)", "I can't reproduce your problem.\r\n\r\n>>> tf.__version__\r\n'1.9.0'\r\n>>> dir(tf.contrib.factorization.KMeansClustering)\r\n['ALL_DISTANCES', 'CLUSTER_CENTERS_VAR_NAME', 'CLUSTER_INDEX', 'COSINE_DISTANCE', 'KMEANS_PLUS_PLUS_INIT', 'RANDOM_INIT', 'SCORE', 'SQUARED_EUCLIDEAN_DISTANCE', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_meta_graph_for_mode', '_assert_members_are_not_overridden', '_call_input_fn', '_call_model_fn', '_convert_eval_steps_to_hooks', '_convert_train_steps_to_hooks', '_create_and_assert_global_step', '_create_global_step', '_evaluate_build_graph', '_evaluate_run', '_export_all_saved_models', '_export_saved_model_for_mode', '_extract_batch_length', '_extract_keys', '_get_export_outputs_for_spec', '_get_features_and_labels_from_input_fn', '_get_features_from_input_fn', '_maybe_warm_start', '_predict_one_key', '_tf_api_names', '_train_model', '_train_model_default', '_train_model_distributed', '_train_with_estimator_spec', '_validate_features_in_predict_input', 'cluster_centers', 'config', 'eval_dir', 'evaluate', 'export_savedmodel', 'get_variable_names', 'get_variable_value', 'latest_checkpoint', 'model_dir', 'model_fn', 'params', 'predict', 'predict_cluster_index', 'score', 'train', 'transform']"]}, {"number": 16225, "title": "maxout lose the number of features in the shape of its output", "body": "In tf.contrib.layers.maxout(), when the shape of \"inputs\" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3d case.\r\nSince \"num_units\" has specified the final number of features in the maxout axis, the output should set its shape accordingly:\r\nhttps://github.com/tensorflow/tensorflow/pull/16114", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "leave them as N/A", "The original poster has replied to this issue after the stat:awaiting response label was applied."]}, {"number": 16224, "title": "Suppress AWS curl init warning", "body": "This shows up each time we run the TensorBoard command, even if we're\r\nnot using anything AWS related.\r\n\r\n```sh\r\njart@compy:~/tmp/aws-sdk-cpp-1.3.15$ grep -R \"Initializing Curl library\" .\r\n./aws-cpp-sdk-core/source/http/curl/CurlHttpClient.cpp:        AWS_LOGSTREAM_INFO(CURL_HTTP_CLIENT_TAG, \"Initializing Curl library\");\r\n```", "comments": ["@jart Indeed it was really annoying to see \"Initializing Curl library\" at the beginning of every run. Thanks for the fix!"]}, {"number": 16223, "title": "Fixing a typo for the argument to docker push. (#16204)", "body": "", "comments": []}, {"number": 16222, "title": "Branch 182384458", "body": "", "comments": []}, {"number": 16221, "title": "Meaning of report_tensor_allocations_upon_oom output", "body": "Python: 3.6.2\r\nOS: Ubuntu 16.04\r\nTensorflow: 1.5.0rc1\r\n\r\nWhen running a session with `tf.RunOptions` and `report_tensor_allocations_upon_oom=True` I get the following output at the end of my log.\r\n\r\n1. I am wondering why some entries occur multiple times? How can a single node have multiple allocations? Why are they not summed?\r\n2. Does `Remaining 1252 nodes with 98.80MiB` mean that all 1252 nodes together use 98.80MiB or each single one uses that amount?\r\n3. When summing up all values I get `10.607822265625GiB` but my free GPU space when starting my program is `11.92GiB` so shouldn't there still be enough space??\r\n\r\n```\r\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\r\n  250.78MiB from network/convolutions/conv2d_5/Conv2D\r\n  217.34MiB from network/convolutions/conv2d_5/Conv2D\r\n  203.75MiB from network/convolutions/conv2d_5/Conv2D\r\n  192.91MiB from network/convolutions/conv2d_11/Conv2D\r\n  168.05MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_5/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_5/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_5/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_5/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_2/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_3/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_4/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_5/Conv2D\r\n  167.19MiB from network/convolutions/conv2d_11/Conv2D\r\n  163.84MiB from network/convolutions/conv2d_7/Conv2D\r\n  160.50MiB from network/convolutions/conv2d_7/Conv2D\r\n  140.99MiB from network/convolutions/conv2d_12/Conv2D\r\n  133.75MiB from network/convolutions/conv2d_6/Conv2D\r\n  133.75MiB from network/convolutions/conv2d_7/Conv2D\r\n  133.75MiB from network/convolutions/conv2d_11/Conv2D\r\n  133.75MiB from network/convolutions/conv2d_11/Conv2D\r\n  133.75MiB from network/convolutions/conv2d_11/Conv2D\r\n  133.75MiB from network/convolutions/conv2d_12/Conv2D\r\n  103.66MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_7/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_6/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_7/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_7/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_7/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_7/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_8/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_9/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  83.59MiB from network/convolutions/conv2d_10/Conv2D\r\n  Remaining 1252 nodes with 98.80MiB\r\n```", "comments": ["> I am wondering why some entries occur multiple times? How can a single node have multiple\r\n> allocations? Why are they not summed?\r\n\r\nProbably each allocation is reported separately. One op might use multiple allocations.  It should be easy to filter them together with uniq and count in a unix shell pipe chain.\r\n\r\n> Does Remaining 1252 nodes with 98.80MiB mean that all 1252 \r\n> nodes together use 98.80MiB or each single one uses that amount?\r\nThat means that there are smaller allocations that sum to 98MiB, i.e. it's stuff that is not a big enough allocation to report.\r\n\r\n> When summing up all values I get 10.607822265625GiB but my free GPU space when starting my \r\n> program is 11.92GiB so shouldn't there still be enough space??\r\n\r\nMemory fragmentation can occur just like it can on the CPU. A tensor needs to be contiguous i.e. if you need 40MiB and you only have 10 x 4MiB chunks, you cannot complete the allocation\r\n\r\nHope this helps.\r\n", "That's what I thought but I wasn't sure. Thank you for clarifying."]}, {"number": 16220, "title": "Fix result shape of tf.tensordot unknown when axes is an integer number", "body": "#8452 add the function \"Tensordot partial shape inference\", solves the problem #6682.\r\nHowever, the shape of result of `tensordot` is still `<unknown>` when `axes` is an integer N, which is in common use.\r\nFor example, \r\n```\r\na = tf.placeholder('float32', shape=[None, 100])\r\nb = tf.placeholder('float32', shape=[100, 300])\r\n```\r\nset `axes=1`,\r\n```\r\nresult_tensordot = tf.tensordot(a, b, axes=1)\r\nresult_tensordot.get_shape()  # TensorShape(None)\r\nresult_tensordot.get_shape().as_list()  # Error\r\n```\r\nThe equivalent `axes=[[1], [0]]` behaves correctly,\r\n```\r\nresult_tensordot = tf.tensordot(a, b, axes=[[1], [0]])\r\nresult_tensordot.get_shape()  # TensorShape([Dimension(None), Dimension(300)])\r\nresult_tensordot.get_shape().as_list()  # [None, 300]\r\n```\r\nThe simplified is more common and the partial shape should be inferred correctly.\r\nThis PR solves the problem.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@rmlarsen \r\nI also found a problem with the previous check code on `axes`, which needs `axes>= 1`. \r\n```\r\n      if axes < 1:\r\n        raise ValueError(\"'axes' must be at least 1.\")\r\n```\r\nHowever, `axes=0` is also correct in numpy meaning tensor outer product a \u2297 b in [np.tensordot](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.tensordot.html). Moreover,\r\nthe equivalent `axes=[[], []]` is  correctly behaved in current tensorflow code. So, I change the code \r\nto\r\n```\r\n      if axes < 0:\r\n        raise ValueError(\"'axes' must be at least 0.\")\r\n```\r\nAnd also, checks that `axes` less equal than the ndim of tensor's shape for meaning of the last or first N dimensions.\r\n\r\n\r\nAnother doubt on that `np.tensordot` has a default `axes=2`  for tensor double contraction, is it necessary to change the `tf.tensordot` behavior more like numpy?", "@lspvic Thanks for fixing the additional bug. I would prefer to keep the existing default for now, since changing it might break existing code. We don't have to match Numpy in all cases, but I agree that it might make sense here. Would you be willing to make that single change in a separate PR?", "@rmlarsen Thanks for your advise. Now this PR do the following things and should be compatible with previous codes:\r\n1. Fix the bug that scalar `axes` will not output correct tensor shape. Test is add for `tensordot(a, b, 1)` on `test_partial_shape_inference`;\r\n2. `axes` can be set to 0 now while previous codes just disable 0. Actually `axes=0` is supported before with the equivalent format `axes=[[[], []]`. Tests for `axes=0` and `axes=[[], []]` are added and they both works well.\r\n3. Additional check on that `axes` scalar values must be less or equal than either of input tensors. Test case is also added.", "@lspvic excellent, thank you for this contribution!"]}, {"number": 16219, "title": "build&link tensorflow lite c++ library Error", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1\r\nPython version: 2.7.12\r\nBazel version (if compiling from source): 0.8.1\r\nGCC/Compiler version (if compiling from source): g++ 5.4.0\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\nExact command to reproduce: g++ -std=c++11 -I...tensorflow -L. -lframework demo.cpp\r\nDescribe the problem\r\n\r\nI run 'bazel build //tensorflow/contrib/lite:framework' and get libframework.so. Then I use libframework.so in my own code, but get undefined reference error when compile with g++:\r\n/temp/ccYTZw2h.o: In function 'main':\r\ndemo.cpp:(.text+0x46): undefined reference to 'tflite::DefaultErrorReporter()'\r\ndemo.cpp:(.text+0x6a): undefined reference ro 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n......\r\n\r\nI get following lines by 'nm libframework.so | grep 'DefaultErrorReporter'':\r\n000000000001b1b0 b _ZGVZN6tflite20DefaultErrorReporterEvE14error_reporter\r\n0000000000007990 T _ZN6tflite20DefaultErrorReporterEv\r\n000000000001b1a8 b _ZZN6tflite20DefaultErrorReporterEvE14error_reporter\r\n\r\nI'm not familiar with how to use tensorflow lite. Where is the problem could be?", "comments": ["Could you provide demo.cpp? TO get a separate linkable Dso, you need to make a separate cc_binary rule in tensorflow/contrib/lite/BUILD i.e.\r\n\r\n```python\r\ncc_binary(\r\n    name = \"libtflite.so\",\r\n    deps = [\":framework\"],\r\n    linkshared=1\r\n)\r\n```\r\nThen you can do\r\n\r\n```\r\ng++ --std=c++11 -Itensorflow/contrib/lite -I. /tmp/foo.cpp -Lbazel-bin/tensorflow/contrib/lite -ltflite\r\n```\r\n\r\nGood luck!", "@aselle Thank you very much! This is the code of demo.cpp :\r\n\r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/string_util.h\"\r\n#include \"tensorflow/contrib/lite/tools/mutable_op_resolver.h\"\r\n\r\nusing namespace std;\r\n\r\nint main() {\r\n\r\n    const string graph_file=\"\";\r\n    std::unique_ptr<tflite::FlatBufferModel> model(\r\n            tflite::FlatBufferModel::BuildFromFile(graph_file.c_str())\r\n    );\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n// Resize input tensors, if desired.\r\n    interpreter->AllocateTensors();\r\n    float* input = interpreter->typed_input_tensor<float>(0);\r\n// Fill `input`.\r\n    interpreter->Invoke();\r\n//  float* output = interpreter->type_output_tensor<float>(0);\r\n}", "# Oh! It's a pity that the same error occurred.\r\n I have built the 'libtflite.so' by using bazel and tried to link it by using :\r\n'g++ --std=c++11 -I/usr/local/include/tf -I/usr/include/flatbuffers/include -L/home/yd/workspace/tensorflow/bazel-bin/tensorflow/contrib/lite -ltflite demo.cpp -o demo'.\r\nBut it does not work. The errors also are:\r\n\r\ndemo.cpp:(.text+0x52): undefined reference to `tflite::DefaultErrorReporter()'\r\ndemo.cpp:(.text+0x79): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\ndemo.cpp:(.text+0x88): undefined reference to `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\ndemo.cpp:(.text+0xbe): undefined reference to `tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\ndemo.cpp:(.text+0xd7): undefined reference to `tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\ndemo.cpp:(.text+0xfd): undefined reference to `tflite::Interpreter::AllocateTensors()'\r\ndemo.cpp:(.text+0x137): undefined reference to `tflite::Interpreter::Invoke()'\r\n\r\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@lllidan-yuandian if you have not figured out how to resolve it. Add \r\n`\"//tensorflow/contrib/lite/kernels:builtin_ops\"` to libtflite.so should work. That is, the rule should be \r\n\r\n```\r\ncc_binary(\r\n    name = \"libtflite.so\",\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n    ],\r\n    linkshared=1\r\n)\r\n```\r\n", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@lllidan-yuandian, any luck with your problem with @freedomtan's suggestion?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@lllidan-yuandian Hi\uff0chave you solved this? I see you add \"flatbuffers\" when compiling\uff1a\"/usr/include/flatbuffers/include\". Is this need to install additionally?\r\n\r\nI have the compiling problem.\r\nThe error infomation:\r\n/tensorflow/contrib/lite/schema/schema_generated.h:21:37: fatal error: flatbuffers/flatbuffers.h: No such file or directory\r\n #include \"flatbuffers/flatbuffers.h\"\r\n\r\n**Can you tell how the solve the \"flatbuffers\" problem?**\r\nThis is my include in cpp file:\r\n#include \"tensorflow/contrib/lite/kernels/register.h\"\r\n#include \"tensorflow/contrib/lite/model.h\"\r\n#include \"tensorflow/contrib/lite/string_util.h\"\r\n#include \"tensorflow/contrib/lite/tools/mutable_op_resolver.h\"", "@conser12 Have you have some idea? I have the same problem.", "@sosong I'm trying to install \"flatbuffers\" library, the missing library error not occurred. Now i'm fixing another compiling problem.", "I had the same problem and solved it by adding a new target similar to @aselle to `tensorflow/lite/experimental/c`\r\n```\r\ncc_binary(\r\n    name = \"libtflite.so\",\r\n    deps = [\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\"\r\n    ],\r\n    linkshared=1\r\n)\r\n```"]}, {"number": 16218, "title": "[feature request ? ]  How to return SparseTensor when custom ops", "body": "------------------------\r\n### Describe the problem\r\nthere exists some ops (eg: decode_libsvm) that can return SparseTensor by three dense tensor\r\n```\r\n#see: tensorflow/tensorflow/contrib/libsvm/\r\nREGISTER_OP(\"DecodeLibsvm\")\r\n    .Input(\"input: string\")\r\n    .Output(\"label: label_dtype\")\r\n    .Output(\"feature_indices: int64\")\r\n    .Output(\"feature_values: dtype\")\r\n    .Output(\"feature_shape: int64\")\r\n    .Attr(\"dtype: {float, double, int32, int64} = DT_FLOAT\")\r\n    .Attr(\"label_dtype: {float, double, int32, int64} = DT_INT64\")\r\n    .Attr(\"num_features: int >= 1\")\r\n```\r\nCan I define my custom ops which can return SparseTensor directly? ,  \r\n\r\nWhat I want to do is modify tf.decode_csv ,  some column can return Tensor,  some column can return SparseTensor. \r\n If i return SparseTensor by (indices,values, shape) dense tensor,  it would be  diffcult . \r\ndoes there exists SparseTensor class in c++ api ?\r\n@all, @mrry @yongtang  \r\nThanks\r\n\r\n```\r\n  def parse_csv(value):\r\n    print('Parsing', data_file)\r\n    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\r\n    features = dict(zip(_CSV_COLUMNS, columns))\r\n    labels = features.pop('income_bracket')\r\n    return features, tf.equal(labels, '>50K')\r\n```\r\n\r\n### Have I written custom code\r\n### OS Platform and Distribution\r\n### TensorFlow installed from\r\n### TensorFlow version\r\n### Bazel version\r\n### CUDA/cuDNN version\r\n### GPU model and memory\r\n### Exact command to reproduce", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16217, "title": "Windows: Add missing dependencies in lib_proto_parsing", "body": "Fix http://ci.tensorflow.org/job/tf-master-win-bzl/2275/console\r\nCulprit: https://github.com/tensorflow/tensorflow/commit/ccbd14b741e6efbe51769f0f1b9cb3719c42c23b\r\n@gunan @panyx0718 ", "comments": ["Test at http://ci.tensorflow.org/view/TF%20pull%20requests/job/tensorflow-pr-win-bazel/57/console", "Thanks. It's probably due to types.h including cpu_info.h on Windows. I'm wondering why didn't the internal test catch it."]}, {"number": 16216, "title": "Resize tensor in tflite", "body": "hi,I save my cnn model into pb file, and  change it into tflite model.  In android I can run the model well with tensorflow lite. \r\n\r\n I have a problem that the input tensor size of inference must be euqal to the input tensor size of training, because the intermediate variables  size of the model are determined by the input tensor size of training. But for my  cnn model, it can process the input tensor size that is not euqal to the input tensor size of training. \r\n\r\n so  How can  i reshape the intermeidate varibales shape to match the  input tensor size of inference before inference? In other words\uff0c How can i do to let my cnn model process the input tensor whose size is not euqal to the input tensor size of traning? thx.", "comments": ["Call ResizeInputTensor() then call AllocateTensors().", "@andrehentz , in the run() function , there  are ResizeInputTensor() and AllocateTensors() before Invoke(), so I'm confused about what your said. \r\nps: I call resizeInput() before run() in my Java code.", "during training, input picture size is 64x64, so the intermediate variables sizes in pb file is related to 64x64. For example, variable A's shape is tf.reshape(input,[-1,h*4,w*4,4]),for 64x64,variable A 's shape is [-1,256,256,3]. During inference, intput size is 128X128, for variable A should be [-1,512,512,3], but variable A is  still [-1,256,256,3],so the result is wrong because the first dimension is error.", "Try passing [1,128,128,3] to your resizeInput call. (The special value -1 won't work in inference because the batch size needs to be fully specified)", "@lucaswu did that fix your problem?", "@andrehentz 's suggestion is not OK for me. After i have modify  c plus plus code of tflite, it can run now for any size. @drpngx ", "OK, is that something that needs a fix in tflite?", "Closing this one. Reopen if you still encounter issues.", "@andrehentz \r\nI encount the same problem when I use mobilenet ssd models. The training input tensor size is 300x300, and I want to use different input tensor size in inference, so I use the **ResizeInputTensor()** function:\r\n`\r\n\r\n    std::vector<int> sizes = {1, 240, 240, 3};\r\n\r\n    interpreter_->ResizeInputTensor(interpreter_->inputs()[0], sizes);\r\n\r\n    if (interpreter_->AllocateTensors() != kTfLiteOk) {\r\n        LOG(INFO) << \"Failed to allocate tensors!\" << \"\\n\";\r\n        return false;\r\n    }`\r\n\r\n**But I got an error in AllocateTensors():**\r\ntensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (2700 != 4332)\r\nNode number 36 (RESHAPE) failed to prepare.\r\nFailed to allocate tensors!\r\n\r\nHow to fix it? if I want to change input tensor size freely, what should I do?\r\n", "@andrehentz Does have same problem in using `ResizeInputTensor`", "Thanks for the info. Generally speaking it should be possible to change the input tensor size just like you are doing. However, in some cases the conversion process hardcode the input shapes, which then prevents resizing at inference time. This is mostly dependent on the model architecture. In your case, I'd suggest running the conversion again, and passing --input_shapes=1,240,240,3. (Of course, this assumes you are not using a pre-converted model. If that's the case, you will need to rescale the image before passing it into the model).", "https://github.com/tensorflow/tensorflow/issues/22377#issuecomment-444550814", " See https://github.com/tensorflow/tensorflow/issues/23600 for a possible dupe.", "https://github.com/tensorflow/tensorflow/issues/16216#issuecomment-361487585\r\n\r\n@lucaswu what did you change in c++? I'm resizing the batch size, keeping width, height and channels the same, but only the input tensors change, when allocation is happening I get that a reshape node fails because the input & output shapes aren't the same.\r\n\r\nI don't know how to find which operation doesn't modify the batch size of the output tensors", "Any update on this? The result of batch are all the same at the output"]}, {"number": 16215, "title": "Tensorflow doesn't delete previous checkpoints", "body": "### System information\r\n- Linux Ubuntu 16.04:\r\n- Tensorflow version 1.4.1*:\r\n- Python 3.5.2: \r\n\r\n### Describe the problem\r\n\r\nA brief summary is that, if I run multiple times my training script tensorflow doesn't delete the checkpoints created in previous runs of the script.\r\n\r\nI am preparing a automatic script that every X days runs and train with the new data collected. But I am facing a problem, even that I have configured the saver to keep the 2 last checkpoints, it doesn't work as I expected. \r\n\r\nExample:\r\nI configure to run 100.000 iterations and each 10.000 to save the checkpoint. The system works and starts saving 10.000, 20.000, ... And when get to 30.000 starts deleting the firsts checkpoints. When the script ends I have the 2 last checkpoints(90.000 and 100.000). \r\n\r\nThen when I train again the system starts from the last checkpoint, in this example the 100.000, and do the same as the previous, 110.000, 120.000,.. and when gets to the 130.000 starts to delete the 100.000 and so on. But the 2 checkpoints from the previous run(90.000 and 100.000) remain there even that in the checkpoint txt are not listed there.\r\n\r\nThis will be repeated in every run of the script, creating files that I don't need anymore and growing during the time.\r\n\r\nThis is an intended behavior(expecting to the user to delete or manage manually) or it is really a problem?\r\nIt exist any workaround?\r\n\r\nThank you for your time and amazing work. \r\n ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is intended behavior. I suggest you create a wrapper script and use a separate directory for each new run.\r\n", "Take a look at the source code of tf.train.SessionManager. In order to restore all the checkpoints listed in the \"checkpoint\" file, and hence to delete the old ones during the next run, they use something like this:\r\n```python\r\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\r\nsaver.restore(sess, ckpt.model_checkpoint_path)\r\nsaver.recover_last_checkpoints(ckpt.all_model_checkpoint_paths)\r\n```\r\n"]}, {"number": 16214, "title": "Unable to locate package cuda-command-line-tools", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:r1.5\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\nIn the official installing guide of r1.5, the libcupti-dev library is required to run tensorflow with GPU support. When issue the following command line for CUDA Toolkit >= 8.0:\r\n`$ sudo apt-get install cuda-command-line-tools`\r\nI got this error:\r\n`$ E: Unable to locate package cuda-command-line-tools`\r\nIt can't be solved after updating source list.\r\nI have tried  on my desktop and a VM instance on Google Cloud Platform, both with Linux Ubuntu 16.04.\r\n### Source code / logs", "comments": ["As part of an answer to your email @Queequeg92\r\nI found it important to explicitly set the ENV vars to the correct version like\r\n\r\n```bash\r\nexport CUPIT_LIB_PATH=${OPT_PATH}/cuda/toolkit_9.0/cuda/extras/CUPTI/lib64\r\nexport LD_LIBRARY_PATH=${CUPIT_LIB_PATH}:$LD_LIBRARY_PATH\r\n```\r\n(same for Toolkit 8.0)\r\nin my `.bashrc`. Note there might be some old `libcupti.so*` in `/usr/lib/x86_64-linux-gnu/libcupti.so`:\r\n\r\n```console\r\nuser@host $ ll ${OPT_PATH}/cuda/toolkit_9.0/cuda/extras/CUPTI/lib64\r\nlibcupti.so\r\nlibcupti.so.9.0\r\nlibcupti.so.9.0.176*\r\nuser@host $ locate libcupti.so\r\n/usr/lib/x86_64-linux-gnu/libcupti.so\r\n/usr/lib/x86_64-linux-gnu/libcupti.so.7.5\r\n/usr/lib/x86_64-linux-gnu/libcupti.so.7.5.18\r\n```\r\n\r\nThe correct package for apt-get would be `sudo apt-get install libcupti-dev`", "Tensorflow's Linux installation guide is misleading. Try a search and here is the result.\r\n\r\n$ sudo apt-cache search cuda-command-line-tool\r\ncuda-command-line-tools-8-0 - CUDA command-line tools\r\ncuda-command-line-tools-9-0 - CUDA command-line tools\r\ncuda-command-line-tools-9-1 - CUDA command-line tools\r\n\r\nWhat you should run is something like \"sudo apt install cuda-command-line-tools-9-1\"\r\n\r\n\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler  yup still an issue.  @cyrilzh  solution works. tensorflow documentation here has to be updated : sudo apt-cache search cuda-command-line-tools-9-0", "Hello Team,\r\nI am still getting the same error after using @cyrilzh solution.\r\n$sudo apt-cache search cuda-command-line-tool\r\n$ sudo apt install cuda-command-line-tools-9-1\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Unable to locate package cuda-command-line-tools-9-1\r\n\r\n", "I am in the same boat as AIGyan.  Unable to locate command line tools package.", "@AIGyan  @bjenkinsgit \r\nhttps://www.tensorflow.org/install/install_linux   is a some parts accurate some parts misleading and someparts inaccurate. \r\n\r\nLike @cyrilzh  suggested do :+1: \r\n:~$ sudo apt-cache search cuda-command-line-tool\r\n[sudo] password for manbharae:\r\ncuda-command-line-tools-8-0 - CUDA command-line tools\r\ncuda-command-line-tools-9-0 - CUDA command-line tools\r\ncuda-command-line-tools-9-1 - CUDA command-line tools  \r\n\r\nbut don't do use cuda-command-line-tools-9-1, \r\nTensorflow supports only 9-0 at the moment. \r\n\r\nuninstall cuda 9-1 and install cuda-9-0.\r\nthen install cuda-command-line-tools-9-0\r\n\r\nhope it helps.\r\nThis worked for me.\r\n", "@bjenkinsgit @AIGyan I'm using Ubuntu 16.04 and I resolved the issue by downloading the package from http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/\r\n```\r\ncurl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-9-0_9.0.176-1_amd64.deb\r\ndpkg -i ./cuda-9-0_9.0.176-1_amd64.deb\r\napt-get update\r\napt-get install cuda-9-0\r\n```\r\nThis will also install `cuda-command-line-tools-9-0`", "@lejafar I had installed Cuda 9.0 from Nvidia + cudnn 7 and when I tried your suggestion...\r\n```\r\nsudo dpkg -i ./cuda-9-0_9.0.176-1_amd64.deb\r\nSelecting previously unselected package cuda-9-0.\r\n(Reading database ... 127348 files and directories currently installed.)\r\nPreparing to unpack ./cuda-9-0_9.0.176-1_amd64.deb ...\r\nUnpacking cuda-9-0 (9.0.176-1) ...\r\ndpkg: dependency problems prevent configuration of cuda-9-0:\r\n cuda-9-0 depends on cuda-toolkit-9-0 (>= 9.0.176); however:\r\n  Package cuda-toolkit-9-0 is not installed.\r\n cuda-9-0 depends on cuda-runtime-9-0 (>= 9.0.176); however:\r\n  Package cuda-runtime-9-0 is not installed.\r\n cuda-9-0 depends on cuda-demo-suite-9-0 (>= 9.0.176); however:\r\n  Package cuda-demo-suite-9-0 is not installed.\r\n\r\ndpkg: error processing package cuda-9-0 (--install):\r\n dependency problems - leaving unconfigured\r\nErrors were encountered while processing:\r\n cuda-9-0\r\n```", "In the end I downloaded the Tensorflow 9.1 source, installed bazel and other tools and compiled the source for my GPU.  I now have Tensorflow 9.1, cudnn 7.0.5 running on Ubuntu 17.10.", "I tried \r\n$ sudo apt-cache search cuda-command-line-tool\r\nbut got nothing.\r\nI have cuda 9.0,cudnn7.05 running on Ubuntu 16.04.\r\nIs there any other solution?\r\nTHX:-D", "@zxr12748 \r\n> I tried\r\n> $ sudo apt-cache search cuda-command-line-tool\r\n> but got nothing.\r\n\r\nSame here... Did you perhaps find a solution? ", "Based on @lejafar 's suggestion, I downloaded http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-command-line-tools-9-1_9.1.85-1_amd64.deb and then ran\r\n\r\n`sudo dpkg -i cuda-command-line-tools-9-1_9.1.85-1_amd64.deb`\r\n\r\nHowever, it will remind you to install some dependency packages first, such as cuda-nvprof-9-1. Thus I installed all dependency packages and then installed cuda-command-line-tools-9-1 successfully.", "Yes, this is still an issue, @tensorflowbutler ", "This is not necessary. CUPTI has been installed along with CUDA. \r\nJust set the LD_LIBRARY_PATH, and tf works fine.", "Do following things. Hope it helps :\r\n$ sudo apt-get install cuda-9.0\r\n$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64\r\n\r\nDownload cuDNN for 9.0 (You need to register before downloading)\r\nhttps://developer.nvidia.com/rdp/form/cudnn-download-survey\r\n\r\n$ sudo dpkg -i libcudnn7_7.1.2.21-1+cuda9.0_amd64.deb \r\n\r\nClose all terminal and open new\r\n$ source activate tensorflow \r\n$ python \r\n#> > import tensorflow as tf\r\n\r\nYou should not get any error now. @NataliaDiaz @Netroman @zxr12748 ", "importing tensorflow doesn't necessarily check if cuda is installed, I may be wrong.", "@cyrilzh solution worked (sudo apt install cuda-command-line-tools-9-0), however it apparently got installed already somewhere along the way (i installed nvidia gpu driver, cuda 9.0, cudnn 7.0.5 runtime and cudnn 7.0.5 dev lib).", "It should be installed already, just set the env to path is fine.", "As pointed out by @vissac and @manbharae, cuda-command-line-tools-9-0 is already installed when you load cuda. (I had to install the CUDA package using the local deb)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I solved the problem and got 9.1 installed.", "The documentation is still incorrect at the moment.", "@lejafar Prior to installing that package, you installed and cuda package using dpkg? I started with ony the vnidia drivers install, the I followed your comment but had the same error as @themightyoarfish . After that whenever I use `sudo apt-get intall` I get a messave of unmenment dependencies that includes `cuda-toolking-9.0`. How did you guuys solve the issue?\r\n", "After about literally an hour of dependency hell, I managed to find all the `.deb`s I needed for `command-line-tools`  for Ubuntu 16.04 compatible with **`cuda 9.2`** on [this page.](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/) (i.e., without having to downgrade to `cuda 9.0` or `cuda 9.1`). Similar to @max0x 's solution, I guess. ", "I also faced the same issue @Queequeg92 then i tried \r\nsudo apt install cuda-command-line-tools-9-0\r\nThis is working"]}, {"number": 16213, "title": "Non-chief replicas freeze after chief completes training", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9, 1.4.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 6.0\r\n- **GPU model and memory**: Titan X (Pascal), 12 GiB\r\n- **Exact command to reproduce**: Custom Script\r\n\r\n### Describe the problem\r\nNon-chief replicas freeze after chief completes training, when in synchronous mode.\r\nThe problem appears to occur immediately after the chief shuts down.\r\n\r\nSee attached source code for a complete example of multi-GPU demonstrating the problem on a toy dataset. Modify the cluster variable and start the PS first, followed by workers then the chief node last. This is somewhat broken out in run_distributed.sh.\r\n\r\n``\r\n        sv = tf.train.Supervisor(\r\n            is_chief=(FLAGS.task_index == 0),\r\n            global_step = global_step,\r\n            init_op = init_op\r\n        )\r\n        ...\r\n        with sv.prepare_or_wait_for_session(server.target, config=config) as sess:\r\n            # is chief\r\n            if FLAGS.task_index == 0:\r\n                sv.start_queue_runners(sess, [chief_queue_runner])\r\n                sess.run(init_token_op)\r\n        ...\r\n``\r\n\r\n### Source code / logs\r\n[rnn-multi-gpu.zip](https://github.com/tensorflow/tensorflow/files/1641799/rnn-multi-gpu.zip)\r\n\r\n", "comments": ["Maybe @mrry or @suharshs have suggestions, or know others who might have suggestions.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I think the answer here is to use an external solution (e.g. a cluster manager like Kubernetes or Mesos) to notice that the chief has terminated and bring down the other replicas. As far as I know there is no signaling built into the `tf.train.Supervisor` or its descendants (`tf.train.MonitoredSession`, `tf.estimator.Estimator`) that will cause the non-chief replicas to exit reliably.\r\n\r\n/cc @ispirmustafa in case he knows of any approaches that might work with `tf.train.MonitoredSession` or `tf.estimator.Estimator`.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16212, "title": "fix typo", "body": "fix typo", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16211, "title": "Branch 182305106", "body": "", "comments": []}, {"number": 16210, "title": "added CMake options to provide external zlib, GRPC, Eigen", "body": "Here are changes necessary to build tensorflow with different version of GRPC, Protobuf, Eigen, zlib, etc. It is not possible to compile project using two different versions of protobuf for example. To deal with that I've chosen approach similar to one in GRPC library. Copy of https://github.com/tensorflow/tensorflow/pull/14463#issuecomment-343576192", "comments": ["Can one of the admins verify this patch?", "I see this error in the build:\r\n```\r\n    40>CustomBuild:\r\n         fatal: reference is not a tree: b04e5cba356212e4e8c66c61bbe0c3a20537c5b9\r\n         CMake Error at T:/src/github/tensorflow/cmake_build/protobuf/tmp/protobuf-gitclone.cmake:65 (message):\r\n           Failed to checkout tag: 'b04e5cba356212e4e8c66c61bbe0c3a20537c5b9'\r\n```\r\nRerunning, but do you think there is a typo in the edited files?", "@Slonegg there is an error:\r\n\r\n```\r\n   168>PrepareForBuild:\r\n         Creating directory \"x64\\Release\\png_create_destination_dir\\\".\r\n         Creating directory \"x64\\Release\\png_create_destination_dir\\png_crea.526FE762.tlog\\\".\r\n       InitializeBuildStatus:\r\n         Creating \"x64\\Release\\png_create_destination_dir\\png_crea.526FE762.tlog\\unsuccessfulbuild\" because \"AlwaysCreate\" was specified.\r\n       CustomBuild:\r\n         Building Custom Rule T:/src/github/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n         CMake does not need to re-run because T:\\src\\github\\tensorflow\\cmake_build\\CMakeFiles\\generate.stamp is up-to-date.\r\n   168>CUSTOMBUILD : CMake error : cmake version 3.7.2 [T:\\src\\github\\tensorflow\\cmake_build\\png_create_destination_dir.vcxproj]\r\n         Usage: C:\\Program Files\\cmake\\bin\\cmake.exe -E <command> [arguments...]\r\n         Available commands: \r\n           capabilities              - Report capabilities built into cmake in JSON format\r\n           chdir dir cmd [args...]   - run command in a given directory\r\n           compare_files file1 file2 - check if file1 is same as file2\r\n           copy <file>... destination  - copy files to destination (either file or directory)\r\n           copy_directory <dir>... destination   - copy content of <dir>... directories to 'destination' directory\r\n           copy_if_different <file>... destination  - copy files if it has changed\r\n           echo [<string>...]        - displays arguments as text\r\n           echo_append [<string>...] - displays arguments as text but no new line\r\n           env [--unset=NAME]... [NAME=VALUE]... COMMAND [ARG]...\r\n                                     - run command in a modified environment\r\n           environment               - display the current environment\r\n           make_directory <dir>...   - create parent and <dir> directories\r\n           md5sum <file>...          - create MD5 checksum of files\r\n           remove [-f] <file>...     - remove the file(s), use -f to force it\r\n           remove_directory dir      - remove a directory and its contents\r\n           rename oldname newname    - rename a file or directory (on one volume)\r\n           server                    - start cmake in server mode\r\n           sleep <number>...         - sleep for given number of seconds\r\n           tar [cxt][vf][zjJ] file.tar [file/dir1 file/dir2 ...]\r\n                                     - create or extract a tar or zip archive\r\n           time command [args...]    - run command and return elapsed time\r\n           touch file                - touch a file.\r\n           touch_nocreate file       - touch a file but do not create it.\r\n         Available on Windows only:\r\n           delete_regv key           - delete registry value\r\n           env_vs8_wince sdkname     - displays a batch file which sets the environment for the provided Windows CE SDK installed in VS2005\r\n           env_vs9_wince sdkname     - displays a batch file which sets the environment for the provided Windows CE SDK installed in VS2008\r\n           write_regv key value      - write registry value\r\n         \r\n   168>Done Building Project \"T:\\src\\github\\tensorflow\\cmake_build\\png_create_destination_dir.vcxproj\" (default targets) -- FAILED.\r\n```", "@drpngx @mrry Thanks, guys.", "@mrry looks like the same error...", "Could be due to case sensitivity? The mkdir command has `${png_INCLUDE_DIRS}` whereas it's defined as `${PNG_INCLUDE_DIRS}`.", "Mind rebasing and fixing the errors? Thanks!", "@Slonegg can you look into the test failures (likely case mismatch), and rebase?\r\n\r\nThanks!", "Too many conflicts, sorry folks, I'm tired.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Marking as abandoned. Feel free to reopen if you manage to make progress.", "This is the sort of bug that we can advertise to the SIG Build list (tf-distribute as-was) as help wanted. I will send a message accordingly. "]}, {"number": 16209, "title": "process blocked in session.run", "body": "when i use a process to read a frame from queue what an other process put the frame in from video stream\u3002 but process blocked in session.run\u3002 but the issue doesn't occurred when i used just one process\u3002\r\n\r\nbelow is my code\uff1a\r\nprocess of handle frame \r\n```\r\ndef parse_origin_video_frame(origin_frame, session, detection_graph, category_index):\r\n    image_np_expanded = np.expand_dims(origin_frame, axis=0)\r\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\n    # Each box represents a part of the image where a particular object was detected.\r\n    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\n    # Each score represent how level of confidence for each of the objects.\r\n    # Score is shown on the result image, together with the class label.\r\n    scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n    classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n    # Actual detection.\r\n    (boxes, scores, classes, num_detections) = session.run(\r\n        [boxes, scores, classes, num_detections],\r\n        feed_dict={image_tensor: image_np_expanded})\r\n\r\n    # Visualization of the results of a detection.\r\n    vis_util.visualize_boxes_and_labels_on_image_array(\r\n        origin_frame,\r\n        np.squeeze(boxes),\r\n        np.squeeze(classes).astype(np.int32),\r\n        np.squeeze(scores),\r\n        category_index,\r\n        use_normalized_coordinates=True,\r\n        line_thickness=2)\r\n\r\n    return origin_frame, scores, classes, boxes\r\n\r\n def run(self):\r\n        # Load a (frozen) Tensorflow model into memory.\r\n        detection_graph = tf.Graph()\r\n        with detection_graph.as_default():\r\n            od_graph_def = tf.GraphDef()\r\n            with tf.gfile.GFile(PathManager.get_ckpt_path(), 'rb') as fid:\r\n                serialized_graph = fid.read()\r\n                od_graph_def.ParseFromString(serialized_graph)\r\n                tf.import_graph_def(od_graph_def, name='')\r\n\r\n            sess = tf.Session(graph=detection_graph)\r\n\r\n        n = 0\r\n        t = time.time()\r\n        while True:\r\n            operation_id, frame_id, origin_frame = self.__in_queue.get()\r\n            # try:\r\n            #     operation_id, frame_id, origin_frame = self.__in_queue.get(False, 0.1)\r\n            # except Empty:\r\n            #     print 'video parser queue is empty'\r\n            #     time.sleep(1)\r\n            #     continue\r\n\r\n            n += 1\r\n            now = time.time()\r\n            if now - t > 1:\r\n                print \"parse FPS: \", n, \" time: \", now - t,\r\n                print \" __in_queue: \", self.__in_queue.qsize(), \" __detected_queue: \", self.__detected_queue.qsize()\r\n                t = now\r\n                n = 0\r\n\r\n            updated_frame, score, classes, boxes = parse_origin_video_frame(origin_frame,\r\n                                                                            sess,\r\n                                                                            detection_graph,\r\n                                                                            self.__category_index)\r\n```\r\nprocess of read video capture\uff1a\r\n```\r\n def run(self):\r\n        thread.start_new_thread(self.__recv_msg_from_main,(\"recv_main_process\", \"11\"))\r\n        # thread.start_new_thread(self.__get_out_queue,(\"get_out_queue\", \"11\"))\r\n\r\n        video_capture = WebcamVideoStream(ConfigManager.get_sources(),\r\n                                          ConfigManager.get_width(),\r\n                                          ConfigManager.get_height()).start()\r\n        fps = FPS().start()\r\n        n = 0\r\n        t = time.time()\r\n        while True:\r\n            if self.__gate_open:\r\n                origin_frame = video_capture.read()\r\n                time.sleep(0.1)\r\n                n += 1\r\n                now = time.time()\r\n                if now - t > 1:\r\n                    print \"read FPS: \", n, \" time: \", now - t,\r\n                    print \" __in_queue: \", self.__in_queue.qsize(), \" output_q: \", self.__out_queue.qsize()\r\n                    t = now\r\n                    n = 0\r\n\r\n                frame_rgb = cv2.cvtColor(origin_frame, cv2.COLOR_BGR2RGB)\r\n                self.__in_queue.put((self.__operation_id, self.__frame_id, frame_rgb))\r\n                self.__frame_id += 1\r\n            else:\r\n                time.sleep(0.1)\r\n```\r\n\r\nbelow is the stack\uff1a\r\n```\r\n(gdb) thread apply all bt\r\n\r\nThread 17 (Thread 0x7fa347eba700 (LWP 31711)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58\r\n#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#3  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 16 (Thread 0x7fa3476b9700 (LWP 31712)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58\r\n#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#3  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 15 (Thread 0x7fa346eb8700 (LWP 31713)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58\r\n#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#3  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 14 (Thread 0x7fa3466b7700 (LWP 31714)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58\r\n#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#3  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 13 (Thread 0x7fa32d393700 (LWP 31715)):\r\n#0  0x00007fa3710e5a0b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0\r\n#1  0x00007fa3710e5a9f in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0\r\n#2  0x00007fa3710e5b3b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0\r\n#3  0x00007fa371403856 in PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fa371407941 in lock_PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#6  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#7  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#9  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#10 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#11 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#12 0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#13 0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#14 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#15 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#16 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#17 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#18 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#19 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#20 0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 12 (Thread 0x7fa32cb92700 (LWP 31716)):\r\n#0  0x00007fa3710e6a9b in recv () from /lib64/libpthread.so.0\r\n#1  0x00007fa35afee3b6 in sock_recv_guts () from /root/anaconda2/lib/python2.7/lib-dynload/_socket.so\r\n#2  0x00007fa35afee5c1 in sock_recv () from /root/anaconda2/lib/python2.7/lib-dynload/_socket.so\r\n#3  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fa3713610c7 in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#6  0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#7  0x00007fa3713d14d0 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#9  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#10 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#11 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#12 0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#13 0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#14 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#15 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#16 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#17 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#18 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n---Type <return> to continue, or q <return> to quit---\r\n#19 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#20 0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 11 (Thread 0x7fa327fff700 (LWP 31717)):\r\n#0  0x00007fa3710e670d in read () from /lib64/libpthread.so.0\r\n#1  0x00007fa3680e247f in conn_recv_string.isra.2 () from /root/anaconda2/lib/python2.7/lib-dynload/_multiprocessing.so\r\n#2  0x00007fa3680e26f0 in connection_recv_obj () from /root/anaconda2/lib/python2.7/lib-dynload/_multiprocessing.so\r\n#3  0x00007fa3713d6192 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#6  0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#7  0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#9  0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#10 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#11 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#12 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#13 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#14 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#15 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#16 0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 10 (Thread 0x7fa324c14700 (LWP 31720)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 9 (Thread 0x7fa31bfff700 (LWP 31721)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 8 (Thread 0x7fa31b7fe700 (LWP 31722)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 7 (Thread 0x7fa31affd700 (LWP 31723)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n---Type <return> to continue, or q <return> to quit---\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 6 (Thread 0x7fa31a7fc700 (LWP 31724)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 5 (Thread 0x7fa319ffb700 (LWP 31725)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 4 (Thread 0x7fa3197fa700 (LWP 31726)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 3 (Thread 0x7fa318ff9700 (LWP 31727)):\r\n#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\r\n#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6\r\n#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6\r\n#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#7  0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 2 (Thread 0x7fa3269fe700 (LWP 31731)):\r\n#0  0x00007fa3710e5a0b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0\r\n#1  0x00007fa3710e5a9f in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0\r\n#2  0x00007fa3710e5b3b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0\r\n#3  0x00007fa371403856 in PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fa371407941 in lock_PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#6  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#7  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n---Type <return> to continue, or q <return> to quit---\r\n#9  0x00007fa3713610c7 in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#10 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#11 0x00007fa3713d14d0 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#12 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#13 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#14 0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#15 0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#16 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#17 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#18 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#19 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#20 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#21 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0\r\n#22 0x00007fa37070434d in clone () from /lib64/libc.so.6\r\n\r\nThread 1 (Thread 0x7fa3718d7740 (LWP 31705)):\r\n#0  0x00007fa3706fb7a3 in select () from /lib64/libc.so.6\r\n#1  0x00007fa369cda144 in time_sleep () from /root/anaconda2/lib/python2.7/lib-dynload/time.so\r\n#2  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#3  0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fa3713d870a in PyEval_EvalCode () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#6  0x00007fa3713f193d in run_mod () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#7  0x00007fa3713f2ab8 in PyRun_FileExFlags () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#8  0x00007fa3713f3cd8 in PyRun_SimpleFileExFlags () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#9  0x00007fa371405d3c in Py_Main () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#10 0x00007fa37062dc05 in __libc_start_main () from /lib64/libc.so.6\r\n#11 0x000055e5ed1a387f in _start ()\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I have faced the similar issue while I was running HPM resnet50 benchmark with Tensorflow-1.5.0. But this does not occurs always. It occurs randomly.\r\n\r\nOS Platform and Distribution: RHEL 7.4\r\nTensorFlow installed from: Tensorflow 1.5.0 tag and build myself\r\nTensorFlow version: v1.5.0\r\nBazel version: 0.8.0\r\nCUDA/cuDNN version: CUDA-9.1.85 and CuDNN 7.1.1\r\nGPU model and memory: Tesla V100-SXM2-16GB\r\n\r\nExact command to reproduce:\r\n$ git clone https://github.com/tensorflow/benchmarks.git\r\n$ cd benchmarks\r\n$ git checkout f5d85aef2851881001130b28385795bc4c59fa38\r\n$ cd scripts/tf_cnn_benchmarks/\r\n\r\n$ python tf_cnn_benchmarks.py --num_batches=200 --num_gpus=4 --model=resnet50 --batch_size=64 --local_parameter_device=gpu --variable_update=replicated --data_name=imagenet --data_dir=/data/TF_records", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am not seeing the issue after upgrading the NVIDIA CUDA stack.\r\n\r\nThanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16208, "title": "using string_input_producer with train dataset and validate dataset", "body": "I have two datasets(files), for train and validate respectively. I can successfully load training set thru tf.train.string_input_producer, set num_epochs=5. Then I can iteratively get batch of data to optimize my model.\r\nBut, I got stuck when trying to load my validation set by the same way, the program keeps saying \"OutOfRange Error\" even I didn't set num_epochs in string_input_producer.\r\nCan you supply an example that using string_input_producer  with two or more dataset?\r\nsame as the question on stackoverflow: [here](https://stackoverflow.com/questions/37068324/read-big-train-validation-test-datasets-in-tensorflow)\r\nPlease help me solve the problem. Thank you very much.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have solved the problem, thank you all the same. "]}, {"number": 16207, "title": "Socket issue of run whl", "body": "Hi ,\r\nI have a problem about run tensorflow .whl.  as follow:\r\n\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/commands/install.py\", line 335, in run\r\n    wb.build(autobuilding=True)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 554, in _prepare_file\r\n    require_hashes\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 278, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py\", line 465, in find_requirement\r\n    all_candidates = self.find_all_candidates(req.name)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py\", line 423, in find_all_candidates\r\n    for page in self._get_pages(url_locations, project_name):\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py\", line 568, in _get_pages\r\n    page = self._get_page(location)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py\", line 683, in _get_page\r\n    return HTMLPage.get_page(link, session=self.session)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py\", line 792, in get_page\r\n    \"Cache-Control\": \"max-age=600\",\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py\", line 488, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/download.py\", line 386, in request\r\n    return super(PipSession, self).request(method, url, *args, **kwargs)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py\", line 475, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py\", line 596, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/adapter.py\", line 47, in send\r\n    resp = super(CacheControlAdapter, self).send(request, **kw)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/adapters.py\", line 390, in send\r\n    conn = self.get_connection(request.url, proxies)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/adapters.py\", line 290, in get_connection\r\n    proxy_manager = self.proxy_manager_for(proxy)\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/adapters.py\", line 184, in proxy_manager_for\r\n    **proxy_kwargs\r\n  File \"/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/packages/urllib3/contrib/socks.py\", line 154, in __init__\r\n    \"Unable to determine SOCKS version from %s\" % proxy_url\r\nValueError: Unable to determine SOCKS version from socks:*********", "comments": ["modify pip source code to use http socket type is ok", "This seems to be a pip / python problem rather than a TensorFlow problem? Are you able to install any pip packages?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to a lack of a response."]}, {"number": 16206, "title": "make label_image for tflite build again", "body": "1. add namespace to label_image.h to make label_image for tflite build again\r\n2. add --config monolithic and mention NDK settings in label_image.md\r\n3. fix a typo in display_usage()", "comments": ["Can one of the admins verify this patch?", "@freedomtan Thanks!"]}, {"number": 16205, "title": "Graph Transform Tool unable to build in TF source r1.5?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source \r\n- **TensorFlow version (use command below)**: r1.5\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: 0.5.4\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: 1080Ti\r\n- **Exact command to reproduce**:\r\n```\r\nsudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\nsudo ldconfig\r\nbazel clean\r\nbazel build tensorflow/tools/graph_transforms:transform_graph --verbose_failures\r\n```\r\n\r\n\r\n### Describe the problem\r\nI'm having issue trying to build the graph transform tool with bazel although I've look at existing solutions to similar problem such as #13481. I have been able to build the graph transform tool in previous versions but not in this version, so I'm not too sure what went wrong. Note that previously I got a similar problem when I installed TF from source but it was related to CUDA and I solved it after reinstalling nvcc.\r\n\r\nI also rebooted my comp just in case it was a temporary system error, but the error still persists.\r\n\r\nLooking at the error, does it have anything to do with \"JEMALLOC\"? I enabled this option when configuring tensorflow as seen in the official installation guide.\r\n\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/kwotsin/tensorflow/tensorflow/core/kernels/BUILD:3945:1: C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    TF_CUDNN_VERSION=7.0.5 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.o' -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.o)\r\nIn file included from tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.cc:18:0:\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h: In instantiation of 'Index tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, T, Index, OP, IXDIM>::operator()(const CPUDevice&, Index, Eigen::array<long int, IXDIM>, typename tensorflow::TTypes<T, 2>::Tensor, typename tensorflow::TTypes<T, 2>::ConstTensor, typename tensorflow::TTypes<T, 2>::ConstTensor, typename tensorflow::TTypes<T, 2>::Tensor) [with T = float; Index = int; tensorflow::scatter_nd_op::UpdateOp OP = (tensorflow::scatter_nd_op::UpdateOp)0; int IXDIM = 5; tensorflow::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<T, 2>::Tensor = Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T, 2>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T, 2>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16, Eigen::MakePointer>]':\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:162:1:   required from here\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:137:3: internal compiler error: Segmentation fault\r\n   }\r\n   ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nINFO: Elapsed time: 147.229s, Critical Path: 24.74s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["You seem to have a version mismatch on your command line to do setup\r\n```\r\nsudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\n```\r\n\r\nbut you are building with cuda 9.0. Try removing the cuda 8 stuff if it is still there.", "@aselle Thanks for the reply. My system doesn't have CUDA 8.0 anymore since the previous install but I've mistakenly typed 8.0 instead. However, after changing the command to use 9.0 instead, the same problem still appears, and is related to same op once again.\r\n\r\n```\r\nERROR: /home/kwotsin/tensorflow/tensorflow/core/kernels/BUILD:3945:1: C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    TF_CUDNN_VERSION=7.0.5 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op.o' -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/kernels/scatter_nd_op.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op.o)\r\ntensorflow/core/kernels/scatter_nd_op.cc: In constructor 'Eigen::Symbolic::SymbolExpr<tag>::SymbolExpr() [with tag = Eigen::placeholders::internal::symbolic_last_tag]':\r\ntensorflow/core/kernels/scatter_nd_op.cc:556:1: internal compiler error: Segmentation fault\r\n }  // namespace tensorflow\r\n ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nINFO: Elapsed time: 160.245s, Critical Path: 24.60s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "/CC @gunan, any ideas what the issue could be?\r\n\r\nAlso, I doubt this makes a different, but the command \r\n```\r\nsudo sh -c \"echo '/usr/local/cuda-9.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\n```\r\nappends the line to the file, keeping the old lines. Try replacing >> with >.", "The error strictly looks like a compiler bug, so I do not have any ideas.\r\n```\r\n./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:137:3: internal compiler error: Segmentation fault\r\n   }\r\n   ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\n```\r\nI recommend looking into existing GCC issues.", "Since we cannot reproduce, marking as contributions welcome.", "@kwotsin We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/16205\">No</a>\n"]}, {"number": 16204, "title": "Fixing a typo for the argument to docker push.", "body": "", "comments": []}, {"number": 16203, "title": "Feature Request: Add CheckpointSaverListener to tf.contrib.learn.Experiment", "body": "Hello, \r\n\r\nI'm using the `tf.contrib.learn.Experiment` system to manage experiments built using the `tf.Estimator` framework. This setup allows me to specify checkpoint saving frequencies very easily, using just the `min_eval_frequency` argument to `tf.contrib.learn.Experiment`. However, I would like to be able to add a `tf.train.CheckpointSaverListener` (for example, to upload files to AWS after each checkpoint). Can there be a param added to `tf.contrib.learn.Experiment` to pass an optional `CheckpointSaverListener` object, which is then passed to the underlying `CheckpointSaverHook` object?\r\n\r\nThanks!\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Amazon Deep Learning AMI\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8\r\n- **GPU model and memory**: NVIDIA K80\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@ispirmustafa, could you consider this feature request. Perhaps if it sounds good, it would make a great contributions welcome suggestion?", "Agreed. Added to contributions.", "Ok, I think I have a pretty simple PR that addresses this, and also adds early stopping params for the ValidationMonitor. It is here: https://github.com/tensorflow/tensorflow/pull/16391", "Also, it looks like my feature request was already partly implemented in the master branch. However, the `train_and_evaluate` function was not using the `saving_listeners` parameter that was added to the constructor, so that's the gist of my addition there.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@xiejw this is about adding saver listeners to TrainSpec", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing this since Experiment is deprecated."]}, {"number": 16202, "title": "regarding the deconvolution operation for conv1d", "body": "There are `conv2d_transpose` for` conv2d` and `conv3d_transpose `for `conv3d` respectively.  How about deconvolution operation for `tf.nn.conv1d`?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}]