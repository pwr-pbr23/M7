[{"number": 23243, "title": "Using TensorRT in TensorFlow from java", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.11\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nUsing TensorRT in TensorFlow from java for inference only\r\n\r\n**Will this change the current api? How?** \r\nnot sure if the current java api needs to change in order to load graph and infer\r\n\r\n**Who will benefit with this feature?**\r\nanyone using java runtime\r\n\r\n**Any Other info.**\r\n", "comments": ["Nagging Assignee @samikama: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@samikama any update, when this feature will be available?", "@asimshankar @samikama \r\n\r\nI am following the tf-trt example  here\r\nhttps://medium.com/tensorflow/speed-up-tensorflow-inference-on-gpus-with-tensorrt-13b49f3db3fa\r\n\u00a0\r\nThe example start with\u00a0 resnetV150_frozen.pb \u00a0and generates a set of TF-TRT optimized \u00a0inference graph\r\n\u00a0\r\n1. resnetV150_TRTFP16.pb\r\n2. resnetV150_TRTFP32.pb\r\n3. resnetV150_TRTINT8.pb\r\n\u00a0\r\n\u00a0\r\nIdea is to infer using the tf-trt optimized graph using the java API, I am running on\r\n\u00a0\r\nUbuntu linux 16.04\r\nTensorflow \u00a01.12.0\r\nCuda 9.0\r\nCudnn 7.4.1\r\nTensorRT 4.1.2\r\n\u00a0\r\nwith the java API for inference using resnetV150_frozen.pb \u00a0works fine , however when I try to use\u00a0 TF-TRT optimized \u201cresnetV150_TRTFP16.pb\u201d or resnetV150_TRTFP32.pb it runs into the following, \r\n\u00a0\r\n\u00a0\r\n2018-11-22 23:08:44.747554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11297 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\nException in thread \"main\" org.tensorflow.TensorFlowException: **Op type not registered 'TRTEngineOp'** in binary running on sujit-desktop. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) **`tf.contrib.resampler`** should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.tensorflow.Graph.importGraphDef(Native Method)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.tensorflow.Graph.importGraphDef(Graph.java:130)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.tensorflow.Graph.importGraphDef(Graph.java:114)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.nvidia.tf.InspectModel2$.main(InspectModel2.scala:64)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.nvidia.tf.InspectModel2.main(InspectModel2.scala)\r\n\u00a0\r\n\u00a0\r\nplease let me how to resolve the issue?\r\n\u00a0\r\nDoes https://mvnrepository.com/artifact/org.tensorflow/libtensorflow_jni_gpu/1.12.0 includes tf-trt related ops\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensorrt \u00a0?\r\n\u00a0\r\n\u00a0\r\n\u00a0\r\n", "@sujitbiswas As message tells you, you need to load op libs first to make TRTEngineOp available to the system. This happens automatically in python case when the module is imported. For other languages, it needs to be done manually.\r\n", "@sujitbiswas : To elaborate on what @samikama said, `contrib` libraries aren't packaged with the TensorFlow Java distribution and currently need to be explicitly loaded. See [this StackOverflow answer](https://stackoverflow.com/a/47277367/6708503)\r\n\r\nHere is what I did on my installation:\r\n\r\n- Get the path to the shared library defining the operation in contrib\r\n```\r\npython -c \"import tensorflow.contrib.tensorrt as trt; print(trt.__path__)\"\r\n\r\n# Which printed /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt\r\n# Specifically, the library is: \r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/ops/_trt_engine_op.so\r\n```\r\n\r\n- Load that library in Java using something like this:\r\n\r\n```java\r\nimport java.nio.file.Files;\r\nimport java.nio.file.Paths;\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.TensorFlow;\r\n\r\npublic class HelloTF {\r\n  public static void main(String[] args) throws Exception {\r\n    // You could copy the library and package it with your Java application instead.\r\n    TensorFlow.loadLibrary(\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensorrt/python/ops/_trt_engine_op.so\");\r\n    try (Graph graph = new Graph();\r\n        Session session = new Session(graph)) {\r\n      graph.importGraphDef(Files.readAllBytes(Paths.get(\"resnetV150_TRTFP16.pb\")));\r\n      // Use session to execute the graph\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nHope that helps.\r\nFeel free to reopen this issue if the question isn't answered or if the above didn't work.", "Thanks for the help @samikama @asimshankar \r\n\r\nnote the error changes when we load\r\n\r\nTensorFlow.loadLibrary(\"/home/sujitb/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/python/ops/**_trt_engine_op.so\"**)\r\n \r\n \r\n \r\nException in thread \"main\" java.lang.IllegalArgumentException: **No shape inference function exists for op 'TRTEngineOp',** did you forget to define it?\r\n                at org.tensorflow.Graph.importGraphDef(Native Method)\r\n                at org.tensorflow.Graph.importGraphDef(Graph.java:130)\r\n                at org.tensorflow.Graph.importGraphDef(Graph.java:114)\r\n                at com.nvidia.tf.InspectModel2$.main(InspectModel2.scala:31)\r\n                at com.nvidia.tf.InspectModel2.main(InspectModel2.scala)\r\n \r\n \r\nI see a similar issue open, someone tried the same with C++\r\nhttps://github.com/tensorflow/tensorflow/issues/23853\r\n \r\nplease let me know how to proceed?", "Ugh, that's a shame.\r\nI'll look into it and follow up on #23853 \r\nSorry for the trouble.", "> Thanks for the help @samikama @asimshankar\r\n> \r\n> note the error changes when we load\r\n> \r\n> TensorFlow.loadLibrary(\"/home/sujitb/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/tensorrt/python/ops/**_trt_engine_op.so\"**)\r\n> \r\n> Exception in thread \"main\" java.lang.IllegalArgumentException: **No shape inference function exists for op 'TRTEngineOp',** did you forget to define it?\r\n> at org.tensorflow.Graph.importGraphDef(Native Method)\r\n> at org.tensorflow.Graph.importGraphDef(Graph.java:130)\r\n> at org.tensorflow.Graph.importGraphDef(Graph.java:114)\r\n> at com.nvidia.tf.InspectModel2$.main(InspectModel2.scala:31)\r\n> at com.nvidia.tf.InspectModel2.main(InspectModel2.scala)\r\n> \r\n> I see a similar issue open, someone tried the same with C++\r\n> #23853\r\n> \r\n> please let me know how to proceed?\r\n\r\n@sujitbiswas did you find a solution to this? I am stuck with the same issue."]}, {"number": 23242, "title": "[Features]mixed precision support enhancement using decorator", "body": "This feature realize the automatic/constant loss scaling algorithm in Backprop, which can help preserve small gradient magnitudes ( ref to  https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html)\r\nMixed precision with loss scaling is already support by tf.contrib.mixed_precision.LossScaleOptimizer,  with the following usage:\r\n```python\r\nloss = loss_fn()\r\nopt = tf.AdamOptimizer(learning_rate=...)\r\n\r\n# Choose a loss scale manager which decides how to pick the right loss scale\r\n# throughout the training process.\r\nloss_scale_manger = tf.contrib.mixed_precision.FixedLossScaleManager(5000)\r\n\r\n# Wraps the original optimizer in a LossScaleOptimizer.\r\nloss_scale_optimizer = LossScaleOptimizer(opt, loss_scale_manager)\r\n\r\n# Call minimize() on the loss scale optimizer.\r\ntrain_op = loss_scale_optimizer.minimize(loss)\r\n```\r\n\r\nThe optimizer `LossScaleOptimizer` is actually a wrapper of other optimizers.  However, the usage using wrapper is too limited to support all cases.  For example,\r\n    + if the users  are using `tf.gradients()` to compute the gradients rather than `optimizer.compute_gradients()`/`optimizer.minimize()`,  the wrapper is no longer in user. \r\n    + the `LossScaleOptimizer` wrapper may have compatibility problem with other optimizer wrappers, e.g. `SyncReplicasOptimizer`\r\n    + the current implementation in `LossScaleOptimizer` did not take the colocation parameter into consideration. \r\n\r\nInstead, we implement loss scaling using a decorator on `gradients()`, which is the atom function to compute the backprop gradients. The usage is as following:\r\n```python\r\nloss = loss_fn()\r\nopt = tf.AdamOptimizer(learning_rate=...)\r\n# constant loss scaling with loss_scale=64\r\nwith tf.mixed_precision_scope(automatic_loss_scaling=False, loss_scale=64):\r\n      grad = tf.gradients()  # if using tf.gradients\r\n      var_and_grad =opt.minimize(loss) # if using optimizer\r\n\r\n# automatic loss scaling\r\nwith tf.mixed_precision_scope(automatic_loss_scaling=True):\r\n      grad = tf.gradients()  # if using tf.gradients\r\n      var_and_grad =opt.minimize(loss) # if using optimizer\r\n```\r\n\r\nThe implementation can handle all of the above problems encounter by `LossScaleOptimizer` .", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 23241, "title": "Some problems in compile tensorflow in c++ (debug mode).", "body": "Using tensorflow 1.8 and vs2015, evertime I compile the src file in debug mode, there is a problem like this:\r\n![image](https://user-images.githubusercontent.com/33054583/47475426-d7d5e200-d84d-11e8-934b-8e493eb52ae3.png)\r\nEven if I use a 32G computer. Any suggestions?", "comments": ["Please try to use the latest version of TensorFlow and build again.\r\nIn addition, It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23240, "title": "a question about ,node naming rules of model network", "body": "\r\n```\r\nfrom keras import backend as K\r\nIn [40]: input = K.placeholder(ndim=3,name='x')\r\n\r\nIn [41]: input\r\nOut[41]: <tf.Tensor 'x:0' shape=(?, ?, ?) dtype=float32>\r\n\r\nIn [42]: input2 = K.placeholder(ndim=3,name='x')\r\n\r\nIn [43]: input2\r\nOut[43]: <tf.Tensor 'x_1:0' shape=(?, ?, ?) dtype=float32>\r\n```\r\nthe name 'x:0' and 'x_1:0', the ':0' what mean, how to change it?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23239, "title": "pull", "body": "", "comments": []}, {"number": 23237, "title": "1.12.0-rc2 cherry-pick request: Fix string comparison in `configured`", "body": "PiperOrigin-RevId: 218607372", "comments": []}, {"number": 23236, "title": "Unnecessary gpu allocations when reusing resource variables that are initialized from checkpoint", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n**binary**\r\n- TensorFlow version (use command below):\r\n**b'v1.11.0-rc1-0-ge4c4b20' 1.11.0-rc1**\r\n\r\n- Python version:\r\n**3.6.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n**9.2**\r\n- GPU model and memory:\r\n**Tesla K80**\r\n\r\n**Describe the current behavior**\r\nGPU memory is very high when using init_from_checkpoint and reuse=True with resource variables, in certain cases.\r\n\r\n**Describe the expected behavior**\r\nGPU memory should be at most parameters+activations\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\n\"\"\"\r\n# first:\r\npython repro_bug.py --write_ckpt\r\n\r\n# then, compare:\r\npython repro_bug.py # good\r\npython repro_bug.py --use_ckpt # bad\r\n\"\"\"\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import HParams\r\nimport fire\r\n\r\ndef model(*, X, hparams):\r\n    with tf.variable_scope('model'):\r\n        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\r\n                             initializer=tf.random_normal_initializer(stddev=0.02))\r\n        h = tf.gather(wte, X)\r\n        return tf.matmul(h, wte, transpose_b=True)\r\n\r\ndef main(\r\n        batch_size=20, \r\n        length=100, # increase this to leak more memory!\r\n        write_ckpt=False, use_ckpt=False, ckpt_path='/tmp/reprobug.ckpt',\r\n        # hparams\r\n        n_vocab=400, n_embd=300,\r\n):\r\n    hparams = HParams(n_vocab=n_vocab, n_embd=n_embd)\r\n\r\n    if write_ckpt:\r\n        with tf.Graph().as_default() as g:\r\n            X = tf.placeholder(shape=[batch_size], dtype=tf.int32)\r\n            results = model(X=X, hparams=hparams)\r\n            saver = tf.train.Saver()\r\n            with tf.Session() as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                saver.save(sess, ckpt_path)\r\n        return\r\n\r\n    X = tf.fill([batch_size], 0) # initial value\r\n    for _ in range(length):\r\n        # NOTE: doing something like reuse=(i > 0) doesn't help\r\n        with tf.variable_scope('step', reuse=tf.AUTO_REUSE, use_resource=True):\r\n            logits_flat = model(X=X, hparams=hparams)\r\n        X = tf.squeeze(tf.multinomial(logits_flat, num_samples=1, output_dtype=tf.int32), axis=[1])\r\n\r\n    if use_ckpt:\r\n        print('setting initializers')\r\n        tf.train.init_from_checkpoint(ckpt_path, {'/': 'step/'})\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.visible_device_list = '0'\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        run_metadata = tf.RunMetadata()\r\n        sess.run(X, feed_dict={}, options=tf.RunOptions(trace_level=tf.RunOptions.HARDWARE_TRACE), run_metadata=run_metadata)\r\n        mbs_in_use = []\r\n        for dev_stat in run_metadata.step_stats.dev_stats[:]:\r\n            for node_stat in dev_stat.node_stats[:]:\r\n                for mem in node_stat.memory:\r\n                    if mem.allocator_bytes_in_use and mem.allocator_name.startswith(\"GPU\"):\r\n                        mbs_in_use.append(mem.allocator_bytes_in_use // 1000000)\r\n        print(\"mbs in use: \" , \" -> \".join(map(str, mbs_in_use)))\r\n\r\nif __name__ == '__main__':\r\n    fire.Fire(main)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nOutput looks like this:\r\n```\r\nsetting initializers\r\nmbs in use:  0 -> 1 -> 1 -> 8 -> 10 -> 18 -> 27 -> 27 -> 35 -> [etc...]\r\n```\r\nwithout calling initialize_from_checkpoint, it looks like:\r\n```\r\nmbs in use:  0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> [etc...]\r\n```\r\n", "comments": ["@girving ", "Also, I should mention that I was unable to reproduce the same error with a simple matrix multiply instead of this gather/multinomial business", "Also reproducible with nightly, as of today, (version 1.13.0-dev20181025)", "I'm a little confused; by reading your code it seems like the mbs in use grows not with time but with the number of ops being executed? As in, you're always running a single step, but in the case where you restore from checkpoint the memory grows as ops execute while in the case where you don't they do not, right?\r\n\r\nMoreover, this growth happens not on the call to run global_variables_initializer but on the call to run the forward pass of the model.\r\n\r\nThis is very weird since you're not writing to any of the variables and running the forward pass shouldn't trigger more allocations. Can you print the partition graphs on the run metadata with and without checkpoint initialization so I can see what's different?", "/sub", "@alextp  Yes, your reading of the code is correct; it is quite weird!  `print(run_metadata.partition_graphs)` gives empty list in both cases.", "Sorry, I should have been clearer; to output the partition graphs you need RunOptions(..., output_partition_graphs=True) in there as well.", "`good` = full output without checkpoints\r\n`bad` = full output with checkpoints\r\n`diff` = output of `diff good bad`\r\n`2columncoloreddiff` = colored/2 column version\r\n\r\n[partition_graphs.zip](https://github.com/tensorflow/tensorflow/files/2550690/partition_graphs.zip)\r\n\r\nEDIT: re-did with --length=2", "Ah, I see what is going on. It seems like the bad version is moving a lot of computation to the CPU because of int32 dtypes and colocation constraints. These copies are triggering more memory allocation, which is what you're seeing.\r\n\r\nFor example, look at the \"step/model/gather\" node, which runs on the CPU in the bad version and on the GPU on the good version, mostly because the variable is being placed on the CPU in the bad version.\r\n\r\nEssentially, I think a fix is to stop force-placing the init op in the CPU (that is, deindenting the block which starts in https://github.com/tensorflow/tensorflow/blob/5fb8c844512f50ec58fe3b856c8718816d7a504a/tensorflow/python/training/checkpoint_utils.py#L321 ).\r\n\r\nCan you do this and see if the issue goes away?\r\n\r\n@jpienaar for FYI since this smells a lot like a colocation-group bug (which Jacques is working on removing from TF entirely) but is only indirectly so.", "deindenting that line (and the 3 after) fixes it!  memory usage is fine and the partition_graphs are the same (modulo names)"]}, {"number": 23235, "title": "1.12.0-rc2 cherry-pick request: Various XLA scatter improvements.", "body": "There are various piper origin CLs cherrypicked into this PR:\r\n\r\nPiperOrigin-RevId: 215687800\r\nPiperOrigin-RevId: 216412467 \r\nPiperOrigin-RevId: 216437329\r\nPiperOrigin-RevId: 216448063\r\nPiperOrigin-RevId: 216624225\r\nPiperOrigin-RevId: 216798034\r\nPiperOrigin-RevId: 216921512\r\nPiperOrigin-RevId: 216968475", "comments": ["I'm ignoring the clang-format check, since it's suggesting weird formatting changes, and it's not critical anyways."]}, {"number": 23234, "title": "TFLiteConverter produces empty .tflite file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Adapted code from [here](https://www.tensorflow.org/lite/convert/python_api#exporting_a_graphdef_from_file_)\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-gpu\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GeForce GTX 980 8GB\r\n\r\n**Describe the current behavior**\r\nI am trying to convert a frozen .pb (which I have working with TF Mobile java inference interface) to .tflite format using the python TFLiteConverter API. When I run the code below, it completes without error or warning, except it produces an output file SEGNET.tflite with 0 size.\r\n\r\n**Describe the expected behavior**\r\nI would like it you produce a non-zero file...\r\n\r\n**Code to reproduce the issue**\r\nI cannot provide the .pb file, but here is the code im using\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"models/SEGNET.pb\"\r\ninput_arrays = [\"input_1\"]\r\noutput_arrays = [\"output_node0\"]\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"SEGNET.tflite\", \"wb\").write(tflite_model)\r\n```\r\nI know that both the input and output are correctly named.", "comments": ["I have also tried using the command line \"tflite_convert\" interface and get the same behaviour. \r\nWhen I include the \"--allow_custom_ops\" flag, it does produce a non-empty .tflite, however it does not tell me which ops I should be customizing, and when I try to load this file in my Android app I get the following error:\r\n```\r\njava.lang.IllegalArgumentException: Contents of /file:/android_asset/SEGNET.tflite does not encode a valid TensorFlowLite model: Could not open '/file:/android_asset/SEGNET.tflite'.The model is not a valid Flatbuffer file\r\n  at org.tensorflow.lite.NativeInterpreterWrapper.createModel(Native Method)\r\n  at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:41)\r\n  at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:135)\r\n  at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:112)\r\n  at org.qus.segmentLite.TensorFlowQUSRunnerSegmentLite.create(TensorFlowQUSRunnerSegmentLite.java:117)\r\n```", "If you can provide us your model that will be great and if not can you a provide us with a list of ops that you have used in your model?", "Hi @Mr-Grieves \r\n\r\nA few additional questions:\r\n\r\n* Is there any warning or error message when calling `converter.convert()`?\r\n* If you try `print(len(tflite_model))`, what's the length of the binary bytes?\r\n* If the answer for the last question isn't 0, it may be the case that you didn't close the file so the content isn't flush out. Could you try to write the model like this?\r\n\r\n```\r\nwith open(\"SEGNET.tflite\", \"wb\") as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\nThanks!", "Thank you for your quick replies\r\n\r\n@ymodak, I cannot provide the .pb file, but here are all the unique operation types within it:\r\n\r\n```\r\nPlaceholder\r\nConst\r\nIdentity\r\nConv2D\r\nBiasAdd\r\nRelu\r\nMaxPool\r\nShape\r\nStridedSlice\r\nMul\r\nResizeNearestNeighbor\r\nConcatV2\r\nSigmoid\r\n```\r\n\r\n@miaout17:\r\n- there are no warnings or errors.\r\n- `print(len(tflite_model))` prints 0\r\n- Writing the model in the way you suggested still gives a 0 filesize\r\n", "@Mr-Grieves Thanks! I will send a commit to fix the error reporting soon. \r\nThen you can see the error message and we can figure out the next step. ", "85b85a46c625d0b47cddfa8c7b399f32fe5a3ccc fixes the error reporting. \r\nNow it should report an error instead of producing an empty file silently. \r\n\r\nYou should be able to see the proper error message now. \r\nI'm closing this bug. Feel free to create a separated issue if you need further help converting your model. Thanks!"]}, {"number": 23233, "title": "Surface the number of threads used by ctx->runner() for ParallelMapIteratorBase", "body": "This PR surfaces the number of threads used by `ctx->runner()` for [ParallelMapIteratorBase::AddTunableParameter()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_iterator.cc#L63-L66). ", "comments": ["@jsimsa Could you help review this PR when you have time?", "Hi @feihugis thank you for the initiative. Unfortunately, this is not the right way to address the TODO.\r\n\r\nThe problem with your solution is that it assumes that the threadpool use for executing parallel map is always the interop threadpool, which is not the case. The input pipeline can be configured with a custom threadpool (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/threadpool.py and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc).\r\n\r\nI think that the proper way to address the TODO is to introduce a new member to the `IteratorContext` called `runner_threadpool_size`, which is initialized with `ctx->device()->tensorflow_cpu_worker_threads()` when created and this value is overridden when appropriate (e.g. when `override_threadpool`).\r\n\r\nEDIT: Talked to @mrry offline and updated my original response accordingly (removing connection to `use_inter_op_parallelism` as that was misguided).", "@jsimsa and @mrry Thanks for your comments and suggestions! Yes, the current implementation did not consider some cases. Will change the code as your suggestion.", "@jsimsa `runner_threadpool_size` has been added to `IteratorContext`. Could you have a look at these changes?", "@jsimsa Thanks for your review. The comment has been revised. The failures in the last test seem to be unrelated to this PR.", "@jsimsa The code has been revised to solve the conflicts caused by the recent commits. Could you review these changes at [here](https://github.com/tensorflow/tensorflow/pull/23233/files#diff-696c1a7a179fff088f071807abd4f81aR67) and [here](https://github.com/tensorflow/tensorflow/pull/23233/files#diff-11a06473028adea81ee960e8b522a0fcR507)? ", "@feihugis your PR looks good but it prompted me to perform an internal refactoring ... please sync your branch and resolve conflicts ... thanks!", "@jsimsa The refactoring makes the code clean! The conflicts have been resolved. Could you review the change?", "@jsimsa Thanks for your comments! The code has been revised. Could you have another review?", "@jsimsa Add the change to [map_and_batch_dataset_op.cc](https://github.com/tensorflow/tensorflow/pull/23233/commits/634005cb53d2a136c04885ffa6cbf5323649fe32). Also, found another one in [numa_map_and_batch_dataset_op.cc](https://github.com/tensorflow/tensorflow/pull/23233/commits/0ff931a5965241ffdb1551493617b6d9592976e8).\r\n\r\nBesides, change the maximum input for [model::MakeParameter::parallelism](https://github.com/tensorflow/tensorflow/pull/23233/commits/689d9974b95a764759be2028cff36735fde3001f). Are these changes right here?"]}, {"number": 23232, "title": "Broken distributed training with tf.estimator.Estimator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): 0.18.0 (PPA)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.3.1\r\n- GPU model and memory: 2x1080ti + 2x1070\r\n\r\nWhen I am trying to train network with Estimator and MirroredStrategy there is naming issue in estimator's code.\r\nI get following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/snap/pycharm-community/85/helpers/pydev/pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"/snap/pycharm-community/85/helpers/pydev/pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/snap/pycharm-community/85/helpers/pydev/pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/snap/pycharm-community/85/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/alexandr/Code/DRIT_Tensorflow/train_est.py\", line 102, in <module>\r\n    tf.app.run(main)\r\n  File \"/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/alexandr/Code/DRIT_Tensorflow/train_est.py\", line 79, in main\r\n    estimator.train(train_input_fn)\r\n  File \"/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1179, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1309, in _train_model_distributed\r\n    grouped_estimator_spec.training_hooks)\r\n  File \"/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1305, in get_hooks_from_the_first_device\r\n    for per_device_hook in per_device_hooks\r\n  File \"/home/alexandr/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1305, in <listcomp>\r\n    for per_device_hook in per_device_hooks\r\nAttributeError: 'Estimator' object has no attribute '_distribution'\r\n```\r\n\r\nProblem occurs here:\r\n```\r\n        def get_hooks_from_the_first_device(per_device_hooks):\r\n          return [\r\n              self._distribution.unwrap(per_device_hook)[0]\r\n              for per_device_hook in per_device_hooks\r\n          ]\r\n```\r\nwhen estimator tries to call `unwrap()` method of `_distribution` but distribution strategy is stored in `_train_distribution` field.\r\n\r\nThis is from PyCharm's debugger:\r\n```\r\nper_device_hooks = {tuple} <class 'tuple'>: (PerDevice({'/replica:0/task:0/device:GPU:0': <model.model_fn.<locals>.TrainOpSelectorHook object at 0x7f0189a444e0>, '/replica:0/task:0/device:GPU:1': <model.model_fn.<locals>.TrainOpSelectorHook object at 0x7f01c3331d68>, '/replica:0/tas\r\nself = {Estimator} <tensorflow.python.estimator.estimator.Estimator object at 0x7f0c3f44f4a8>\r\n _config = {RunConfig} <tensorflow.python.estimator.run_config.RunConfig object at 0x7f0c3f44f4e0>\r\n _device_fn = {NoneType} None\r\n _estimator_api_names = {tuple} <class 'tuple'>: ('estimator.Estimator',)\r\n _estimator_api_names_v1 = {tuple} <class 'tuple'>: ('estimator.Estimator',)\r\n _eval_distribution = {NoneType} None\r\n _model_dir = {str} 'train_logs/37/checkpoints'\r\n _params = {dict} {'batch_size': 1, 'image_size': [216, 216], 'float_type': tf.float32, 'gradient_scale': 1.0, 'leaky_relu_alpha': 0.2, 'attribute_tensor_depth': 32, 'attribute_subsample_factor': 4, 'attribute_tensor_global_pooling': False, 'remap_attribute_with_content': True, 'weight_decay': 1e-05, 'learning_rate': 1e-05, 'content_discriminator_lr_multiplier': 0.4, 'attribute_discriminator_lr_multiplier': 0.4, 'quantize': False, 'content_discriminator_steps': 1, 'domain_discriminator_steps': 1, 'num_steps_start_lr_decay': 100000, 'max_iter': 250000}\r\n _session_config = {ConfigProto} \r\nLOOK HERE ==> _train_distribution = {MirroredStrategy} <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f0c3f44f5c0>\r\n _warm_start_settings = {NoneType} None\r\n config = {RunConfig} <tensorflow.python.estimator.run_config.RunConfig object at 0x7f017d7b0f98>\r\n model_dir = {str} 'train_logs/37/checkpoints'\r\n params = {dict} {'batch_size': 1, 'image_size': [216, 216], 'float_type': tf.float32, 'gradient_scale': 1.0, 'leaky_relu_alpha': 0.2, 'attribute_tensor_depth': 32, 'attribute_subsample_factor': 4, 'attribute_tensor_global_pooling': False, 'remap_attribute_with_content': True, 'weight_decay': 1e-05, 'learning_rate': 1e-05, 'content_discriminator_lr_multiplier': 0.4, 'attribute_discriminator_lr_multiplier': 0.4, 'quantize': False, 'content_discriminator_steps': 1, 'domain_discriminator_steps': 1, 'num_steps_start_lr_decay': 100000, 'max_iter': 250000}\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nWhat I expect? I expect tf.estimator's code to not be broken.\r\n\r\n**Code to reproduce the issue**\r\nAny example with MirroredStrategy should be fine.\r\n\r\n", "comments": ["thanks for filing, https://github.com/tensorflow/estimator/pull/3#pullrequestreview-168946557 is going to fix this. I am working on getting that merged. ", "As soon as fix is merged to master I am closing this issue"]}, {"number": 23231, "title": "TFProf is unable to parse the profile generated by profileContext.", "body": "TFProf is unable to parse the profile generated by profileContext.\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution : Linux Ubuntu 18.04: \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: V100, 16GB\r\n\r\n**Describe the current behavior**\r\nTFProf unable to parse the profile generated by profileContext. \r\n\r\n**Describe the expected behavior**\r\nWhen I generate the profile file by using the low level approach of runOptions, runMetadata, and then writing the string generated by profiler.serialize_to_string(), I can load it with tfprof and analyze it just fine. This should happen even when using ProfileContext.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nwith tf.contrib.tfprof.ProfileContext('./profiles', trace_steps=range(50,100), dump_steps=[100]) as pctx:\r\n\r\n ... Regular session.run\r\n\r\n```\r\n", "comments": ["I tried this twice, one where I killed the program after the trace was dumped. Other where I made the training loop exit cleanly. \r\n```\r\n/home/ubuntu/tensorflow/bazel-bin/tensorflow/core/profiler/profiler\r\n--profile_path=profiles/profile_100\r\nReading Files...\r\nFailed to parse profile\r\ntfprof>\r\nCannot start interative shell, use 'bazel-bin' instead of 'bazel run'.\r\n(tf_p36_cu10) ubuntu@ip-172-31-43-37:~/OpenSeq2Seq$\r\n```\r\n```\r\n(tf_p36_cu10) ubuntu@ip-172-31-43-37:~/OpenSeq2Seq$ tfprof --profile_path=profiles/profile_70\r\n/home/ubuntu/tensorflow/bazel-bin/tensorflow/core/profiler/profiler\r\n--profile_path=profiles/profile_70\r\nReading Files...\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/wire_format_lite.cc:626] String field 'tensorflow.tfprof.ProfileProto.IdToStringEntry.value' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.\r\nFailed to parse profile\r\ntfprof> op -select micros,bytes,occurrence -order_by micro\r\nCannot start interative shell, use 'bazel-bin' instead of 'bazel run'.\r\n(tf_p36_cu10) ubuntu@ip-172-31-43-37:~/OpenSeq2Seq$ tfprof --profile_path=profiles/profile_100\r\n```", "Wish I could help, maybe another TensorFlow contributor is more familiar with the C++ behind TFProf?", "Wish I could help but I do not know much about the C++ interface. ", "@shlens @ChrisAntaki   -  Could you please suggest a right person to assign this issue ?", "Hi @rahul003 !We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23230, "title": "1.12.0-rc2 cherry-pick request: Update tensorboard dependency to 1.12.x", "body": "Also updated tb-nightly to +1 minor version, 1.13.x.\r\n\r\nPiperOrigin-RevId: 218582588", "comments": []}, {"number": 23229, "title": "MirroredStrategy error with Object detection retraining", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):b'v1.11.0-0-gc19e293' 1.11.0\r\n- Python version:3.6.5\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version:9.1/7.1.2\r\n- GPU model and memory: AWS g2.8large Grid K520 4036MiB\r\n\r\n**Describe the current behavior**\r\nTraining stops with the attached error log when retraining Mobilenet V2 SSD model.\r\n**Describe the expected behavior**\r\nTrainig should start and use the available 4 GPUs\r\n**Code to reproduce the issue**\r\nFollowing is the code change I made to https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py\r\n```\r\ndef main(unused_argv):\r\n  flags.mark_flag_as_required('model_dir')\r\n  flags.mark_flag_as_required('pipeline_config_path')\r\n\r\n  distribution = tf.contrib.distribute.MirroredStrategy()\r\n  config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,train_distribute=distribution)\r\n```\r\n\r\n**Other info / logs**\r\nError Log: \r\n[TF_MirroredStrategy_error.txt](https://github.com/tensorflow/tensorflow/files/2512545/TF_MirroredStrategy_error.txt)\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation. Thanks!\r\n\r\n", "@ymodak I will post the issue on StackOverflow as well. Can you please tell what other information is needed? I thought I provided all details asked by the issue template. Thanks.", "@anilmaddala Yes you did provide all the info. requested by the template. Actually I didn't update the snippet completely. Now I did.  Apologies."]}, {"number": 23228, "title": "Bug in problem.py input_fn", "body": "Using tensor2tensor 1.9 and tf 1.9.0, while overriding the problem.py input_fn:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/t2t-trainer\", line 33, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/bin/t2t-trainer\", line 28, in main\r\n    t2t_trainer.main(argv)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/bin/t2t_trainer.py\", line 355, in main\r\n    execute_schedule(exp)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/bin/t2t_trainer.py\", line 321, in execute_schedule\r\n    getattr(exp, FLAGS.schedule)()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py\", line 331, in continuous_train_a\r\nnd_eval\r\n    self._eval_spec)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 447, in train_and_eva\r\nluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 531, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 669, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1119, in _train_mode\r\nl\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1132, in _train_mode\r\nl_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1992, in _\r\ncall_model_fn\r\n    features, labels, mode, config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1107, in _call_model\r\n_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2212, in _\r\nmodel_fn\r\n    input_holders.generate_infeed_enqueue_ops_and_dequeue_fn())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1001, in g\r\nenerate_infeed_enqueue_ops_and_dequeue_fn\r\n    self._invoke_input_fn_and_record_structure())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1065, in _\r\ninvoke_input_fn_and_record_structure\r\n    host_device, host_id))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 692, in ge\r\nnerate_per_host_enqueue_ops_fn_for_host\r\n    inputs = _Inputs.from_input_fn(input_fn(user_context))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2163, in _\r\ninput_fn\r\n    return input_fn(**kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py\", line 735, in estimator_in\r\nput_fn\r\n    dataset_kwargs=dataset_kwargs)\r\n  File \"/home/michaelbesc/BESC_Source/route-prediction_MMedts/training_minimal_wCat_decOnly/trainer/routes.py\", lin\r\ne 579, in input_fn\r\n    batch_size, padded_shapes, drop_remainder=True)\r\nTypeError: padded_batch() got an unexpected keyword argument 'drop_remainder'\r\n\r\n\r\nThis is because in tf 1.9 the 'drop remainder' argument doesn't yet exist (becomes available tf 1.10.0)\r\nNot sure why this issue doesn't seem to occur when input_fn is not overridden. ", "comments": ["Can you please use the latest version of TensorFlow and check if the issue still persists for you?\r\nIf it does, Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n\r\n"]}, {"number": 23227, "title": "1.12.0-rc2 cherry-pick request: Declare that stateless random ops are not differentiable in C++ code.", "body": "PiperOrigin-RevId: 215935319", "comments": ["Thanks Todd!"]}, {"number": 23226, "title": "Fix documentation for deprecated args", "body": "While reading the API docs, I noticed that some of the deprecation notices seem to have missing pieces. For example, this notice in `tf.math.argmax`:\r\n<img width=\"712\" alt=\"screen shot 2018-10-22 at 10 16 08 am\" src=\"https://user-images.githubusercontent.com/12436991/47307300-8f1af080-d5e3-11e8-81c6-f04beb7b257f.png\">\r\n\r\nI traced the problem to a bug in the `@deprecated_args` decorator. The message that the decorator generates for docstrings is different from the message it generates for runtime warnings. Instructions that make sense in the context of the runtime warning message don't make sense in the context of the API docs.\r\n\r\nThis PR makes the deprecation message for docstrings more like the one for warnings. After the changes in this PR, the deprecation notice for `tf.math.argmax` looks like this:\r\n![screen shot 2018-10-22 at 5 02 30 pm](https://user-images.githubusercontent.com/12436991/47326191-6fec8500-d61c-11e8-9f8b-5f8037c8917e.png)\r\n\r\nOverall, this change fixes problems with the deprecation warnings for the following functions:\r\n* `tf.graph_util.import_graph_def`\r\n* `tf.expand_dims`\r\n* `tf.squeeze`\r\n* `tf.math.argmax`\r\n* `tf.math.argmin`\r\n\r\nI regenerated the docs on my local copy and checked each of these functions by hand. I also ran regression tests and pylint.\r\n\r\nI updated the expected results in `deprecation_test.py` and added a new test to cover docstrings for functions with 2 or more deprecated arguments.", "comments": ["Thanks!", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think the equivalent of this was already done internally. @frreiss can you check that it's equivalent? I'm sorry we didn't get this in before someone fixed it internally.", "Yes, it does appear that a roughly-equivalent set of changes were committed on November 5 from the internal Google branch. Closing out this PR."]}, {"number": 23225, "title": "Update cast_op_test for ARM behavior", "body": "-Casting \"np.inf\" to an integer on ARM results in a positive value, instead of a negative value, similar to Power\r\n-Casting \"np.nan\" to an integer on ARM results in 0 instead of the integer's minimum value", "comments": []}, {"number": 23224, "title": "Disable denormal_test on aarch64 platform", "body": "Flushing denormals is not yet implemented on the ARM architecture, similar to the other platforms discussed here: https://github.com/tensorflow/tensorflow/issues/11902\r\n", "comments": ["@MattConley thanks for the PR. pc64el is already disabled, aarch64 should be skipped. denormal test is controlled locally.", "Nagging Reviewer @petrosmol, @gunan: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@gunan  Can I bypass these Kokoro failures and proceed with \"ready to pull\""]}, {"number": 23223, "title": "TFTRT: Add inference examples for classification", "body": "This example can be used to check for accuracy and performance of TF-TRT on image classification models.\r\n\r\nFollow the README for more information.\r\n\r\nThanks to @trevor-m, @mdrozdowski1996, and @benbarsdell for the contribution.", "comments": ["Moving the PR https://github.com/tensorflow/tensorrt\r\nWill address some of the comments there."]}, {"number": 23222, "title": "Compiled source in Debian Stretch: Finds GPU Card then hangs", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Deban stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.11\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: pipenv\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version (if compiling from source): 6.3.0 20170516\r\n- CUDA/cuDNN version: 9.0, 7.0 from runfile\r\n- GPU model and memory: NV 1050 Ti 4GB\r\n\r\n**Describe the problem**\r\nCompilation for GPU runs flawlessly.\r\nRunning a simple TF script fails like:\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.test.gpu_device_name()\r\n2018-10-24 21:26:49.111741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-10-24 21:26:49.112489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.43\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.87GiB\r\n2018-10-24 21:26:49.112522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n\r\nIt does obviously find the GPU Card but then the process hangs and can only be killed. Nothing in the dmesg or system logs. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nmkdir tf_compile2\r\ncd tf_compile2/\r\npipenv install --python python3\r\npipenv shell\r\npipenv install --skip-lock pip six numpy wheel mock\r\npipenv install --skip-lock keras_applications==1.0.5\r\npipenv install --skip-lock keras_preprocessing==1.0.3\r\n./configure (every options default, only choices are CUDA and CUDA Level 3.5)\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package ..\r\n\r\nIn a fresh pipenv I then installed the wheel \r\npipenv install ../tf_compile/tensorflow-1.11.0-cp35-cp35m-linux_x86_64.whl\r\n\r\n**Any other info / logs**\r\nThe system is a Dual Intel XEON X5680 without AVX therefore the compilation from source.\r\n\r\nroot@cuda:/home/volker/workspace/tf_test# nvidia-smi \r\nWed Oct 24 21:47:49 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.66       Driver Version: 410.66       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  Off  | 00000000:07:00.0  On |                  N/A |\r\n|  0%   22C    P8    N/A /  72W |     28MiB /  4039MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       721      G   /usr/lib/xorg/Xorg                            25MiB |\r\n+-----------------------------------------------------------------------------+\r\nroot@cuda:/home/volker/workspace/tf_test# nvidia-smi -q\r\n\r\n==============NVSMI LOG==============\r\n\r\nTimestamp                           : Wed Oct 24 21:47:54 2018\r\nDriver Version                      : 410.66\r\nCUDA Version                        : 10.0\r\n\r\nAttached GPUs                       : 1\r\nGPU 00000000:07:00.0\r\n    Product Name                    : GeForce GTX 1050 Ti\r\n    Product Brand                   : GeForce\r\n    Display Mode                    : Enabled\r\n    Display Active                  : Enabled\r\n    Persistence Mode                : Disabled\r\n    Accounting Mode                 : Disabled\r\n    Accounting Mode Buffer Size     : 4000\r\n    Driver Model\r\n        Current                     : N/A\r\n        Pending                     : N/A\r\n    Serial Number                   : N/A\r\n    GPU UUID                        : GPU-54012771-5371-0894-d451-63b8bf4b46cd\r\n    Minor Number                    : 0\r\n    VBIOS Version                   : 86.07.42.00.99\r\n    MultiGPU Board                  : No\r\n    Board ID                        : 0x700\r\n    GPU Part Number                 : N/A\r\n    Inforom Version\r\n        Image Version               : G001.0000.01.04\r\n        OEM Object                  : 1.1\r\n        ECC Object                  : N/A\r\n        Power Management Object     : N/A\r\n    GPU Operation Mode\r\n        Current                     : N/A\r\n        Pending                     : N/A\r\n    GPU Virtualization Mode\r\n        Virtualization mode         : None\r\n    IBMNPU\r\n        Relaxed Ordering Mode       : N/A\r\n    PCI\r\n        Bus                         : 0x07\r\n        Device                      : 0x00\r\n        Domain                      : 0x0000\r\n        Device Id                   : 0x1C8210DE\r\n        Bus Id                      : 00000000:07:00.0\r\n        Sub System Id               : 0x37631458\r\n        GPU Link Info\r\n            PCIe Generation\r\n                Max                 : 2\r\n                Current             : 1\r\n            Link Width\r\n                Max                 : 16x\r\n                Current             : 16x\r\n        Bridge Chip\r\n            Type                    : N/A\r\n            Firmware                : N/A\r\n        Replays since reset         : 0\r\n        Tx Throughput               : 0 KB/s\r\n        Rx Throughput               : 0 KB/s\r\n    Fan Speed                       : 0 %\r\n    Performance State               : P8\r\n    Clocks Throttle Reasons\r\n        Idle                        : Active\r\n        Applications Clocks Setting : Not Active\r\n        SW Power Cap                : Not Active\r\n        HW Slowdown                 : Not Active\r\n            HW Thermal Slowdown     : Not Active\r\n            HW Power Brake Slowdown : Not Active\r\n        Sync Boost                  : Not Active\r\n        SW Thermal Slowdown         : Not Active\r\n        Display Clock Setting       : Not Active\r\n    FB Memory Usage\r\n        Total                       : 4039 MiB\r\n        Used                        : 28 MiB\r\n        Free                        : 4011 MiB\r\n    BAR1 Memory Usage\r\n        Total                       : 256 MiB\r\n        Used                        : 5 MiB\r\n        Free                        : 251 MiB\r\n    Compute Mode                    : Default\r\n    Utilization\r\n        Gpu                         : 0 %\r\n        Memory                      : 3 %\r\n        Encoder                     : 0 %\r\n        Decoder                     : 0 %\r\n    Encoder Stats\r\n        Active Sessions             : 0\r\n        Average FPS                 : 0\r\n        Average Latency             : 0\r\n    FBC Stats\r\n        Active Sessions             : 0\r\n        Average FPS                 : 0\r\n        Average Latency             : 0\r\n    Ecc Mode\r\n        Current                     : N/A\r\n        Pending                     : N/A\r\n    ECC Errors\r\n        Volatile\r\n            Single Bit            \r\n                Device Memory       : N/A\r\n                Register File       : N/A\r\n                L1 Cache            : N/A\r\n                L2 Cache            : N/A\r\n                Texture Memory      : N/A\r\n                Texture Shared      : N/A\r\n                CBU                 : N/A\r\n                Total               : N/A\r\n            Double Bit            \r\n                Device Memory       : N/A\r\n                Register File       : N/A\r\n                L1 Cache            : N/A\r\n                L2 Cache            : N/A\r\n                Texture Memory      : N/A\r\n                Texture Shared      : N/A\r\n                CBU                 : N/A\r\n                Total               : N/A\r\n        Aggregate\r\n            Single Bit            \r\n                Device Memory       : N/A\r\n                Register File       : N/A\r\n                L1 Cache            : N/A\r\n                L2 Cache            : N/A\r\n                Texture Memory      : N/A\r\n                Texture Shared      : N/A\r\n                CBU                 : N/A\r\n                Total               : N/A\r\n            Double Bit            \r\n                Device Memory       : N/A\r\n                Register File       : N/A\r\n                L1 Cache            : N/A\r\n                L2 Cache            : N/A\r\n                Texture Memory      : N/A\r\n                Texture Shared      : N/A\r\n                CBU                 : N/A\r\n                Total               : N/A\r\n    Retired Pages\r\n        Single Bit ECC              : N/A\r\n        Double Bit ECC              : N/A\r\n        Pending                     : N/A\r\n    Temperature\r\n        GPU Current Temp            : 22 C\r\n        GPU Shutdown Temp           : 102 C\r\n        GPU Slowdown Temp           : 99 C\r\n        GPU Max Operating Temp      : N/A\r\n        Memory Current Temp         : N/A\r\n        Memory Max Operating Temp   : N/A\r\n    Power Readings\r\n        Power Management            : Supported\r\n        Power Draw                  : N/A\r\n        Power Limit                 : 72.00 W\r\n        Default Power Limit         : 72.00 W\r\n        Enforced Power Limit        : 72.00 W\r\n        Min Power Limit             : 52.50 W\r\n        Max Power Limit             : 75.00 W\r\n    Clocks\r\n        Graphics                    : 139 MHz\r\n        SM                          : 139 MHz\r\n        Memory                      : 405 MHz\r\n        Video                       : 544 MHz\r\n    Applications Clocks\r\n        Graphics                    : N/A\r\n        Memory                      : N/A\r\n    Default Applications Clocks\r\n        Graphics                    : N/A\r\n        Memory                      : N/A\r\n    Max Clocks\r\n        Graphics                    : 1936 MHz\r\n        SM                          : 1936 MHz\r\n        Memory                      : 3504 MHz\r\n        Video                       : 1708 MHz\r\n    Max Customer Boost Clocks\r\n        Graphics                    : N/A\r\n    Clock Policy\r\n        Auto Boost                  : N/A\r\n        Auto Boost Default          : N/A\r\n    Processes\r\n        Process ID                  : 721\r\n            Type                    : G\r\n            Name                    : /usr/lib/xorg/Xorg\r\n            Used GPU Memory         : 25 MiB\r\n\r\n", "comments": ["I am closing this issue!\r\nSome Cuda dependencies were not installed properly.\r\nTo anyone running into the same situation.  Follow the NVIDIA documentation line by line.\r\n\r\nCheers\r\n\r\nVolker\r\n\r\n"]}, {"number": 23221, "title": "verbs: make sure the memory region to send from is registered", "body": "Fixes #23220 ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "I am not the expert for this. Re: Admin, please find a new reviewer. ", "Thanks for the PR. Did you test it on 1.9?\r\nI am asking because verbs is currently fails on build. \r\n\r\nWhen testing it on my own branch, it did seem to fix #23220, but I am worried about the registered memory not being properly de-registered.", "Yes I tested it on 1.9. Does the mr get deregistered in `RdmaTensorResponse::Destroy()`?", "It is normally dereged from MRDestroy, called on iterator erasure \r\n`void RdmaMemoryMgr::EvictMemoryRegion(void* addr, size_t length);`\r\n\r\nYou can use InsertMemoryRegion to register memory so it can also be used in the future (using FindMemoryRegion) and automatically evicted in the end.", "The cost of registration on every single transmission is higher than cost of actual transmission for small sized tensor (could be up to 32KB). \r\n\r\nI believe @yanivbl6 is working on a better fix.", "If you use InsertMemoryRegion, next attempts to access the same memory should work with FindMemoryRegion without extra registration, so performance will be fine in the long run as long it's not just random data access (unlikely in the case of graph-computation).\r\n\r\nI don't want people to wait for my fix as I am having an hard time finding the time to work on this.\r\nthat being said, I would like to have this tested on master due to the major changes pushed to the memory allocation mechanism since 1.9 (In the same commit that also broke the build). ", "I defer to @yanivbl6 on any changes to contrib/verbs.  When he's ready to accept, I will approve.  (I can't test any of this code.)", "@yanivbl6 Thanks for your explanation. Changed to use InsertMemoryRegion. Not sure what allocator_name should be set to. Seems it's only for display purposes.", "@yanivbl6 gentle ping", "Sorry, I added the comments but forgot to submit the review.", "OK with me.\r\nnoting again that verbs is still broken in master branch, so one would need to cherry-pick this patch from older version you get this fix.", "@yanivbl6 Do you know what's the last release version where verbs still built?\r\n\r\n@poxvoculi Can you approve? Thanks!", "The patch that broke the code was [33170cc)](https://github.com/tensorflow/tensorflow/commit/33170cc661f3838aa7d0d7fc19bb0c6ba4812a3c)\r\n\r\nThe last version I tried myself which worked was 1.9, but 1.10 might also work.", "I patched this PR in r1.12 branch with other commits cherry-picked from master branch, but \"local protection error\" still exists for slim based inception model.", "I believe the issue has been fixed in #24250 without this PR. @2sin18 @guoshimin could you confirm?", "I have cherry-pick your commit from master, but it still not works in r1.12. I will try this patch in r1.13 branch. Thanks.", "I add `force_gpu_compatible=True` to `GPUOptions`, and #24250 works without this PR.", "You could patch 7dea8383bbb97a1e78cdece876e083b22191974e as well.", "I can't test the new fix since we are still using 1.9.  I'll close my PR for now."]}, {"number": 23220, "title": "verbs: local protection error when doing rdma send", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.9.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 6.3\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: TITAN Xp. 12 GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen verbs support is enabled, getting\r\n\r\n```\r\nmlx5: 4dfe187391fa: got completion with error:\r\n00000000 00000000 00000000 00000000\r\n00000000 00000000 00000000 00000000\r\n00000000 00000000 00000000 00000000\r\n00000000 92005204 0900013b 05cc37d3\r\n2018-09-21 23:48:04.608288: F external/org_tensorflow/tensorflow/contrib/verbs/rdma.cc:451] Check failed: wc_[i].status == IBV_WC_SUCCESS Failed status \r\nlocal protection error 4 139821989727968 82\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould not produce this error.\r\n\r\n**Code to reproduce the issue**\r\nNeed substantial infrastructure (RoCE) and code to reproduce.\r\n\r\n**Other info / logs**\r\nThe `wr`s (verbs work request) that trigger this error has lkey set to 0 and non-zero buffer length, indicating that it's trying to send unregistered memory region.", "comments": ["Currently a PR is in progress targeting this issue.", "@guoshimin could you confirm if #24250 fix your issue? If so, could you close it?", "I'm not in a position to test it since we are still using 1.9. I'll close\nmy PR for now.\n\nOn Mon, Jan 7, 2019, 5:24 AM Bairen Yi <notifications@github.com wrote:\n\n> @guoshimin <https://github.com/guoshimin> could you confirm if #24250\n> <https://github.com/tensorflow/tensorflow/pull/24250> fix your issue? If\n> so, could you close it?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23220#issuecomment-451932338>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALPcHdW9iqH-sEEOea5LZ95qHhtTFOzmks5vA0qkgaJpZM4X4nbz>\n> .\n>\n"]}, {"number": 23219, "title": "CUDA 10 Support", "body": "I am using a containerized env on CentOS with Anaconda, and a RTX 2080 Ti GPU card with 540 Tensor Core Processing Units.\r\n\r\nAccording to the docs: https://www.tensorflow.org/install/gpu\r\nit may not work due to lack of versioning support.\r\n\r\nAlso - how do I leverage the NVIDIA TPUs/is there support for that?", "comments": ["I'm also interested in this issue. Planning to upgrade my hardware for new RTX boards and CUDA 10. Would like to know if all this works properly. I know (not sure though) that to use Tensor core you need to train in FP16 end to end (but I know MaxPool3D is only FP32 which is very impactful on performance).\r\n\r\nFound this, so it seems to work on RTX boards : https://www.pugetsystems.com/labs/hpc/NVIDIA-RTX-2080-Ti-vs-2080-vs-1080-Ti-vs-Titan-V-TensorFlow-Performance-with-CUDA-10-0-1247/", "I'm using cuda 10 with tensorflow( build by myself ) and it's working just fine", "@alanpurple what methods do you use to pipe data to GPU vs TPU on the local video card?\r\n\r\nCUDA 10 support CUDA 9, so as with any upgrade, there should be no issues going down to CUDA 2 or something. \r\n\r\nIBM - Python has no way of accessing tensor cores at this time:\r\nhttps://developer.ibm.com/articles/ba-accelerate-python/\r\n\r\n\r\nHow to use Tensor Cores in CUDA 9:\r\nhttps://devblogs.nvidia.com/programming-tensor-cores-cuda-9/\r\n", "The phrase \"TPU\" refers to [Google's TPUs](https://cloud.google.com/tpu/). If you are using a GPU, you are not using a TPU, but Volta and Turing GPUs, like the RTX 2080, do have [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensorcore/).\r\n\r\nAs @alanpurple said, you can use CUDA 10 by building from source, and TensorFlow has full support for Volta's tensor cores. I am not sure if any special support is need for Turing's tensor cores. @jlebar, do you know if anything special is needed for Turing support?", "Nothing special should be needed for Turing support per se.  TF may have to grow support for int4 and int1 convolutions, I dunno.", "Ok, it seems this is mostly a duplicate of #22706 then, so I'm closing this issue."]}, {"number": 23218, "title": "Updating issue templates to add StackOverflow links", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@martinwicke Can you please take a look at this PR? Thanks!", "closing this PR for now."]}, {"number": 23217, "title": "Typo", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks @vitor-alves. Could you resolve the conflict?", "@vitor-alves Ping. Can you please resolve the conflict?", "Hi\r\nI will update!", "Updated."]}, {"number": 23216, "title": "Failing isinstance check in tensorflow.python.keras.models._clone_functional_model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS, specifically\r\nLinux 3.10.0-693.5.2.el7.x86_64 #1 SMP Fri Oct 20 20:32:50 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n- TensorFlow installed from (source or binary): Through conda\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version:\r\nPython 2.7.15 |Anaconda, Inc.| (default, Oct 10 2018, 21:32:13)\r\n[GCC 7.3.0] on linux2\r\n-Bazel version: NA\r\n-CUDA/cuDNN version: NA\r\n-GPU model and memory: NA\r\n-Exact command to reproduce: NA\r\n-Mobile device: NA\r\n\r\n**Describe the current behavior**\r\n`tensorflow.python.keras.models._clone_functional_model `raises a `ValueError `because the model passed to it is supposedly not an instance of `Model`, even though it is.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\nfrom keras.models import Model\r\nfrom tensorflow.python.keras.engine import training\r\nModel2 = training.Model\r\nisinstance(Model(), Model2)\r\n```\r\nThis returns False. Even though `keras.models.Model` should just be an alias for `tensorflow.python.keras.engine.training.Model`.\r\n\r\n**Other info / logs**\r\nI run into this issue when calling` tensorflow.python.keras.estimator.model_to_estimator`.\r\nI can work around it by importing all my keras imports using the full `tensorflow.python.keras...., `but I'm not sure why that should be necessary.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@sytham,\r\nThis issue has been resolved. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/cf5a882c7db7df102219ac74d8bb7b52/gh_23216.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23216\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23216\">No</a>\n"]}, {"number": 23215, "title": "what does  \"Device peer to peer matrix\" mean?", "body": "I have 2 system  both with 4 1080Ti. but when tensorflow startup. some information is not same: \r\n\r\n1. first of all, the information is printed by different code and the content is not the same. \r\n\r\n1. on second system the diagonal is marked as N.\r\n\r\ndoes someone know what does these log means? **and more importantly, does it imply some error or performance degrade?**\r\n\r\n\r\n**system 1:**\r\n\r\n> 2018-09-29 10:19:05.759292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] Device peer to peer matrix\r\n> 2018-09-29 10:19:05.771338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1233] DMA: 0 1 2 3 \r\n> 2018-09-29 10:19:05.771362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 0:   Y Y Y Y \r\n> 2018-09-29 10:19:05.771367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 1:   Y Y Y Y \r\n> 2018-09-29 10:19:05.771371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 2:   Y Y Y Y \r\n> 2018-09-29 10:19:05.771376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1243] 3:   Y Y Y Y \r\n\r\n**system 2:**\r\n\r\n> 2018-09-29 10:54:20.831299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2018-09-29 10:54:20.831470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-09-29 10:54:20.831480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 2 3 \r\n> 2018-09-29 10:54:20.831486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y Y Y \r\n> 2018-09-29 10:54:20.831491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N Y Y \r\n> 2018-09-29 10:54:20.831495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 2:   Y Y N Y \r\n> 2018-09-29 10:54:20.831499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 3:   Y Y Y N \r\n\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "@ymodak , I think this should classified to others? So, there is actually no template?", "Yes you are correct about that. Reopening this issue for others to share their answers.", "See nvidia's documentation on what is peer-to-peer access: \r\nhttps://developer.nvidia.com/gpudirect\r\nhttps://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__PEER.html ", "according the link you sent me, I found these information:\r\n\r\n> GPUDirect  peer-to-peer transfers and memory access are supported natively by the CUDA Driver. All you need is CUDA Toolkit v4.0 and R270 drivers (or later) and a system with two or more Fermi- or Kepler-architecture GPUs on the same PCIe bus\r\n\r\nconfiguration of HW for these two system is exactly same. so I think all the requirements are met.", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue. Feel free to reopen if have any follow up questions. Thanks!"]}, {"number": 23214, "title": "Try to Run Tensorflow on CUP clusters", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.9 \r\n- TensorFlow version: 1.9.0-rc0\r\n- Python version:3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nFirstly, I want to run tensorflow on a cluster only with CPU nodes. Therefore no CUDA library is installed and only has the CPU-only version of tensorflow installed. I have many questions on the correct setting. Let's start from the very basic situation:\r\n\r\n## 1. node with multiple CPU cores\r\n\r\nFrom this [answer](https://stackoverflow.com/a/37660913/4468490), it propose to set `device_count = {'GPU': 0}` in `tf.ConfigProto` to force the code to be run on CPU only. since my platform has GPUs, so I think this step deosn't work for me. *However*, I am curious on under such setting, how the resources of CPUs are assigned. Will the code only run on only 1 CPU cores or not?\r\n\r\nSo I turned to [yaroslavvb's example](https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21), but I still have some problems:\r\n\r\n1. can I avoid using more statement `with tf.device(\"/cpu:0\"):`, etc?\r\n\r\n2. what's the relation between `device_count={\"CPU\": 8}` and `inter_op_parallelism_threads=3,                    intra_op_parallelism_threads=1`? From the [answer from mrry](https://stackoverflow.com/a/41233901/4468490), I think `inter` controls the distribution with threads working on independent blocks in my graph (say, multiple independent matrix operations), and `intra` works on \"internally parallelable\" tasks (say, a matrix multiplication). Am I correct?  Back to [yaroslavvb's example](https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21), I think the task is assigned to 4 CPU cores, and `{\"CPU\": 8}` requests 8 cores. If I am correct, why request more than needed? \r\n\r\n3. how the task is distributed among each CPU cores, without using `with tf.device()` statement?\r\n\r\n\r\nAfter all these, here is my code, for a simple MNIST example:\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport datetime as dt\r\nimport urllib.request, json\r\nimport os, sys\r\nimport numpy as np\r\nsys.path.append('./stanford-tensorflow-tutorials/examples')\r\nimport utils\r\n# This code has been tested with TensorFlow 1.6\r\nimport tensorflow as tf\r\nimage_size = 28\r\nn_channels = 1\r\nn_classes = 10\r\nn_train = 55000\r\nn_valid = 5000\r\nn_test = 10000\r\nn_epochs = 25\r\nbatch_size = 100\r\nmnist_folder = './MNIST_data'\r\nutils.download_mnist(mnist_folder)\r\ntrain, val, test = utils.read_mnist(mnist_folder, flatten=True)\r\n#mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nconfig = tf.ConfigProto(device_count={\"CPU\": 6}, # limit to num_cpu_core CPU usage  \r\n                inter_op_parallelism_threads = 6,   \r\n                intra_op_parallelism_threads = 6,\r\n                allow_soft_placement=True,                \r\n                log_device_placement=True)  \r\n                \r\n                \r\n                \r\n                \r\ntf.reset_default_graph()\r\n....\r\n```\r\nI am not sure if this `config` setting makes me be able to use 6 CPU cores. \r\n\r\nThen we move further:\r\n## 2. multiple nodes with multiple CPU cores\r\n\r\nI think this belongs to **distributed tensorflow** concept. Is this senario comparable to multiple GPUs running?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nBazel version\nExact command to reproduce\nMobile device"]}, {"number": 23213, "title": "When using TFLite for network inference on mobile phone, the shallow layer of same model takes more time to infer than the deep layer.", "body": "**System information**\r\n- Have I written custom code:\r\nN/A\r\n- OS Platform and Distribution:\r\nUbuntu14.04, Win 10, Android Studio 3.0.1, sdk version 26\r\n- Mobile device:\r\nGome K1 (CPU: MTK MT6757,  2.3GHZ)\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.11.0\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI am using the [offical demo(Java interface)](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo) of TFLite on Android, which classify the frame captured by the camera.  The model is offical released [Mobilenet_v1_1.0_224 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md). The inference time is about 240ms in my mobile phone. Now, I need to get the feature maps of the last convolution layer, so I get the new tflite interpreter based on [TF Lite converter ](https://www.tensorflow.org/lite/convert/cmdline_examples) and the frozen graphs in the offical released  [Mobilenet_v1_1.0_224 ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md), the comandline that I used is:\r\n`tflite_convert --graph_def_file=mobilenet_v1_1.0_224_frozen.pb --output_file=mobilenet_v1_1.0_224.tflite --input_shapes=1,224,224,3 --input_array=input --output_arrays=MobilenetV1/MobilenetV1/Conv2d_13_pointwise/BatchNorm/FusedBatchNorm`\r\nWhen I use the new tflite interpreter for inference to get the feature maps of the last conv layer, **the inference time is 320ms, which is more than the original 240ms when get the prob.** This makes me very confused, since the global avg pooling and FC is no longer needed when using the new interpreter to get the feature map of last conv layer and the compution costs is decreased, so the inference time should not increase. By the way, the code for time measurement remains unchanged, which is:\r\n```\r\n// Here's where the magic happens!!!\r\nlong startTime = SystemClock.uptimeMillis();\r\nrunInference();\r\nlong endTime = SystemClock.uptimeMillis();\r\nLog.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\r\n```\r\nI also test the inference time when I get the feature maps of the middle conv layers: \r\n--------layer--------------------shape of feature map------------inference time(ms)\r\n-------prob------------------------1x1001-------------------------------~240\r\nconv2d_13_pointwise-----------7x7x1024------------------------------~320\r\nconv2d_10_pointwise-----------14x14x512----------------------------~340\r\nconv2d_6_pointwise------------14x14x512----------------------------~260\r\nconv2d_5_pointwise------------28x28x256----------------------------~380\r\nconv2d_3_pointwise------------56x56x128----------------------------~600\r\n\r\n**It seems that the larger the feature map is, the bigger the inference time is,** and the effect of depth on inferenc time is much smaller. \r\nSo, What is the reason? Any help would be grateful. Thanks!\r\n\r\n**Exact command to reproduce:**\r\nN/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Thanks. The problem description has been updated according to the issue template.", "@ymodak @andrehentz \r\nWhen I dig into the source code of TFLite, I found that the above issue was caused by the time-consuming `Shapeof()` functions in Tensor class. To be more specific, the `runInference()` in ImageClassifier.java \r\n```\r\n    // Here's where the magic happens!!!\r\n    long startTime = SystemClock.uptimeMillis();\r\n    runInference();\r\n    long endTime = SystemClock.uptimeMillis();\r\n    Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));\r\n```\r\nwill call the tflite.run() in ImageClassifierFloatMobileNet.java which is almost same as ImageClassifierFloatInception.java,\r\n```\r\n  @Override\r\n  protected void runInference() {\r\n    //tflite.run(imgData, labelProbArray);  //original official code\r\n    tflite.run(imgData, featMap);\r\n  }\r\n```\r\nwhere the featMap is defined as:\r\n```\r\n    labelProbArray = new float[1][getNumLabels()];  //float[1][1001]  //original official code\r\n    featMap = new float[1][getFeatSizeH()][getFeatSizeW()][getFeatSizeC()];  //float[1][7][7][1024]\r\n```\r\n\r\n************\r\nThe` tflite.run()` function will call the `run()` method of NativeInterpreterWrapper :\r\n```\r\nvoid run(Object[] inputs, Map<Integer, Object> outputs) {\r\n        this.inferenceDurationNanoseconds = -1L;\r\n        if(inputs != null && inputs.length != 0) {\r\n            if(outputs != null && !outputs.isEmpty()) {\r\n                int i;\r\n                for(i = 0; i < inputs.length; ++i) {\r\n                    Tensor tensor = this.getInputTensor(i);\r\n                    int[] newShape = tensor.getInputShapeIfDifferent(inputs[i]);\r\n                    if(newShape != null) {\r\n                        this.resizeInput(i, newShape);\r\n                    }\r\n                }\r\n\r\n                if(!this.isMemoryAllocated) {\r\n                    allocateTensors(this.interpreterHandle, this.errorHandle);\r\n                    this.isMemoryAllocated = true;\r\n                    Arrays.fill(this.outputTensors, (Object)null);\r\n                }\r\n\r\n                for(i = 0; i < inputs.length; ++i) {\r\n                    this.getInputTensor(i).setTo(inputs[i]);\r\n                }\r\n\r\n                long inferenceStartNanos = System.nanoTime();\r\n                run(this.interpreterHandle, this.errorHandle);\r\n                long inferenceDurationNanoseconds = System.nanoTime() - inferenceStartNanos;\r\n                Iterator var7 = outputs.entrySet().iterator();\r\n\r\n                while(var7.hasNext()) {\r\n                    Entry<Integer, Object> output = (Entry)var7.next();\r\n                    this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());\r\n                }\r\n\r\n                this.inferenceDurationNanoseconds = inferenceDurationNanoseconds;\r\n            } else {\r\n                throw new IllegalArgumentException(\"Input error: Outputs should not be null or empty.\");\r\n            }\r\n        } else {\r\n            throw new IllegalArgumentException(\"Input error: Inputs should not be null or empty.\");\r\n        }\r\n    }\r\n```\r\nFrom the code, we can find that network inference is done by `run(this.interpreterHandle, this.errorHandle);` and `this.getOutputTensor(((Integer)output.getKey()).intValue()).copyTo(output.getValue());` copy the feature maps to the output array (featMap =new float[1][7][7][1024]).\r\n\r\nThe time of network inference is decreased with shallower layer in Mobilenet (from 220ms for conv2d_13_pointwise to 77ms forconv2d_3_pointwise). However, the `copyTo()` function is the opposite, the shallowr layer with larger feature maps costs more time to finish the copy operation. This lead to my initial issue described above.\r\n\r\nAs for `copyTo()` function which belong to Tensor class, `this.throwExceptionIfTypeIsIncompatible(dst);` is where the issue occured. When judging whether the shape of the pre-defined output array (featMap) is consistent with the shape of feature map of conv layer, `shapeOf()` function in Tensor class is used to get the shape of pre-defined output array (featMap).\r\n```\r\n    static int[] shapeOf(Object o) {\r\n        int size = numDimensions(o);\r\n        int[] dimensions = new int[size];\r\n        fillShape(o, 0, dimensions);\r\n        return dimensions;\r\n    }\r\n    static void fillShape(Object o, int dim, int[] shape) {\r\n        if(shape != null && dim != shape.length) {\r\n            int len = Array.getLength(o);\r\n            if(shape[dim] == 0) {\r\n                shape[dim] = len;\r\n            } else if(shape[dim] != len) {\r\n                throw new IllegalArgumentException(String.format(\"Mismatched lengths (%d and %d) in dimension %d\", new Object[]{Integer.valueOf(shape[dim]), Integer.valueOf(len), Integer.valueOf(dim)}));\r\n            }\r\n\r\n            for(int i = 0; i < len; ++i) {\r\n                fillShape(Array.get(o, i), dim + 1, shape);\r\n            }\r\n\r\n        }\r\n    }\r\n```\r\n**However, the time complexity of `fillShape()` is O(NxHxWxC), where N is the batchsize, H,W,C is the size of feature map. So, the larger the feature map is, the more time it will costs.** Since the output array (feataMap) is defined as new float[1][7][7][1024], I think `shapeOf()` can be simpler and the fillShape() is not necessary.\r\n\r\nNow, I have figured out what the issue is. However, since I use TFLite by adding `compile 'org.tensorflow:tensorflow-lite:+'` in the build.gradle in Android Studio project, the source code of TFLite is read-only and I can't modify it for my project. Is there any other way for me to use the modified source code? Thanks.", "When running a semantic segmentation model on an android device, I run into another issue that `readMultiDimentionalArray()` in `copyTo()` costs a lot of time.\r\n\r\nWhen using android profiler, it looks like this:\r\n![image](https://user-images.githubusercontent.com/8468820/48178309-db8c5d00-e353-11e8-8b8d-df4d33f6ebd5.png)\r\n\r\nIn the figure, `copyTo()` costs totally 88.22ms. `shapeOf()` costs 7.14ms and `readMultiDimentionalArray()` costs 81.07ms. The shape of the output tensor is (1, 128, 128, 1). \r\n\r\nIs there possible solution to reduce the time of this function?", "multi-dimensional arrays incur an expensive copy in the interface between java and C++. Refer to \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L217\r\n\r\n>  ByteBuffer is the preferred way to pass large input data", "Closing due to inactivity. "]}]