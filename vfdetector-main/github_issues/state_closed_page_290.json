[{"number": 45598, "title": "Please let tf.keras.metrics.MeanIoU support y_pred from logits", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n tf.keras.metrics.MeanIoU doesn't have an option that allow y_pred passed as logits, we want to pass y_pred as a logits tensor, you can simply add an optional step into the ```update_state``` function of this class: \r\n```\r\n    y_pred = math_ops.argmax(y_pred, axis=-1)\r\n```\r\n**Will this change the current api? How?**\r\nYes, add one parameter from_logits, you can keep default as False though:\r\n```tf.keras.metrics.MeanIoU(num_classes, from_logits=False, name=None, dtype=None)```\r\n\r\n**Who will benefit with this feature?**\r\nComputer vision object detection/segmentation community\r\n\r\n**Any Other info.**\r\n", "comments": ["@motionlife Can you prepare a small PR?", "> @motionlife Can you prepare a small PR?\r\n\r\nSure, just did :)", "Thanks for the feature request -- moving discussion to the PR you created."]}, {"number": 45597, "title": "TF-TRT ConvertPad in dynamic shape mode", "body": "This PR enables Pad op conversion in dynamic shape mode and adds unit test.\r\n\r\nAdditionally the converter is improved:\r\n- handle tensors with rank > 4,\r\n- no specific restrictions on which dim to pad (apart from no batch dim padding in implicit batch mode),\r\n- return error message if rank < 4.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n\r\nTracker: #45481", "comments": ["Converted to draft: still needs to extend the test cases and additionally there seem to be a problem with our way of transposing input to match TRT requirements. I am working on a fix.", "Converter improved, ready for review.", "@tfeher  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@tfeher  Any update on this PR? Please. Thanks!", "Rebased again."]}, {"number": 45596, "title": "Tensorflow float32 conversion bug from python to tensorflow constants", "body": "Tensorflow changes values well within tf.float32 range to nearby values.\r\n\r\nIs this expected behaviour? Numpy doesn't have this issue and given the values are within float32 range I'm not sure why this is happening.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10, same issue on linux though**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **No**\r\n- TensorFlow version (use command below): **version 2.1.0**\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): **Not build from source**\r\n- GCC/Compiler version (if compiling from source): **Not build from source**\r\n- CUDA/cuDNN version:  **running on cpu**\r\n- GPU model and memory:  ****\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**\r\nFor example\r\n`tf.constant(20200131, dtype=tf.float32)`\r\nBecomes <tf.Tensor: shape=(), dtype=float32, numpy=20200132.0>\r\n\r\nWhy the extra 1?**\r\n\r\nExpected behaviour:\r\n**\r\n`tf.constant(20200131, dtype=tf.float32)`\r\nBecomes <tf.Tensor: shape=(), dtype=float32, numpy=20200131.0>\r\n**\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\ntf.constant(20200131, dtype=tf.float32)\r\n", "comments": ["In fact, `numpy` **does** have the same behaviour:\r\n```python\r\n>>> import numpy as np\r\n>>> np.full([], 20200131, dtype=np.float32)\r\narray(20200132., dtype=float32)\r\n```\r\nIt's just that `numpy` defaults to `float64`, which is why you do not notice it when invoking without specifying `dtype`:\r\n```python\r\n>>> a = np.full([], 20200131)\r\na = np.full([], 20200131)\r\n>>> a\r\narray(20200131)\r\n>>> a.dtype\r\ndtype('int64')\r\n```\r\nFloat32 rounds to even numbers for large values. See also this interesting [Wikipedia page](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):\r\n> Integers between 224=16777216 and 225=33554432 round to a multiple of 2 (even number)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45596\">No</a>\n"]}, {"number": 45595, "title": "README: update Linaro CI links", "body": "We have two CI jobs now:\r\n\r\n- Tensorflow stable (1.15.x and 2.x)\r\n- Tensorflow nightly (git master HEAD)\r\n\r\nBuilds are done on CentOS 8 (Python 3.6) and Debian 'buster' (Python 3.7).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45595) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45595) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 45593, "title": "[Intel MKL] Fix memory leak in MklAddN", "body": "", "comments": []}, {"number": 45591, "title": "tfjs-models/face-landmarks-detection/demo/ TypeError: Fail to fetch", "body": "Hi ,  I run this demo in my macbook, I don't have VPN,  How to synchronize the model and store it myself. thank you!\r\n", "comments": ["@Sunyingbin \r\nPlease create this issue in tfjs repo, and move this to closed status."]}, {"number": 45590, "title": "Value 'sm_86' is not defined for option 'gpu-name'", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):docker\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):tf-nightly-gpu         2.5.0.dev20201210\r\n- Python version:3.6.9\r\n- CUDA/cuDNN version:11.0/8.0.4\r\n- GPU model and memory:3090/24GB\r\n\r\nwhen i inference my tf model,it appears hundreds of warnings like the following image \r\nabout 1 minute later ,the normal inference begin\r\ni watch the issus and try to used tf2.4re02 but it was useless\r\n![image](https://user-images.githubusercontent.com/33931180/101855756-ef43c680-3b9e-11eb-807e-7756853c75ab.png)\r\n\r\n", "comments": ["@liuxufenfeiya \r\n\r\nYou can ignore those warning.If you want to disable the warning messages add the below code.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\n\r\n```\r\nIf you feel the issue still persists request you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.\r\nThanks!", "> @liuxufenfeiya\r\n> \r\n> You can ignore those warning.If you want to disable the warning messages add the below code.\r\n> \r\n> ```\r\n> import os\r\n> import tensorflow as tf\r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\n> ```\r\n> \r\n> If you feel the issue still persists request you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.\r\n> Thanks!\r\n\r\ni had already try the way,but it was no use ", "Try upgrading your cuda version to 11.1", "cuda 11.1 does not solve the problem.\r\n\r\n```\r\n2020-12-14 01:30:45.379556: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n```\r\n\r\n```\r\n(venv)crutcher@heatlamp:~/git/smot$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 20.04.1 LTS\r\nRelease:\t20.04\r\nCodename:\tfocal\r\n```\r\n\r\n```\r\n(venv)crutcher@heatlamp:~/git/smot$ nvidia-smi\r\nMon Dec 14 01:34:58 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.38       Driver Version: 455.38       CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3090    Off  | 00000000:21:00.0  On |                  N/A |\r\n| 30%   38C    P8    34W / 350W |  23683MiB / 24265MiB |     14%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 3090    Off  | 00000000:4A:00.0 Off |                  N/A |\r\n| 30%   33C    P8    31W / 350W |    413MiB / 24268MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2618      G   /usr/lib/xorg/Xorg                555MiB |\r\n|    0   N/A  N/A      2895      G   /usr/bin/gnome-shell               95MiB |\r\n|    0   N/A  N/A      7327      G   ...AAAAAAAA== --shared-files      150MiB |\r\n|    0   N/A  N/A     33604      G   ...AAAAAAAAA= --shared-files       36MiB |\r\n|    0   N/A  N/A     60502      G   ..._59022.log --shared-files       35MiB |\r\n|    0   N/A  N/A     67654      C   .../git/smot/venv/bin/python    22781MiB |\r\n|    1   N/A  N/A      2618      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    1   N/A  N/A      2895      G   /usr/bin/gnome-shell                0MiB |\r\n|    1   N/A  N/A      7327      G   ...AAAAAAAA== --shared-files        0MiB |\r\n|    1   N/A  N/A     33604      G   ...AAAAAAAAA= --shared-files        0MiB |\r\n|    1   N/A  N/A     60502      G   ..._59022.log --shared-files        0MiB |\r\n|    1   N/A  N/A     67654      C   .../git/smot/venv/bin/python      405MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n```\r\n(venv)crutcher@heatlamp:~/git/smot$ dpkg -l | grep cudnn\r\nii  libcudnn8                                  8.0.5.39-1+cuda11.1                                      amd64        cuDNN runtime libraries\r\nii  libcudnn8-dev                              8.0.5.39-1+cuda11.1                                      amd64        cuDNN development libraries and headers\r\n```\r\n\r\n```\r\n(venv)crutcher@heatlamp:~/git/smot$ dpkg -l | grep cuda\r\nii  cuda                                       11.1.1-1                                                 amd64        CUDA meta-package\r\nii  cuda-11-1                                  11.1.1-1                                                 amd64        CUDA 11.1 meta-package\r\nii  cuda-command-line-tools-11-1               11.1.1-1                                                 amd64        CUDA command-line tools\r\nii  cuda-compiler-11-1                         11.1.1-1                                                 amd64        CUDA compiler\r\nii  cuda-cudart-11-1                           11.1.74-1                                                amd64        CUDA Runtime native Libraries\r\nii  cuda-cudart-dev-11-1                       11.1.74-1                                                amd64        CUDA Runtime native dev links, headers\r\nii  cuda-cuobjdump-11-1                        11.1.74-1                                                amd64        CUDA cuobjdump\r\nii  cuda-cupti-11-1                            11.1.105-1                                               amd64        CUDA profiling tools runtime libs.\r\nii  cuda-cupti-dev-11-1                        11.1.105-1                                               amd64        CUDA profiling tools interface.\r\nii  cuda-demo-suite-11-1                       11.1.74-1                                                amd64        Demo suite for CUDA\r\nii  cuda-documentation-11-1                    11.1.105-1                                               amd64        CUDA documentation\r\nii  cuda-driver-dev-11-1                       11.1.74-1                                                amd64        CUDA Driver native dev stub library\r\nii  cuda-drivers                               455.32.00-1                                              amd64        CUDA Driver meta-package, branch-agnostic\r\nii  cuda-drivers-455                           455.32.00-1                                              amd64        CUDA Driver meta-package, branch-specific\r\nii  cuda-gdb-11-1                              11.1.105-1                                               amd64        CUDA-GDB\r\nii  cuda-libraries-11-1                        11.1.1-1                                                 amd64        CUDA Libraries 11.1 meta-package\r\nii  cuda-libraries-dev-11-1                    11.1.1-1                                                 amd64        CUDA Libraries 11.1 development meta-package\r\nii  cuda-memcheck-11-1                         11.1.105-1                                               amd64        CUDA-MEMCHECK\r\nii  cuda-nsight-11-1                           11.1.105-1                                               amd64        CUDA nsight\r\nii  cuda-nsight-compute-11-1                   11.1.1-1                                                 amd64        NVIDIA Nsight Compute\r\nii  cuda-nsight-systems-11-1                   11.1.1-1                                                 amd64        NVIDIA Nsight Systems\r\nii  cuda-nvcc-11-1                             11.1.105-1                                               amd64        CUDA nvcc\r\nii  cuda-nvdisasm-11-1                         11.1.74-1                                                amd64        CUDA disassembler\r\nii  cuda-nvml-dev-11-1                         11.1.74-1                                                amd64        NVML native dev links, headers\r\nii  cuda-nvprof-11-1                           11.1.105-1                                               amd64        CUDA Profiler tools\r\nii  cuda-nvprune-11-1                          11.1.74-1                                                amd64        CUDA nvprune\r\nii  cuda-nvrtc-11-1                            11.1.105-1                                               amd64        NVRTC native runtime libraries\r\nii  cuda-nvrtc-dev-11-1                        11.1.105-1                                               amd64        NVRTC native dev links, headers\r\nii  cuda-nvtx-11-1                             11.1.74-1                                                amd64        NVIDIA Tools Extension\r\nii  cuda-nvvp-11-1                             11.1.105-1                                               amd64        CUDA Profiler tools\r\nii  cuda-repo-ubuntu2004-11-1-local            11.1.1-455.32.00-1                                       amd64        cuda repository configuration files\r\nii  cuda-runtime-11-1                          11.1.1-1                                                 amd64        CUDA Runtime 11.1 meta-package\r\nii  cuda-samples-11-1                          11.1.105-1                                               amd64        CUDA example applications\r\nii  cuda-sanitizer-11-1                        11.1.105-1                                               amd64        CUDA Sanitizer\r\nii  cuda-toolkit-11-1                          11.1.1-1                                                 amd64        CUDA Toolkit 11.1 meta-package\r\nii  cuda-tools-11-1                            11.1.1-1                                                 amd64        CUDA Tools meta-package\r\nii  cuda-visual-tools-11-1                     11.1.1-1                                                 amd64        CUDA visual tools\r\nii  libcudart10.1:amd64                        10.1.243-3                                               amd64        NVIDIA CUDA Runtime Library\r\nii  libcudnn8                                  8.0.5.39-1+cuda11.1                                      amd64        cuDNN runtime libraries\r\nii  libcudnn8-dev                              8.0.5.39-1+cuda11.1                                      amd64        cuDNN development libraries and headers\r\nii  nvidia-cuda-dev                            10.1.243-3                                               amd64        NVIDIA CUDA development files\r\nii  nvidia-cuda-doc                            10.1.243-3                                               all          NVIDIA CUDA and OpenCL documentation\r\nii  nvidia-cuda-gdb                            10.1.243-3                                               amd64        NVIDIA CUDA Debugger (GDB)\r\nii  nvidia-cuda-toolkit                        10.1.243-3                                               amd64        NVIDIA CUDA development toolkit\r\n\r\n```\r\n\r\n```\r\n(venv)crutcher@heatlamp:~/git/smot$ pip show tf-nightly\r\nName: tf-nightly\r\nVersion: 2.5.0.dev20201213\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/crutcher/git/smot/venv/lib/python3.8/site-packages\r\nRequires: tb-nightly, six, tf-estimator-nightly, grpcio, opt-einsum, termcolor, google-pasta, flatbuffers, h5py, astunparse, keras-preprocessing, numpy, wheel, wrapt, protobuf, gast, absl-py, typing-extensions\r\nRequired-by: \r\n```\r\n\r\n", "I think that this is a duplicate of https://github.com/tensorflow/tensorflow/issues/44750", "exactly the same issue", "> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):docker\r\n> * TensorFlow installed from (source or binary):pip\r\n> * TensorFlow version (use command below):tf-nightly-gpu         2.5.0.dev20201210\r\n> * Python version:3.6.9\r\n> * CUDA/cuDNN version:11.0/8.0.4\r\n> * GPU model and memory:3090/24GB\r\n> \r\n> when i inference my tf model,it appears hundreds of warnings like the following image\r\n> about 1 minute later ,the normal inference begin\r\n> i watch the issus and try to used tf2.4re02 but it was useless\r\n> ![image](https://user-images.githubusercontent.com/33931180/101855756-ef43c680-3b9e-11eb-807e-7756853c75ab.png)\r\n\r\nDid you solve the issue? If you do, can you share how you do it.", "These warnings are mainly there for diagnostic purposes (so that when a user reports a bug we know that they were using the driver JIT and not ptxas).  For now you can ignore them, and we'll fix the warning to be less verbose / scary.", "@sanjoy but it has been reported performance loss due to this ptxas fatal . e.g. https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf \r\n", "I had the same issue. \r\nUsing tensorflow compiled with cuda 11.1 solved it.", "I had the same issue.\r\nIn my case, replacing the ptxas file from cuda 11.0 by the ptxas file from 11.1 (or 11.2) solved the problem. This file can be found in \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\\" on Windows and in \"/usr/local/cuda-11.0/bin/\" on Linux. \r\n(I also have a rtx 3090 with tf 2.4.1)", "> I had the same issue.\r\n> In my case, replacing the ptxas file from cuda 11.0 by the ptxas file from 11.1 (or 11.2) solved the problem. This file can be found in \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\" on Windows and in \"/usr/local/cuda-11.0/bin/\" on Linux.\r\n> (I also have a rtx 3090 with tf 2.4.1)\r\n\r\nDid you just replace the ptax file or upgrade cuda from 11.0 to 11.1/11.2? @GuillaumeMougeot ", "> Did you just replace the ptax file or upgrade cuda from 11.0 to 11.1/11.2? @GuillaumeMougeot\r\n\r\nI just replaced the ptxas file. After installing cuda 11.0, I had to download cuda 11.2. I then copied the ptxas file from cuda 11.2 into cuda 11.0 and the warnings stopped. @rajasd27 ", "> I had the same issue.\r\n> In my case, replacing the ptxas file from cuda 11.0 by the ptxas file from 11.1 (or 11.2) solved the problem. This file can be found in \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\" on Windows and in \"/usr/local/cuda-11.0/bin/\" on Linux.\r\n> (I also have a rtx 3090 with tf 2.4.1)\r\n\r\nIt works for me\r\n\r\n", "> > Did you just replace the ptax file or upgrade cuda from 11.0 to 11.1/11.2? @GuillaumeMougeot\r\n> \r\n> I just replaced the ptxas file. After installing cuda 11.0, I had to download cuda 11.2. I then copied the ptxas file from cuda 11.2 into cuda 11.0 and the warnings stopped. @rajasd27\r\n\r\nWorked for me. I was using tf-gpu docker so had to rebuild a new image by replacing the ptxas file. Thanks a lot!", "1. Upgrade to cuda 11.1+ doesn't work for me. There is an error:\r\n    Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory.\r\n2. Manually copy ptxas in cuda 11.1+ doesn't work neither. There would be an exception during evaluation.\r\n3. Still waiting for an update. \r\n( linux version: linux 5.10.16.arch1-1 / RTX3090/ cuda 11.0.3-1 /cudnn 8.1.0.77-1/ nvidia 460.39-8 / tensorflow 2.4.1)", "TF with new cuda versions is tracked at https://github.com/tensorflow/tensorflow/issues/46093", "I had the same ptax related error when using the Tensorflow provided container (tensorflow/tensorflow:latest-gpu-jupyter -> tensorflow 2.4)\r\n\r\nFixed by using the NVIDIA provided container (nvcr.io/nvidia/tensorflow:21.02-tf2-py3 also tensorflow 2.4)\r\navailable at\r\nhttps://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/running.html#running", "> > I had the same issue.\r\n> > In my case, replacing the ptxas file from cuda 11.0 by the ptxas file from 11.1 (or 11.2) solved the problem. This file can be found in \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\" on Windows and in \"/usr/local/cuda-11.0/bin/\" on Linux.\r\n> > (I also have a rtx 3090 with tf 2.4.1)\r\n> \r\n> It works for me\r\n\r\nThanks\uff01\uff01!It also works for me.", "@liuxufenfeiya,\r\nCan you please let us know if we can close this issue as it has been resolved? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45590\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45590\">No</a>\n", "> These warnings are mainly there for diagnostic purposes (so that when a user reports a bug we know that they were using the driver JIT and not ptxas). For now you can ignore them, and we'll fix the warning to be less verbose / scary.\r\n\r\nI think this issue is causing:\r\n- Model does not begin training until GPU memory is full and then it switches to CPU.\r\n- So this warning prints out until the GPU can't be used and then it stops and the CPU starts. "]}, {"number": 45589, "title": "[TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertClipByValue", "body": "@bixia1 @tfeher for review\r\n\r\nFeature Tracker: https://github.com/tensorflow/tensorflow/issues/45481\r\n\r\nPlease merge this PR after: https://github.com/tensorflow/tensorflow/pull/45587", "comments": ["@bixia1 rebased on master (with the new `OpConverterTest` name).\r\nGood for review / merge", "The test doesn't even compiled and here is just one example of the error message:\r\nthird_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:6637:5: error: no matching member function for call to 'AddTestTensor'\r\n    AddTestTensor(\"t\", {1, 2, 3}, trt_type_);\r\n    ^~~~~~~~~~~~~\r\nthird_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1811:8: note: candidate template ignored: could not match 'vector<type-parameter-0-0>' against 'nvinfer1::DataType'\r\n  void AddTestTensor(const string& name, const std::vector<int32>& dims,\r\n       ^\r\n\r\n\r\n", "The bug is fixed and we are good.\r\n\r\n```bash\r\nINFO: 318 processes: 318 local.\r\nINFO: Build completed successfully, 325 total actions\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test                     PASSED in 16.7s\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                 PASSED in 14.9s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test                     PASSED in 96.0s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                 PASSED in 86.8s\r\n//tensorflow/compiler/tf2tensorrt:segment_test                           PASSED in 0.2s\r\n//tensorflow/compiler/tf2tensorrt:segment_test_gpu                       PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                       PASSED in 16.7s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                   PASSED in 15.9s\r\n//tensorflow/compiler/tf2tensorrt:trt_allocator_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test                     PASSED in 13.0s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                 PASSED in 12.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test           PASSED in 13.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu       PASSED in 12.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test   PASSED in 15.9s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu PASSED in 12.1s\r\n\r\nExecuted 16 out of 16 tests: 16 tests pass.\r\nINFO: Build completed successfully, 325 total actions\r\n```", "I am still seeing this error when using TRT7:\r\nthird_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:6687:44: error: type 'float' cannot be narrowed to 'int' in initializer list [-Wc++11-narrowing]\r\n    AddTestWeights(\"clip_value_min\", {1}, {p.clip_value_min}, tf_type_);\r\n                                           ^~~~~~~~~~~~~~~~\r\nthird_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:6687:44: note: insert an explicit cast to silence this issue\r\n    AddTestWeights(\"clip_value_min\", {1}, {p.clip_value_min}, tf_type_);\r\n                                           ^~~~~~~~~~~~~~~~\r\n                                           static_cast<int>( )\r\n", "Hum... I'm not exactly sure why it fails on your side. It works perfectly on my end...\r\n\r\nI changed the dtype of `clip_value_min` and `clip_value_max` to int. That should fix the error message you saw: https://github.com/tensorflow/tensorflow/pull/45589/files#diff-b9ae60f058d9ae72ddc0625fa93ff1b8441e57db5695e8444d18becdb3abea2dR6655-R6656\r\n\r\n```bash\r\nINFO: Build completed successfully, 21 total actions\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test                     PASSED in 14.7s\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                 PASSED in 14.6s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test                     PASSED in 91.2s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                 PASSED in 91.8s\r\n//tensorflow/compiler/tf2tensorrt:segment_test                           PASSED in 0.3s\r\n//tensorflow/compiler/tf2tensorrt:segment_test_gpu                       PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                       PASSED in 13.2s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                   PASSED in 13.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_allocator_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test                     PASSED in 12.6s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                 PASSED in 12.7s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test           PASSED in 12.4s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu       PASSED in 12.2s\r\n//tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test   PASSED in 12.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu PASSED in 12.0s\r\n\r\nExecuted 16 out of 16 tests: 16 tests pass.\r\nINFO: Build completed successfully, 21 total actions\r\n```"]}, {"number": 45588, "title": "Add TF-TRT optimization profiles for testing", "body": "TF-TRT dynamic shape feature requires the creation of optimization profiles. Currently just a basic optimization profile is created where we set min = opt = max = input_shape. Let's call this `optimal` profile.\r\n\r\nThis PR implements an additional optimization profile generation strategies:\r\n- kImplicitBatchLike: the batch dimension has min = 1, opt = max = input_batch_size, other dims agree with the input dim\r\n\r\nFor any shape tensor compatible input the profile is adjusted to have max > min. The goal here is to enable unit testing of converters that have shape input. This requires optimization profile where max != min. Otherwise the shape input tensor is known at build time (equals with min=opt=max), and TRT7 drops it from the list of input bindings. (This problem will probably be corrected in an upcoming version of TRT).\r\n\r\nAdditionally, profile handling improved in dynamic shape mode. Previously it required to run `profile_generation_mode`, otherwise engine creation failed. This condition was relaxed: if profile generation mode was not used, then we create a single implicit batch like profile using the current input shape. This improves testing of the trt_engine_op, additionally this will provide an implicit batch mode compatible fall-back for users that do not wish to build the engine in advance.\r\n\r\nA follow up PR shall implement additional optimization profile strategies.\r\n\r\nAdditionally, two bugs are fixed:\r\n- The profile index in case of newly created engine was not set,\r\n- Error was not handled while setting the input binding size.\r\n- \r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n", "comments": ["@tfeher Can you please check @DEKHTIARJonathan's comments and keep us posted ? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "One more thing:  with this PR, now tensorflow/python/compiler/tensorrt:trt_mode_test  fails in TF2  with cuda 11 and TensorRT 7 (--test_env=TF2_BEHAVIOR=1). Here is one subtest and error message:\r\nExplicitBatchTest.testTfTrtV2_OfflineConversion_DynamicEngine_FP16_NoCalibration\r\n\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.AbortedError:  User disallowed engine native segment execution\r\n\t [[node TRTEngineOp_3_0 (defined at /third_party/py/absl/testing/absltest.py:2494) ]] [Op:__inference_pruned_2404]\r\n", "@tfeher  Can you please resolve conflicts? Thanks!", "We are having some offline discussion on the solution to workaround the TensorRT bugs mentioned in the PR.", "@bixia1, I have updated the PR based on our offline discussion: the TRT bug workaround is handled now in any optimization profile strategy by adjusting profile parameters for input tensors that are shape value compatible. Additionally, the error in the python test is fixed. Please have a look.", "@bixia1, this seems to be stuck. Can you have a look?", "I couldn't figure out what is going on either. I tried to do it manually, and got this error. Trying to seek help internally.\r\nERROR: Unexpected error (please file a bug against copybara): Multiple entries with same key: tfeher=CONTRIBUTOR and tfeher=CONTRIBUTOR (java.lang.IllegalArgumentException: Multiple entries with same key: tfeher=CONTRIBUTOR and tfeher=CONTRIBUTOR)\r\n210209 08:05:40.487:I 19 [Thread-3] [com.google.chubby.svelte.WaiterThread.endSession:718] shutting down svelte session\r\n\r\n"]}, {"number": 45587, "title": "[TFTRT] Rename ParameterizedOpConverterTest classes with more descriptive names", "body": "Rename the derived classes of `ParameterizedOpConverterTestBase` as follows for better readability:\r\n\r\n`OpConverterTest1` => `OpConverter_FP32_Test`\r\n`OpConverterTest2` => `OpConverter_FP32_FP16_Test`\r\n`OpConverterTest3` => `OpConverter_FP32_FP16_INT32_Test`", "comments": ["Sounds fair with me. @tfeher what do you think ? If you agree, I'll push Bixia's recommendation", "I will take @tfeher and @bixia1 comments into account and resubmit. Expect the update today ;) ", "@bixia1 @tfeher updated as asked. We should be good to merge", "You missed the first comment in https://github.com/tensorflow/tensorflow/pull/45587#pullrequestreview-549693611. I will try to make this change myself."]}, {"number": 45586, "title": "Object Detection doesn't work", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Windows\r\n- TensorFlow installed from source:\r\n- TensorFlow version 2.3.1\r\n- Python version 3.8.3\r\n- CUDA/cuDNN version 11.1, 11, 10.1, and 9.0 installed on my pc\r\n- GPU model: GeForce GTX 1050 (4 GB of memory)\r\n\r\n**Actual Behavior**\r\nWhen I run the script I use to do object_detection on a selection of images the script ran to the end without actually doing anything. Outputting to the terminal:\r\n\r\nRunning inference image001.jpg... Done\r\n\r\nAfter some time it started producing a new error at \"detections = detect_fn(input_tensor)\":\r\n\r\n```\r\nPS C:\\Users\\mauri> & C:/Users/mauri/AppData/Local/Programs/Python/Python38/python.exe d:/Maurice_Doc/AI/Tensorflow/scripts/preprocessing/detect_object.py\r\n2020-12-10 22:17:31.572529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-10 22:17:45.294140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-12-10 22:17:46.342342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-12-10 22:17:46.343628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-10 22:17:46.362359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-10 22:17:46.385461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-10 22:17:46.411085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-10 22:17:46.441132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-10 22:17:46.458760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-10 22:17:46.492894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-10 22:17:46.493462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-12-10 22:17:46.494738: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-12-10 22:17:46.532117: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ffbf1b1960 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-10 22:17:46.537696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-10 22:17:46.538486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-12-10 22:17:46.561378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-10 22:17:46.562003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-10 22:17:46.565033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-10 22:17:46.565437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-10 22:17:46.565838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-10 22:17:46.568388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-10 22:17:46.568766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-10 22:17:46.569282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-12-10 22:17:47.930696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-10 22:17:47.931148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-12-10 22:17:47.931566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2020-12-10 22:17:47.932159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2993 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-12-10 22:17:47.955511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ffdb384350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-10 22:17:47.969376: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1\r\n2020-12-10 22:18:53.677422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-10 22:18:55.271299: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-12-10 22:18:55.441293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\nRunning inference for bdc1db0ac17.jpg... Done\r\nRunning inference for test.png... Traceback (most recent call last):\r\n  File \"d:/Maurice_Doc/AI/Tensorflow/scripts/preprocessing/detect_object.py\", line 59, in <module>\r\n    detections = detect_fn(input_tensor)\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 486, in _call_attribute\r\n    return instance.__call__(*args, **kwargs)\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 618, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2419, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2734, in _maybe_define_function\r\n    args, kwargs = self._function_spec.canonicalize_function_inputs(\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2235, in canonicalize_function_inputs\r\n    inputs = _convert_inputs_to_signature(\r\n  File \"C:\\Users\\mauri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2304, in _convert_inputs_to_signature\r\n    raise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    tf.Tensor(\r\n[[[[255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   ...\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]]\r\n\r\n  [[255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   ...\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]]\r\n\r\n  [[255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   ...\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]]\r\n\r\n  ...\r\n\r\n  [[255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   ...\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]]\r\n\r\n  [[255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   ...\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n\r\n  [[255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   ...\r\n   [255 255 255 255]\r\n   [255 255 255 255]\r\n   [255 255 255 255]]]], shape=(1, 480, 640, 4), dtype=uint8))\r\n  input_signature: (\r\n    TensorSpec(shape=(1, None, None, 3), dtype=tf.uint8, name='input_tensor'))\r\n```\r\n\r\n**Expected Behavior**\r\n\r\nThe expected behavior is that it should display the images with the detected objects based on the trained model that I have. As shown in the example here(https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model.html#load-the-model)\r\n\r\nI've tried changing matplotlib.rcParams withing the script too matplotlib.rcParams['backend'] = \"Qt4Agg\".\r\nI've tried changing  backend in config/matplotlib/matplotlibrc to backend:qt4agg aswell as other values.\r\nFor the second error I've deleting and recreating the saved_model.pb file but nothing seems to solve the problem.\r\n\r\n\r\n\r\n**Source Code**\r\n\r\n```\r\nimport numpy as np\r\nimport os\r\nfrom PIL import Image\r\nimport tensorflow as tf\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import visualization_utils as viz_utils\r\nimport matplotlib.pyplot as plt\r\nimport warnings\r\nwarnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\r\n\r\ncategory_index = label_map_util.create_category_index_from_labelmap(\"D:/Maurice_Doc/AI/Tensorflow/workspace/training_demo/images/annotations/label_map.pbtxt\",\r\n                                                                    use_display_name=True)\r\n\r\nos.chdir('D:/Maurice_Doc/AI/Tensorflow/workspace/training_demo/images/test')\r\nIMAGE_PATHS =os.listdir()\r\ndetect_fn = tf.saved_model.load(\"D:/Maurice_Doc/AI/Tensorflow/workspace/training_demo/exported-models/my_model/saved_model\")\r\n\r\ndef load_image_into_numpy_array(path):\r\n    \"\"\"Load an image from file into a numpy array.\r\n\r\n    Puts image into numpy array to feed into tensorflow graph.\r\n    Note that by convention we put it into a numpy array with shape\r\n    (height, width, channels), where channels=3 for RGB.\r\n\r\n    Args:\r\n      path: the file path to the image\r\n\r\n    Returns:\r\n      uint8 numpy array with shape (img_height, img_width, 3)\r\n    \"\"\"\r\n    return np.array(Image.open(path))\r\n\r\n\r\nfor image_path in IMAGE_PATHS:\r\n\r\n    print('Running inference for {}... '.format(image_path), end='')\r\n\r\n    image_np = load_image_into_numpy_array(image_path)\r\n\r\n    # Things to try:\r\n    # Flip horizontally\r\n    # image_np = np.fliplr(image_np).copy()\r\n\r\n    # Convert image to grayscale\r\n    # image_np = np.tile(\r\n    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\r\n\r\n    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\r\n    input_tensor = tf.convert_to_tensor(image_np)\r\n    # The model expects a batch of images, so add an axis with `tf.newaxis`.\r\n    input_tensor = input_tensor[tf.newaxis, ...]\r\n\r\n    # input_tensor = np.expand_dims(image_np, 0)\r\n    detections = detect_fn(input_tensor)\r\n\r\n    # All outputs are batches tensors.\r\n    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\r\n    # We're only interested in the first num_detections.\r\n    num_detections = int(detections.pop('num_detections'))\r\n    detections = {key: value[0, :num_detections].numpy()\r\n                   for key, value in detections.items()}\r\n    detections['num_detections'] = num_detections\r\n\r\n    # detection_classes should be ints.\r\n    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\r\n\r\n    image_np_with_detections = image_np.copy()\r\n\r\n    viz_utils.visualize_boxes_and_labels_on_image_array(\r\n          image_np_with_detections,\r\n          detections['detection_boxes'],\r\n          detections['detection_classes'],\r\n          detections['detection_scores'],\r\n          category_index,\r\n          use_normalized_coordinates=True,\r\n          max_boxes_to_draw=200,\r\n          min_score_thresh=.30,\r\n          agnostic_mode=False)\r\n\r\n    plt.figure()\r\n    plt.imshow(image_np_with_detections)\r\n    print('Done')\r\nplt.show()\r\n\r\n```\r\nHas someone possibly had this same issue or know how I could solve it?", "comments": ["@CloneHub94,\r\nIssues related to TensorFlow object detection module are tracked in the tensorflow/models repo. \r\n\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/models/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks! "]}, {"number": 45585, "title": "Inconsistent results when using tf_upgrade_v2 script", "body": "**System information**\r\n\r\n- Run in Google Collab Notebook with macOS Big Sur\r\n- There are two files run: \r\n    - a version running code with Tensorflow 2\r\n    - a version running code with Tensorflow 1.x\r\n- Python 3.6.9\r\n\r\n**Current behavior**\r\n\r\nWe wanted to see if the result of the migration script from TF 1.x to TF 2 gives the same output. To evaluate this, we took code from Tensorflow 1.x taken from this tutorial (https://medium.com/@udolf15/mnist-digits-classification-with-tensorflow-7f7dcda0fc1e) and ran it with the migration script and got a compat version of the code. I set the graph-level and operation-level seeds to be able to have reproducible outputs on both the compat(TF2 using migration script) and original version (TF 1.x). \r\n\r\nThe results from running the Original TF 1.x version are: \r\n```\r\n(train_loss, train_accuracy)\r\n0.8028196 0.91954\r\n0.23598278 0.9466\r\n0.1635617 0.96346\r\n0.12261735 0.97066\r\n0.09810413 0.97426\r\n0.07586458 0.97328\r\n0.057922967 0.98538\r\n0.0451939 0.98892\r\n0.036826767 0.99028\r\n0.032206446 0.98698\r\n0.027886271 0.99306\r\n0.023077298 0.99478\r\n0.016734073 0.99372\r\n0.016260928 0.98992\r\n0.015216767 0.99534\r\n0.012952065 0.9955\r\n0.008880938 0.99868\r\n0.006781239 0.99742\r\n0.006611649 0.99772\r\n0.007158364 0.99824\r\n```\r\n\r\nThe results from running the Migration Script TF 2 version are: \r\n```\r\n(train_loss, train_accuracy)\r\n0.86170954 0.91768\r\n0.23816809 0.9488\r\n0.16750608 0.96032\r\n0.12643586 0.96756\r\n0.0971571 0.97194\r\n0.07221356 0.9815\r\n0.05393147 0.9875\r\n0.04196974 0.98938\r\n0.03490297 0.98796\r\n0.029905643 0.98764\r\n0.027600855 0.98814\r\n0.026849583 0.9914\r\n0.021233767 0.99068\r\n0.015700594 0.99518\r\n0.0131430775 0.99696\r\n0.010740909 0.99608\r\n0.0077435044 0.99426\r\n0.0060562133 0.99568\r\n0.006112121 0.99866\r\n0.0034680425 0.99874\r\n```\r\n\r\nAs we can see, they are significantly different. We ran this multiple times, the original version gave us consistent results whereas the compat version gave us varying results.\r\n\r\nFrom the migration script, we get a warning:\r\n```\r\n31:21: INFO: Changing labels arg of tf.nn.softmax_cross_entropy_with_logits to tf.stop_gradient(labels). Please check this transformation.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior was that the results between the Migration Script TF 2 version would yield similiar results to the original TF 1.x results.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nYou can reproduce this results running them on the following Colab Notebooks: \r\n\r\nTF 1.x Original Version: https://colab.research.google.com/drive/1OL_qPqdF9Hfhksqnri2BpJmbRc5LT84d?usp=sharing\r\n\r\nTF 2 Migration Script version (with tf.stop_gradient): https://colab.research.google.com/drive/12sao4pWQ_x5tLLazux0LkfqeoqhnaCuL?usp=sharing", "comments": ["I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/1c864f75d99bbf54f41ae4d81652b16b/untitled484.ipynb) and [2.x here](https://colab.research.google.com/gist/Saduf2019/a9ebb6debac783e304d1423391b4865c/untitled481.ipynb), [2.3](https://colab.research.google.com/gist/Saduf2019/329d0c2421d3e1d14f98b5404042ac50/untitled485.ipynb)", "Can you please change your `keras` import to `tensorflow.keras` and try again.\r\n```python\r\n# TF 2.4\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow import keras\r\n...\r\nOutput:\r\n0.8170178 0.91674\r\n0.23980403 0.94668\r\n0.16789182 0.96192\r\n0.13041496 0.971\r\n0.09908046 0.97144\r\n0.07447701 0.97742\r\n0.059314076 0.98188\r\n0.04840622 0.98608\r\n0.04056797 0.9865\r\n0.037097402 0.98898\r\n0.031317815 0.99174\r\n0.022105625 0.9934\r\n0.019010812 0.99446\r\n0.015984952 0.99136\r\n0.012451912 0.99478\r\n0.012425626 0.998\r\n0.011714706 0.9949\r\n0.009512669 0.99492\r\n0.0075133997 0.9958\r\n0.0081367865 0.9977\r\n```\r\n", "Hello, \r\n\r\nI have tried what was recommended but the output is still behaving unexpectedly. When I ran the code updated with your recommendations, sometimes I get an output that is close to the original code (TF 1.x) but other times the output is significantly different (see example of an output below).  So I don't believe that the change of changing keras import to tensorflow.keras worked to solve the unexpected behavior. \r\n\r\n```\r\n0.9402422 0.9171\r\n0.23724595 0.94824\r\n0.15908638 0.9646\r\n0.11688811 0.97304\r\n0.08697565 0.97676\r\n0.06634963 0.9817\r\n0.052655857 0.9826\r\n0.043853503 0.98538\r\n0.041294258 0.9871\r\n0.035308436 0.99142\r\n0.024075007 0.9912\r\n0.016586887 0.99578\r\n0.013315758 0.9951\r\n0.015305804 0.99648\r\n0.010092826 0.99042\r\n0.009263903 0.99702\r\n0.0077375784 0.99706\r\n0.0074062524 0.99856\r\n0.0066001886 0.99648\r\n0.0050791865 0.99838\r\n```", "I got results on last epoch for multiple runs as :\r\n```python\r\nSEED = 123\r\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\r\nos.environ['PYTHONHASHSEED']=str(SEED)\r\nrandom.seed(SEED)\r\nnp.random.seed(SEED)\r\ntf.random.set_seed(SEED)\r\n\r\n#last epoch result on multiple runs:\r\n0.004884758 0.9988\r\n0.0038588147 0.99928\r\n0.0040086186 0.99872\r\n```\r\nThe inconsistency with precision `4` can be explained by gradient gating in `tf.compat.v1.train.AdamOptimizer`.\r\nSee https://github.com/NVIDIA/framework-determinism#gradient-gating to know more.", "After adding what was said before,\r\n```\r\nSEED = 123\r\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\r\nos.environ['PYTHONHASHSEED']=str(SEED)\r\nrandom.seed(SEED)\r\nnp.random.seed(SEED)\r\ntf.random.set_seed(SEED)\r\n```\r\n I still do not get consistent results when comparing the results from TF 1.x code and TF 2.x code, as we would expect. The results I get are \r\n \r\n ```\r\nTF 2.x\r\n 0.8727456 0.91456\r\n0.23801954 0.94734\r\n0.16402669 0.96284\r\n0.12854795 0.9697\r\n0.096198834 0.97468\r\n0.07094173 0.98196\r\n0.054136228 0.98686\r\n0.043054197 0.99002\r\n0.036161695 0.9898\r\n0.032770615 0.98578\r\n0.033984397 0.9837\r\n0.033502463 0.99002\r\n0.025235573 0.9934\r\n0.015609697 0.99556\r\n0.011150459 0.99752\r\n0.009088196 0.9979\r\n0.0058030323 0.99914\r\n0.0038966627 0.99862\r\n0.0031383259 0.99918\r\n0.0020442707 0.99934\r\n```\r\n\r\n```\r\nTF 1.x\r\n0.8388736 0.91824\r\n0.24215287 0.94656\r\n0.17105241 0.96026\r\n0.12800093 0.97012\r\n0.095361866 0.97358\r\n0.07280199 0.97786\r\n0.05656019 0.98174\r\n0.04541241 0.9866\r\n0.03805694 0.98696\r\n0.03796135 0.98726\r\n0.033143476 0.99358\r\n0.021085953 0.99454\r\n0.015692161 0.9952\r\n0.0122016715 0.99466\r\n0.010179433 0.99418\r\n0.009387239 0.99456\r\n0.009418501 0.99698\r\n0.007646892 0.99478\r\n0.007249856 0.9975\r\n0.009419065 0.99574\r\n```", "Was able to reproduce the issue and getting inconsistent results. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c4dcc9a7272e60c7766112e46f60ad31/untitled88.ipynb).Thanks!", "There will be floating point precision errors from Tensorflow 1.x to Tensorflow 2.x and it is not possible to get deterministic behavior from Tensorflow 1.x and Tensorflow 2.x results. The outputs will be almost close.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45585\">No</a>\n"]}, {"number": 45584, "title": "Autograph warning seems to lead to reduced performance", "body": "\r\n\r\n**System information**\r\n- I am using a toolbox called EEGNet and using my own script\r\n- OS Platform and distribution: macOS catalina version 10.15.7 (19H2)\r\n- TensorFlow installed from (source or binary) - i don't know how to check this\r\n- Tensorflow version 2.3.1\r\n- Python version: 3.7.4\r\n\r\n- GPU model and memory: Radeon Pro 560X 4GB Intel UHD graphics 630 1536MB, and memory = 16GB 2400MHz DDR4\r\n\r\n\r\n**Describe the current behavior**\r\nI am getting the warning:\r\n`WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd0e8162b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd0e8162b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause:`\r\n\r\nI get this warning somewhat sporadically. When I reopen Spyder the issue is gone until the second iteration in which I need to call it. I am also noticing that the performance as seen through validation accuracy is very much lower when this warning appears.\r\n\r\nI checked other posts and it looked like changing gast==0.2.2 fixed it. But not for me.\r\n\r\n**Describe the expected behavior**\r\nIdeally the warning shouldn't appear, as it does the very first time around when I reopen Spyder. And the validation accuracy should be higher\r\n\r\n**Standalone code to reproduce the issue**\r\nN/A\r\nI am using a module/toolbox called EEGNet.  https://github.com/vlawhern/arl-eegmodels\r\nbut this seems to be the line that causes the warning:\r\n`fittedModel = model.fit(X_train, Y_train, batch_size = 64, epochs = 300, \r\n                                    verbose = 2, validation_data=(X_validate, Y_validate),\r\n                                    callbacks=[checkpointer], class_weight = class_weights)`\r\n\r\n**Other info / logs**\r\n\r\nThis is when I set autograph to verbose, 3\r\n`INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076e1bb90>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>\r\n    args: (<tf.Tensor 'args_0:0' shape=(150,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>\r\n    args: (<tf.Tensor 'args_0:0' shape=(150,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0a290>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(150, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(150, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(150,) dtype=int64>))\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(150, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(150, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(150,) dtype=int64>))\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd076f0a680>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7fd076f0a7a0>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>\r\n    args: (<tf.Tensor 'args_0:0' shape=(75,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>\r\n    args: (<tf.Tensor 'args_0:0' shape=(75,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fd076f0ab90>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(75, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(75, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(75,) dtype=int64>))\r\n    kwargs: {}\r\n\r\nConverted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(75, 64, 128, 1) dtype=float64>, <tf.Tensor 'args_2:0' shape=(75, 3) dtype=float32>, <tf.Tensor 'args_3:0' shape=(75,) dtype=int64>))\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>: DoNotConvert rule for tensorflow\r\nWhitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7fd0778ff170>: DoNotConvert rule for tensorflow\r\nTrain on 150 samples, validate on 75 samples\r\nEpoch 1/300\r\nINFO:tensorflow:Converted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nConverted call: <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>\r\n    args: ()\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Cache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> key <code object initialize_variables at 0x7fd17c1c40c0, file \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7fd077f74d90>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmpaqdi9bka)\r\nCache hit for entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> key <code object initialize_variables at 0x7fd17c1c40c0, file \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 603> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7fd077f74d90>, frozenset({'initializer_map'})): _ConvertedEntityFactoryInfo(tf__initialize_variables in tmpaqdi9bka)\r\nINFO:tensorflow:Error transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nError transforming entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00>\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd077965b00> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 324, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError`\r\n\r\nCan you help me troubleshoot this?", "comments": ["@moonj94 \r\n\r\nCan you please provide colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "I don't have a colab link but here is my code snippet:\r\n`F1,D,F2,KL=8,2,16,9`\r\n`model = EEGNet(nb_classes = 3, Chans = 64, Samples = 128, \r\n                        dropoutRate = 0.5, kernLength = KL, F1 = F1, D = D, F2 = F2, \r\n                        dropoutType = 'Dropout')`\r\n`checkpointpath = '/tmp/checkpoint.h5`\r\n`checkpointer = ModelCheckpoint(filepath=checkpointpath, verbose=1,\r\n                                        save_best_only=True)\r\n            \r\n            ###############################################################################\r\n            # if the classification task was imbalanced (significantly more trials in one\r\n            # class versus the others) you can assign a weight to each class during \r\n            # optimization to balance it out. This data is approximately balanced so we \r\n            # don't need to do this, but is shown here for illustration/completeness. \r\n            ###############################################################################\r\n            \r\n            # the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\r\n            # the weights all to be 1\r\n            class_weights = {1:1, 2:1, 0:1}#, 3:1}\r\n            \r\n            ################################################################################\r\n            # fit the model. Due to very small sample sizes this can get\r\n            # pretty noisy run-to-run, but most runs should be comparable to xDAWN + \r\n            # Riemannian geometry classification (below)\r\n            ################################################################################\r\n#            tf.autograph.set_verbosity(3,True)\r\n            fittedModel = model.fit(X_train, Y_train, batch_size = 64, epochs = 200, \r\n                                    verbose = 2, validation_data=(X_validate, Y_validate),\r\n                                    callbacks=[checkpointer], class_weight = class_weights)\r\n            \r\n        #     load optimal weights\r\n            model.load_weights(checkpointpath)`\r\n\r\nYou're going to need to download EEGnet: https://github.com/vlawhern/arl-eegmodels\r\n\r\nAnd I'm afraid I can't share my data due to my workplace policy. But you can probably set X to a random array of 300 x 64 x 128.\r\n`numtrials=300\r\ny=random binary array 300x 3 (labels)\r\nX_train      = X[0:round(numtrials/2),]\r\n        Y_train      = y[0:round(numtrials/2)]\r\n        X_validate   = X[round(numtrials/2):round(numtrials/2)+round(numtrials/4),]\r\n        Y_validate   = y[round(numtrials/2):round(numtrials/2)+round(numtrials/4)]\r\n        X_test       = X[round(numtrials/2)+round(numtrials/4):,]\r\n        Y_test       = y[round(numtrials/2)+round(numtrials/4):]`", "When I open a fresh instance of spyder, this is the message I get:\r\n\r\n`020-12-11 11:17:24.488103: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-11 11:17:24.488508: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\r\nTrain on 150 samples, validate on 75 samples\r\nEpoch 1/300`\r\n\r\nBut after I do control+c to stop the code, it gives me the warnings stated above.", "Hi @moonj94 you said that you see the warnings when you do control+c. What happens if you don't stop the code? Does it run?\r\n\r\nAdditionally, please provide your code with the proper indentation and as close to reproducible as possible. This helps us to troubleshoot and debug quicker. Thanks.", "Yeah so it runs properly if I don't do control+c.", "So to summarize, you do not see this warning every time you run the code? It only occurs sometimes. And when you don't see the warning, the code runs properly. Is that correct?\r\n\r\nAdditionally, please provide your code with the proper indentation and as close to reproducible as possible. Without this, it is difficult for us to troubleshoot.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45584\">No</a>\n"]}, {"number": 45583, "title": "[ROCm] Updates required to add ROCm support for JAX", "body": "This PR is a collection of commits required to add ROCm support for JAX.   Based on the work done by @inailuig \r\n\r\nsee individual commit message for details.\r\n\r\n---------------------------------\r\n\r\n\r\n/cc @hawkinsp @cheshire @chsigg @nvining-work @whchung ", "comments": []}, {"number": 45582, "title": "Layers with non-zero gradients are not updated with optimizer.apply_gradients()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):\r\nTested with both 2.2.0 and a recent 2.5.0 nightly build\r\nv1.12.1-47149-g0939f0b7a8a 2.5.0-dev20201208\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.1.243, CUDNN 7.6.5\r\n- GPU model and memory: GTX 1070Ti\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIn the following example, loading a second Keras model causes a situation where layers of the map_model are not updated with optimizer.apply_gradients() even though the gradients are non-zero. This is a silent failure, so I have added a callback called WeightChecker to demonstrate the issue. Otherwise, this code is meant to closely follow this (working) example: https://keras.io/examples/generative/dcgan_overriding_train_step/\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\nclass WeightChecker:\r\n    \"\"\"Automated health checks for training Keras models.\"\"\"\r\n    def __init__(self, model):\r\n        self.initial_model = model\r\n        self.var_names = [var.name for var in model.trainable_variables]\r\n        self.prev_weights = model.get_weights()\r\n\r\n    def check_epoch(self, model):\r\n        \"\"\"Checks to run at the end of an epoch\"\"\"\r\n        self.check_untrained_params(model)\r\n\r\n    def check_untrained_params(self, model):\r\n        \"\"\"Compare self.model.trainable_variables to self.prev_weights\"\"\"\r\n        passed = True\r\n        curr_weights = model.get_weights()\r\n        for curr_var, prev_var, var_name in zip(curr_weights, self.prev_weights, self.var_names):\r\n            eq = np.equal(curr_var, prev_var).all()\r\n            if eq:\r\n                passed = False\r\n                print(f\"\\nWarning: Variable {var_name} was not updated with training. \"\r\n                      f\"Confirm that this layer is correctly \"\r\n                      f\"connected to the computation graph.\")\r\n        self.prev_weights = [w.copy() for w in curr_weights]\r\n        return passed\r\n\r\n\r\nclass WeightCheckerCallback(keras.callbacks.Callback):\r\n    \"\"\"Check model initialization and run training checks.\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.weight_check = None\r\n\r\n    def setup_weight_checker(\r\n            self,\r\n            model: keras.Model = None):\r\n        \"\"\"Initialize the callback with an input_batch and targets.\"\"\"\r\n        self.weight_check = WeightChecker(model)\r\n\r\n    def on_train_begin(self, logs=None):\r\n        if self.weight_check is None:\r\n            raise ValueError(\"setup_weight_checker() must be called to use WeightCheckerCallback.\")\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        self.weight_check.check_epoch(self.model)\r\n\r\n\r\ndef data_gen():\r\n    \"\"\"Generate random data for training.\"\"\"\r\n    data = (np.random.random((audio_len, 1)).astype(np.float32),\r\n            np.random.random((audio_len, 1)).astype(np.float32))\r\n    while True:\r\n        yield data\r\n\r\n\r\nbatch_size = 64\r\naudio_len = 16000\r\nsteps_per_epoch = 10\r\ndataset = tf.data.Dataset.from_generator(data_gen,\r\n                                         (tf.float32, tf.float32),\r\n                                         (tf.TensorShape((audio_len, 1)), tf.TensorShape((audio_len, 1))))\r\ndataset = dataset.batch(batch_size)\r\n\r\nmap_model = tf.keras.Sequential([tf.keras.layers.Conv1D(\r\n        64, 3, padding='same'\r\n    ),\r\n    tf.keras.layers.Conv1D(\r\n        1, 3, padding='same'\r\n    )])\r\nmap_model(np.random.random((batch_size, audio_len, 1)))\r\n\r\naux_model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\naux_model.trainable = False\r\naux_model(np.random.random((batch_size, 1)))\r\n\r\n\r\nclass MainModel(tf.keras.Model):\r\n    \"\"\"Main Model.\"\"\"\r\n    def __init__(self, map_model, aux_model):\r\n        super().__init__()\r\n        self.feature_dim = 128\r\n        self.aux_model = aux_model\r\n        self.map_model = map_model\r\n        self.global_step = 0\r\n\r\n    def call(self, inputs, training=True):\r\n        output = self.map_model(inputs)\r\n        return output\r\n\r\n    def train_step(self, data):\r\n        mixed_audio = data[0]\r\n        clean_audio = data[1]\r\n\r\n        with tf.GradientTape() as tape:\r\n            decoded_audio = self.map_model(mixed_audio)\r\n            total_loss = tf.reduce_mean(tf.abs(clean_audio - decoded_audio))\r\n\r\n        grads = tape.gradient(total_loss, self.trainable_variables)\r\n        [tf.print(f'Gradient std for {tv.name}: '\r\n                  f'{np.std(g.numpy())}')\r\n         for (g, tv) in zip(grads, self.trainable_variables)\r\n         if 'conv1d/' in tv.name]\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n\r\n        losses = {\r\n            'loss': total_loss,\r\n        }\r\n        # tf.summary.scalar('loss', total_loss, step=self.global_step)\r\n        self.global_step += 1\r\n        return losses\r\n\r\n\r\nepochs = 5\r\n\r\nmodel = MainModel(map_model, aux_model)\r\nmodel.compile(\r\n    loss='mae',\r\n    optimizer='adam',\r\n    run_eagerly=True\r\n)\r\n\r\nweight_checker = WeightCheckerCallback()\r\nweight_checker.setup_weight_checker(model)\r\n\r\nmodel.fit(\r\n    dataset,\r\n    epochs=epochs,\r\n    callbacks=[weight_checker],\r\n    steps_per_epoch=steps_per_epoch\r\n)\r\n```\r\n\r\nRunning this code demonstrates that the gradients for conv1d are non-zero,\r\n```\r\nGradient std for sequential/conv1d/kernel:0: 0.003920636139810085\r\nGradient std for sequential/conv1d/bias:0: 0.00040249430458061397\r\n```\r\nbut the layer weights of conv1d are not updated\r\n```\r\nWarning: Variable sequential/conv1d/kernel:0 was not updated with training. Confirm that this layer is correctly connected to the computation graph.\r\nWarning: Variable sequential/conv1d/bias:0 was not updated with training. Confirm that this layer is correctly connected to the computation graph.\r\n```\r\nIn this case, the aux_model is not doing anything, it is only defined. However, the problem remains even if it is used in the computation.\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour can be seen by setting `aux_model = None` in the above code. When this is done, the conv1d layer updates as expected.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nSee problem description\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/253fc483f128e86c9b6dc3f3b8f886c8/45582-tf-nightly.ipynb). Thanks!", "Hi @QEDan, I think it's because `get_weights` returns all weights including trainable and non-trainable. You can validate this by printing the length of `trainable_variables` and `get_weights`. Therefore, your weight checker cannot pass because it examines the non-trainable variables.\r\n\r\nTo fix it, just extract trainable variables only in the weight checker.\r\n\r\nhttps://colab.research.google.com/drive/1ytXiosdXa-4SMCRETVcstq4ubS0GRU84?usp=sharing", "Ah, you are right. Thanks for looking into this. I'm sorry for the false alarm.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45582\">No</a>\n"]}, {"number": 45581, "title": "Remove llvm unavailable mirror in Bazel", "body": "", "comments": ["Ok It seems that the mirror is mandatory. Probably the mirror has just lags.", "Yes, the mirror is lagging recently. I'm trying to debug why Bazel stops pulling instead of trying the upstream.", "> Yes, the mirror is lagging recently. I'm trying to debug why Bazel stops pulling instead of trying the upstream.\r\n\r\nCause:\r\nhttps://github.com/tensorflow/tensorflow/blob/b9387225565226e5c8a497fc8b5e28ac832529c1/third_party/repo.bzl#L76-L84\r\n", "I meant this was for the mandatory of the tensorflow mirror for llvm cause it is not in the `SINGLE_URL_WHITELIST`"]}, {"number": 45580, "title": "Cannot load saved model when using multiple tf.keras.layers.experimental.preprocessing.StringLookup layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 and Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5 (Ubuntu) and 3.6.9 (Colab)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n`v2.3.0-54-gfcc4b966f1 2.3.1`\r\n\r\n**Describe the current behavior**\r\n\r\nI am unable to load a saved model that has multiple `tf.keras.layers.experimental.preprocessing.StringLookup` layers.\r\n\r\nWhen I load the model, I get the following error:\r\n```\r\nValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect to be able to load the model.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1BrlOjxYVyVM3lEJRmeK9lNvev8Z4MVwT?usp=sharing\r\n\r\nThe notebook does the following:\r\n1. Create a model with two `tf.keras.layers.experimental.preprocessing.StringLookup` layers.\r\n2. Save the model with `tf.saved_model.save` in the same Keras model\r\n3. Reload the model with `tf.saved_model.load`\r\n4. `ValueError` is thrown\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n- It works when using a single `StringLookup` layer.\r\n- I have observed the same problem with `tf.keras.layers.experimental.preprocessing.TextVectorization`.\r\n\r\nFull error stacktrace (from Colab): [error.txt](https://github.com/tensorflow/tensorflow/files/5674123/error.txt)", "comments": ["I tried your code with the latest `tf-nightly`, and it works perfectly. I also tried with `tensorflow==2.4.0rc4` and also works. I would suggest upgrading your TensorFlow version.", "@huberemanuel thanks for the quick response! This is great news, and I am a bit embarrassed that I did not try the release candidate before posting. I will close the issue and upgrade to the bleeding edge :+1: Thank you for your time.\r\n\r\n(I have verified now that it worked for my use case in 2.4.0 RC4)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45580\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45580\">No</a>\n"]}, {"number": 45578, "title": "\"\"TypeError: 'NoneType' object is not iterable\"\" when instantiating keras.models.Model() or keras.models.Sequential()", "body": "Whenever i try to instantiate a keras model either Model() or Sequential() i get **TypeError: 'NoneType' object is not iterable**. \r\nAs an example I've taken screenshots of a Sequential model i wanted to create on the Mnist data using PyCharm as follows.\r\n\r\n![capture-20201210-164722](https://user-images.githubusercontent.com/72258001/101787332-a4c33b00-3b07-11eb-8c4c-23ad30821723.png)\r\n\r\nAfter running i got the following error message, #Note the first 3 lines are not important as they're just saying that there is no GPU found on my laptop;;; \r\n\r\n![capture-20201210-164800](https://user-images.githubusercontent.com/72258001/101788484-d38de100-3b08-11eb-9708-fd66b50d495d.png)\r\n![capture-20201210-164815](https://user-images.githubusercontent.com/72258001/101788736-1ea7f400-3b09-11eb-8c8e-69d3b68ad83c.png)\r\n\r\nI think my error is emanating from line 9 `network = models.Sequential()`.\r\nI also think my imports are fine.  I have also tried running the same code in my jupyter notebook with the same apparent error. I also tried instantiating `model = models.Model()` from my notebook but with the same apparent error. \r\n\r\nThe following packages are the ones mainly involved in deep learning and their respective versions on my laptop;\r\n- python v3.7.7\r\n- tensorflow 2.3.1\r\n- keras 2.4.3\r\n\r\nPlease comment with anymore information you may need to assist me.\r\nThanks in advance.\r\n\r\n", "comments": ["Please copy paste your code and the stacktrace in the issue template with proper formatting.\r\nScreenshots do not help in reproducing the problem. Thanks!\r\nYou may try changing your `keras` imports to `tf.keras`;\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nnetwork = tf.keras.Sequential()\r\n...\r\n```", "Okay thanks @ymodak let me copy paste my code and the stacktrace here.\r\nAnd on this one I changed `keras` to `tf.keras` as you had previously advised;\r\n\r\n```import tensorflow as tf\r\nfrom keras.datasets import mnist\r\nfrom tensorflow.keras import layers\r\nfrom keras import utils\r\n\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\nnetwork = tf.keras.Sequential()\r\nnetwork.add(layers.Dense(512, activation=\"relu\", input_shape=(28 * 28,)))\r\nnetwork.add(layers.Dense(10, activation=\"softmax\"))\r\n\r\nnetwork.compile(optimizer=\"rmsprop\",\r\n                loss=\"categorical_crossentropy\",\r\n                metrics=[\"accuracy\"])\r\n\r\ntrain_images = train_images.reshape((60000, 28 * 28))\r\ntrain_images = train_images.astype(\"float32\") / 255\r\ntest_images = test_images.reshape((10000, 28 * 28))\r\ntest_images = test_images.astype(\"float32\") / 255\r\n\r\ntrain_labels = utils.to_categorical(train_labels)\r\ntest_labels = utils.to_categorical(test_labels)\r\n\r\nnetwork.fit(train_images, train_labels, epochs=5, batch_size=128)\r\n\r\n# # Check how the model does on our test data\r\ntest_loss, test_acc = network.evaluate(test_images, test_labels)\r\nprint(\"test_acc:\", test_acc)\r\n```\r\n\r\n**The stacktrace is as below on my notebook**\r\n\r\n\r\n```TypeError                                 Traceback (most recent call last)\r\n<ipython-input-42-6cc7110698f3> in <module>\r\n     5 (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n     6 \r\n----> 7 network = tf.keras.Sequential()\r\n      8 network.add(layers.Dense(512, activation=\"relu\", input_shape=(28 * 28,)))\r\n      9 network.add(layers.Dense(10, activation=\"softmax\"))\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py in __init__(self, layers, name)\r\n    115     # Skip the init in FunctionalModel since model doesn't have input/output yet\r\n    116     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n--> 117         name=name, autocast=False)\r\n    118     self.supports_masking = True\r\n    119     self._compute_output_and_mask_jointly = True\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in __init__(self, *args, **kwargs)\r\n    306     self._steps_per_execution = None\r\n    307 \r\n--> 308     self._init_batch_counters()\r\n    309     self._base_model_initialized = True\r\n    310     _keras_api_gauge.get_cell('model').set(True)\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _init_batch_counters(self)\r\n    315     # `evaluate`, and `predict`.\r\n    316     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\r\n--> 317     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    318     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    319     self._predict_counter = variables.Variable(\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n    260       return cls._variable_v1_call(*args, **kwargs)\r\n    261     elif cls is Variable:\r\n--> 262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n    254         synchronization=synchronization,\r\n    255         aggregation=aggregation,\r\n--> 256         shape=shape)\r\n    257 \r\n    258   def __call__(cls, *args, **kwargs):\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in <lambda>(**kws)\r\n    235                         shape=None):\r\n    236     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    239       previous_getter = _make_getter(getter, previous_getter)\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n   2644       synchronization=synchronization,\r\n   2645       aggregation=aggregation,\r\n-> 2646       shape=shape)\r\n   2647 \r\n   2648 \r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n    262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    265 \r\n    266 \r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1516           aggregation=aggregation,\r\n   1517           shape=shape,\r\n-> 1518           distribute_strategy=distribute_strategy)\r\n   1519 \r\n   1520   def _init_from_args(self,\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1650             initial_value = ops.convert_to_tensor(\r\n   1651                 initial_value() if init_from_fn else initial_value,\r\n-> 1652                 name=\"initial_value\", dtype=dtype)\r\n   1653           if shape is not None:\r\n   1654             if not initial_value.shape.is_compatible_with(shape):\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1497 \r\n   1498     if ret is None:\r\n-> 1499       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1500 \r\n   1501     if ret is NotImplemented:\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n     54 \r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    262   \"\"\"\r\n    263   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 264                         allow_broadcast=True)\r\n    265 \r\n    266 \r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    273       with trace.Trace(\"tf.constant\"):\r\n    274         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 275     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    276 \r\n    277   g = ops.get_default_graph()\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    298 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    299   \"\"\"Implementation of eager constant.\"\"\"\r\n--> 300   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    301   if shape is None:\r\n    302     return t\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     95     except AttributeError:\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n---> 97   ctx.ensure_initialized()\r\n     98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in ensure_initialized(self)\r\n    522       opts = pywrap_tfe.TFE_NewContextOptions()\r\n    523       try:\r\n--> 524         config_str = self.config.SerializeToString()\r\n    525         pywrap_tfe.TFE_ContextOptionsSetConfig(opts, config_str)\r\n    526         if self._device_policy is not None:\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in config(self)\r\n    904     \"\"\"Return the ConfigProto with all runtime deltas applied.\"\"\"\r\n    905     # Ensure physical devices have been discovered and config has been imported\r\n--> 906     self._initialize_physical_devices()\r\n    907 \r\n    908     config = config_pb2.ConfigProto()\r\n\r\nc:\\users\\danda\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in _initialize_physical_devices(self)\r\n   1225       self._physical_devices = [\r\n   1226           PhysicalDevice(name=d.decode(),\r\n-> 1227                          device_type=d.decode().split(\":\")[1]) for d in devs]\r\n   1228       self._physical_device_to_index = {\r\n   1229           p: i for i, p in enumerate(self._physical_devices)\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```", "Can you try setting up a [virtual env](https://www.jetbrains.com/help/pycharm/creating-virtual-environment.html) and run following lines;\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nnetwork = tf.keras.Sequential()\r\n```", "Unfortunately I got the same error even in a virtual environment\r\n\r\n![capture-20201211-095104](https://user-images.githubusercontent.com/72258001/101877333-82c4c980-3b96-11eb-8132-cbbf3eeacf9a.png)\r\n\r\nI thought the screenshot would be better to show my error rather than put in all the code like before since it's giving the exact same error.", "@KbeeArthur,\r\nCould you please update TensorFlow to v2.4 and check if you are facing the same issue. Thanks!", "@amahendrakar okay thanks let me do that and I'll get back to you just now. Thanks.", "Thanks guys for all your assistance on the issue. After @amahendrakar suggested i download tensorflow 2.4, i initially downloaded using `pip install --upgrade tensorflow==2.4 --user` but the code still did not run but i got a new error on `network = tf.keras.models.Sequential()` which I'm sorry but did not take note of it at that time. I then went to the tensorflow [website](https://www.tensorflow.org/install/pip#system-install) where i got the package location and then used my cmd again and this time used pip to download but with location.\r\n\r\nSo it's just now that I've figured I must have initially downloaded a tensorflow version for gpu whilst my laptop does not have one. So I downloaded the **Python 3.7 CPU-only** version from the website and everything now works just fine.\r\n\r\nThanks! :100: :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45578\">No</a>\n"]}, {"number": 45577, "title": "Tutorials still mention boosted_trees_classifier_train_in_memory", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/estimator/boosted_trees_model_understanding.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe tutorials says \r\n\r\n> For performance reasons, when your data fits in memory, we recommend use the boosted_trees_classifier_train_in_memory function. \r\n\r\nHowever, function `boosted_trees_classifier_train_in_memory` doesn't exist. It's actually removed in ffc25308ce2be84240ec90502b38800bc5a4dd60.\r\n\r\n### Submit a pull request?\r\n\r\nI'm not sure whether it's fine to simply remove the sentense. You guys may want to add something else.", "comments": ["@gqqnbig I think that there is a new arg `train_in_memory`at  https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier.\r\nCan you send a small PR to refactor that sentence?", "Closing this issue since the associated PR has merged and fixes it. Thanks!"]}, {"number": 45576, "title": "implement GetCurrentClockCycle() for s390x", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45576) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 45575, "title": "Error when building tensorflow from source", "body": "\r\n**System information**\r\n- OS Platform and Distribution: ubuntu 20.4\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0 & 2.3.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: from source, via bazel \r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: no GPU\r\n\r\n\r\nHi! I'm trying to install tf in ubuntu 20.4. Since for some reason AVX is not supported on ubuntu (I don't know why), the pip installation results in a core dump in python. So I tried installing from source via bazel 3.1.0. The bazel compilation is successful. However, when I build tensorflow via the following command:  bazel build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package, I get the following error:\r\n\r\nERROR: An error occurred during the fetch of repository 'eigen_archive':\r\njava.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz, https://gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz] to /home/rey/.cache/bazel/_bazel_rey/7a2ccf9885a6b6731b7d3780dd19d183/external/eigen_archive/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz: GET returned 406 Not Acceptable\r\n\r\nSo it's as if it's not finding the tar file on that address (https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz). I actually went to that address, and it works correctly. So I'm not sure why it cannot get it. Do you think I could just get them manually and put them in /.cache/bazel/... ? Any help would be appreciated. \r\n\r\nThanks,\r\nRey\r\n\r\n", "comments": ["Some more info: this ubuntu OS is on an Oracle virtual box. When I open that address (https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/386d809bde475c65b7940f290efe80e6a05878c4/eigen-386d809bde475c65b7940f290efe80e6a05878c4.tar.gz) on the Windows host, it works well. However on the ubuntu guest, it gives me this:\r\n\r\nDid Not Connect: Potential Security Issue\r\n\r\nFirefox detected a potential security threat and did not continue to storage.googleapis.com because this website requires a secure connection.\r\n\r\nWhat can you do about it?\r\n\r\nstorage.googleapis.com has a security policy called HTTP Strict Transport Security (HSTS), which means that Firefox can only connect to it securely. You can\u2019t add an exception to visit this site.\r\n\r\nThe issue is most likely with the website, and there is nothing you can do to resolve it.\r\n\r\nIf you are on a corporate network or using anti-virus software, you can reach out to the support teams for assistance. You can also notify the website\u2019s administrator about the problem.\r\n\r\n\r\nSo there is a problem with the connection to the website on ubuntu guest. Any idea how to solve it?\r\n\r\nThanks,\r\nRey", "\r\nMore details of the error:\r\n\r\nRROR: An error occurred during the fetch of repository 'icu':\r\n   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip, https://github.com/unicode-org/icu/archive/release-64-2.zip] to /home/rey/.cache/bazel/_bazel_rey/7a2ccf9885a6b6731b7d3780dd19d183/external/icu/release-64-2.zip: Checksum was 10cd92f1585c537d937ecbb587f6c3b36a5275c87feabe05d777a828677ec32f but wanted dfc62618aa4bd3ca14a3df548cd65fe393155edd213e49c39f3a30ccd618fc27\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/glennrp/libpng/archive/v1.6.37.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/pypi.python.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/googleapis/google-cloud-cpp/archive/v1.14.0.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/dmlc/dlpack/archive/3efc489b55385936531a06ff83425b719387ec63.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/36dbc057604f00aacfc0288ddad57e3b21cfc1b8.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/pasta/archive/v0.1.8.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/files.pythonhosted.org/packages/f3/af/4182184d3c338792894f34a62672919db7ca008c89abee9b564dd34d8029/astunparse-1.6.3.tar.gz failed: class javax.net.ssl.SSLHandshakeException PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\r\nERROR: /home/rey/bazel-3.1.0/tensorflow/tensorflow/tools/pip_package/BUILD:169:1: //tensorflow/tools/pip_package:licenses depends on @icu//:icu4c/LICENSE in repository @icu which failed to fetch. no such package '@icu//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip, https://github.com/unicode-org/icu/archive/release-64-2.zip] to /home/rey/.cache/bazel/_bazel_rey/7a2ccf9885a6b6731b7d3780dd19d183/external/icu/release-64-2.zip: Checksum was 10cd92f1585c537d937ecbb587f6c3b36a5275c87feabe05d777a828677ec32f but wanted dfc62618aa4bd3ca14a3df548cd65fe393155edd213e49c39f3a30ccd618fc27\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip, https://github.com/unicode-org/icu/archive/release-64-2.zip] to /home/rey/.cache/bazel/_bazel_rey/7a2ccf9885a6b6731b7d3780dd19d183/external/icu/release-64-2.zip: Checksum was 10cd92f1585c537d937ecbb587f6c3b36a5275c87feabe05d777a828677ec32f but wanted dfc62618aa4bd3ca14a3df548cd65fe393155edd213e49c39f3a30ccd618fc27\r\nINFO: Elapsed time: 36.445s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 428 targets configured)\r\n", "Hi @jediRey! \r\nWe are checking to see if you still need help in this issue , Have you checked this[ thread ](https://stackoverflow.com/questions/60864626/cannot-fetch-eigen-with-bazel-406-not-acceptable)yet ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45575\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45575\">No</a>\n"]}, {"number": 45574, "title": "TF 2.4rc3 fails with CUDNN_STATUS_INTERNAL_ERROR on GeForce RTX 3070", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker image tensorflow/tensorflow:2.4.0rc3-gpu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Docker binary\r\n- TensorFlow version (use command below): v2.4.0-rc2-20-g68f236364c 2.4.0-rc3\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA Version: 11.1\r\n- GPU model and memory: GeForce RTX 3070 computeCapability: 8.6 coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n\r\n**Describe the current behavior**\r\n\r\nTensorflow fails in initialization with CUDNN_STATUS_INTERNAL_ERROR.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow should start up and train convolutional network.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nMay be able to provide if useful. But it's a bit of work so please let me know if there is some trivial reason for this first.\r\n\r\n**Other info / logs**\r\n\r\nLog output:\r\n```\r\n2020-12-10 11:32:15.021193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-10 11:32:16.006649: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-10 11:32:16.007443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-12-10 11:32:16.065141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.065643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-10 11:32:16.065669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-10 11:32:16.067864: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-10 11:32:16.067966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-10 11:32:16.068967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-10 11:32:16.069211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-10 11:32:16.073175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-12-10 11:32:16.074124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-10 11:32:16.074283: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-10 11:32:16.074455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.074971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.075363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-10 11:32:16.076134: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-10 11:32:16.076244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.076736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-10 11:32:16.076808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-10 11:32:16.076855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-10 11:32:16.076878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-10 11:32:16.076897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-12-10 11:32:16.076917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-12-10 11:32:16.076938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-12-10 11:32:16.076959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-12-10 11:32:16.076975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-10 11:32:16.077076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.077578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.078120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-10 11:32:16.078276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-12-10 11:32:16.934447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-10 11:32:16.934503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-12-10 11:32:16.934513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-12-10 11:32:16.934781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.935233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.935641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-12-10 11:32:16.936014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7148 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6)\r\n2020-12-10 11:32:17.278390: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-12-10 11:32:17.298521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593615000 Hz\r\n2020-12-10 11:32:23.537921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-12-10 11:32:23.970919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-12-10 11:32:23.973136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-12-10 11:32:24.772020: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-12-10 11:32:24.786874: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 68, in <module>\r\n    train_step(batch)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node encoder/conv2d/Conv2D (defined at /code/model.py:101) ]] [Op:__inference_train_step_9520]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node encoder/conv2d/Conv2D:\r\n truediv (defined at /code/model.py:8)\r\n\r\nFunction call stack:\r\ntrain_step\r\n```\r\n\r\nFull environment:\r\n```\r\n== check python ===================================================\r\npython version: 3.6.9\r\npython branch: \r\npython build version: ('default', 'Oct  8 2020 12:12:24')\r\npython compiler version: GCC 8.4.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020\r\nos release version: 5.4.0-56-generic\r\nos platform: Linux-5.4.0-56-generic-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='e377002ed67b', release='5.4.0-56-generic', version='#62-Ubuntu SMP Mon Nov 23 19:20:19 UTC 2020', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                  1.19.4\r\nprotobuf               3.13.0\r\ntensorflow-estimator   2.4.0rc0\r\ntensorflow-gpu         2.4.0rc3\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.4.0-rc3\r\ntf.version.GIT_VERSION = v2.4.0-rc2-20-g68f236364c\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n        65:\tfind library=libc.so.6 [0]; searching\r\n        65:\t search path=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls:/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/tls/x86_64/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:/usr/local/nvidia/lib/tls/x86_64/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/tls/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/tls/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/cuda/lib64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/tls/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/tls/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/tls/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/tls/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/x86_64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n        65:\t  trying file=/usr/local/nvidia/lib64/libc.so.6\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n        65:\t\r\n        65:\tfind library=libpthread.so.0 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libpthread.so.0\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n        65:\t\r\n        65:\tfind library=libdl.so.2 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libdl.so.2\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n        65:\t\r\n        65:\tfind library=libutil.so.1 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libutil.so.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n        65:\t\r\n        65:\tfind library=libexpat.so.1 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libexpat.so.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libexpat.so.1\r\n        65:\t\r\n        65:\tfind library=libz.so.1 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libz.so.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libz.so.1\r\n        65:\t\r\n        65:\tfind library=libm.so.6 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libm.so.6\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libpthread.so.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libc.so.6\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libm.so.6\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libz.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libexpat.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n        65:\t\r\n        65:\t\r\n        65:\tinitialize program: /usr/local/bin/python\r\n        65:\t\r\n        65:\t\r\n        65:\ttransferring control: /usr/local/bin/python\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_opcode.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libffi.so.6 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libffi.so.6\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/usr/lib/x86_64-linux-gnu/libffi.so.6\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/x86_64-linux-gnu/libffi.so.6\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libtensorflow_framework.so.2 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls:/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/tls/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/tls/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../tls/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.2\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n        65:\t\r\n        65:\tfind library=librt.so.1 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/librt.so.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../librt.so.1\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/librt.so.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n        65:\t\r\n        65:\tfind library=libstdc++.so.6 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libstdc++.so.6\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libstdc++.so.6\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libstdc++.so.6\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n        65:\t\r\n        65:\tfind library=libgcc_s.so.1 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libgcc_s.so.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libgcc_s.so.1\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libgcc_s.so.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libgcc_s.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/librt.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n        65:\t\r\n        65:\tfind library=libcudart.so.11.0 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/tensorflow/python:/usr/local/lib/python3.6/dist-packages/tensorflow/python/..\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/libcudart.so.11.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/tensorflow/python/../libcudart.so.11.0\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libcudart.so.11.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/cuda/lib64/libcudart.so.11.0\r\n        65:\t\r\n2020-12-10 12:08:45.967347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n        65:\tfind library=libcrypto.so.1.1 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libcrypto.so.1.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/termios.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_csv.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libopenblasp-r0-ae94cfde.3.9.dev.so [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/tls/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/x86_64/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t\r\n        65:\tfind library=libgfortran-2e0d59d6.so.5.0.0 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0\r\n        65:\t\r\n        65:\tfind library=libquadmath-2d0c479f.so.0.0.0 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0\r\n        65:\t\r\n        65:\tfind library=libz-eb09ad1d.so.1.2.3 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libopenblasp-r0-ae94cfde.3.9.dev.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_tests.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libbz2.so.1.0 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libbz2.so.1.0\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libbz2.so.1.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libbz2.so.1.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=liblzma.so.5 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/liblzma.so.5\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/liblzma.so.5\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/liblzma.so.5\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libmpdec.so.2 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libmpdec.so.2\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/fft/_pocketfft_internal.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/bit_generator.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_common.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_bounded_integers.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_mt19937.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_philox.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_pcg64.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_sfc64.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/numpy/random/_generator.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfe.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_session.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_tf_stack.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/wrapt/_wrappers.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_utils.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_exception_registry.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_bfloat16.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_dtypes.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/fast_tensor_util.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_op_def_registry.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_events_writer.so\r\n        65:\t\r\n        65:\tfind library=libuuid.so.1 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libuuid.so.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/lib/x86_64-linux-gnu/libuuid.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /lib/x86_64-linux-gnu/libuuid.so.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_file_io.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_record_io.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_checkpoint_reader.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_parallel_device.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensor_float_32_execution.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_func.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/service/_pywrap_server_lib.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_device_lib.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_multiprocessing.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libssl.so.1.1 [0]; searching\r\n        65:\t search path=/usr/local/cuda/lib64\t\t(LD_LIBRARY_PATH)\r\n        65:\t  trying file=/usr/local/cuda/lib64/libssl.so.1.1\r\n        65:\t search cache=/etc/ld.so.cache\r\n        65:\t  trying file=/usr/lib/x86_64-linux-gnu/libssl.so.1.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/x86_64-linux-gnu/libssl.so.1.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_uarray/_uarray.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/fft/_pocketfft/pypocketfft.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_flow.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_matching.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libhdf5-beac1db3.so.103.0.0 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/x86_64/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/x86_64/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/tls/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/x86_64/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/x86_64/libhdf5-beac1db3.so.103.0.0\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0\r\n        65:\t\r\n        65:\tfind library=libhdf5_hl-db841637.so.100.1.1 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n        65:\t\r\n        65:\tfind library=libsz-1c7dd0cf.so.2.0.1 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.6/dist-packages/h5py/.libs/.\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n        65:\t\r\n        65:\tfind library=libaec-2147abcd.so.0.0.4 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/.\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n        65:\t\r\n        65:\tfind library=libz-a147dcb0.so.1.2.3 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/h5py/.libs/.\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        82:\tfind library=libc.so.6 [0]; searching\r\n        82:\t search path=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64:/usr/local/cuda/extras/CUPTI/lib64/tls:/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64/x86_64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/tls/x86_64/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:/usr/local/nvidia/lib/tls/x86_64/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64\t\t(LD_LIBRARY_PATH)\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/tls/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/extras/CUPTI/lib64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/tls/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/tls/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/cuda/lib64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/tls/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/tls/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/tls/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/tls/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/x86_64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n        82:\t  trying file=/usr/local/nvidia/lib64/libc.so.6\r\n        82:\t search cache=/etc/ld.so.cache\r\n        82:\t  trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n        82:\t\r\n        82:\t\r\n        82:\tcalling init: /lib/x86_64-linux-gnu/libc.so.6\r\n        82:\t\r\n        82:\t\r\n        82:\tinitialize program: /bin/sh\r\n        82:\t\r\n        82:\t\r\n        82:\ttransferring control: /bin/sh\r\n        82:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5pl.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_optimizer.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_cluster.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfprof.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_quantize_training.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_stacktrace_handler.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_util_port.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_debug_events_writer.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_mlir.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_python_op_gen.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_toco_api.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\tfind library=libopenblasp-r0-085ca80a.3.9.so [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64:/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/x86_64/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/x86_64/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/tls/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/x86_64/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/x86_64/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t\r\n        65:\tfind library=libgfortran-ed201abd.so.3.0.0 [0]; searching\r\n        65:\t search path=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs\t\t(RPATH from file /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so)\r\n        65:\t  trying file=/usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libgfortran-ed201abd.so.3.0.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libgfortran-ed201abd.so.3.0.0\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libopenblasp-r0-085ca80a.3.9.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs_cxx.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/specfun.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_comb.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/special/_ellip_harm_2.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling init: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/bin/python [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_opcode.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/x86_64-linux-gnu/libffi.so.6 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/cuda/lib64/libcudart.so.11.0 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/termios.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_csv.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_tests.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libopenblasp-r0-ae94cfde.3.9.dev.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/fft/_pocketfft_internal.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/bit_generator.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_common.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_bounded_integers.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_mt19937.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_philox.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_pcg64.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_sfc64.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/_generator.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfe.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_session.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_tf_stack.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/wrapt/_wrappers.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_utils.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_exception_registry.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_bfloat16.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_dtypes.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_traceme.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/fast_tensor_util.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_op_def_registry.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_events_writer.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_file_io.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/internal/_pywrap_profiler.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_record_io.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_checkpoint_reader.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_parallel_device.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensor_float_32_execution.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_py_func.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/service/_pywrap_server_lib.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_device_lib.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_multiprocessing.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/x86_64-linux-gnu/libssl.so.1.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/_lib/_uarray/_uarray.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/fft/_pocketfft/pypocketfft.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_flow.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_matching.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5pl.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-beac1db3.so.103.0.0 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_optimizer.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tf_cluster.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfprof.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_quantize_training.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_stacktrace_handler.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_util_port.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_debug_events_writer.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_mlir.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_python_op_gen.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_toco_api.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_ufuncs_cxx.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/specfun.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_comb.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/_ellip_harm_2.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libopenblasp-r0-085ca80a.3.9.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/special/../../scipy.libs/libgfortran-ed201abd.so.3.0.0 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n        65:\t\r\n        65:\t\r\n        65:\tcalling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]\r\n        65:\t\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Dec 10 12:08:47 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.38       Driver Version: 455.38       CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3070    Off  | 00000000:08:00.0 Off |                  N/A |\r\n|  0%   41C    P8    19W / 270W |     68MiB /  7979MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 9, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n\r\n```", "comments": ["@bjornsing,\r\nPlease try setting a hard limit on the total GPU memory as per [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if it works.\r\n\r\nAlso, please go through similar issues [#24828](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-464910864), [#40646](https://github.com/tensorflow/tensorflow/issues/40646#issuecomment-647224203) and check if it helps. Thanks!", "Thanks @amahendrakar,\r\n\r\nI could get it to run by limiting the total GPU memory.\r\n\r\nStill getting annoying warnings:\r\n```\r\n2020-12-10 15:48:00.345917: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\n2020-12-10 15:48:00.401440: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\n2020-12-10 15:48:00.453867: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\n2020-12-10 15:48:00.509598: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\n2020-12-10 15:48:00.557828: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\n...\r\n```\r\n\r\nBut that's not the end of the world. Which docker image would you recommend by the way for TF 2.X.X on GeForce RTX 3700?\r\n\r\nCheers,\r\n\r\nBj\u00f6rn", "@bjornsing,\r\n> Still getting annoying warnings:\r\n\r\nYou can suppress the warnings by changing the log level at the start of the program as shown below.  \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\n> Which docker image would you recommend by the way for TF 2.X.X on GeForce RTX 3700\r\n\r\nI'd recommend the latest stable release TensorFlow v2.4, which is compatible with CUDA 11 (not 11.1) and cuDNN 8. \r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45574\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45574\">No</a>\n"]}, {"number": 45573, "title": "save_model/load_model pair does not preserve model's metric", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian testing\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nAfter saving and then loading a model it produces different metric results.\r\n\r\n**Describe the expected behavior**\r\nEverything should be the same \r\n\r\n**Standalone code to reproduce the issue**\r\nPlease see a colab notebook [here](https://colab.research.google.com/drive/1-vUEU1gFf7GRrhy95E-h6xw_UB80yDSB?usp=sharing)\r\n\r\nNote that the metric of the loaded model is `accuracy: 1.0`, whereas original model produced `accuracy: 0.125'\r\n", "comments": ["Please note also that in this case I do get same loss for both models, however, in my original (more complex) case even the losses are different. ", "I think it could be related to https://github.com/tensorflow/tensorflow/pull/41701.\r\nTo verify can you pass an explicit metric name e.g. `metrics=[\"sparse_categorical_accuracy\"]` or another explicit one in your colab?", "@bhack \r\nIt is explicit already ('metic'), do you want to change it to another one?\r\n", "As mentioned in the reference ticket I meant \"explicit\" string in the context of:\n\n> When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well.\n\nSo to pass an explicit metric strings not an automatically selected like \"accurancy\" or \"acc\".", "I updated the  [notebook](https://colab.research.google.com/drive/1-vUEU1gFf7GRrhy95E-h6xw_UB80yDSB?usp=sharing) so that now it demonstrated two nets: one with implicit metrics (`metrics`=[\"acc\"]) and one with explicit metrics (`metrics=[\"sparse_categorical_accuracy\"]`). \r\n\r\nThis time, the behavior is even stranger: \r\n1. implicit metrics model produces same loss, different accuracy after saving and loading (**but both the loss and accuracy are wrong**)\r\n2. explicit metrics model produces same loss, same accuracy after saving and loading (**but both the loss and accuracy are wrong**) ", "What do you mean that both loss and accurancy are wrong?", "Also try your colab with:\n\n`pip\u00a0install\u00a0tf-nightly`\n\nAnd `tf.random.set_seed(1234)` just after your import", "@bhack .\r\nPlease have a look at the notebook I posted. By \"wrong\" I meant that the numbers do not correspond to those printed during training (I know these may be different).\r\n", "> @bhack .\n> Please have a look at the notebook I posted. By \"wrong\" I meant that the numbers do not correspond to those printed during training (I know these may be different).\n> \n\nYes about this you cannot expect train/fit logs and evaluate with the same loss/accurancy values.\n\nThe old long thread that explain this is at https://github.com/keras-team/keras/issues/6977 and also in others Stackoverflow questions/Github issues\n\nOther than this can you try your colab with my previous feedback?", "In fact, I look at validation loss before and after save/reload -- I see no reason why these should be different.", "I am able to replicate the issue reported, please find the gist here for[ tf 2.3](https://colab.research.google.com/gist/Saduf2019/7ccec13796c777a682ace461be587c9b/untitled487.ipynb) and[ nightly](https://colab.research.google.com/gist/Saduf2019/9327e063a6bb9934944a8b9c2877bbda/untitled487.ipynb)", "Could you try compile your model after loading it back?", "See https://github.com/keras-team/keras/issues/14231", "1. According to my understanding, `load_model` compiles the model by default [documentaion](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)\r\n2. I tried to compile -- it does not help (as expected)\r\n3. The notebook is available to everyone, hence, I assume that everyone (including developers) can play with compilation, evaluation sets, etc. Am I getting this wrong?\r\n", "Was able to replicate the issue with TF 2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9f1214a0547dbed6dba81f4eb364d528/untitled301.ipynb) ..Thanks!", "@sushreebarsa I cannot reproduce with `tf.random.set_seed(1234)` in your colab", "@eli-osherovich \r\nLooks like this was resolved in recent TF versions. [Here](https://colab.research.google.com/gist/jvishnuvardhan/75fe1b92033caa59c6114900536e766c/save_load_metrics_bug.ipynb) is a gist for reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you.\r\n\r\nIf this was not resolved, can you please open the issue in keras-team/keras repo repo as keras development moved to that repo to focus mainly on Keras. Thanks!\r\n\r\n\r\n", "@jvishnuvardhan \r\nIs there a unit test that checks it? If not, this is just a coincidence.\r\n", "> @jvishnuvardhan \n> Is there a unit test that checks it? If not, this is just a coincidence.\n> \n\nIf you can extend a test with a PR to cover this it could be useful", "I will raise a PR to add a unit test. I am closing this issue as this was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45573\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45573\">No</a>\n"]}, {"number": 45572, "title": "A quantized tf code that contain tf.where op cannot be converted to tflite (operator of type Where is not yet implemented.)", "body": "**System information**\r\n- OS Platform and Distribution: ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 1.15.3\r\n\r\n\r\n**Text output from tflite_convert**\r\n\r\n```\r\n2020-12-10 11:34:41.253749: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 83 operators, 206 arrays (1 quantized)\r\n2020-12-10 11:34:41.277151: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Where for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nWhen converting a model to tflite, got the error above.\r\n\r\n- Added tf.where() op to the model code (as a custom output layer).\r\n- Model: DeepLabV3 (using MobileNetV2 backbone) - [official itensorflow inplementation](https://github.com/tensorflow/models/tree/master/research/deeplab).\r\n- Finetuned using Quantization-aware training.\r\n- Convertion to tflite was done using the [provided script](https://github.com/tensorflow/models/blob/master/research/deeplab/convert_to_tflite.py).\r\n\r\n\r\nWould appreciate any help.", "comments": ["@yairkit \r\n\r\nPlease, provide the colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "I saw the error log is coming from TOCO converter, which is being deprecated and TFLite has a significant conversion improvements based on a new MLIR converter in the recent TensorFlow Lite version. Could you try it with the recent TF version?", "@ravikyram here is the code for our extra network layer to the deeplab model that makes the conversion fail (added to [export_model.py file](https://github.com/tensorflow/models/blob/master/research/deeplab/export_model.py)):\r\n\r\n    def _calc_pixel_size(raw_predictions, class_num):\r\n        classes_img = tf.squeeze(raw_predictions, axis=0) #make a scalar\r\n        class_inidices = tf.where(tf.equal(classes_img, class_num))\r\n        yx_max = tf.math.reduce_max(class_inidices, axis=0, keepdims=True)\r\n        yx_min = tf.math.reduce_min(class_inidices, axis=0, keepdims=True)\r\n        x_max = tf.gather_nd(yx_max, [0, 1])\r\n        x_min = tf.gather_nd(yx_min, [0, 1])\r\n        pixel_size_tmp = tf.math.subtract(x_max, x_min, name='sub')\r\n        pixel_size = tf.math.add(pixel_size_tmp, 1, name='add1')\r\n        pixel_size = tf.expand_dims(pixel_size, 0)\r\n        return pixel_size\r\n\r\n    my_raw_predictions = predictions[common.OUTPUT_TYPE]\r\n    pix_size = _calc_pixel_size(my_raw_predictions, class_num=1)\r\n    pix_size = tf.identity(\r\n        tf.cast(pix_size, tf.int32), name=_OUTPUT_PIX_SIZE_NAME)\r\n\r\n--------\r\n\r\nAnd @abattery regarding your conversion suggestion, we are working with the [deeplab project](https://github.com/tensorflow/models/tree/master/research/deeplab) that if I understand correctly uses tensorflow 1.15.3.\r\nWe actually tried switching to tf2 and had a few difficulties so it would require a lot of work to try the new converter.. (and if possible we would prefer to stick to the same tensorflow version the project is written in)", "After exporting the model to a saved model, can you try the conversion with the recent TF version? You can use two TF versions, one for model making and one for TFLite conversion.", "@abattery Thanks for the super quick response! I'll check and get back to you", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@abattery \r\nConversion using tf2 got us a tflite file so that's progress :)\r\nBut when running inference in android code (`tfLiteInterpreter.runForMultipleInputsOutputs`) the app crashes with cpp error. Do you have an idea on how to proceed from here?\r\n\r\n```\r\n2020-12-23 11:52:56.783 8318-9151/com.sixoversix.algohelper A/libc: Fatal signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0x6efcb8a000 in tid 9151 (AsyncTask #1), pid 8318 (rsix.algohelper)\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: Build fingerprint: 'samsung/y2sxx/y2s:11/RP1A.200720.012/G985FXXU5CTKG:user/release-keys'\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: Revision: '22'\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: ABI: 'arm64'\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: Timestamp: 2020-12-23 11:52:56+0200\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: pid: 8318, tid: 9151, name: AsyncTask #1  >>> com.sixoversix.algohelper <<<\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: uid: 10260\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG: signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0x6efcb8a000\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG:     x0  7fff51a3000ccb0c  x1  0000000000000000  x2  0000000000000008  x3  0000000000000002\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG:     x4  0000006e7c8736c8  x5  0000006e7c873700  x6  0000000000000001  x7  0000006e7c8736c0\r\n2020-12-23 11:52:56.843 9162-9162/? A/DEBUG:     x8  000000000000e066  x9  0000000000000000  x10 0000000000000002  x11 0000000000000000\r\n2020-12-23 11:52:56.844 9162-9162/? A/DEBUG:     x12 0000000000000002  x13 0000000000000001  x14 0000000000007033  x15 7fffffffffffffff\r\n2020-12-23 11:52:56.844 9162-9162/? A/DEBUG:     x16 0000006ed93d3250  x17 0000007180b62400  x18 0000000000000000  x19 0000006e7c8736c0\r\n2020-12-23 11:52:56.844 9162-9162/? A/DEBUG:     x20 0000006efcb10c34  x21 0000006efcb19cd0  x22 0000000000000000  x23 0000006e7c873700\r\n2020-12-23 11:52:56.844 9162-9162/? A/DEBUG:     x24 0000000000000001  x25 0000006e7c873680  x26 0000006ed92adf54  x27 0000000000000002\r\n2020-12-23 11:52:56.844 9162-9162/? A/DEBUG:     x28 0000000000000001  x29 0000006ed9a42bf0\r\n2020-12-23 11:52:56.844 9162-9162/? A/DEBUG:     lr  0000006ed92ade14  sp  0000006ed9a42b90  pc  0000006ed92ade0c  pst 0000000060000000\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG: backtrace:\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #00 pc 000000000012ae0c  /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!libtensorflowlite_jni.so (offset 0xd1db000)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #01 pc 000000000012ac6c  /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!libtensorflowlite_jni.so (offset 0xd1db000)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #02 pc 0000000000129184  /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!libtensorflowlite_jni.so (offset 0xd1db000)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #03 pc 00000000001b269c  /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!libtensorflowlite_jni.so (offset 0xd1db000)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #04 pc 00000000001b546c  /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!libtensorflowlite_jni.so (offset 0xd1db000)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #05 pc 0000000000046738  /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!libtensorflowlite_jni.so (offset 0xd1db000) (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #06 pc 000000000013ded4  /apex/com.android.art/lib64/libart.so (art_quick_generic_jni_trampoline+148) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #07 pc 00000000001347e8  /apex/com.android.art/lib64/libart.so (art_quick_invoke_static_stub+568) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #08 pc 00000000001a9a94  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+228) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #09 pc 0000000000320b6c  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+376) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #10 pc 0000000000316e98  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+996) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #11 pc 00000000006875d8  /apex/com.android.art/lib64/libart.so (MterpInvokeStatic+548) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #12 pc 000000000012e994  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #13 pc 00000000004a32dc  [anon:dalvik-classes.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+156)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #14 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #15 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #16 pc 00000000004a28da  [anon:dalvik-classes.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk] (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #17 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #18 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #19 pc 00000000000431d2  [anon:dalvik-classes2.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!classes2.dex] (com.sixoversix.algohelper.networks.tflite.TFLiteModelWrapper.analyzeFrameImpl+198)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #20 pc 00000000006856d8  /apex/com.android.art/lib64/libart.so (MterpInvokeSuper+2468) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #21 pc 000000000012e894  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_super+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #22 pc 000000000004373e  [anon:dalvik-classes2.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!classes2.dex] (com.sixoversix.algohelper.networks.tflite.TflSegmentationModelWrapper.analyzeFrameImpl+10)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #23 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #24 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #25 pc 00000000000430e8  [anon:dalvik-classes2.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!classes2.dex] (com.sixoversix.algohelper.networks.tflite.TFLiteModelWrapper.analyzeImage)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #26 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #27 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #28 pc 000000000003ce5a  [anon:dalvik-classes2.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!classes2.dex] (com.sixoversix.algohelper.controller.analyzer.ImageAnalyzer$1.doInBackground+10)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #29 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #30 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #31 pc 000000000003cea0  [anon:dalvik-classes2.dex extracted in memory from /data/app/~~ATqhddi-i7yAYw3QIZRpQQ==/com.sixoversix.algohelper-VXvd1dKA3pROsGiqGmn-Yg==/base.apk!classes2.dex] (com.sixoversix.algohelper.controller.analyzer.ImageAnalyzer$1.doInBackground+4)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #32 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #33 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #34 pc 00000000003ddc88  /system/framework/framework.jar (offset 0x966000) (android.os.AsyncTask$3.call+40)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #35 pc 0000000000686358  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #36 pc 000000000012ea14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #37 pc 00000000001ef29a  /apex/com.android.art/javalib/core-oj.jar (java.util.concurrent.FutureTask.run+62)\r\n2020-12-23 11:52:57.015 9162-9162/? A/DEBUG:       #38 pc 0000000000686358  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #39 pc 000000000012ea14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #40 pc 00000000003ddec4  /system/framework/framework.jar (offset 0x966000) (android.os.AsyncTask$SerialExecutor$1.run+4)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #41 pc 0000000000686358  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #42 pc 000000000012ea14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #43 pc 00000000001fa0ea  /apex/com.android.art/javalib/core-oj.jar (java.util.concurrent.ThreadPoolExecutor.runWorker+158)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #44 pc 0000000000684920  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #45 pc 000000000012e814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #46 pc 00000000001f8eac  /apex/com.android.art/javalib/core-oj.jar (java.util.concurrent.ThreadPoolExecutor$Worker.run+4)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #47 pc 0000000000686358  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #48 pc 000000000012ea14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #49 pc 00000000000eb858  /apex/com.android.art/javalib/core-oj.jar (java.lang.Thread.run+8)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #50 pc 000000000030e494  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.13348431324070995132)+268) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #51 pc 0000000000673104  /apex/com.android.art/lib64/libart.so (artQuickToInterpreterBridge+780) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #52 pc 000000000013dff8  /apex/com.android.art/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #53 pc 0000000000134564  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #54 pc 00000000001a9a78  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #55 pc 000000000055c624  /apex/com.android.art/lib64/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*)+460) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #56 pc 00000000005abde4  /apex/com.android.art/lib64/libart.so (art::Thread::CreateCallback(void*)+1308) (BuildId: f55ff0120a5a7cdb5e5e027a7f38147d)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #57 pc 00000000000affe8  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) (BuildId: 5e6bce0c0e1a8980118c76eab62b5c17)\r\n2020-12-23 11:52:57.016 9162-9162/? A/DEBUG:       #58 pc 00000000000504cc  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (BuildId: 5e6bce0c0e1a8980118c76eab62b5c17)\r\n2020-12-23 11:52:57.045 1132-1223/? E/Watchdog: !@Sync: 16476 heap: 78 / 79 [2020-12-23 11:52:57.045] sdogWay: softdog\r\n2020-12-23 11:52:57.381 636-636/? E/tombstoned: Tombstone written to: /data/tombstones/tombstone_19\r\n\r\n```", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45572\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45572\">No</a>\n", "@ori-6over6 is it possible to share your model file to us for debugging purpose. Can you try the tf nightly version of the TFLite in the android application?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45572\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45572\">No</a>\n", "@abattery Pardon the late response, we've checked our options and for the time being we decided to do the extra logic outside of the model (avoid the extra network layer), so our problem is solved.\r\nThanks for the help anyway!"]}, {"number": 45571, "title": "NCCL version is always 2.7.3", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / cudnn7.6\r\n- GPU model and memory: Tesla V100-SXM3-32GB\r\n\r\n**Describe the current behavior**\r\nWhen I run [image classification model](https://github.com/tensorflow/models/tree/master/official/vision/image_classification) from models repository with the following command:\r\n\r\n```\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 mnist_main.py   --model_dir=modeldir   --data_dir=datadir   --train_epochs=10   --distribution_strategy=mirrored\r\n```\r\nI always get NCCL version as \"2.7.3+cudaCUDA_MAJOR.CUDA_MINOR\". The problem is that I don't have NCCL 2.7.3 but it still seems to be working because I can get ncclAllReduce calls with NCCL_DEBUG_SUBSYS=COLL. NCCL version of the machine that I am using is 2.7.8.   \r\n\r\nI also cannot intercept NCCL collectives such as ncclAllReduce with LD_PRELOAD trick. Does Tensorflow use a static library or something else because in normal case I should be able to intercept any nccl call with LD_PRELOAD. It works with PyTorch, C++ NCCL apps etc. \r\n\r\n**Describe the expected behavior**\r\nI expect to get NCCL version as 2.7.8\r\nI expect to be able to intercept NCCL functions with a preloaded library (LD_PRELOAD trick)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 mnist_main.py   --model_dir=modeldir   --data_dir=datadir   --train_epochs=10   --distribution_strategy=mirrored\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. \r\n\r\n```\r\n2020-12-10 09:16:28.794595: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-12-10 09:16:31.039310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-12-10 09:16:31.360781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:34:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:31.363748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:be:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:31.366700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties: \r\npciBusID: 0000:e0:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:31.369582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties: \r\npciBusID: 0000:e2:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:31.369617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-12-10 09:16:31.372516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-12-10 09:16:31.374946: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-12-10 09:16:31.375800: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-12-10 09:16:31.378188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-12-10 09:16:31.379703: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-12-10 09:16:31.384111: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-12-10 09:16:31.406596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-12-10 09:16:31.408615: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-10 09:16:31.437675: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz\r\n2020-12-10 09:16:31.437913: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5555595fabf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-10 09:16:31.437935: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-10 09:16:32.895096: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558f709a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-10 09:16:32.895130: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM3-32GB, Compute Capability 7.0\r\n2020-12-10 09:16:32.895136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM3-32GB, Compute Capability 7.0\r\n2020-12-10 09:16:32.895140: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM3-32GB, Compute Capability 7.0\r\n2020-12-10 09:16:32.895145: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM3-32GB, Compute Capability 7.0\r\n2020-12-10 09:16:32.898035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:34:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:32.900943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:be:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:32.903831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties: \r\npciBusID: 0000:e0:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:32.906747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties: \r\npciBusID: 0000:e2:00.0 name: Tesla V100-SXM3-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 913.62GiB/s\r\n2020-12-10 09:16:32.906777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-12-10 09:16:32.906803: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-12-10 09:16:32.906813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-12-10 09:16:32.906823: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-12-10 09:16:32.906832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-12-10 09:16:32.906853: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-12-10 09:16:32.906863: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-12-10 09:16:32.928663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-12-10 09:16:32.928717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-12-10 09:16:35.890042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-10 09:16:35.890086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 2 3 \r\n2020-12-10 09:16:35.890095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y Y Y \r\n2020-12-10 09:16:35.890101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N Y Y \r\n2020-12-10 09:16:35.890106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 2:   Y Y N Y \r\n2020-12-10 09:16:35.890112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 3:   Y Y Y N \r\n2020-12-10 09:16:35.904988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3070 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM3-32GB, pci bus id: 0000:34:00.0, compute capability: 7.0)\r\n2020-12-10 09:16:35.909256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30099 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM3-32GB, pci bus id: 0000:be:00.0, compute capability: 7.0)\r\n2020-12-10 09:16:35.913101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 30099 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM3-32GB, pci bus id: 0000:e0:00.0, compute capability: 7.0)\r\n2020-12-10 09:16:35.916761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 30099 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM3-32GB, pci bus id: 0000:e2:00.0, compute capability: 7.0)\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\r\nI1210 09:16:35.924412 23456247883584 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\r\nI1210 09:16:35.927785 23456247883584 dataset_info.py:362] Load dataset info from datadir/mnist/3.0.1\r\nI1210 09:16:35.930115 23456247883584 dataset_info.py:411] Field info.citation from disk and from code do not match. Keeping the one from code.\r\nI1210 09:16:35.930334 23456247883584 dataset_builder.py:528] Constructing tf.data.Dataset for split ['train', 'test'], from datadir/mnist/3.0.1\r\n2020-12-10 09:16:36.191140: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\n2020-12-10 09:16:36.191203: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 4 GPUs\r\n2020-12-10 09:16:36.192815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcupti.so.10.1\r\n2020-12-10 09:16:37.043438: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed\r\nEpoch 1/10\r\nWARNING:tensorflow:From /home/pakthar/.conda/envs/tfmodels/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nW1210 09:16:37.291702 23456247883584 deprecation.py:317] From /home/pakthar/.conda/envs/tfmodels/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nINFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\r\nI1210 09:16:37.575582 23456247883584 cross_device_ops.py:699] batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:37.790755 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:37.793543 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:37.797662 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:37.800141 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\r\nI1210 09:16:39.279846 23456247883584 cross_device_ops.py:699] batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:39.472889 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:39.475625 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:39.479524 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nI1210 09:16:39.482020 23456247883584 cross_device_ops.py:441] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n2020-12-10 09:16:40.931919: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-12-10 09:16:45.904130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\nNCCL version 2.7.3+cudaCUDA_MAJOR.CUDA_MINOR\r\ng001:35039:35203 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x1553c2000000 recvbuff 0x1553c2000000 count 1643370 datatype 7 op 0 root 0 comm 0x154f80001340 [nranks=4] stream 0x15506756bc20\r\ng001:35039:35205 [2] NCCL INFO AllReduce: opCount 0 sendbuff 0x1554192c6e00 recvbuff 0x1554192c6e00 count 1643370 datatype 7 op 0 root 0 comm 0x154f74001310 [nranks=4] stream 0x1550675b3630\r\ng001:35039:35204 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x155108000000 recvbuff 0x155108000000 count 1643370 datatype 7 op 0 root 0 comm 0x154f78001310 [nranks=4] stream 0x1550672ff1a0\r\ng001:35039:35206 [3] NCCL INFO AllReduce: opCount 0 sendbuff 0x155058000000 recvbuff 0x155058000000 count 1643370 datatype 7 op 0 root 0 comm 0x154f6c001310 [nranks=4] stream 0x15506756bca0\r\n```\r\n", "comments": ["Hi @mabdullahsoyturk, can you try running the code with tf-nightly and let me know what NCCL version you see?\r\n\r\nAnd to clarify, the code does run correctly even with the version mismatch?", "> Hi @mabdullahsoyturk, can you try running the code with tf-nightly and let me know what NCCL version you see?\r\n> \r\n> And to clarify, the code does run correctly even with the version mismatch?\r\n\r\nHi @nikitamaia, I did try running the code with  tf-nightly but apparently it requires cuda 11 and I have 10.1. \r\n\r\nYes, the code does run correctly even with the version mismatch. The weird thing is that if it does call the libnccl shared library functions, I should be able to intercept those calls with LD_PRELOAD but it doesn't seem to be working. That's why I am looking where tensorflow calls nccl functions. ", "Yes, Tensorflow uses a statically linked NCCL. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45571\">No</a>\n"]}, {"number": 45570, "title": "failed to compile tensorflow (V2.4_RC4) in debug mode", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19.117.bsk.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4_rc4\r\n- Python version:Python 3.7.3\r\n- Installed using virtualenv? pip? conda?: build from source \r\n- Bazel version (if compiling from source):bazel 3.1.0\r\n- GCC/Compiler version (if compiling from source):gcc (Debian 8.3.0-6) 8.3.0\r\n- CUDA/cuDNN version: Cuda compilation tools, release 11.1, V11.1.105\r\n- GPU model and memory: V100(32G), host memory>120GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to compile Tensorflow from source code, using the debug model, when I pass args to the bazel, it ends up with failure as below:\r\nERROR: /home/chengyang/src/tf-src/tensorflow_2.4_rc4/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.0' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/chengyang/.cache/bazel/_bazel_zhoudongyan.daniel/b440c4e0619e08e4030082228cc9dfe0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-8 \\\r\n    LD_LIBRARY_PATH=/opt/XXX/jdk/jre/lib/amd64/server:/opt/XXX/lib/native:/opt/XXX/lib/native/ufs:/opt/XXX/lib/native:/opt/XXX/lib/native:/opt/XXX/lib:/usr/local/cuda/compat:/usr/local/cuda/lib64: \\\r\n    PATH=/home/chengyang/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/home/chengyang/software/bazelisk:/usr/local/cuda/bin:/home/chengyang/system_op/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tige\r\nr/arnold/arnold_entrypoint/tools:/opt/XXX/bin:/opt/XXX/bin:/home/chengyang/.local/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_framework.so.2.4.0-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\n/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcublas_plugin.pic.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can n\r\not be used when making a shared object; recompile with -fPIC\r\n/usr/bin/ld: final link failed: nonrepresentable section on output\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/compiler/xla/tools:run_hlo_module failed to build\r\nINFO: Elapsed time: 111.012s, Critical Path: 93.19s\r\nINFO: 2174 processes: 2174 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n ```bazel build //tensorflow/compiler/xla/tools:run_hlo_module  --config=v2 --config=noaws --config=nogcp --config=nohdfs --config=cuda --compilation_mode=dbg --strip=never --copt=\"-g\" --cxxopt=\"-g\" --copt=\"-O0\" --cxxopt=\"-O0\"  --verbose_failures```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI have successfully compiled the source code with ```--compilation_mode=opt --strip=never --copt=\"-g\" --cxxopt=\"-g\"  --verbose_failures```,  however, it turns out to be that \"-O2\" is enabled, and I hope to pass \"-O0\" to track the execution with GDB. ", "comments": ["@NEWPLAN \r\nCan you please try with CUDA 11.0? TF 2.4 (and nightly) is built and tested against CUDA 11.0, not 11.1.", "> @NEWPLAN\r\n> Can you please try with CUDA 11.0? TF 2.4 (and nightly) is built and tested against CUDA 11.0, not 11.1.\r\n\r\nThanks for your reply, I would follow your suggestion to recompile it. Additionally, what are the expected arguments passing to the bazel?  That's how can I disable the \"-O2\" flag in debug mode? ", "@NEWPLAN  Have you tried with the `dbg` conf? See https://github.com/tensorflow/tensorflow/issues/27744", "> @NEWPLAN Have you tried with the `dbg` conf? See #27744\r\n\r\ndid not try all the approaches mentioned in the link, but compiling the src codes with ```dbg``` has been practiced. Still, got the same errors as above.", "As `--compilation_mode=opt --strip=never` Is already working for you have you tried the same with `dbg` instead of `opt`?", "> As `--compilation_mode=opt --strip=never` Is already working for you have you tried the same with `dbg` instead of `opt`?\r\n\r\nsure, when repalcing the ```opt``` with ```dbg```, the compiling is failed (as above), however, using the opt means the src code would be optimized by the compiler such as ```-O2```, which is conflicted with debugging the code step by step.", "Ok Just to be sure about the reproducibilty can you try in the devel image (you have already the source there) https://www.tensorflow.org/install/docker?hl=en?", "> Ok Just to be sure about the reproducibilty can you try in the devel image (you have already the source there) https://www.tensorflow.org/install/docker?hl=en?\r\n\r\nsure, thanks for your help, I would like to try it soon.", "Hi, I have tried the new command to compile the tensorflow in debug mode, the compile cmd is\r\n\" bazel build //tensorflow/tools/pip_package:build_pip_package  --config=v2 --config=noaws --config=nogcp --config=nohdfs --config=cuda --compilation_mode=dbg --strip=never --copt=\"-fPIC\" --cxxopt=\"-fPIC\"  --copt=\"-O0\" --cxxopt=\"-O0\" --dynamic_mode=off --verbose_failures --linkopt=-L/usr/local/cuda/lib64 --linkopt=-lcudart\" \r\n\r\nthe log is shown as follows.\r\n\"tf-src/tensorflow_2.4_rc4/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.0' failed (Exit 1): crosstool_wrapper_driver_is_no\r\nt_gcc failed: error executing command\r\n  (cd /home/tiger/.cache/bazel/_bazel_zhoudongyan.daniel/b440c4e0619e08e4030082228cc9dfe0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-8 \\\r\n    LD_LIBRARY_PATH=/opt/tiger/yarn_deploy/jdk/jre/lib/amd64/server:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native/ufs:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lzo/lib:/usr/local/cuda/compat:/usr/local/cuda/lib64: \\\r\n    PATH=/home/tiger/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/home/tiger/chengyang/software/bazelisk:/usr/local/cuda/bin:/home/tiger/system_op/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tiger/arnold/arnold_entrypoint/tools:/opt/tiger/yarn_deploy/hadoop/bin:/opt/tiger/yarn_deploy/hive/bin:/home/tiger/.local/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-dbg/bin/tensorflow/libtensorflow_framework.so.2.4.0-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\n/usr/bin/ld: bazel-out/k8-dbg/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_12\r\n0CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object; recompile with -fPIC\"", "> Hi, I have tried the new command to compile the tensorflow in debug mode, the compile cmd is\r\n> \" bazel build //tensorflow/tools/pip_package:build_pip_package --config=v2 --config=noaws --config=nogcp --config=nohdfs --config=cuda --compilation_mode=dbg --strip=never --copt=\"-fPIC\" --cxxopt=\"-fPIC\" --copt=\"-O0\" --cxxopt=\"-O0\" --dynamic_mode=off --verbose_failures --linkopt=-L/usr/local/cuda/lib64 --linkopt=-lcudart\"\r\n> \r\n> the log is shown as follows.\r\n> \"tf-src/tensorflow_2.4_rc4/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.0' failed (Exit 1): crosstool_wrapper_driver_is_no\r\n> t_gcc failed: error executing command\r\n> (cd /home/tiger/.cache/bazel/_bazel_zhoudongyan.daniel/b440c4e0619e08e4030082228cc9dfe0/execroot/org_tensorflow &&\r\n> exec env -\r\n> CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1\r\n> GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-8\r\n> LD_LIBRARY_PATH=/opt/tiger/yarn_deploy/jdk/jre/lib/amd64/server:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native/ufs:/opt/tiger/yarn_deploy/hadoop/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lib/native:/opt/tiger/yarn_deploy/hadoop_current/lzo/lib:/usr/local/cuda/compat:/usr/local/cuda/lib64:\r\n> PATH=/home/tiger/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/home/tiger/chengyang/software/bazelisk:/usr/local/cuda/bin:/home/tiger/system_op/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tiger/arnold/arnold_entrypoint/tools:/opt/tiger/yarn_deploy/hadoop/bin:/opt/tiger/yarn_deploy/hive/bin:/home/tiger/.local/bin\r\n> PWD=/proc/self/cwd\r\n> PYTHON_BIN_PATH=/usr/bin/python3\r\n> PYTHON_LIB_PATH=/usr/lib/python3/dist-packages\r\n> TF2_BEHAVIOR=1\r\n> TF_CONFIGURE_IOS=0\r\n> TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0\r\n> TF_NEED_CUDA=1\r\n> external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-dbg/bin/tensorflow/libtensorflow_framework.so.2.4.0-2.params)\r\n> Execution platform: @local_execution_config_platform//:platform\r\n> /usr/bin/ld: bazel-out/k8-dbg/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_12\r\n> 0CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object; recompile with -fPIC\"\r\n\r\nThanks for all of the help I have received. I just find a practical solution to my problem, i.e., compile the tensorflow with \"--per_file_copt\". I would like to share with the community and hope it works  as expected.\r\n```\r\nbazel build //tensorflow/compiler/xla/python:xla_client --per_file_copt=\"//tensorflow/compiler/.*\\.cc,-//tensorflow/stream_executor/.*\\.cc@-g,-O0\"  --per_file_copt=\"//tensorflow/stream_executor/.*\\.cc@-g,-O1\" --config=v2 --config=noaws --config=nogcp --config=nohdfs --config=cuda --compilation_mode=dbg --strip=never -c opt --copt=\"-m64\" --cxxopt=\"-m64\" --copt=\"-mcmodel=large\" --cxxopt=\"-mcmodel=large\" --config=avx2_linux --config=xla --verbose_failures\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45570\">No</a>\n"]}, {"number": 45569, "title": "Add RaggedTensor support to tf.stop_gradient and tf.reduce_logsumexp", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.0rc3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** Tensorflow prohibits use of tf.stop_gradient or tf.reduce_logsumexp with RaggedTensor. Other tf.math.reduce_XXX methods support RaggedTensor though.\r\n\r\n**Will this change the current api? How?** Should not.\r\n\r\n**Who will benefit with this feature?** In my case I was implementing gumbel-softmax with straight-through estimator for seq2seq with sampling from multiple positions in the same sequence. Straight-through estimator requires tf.stop_gradient call. Doing this with RaggedTensor would've been much more efficient.\r\n\r\n**Any Other info.**\r\n", "comments": ["I will see what we can do about adding these.  In the meantime, you could use something like this to stop gradients on ragged tensors:\r\n\r\n```python\r\ndef stop_ragged_gradient(rt):\r\n  return rt.with_flat_values(tf.stop_gradient(rt.flat_values))\r\n```\r\n\r\nAnd for logexpsum, you could use something along the lines of:\r\n\r\n```python\r\ndef reduce_logsumexp(input_tensor, axis=None, keepdims=False):\r\n  raw_max = tf.reduce_max(input_tensor, axis, keepdims=True)\r\n  raw_max = tf.where(tf.math.is_finite(raw_max), raw_max, tf.zeros([], raw_max.dtype))\r\n  if isinstance(raw_max, tf.RaggedTensor):\r\n    raw_max = raw_max.with_flat_values(tf.stop_gradient(raw_max.flat_values))\r\n  else:\r\n    raw_max = tf.stop_gradient(raw_max)\r\n  result = raw_max + tf.math.log(\r\n      tf.reduce_sum(tf.math.exp(input_tensor - raw_max),\r\n                    axis=axis, keepdims=True))\r\n  if not keepdims:\r\n    result = tf.squeeze(result, axis)\r\n  return result\r\n```\r\n\r\n(c.f. the source code for tf.reduce_logsumexp [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L3122).)\r\n", "Thanks. I also found another way suitable for me - using common tf.boolean_mask and tf.scatter_nd.", "@ivodopyanov,\r\nCan you please confirm if we can close this issue. Thanks! ", "@rmothukuru I think supporting RaggedTensors across the board is a good idea anyway (according to https://www.tensorflow.org/guide/ragged_tensor, they are supported \"by more than a hundred TF ops\"), so I would not close the issue just because there is a workaround.", "Since making `tf.stop_gradient` work with ragged tensors appears to be trivial to do, but nonetheless presents a footgun to the end user, can we prioritize adding this functionality to `tf.stop_gradient`? I'm fine for now using this workaround, but it'd be nice if new users didn't have to google `\"tf stop_gradient ragged tensor\"` and find this issue, then copy-paste a function to make this work. This seems like an easy win!", "Thanks for raising the issue and the patience. \r\nI just made a fix for this issue. \r\nHopefully in a couple of days the `tf.stop_gradient` and `tf.reduce_logsumexp` of tf-nightly should support RaggedTensor. \r\nFeel free to re-open the ticket if the bug persists."]}, {"number": 45567, "title": "Update README.md", "body": "previous link shows only .gitignore which doesn't provide any information", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45567) for more info**.\n\n<!-- need_sender_cla -->", "@jsrimr  Can you please sign CLA. Thanks!", "> @googlebot I signed it\r\n", "> @jsrimr Can you please sign CLA. Thanks!\r\n\r\nI forgot to reply to the bot. Now I signed!", "Over to Khanh for review."]}, {"number": 45566, "title": "raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other)) ValueError: Shapes (2, 56, 56, 1024) and () are incompatible", "body": "Traceback (most recent call last):\r\n  File \"main.py\", line 191, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"main.py\", line 187, in main\r\n    run_training()\r\n  File \"main.py\", line 164, in run_training\r\n    loss_align_reg, vs_train_op, vs_eval_op, offset_pred, loss_reg = model.construct_model()\r\n  File \"C:\\Users\\User\\Desktop\\video search\\TALL-master\\ctrl_model.py\", line 167, in construct_model\r\n    sim_reg_mat, sim_reg_mat_test = self.visual_semantic_infer(self.visual_featmap_ph_train, self.sentence_ph_train, self.visual_featmap_ph_test, self.sentence_ph_test)\r\n  File \"C:\\Users\\User\\Desktop\\video search\\TALL-master\\ctrl_model.py\", line 78, in visual_semantic_infer\r\n    cross_modal_vec_train = self.cross_modal_comb(transformed_clip_train_norm, transformed_sentence_train_norm, self.batch_size)\r\n  File \"C:\\Users\\User\\Desktop\\video search\\TALL-master\\ctrl_model.py\", line 59, in cross_modal_comb\r\n    concat_feature = tf.reshape(tf.concat(2,[vv_feature, ss_feature]),[batch_size, batch_size, self.semantic_size+self.semantic_size])\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1297, in concat\r\n    tensor_shape.scalar())\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 1103, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (2, 56, 56, 1024) and () are incompatible\r\n\r\nCan anyone fix this issue?", "comments": ["@Malithi-gif \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "After updating TensorFlow it was fixed.\n\nThank you\n\nOn Thu, Dec 10, 2020 at 11:48 AM Saduf2019 <notifications@github.com> wrote:\n\n> @Malithi-gif <https://github.com/Malithi-gif>\n> We see that the issue template has not been filled, could you please do so\n> as it helps us analyse the issue [tf version, steps followed before you ran\n> into this error or stand alone code to reproduce the issue faced]\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45566#issuecomment-742268229>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANACA3CTIROCHPI4S47SLN3SUBR3LANCNFSM4UUPSZCA>\n> .\n>\n", "@Malithi-gif \r\nGlad the issue is resolved, please move this to closed status.", "Saduf2019 again it happens. Tensorflow Version: 2.3.1\r\n\r\n\r\n", "@Malithi-gif \r\nAs requested before please share information for us to analyse[ or if possible share  a colab gist with the error], can you upgrade tf and check if the issue persist.", "Saduf2019 I'm trying to run temporary activity localization code, written in Python and TensorFlow. Code is too long. I will share it.", "Please minimize code. Likely, as the error suggests, this is an error on your side.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45565, "title": "tf.SparseTensorSpec in input_signature of tf.function does not take effect", "body": "**Describe the current behavior**\r\nWhen we define a `tf.SparseTensorSpec` with some specific shape in the `input_signature` of `tf.function`, we still can pass a `tf.SparseTensor` with any arbitrary shape into the decorated function.\r\n\r\nRelated to #29198. For now, the shape of input sparse tensor is always `None` inside tf.function, which I think contradicts with the purpose of `input_signature`.\r\n\r\n**Describe the expected behavior**\r\nLike `tf.TensorSpec`, it should raise an error when the input `tf.SparseTensor` is not compatible with the `tf.SparseTensorSpec`.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee [this notebook](https://colab.research.google.com/drive/1ArjLQqS6VufIla_IN0JykHSPpYUEG7gW?usp=sharing).\r\n", "comments": ["@llan-ml \r\n\r\n have tried n colab with TF 2.3, nightly verson(`2.5.0-dev20201210`)  and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/7530ff249cd234b8d881ec726476cfac/untitled573.ipynb).Thanks!", "@llan-ml,\r\nAs this error is Originating because of  **`None Shape`** from **`Sparse Tensor`**, as you rightly specified in #29198, can you please confirm if we can close this issue to avoid redundancy, and track this issue in #29198? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45565\">No</a>\n"]}]