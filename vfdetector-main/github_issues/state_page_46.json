[{"number": 44481, "title": "TFlite Custom trained model Issue (IndexError: invalid index to scalar variable)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Raspbian\r\n- TensorFlow installed from (source or binary): TF\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\npython /content/models/research/object_detection/exporter_main_v2.py \\\r\n    --trained_checkpoint_dir training \\\r\n    --output_directory inference_graph \\\r\n    --pipeline_config_path training/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nOutput succeeded. (not sure what to put here)\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nModel: https://drive.google.com/drive/folders/17iEVFzQjqhI6zP4zUJP0uTAnBy2TnWPR?usp=sharing\r\nTFLiteModel :https://drive.google.com/drive/folders/1BCcWG-89OHts-510sev1VJepbDQkt9s0?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n\r\nI trained a SSD_MobilenetV2_FPN_680x680 to detect only black bears successfully and converted to tflite by following this guide: https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model.\r\n\r\nI then followed EdjeElectronics' guide on how to run tflite on Raspberry Pi: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md.\r\n\r\nBut, when I try to run TFLite_detection_webcam.py It gives an error:\r\n\r\npi@raspberrypi:~ $ source tflite1-env/bin/activate\r\nbash: tflite1-env/bin/activate: No such file or directory\r\npi@raspberrypi:~ $ cd tflite1\r\npi@raspberrypi:~/tflite1 $ source tflite1-env/bin/activate\r\n(tflite1-env) pi@raspberrypi:~/tflite1 $ python3 TFLite_detection_webcam.py --modeldir=Bear_Detector_TFLite_model\r\nTraceback (most recent call last):\r\n  File \"TFLite_detection_webcam.py\", line 185, in <module>\r\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\r\nIndexError: invalid index to scalar variable.\r\n\r\n\r\n\r\nI run his sample tflite model and it works.\r\nI'm guessing my the SSD_MobilenetV2_fpn model isnt compatible for Raspberry PI?\r\n\r\nIm sorry if this post is not in the correct place or if I'm missing key info for help. I'm pretty new to all this stuff.\r\n\r\n**Any other info / logs**\r\n\r\n**TFLite_detection_webcam.py**\r\nhttps://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_webcam.py\r\n\r\n", "comments": ["TF2 Detection models cannot be converted to TFLite directly. There is an intermediate step to generate a TFLite-friendly model, see [these instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md). To see a Python example of inference with such a model, see [this Colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb).", "> TF2 Detection models cannot be converted to TFLite directly. There is an intermediate step to generate a TFLite-friendly model, see [these instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md). To see a Python example of inference with such a model, see [this Colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb).\r\n\r\nYes, I used the recommended method. Im thinking maybe the problem is in how I exported the inference graph? Was this to be done while training? I stopped the training used checkpoint-0.", "That should still work. Can you post the exact commands you used?", "> > TF2 Detection models cannot be converted to TFLite directly. There is an intermediate step to generate a TFLite-friendly model, see [these instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md). To see a Python example of inference with such a model, see [this Colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb).\r\n> \r\n> Yes, I used the recommended method. Im thinking maybe the problem is in how I exported the inference graph? Was this to be done while training? I stopped the training used checkpoint-0.\r\n\r\nI have the same error and I also followed the same colab. \r\nI have also tried on Python3.9 and Python3.8. The same error persists.  \r\n\r\nI look at boxes, classes, and scores used for plot_detection(), and they have very different shapes: \r\nboxes [[0.68488944 0.06575927 0.0612663  0.03251293 0.0133186  0.01068375\r\n  0.01012135 0.01006672 0.00997183 0.00978112]]\r\nclasses [[[ 5.3712845e-01  3.4228128e-01  8.4705698e-01  4.7190589e-01]\r\n  [-7.0969015e-04  9.1059554e-01  1.6321224e-01  9.9637425e-01]\r\n  [ 1.7573684e-04  7.2726512e-01  1.7673787e-01  8.2486808e-01]\r\n  [ 7.1139622e-01  3.9257938e-01  8.1933236e-01  4.5051664e-01]\r\n  [-1.8495396e-03  1.0394566e-01  1.5567529e-01  4.1947818e-01]\r\n  [ 5.5734855e-01  3.3228156e-01  7.8419203e-01  4.5065954e-01]\r\n  [ 2.4230778e-02  3.4894028e-01  8.3006464e-02  4.3349090e-01]\r\n  [ 5.5638200e-01  3.4166583e-01  7.1462268e-01  5.3918886e-01]\r\n  [ 9.8083839e-03  3.1147084e-01  9.2084132e-02  4.3107238e-01]\r\n  [ 5.0306618e-03  4.6566434e-02  9.8932646e-02  1.9595167e-01]]]\r\nscores [10.]\r\n\r\nSo I tried to modify the data type, but come to a point that \"ValueError: too many values to unpack (expected 4)\" - for boxes, where having 8 values in the array.\r\n\r\nBtw, I run this on my local machine, with Jupiter lab. \r\n\r\nAny help? \r\n", "Any update on this issue? I have the same issue after converting into tflite.\r\n\r\n**_The command I used to convert the model to tflite friendly inference graph is,_**\r\n`!python object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path object_detection/inference_graph/pipeline.config \\\r\n    --trained_checkpoint_dir object_detection/inference_graph/checkpoint \\\r\n    --output_directory tflite`\r\n\r\n**_Then I used the tflite_convert,_**\r\n`!tflite_convert --saved_model_dir=/output/tflite/saved_model --output_file=/output/indoor-model.tflite`\r\n\r\n**_When converting to tflite the output logs are as follows,_** \r\n\r\n2021-09-17 05:06:53.045188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-17 05:06:53.057318: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\r\n2021-09-17 05:06:53.058332: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-09-17 05:06:53.644501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-17 05:07:08.670831: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\r\n2021-09-17 05:07:08.670892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\r\n2021-09-17 05:07:08.670914: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\r\n2021-09-17 05:07:08.672066: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /output/tflite/saved_model\r\n2021-09-17 05:07:08.773055: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\r\n2021-09-17 05:07:08.773203: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /output/tflite/saved_model\r\n2021-09-17 05:07:09.104013: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\r\n2021-09-17 05:07:09.720913: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: /output/tflite/saved_model\r\n2021-09-17 05:07:09.998334: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 1326264 microseconds.\r\n2021-09-17 05:07:10.801085: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-09-17 05:07:11.557933: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1899] Estimated count of arithmetic ops: 1.321 G  ops, equivalently 0.661 G  MACs\r\n\r\n**_When inferencing on the .tflite model the error I get is as follows._**\r\n/usr/local/lib/python3.7/dist-packages/object_detection/utils/visualization_utils.py in visualize_boxes_and_labels_on_image_array(image, boxes, classes, scores, category_index, instance_masks, instance_boundaries, keypoints, keypoint_scores, keypoint_edges, track_ids, use_normalized_coordinates, max_boxes_to_draw, min_score_thresh, agnostic_mode, line_thickness, mask_alpha, groundtruth_box_visualization_color, skip_boxes, skip_scores, skip_labels, skip_track_ids)\r\n\r\nIndexError: invalid index to a scalar variable.\r\n\r\nI trained several models about 4 months ago and converted them to tflite, and they worked without any issue. If there is any help regarding this issue I would much appreciate it. Thanks a lot!\r\n", "With some recent converter changes, it looks like the order of outputs from the detection TFLite models has changed. Can you print `interpreter.get_output_details()` in your inference code & see what you get?", "> With some recent converter changes, it looks like the order of outputs from the detection TFLite models has changed. Can you print `interpreter.get_output_details()` in your inference code & see what you get?\r\n\r\n**Upon**  `interpreter.get_output_details()`:\r\n\r\n[{'name': 'StatefulPartitionedCall:1', 'index': 335, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, \r\n\r\n{'name': 'StatefulPartitionedCall:3', 'index': 333, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, \r\n \r\n{'name': 'StatefulPartitionedCall:0', 'index': 336, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32),    'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, \r\n\r\n{'name': 'StatefulPartitionedCall:2', 'index': 334, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\n\r\n\r\n### Inspecting model outputs:\r\ncode taken from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb)\r\n```\r\nboxes = interpreter.get_tensor(output_details[0]['index'])\r\nclasses = interpreter.get_tensor(output_details[1]['index'])\r\nscores = interpreter.get_tensor(output_details[2]['index'])\r\n```\r\n\r\n**Upon printing these after** `interpreter.invoke()`, in the same order:     \r\n\r\n  [[0.23682949 0.07426468 0.06343907 0.03482482 0.03126174 0.0246169\r\n    0.022504   0.01281145 0.01089162 0.01080516]]              **//Boxes**\r\n\r\n  [[[ 5.42102814e-01  3.35466176e-01  8.34305406e-01  4.55916852e-01]\r\n    [-7.09682703e-04  9.10595536e-01  1.63212255e-01  9.96374249e-01]\r\n    [ 2.39851326e-03  7.29886234e-01  1.88307345e-01  8.24188054e-01]\r\n    [ 8.10296059e-01  4.75230813e-03  9.96720552e-01  1.48132056e-01]\r\n    [ 2.04751641e-03 -5.52492589e-03  1.29257411e-01  1.77513599e-01]\r\n    [ 7.14018226e-01  1.12694576e-02  9.92248535e-01  1.80131346e-01]\r\n    [ 8.67153883e-01  5.06623805e-01  9.98734474e-01  6.41331732e-01]\r\n    [ 8.61540318e-01  4.15937781e-01  1.00139987e+00  5.49726427e-01]\r\n    [-1.41584054e-02 -1.72429532e-02  1.37187302e-01  3.87391388e-01]\r\n    [-4.36818972e-02 -8.47322345e-02  1.20016895e-01  2.35268712e-01]]]        **//Classes**\r\n\r\n  [10.]     **//Scores**\r\n\r\n### After changing the order:\r\nBut even after taking the correct outputs for boxes,classes and scores i.e. \r\noutput_details[1]['index']        \r\noutput_details[0]['index']\r\noutput_details[3]['index']\r\n\r\nThe tflite model doesn't give the same inference as the regular tf mobilenet model.\r\n", "https://colab.research.google.com/drive/1lUOEUpbsAbjgz_P8hbv8bcolM-2VtUx7?usp=sharing\r\n\r\nThis notebook contains the code taken from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb), and contains everything from Data preparation & visualization to inference using the base tf model, conversion to tflite and the failed inference from the tflite model; even after the above index changes to the output_details list.\r\n\r\nAs of Nov 19 '21, Everything works in the notebook till the last tflite inference section (which has no matching predictions with the base model). \r\nBut before running through the cells in the notebook download cudnn 8.1 and upload it to my drive of your google drive (The notebook handles its installation from there), to avoid the dreaded 'failed to fetch convolutional algorithm ' error near the model building part(checkpoint creation).  See https://github.com/tensorflow/models/issues/10360#issuecomment-965560460\r\n\r\nAlso do run the upgrade pip code cell, to avoid tensorflow/models#10375 \r\n\r\nIf there's any help regarding this from the maintainers or anyone, i would greatly appreciate it. Thanks.", "@Faiz-hmed When you say that the TFLite model doesn't provide the 'same' inference, are the detections completely off, or just not exactly equal to TF's outputs?", "@srjoglekar246  They are completely off, absent in the case of the TFLite model on default test images provided in the 'models/research/object_detection/test_images/ducky/test/'  directory. The detections seen from the TF's model aren't seen at all in the TFLite model. \r\nWhile Running through the notebook, there was not a single detection on those images making up the gif.", "**FIX:** Rather than taking the classes at the 0th index of the output list, taking the 3rd one seemed to fix the issue.\r\n```\r\nboxes = interpreter.get_tensor(output_details[1]['index'])\r\nclasses = interpreter.get_tensor(output_details[3]['index'])\r\nscores = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\nAt first, printing all the outputs after `interpreter.invoke()` seemed to give a zero array at the 3rd index of the list.\r\nThen I did remember that we had a label_id_offset that was causing the values to go to zero.\r\n", "\r\nThank you so much for the nudges in the right direction, @srjoglekar246 ", "Np! Glad you got things to work", "Hi @ReyCoPeruvit ! We are checking to see whether you still need help in this issue or not.  Have you checked the above [comment ](https://github.com/tensorflow/tensorflow/issues/44481#issuecomment-974325834)yet ? Thanks!"]}, {"number": 44472, "title": "ByteBuffer is not a valid flatbuffer model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *Ubuntu 18.04.5 LTS [Google Colab]*\r\n- TensorFlow installed from (source or binary): *nightly*\r\n- TensorFlow version (or github SHA if from source): *2.5.0-dev20201029*\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nI am trying to covert custom efficientdet d2 object detection model that I trained for Android.\r\nI successfully trained it and also was able to convert it to tflite by using various github issue threads.\r\nI followed this [colab file](https://colab.research.google.com/gist/ravikyram/c2832e117a14c172c75146275381a8fc/untitled282.ipynb#scrollTo=U8oeT58f3qmr) to convert and was able to convert also used `tf_convert` and was able to convert\r\n\r\n```bash\r\n# did ran this after running export_tflite_graph_tf2.py\r\ntflite_convert --saved_model_dir /content/model_nightly/saved_model --output_file /content/tflite-conversion.tflite\r\n```\r\nand also\r\n```python\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/model_nightly/saved_model/',signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('/content/model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n**from command 1**\r\n```\r\nW1030 22:40:38.686931 139654416934784 function_deserialization.py:416] Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nW1030 22:40:38.687373 139654416934784 function_deserialization.py:416] Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nW1030 22:40:38.687821 139654416934784 function_deserialization.py:416] Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\n2020-10-30 22:40:56.738889: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n2020-10-30 22:40:56.738961: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\n2020-10-30 22:40:56.738973: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:325] Ignored change_concat_input_ranges.\r\n2020-10-30 22:40:56.740050: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: /content/model_graph_100K_nightly/saved_model\r\n2020-10-30 22:40:57.034144: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n2020-10-30 22:40:57.034227: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /content/model_graph_100K_nightly/saved_model\r\n2020-10-30 22:40:57.034311: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-30 22:40:57.901184: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2020-10-30 22:40:58.067181: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2020-10-30 22:40:58.215865: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\r\n2020-10-30 22:41:00.181931: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /content/model_graph_100K_nightly/saved_model\r\n2020-10-30 22:41:01.017615: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 4277564 microseconds.\r\n2020-10-30 22:41:04.164837: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2020-10-30 22:41:06.474280: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nException ignored in: <bound method Buckets.__del__ of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f037e83e798>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n```\r\n\r\n**from command 2**\r\n```\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:absl:Importing a function (__inference_EfficientDet-D2_layer_call_and_return_conditional_losses_131625) with ops with custom gradients. Will likely fail if a gradient is requested.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\n- While using it in the android application from the repository [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/README.md), I get the following error.\r\n```\r\nE/tensorflow: CameraActivity: Exception!\r\n    java.lang.RuntimeException: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:153)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7356)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\r\n     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:62)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:277)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:148)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\u00a0\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\u00a0\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\u00a0\r\n        at android.os.Looper.loop(Looper.java:214)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:7356)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\u00a0\r\n```\r\nI am trying to do this for my college project.", "comments": ["Hi Santosh,\r\n\r\nYou used SELECT_TF_OPS in the conversion, so in the app, it is necessary to add the dependency to gradle as well. Please check the instructions [here](https://www.tensorflow.org/lite/guide/ops_select). \r\n\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}\r\n```\r\n\r\nBTW, only for [EfficientNet-**Lite** version](https://tfhub.dev/tensorflow/lite-model/efficientnet/lite0/fp32/2), they do not require SELECT_TF_OPS.", "Hey Lin,\r\n\r\nI have the dependency added already and still getting same error. Do we have to add metadata to tflite to make it work?\r\nAlso, I did try to compile model without the ops and it failed. Any ideas on how I can achieve this.", "I also have the dependency added and am getting this error.", "Hi Santosh and MCR,\r\n\r\nLooks like there were failures of EfficientDet model in conversion, and then the interpreter cannot load the model file correctly.\r\n\r\nI am not sure about the detail of EfficientDet you used. Can you provide more information? BTW, is there any EfficientDet lite model that you can find from the repo?\r\n\r\nThanks!\r\n\r\n", "@ssuwal \r\nPlease provide complete code or if possible share a colab gist with the issue faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for delayed reply here. To give more info on what I am doing is, I have trained object detection model with Efficient Det D2 from Model Zoo. The model seems to be exported successfully. Only TFLite export doesn't work on current stable version so I tried beta version as someone was successful in doing that, I have linked the file in the main issue.\r\nI have tried different ways of exporting as mentioned above and here is the link to the gist.\r\n[Gist TFlite Export](https://colab.research.google.com/gist/ssuwal/e12af11e3570a0228d71acbdc3b6ad42/tf-nightly-tflite.ipynb)\r\n\r\nAlso, linking the page in example android tflite object detection example where this is consumed\r\n[Function Loading TFLite](https://github.com/tensorflow/examples/blob/47307e16aee169aca8265032bf7ab420b715b808/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L94)\r\n\r\nLet me know if you require more info.", "I don't think this is related to the converter.\r\nI think this is similar to \r\nhttps://github.com/tensorflow/tensorflow/issues/44192\r\nwhich should be fixed already", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44472\">No</a>\n", "@karimnosseir \r\nI am still having the same problem here is the github gist, please ignore the last code box.\r\n[Github Gist](https://colab.research.google.com/gist/ssuwal/0b48560597b305860d52393c00df8c86/tf-nightly-tflite.ipynb)\r\nI did pull latest nightly available\r\n- TensorFlow version (or github SHA if from source): 2.5.0-dev20201203\r\n", "@ssuwal That doesn't look the same problem earlier. I see\r\n\"\r\nfailed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Size {device = \"\"}\r\n\"\r\n\r\nTFLite doesn't support tf.Size op natively, you can use TF SELECT which enables fallback to TF for unsupported Op.\r\nSee the guide\r\nhttps://www.tensorflow.org/lite/guide/ops_select", "@karimnosseir \r\nThanks for the quick reply here,\r\nI was never able to get that last code block in the gist get to work, if you see the previous gist, it also fails with same error.\r\nsorry if this mislead the issue.\r\nMy main concern is still the same, I have a custom model trained and able to convert to tflite using current nightly version, however the android sample application give the Bytebuffer is not valid flatbuffer error. I hope this clear things up.\r\n\r\nIs there a way to check what is wrong with the tflite, or why is the bytebuffer not a valid flatbuffer?\r\n", "@ssuwal I see, the gist confused me. Can you please paste the Java sample code ? Also, can you run it in python for example or fails also ?", "```java\r\n  /** Memory-map the model file in Assets. */\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)\r\n      throws IOException {\r\n    AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n```\r\nthis is the code in the sample example, I haven't tried to load tflite in python, I will try that post update as soon as I can.\r\nAlso here is the file with the java code [Load Model](https://github.com/tensorflow/examples/blob/47307e16aee169aca8265032bf7ab420b715b808/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L94)", "```python\r\n# tf.__version__ = 2.5.0-dev20201028\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./tflite-models/noops-model.tflite\")\r\n\r\n# interpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, 1008, 756, 3))\r\ninterpreter.allocate_tensors()\r\n\r\nimg = cv2.imread('./assets/dataset_test_old/JPEGImages/IMG_0644.jpg')\r\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n# img_rgb = cv2.resize(img_rgb, (300, 300), cv2.INTER_AREA)\r\nimg_rgb = img_rgb.reshape([1, 1008, 756, 3])\r\ninterpreter.set_tensor(input_details[0]['index'], img_rgb)\r\ninterpreter.invoke()\r\nde_boxes = interpreter.get_tensor(output_details[0]['index'])[0]\r\ndet_classes = interpreter.get_tensor(output_details[1]['index'])[0]\r\ndet_scores = interpreter.get_tensor(output_details[2]['index'])[0]\r\nnum_det = interpreter.get_tensor(output_details[3]['index'])[0]\r\n```\r\nI loaded the tflite model using above code and worked without any errors. Let me know if you need anything else from me.", "This issue has be stagnant for 3 months now, any feed back or updates would be great.\r\nLet me know if you anything is needed from my side to resolve this issue or assist in any way.", "Am also having this issue as well", "Am also having this issue as well\r\n\r\n", "It seems that I get that error only when I load the module on a separate thread and only for some devices."]}, {"number": 44463, "title": "Add WinRT (UWP) as a supported platform", "body": "Feature request: Add WinRT as a supported target platform for the native library. This would allow using TF on Microsoft Store apps and would benefit projects like https://github.com/SciSharp/TensorFlow.NET.\r\n\r\nI've performed a fair amount of analysis work in the scope of the vcpkg project (see https://github.com/microsoft/vcpkg/issues/14252 and https://github.com/microsoft/vcpkg/pull/13028), so this looks realistically feasible.\r\n\r\nI realize that this feature has been requested already in #16514 long ago, but unfortunately it was closed without further explanation.", "comments": ["@amahendrakar : You may refer to https://github.com/microsoft/vcpkg/issues/14252#issuecomment-720606994 for a list of patches that result in a valid DLL.", "We won't be able to officially support this, but you could create it as a community build and maintain the build.", "That's a pity, because it doesn't seem too complicated. I can live with the patching we've done at https://github.com/microsoft/vcpkg/issues/14252#issuecomment-720606994, which is relatively straightforward and produces a working binary.\r\n\r\n@mihaimaruseac : however, would you be able to tackle at least #45151? \r\n\r\n", "The main issue is that our support matrix is too large whereas the team got severely reduced and rescoped. So we can only offer full support for x86_64 builds that are mentioned on the official site and anything that diverges is on best effort basis and left to community."]}, {"number": 44462, "title": "GradCAM and nested models", "body": "- TensorFlow version: 2.3.0\r\n- Python version: 3.7.6\r\n\r\nI'm trying to reproduce the Keras GradCAM example (https://keras.io/examples/vision/grad_cam/) using a custom model.\r\nMy custom model is relatively simple and inspired by the TF/Keras fine tuning tutorial (https://www.tensorflow.org/guide/keras/transfer_learning).\r\n\r\nThe problem is, adding a model such as vgg16 with the functional API, as a layer to my main model, seems to \"break the graph\" and not allow part of the GradCAM tutorial code to work properly. The model itself works fine for prediction and etc, but upon trying to create a 'last_conv_layer_model' I keep getting a \"Graph disconnected\" error.\r\n\r\nI have tried flattening the model, so that vgg16 is not nested within the main model anymore, but this didn't change the error.\r\n\r\nI've seen many people trying to figure out this problem and apparently the only solution is to never use the functional API in this way (allowing for nested models to happen), but that seems like a problem especially for already trained models.  I thought I'd create an issue here since maybe someone has a more elegant solution.\r\n\r\nI can't upload my actual model but I made a notebook that reproduces the error and all of the described process with a similar architecture. \r\n\r\nhttps://github.com/palatos/mynotes/blob/main/gradcam-with-nested-models-error.ipynb\r\n\r\n(The model is not fine tuned yet in the notebook but that doesn't matter. We could generate GradCAM heatmaps even for gibberish, non-fine tuned, predictions)\r\n\r\nThanks in advance for any clarification on this issue.\r\n\r\n\r\n", "comments": ["I am able to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/00393cd5c9f62227595cc0c6569c2ac1/untitled458.ipynb)", "Adding the `contributions welcome` label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks!", "@palatos More then Gradcam specific, that is hosted on [another repository](https://github.com/keras-team/keras-io/blob/master/examples/vision/grad_cam.py), is in your specific case more related to  https://github.com/tensorflow/tensorflow/issues/34977 ?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/1aa531794cc8e01a8c8763069e30881c/44462.ipynb). Thanks!", "Are there any new findings/solutions to this problem yet? I'm encountering the same issue and could need some help. ", "Just to give an update.\r\nI wasn't able to successfully fix the problem with the get_output_at method in this case, but if someone manages to do it please share the solution.\r\n\r\nI have a paliative solution for this issue but it's not very elegant and it requires a lot of manual changes for each specific nested model situation. I've uploaded the solution to this notebook:\r\n\r\nhttps://github.com/palatos/mynotes/blob/main/gradcam_paliative.ipynb\r\n\r\nEssentially what changes is that I manually separate the preprocessing, convolutional and classification blocks while generating the heatmaps, avoiding the whole ambiguity with inputs and outputs. That way I can preprocess the image before we begin, then pass it only through the convolutional layers, and then pass the result of that only through the classification layers. This eliminates the problem, but requires a lot of changes if you want to adapt it to your particular problem.\r\n\r\n```\r\n\r\n#Make GradCAM heatmap following the Keras tutorial.\r\nlast_conv_layer = model.layers[-4].layers[-1]\r\nlast_conv_layer_model = keras.Model(model.layers[-4].inputs, last_conv_layer.output)\r\n\r\n# Second, we create a model that maps the activations of the last conv\r\n# layer to the final class predictions\r\nclassifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\r\nx = classifier_input\r\nfor layer in model.layers[-3:]:\r\n    x = layer(x)\r\nclassifier_model = keras.Model(classifier_input, x)\r\n\r\n#Preparing the image with the preprocessing layers\r\npreprocess_layers = keras.Model(model.inputs, model.layers[-5].output)\r\nimg_array = preprocess_layers(prepared_image)\r\n\r\n# Then, we compute the gradient of the top predicted class for our input image\r\n# with respect to the activations of the last conv layer\r\nwith tf.GradientTape() as tape:\r\n    # Compute activations of the last conv layer and make the tape watch it\r\n    last_conv_layer_output = last_conv_layer_model(img_array)\r\n    tape.watch(last_conv_layer_output)\r\n    # Compute class predictions\r\n    preds = classifier_model(last_conv_layer_output)\r\n    top_pred_index = tf.argmax(preds[0])\r\n    top_class_channel = preds[:, top_pred_index]\r\n\r\n# This is the gradient of the top predicted class with regard to\r\n# the output feature map of the last conv layer\r\ngrads = tape.gradient(top_class_channel, last_conv_layer_output)\r\n\r\n```\r\n\r\nFrom what I read I should be able to get the same result in a much more elegant way by using get_output_at(), but I wasn't able to.", "@fchollet cc'ing you for your thoughts."]}, {"number": 44447, "title": "Compiler Test cases with tf-mlir-translate pass/crash with specific build flag on s390x architecture", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): Ubuntu 7.5.0-3ubuntu1~18.04\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen running test case `//tensorflow/compiler/mlir/xla/tests/translate:while.hlotxt.test` on s390x machine, if I include the build flag: `--per_file_copt=mlir,llvm-project@-UNDEBUG` the Test case passes, but if I remove the build flag, it fails with a `bad alloc` crash. Backtrace is attached below.\r\nAnother test case:\r\n `//tensorflow/compiler/tf2xla:fused_batchnorm_reserve_space_test`, unlike `while.hlotxt.test`, fails with this build flag but passes without it.\r\n\r\nThere are multiple test case failures in `//tensorflow/compiler/...` with similar crash. The command I am using to test:\r\n```\r\nbazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test --host_javabase=\"@local_jdk//:jdk\" --test_tag_filters=-gpu,-benchmark-test,-v1only,-no_oss,-oss_serial  -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --per_file_copt=mlir,llvm-project@-UNDEBUG -- //tensorflow/compiler/...\r\n```\r\nPlease note that there is no regression in the compiler test cases with `--per_file_copt=mlir,llvm-project@-UNDEBUG` on x86 machine.\r\n\r\n**Describe the expected behavior**\r\nTest case should pass and test case behaviour should not vary with build flag.\r\n\r\n\r\n**Other info / logs** \r\n[while.hlotxt.test.log](https://github.com/tensorflow/tensorflow/files/5461275/while.hlotxt.test.log)\r\n\r\n", "comments": ["@abattery could you take a look?", "I am not familiar with xla compiler. @smit-hinsu could you take a look?", "@River707 , do you know if this build config supported and what might be causing the failure?", "Hi @River707 Can you please take a look on the above comment by @smit-hinsu . Thanks!", "Hmm, is this the only mlir related test failing? Without seeing a symbolized stack trace I can't think of anything that would cause this specific test to crash with bad alloc.", "@River707 No, there are many mlir test cases(more than 20) failing with \"bad alloc\". For example, ```//tensorflow/compiler/mlir/xla/tests/translate:case_conditional.hlotxt.test```, ```//tensorflow/compiler/mlir/xla/tests/translate:fully_connected_reference_model.hlotxt.test```\r\n```//tensorflow/compiler/mlir/lite/tests/end2end:quant_stats.pbtxt.test``` etc.\r\n\r\nIf you can tell me how to run this TC with gdb or any other debugging tool,  I can provide you a symbolized stack trace.", "That goes beyond my knowledge. @smit-hinsu may be able to help here.", "@Simrit-Kaur You need to build tensorflow/compiler/mlir:tf-mlir-translate binary with debug symbols and execute the command on the first line of the test while replacing \"%s\" with path of the particular test.\r\n\r\nSee https://llvm.org/docs/CommandGuide/FileCheck.html to better understand the testing mechanism.", "Hmm Thanks @smit-hinsu , I will try and update soon.", "@smit-hinsu I tried to execute the command with your suggestion, when I am running the test command with optimized/normal binary, the test case is failing with a crash but when I am running the same test command with debug binary, the test case is passing.\r\n\r\nIs there a way to maybe create a core file with optimized binary and then run it in gdb with debug binary?\r\nTo build the debug binary, I used this bazel command:\r\n```\r\nbazel build -c dbg --copt=-O -c opt --copt=-g --strip never --color=yes --curses=yes --verbose_failures --local_cpu_resources 1 --local_ram_resources=2048 //tensorflow/compiler/mlir:tf-mlir-translate\r\n```\r\nTo run the TestCase, this is the command I used:\r\n```/home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate -hlo-text-to-mlir-hlo /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt -o - | /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/llvm-project/llvm/FileCheck /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt --dump-input=always -v```", "You could try running the optimized binary with GDB and see if that gives out trace of the crash.", "@smit-hinsu It does give out a trace for crash. I tried to save it to a core file and open it with debug binary but it isn't able to map the symbols. If I run debug binary in gdb, the test case simply passes. \r\nThe trace with optimized binary doesn't have a lot of information.\r\n\r\n@River707 Can you see if this information helps at all? \r\n\r\nThis is the bt that I am getting on running optimized binary with gdb:\r\n```\r\n(gdb) run -hlo-text-to-mlir-hlo /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt\r\nStarting program: /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate -hlo-text-to-mlir-hlo /home/test/.cache/bazel/_bazel_test/3a51eda24c560ebddf29b66b8fa0460e/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/xla/tests/translate/while.hlotxt\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/s390x-linux-gnu/libthread_db.so.1\".\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n\r\nProgram received signal SIGABRT, Aborted.\r\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) bt\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x000003fffc3bf3ca in __GI_abort () at abort.c:79\r\n#2  0x000003fffddab2a8 in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/s390x-linux-gnu/libstdc++.so.6\r\n#3  0x000003fffdda8b76 in ?? () from /usr/lib/s390x-linux-gnu/libstdc++.so.6\r\n#4  0x000003fffdda8bd8 in std::terminate() () from /usr/lib/s390x-linux-gnu/libstdc++.so.6\r\n#5  0x000003fffdda8eb2 in __cxa_throw () from /usr/lib/s390x-linux-gnu/libstdc++.so.6\r\n#6  0x000002aa057757f4 in ?? ()\r\nPC not saved\r\n(gdb)\r\n```"]}, {"number": 44440, "title": "tfl_quantizer", "body": "I want to use this [tfl_quantizer](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/mlir/lite/quantization/lite) application. How to build and use it?", "comments": ["This issue is more suitable on [LLVM Discourse](https://llvm.discourse.group/c/mlir/31).\r\nOn a side note you may also try [tf.quantization.quantize](https://www.tensorflow.org/api_docs/python/tf/quantization/quantize).\r\nThanks!", "it's within TensorFlow repo - why other (llvm) to build and use?", "using [tf.quantization.quantize](https://www.tensorflow.org/api_docs/python/tf/quantization/quantize) how do I setup `disable_per_channel` bool option?"]}, {"number": 44430, "title": "Cannot save a subclassed keras model that relies on AutoGraph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen a subclassed model's ```call()``` method relies on AutoGraph, e.g. it is iterating over a tensor, ```model.save()``` fails with the following exception:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\r\n```\r\n\r\nThis is inconsistent with ```model.fit()``` which does not require any explicit ```@tf.function``` decorations. The error happens because of the following code in saving_utils.py:\r\n\r\n```\r\n  # TODO(mdan): Should the model's call be autographed by default?\r\n  @def_function.function(input_signature=input_signature, autograph=False)\r\n  def _wrapped_model(*args):\r\n    \"\"\"A concrete tf.function that wraps the model's call function.\"\"\"\r\n```\r\n\r\nDo you think this code can be changed to say ```autograph=True``` (or just removing ```autograph=False``` since the default value is True) as suggested in the TODO comment? This will make it consistent with the code in ```make_train_function()``` in training.py which uses autograph by default. \r\n\r\n**Describe the expected behavior**\r\n```model.save()``` should be consistent with ```model.fit()``` and not require any explicit ```@tf.function``` decorations.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, input):\r\n        x = input\r\n        for i in tf.range(2):\r\n            x = self.layer(x)\r\n        return x\r\n\r\nif __name__ == \"__main__\":\r\n    x = np.arange(10.)\r\n    y = 2 * x\r\n\r\n    model = MyModel()\r\n    model.compile(loss=\"mse\", optimizer=\"adam\")\r\n    model.fit(x, y, epochs=1)\r\n    print(\"Done training\")\r\n\r\n    model.save(\"saved_model\")\r\n```\r\n", "comments": ["@zhezherun \r\nI ran the code and face a different issue please refer to this [gist here](https://colab.research.google.com/gist/Saduf2019/35fd91ed2c40939b88f451cbd3a6f51d/untitled455.ipynb).", "@Saduf2019 That gist shows exactly the same exception as I mentioned, i.e.:\r\n\r\n```\r\nOperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\r\n```", "@zhezherun As mentioned in the error, decorating the `call` function with @tf.function will resolve the issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d495e679cb482984093b6f98a9109e28/untitled455.ipynb). Thanks!\r\n\r\nPlease verify once and close the issue. Thanks!", "Thanks, decorating the ```call``` function with ```@tf.function``` can be used as a temporary workaround until this issue is fixed. However, I think that it is indeed only a workaround, and the underlying issue stills need to be fixed. That comment in the code (i.e. ```# TODO(mdan): Should the model's call be autographed by default?```) suggests that the current behaviour is not necessarily the correct one. Do you think my suggestion to implement this TODO item makes sense?", "Hi @zhezherun, can you point to the code where you're seeing this `TODO` ? I looked in [saving_utils.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saving_utils.py) and could not find the comment or `autograph=False`. Thanks", "@nikitamaia Here is the code as of TF2.4: https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/python/keras/saving/saving_utils.py\r\n\r\nLooks like this issue was fixed in the main branch in https://github.com/tensorflow/tensorflow/commit/ad95899595cba5d6bc3e4936340b62ed7b7cea58, but the change has not made it to any release branches yet.", "Ah yes, makes sense. We can leave this thread open until the change is visible in a release branch. ", "Unfortunately I can still reproduce this error with TF 2.5.0, so it looks like ad95899595cba5d6bc3e4936340b62ed7b7cea58 did not fix it. This time the exception is saying that AutoGraph did convert this function, however iterating over a tensor was still disallowed. As before, the error goes away if the `call` method is explicitly decorated with `@tf.function`.\r\n<pre>\r\n  File \"C:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 520, in __iter__\r\n    self._disallow_iteration()\r\n  File \"C:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 513, in _disallow_iteration\r\n    self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n  File \"C:\\Python\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 491, in _disallow_when_autograph_enabled\r\n    \" indicate you are trying to use an unsupported feature.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n</pre>", "It looks like the original cause was fixed, but this new error has a new root cause. Indeed, things should be consistent - if the layer can be called without error, then it should also save without error."]}, {"number": 44428, "title": "OP_REQUIRES failed at constant_op.cc", "body": "There is a mistake,\r\nIn Python, I use tf.while_loop to send LSTM the initial state. The following error occurred when freezing the model to call C language. \r\n\r\n```\r\n\r\n2020-10-29 17:19:01.449470: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-10-29 17:19:01.451491: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from tensor_proto.\r\n2020-10-29 17:19:02.134350: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-10-29 17:19:02.194184: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT\r\n```\r\n\r\nIs it necessary to compile source code to support it? Or is my usage incorrect?\r\n\r\n", "comments": ["@Z-yq,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "@amahendrakar\uff0c\r\nThank you for your help.\r\n\r\nThe specific code is in my project. https://github.com/Z-yq/TensorflowASR\r\nHere, I pick the important part.\r\nOkay, I implemented the Transducer structure for speech recognition using the Python version of Tensorflow (2.2.0).\r\nWhen doing inference, I used tf.while_loop to speed up decoding.\r\nlstm init code:\r\n```\r\n        self.lstm_cells = []\r\n        # lstms units must equal (for using beam search)\r\n        for i in range(num_lstms):\r\n            lstm = tf.keras.layers.LSTMCell(units=lstm_units,\r\n                                            )\r\n            self.lstm_cells.append(lstm)\r\n        self.decoder_lstms = tf.keras.layers.RNN(\r\n            self.lstm_cells, return_sequences=True, return_state=True)\r\n```\r\nAnd lstm's call function:\r\n```\r\n    def call(self,\r\n             inputs,\r\n             training=False,\r\n             p_memory_states=None,\r\n             **kwargs):\r\n \r\n        outputs = self.embed(inputs, training=training)#Embedding\r\n        outputs = self.do(outputs, training=training)#dropout\r\n        if p_memory_states is None:  # Zeros mean no initial_state\r\n            p_memory_states = self.get_initial_state(outputs)\r\n\r\n        outputs = self.decoder_lstms(outputs, training=training,initial_state=p_memory_states)\r\n        new_memory_states =outputs[1:]\r\n        outputs = outputs[0]\r\n       \r\n        return outputs  , new_memory_states\r\n```\r\nPreviously, in the main body of the loop, the LSTM state at the last prediction is passed in each time to optimize decoding speed.It can run successfully in Python.\r\n\r\nbody like this:\r\n```\r\n\r\n def _body(enc, i, decoded, h,T):\r\n            hi = tf.reshape(enc[i], [1, 1, -1])  # [1, 1, E]\r\n            y ,h_= self.predict_net(\r\n                inputs=tf.reshape(decoded[-1], [1, 1]),  # [1, 1]\r\n                p_memory_states=h,\r\n                training=False\r\n            )\r\n            ytu = tf.nn.log_softmax(self.joint_net([hi, y], training=False))\r\n            ytu = tf.squeeze(ytu, axis=None)  # [1, 1, 1, V] => [V]\r\n            n_predict = tf.argmax(ytu, axis=-1, output_type=tf.int32)  # => argmax []\r\n            n_predict = tf.reshape(n_predict, [1])\r\n\r\n            def return_no_blank():\r\n\r\n                return [tf.concat([decoded, n_predict], axis=0),h_]\r\n\r\n            decoded,h= tf.cond(\r\n                n_predict != self.text_featurizer.blank and n_predict != 0,\r\n                true_fn=return_no_blank,\r\n                false_fn=lambda: [decoded,h]\r\n            )\r\n\r\n            return enc, i + 1, decoded,h, T\r\n```\r\nSo I freeze the model to PB file and make use of Python's function calls.It works well.\r\nThe following error occurred when making a call using **TensorflowC (2.3.0)** that I downloaded the compiled library from the official.\r\n```\r\n2020-10-29 17:19:01.449470: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-10-29 17:19:01.451491: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from tensor_proto.\r\n2020-10-29 17:19:02.134350: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-10-29 17:19:02.194184: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\003\\010\\300\\002\"\r\n}\r\n\r\nCannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\003\\010\\300\\002\"\r\n}\r\n\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\003\\010\\300\\002\"\r\n}\r\n\r\n         [[{{node StatefulPartitionedCall/StatefulPartitionedCall/while/body/_1143/StatefulPartitionedCall/while/body/_4035/ConformerTransducer_prediction/rnn/TensorArrayV2_1/_0__cf__0}}]]\r\n```\r\n Then I tried some ways to solve this problem.\r\n\r\nA feasible method\uff1a\r\n```\r\n     def _body(enc, i, decoded, T):\r\n            hi = tf.reshape(enc[i], [1, 1, -1])  # [1, 1, E]\r\n            y = self.predict_net(\r\n                inputs=tf.reshape(decoded, [1, -1]),  # [1, 1]\r\n                p_memory_states=None,\r\n                training=False\r\n            )\r\n            y = y[:, -1:]\r\n            # [1, 1, P], [1, P], [1, P]\r\n            # [1, 1, E] + [1, 1, P] => [1, 1, 1, V]\r\n            ytu = tf.nn.log_softmax(self.joint_net([hi, y], training=False))# self.joint_net is a Dense\r\n            ytu = tf.squeeze(ytu, axis=None)  # [1, 1, 1, V] => [V]\r\n            n_predict = tf.argmax(ytu, axis=-1, output_type=tf.int32)  # => argmax []\r\n            n_predict = tf.reshape(n_predict, [1])\r\n\r\n            def return_no_blank():\r\n                return tf.concat([decoded, n_predict], axis=0)\r\n\r\n            decoded = tf.cond(\r\n                n_predict != self.text_featurizer.blank and n_predict != 0,\r\n                true_fn=return_no_blank,\r\n                false_fn=lambda: decoded\r\n            )\r\n\r\n            return enc, i + 1, decoded, T\r\n```\r\n\r\nCancel the input and return of the LSTM's states, and input the whole decoded value .But in this case, it will be slow when there are many decoded values.\r\nI still want to use the way of input and return LSTM's states, so I come here for help.", "`Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?`\r\n\r\nThis looks like something is missing/not supported", "Currently I can only find operations about **LIST** in RNN's **initial_state** ,and I'm a little confused\uff1a\r\n\r\nIn the case of no input and output states, the state of LSTM is also obtained by assignment in the built-in function.\r\n\r\nBut when this state is entered and returned as a parameter, it will fail.Even if I use **tf.stack** to make states as a tensor, that's the same.\r\n\r\nSo I think there may be a problem with the operation of the state of lstm.Or there are problems with the freeze graph API.\r\n", "@Z-yq,\r\n> The specific code is in my project. https://github.com/Z-yq/TensorflowASR\r\n\r\nThe example you've provided is fairly complex and it would be difficult for us to pinpoint the issue. Can you remove the dependencies and get the example down to the simplest possible repro? That will allow us to determine the source of the error easily. Thanks!", "@mihaimaruseac \uff0c\r\nThanks for your help,\r\n\r\nI've streamlined the code and there should be only tensorflow dependencies.  here:https://github.com/Z-yq/rnn-debug\r\n\r\nthe python/C script will recongnize test.wav . And python needs a **librosa** dependency only for read wav file.Using **wave** library is also possible.\r\n\r\nIn the python script, the example is called and converted to a pb file(The work has been completed in with_state/without_state ).\r\n\r\nIn CppInference folder,Two versions have been compiled\uff1acppinference_with_state/cppinference_without_state, and cppinference_without_state will success.", "<details>\r\n<summary>I'm facing the same problem.</summary>\r\n\r\n```\r\n2020-11-13 11:19:55.600020: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-11-13 11:19:55.600058: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from tensor_proto.\r\n2020-11-13 11:19:55.616901: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-11-13 11:19:55.617324: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\000\\004\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\013\\010\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\003\\010\\264\\001\\022\\003\\010\\264\\001\\022\\002\\010\\003\"\r\n}\r\n\r\nerr=Run(): Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\000\\004\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\013\\010\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\003\\010\\264\\001\\022\\003\\010\\264\\001\\022\\002\\010\\003\"\r\n}\r\n\r\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/TensorArrayV2/_0__cf__0}}]]\r\n```\r\n\r\n</details>\r\n\r\nWith the help of `git bisect` I was able to isolate breaking commit (Initially I've found that it was broken somewhere between [v2.1.1](https://github.com/tensorflow/tensorflow/releases/tag/v2.1.1) and [v2.2.0-rc0](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.2-rc0)).\r\nIt has shown that the problem was caused by changes made in https://github.com/tensorflow/tensorflow/commit/de37b1eaca05431822223e5c996bc08245cf523b.\r\n\r\nMoreover, I was able to work it around by partially reverting the commit (I've undone changed made in `variant_op_registry.h` and `variant_op_registry.cc`): https://github.com/bayandin/tensorflow/commit/2c64044fafde13490903eb6775088f0fb1986af9.\r\n\r\nI've applied the patch on the top on [v2.4.0-rc1](https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0-rc1) and it works fine for me. \r\nI'm not familiar enough with this part of the codebase and not completely sure if these won't cause any regressions in other parts, but if it's ok I'll be happy to submit a PR with the fix.\r\n\r\nAlso, If you come up with a proper fix, I could help to verify it locally for my case.\r\n\r\n<details>\r\n<summary>here's my git bisect log, if you're interested.</summary>\r\n\r\n```\r\n$ git bisect log\r\ngit bisect start\r\n# bad: [3c1e8c03419266bb6ba379d303d3e03a380617a8] Merge pull request #37486 from tensorflow/mm-r2.2-debug-win-build\r\ngit bisect bad 3c1e8c03419266bb6ba379d303d3e03a380617a8\r\n# good: [54daf3c5700897a6062313983933ca28e92c410d] Pin scipy to 1.4.1.\r\ngit bisect good 54daf3c5700897a6062313983933ca28e92c410d\r\n# bad: [579287b839d410b859efce68cb093995f27ef13c] Try to fix spacing/formatting in returns section of Keras dataset docs.\r\ngit bisect bad 579287b839d410b859efce68cb093995f27ef13c\r\n# good: [085db02feb6065d5de4b426c8d530b4178b50061] Merge pull request #35686 from samikama:GBBPFix\r\ngit bisect good 085db02feb6065d5de4b426c8d530b4178b50061\r\n# bad: [ca87dcae5568d1b44016be46ebfd8b4972a6959b] Go: Update generated wrapper functions for TensorFlow ops.\r\ngit bisect bad ca87dcae5568d1b44016be46ebfd8b4972a6959b\r\n# bad: [6ab77f3be330714746054eb678c9c4116f300692] Add complex number support for tf.extract_image_patches\r\ngit bisect bad 6ab77f3be330714746054eb678c9c4116f300692\r\n# bad: [335b20de311d9aeb3a2aaf13527c65f0fab5540e] Add a missing ProtobufStringToString.\r\ngit bisect bad 335b20de311d9aeb3a2aaf13527c65f0fab5540e\r\n# good: [7e17fdff2868df65139f5f0eb79bca40b6bbc041] server_lib.create_local_server() should create TF server with job_name=localhost\r\ngit bisect good 7e17fdff2868df65139f5f0eb79bca40b6bbc041\r\n# good: [0fd0b290824ce4d1dad23c3e050f2d859d2b845b] Adds an example showing a fault-tolerant custom training loop with MultiWorkerMirroredStrategy.\r\ngit bisect good 0fd0b290824ce4d1dad23c3e050f2d859d2b845b\r\n# good: [0fb8ffc796fe6ce191eacb6654ab394449bec065] Add quantized types support for Arg min/max in TF Lite dialect\r\ngit bisect good 0fb8ffc796fe6ce191eacb6654ab394449bec065\r\n# bad: [de37b1eaca05431822223e5c996bc08245cf523b] Refactor the mobile/portable/android/ios targets to use filegroups where possible.\r\ngit bisect bad de37b1eaca05431822223e5c996bc08245cf523b\r\n# good: [20d5c6a05070d0ca20eb1f38877b91cc95b841f1] Add new Adafruit board to supported devices\r\ngit bisect good 20d5c6a05070d0ca20eb1f38877b91cc95b841f1\r\n# good: [fcef1fc494fc9410362c3659b62928b88079bef1] [MLIR:TF/XLA] Handle function argument aliasing in side-effect analysis.\r\ngit bisect good fcef1fc494fc9410362c3659b62928b88079bef1\r\n# good: [fd2cd3e10e799c1b99018ff82cc5fa32016726af] Add an XSpace to TraceEvents converter and unittest\r\ngit bisect good fd2cd3e10e799c1b99018ff82cc5fa32016726af\r\n# first bad commit: [de37b1eaca05431822223e5c996bc08245cf523b] Refactor the mobile/portable/android/ios targets to use filegroups where possible.\r\n```\r\n</details>", "I'm facing the same problem, but I can't solve it making the RNN stateless anyway. When I load the model and I try to feed it, I get the very same error\r\n\r\n```\r\n2020-11-14 20:59:37.164271: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-11-14 20:59:37.164517: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:81 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\000\\003\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\"\r\n}\r\n\r\npanic: {{function_node __forward_standard_gru_2677_specialized_for_StatefulPartitionedCall_1_StatefulPartitionedCall_sequential_gru_PartitionedCall_at_tf_graph}} {{function_node __forward_standard_gru_2677_specialized_for_StatefulPartitionedCall_1_StatefulPartitionedCall_sequential_gru_PartitionedCall_at_tf_graph}} Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\000\\003\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\"\r\n}\r\n\r\n         [[{{node while/Placeholder_0/accumulator/_0__cf__0}}]]\r\n         [[StatefulPartitionedCall_1/StatefulPartitionedCall/sequential/gru/PartitionedCall]]\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/galeone/tfgo.(*Model).Exec(0xc000010030, 0xc00015e4f0, 0x1, 0x1, 0xc00015e520, 0x0, 0x80, 0x80)\r\n        /home/paolo/projects/go/src/github.com/galeone/tfgo/model.go:73 +0xce\r\nmain.learn(0xc000010030, 0xc000cc30c0, 0xc000cc30e0, 0x0)\r\n        /home/paolo/projects/galeone.github.io/code/all-in-graph/wat.go:85 +0x2e7\r\nmain.Learn(0xc000010030, 0x21, 0xc000c95fd8, 0x7, 0x1, 0x0, 0x2cb7d702cef0, 0x84f780, 0x414ae382bf31d3c7, 0x3f01030e, ...)\r\n        /home/paolo/projects/galeone.github.io/code/all-in-graph/wat.go:115 +0x274\r\nmain.main()\r\n        /home/paolo/projects/galeone.github.io/code/all-in-graph/wat.go:238 +0x6f0\r\n```", "@bayandin have you compiled the C version for Linux or macOS? with or without GPU support? It would be helpful if you can share your built library (I'm looking for the Linux version without GPU support)", "> Moreover, I was able to work it around by partially reverting the commit (I've undone changed made in `variant_op_registry.h` and `variant_op_registry.cc`): [bayandin@2c64044](https://github.com/bayandin/tensorflow/commit/2c64044fafde13490903eb6775088f0fb1986af9).\r\n\r\n@Z-yq,\r\nPlease take a look at @bayandin's comment and let us know if you are still facing the same issue. Thanks!", "> @bayandin have you compiled the C version for Linux or macOS? with or without GPU support? It would be helpful if you can share your built library (I'm looking for the Linux version without GPU support)\r\n\r\nInitially, I tested my patch on macOS (CPU) and it worked fine.\r\nBut it turns out that the patch didn't fix the issue on my Linux setup (neither on CPU nor on GPU).", "@bayandin,Thank you for your help.\r\n\r\n\r\n@amahendrakar ,\r\n\r\n> Moreover, I was able to work it around by partially reverting the commit (I've undone changed made in variant_op_registry.h and variant_op_registry.cc): bayandin@2c64044.\r\n\r\nI tried the method, but still failed.\r\n\r\n```\r\n\r\n2020-11-17 17:49:54.518639: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-11-17 17:49:54.518695: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from tensor_proto.\r\n2020-11-17 17:49:54.567266: E tensorflow/core/framework/tensor.cc:555] Could not decode variant with type_name: \"tensorflow::TensorList\".  Perhaps you forgot to register a decoder via REGISTER_UNARY_VARIANT_DECODE_FUNCTION?\r\n2020-11-17 17:49:54.567633: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at constant_op.cc:82 : Invalid argument: Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\003\\010\\300\\002\"\r\n}\r\n\r\nCannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\003\\010\\300\\002\"\r\n}\r\n\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  Cannot parse tensor from proto: dtype: DT_VARIANT\r\ntensor_shape {\r\n}\r\nvariant_val {\r\n  type_name: \"tensorflow::TensorList\"\r\n  metadata: \"\\001\\000\\001\\377\\377\\377\\377\\377\\377\\377\\377\\377\\001\\022\\002\\010\\001\\022\\003\\010\\300\\002\"\r\n}\r\n\r\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/while/body/_1317/while/rnn/TensorArrayV2_1/_0__cf__0}}]]\r\nAborted (core dumped)\r\n```\r\n\r\n\r\nMy environment is \uff1a\r\n\r\n```\r\nUbuntu 18.04\r\ngcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n```\r\n\r\nAnd CPU info\uff1a\r\n```\r\nprocessor       : 7\r\nvendor_id       : GenuineIntel\r\ncpu family      : 6\r\nmodel           : 142\r\nmodel name      : Intel(R) Core(TM) i7-10610U CPU @ 1.80GHz\r\nstepping        : 12\r\ncpu MHz         : 2304.000\r\n\r\n```\r\n\r\n", "Is there any news? This bug is making the deployment of RNNs impossible...", "Please check if this patch against 2.4.1 helps, there are 2 bugs with static/dynamic loading of TF libraries and this works it around\r\n\r\n```\r\ndiff --git a/tensorflow/core/framework/type_index.h b/tensorflow/core/framework/type_index.h\r\nindex 7986904dd7a..5519df474d0 100644\r\n--- a/tensorflow/core/framework/type_index.h\r\n+++ b/tensorflow/core/framework/type_index.h\r\n@@ -24,9 +24,10 @@ limitations under the License.\r\n \r\n #include \"tensorflow/core/platform/types.h\"\r\n \r\n-#if defined(MACOS) || defined(TARGET_OS_MAC)\r\n+#define USE_NAME_BASED_RTTI_ONLY\r\n+#if defined(MACOS) || defined(TARGET_OS_MAC) || defined(USE_NAME_BASED_RTTI_ONLY)\r\n #include \"tensorflow/core/platform/hash.h\"\r\n-#endif  // defined(MACOS) || defined(TARGET_OS_MAC)\r\n+#endif  // defined(MACOS) || defined(TARGET_OS_MAC) || defined(USE_NAME_BASED_RTTI_ONLY)\r\n \r\n namespace tensorflow {\r\n \r\n@@ -62,7 +63,7 @@ class TypeIndex {\r\n \r\n #if defined(__GXX_RTTI) || defined(_CPPRTTI)\r\n \r\n-#if defined(MACOS) || defined(TARGET_OS_MAC)\r\n+#if defined(MACOS) || defined(TARGET_OS_MAC) || defined(USE_NAME_BASED_RTTI_ONLY)\r\n     // Use a hash based on the type name to avoid issues due to RTLD_LOCAL on\r\n     // MacOS (b/156979412).\r\n     return TypeIndex(Hash64(typeid(T).name()), typeid(T).name());\r\ndiff --git a/tensorflow/core/framework/variant_op_registry.cc b/tensorflow/core/framework/variant_op_registry.cc\r\nindex aa3bdeab5e2..069542d784a 100644\r\n--- a/tensorflow/core/framework/variant_op_registry.cc\r\n+++ b/tensorflow/core/framework/variant_op_registry.cc\r\n@@ -26,6 +26,16 @@ limitations under the License.\r\n \r\n namespace tensorflow {\r\n \r\n+// Get a pointer to a global UnaryVariantOpRegistry object\r\n+UnaryVariantOpRegistry* UnaryVariantOpRegistryGlobal() {\r\n+  static UnaryVariantOpRegistry* global_unary_variant_op_registry = NULL;\r\n+\r\n+  if (global_unary_variant_op_registry == NULL) {\r\n+    global_unary_variant_op_registry = new UnaryVariantOpRegistry;\r\n+  }\r\n+  return global_unary_variant_op_registry;\r\n+}\r\n+\r\n std::unordered_set<string>* UnaryVariantOpRegistry::PersistentStringStorage() {\r\n   static std::unordered_set<string>* string_storage =\r\n       new std::unordered_set<string>();\r\n@@ -55,6 +65,13 @@ bool DecodeUnaryVariant(Variant* variant) {\r\n   if (variant->TypeName().empty()) {\r\n     VariantTensorDataProto* t = variant->get<VariantTensorDataProto>();\r\n     if (t == nullptr || !t->metadata().empty() || !t->tensors().empty()) {\r\n+      LOG(ERROR) << __PRETTY_FUNCTION__ << \": empty typename, malformed variant: t == nullptr: \" << (t == nullptr) << \": \" << variant->DebugString() << std::endl;\r\n+      if (t != nullptr) {\r\n+         LOG(ERROR) << __PRETTY_FUNCTION__ << \": empty typename, malformed variant: t != nullptr: !t->metadata().empty(): \" << !t->metadata().empty()\r\n+\t\t << \", !t->tensors().empty(): \" << !t->tensors().empty()\r\n+\t\t << \": \" << variant->DebugString()\r\n+\t\t << std::endl;\r\n+      }\r\n       // Malformed variant.\r\n       return false;\r\n     } else {\r\n@@ -66,11 +83,15 @@ bool DecodeUnaryVariant(Variant* variant) {\r\n   UnaryVariantOpRegistry::VariantDecodeFn* decode_fn =\r\n       UnaryVariantOpRegistry::Global()->GetDecodeFn(variant->TypeName());\r\n   if (decode_fn == nullptr) {\r\n+    LOG(ERROR) << __PRETTY_FUNCTION__ << variant->TypeName() << \": decode_fn == nullptr: \" << variant->DebugString() << std::endl; \r\n     return false;\r\n   }\r\n   const string type_name = variant->TypeName();\r\n   bool decoded = (*decode_fn)(variant);\r\n-  if (!decoded) return false;\r\n+  if (!decoded) {\r\n+    LOG(ERROR) << type_name << \" ->\" << variant->TypeName() << \": not decoded:\" << variant->DebugString() << std::endl; \r\n+    return false;\r\n+  }\r\n   if (variant->TypeName() != type_name) {\r\n     LOG(ERROR) << \"DecodeUnaryVariant: Variant type_name before decoding was: \"\r\n                << type_name\r\ndiff --git a/tensorflow/core/framework/variant_op_registry.h b/tensorflow/core/framework/variant_op_registry.h\r\nindex edfb9c544c0..ed8bec6cc6e 100644\r\n--- a/tensorflow/core/framework/variant_op_registry.h\r\n+++ b/tensorflow/core/framework/variant_op_registry.h\r\n@@ -56,6 +56,9 @@ enum VariantDeviceCopyDirection {\r\n   DEVICE_TO_DEVICE = 3,\r\n };\r\n \r\n+class UnaryVariantOpRegistry;\r\n+extern UnaryVariantOpRegistry* UnaryVariantOpRegistryGlobal();\r\n+\r\n class UnaryVariantOpRegistry {\r\n  public:\r\n   typedef std::function<bool(Variant*)> VariantDecodeFn;\r\n@@ -170,9 +173,7 @@ class UnaryVariantOpRegistry {\r\n \r\n   // Get a pointer to a global UnaryVariantOpRegistry object\r\n   static UnaryVariantOpRegistry* Global() {\r\n-    static UnaryVariantOpRegistry* global_unary_variant_op_registry =\r\n-        new UnaryVariantOpRegistry;\r\n-    return global_unary_variant_op_registry;\r\n+    return UnaryVariantOpRegistryGlobal();\r\n   }\r\n \r\n   // Get a pointer to a global persistent string storage object.\r\n```", "Btw, assigning `TF 2.3` tag shoud presumably imply that it has been fixed in 2.4 or it is not relevant anymore, but no, this bug is still present in 2.4.1.\r\n\r\nIn fact, there are 2 bugs.\r\nFor some inference models (maybe when there are 2 or more saved models in one global inference context) some TF libraries got loaded twice and some global variables got overwritten.\r\n\r\nThe second bug was introduced in https://github.com/tensorflow/tensorflow/commit/de37b1eaca05431822223e5c996bc08245cf523b as found by Alexander Bayandin above - TF statically loads a bunch of `TF_VARIANT` decoders, but then it dynamically loads the same (or some) libraries and overwrites global lists, this bug actually existed forever, but no one uses TF_VARIANT to store bool or int32, so those got overwritten (actually, lost), but no one cares. But variant decoder for TensorList got lost and reverting de37b1eaca05431822223e5c996bc08245cf523b should have fixed that. I have fixed this bug by explicitly calling single static function defined not in a header, but in C code.\r\n\r\nBut there is the first bug introduced in 15275d3a14c77e2244ae1155f93243256f08e3ed - again, because of the \"second\" library loading RTTI changes for every class for every new dynamic library loading, thus TensorList got different RTTI ID and decoder in `variant->get<VariantTensorDataProto>()` refuses to decode protobuf. You have fixed this for MacOS by reverting to the old behaviour, now my patch forces this for everyone else. This first bug ends up with the debug message printed above.\r\n\r\nOr these two bugs can be different issue with TF loading some of its modules/libraries/anything multiple time for the inference environment with multiple saved_model in them.\r\n\r\nThis patch fixes problem for me with different models on CPU and GPU on linux. I will wait for some time for others to test and/or confirm whether it works or not and then will make a proper pull request.", "> This patch fixes problem for me with different models on CPU and GPU on linux. I will wait for some time for others to test and/or confirm whether it works or not and then will make a proper pull request.\r\n\r\nThe patch helped for my case on macOS CPU\r\nThanks!", "This patch also fixed the same error for me.  Linux assorted (ancient) cpus, V100 + P6000 gpus.", "I can see there's a PR with this patch: https://github.com/tensorflow/tensorflow/pull/47072", "> Please check if this patch against 2.4.1 helps, there are 2 bugs with static/dynamic loading of TF libraries and this works it around\r\n> \r\n> ```\r\n> diff --git a/tensorflow/core/framework/type_index.h b/tensorflow/core/framework/type_index.h\r\n> index 7986904dd7a..5519df474d0 100644\r\n> --- a/tensorflow/core/framework/type_index.h\r\n> +++ b/tensorflow/core/framework/type_index.h\r\n> @@ -24,9 +24,10 @@ limitations under the License.\r\n>  \r\n>  #include \"tensorflow/core/platform/types.h\"\r\n>  \r\n> -#if defined(MACOS) || defined(TARGET_OS_MAC)\r\n> +#define USE_NAME_BASED_RTTI_ONLY\r\n> +#if defined(MACOS) || defined(TARGET_OS_MAC) || defined(USE_NAME_BASED_RTTI_ONLY)\r\n>  #include \"tensorflow/core/platform/hash.h\"\r\n> -#endif  // defined(MACOS) || defined(TARGET_OS_MAC)\r\n> +#endif  // defined(MACOS) || defined(TARGET_OS_MAC) || defined(USE_NAME_BASED_RTTI_ONLY)\r\n>  \r\n>  namespace tensorflow {\r\n>  \r\n> @@ -62,7 +63,7 @@ class TypeIndex {\r\n>  \r\n>  #if defined(__GXX_RTTI) || defined(_CPPRTTI)\r\n>  \r\n> -#if defined(MACOS) || defined(TARGET_OS_MAC)\r\n> +#if defined(MACOS) || defined(TARGET_OS_MAC) || defined(USE_NAME_BASED_RTTI_ONLY)\r\n>      // Use a hash based on the type name to avoid issues due to RTLD_LOCAL on\r\n>      // MacOS (b/156979412).\r\n>      return TypeIndex(Hash64(typeid(T).name()), typeid(T).name());\r\n> diff --git a/tensorflow/core/framework/variant_op_registry.cc b/tensorflow/core/framework/variant_op_registry.cc\r\n> index aa3bdeab5e2..069542d784a 100644\r\n> --- a/tensorflow/core/framework/variant_op_registry.cc\r\n> +++ b/tensorflow/core/framework/variant_op_registry.cc\r\n> @@ -26,6 +26,16 @@ limitations under the License.\r\n>  \r\n>  namespace tensorflow {\r\n>  \r\n> +// Get a pointer to a global UnaryVariantOpRegistry object\r\n> +UnaryVariantOpRegistry* UnaryVariantOpRegistryGlobal() {\r\n> +  static UnaryVariantOpRegistry* global_unary_variant_op_registry = NULL;\r\n> +\r\n> +  if (global_unary_variant_op_registry == NULL) {\r\n> +    global_unary_variant_op_registry = new UnaryVariantOpRegistry;\r\n> +  }\r\n> +  return global_unary_variant_op_registry;\r\n> +}\r\n> +\r\n>  std::unordered_set<string>* UnaryVariantOpRegistry::PersistentStringStorage() {\r\n>    static std::unordered_set<string>* string_storage =\r\n>        new std::unordered_set<string>();\r\n> @@ -55,6 +65,13 @@ bool DecodeUnaryVariant(Variant* variant) {\r\n>    if (variant->TypeName().empty()) {\r\n>      VariantTensorDataProto* t = variant->get<VariantTensorDataProto>();\r\n>      if (t == nullptr || !t->metadata().empty() || !t->tensors().empty()) {\r\n> +      LOG(ERROR) << __PRETTY_FUNCTION__ << \": empty typename, malformed variant: t == nullptr: \" << (t == nullptr) << \": \" << variant->DebugString() << std::endl;\r\n> +      if (t != nullptr) {\r\n> +         LOG(ERROR) << __PRETTY_FUNCTION__ << \": empty typename, malformed variant: t != nullptr: !t->metadata().empty(): \" << !t->metadata().empty()\r\n> +\t\t << \", !t->tensors().empty(): \" << !t->tensors().empty()\r\n> +\t\t << \": \" << variant->DebugString()\r\n> +\t\t << std::endl;\r\n> +      }\r\n>        // Malformed variant.\r\n>        return false;\r\n>      } else {\r\n> @@ -66,11 +83,15 @@ bool DecodeUnaryVariant(Variant* variant) {\r\n>    UnaryVariantOpRegistry::VariantDecodeFn* decode_fn =\r\n>        UnaryVariantOpRegistry::Global()->GetDecodeFn(variant->TypeName());\r\n>    if (decode_fn == nullptr) {\r\n> +    LOG(ERROR) << __PRETTY_FUNCTION__ << variant->TypeName() << \": decode_fn == nullptr: \" << variant->DebugString() << std::endl; \r\n>      return false;\r\n>    }\r\n>    const string type_name = variant->TypeName();\r\n>    bool decoded = (*decode_fn)(variant);\r\n> -  if (!decoded) return false;\r\n> +  if (!decoded) {\r\n> +    LOG(ERROR) << type_name << \" ->\" << variant->TypeName() << \": not decoded:\" << variant->DebugString() << std::endl; \r\n> +    return false;\r\n> +  }\r\n>    if (variant->TypeName() != type_name) {\r\n>      LOG(ERROR) << \"DecodeUnaryVariant: Variant type_name before decoding was: \"\r\n>                 << type_name\r\n> diff --git a/tensorflow/core/framework/variant_op_registry.h b/tensorflow/core/framework/variant_op_registry.h\r\n> index edfb9c544c0..ed8bec6cc6e 100644\r\n> --- a/tensorflow/core/framework/variant_op_registry.h\r\n> +++ b/tensorflow/core/framework/variant_op_registry.h\r\n> @@ -56,6 +56,9 @@ enum VariantDeviceCopyDirection {\r\n>    DEVICE_TO_DEVICE = 3,\r\n>  };\r\n>  \r\n> +class UnaryVariantOpRegistry;\r\n> +extern UnaryVariantOpRegistry* UnaryVariantOpRegistryGlobal();\r\n> +\r\n>  class UnaryVariantOpRegistry {\r\n>   public:\r\n>    typedef std::function<bool(Variant*)> VariantDecodeFn;\r\n> @@ -170,9 +173,7 @@ class UnaryVariantOpRegistry {\r\n>  \r\n>    // Get a pointer to a global UnaryVariantOpRegistry object\r\n>    static UnaryVariantOpRegistry* Global() {\r\n> -    static UnaryVariantOpRegistry* global_unary_variant_op_registry =\r\n> -        new UnaryVariantOpRegistry;\r\n> -    return global_unary_variant_op_registry;\r\n> +    return UnaryVariantOpRegistryGlobal();\r\n>    }\r\n>  \r\n>    // Get a pointer to a global persistent string storage object.\r\n> ```\r\n\r\nHi @bioothod. How can I apply these patch fixes?  Can you give me some directions?\r\n", "Hi @schramm , you need to use `patch` tool to apply this diff to 2.4 tensorflow sources, and then to rebuild tensorflow. These bugfixes are alsow in master branch for the upcoming 2.6 version.", "I have built the master brunch for the 2.6 version and it is working\nfine now.\nThank you!\n\nEm qui., 3 de jun. de 2021 \u00e0s 13:48, Evgeniy Polyakov <\n***@***.***> escreveu:\n\n> Hi @schramm <https://github.com/schramm> , you need to use patch tool to\n> apply this diff to 2.4 tensorflow sources, and then to rebuild tensorflow.\n> These bugfixes are alsow in master branch for the upcoming 2.6 version.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44428#issuecomment-853808894>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACEAZ5USXXUG73FTPC65C3TQ5T2NANCNFSM4TDPFF7Q>\n> .\n>\n\n\n-- \nDr Rodrigo Schramm\n", "The fix from 2.4.1 is also available on 2.5 and should also be available on the other patch releases that were submitted at the same time as 2.4.1.\r\n\r\nSince this is fixed at head, I think we can close this. Please reopen if that is not the case. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44428\">No</a>\n", "@mihaimaruseac what do you mean that fix is available in 2.4.1 and 2.5? This fix has not been committed neither into 2.4 nor into 2.5 tree although was submitted, those branches currently have this bug, bugfix has only been pushed into the current master tree (future 2.6).", "Sorry, I misunderstood. You are right.\r\n\r\nIf we want to patch this in older versions, we need to open PRs against those branches, but given the complexity of the branch and the current timelines, I think it's better to just wait for 2.6 release in a few weeks.", "I have a similar issue and tried everything suggested under this issue and it didn't work. \r\nI created a GitHub issue \r\nhttps://github.com/tensorflow/tensorflow/issues/51052\r\n\r\n", "@mihaimaruseac, would it be conceivable to backport the fix to TF 2.4? Later versions of TensorFlow require updating CUDA from 11.0 to 11.2, which complicates deployments.", "2.4 series will get EOLed in December, so probably will get at most one single patch release until then.\r\n\r\nIf you create a PR on the branch, we can try merging it in and running tests on it, but if there are failures we won't be able to fully patch this. Given the large refactoring that occurred since, there is a potential that even if our tests works the patch might break for other users."]}, {"number": 44394, "title": "How are batch gradients computed on embedding layers?", "body": "Consider the following model, which is more or less a 12-dimensional vector lookup table with 10 rows, initialized to all zeros.\r\n\r\n```\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Embedding(input_dim=10, output_dim=12, embeddings_initializer=keras.initializers.zeros))\r\nmodel.compile(optimizer=keras.optimizers.SGD(),loss=keras.losses.MeanSquaredError())\r\n```\r\n\r\nI simply want it to train to the following data:\r\n```\r\nx = numpy.append(numpy.zeros(10000), numpy.ones(10000))\r\ny = numpy.append(numpy.random.multivariate_normal(numpy.zeros(12), numpy.diag(numpy.ones(12)), 10000),\r\n                 numpy.random.multivariate_normal(numpy.ones(12)*2, numpy.diag(numpy.ones(12)), 10000), axis=0)\r\nmodel.fit(x,y,epochs=1,batch_size=1)\r\n```\r\n\r\nWhen the batch size is 1, the model behaves predictably; using stochastic gradient descent, we train the weights of the embedding layer towards the means of the two conditional distribution - i.e. model(0) tends towards [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], and model(1) tends toward [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2].\r\n\r\nHowever, for batch sizes larger than 1, the weights of the model tend toward the average value of y, without respect for the value of x - i.e. both model(0) and model(1) tend toward [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\r\n\r\nWhy does this happen? If we just want a lookup table, we can alternately implement it using a dense layer with no bias term; this simply requires that we one-hot encode the input.\r\n\r\n```\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Dense(12, input_shape=(10,), use_bias=False, kernel_initializer=keras.initializers.zeros))\r\nmodel.compile(optimizer=keras.optimizers.SGD(),loss=keras.losses.MeanSquaredError())\r\n\r\nx = tensorflow.one_hot(numpy.append(numpy.zeros(10000), numpy.ones(10000)), 10)\r\ny = numpy.append(numpy.random.multivariate_normal(numpy.zeros(12), numpy.diag(numpy.ones(12)), 10000),\r\n                 numpy.random.multivariate_normal(numpy.ones(12)*2, numpy.diag(numpy.ones(12)), 10000), axis=0)\r\nmodel.fit(x,y,epochs=16,batch_size=128)\r\n```\r\n\r\nThis behaves as I would expect, even when using mini-batch for training. So why doesn't it work correctly using the embedding layer?", "comments": ["Adding the `contributions welcome` label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks!", "I'm interested."]}, {"number": 44369, "title": "How to get detailed performance profiling results of TFLite for Apple Metal on iOS devices?", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 14.0.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone xs Max\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): github master with commit: 6f58b4d46ee25633052a844531c3151affdf3635\r\n- Python version:  3.7.6\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):  Apple clang version 12.0.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe iOS benchmark app could show detailed performance profiling results for cpu. But with metal delegate, no detailed results are shown.\r\n\r\n**Describe the expected behavior**\r\nShwoing defailed profiling results for metal gpu delegate like CPU\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe output in xcode  console output section for cpu is shown below:\r\n\r\n> Profile (4 threads):\r\n> \r\n> Thread 0 (932 samples)\r\n> \r\n> * 70.71% Conv\r\n>   * 65.77% cpu_backend_gemm::Gemm\r\n>     * 63.73% Mul\r\n>       * 28.33% matmul shape: 512x512x196\r\n>         * 28.33% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 28.33% TrMulImpl, general case\r\n>             * 16.09% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 10.41% [other]\r\n>             * 1.82% Pack (kNeon, optimized for out-of-order cores)\r\n>       * 6.87% matmul shape: 1024x1024x49\r\n>         * 6.87% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 6.87% TrMulImpl, general case\r\n>             * 3.86% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 2.15% Pack (kNeon, optimized for out-of-order cores)\r\n>             * 0.86% [other]\r\n>       * 6.12% matmul shape: 256x256x784\r\n>         * 6.12% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 6.12% TrMulImpl, general case\r\n>             * 3.65% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 2.36% [other]\r\n>             * 0.11% Pack (kNeon, optimized for out-of-order cores)\r\n>       * 4.51% matmul shape: 128x128x3136\r\n>         * 4.51% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 4.51% TrMulImpl, general case\r\n>             * 3.54% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 0.54% [other]\r\n>             * 0.43% Pack (kNeon, optimized for out-of-order cores)\r\n>       * 3.76% matmul shape: 64x32x12544\r\n>         * 3.76% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 3.76% TrMulImpl, general case\r\n>             * 2.90% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 0.86% [other]\r\n>       * 3.43% matmul shape: 1024x512x49\r\n>         * 3.43% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 3.43% TrMulImpl, general case\r\n>             * 1.61% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 0.97% Pack (kNeon, optimized for out-of-order cores)\r\n>             * 0.86% [other]\r\n>       * 2.90% matmul shape: 512x256x196\r\n>         * 2.90% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 2.90% TrMulImpl, general case\r\n>             * 1.50% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 1.07% [other]\r\n>             * 0.32% Pack (kNeon, optimized for out-of-order cores)\r\n>       * 2.90% matmul shape: 256x128x784\r\n>         * 2.90% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 2.90% TrMulImpl, general case\r\n>             * 1.61% [other]\r\n>             * 1.29% Kernel (kNeon, optimized for out-of-order cores)\r\n>       * 2.68% matmul shape: 128x64x3136\r\n>         * 2.68% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 2.68% TrMulImpl, general case\r\n>             * 1.93% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 0.54% [other]\r\n>             * 0.21% Pack (kNeon, optimized for out-of-order cores)\r\n>       * 2.25% matmul shape: 32x27x12544\r\n>         * 2.25% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n>           * 2.25% TrMulImpl, general case\r\n>             * 1.29% Kernel (kNeon, optimized for out-of-order cores)\r\n>             * 0.86% [other]\r\n>             * 0.11% GetBlockMatrixCoords\r\n>     * 2.04% cpu_backend_gemm::Gemm: CustomGemv\r\n>   * 4.83% Im2col\r\n>     * 3.00% ExtractPatchIntoBufferColumn\r\n>     * 1.82% [other]\r\n>   * 0.11% [other]\r\n> * 29.08% DepthwiseConv\r\n>   * 22.85% [other]\r\n>   * 6.22% DepthwiseConv/float/DepthwiseConvImpl\r\n>     * 4.83% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n>     * 1.39% [other]\r\n> * 0.21% AveragePool\r\n> \r\n> Thread 1 (718 samples)\r\n> \r\n> * 67.55% Kernel (kNeon, optimized for out-of-order cores)\r\n> * 25.63% DepthwiseConv/float/DepthwiseConvImpl\r\n>   * 21.31% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n>   * 4.32% [other]\r\n> * 6.69% Pack (kNeon, optimized for out-of-order cores)\r\n> * 0.14% GetBlockMatrixCoords\r\n> \r\n> Thread 2 (681 samples)\r\n> \r\n> * 65.49% Kernel (kNeon, optimized for out-of-order cores)\r\n> * 26.73% DepthwiseConv/float/DepthwiseConvImpl\r\n>   * 21.44% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n>   * 5.29% [other]\r\n> * 7.64% Pack (kNeon, optimized for out-of-order cores)\r\n> * 0.15% GetBlockMatrixCoords\r\n> \r\n> Thread 3 (645 samples)\r\n> \r\n> * 66.82% Kernel (kNeon, optimized for out-of-order cores)\r\n> * 25.27% DepthwiseConv/float/DepthwiseConvImpl\r\n>   * 20.16% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n>   * 5.12% [other]\r\n> * 7.60% Pack (kNeon, optimized for out-of-order cores)\r\n> * 0.16% GetBlockByIndex\r\n> * 0.16% GetBlockMatrixCoords\r\n\r\nWhile for metal gpu delegates, the output is:\r\n\r\n> Duplicate flags: num_threads\r\n> Log parameter values verbosely: [0]\r\n> Min num runs: [20]\r\n> Inter-run delay (seconds): [-1]\r\n> Benchmark name: [mobile_net_benchmark]\r\n> Min warmup runs: [1]\r\n> Graph: [/private/var/containers/Bundle/Application/29E02AE1-F22E-4454-A049-042C04929CBE/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite]\r\n> Input layers: [input]\r\n> Input shapes: [1,224,224,3]\r\n> Use gpu: [1]\r\n> GPU delegate wait type: [aggressive]\r\n> Loaded model /private/var/containers/Bundle/Application/29E02AE1-F22E-4454-A049-042C04929CBE/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite\r\n> 2020-10-28 14:21:10.624179+0800 TFLiteBenchmark[3858:924590] Initialized TensorFlow Lite runtime.\r\n> 2020-10-28 14:21:10.632180+0800 TFLiteBenchmark[3858:924590] Created TensorFlow Lite delegate for Metal.\r\n> 2020-10-28 14:21:10.635001+0800 TFLiteBenchmark[3858:924590] Metal GPU Frame Capture Enabled\r\n> 2020-10-28 14:21:10.638319+0800 TFLiteBenchmark[3858:924590] Metal API Validation Enabled\r\n> 2020-10-28 14:21:10.876638+0800 TFLiteBenchmark[3858:924590] Following operations are not supported by GPU delegate:\r\n> SQUEEZE: Operation is not supported.\r\n> 29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n> 2020-10-28 14:21:11.164313+0800 TFLiteBenchmark[3858:924590] [Metal Compiler Warning] Warning: Compilation succeeded with: \r\n> \r\n> program_source:46:11: warning: unused variable 'gid'\r\n>     uint3 gid = uint3(0u, 0u, uint(linear_index));\r\n>           ^\r\n> Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\n> The input model file size (MB): 16.9008\r\n> Initialized session in 558.992ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=86 first=9867 curr=4858 min=4704 max=9867 avg=5813.1 std=1163\r\n> \r\n> Running benchmark for at least 20 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=183 first=5456 curr=5688 min=4690 max=29198 avg=5447.84 std=1850\r\n> \r\n> Inference timings in us: Init: 558992, First inference: 9867, Warmup (avg): 5813.1, Inference (avg): 5447.84\r\n> \r\n> Profile (0 threads):\r\n\r\nDeos the TFLite iOS benchmark app supports profiling metal operator performance? If not, how to profile the operator performance on metal?", "comments": ["I try to enable the \"enable_op_profiling\" option for benchmark. The output for metal now is:\r\n\r\n> Duplicate flags: num_threads\r\n> Log parameter values verbosely: [0]\r\n> Min num runs: [20]\r\n> Inter-run delay (seconds): [-1]\r\n> Benchmark name: [mobile_net_benchmark]\r\n> Min warmup runs: [1]\r\n> Graph: [/private/var/containers/Bundle/Application/E7A4C2AA-FBD7-4970-A38A-6472C8AC564D/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite]\r\n> Input layers: [input]\r\n> Input shapes: [1,224,224,3]\r\n> Enable op profiling: [1]\r\n> Use gpu: [1]\r\n> GPU delegate wait type: [aggressive]\r\n> Loaded model /private/var/containers/Bundle/Application/E7A4C2AA-FBD7-4970-A38A-6472C8AC564D/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite\r\n> 2020-10-28 14:41:14.151361+0800 TFLiteBenchmark[3921:935759] Initialized TensorFlow Lite runtime.\r\n> 2020-10-28 14:41:14.159952+0800 TFLiteBenchmark[3921:935759] Created TensorFlow Lite delegate for Metal.\r\n> 2020-10-28 14:41:14.161967+0800 TFLiteBenchmark[3921:935759] Metal GPU Frame Capture Enabled\r\n> 2020-10-28 14:41:14.166016+0800 TFLiteBenchmark[3921:935759] Metal API Validation Enabled\r\n> 2020-10-28 14:41:14.359915+0800 TFLiteBenchmark[3921:935759] Following operations are not supported by GPU delegate:\r\n> SQUEEZE: Operation is not supported.\r\n> 29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n> 2020-10-28 14:41:14.611665+0800 TFLiteBenchmark[3921:935759] [Metal Compiler Warning] Warning: Compilation succeeded with: \r\n> \r\n> program_source:46:11: warning: unused variable 'gid'\r\n>     uint3 gid = uint3(0u, 0u, uint(linear_index));\r\n>           ^\r\n> Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\n> The input model file size (MB): 16.9008\r\n> Initialized session in 476.042ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=90 first=9171 curr=5229 min=4540 max=9219 avg=5581.44 std=920\r\n> \r\n> Running benchmark for at least 20 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=165 first=4757 curr=6095 min=4710 max=39994 avg=6034.32 std=2914\r\n> \r\n> Inference timings in us: Init: 476042, First inference: 9171, Warmup (avg): 5581.44, Inference (avg): 6034.32\r\n> Profiling Info for Benchmark Initialization:\r\n> ============================== Run Order ==============================\r\n> \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n> \t ModifyGraphWithDelegate\t            0.000\t  265.073\t  265.073\t 99.989%\t 99.989%\t     0.000\t        1\tModifyGraphWithDelegate/0\r\n> \t         AllocateTensors\t          265.079\t    0.030\t    0.015\t  0.011%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n> \r\n> ============================== Top by Computation Time ==============================\r\n> \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n> \t ModifyGraphWithDelegate\t            0.000\t  265.073\t  265.073\t 99.989%\t 99.989%\t     0.000\t        1\tModifyGraphWithDelegate/0\r\n> \t         AllocateTensors\t          265.079\t    0.030\t    0.015\t  0.011%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n> \r\n> Number of nodes executed: 2\r\n> ============================== Summary by node type ==============================\r\n> \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n> \t ModifyGraphWithDelegate\t        1\t   265.073\t    99.989%\t    99.989%\t     0.000\t        1\r\n> \t         AllocateTensors\t        1\t     0.030\t     0.011%\t   100.000%\t     0.000\t        2\r\n> \r\n> Timings (microseconds): count=1 curr=265103\r\n> Memory (bytes): count=0\r\n> 2 nodes observed\r\n> \r\n> \r\n> \r\n> Operator-wise Profiling Info for Regular Benchmark Runs:\r\n> ============================== Run Order ==============================\r\n> \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n> \t     TfLiteMetalDelegate\t            0.000\t    4.753\t    6.031\t 99.954%\t 99.954%\t     0.000\t        1\t[MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd]:31\r\n> \t                 SQUEEZE\t            6.031\t    0.000\t    0.001\t  0.009%\t 99.963%\t     0.000\t        1\t[MobilenetV1/Logits/SpatialSqueeze]:29\r\n> \t                 SOFTMAX\t            6.032\t    0.002\t    0.002\t  0.037%\t100.000%\t     0.000\t        1\t[MobilenetV1/Predictions/Reshape_1]:30\r\n> \r\n> ============================== Top by Computation Time ==============================\r\n> \t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n> \t     TfLiteMetalDelegate\t            0.000\t    4.753\t    6.031\t 99.954%\t 99.954%\t     0.000\t        1\t[MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd]:31\r\n> \t                 SOFTMAX\t            6.032\t    0.002\t    0.002\t  0.037%\t 99.991%\t     0.000\t        1\t[MobilenetV1/Predictions/Reshape_1]:30\r\n> \t                 SQUEEZE\t            6.031\t    0.000\t    0.001\t  0.009%\t100.000%\t     0.000\t        1\t[MobilenetV1/Logits/SpatialSqueeze]:29\r\n> \r\n> Number of nodes executed: 3\r\n> ============================== Summary by node type ==============================\r\n> \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n> \t     TfLiteMetalDelegate\t        1\t     6.031\t    99.967%\t    99.967%\t     0.000\t        1\r\n> \t                 SOFTMAX\t        1\t     0.002\t     0.033%\t   100.000%\t     0.000\t        1\r\n> \t                 SQUEEZE\t        1\t     0.000\t     0.000%\t   100.000%\t     0.000\t        1\r\n> \r\n> Timings (microseconds): count=165 first=4755 curr=6095 min=4710 max=39994 avg=6033.93 std=2914\r\n> Memory (bytes): count=0\r\n> 3 nodes observed\r\n> \r\n> \r\n> \r\n> \r\n> Profile (1 threads):\r\n> \r\n> Thread 0 (4 samples)\r\n> \r\n> * 100.00% Softmax\r\n>   * 100.00% Softmax/Impl\r\n\r\nI noticed the output saying that the 'suqeeze' operator is not supported by gpu delegate, and thus it is executed on CPU. So does this output indicating that all operators that supported by the metal delegate are merged into a single kernel(i.e., the TfLiteMetalDelegate kernel in the table)? Or these operators are still executed individually but the profiler treats them as whole.\r\n", "hey guys, any update?\r\nI tried to insert commit&waitUntilCompleted after each metal kernel, but I think these modifications will incur obvious performance penalty", "I met similar problem when profiling TFLite OpenCL backend. There is no per-op running-time even `--enable_op_profiling=true` is set. Hope someone's help.\r\n```\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t     TfLiteGpuDelegateV2\t            0.008\t    5.112\t    5.061\t100.000%\t100.000%\t     0.000\t        1\t[MobilenetV3/Predictions/Softmax]:110\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t     TfLiteGpuDelegateV2\t            0.008\t    5.112\t    5.061\t100.000%\t100.000%\t     0.000\t        1\t[MobilenetV3/Predictions/Softmax]:110\r\n\r\nNumber of nodes executed: 1\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     TfLiteGpuDelegateV2\t        1\t     5.061\t   100.000%\t   100.000%\t     0.000\t        1\r\n\r\nTimings (microseconds): count=186 first=5112 curr=5142 min=4457 max=5621 avg=5061.18 std=246\r\n\r\n```", "> _Please make sure that this is an issue related to performance of TensorFlow. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:performance_template_\r\n> \r\n> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 14.0.1\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone xs Max\r\n> * TensorFlow installed from (source or binary): source\r\n> * TensorFlow version (use command below): github master with commit: [6f58b4d](https://github.com/tensorflow/tensorflow/commit/6f58b4d46ee25633052a844531c3151affdf3635)\r\n> * Python version:  3.7.6\r\n> * Bazel version (if compiling from source): 3.1.0\r\n> * GCC/Compiler version (if compiling from source):  Apple clang version 12.0.0\r\n> * CUDA/cuDNN version: N/A\r\n> * GPU model and memory: N/A\r\n> \r\n> You can collect some of this information using our environment capture\r\n> [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n> You can also obtain the TensorFlow version with:\r\n> \r\n> 1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n> 2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n> \r\n> **Describe the current behavior**\r\n> The iOS benchmark app could show detailed performance profiling results for cpu. But with metal delegate, no detailed results are shown.\r\n> \r\n> **Describe the expected behavior**\r\n> Shwoing defailed profiling results for metal gpu delegate like CPU\r\n> \r\n> **Other info / logs** Include any logs or source code that would be helpful to\r\n> diagnose the problem. If including tracebacks, please include the full\r\n> traceback. Large logs and files should be attached.\r\n> The output in xcode console output section for cpu is shown below:\r\n> \r\n> > Profile (4 threads):\r\n> > Thread 0 (932 samples)\r\n> > \r\n> > * 70.71% Conv\r\n> >   \r\n> >   * 65.77% cpu_backend_gemm::Gemm\r\n> >     \r\n> >     * 63.73% Mul\r\n> >       \r\n> >       * 28.33% matmul shape: 512x512x196\r\n> >         \r\n> >         * 28.33% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 28.33% TrMulImpl, general case\r\n> >             \r\n> >             * 16.09% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 10.41% [other]\r\n> >             * 1.82% Pack (kNeon, optimized for out-of-order cores)\r\n> >       * 6.87% matmul shape: 1024x1024x49\r\n> >         \r\n> >         * 6.87% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 6.87% TrMulImpl, general case\r\n> >             \r\n> >             * 3.86% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 2.15% Pack (kNeon, optimized for out-of-order cores)\r\n> >             * 0.86% [other]\r\n> >       * 6.12% matmul shape: 256x256x784\r\n> >         \r\n> >         * 6.12% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 6.12% TrMulImpl, general case\r\n> >             \r\n> >             * 3.65% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 2.36% [other]\r\n> >             * 0.11% Pack (kNeon, optimized for out-of-order cores)\r\n> >       * 4.51% matmul shape: 128x128x3136\r\n> >         \r\n> >         * 4.51% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 4.51% TrMulImpl, general case\r\n> >             \r\n> >             * 3.54% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 0.54% [other]\r\n> >             * 0.43% Pack (kNeon, optimized for out-of-order cores)\r\n> >       * 3.76% matmul shape: 64x32x12544\r\n> >         \r\n> >         * 3.76% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 3.76% TrMulImpl, general case\r\n> >             \r\n> >             * 2.90% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 0.86% [other]\r\n> >       * 3.43% matmul shape: 1024x512x49\r\n> >         \r\n> >         * 3.43% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 3.43% TrMulImpl, general case\r\n> >             \r\n> >             * 1.61% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 0.97% Pack (kNeon, optimized for out-of-order cores)\r\n> >             * 0.86% [other]\r\n> >       * 2.90% matmul shape: 512x256x196\r\n> >         \r\n> >         * 2.90% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 2.90% TrMulImpl, general case\r\n> >             \r\n> >             * 1.50% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 1.07% [other]\r\n> >             * 0.32% Pack (kNeon, optimized for out-of-order cores)\r\n> >       * 2.90% matmul shape: 256x128x784\r\n> >         \r\n> >         * 2.90% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 2.90% TrMulImpl, general case\r\n> >             \r\n> >             * 1.61% [other]\r\n> >             * 1.29% Kernel (kNeon, optimized for out-of-order cores)\r\n> >       * 2.68% matmul shape: 128x64x3136\r\n> >         \r\n> >         * 2.68% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 2.68% TrMulImpl, general case\r\n> >             \r\n> >             * 1.93% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 0.54% [other]\r\n> >             * 0.21% Pack (kNeon, optimized for out-of-order cores)\r\n> >       * 2.25% matmul shape: 32x27x12544\r\n> >         \r\n> >         * 2.25% TrMul (Path=0x10, max_num_threads=4, is_prepacked=(0,0))\r\n> >           \r\n> >           * 2.25% TrMulImpl, general case\r\n> >             \r\n> >             * 1.29% Kernel (kNeon, optimized for out-of-order cores)\r\n> >             * 0.86% [other]\r\n> >             * 0.11% GetBlockMatrixCoords\r\n> >     * 2.04% cpu_backend_gemm::Gemm: CustomGemv\r\n> >   * 4.83% Im2col\r\n> >     \r\n> >     * 3.00% ExtractPatchIntoBufferColumn\r\n> >     * 1.82% [other]\r\n> >   * 0.11% [other]\r\n> > * 29.08% DepthwiseConv\r\n> >   \r\n> >   * 22.85% [other]\r\n> >   * 6.22% DepthwiseConv/float/DepthwiseConvImpl\r\n> >     \r\n> >     * 4.83% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n> >     * 1.39% [other]\r\n> > * 0.21% AveragePool\r\n> > \r\n> > Thread 1 (718 samples)\r\n> > \r\n> > * 67.55% Kernel (kNeon, optimized for out-of-order cores)\r\n> > * 25.63% DepthwiseConv/float/DepthwiseConvImpl\r\n> >   \r\n> >   * 21.31% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n> >   * 4.32% [other]\r\n> > * 6.69% Pack (kNeon, optimized for out-of-order cores)\r\n> > * 0.14% GetBlockMatrixCoords\r\n> > \r\n> > Thread 2 (681 samples)\r\n> > \r\n> > * 65.49% Kernel (kNeon, optimized for out-of-order cores)\r\n> > * 26.73% DepthwiseConv/float/DepthwiseConvImpl\r\n> >   \r\n> >   * 21.44% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n> >   * 5.29% [other]\r\n> > * 7.64% Pack (kNeon, optimized for out-of-order cores)\r\n> > * 0.15% GetBlockMatrixCoords\r\n> > \r\n> > Thread 3 (645 samples)\r\n> > \r\n> > * 66.82% Kernel (kNeon, optimized for out-of-order cores)\r\n> > * 25.27% DepthwiseConv/float/DepthwiseConvImpl\r\n> >   \r\n> >   * 20.16% void tflite::optimized_ops::FloatDepthwiseConvAccumRow(int, int, int, int, const float *, int, int, int, const float *, int, int, int, float *) [kAllowStrided = true, kFixedInputDepth = 0, kFixedDepthMultiplier = 1]\r\n> >   * 5.12% [other]\r\n> > * 7.60% Pack (kNeon, optimized for out-of-order cores)\r\n> > * 0.16% GetBlockByIndex\r\n> > * 0.16% GetBlockMatrixCoords\r\n> \r\n> While for metal gpu delegates, the output is:\r\n> \r\n> > Duplicate flags: num_threads\r\n> > Log parameter values verbosely: [0]\r\n> > Min num runs: [20]\r\n> > Inter-run delay (seconds): [-1]\r\n> > Benchmark name: [mobile_net_benchmark]\r\n> > Min warmup runs: [1]\r\n> > Graph: [/private/var/containers/Bundle/Application/29E02AE1-F22E-4454-A049-042C04929CBE/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite]\r\n> > Input layers: [input]\r\n> > Input shapes: [1,224,224,3]\r\n> > Use gpu: [1]\r\n> > GPU delegate wait type: [aggressive]\r\n> > Loaded model /private/var/containers/Bundle/Application/29E02AE1-F22E-4454-A049-042C04929CBE/TFLiteBenchmark.app/mobilenet_v1_1.0_224.tflite\r\n> > 2020-10-28 14:21:10.624179+0800 TFLiteBenchmark[3858:924590] Initialized TensorFlow Lite runtime.\r\n> > 2020-10-28 14:21:10.632180+0800 TFLiteBenchmark[3858:924590] Created TensorFlow Lite delegate for Metal.\r\n> > 2020-10-28 14:21:10.635001+0800 TFLiteBenchmark[3858:924590] Metal GPU Frame Capture Enabled\r\n> > 2020-10-28 14:21:10.638319+0800 TFLiteBenchmark[3858:924590] Metal API Validation Enabled\r\n> > 2020-10-28 14:21:10.876638+0800 TFLiteBenchmark[3858:924590] Following operations are not supported by GPU delegate:\r\n> > SQUEEZE: Operation is not supported.\r\n> > 29 operations will run on the GPU, and the remaining 2 operations will run on the CPU.\r\n> > 2020-10-28 14:21:11.164313+0800 TFLiteBenchmark[3858:924590] [Metal Compiler Warning] Warning: Compilation succeeded with:\r\n> > program_source:46:11: warning: unused variable 'gid'\r\n> > uint3 gid = uint3(0u, 0u, uint(linear_index));\r\n> > ^\r\n> > Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\n> > The input model file size (MB): 16.9008\r\n> > Initialized session in 558.992ms.\r\n> > Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> > count=86 first=9867 curr=4858 min=4704 max=9867 avg=5813.1 std=1163\r\n> > Running benchmark for at least 20 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> > count=183 first=5456 curr=5688 min=4690 max=29198 avg=5447.84 std=1850\r\n> > Inference timings in us: Init: 558992, First inference: 9867, Warmup (avg): 5813.1, Inference (avg): 5447.84\r\n> > Profile (0 threads):\r\n> \r\n> Deos the TFLite iOS benchmark app supports profiling metal operator performance? If not, how to profile the operator performance on metal?\r\n\r\nNo, the app doesn't support profiling metal operators. When \"--enable_op_profiling=true\", profiling events are emitted via tflite::Profiler APIs. However, the current GPU delegate implementation hasn't used such APIs to export profiling events. \r\n\r\n@impjdi, could you shed some light on how to profile gpu delegate implementation itself? Many thanks!"]}, {"number": 44358, "title": "Tensorflow randomly crashes running multiple consecutive images through classifier model TF_SessionRun() in c api", "body": "Hi, \r\n\r\nI am hesitant to call this a bug, only because it seems like an obvious use case.  On the other hand, the c api looks pretty basic to me, so I can't figure how what I could be missing here.   Here's all the information I can think of.   Let me know if you need anything else.  I'll be more than happy to oblige.  Thank you.  \r\n\r\n**System information**\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Debian 4.19.132-1 (2020-07-24)\r\nos release version: 4.19.0-10-amd64\r\nos platform: Linux-4.19.0-10-amd64-x86_64-with-debian-10.5\r\nlinux distribution: ('debian', '10.5', '')\r\nlinux os distribution: ('debian', '10.5', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: ('Linux', 'starkdg', '4.19.0-10-amd64', '#1 SMP Debian 4.19.132-1 (2020-07-24)', 'x86_64', '')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n== compiler ====================================================\r\ngcc (Debian 8.3.0-6) 8.3.0\r\n\r\nTensorflow version: 2.3.1\r\n\r\n**Describe the current behavior**\r\nPerforms as expected, except for random crashes on different files - oddly, never the same file. When the same\r\nfiles are submitted individually, the graph session performs as expected.  There are no log messages to report from tensorflow, just a string of filesystem errors.  Like this:\r\n\r\n```\r\nEXT4-fs error (device sda1): ext4_find_entry:1456: inode #390913: common systemd-journal: reading directory lblock 0\r\n```\r\n\r\nWhen this occurs, the only thing possible is a hard reboot.  \r\n\r\nA fsck on those file systems reports all fine.  \r\n\r\n**Describe the expected behavior**\r\n\r\nThe program should read all the images in a given directory.  For each image, read the image data, run it through model \r\nclassifier [mobilenetv2](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz) (Use the frozen.pb model in the download archive), and obtain the feature vector for that image from the model.    \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere is a basic program to reproduce the issue.  I tried to minimize it as much as possible, but it requires\r\nopencv (v3.4.7) and Boost filesystem and system libraries (v1.67.0)  in order to retrieve all files inside a directory and the\r\nimage data for each file.   The directory of images must contain all images  that opencv installation can read. \r\n\r\nOpenCV  v3.4.7\r\nBoost  v1.67.0\r\n\r\n```\r\nInvoke program:  ./tftest /path/to/image/files/dir\r\n```\r\n\r\nPut the frozen .pb model file in the same directory as the example code.  or change the `model_file` variable in the code so it can read in the model graph.\r\n\r\n```\r\n/** \r\n\tg++ -otftest tftest.cpp -g -O0 -Wall -lboost_filesystem -lboost_system \r\n                   -lopencv_core -lopencv_imgcodecs -lopencv_imgproc -ltensorflow\r\n **/\r\n#include <iostream>\r\n#include <cstdlib>\r\n#include <string>\r\n#include <cassert>\r\n#include <boost/filesystem.hpp>\r\n#include <opencv2/core/core_c.h>\r\n#include <opencv2/imgproc/imgproc_c.h>\r\n#include <opencv2/imgcodecs/imgcodecs_c.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nusing namespace std;\r\nusing namespace cv;\r\n\r\nnamespace fs=boost::filesystem;\r\n\r\nstatic const string model_file = \"mobilenet_v2_1.4_224_frozen.pb\";       //location of frozen model file \r\nstatic const string input_layer = \"input\";                                                   // input output layers\r\nstatic const string output_layer = \"MobilenetV2/Logits/AvgPool\";\r\n\r\nstatic int Height = 224;   // target image size \r\nstatic int Width = 224;\r\n\r\nvoid NoOpDeAlloc(void *data, size_t len, void *arg){}\r\n\r\nvoid freebuffer(void *data, size_t len){\r\n\tfree(data);\r\n}\r\n\r\nCvMat* load_image(const string &filename, const int height, const int width){\r\n\r\n\tCvMat *src = cvLoadImageM(filename.c_str(), CV_LOAD_IMAGE_COLOR);\r\n\tif (src == NULL) return NULL;\r\n\r\n\tCvMat *img_prime = cvCreateMat(src->rows, src->cols, CV_32FC3);\r\n\tCvMat *img_resized = cvCreateMat(height, width, CV_32FC3);\r\n\tCvMat *dst = cvCreateMat(height, width, CV_32FC3);\r\n\tCvMat *div = cvCreateMat(height, width, CV_32FC3);\r\n\r\n\t/* convert to float type */\r\n\tcvConvertScale(src, img_prime, 1, 0);\r\n\r\n\t/* resize to heightxwidth */\r\n\tcvResize(img_prime, img_resized, CV_INTER_CUBIC);\r\n\r\n\t/* normalize values */\r\n\tCvScalar norm_constant = {255, 255, 255};\r\n\tcvSet(div, norm_constant, NULL);\r\n\tcvDiv(img_resized, div, dst, 1);\r\n\r\n\tcvReleaseMat(&src);\r\n\tcvReleaseMat(&img_resized);\r\n\tcvReleaseMat(&img_prime);\r\n\tcvReleaseMat(&div);\r\n\treturn dst;\r\n}\r\n\r\nTF_Buffer* readfromfile(const string &filename){\r\n\tFILE *file = fopen(filename.c_str(), \"rb\");\r\n\tif (file == NULL) return NULL;\r\n\t\r\n\tfseek(file, 0, SEEK_END);\r\n\tsize_t file_size = ftell(file);\r\n\tfseek(file, 0, SEEK_SET);\r\n\tassert(file_size > 0);\r\n\t\r\n\tunsigned char *data = (unsigned char*)malloc(file_size);\r\n\tsize_t n = fread(data, 1, file_size, file);\r\n\tassert(n == file_size);\r\n\r\n\tTF_Buffer *buf = TF_NewBuffer();\r\n\tbuf->data = data;\r\n\tbuf->length = file_size;\r\n\tbuf->data_deallocator = freebuffer;\r\n\tfclose(file);\r\n\treturn buf;\r\n}\r\n\r\n\r\nint ProcessFiles(const fs::path &dirname){\r\n\tfs::recursive_directory_iterator dir(dirname), end;\r\n\r\n\tint ndims = 4;\r\n\tint64_t dims[4] = { 1, Height, Width, 3};\r\n\r\n\tcout << \"TF Model graph setup\" << endl;\r\n\tTF_Graph *graph = TF_NewGraph();\r\n\tTF_Status *status = TF_NewStatus();\r\n\r\n\tTF_ImportGraphDefOptions *opts = TF_NewImportGraphDefOptions();\r\n\tTF_Buffer *graph_def = readfromfile(model_file);\r\n\tassert(graph_def != NULL);\r\n\r\n\tTF_GraphImportGraphDef(graph, graph_def, opts, status);\r\n\tassert(TF_GetCode(status) == TF_OK);\r\n\t\r\n\tTF_SessionOptions *sess_opts = TF_NewSessionOptions();\r\n\tTF_Session *sess = TF_NewSession(graph, sess_opts, status);\r\n\tTF_Tensor *img_tensor;\r\n\tTF_Tensor *output_tensor;\r\n\tTF_Output input = { TF_GraphOperationByName(graph, input_layer.c_str()), 0 };\r\n\tTF_Output output = { TF_GraphOperationByName(graph, output_layer.c_str()), 0 };\r\n\tassert(input.oper != NULL);\r\n\tassert(output.oper != NULL);\r\n\t\r\n\tcout << \"Read files \" << endl;\r\n\t\r\n\tint count = 0;\r\n\tfor (;dir!=end;++dir){\r\n\t\tstring filename = dir->path().string();\r\n\t\tif (fs::is_directory(dir->status())){\r\n\t\t\tcout << \"directory: \" << filename << endl;\r\n\t\t} else if (fs::is_regular_file(dir->status())){\r\n\t\t\tCvMat *img = load_image(filename, Height, Width);\r\n\t\t\tif (img != NULL){\r\n\t\t\t\tcout << dec << \"(\" << ++count << \") \" << filename << endl;\r\n\r\n\t\t\t\tint total = Height*Width*3;\r\n\t\t\t\timg_tensor = TF_NewTensor(TF_FLOAT, dims, ndims, (void*)img->data.fl,\r\n\t\t\t\t\t\t\t\t\t\t  total*sizeof(float), NoOpDeAlloc, NULL);\r\n\t\t\t\tassert(img_tensor != NULL);\r\n\t\t\t\t\r\n\t\t\t\tTF_SessionRun(sess, NULL, &input, &img_tensor, 1,\r\n\t\t\t\t\t\t\t  &output, &output_tensor, 1,\r\n\t\t\t\t\t\t\t  NULL, 0, NULL, status);\r\n\r\n\t\t\t\tif (TF_GetCode(status) != TF_OK){\r\n\t\t\t\t\tcout << \"unale to complete session: \" << TF_Message(status) << endl;\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\r\n\t\t\t\tTF_DeleteTensor(img_tensor);\r\n\t\t\t\tTF_DeleteTensor(output_tensor);\r\n\t\t\t\timg_tensor = NULL;\r\n\t\t\t\toutput_tensor = NULL;\r\n\t\t\t\tcvReleaseMat(&img);\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tcout << \"special file: \" << filename << endl;\r\n\t\t}\r\n\t}\r\n\r\n\tTF_DeleteBuffer(graph_def);\r\n\tTF_DeleteSessionOptions(sess_opts);\r\n\tTF_DeleteImportGraphDefOptions(opts);\r\n\tTF_DeleteGraph(graph);\r\n\tTF_DeleteSession(sess, status);\r\n\tTF_DeleteStatus(status);\r\n\treturn count;\r\n}\r\n\r\nint main (int argc, char **argv){\r\n\tassert(argc == 2);\r\n\tstring dir = argv[1];\r\n\r\n\r\n\tcout << \"Tensorflow: \" << TF_Version() << endl;\r\n\t\r\n\tcout << \"directory: \" << dir << endl;\r\n\t\r\n\tint n = ProcessFiles(dir);\r\n\t\r\n\tcout << \"Processed files: \" << n << endl;\r\n\tcout << \"Done\" << endl;\r\n\treturn 0;\r\n}\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n", "comments": ["@starkdg \r\nWhat is the tf version you are facing the issue on.", "@Sadaf2019    \r\nTensorflow version 2.3.0 ", "Just tried compiling from source from the master branch - version 2.5.0 , but same problem.  \r\nAnyone have any ideas ?  "]}, {"number": 44354, "title": "Enable control of use of caching in tf.keras.preprocessing.image_dataset_from_directory", "body": "**System information**\r\n- TensorFlow version (you are using): TF 2.3.0 \r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis concerns the API tf.keras.preprocessing.image_dataset_from_directory(...) I read through the doc as well as the source code, it doesn't seem like there's any control over behavior concerning cache() for the tf.data.Dataset. As best I could understand this, it won't be right to apply .cache(...) to the output tf dataset of this API since shuffling is done (esp. for train) and caching anything after, it freezes the initial shuffling order, such that it stays fixed at subsequent epochs. According to general wisdom, this is highly likely not desirable. The caching should be done before shuffling, but there's no way to specify this at API call time. The only workaround I could think of is to copy/paste the exact code, and write your own version.\r\n\r\n\r\n**Will this change the current api? How?**\r\nOne can add this feature without affecting old behavior. Cache is simply assumed False unless supplied otherwise, and if it is false, the old code path should execute.\r\n\r\n**Who will benefit with this feature?**\r\nimage_dataset_from_directory(...) is a nice convenient method and caching may improve performance by not repeating the image resizing processing. It can also help if your original directories are remote and may suffer outage or unpredictable lag.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 44348, "title": "CTC Error crashes on empty GPU batch", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GeForce RTX 2080 Ti 11GB x8\r\n\r\nWhen I run the following notebook with 8 GPUs activated, it fails:\r\nhttps://colab.research.google.com/drive/1yU02l0WtSW2mp7Ai-UvUF18Z2vbNPbzy?usp=sharing\r\n\r\nIf I run it with 1 GPU, or on CPU, it works. If I change \"valnum\" from 65 to 64, it works. \r\n\r\nIt seems like the CTC Error function doesn't handle empty batches being distributed to GPUs?\r\n\r\nThe error messages when it fails are mostly variations of:\r\n`Invalid argument:  Tried to stack elements of an empty list with non-fully-defined element_shape: [1,?]\r\n\t [[node replica_6/functional_5/ctc/scan/TensorArrayV2Stack/TensorListStack (defined at <ipython-input-3-6220b8957bcc>:14) ]]\r\n\t [[replica_2/functional_5/ctc/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int32_Squeeze_1/_499]]`\r\n\r\nFull error log: [errorlog.txt](https://github.com/tensorflow/tensorflow/files/5445445/errorlog.txt)\r\n\r\n(This has been a problem on a transfer learning application, where I have a very large training set and need to use all 8 GPUs, but a very small validation set, which crashes if I use all 8 GPUs.)", "comments": ["@creidieki,\r\nEach TensorFlow release is compatible with certain CUDA and cuDNN versions. TensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu). \r\n\r\nCould you please check if you are facing the same issue with CUDA 10.1 and cuDNN 7.6? Thanks!", "Sorry, I had been reading the CUDA version off of \"nvidia-smi\". When I look at the console output, it does say it's loading \"libcudart.so.10.1\" and \"libcudnn.so.7\". Is that what you need?", "To clarify my previous comment, this is happening with the stock Ubuntu packages. I haven't recompiled tensorflow against an unsupported version of libcuda, or force-installed any packages without their dependencies, or anything like that. I really think it's a bug, because it either works or crashes depending entirely on whether one of the GPUs receives an empty batch during training.", "Hi @creidieki, I was able to reproduce this on GCP with 4 GPUs if I dropped the `valnum` to 33. 32 and 34 worked okay. To try and isolate what could be causing the problem, have you tried this using `tf.data` instead of numpy? Also is this only happening with the use of ctc loss?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "It seems to only happen with the ctc loss. If I change the loss function to:\r\n`def ctc_lambda_func(args):\r\n    return K.constant(1, shape=(24, 1))`\r\nit doesn't crash. \r\n\r\nI don't know about the tf.data question, because the documentation for tf.data.Dataset doesn't cover creating a dataset from a dictionary of named inputs. I tried adding the line \r\n`inputs = tf.data.Dataset.from_tensor_slices(inputs)`\r\njust before model.predict but it didn't work.", "Since this has been confirmed as a bug, could the label \"Support\" be removed? This is still affecting me. Thanks.", "We're encountering this issue as well. Did you find any workaround?", "I ended up having to rearrange things so that there would never be partial batches. So I've been doing things like:\r\n* choosing a validation set size of 120, so that it would divide evenly among 1, 2, 3, 4, 5, 6, or 8 GPUs\r\n* given n training examples, throwing away the last (n % 32) of them, so that all training batches were size 32\r\n\r\nI'm sorry I can't offer anything better."]}, {"number": 44346, "title": "Problems generating the Hello-World-Project von ESP32", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linuxmint in VirtualBox\r\n- TensorFlow installed from (source or binary): viva git clone https://github.com/tensorflow/tensorflow.git\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform: ESP32 \r\n\r\n**Describe the problem**\r\nHello guys,\r\n\r\nI can't generate projects for ESP32 using make. If  I run:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\n\r\nI get:\r\ntensorflow/lite/micro/tools/make/Makefile:418: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:418: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:378: tensorflow/lite/micro/tools/make/targets/esp_makefile.inc: No such file or directory\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/esp_makefile.inc'. Stop.\r\n\r\n\r\nMaybe this issue is alrady fixed:\r\nhttps://github.com/tensorflow/tensorflow/pull/44316\r\nIf this is so, a big \"Thank you!\" do the person how did this! \r\nWhere do I get the fixed version? (Sry, I'm new to this topic)\r\n\r\nThank you for all attempts! \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\nafter clone git:\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n", "comments": ["I tried out the Fix attempted of https://github.com/tensorflow/tensorflow/pull/44316 but it has the same error.\r\n\r\nI changed tensorflow/lite/micro/tools/make/targets/esp32_makefile.inc to tensorflow/lite/micro/tools/make/targets/esp_makefile.inc \r\nannd the containt of the Makefile from:\r\n\r\n# Settings for Espressif ESP32\r\n\r\nifeq ($(TARGET), esp)\r\n  TARGET_ARCH := xtensa-esp32\r\n  CCFLAGS := $(filter-out -std=c11,$(CCFLAGS))\r\n  CFLAGS += -std=c11\r\nendif\r\n\r\nto:\r\n\r\n# Settings for Espressif ESP32\r\n\r\n\r\nTARGET_ARCH := xtensa-esp32\r\nCCFLAGS := $(filter-out -std=c11,$(CCFLAGS))\r\nCFLAGS += -std=c11\r\n\r\nSo that doesn't help me. But that file could be the problem, it is defintily named wrong.\r\nGreetings,\r\nJan", "@Jan08 \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/29524#issuecomment-683521537) and let us know if it helps, a;so please let us know the tf version used.", "Hi, \r\nI followed the Link nur that has no solution...\r\nThe Problem is realy Connect to: https://github.com/tensorflow/tensorflow/pull/44316\r\n\r\nThe Makefile is Named esp32_makefile.inc\r\nMit esp_makefile.inc,  Like the Error response to.\r\nBut simply rename it dont fix the Problem.\r\n\r\nGreetings", "Hello...\r\n\r\nDid you solve your problem?\r\nUpdate TensorFlow repo to newest and try the build.\r\nIt might be that simply picking up the fix doesn't work on some particular commit.\r\n\r\nAlso, can you please share new logs that you see?\r\n", "This is still a problem in v2.4.0-rc1 that was just created.  Is a new bug report required to get it fixed in 2.4 before final 2.4 release?  I am manually renaming the .inc file to keep working but prefer it be checked in properly to the repo. Thanks.\r\nEdit: Just renaming that one file allows the examples to build, no other file changes needed.", "Hello, I've just checked with version 2.5.0 and ci_build/test_esp32.sh. I got :\r\n tensorflow/lite/micro/tools/ci_build/helper_functions.sh: line 31: ./install.sh: No such file or directory\r\nIDF_PATH variable is in fact empty line 36 : 'cd $IDF_PATH'\r\nNotice also that you cannot run the test_esp32.sh twice because the '/tensorflow/lite/micro/tools/ci_build/../../../../../esp-idf\r\n' directory is not removed/cleaned.", "@evaderan-lab \r\nThanks for reporting.\r\n\r\nThis is something because of `readable_run export ` command not working as expected.\r\nFixed this with following PR: https://github.com/tensorflow/tensorflow/pull/45334"]}, {"number": 44337, "title": "Concrete Function output shape sometimes changes after save/load cycle", "body": "Output of environment capture script:\r\n\r\n```\r\n== check python ===================================================\r\npython version: 3.7.8\r\npython branch:\r\npython build version: ('default', 'Aug 10 2020 13:15:25')\r\npython compiler version: Clang 10.0.0 (clang-1000.10.44.4)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1/RELEASE_X86_64\r\nos release version: 17.7.0\r\nos platform: Darwin-17.7.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.13.6', ('', '', ''), 'x86_64')\r\nuname: uname_result(system='Darwin', node='...', release='17.7.0', version='Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1/RELEASE_X86_64', machine='x86_64', processor='i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 10.0.0 (clang-1000.10.44.4)\r\nTarget: x86_64-apple-darwin17.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                         1.18.5\r\nprotobuf                      3.10.0\r\ntensorflow                    2.3.1\r\ntensorflow-estimator          2.3.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.3.1\r\ntf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.3.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: .../python3.7/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 8, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nThe `structured_outputs` of a concrete function might not match after saving and restoring a model containing that function via tf.saved_model. In the example below, the function doesn't change the shape of the tensor, but, when requesting a concrete function for a tensor with shape [None, 3], a more specific output [7, 3] is returned instead.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe output shape should not be more specific than the true output.\r\n\r\n**Standalone code to reproduce the issue**\r\nReproduction script and stdout of `for i in $(seq 10); do echo $i; python repro.py; done` in this Gist: https://gist.github.com/gmacon/057cc64bf849c8d65974daec56a037b7\r\n\r\n**Other info / logs**\r\nInitially, I thought this bug was deterministic, but it's clearly actually random. Based on running the reproduction script in a loop, it appears to work correctly about half of the time. Question: does this have something to do with the traversal order of a randomized hash table?\r\n\r\nEdit again: I initially thought it was deterministic because it was happening every time in the system I'm building. However, I was reusing the same serialized model over and over in those tests. It appears to work in about half the *saves*, not half the *loads*.\r\n\r\n~As far as I can tell, this was working correctly in Tensorflow 2.2.~ Edit: I'm not sure about that any more.\r\n\r\nEdit again again: I reproduced this in Tensorflow 2.0.0.", "comments": ["I am able to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/834c1245c4a14decdbbb2548a93567cc/untitled450.ipynb).", "@gmacon Can you try with `@tf.function(input_signature=(tf.TensorSpec(shape=[None, 3], dtype=tf.float32),))`?", "Providing an explicit `input_signature` seems to work, as far as I can tell. However, that doesn't really help me, because my actual system uses keyword arguments with boolean values as switches, like this [more complicated reproducer](https://gist.github.com/gmacon/67568c9804b283dfec50ba37a8d9d525), and it's my understanding that `input_signature` can't be used in that situation.", "Yes working with kwargs is still in the todo list.\r\nWhy do you require to trace with `t1(np.ones((7, 3), dtype=np.float32))`?", "Providing some explicit data is required to tickle the bug. If you remove that, it will never manifest. In my real system the \"provide some data\" step is really \"train the model with initial data\", so I don't see an obvious way to avoid providing any example data to the model.\r\n\r\nI don't think there's anything special about 7, it's a \"random\" number in this situation.", "@gmacon If you insert \r\n`print(t1.__call__.pretty_printed_concrete_signatures())`\r\n\r\nYou will see:\r\n```\r\nConcreteFunction __call__(x)\r\n  Args:\r\n    x: float32 Tensor, shape=(None, 3)\r\n  Returns:\r\n    float32 Tensor, shape=(None, 3)\r\n__call__(x)\r\n  Args:\r\n    x: float32 Tensor, shape=(7, 3)\r\n  Returns:\r\n    float32 Tensor, shape=(7, 3)\r\n```\r\n\r\nWhen you save the model the saved concrete functions could be in any order. E.g.\r\n```\r\nTensor(\"StatefulPartitionedCall:0\", shape=(7, 3), dtype=float32)\r\nTensor(\"StatefulPartitionedCall_1:0\", shape=(None, 3), dtype=float32)\r\nTensor(\"StatefulPartitionedCall_2:0\", shape=(None, 3), dtype=float32)\r\n```\r\n\r\nThe logic is that it takes the first saved concrete function with a \"compatible\" input over a loop on all the saved concrete functions: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4e281e9a211729a9a57df04dad22d15c988f2919/tensorflow/python/saved_model/function_deserialization.py#L246-L253\r\n\r\nAnd the compatibility check is:\r\nhttps://github.com/tensorflow/tensorflow/blob/4e281e9a211729a9a57df04dad22d15c988f2919/tensorflow/python/saved_model/function_deserialization.py#L92-L119\r\n\r\nSo as the order of the concrete functions change  on save you could enter in any compatible concrete function.", "/cc @andresusanopinto", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4cd1d2483a8e9b0df5f3a7c8fd7cbb4c/44337.ipynb). Thanks!"]}, {"number": 44336, "title": "[TFLite] [GPU delegate] DataLayout::DHWC4 and ObjectType::OPENGL_SSBO input does not work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nexus 5X\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0 / 2.3.1\r\n- Python version: 2.7.16\r\n- Bazel version (if compiling from source): 2.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nThe GPU delegate readme explains how tensors are always converted to the DHWC4 format, which I understand, and by providing the input in the correct format we can avoid a useless copy.\r\n\r\nI'm using a slightly modified build of tensorflow (See [v2.2.0](https://github.com/deepmedia/tensorflow/commit/7401fbb4fa0c94004865c089d8c89bdd566ad747) and [v2.3.1](https://github.com/deepmedia/tensorflow/commit/ac8aad5c55838566a26ed6725c966d399319c831)) in order to inject `OPENGL_SSBO` inputs into the delegate.\r\n\r\n**In v2.2.0**, adding a SSBO with `DataLayout::DHWC4` would fail with a \"Not supported\" error.\r\nThe error was thrown in [these lines](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/lite/delegates/gpu/gl/api2.cc#L138-L141):\r\n\r\n```c++\r\n    // TODO(akulik): external object should propagate to internal.\r\n    if (IsSameDef()) {\r\n      return UnimplementedError(\"Not supported\");\r\n    }\r\n```\r\n\r\n**In v2.3.1**, a commit was added, named \"Fix propagation of objects with same defs.\", which removes this error:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/3e414c752be1a259c64b1f85a24e095d02dd5e2f\r\n\r\nBut it just removes the exception, without actually implementing the propagation. I could not find any other commit that includes the actual implementation. The result is that if I try to pass a DHWC4 SSBO, I get no error, but the input is all zeros. \r\nAm I missing something? If so, can you point me to the code/commit that actually reuses the input SSBO id? \r\n\r\n**Describe the expected behavior**\r\n\r\nWe should be able to send a SSBO buffer with DataLayout::DHWC4 and avoid a useless copy. If it's not implemented, we should get an error.", "comments": ["Acknowledged.  I have forwarded the message to the person who modified that code but is on vacation.  Ping me again in a week or so if you don't hear back.", "@impjdi pinging you as you requested. \r\n\r\nI also want to take this opportunity to ask about the CL delegate - is this feature currently broken for the CL delegate as well? Can we pass DHWC4 there? And does it even help, or maybe it makes it worse (original image is grayscale)? I guess the SSBO will have to be converted to some OpenCL primitive buffer. Not sure if you use 4 channels there", "Thanks for the reminder.  Here's the response I'm forwarding.\r\n\r\nIndeed the v2.2.0 included the issue which you just faced. The check was removed because objects with the same definitions are supported by DefaultTensorTie (see DefaultTensorTie::Init method). The cl you spotted is not enough to solve this issue, there must be 2 more changes in. If we open tensorflow/lite/delegates/gpu/gl/api2.cc history for the 2.3.1 version of TF:\r\nhttps://github.com/deepmedia/tensorflow/commits/v2.3.1%2B/tensorflow/lite/delegates/gpu/gl/api2.cc\r\nWe see these two cls are also in:\r\n1) https://github.com/deepmedia/tensorflow/commit/4000a5c75cdbe49d77bcac93a7f21070a31c4cce#diff-b767dbfafb3ad2d4475896f019215adc48c9318bc3414c0d2b94f7df18cfc1c2\r\n2) https://github.com/deepmedia/tensorflow/commit/dffe6a0e810f4c3d9968ddb56fd58c8f405eb846#diff-b767dbfafb3ad2d4475896f019215adc48c9318bc3414c0d2b94f7df18cfc1c2\r\nThey help to make sure that in case of same defs the link is constructed correctly and handled by DefaultTensorTie. So, basing on the expected behaviour, I'd expect your code should reach this line when initializing the DefaultTie:\r\nhttps://github.com/deepmedia/tensorflow/blob/ac8aad5c55838566a26ed6725c966d399319c831/tensorflow/lite/delegates/gpu/gl/api2.cc#L175\r\nThe fact that you get the empty values in the input tensor means that you likely reach this line instead\r\nhttps://github.com/deepmedia/tensorflow/blob/ac8aad5c55838566a26ed6725c966d399319c831/tensorflow/lite/delegates/gpu/gl/api2.cc#L179\r\nCan you debug a bit around why the conditions are not met there? If is_same_def == false, maybe you requested the internal object of the other data type than the external one and it really needs to be converted and created separately? More details on what inference options you used will help. Also, please share what data type did you use?\r\n\r\nAnswering your question regarding the OpenCL side support. The CL delegate behavior is similar: if DHWC4 ssbo is met the external gl-object will be just copied to internal cl-object with no internal data conversions. \r\n\r\nHope this explanation will help debugging. Feel free to send a pull request if you'll manage to locate and fix the bug.", "@impjdi that's not the case unfortunately, I correctly hit [L175](https://github.com/deepmedia/tensorflow/blob/ac8aad5c55838566a26ed6725c966d399319c831/tensorflow/lite/delegates/gpu/gl/api2.cc#L175), which means that `user_provided` and `is_same_def` are both true. I setup the object def as follows:\r\n\r\n```cpp\r\ntflite::gpu::ObjectDef get_gpu_object_def(bool is_dhwc4) {\r\n    tflite::gpu::ObjectDef object_def;\r\n    object_def.data_type = tflite::gpu::DataType::FLOAT32;\r\n    object_def.data_layout = is_dhwc4 ? tflite::gpu::DataLayout::DHWC4 : tflite::gpu::DataLayout::BHWC;\r\n    object_def.object_type = tflite::gpu::ObjectType::OPENGL_SSBO;\r\n    object_def.user_provided = true;\r\n    return object_def;\r\n}\r\n```\r\n\r\nSo that if I pass is_dhwc4=false, inference works (is_same_def=false), if not, I get blank results.\r\n\r\nGiven that L175 is hit I think the ObjectDef is not the culprit here. What's more suspicious is the SSBO usage/binding, I fail to see where my external SSBO (not the object def, but the concrete SSBO id passed to `InferenceRunner.SetInputObject`) is retrieved to be bound. Maybe it should be registered into the ObjectManager. Anyway this brings the investigation to much lower level components that I'm not confident with. I can easily add log calls though.\r\n\r\nAbout inference options,\r\n\r\n```\r\n auto options = TfLiteGpuDelegateOptionsV2Default();\r\n options.inference_preference = TfLiteGpuInferenceUsage::TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;\r\n options.inference_priority1 = TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;\r\n options.inference_priority2 = TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n options.inference_priority3 = TfLiteGpuInferencePriority::TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n```\r\n", "@impjdi could you please reach back to the author with my answer above? He seemed willing to help :-) in short, all that happens in the DefaultTensorTie is totally fine in my opinion:\r\n\r\n- `user_provided && is_same_def == true` \u2705\r\n- `MaybeAllocateInternalObject` never called \u2705\r\n- `MaybeAllocateExternalObject` never called \u2705\r\n- `external_obj_` contains the SSBO id that I passed to `InferenceRunner.SetInputObject` \u2705\r\n- `CopyFromExternalObject` does nothing because no conversion should take place \u2705\r\n\r\nBut then, I can't see who should bind this SSBO (contained in external_obj_) for the first op. The components that do the actual job (run shaders) seem to take buffers from the ObjectManager, but my external SSBO is never registered there (because it's not allocated by TF in MaybeAllocate*). Is this the issue? \r\n\r\nMy concern with registering it is that maybe it would transfer ownership of the buffer to TF? Which would be bad, app should be responsible to release the SSBO.", "That is correct, to make the external SSBO available for the InferenceRunner, you need to register it prior to calling the InferenceRunner->Initialize() method. Could you please describe in more details the logic, or better share a piece of code, which is responsible fore the initialization of the InferenceRunner? The steps you described before sound reasonable and match the expected way of using delegate's API and its internal behaviour.", "@ekaterinaignasheva thanks a lot for jumping in and for your hints. My logic is much simpler than you might think at this point, I'm using the regular TFLite flow (using Interpreter and TfLiteGpuDelegateV2Create). The only difference is that I have added setters for tensor definition and tensor object to the gpu delegate:\r\n\r\n```cpp\r\nvoid SetExternalObjectDef(int index, ObjectDef object_def); // called before ModifyGraphWithDelegate\r\nvoid SetExternalTensorObject(int index, TensorObject tensor_object); // called before Invoke\r\n```\r\n\r\nSee [here](https://github.com/deepmedia/tensorflow/commit/ac8aad5c55838566a26ed6725c966d399319c831) for the actual implementation. So the whole flow becomes: create interpreter; create the delegate; configure the delegate by passing a DHWC4 definition (using my `SetExternalObjectDef`. see [here](https://github.com/tensorflow/tensorflow/issues/44336#issuecomment-724700457) for actual def); apply the delegate (`ModifyGraphWithDelegate(delegate)`); allocate tensors; when available, pass the SSBO object to the delegate (using my `SetExternalTensorObject`); run the interpreter (`interpreter->Invoke()`).\r\n\r\nNote that passing the `TensorObject` after modifying the interpreter graph with the delegate is legitimate, because the objects are passed to the inner components (InferenceRunner & TensorTie) at every Invoke call ([reference](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/delegates/gpu/delegate.cc#L217)). The delegate is already aware of the ObjectDef and it doesn't need the actual SSBO id.\r\n\r\n### DHWC4 Issues\r\n\r\nThe **first problem** is that the `DefaultTensorTie` is not registering the SSBO id to the ObjectManager. The tie has all the informations needed to do this, so I think it should be its responsibility to do so, especially because the object manager is a pretty low-level internal component. One possible solution is to add a few lines [here](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/delegates/gpu/gl/api2.cc#L139-L140), like:\r\n\r\n```cpp\r\n...\r\nexternal_obj_ = obj;\r\nif (IsSameDef()) { \r\n  // This is a no-copy tie! Make the object manager aware of the actual ssbo ID.\r\n  if (def().external_def.object_def.object_type == gpu::ObjectType::OPENGL_SSBO) {\r\n      auto ssbo = absl::get_if<OpenGlBuffer>(&obj);\r\n      GlBuffer buffer;\r\n      RETURN_IF_ERROR(WrapSSBO(*ssbo, &buffer));\r\n      RETURN_IF_ERROR(objects_->RegisterBuffer(def().id, std::move(buffer)));\r\n  }\r\n}\r\nreturn absl::OkStatus();\r\n```\r\n\r\nBut then the **second problem** is timing, because such updates to the object manager are not picked up by the `Runtime`. The runtime will create binding functions only once during [InferenceBuilder.Build](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/delegates/gpu/gl/api2.cc#L573-L576), so by the time that the tensor tie receives the SSBO id through the InferenceRunner, the runtime is already initialized and won't pick up the new buffer.\r\n\r\nSo... The runner has setters to update the external TensorObject, yet the runtime expects the internal TensorObjects to be ready during InferenceBuilder.Build, so basically **before the runner is even returned**. This goes unnoticed with normal flow but it is a big design issue for no-copy DHWC4.\r\n\r\n### Thoughts\r\n\r\nOverall, no-copy DHWC4 i/o is pretty impossible at the moment? Or you have to hack very deep into the low level components. So I wouldn't say that this is implemented \ud83d\ude04 now I'm not an expert, not even a c++ developer, but one possible solution I see is to modify the runtime [MakeBindingFunc](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/lite/delegates/gpu/gl/runtime.cc#L147):\r\n\r\n- When the object manager is that of internal nodes, keep everything as is\r\n- When the object manager is that of edge nodes (`Runtime::external_objects_`, the one shared with the runner), the manager should be inspected **as part of the binding function**, not outside. An elegant implementation could use a new function called `MakeDeferredBindingFunc` which creates a lambda that calls `MakeBindingFunc` when invoked.\r\n\r\nThis would have a negligible impact on performance because it will only be done for i/o buffers, not internal nodes, and this object manager is typically extremely small. And it would fix the design issue where SSBO ids (or whatever other tensor type) are not picked after the tie setter is called.\r\n\r\nWhat do you think?"]}, {"number": 44332, "title": "TF Lite GPU delegate - Selection of GPU device", "body": "-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Platform independent\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: X\r\n-   **TensorFlow installed from (source or binary)**: Source\r\n-   **TensorFlow version (use command below)**: 2.3.0\r\n-   **Python version**: 3.8\r\n-   **Bazel version (if compiling from source)**: Doesn't matter\r\n-   **GCC/Compiler version (if compiling from source)**: Doesn't matter\r\n-   **CUDA/cuDNN version**: Doesn't matter\r\n-   **GPU model and memory**: Nvidia Titan, Jetson, etc\r\n-   **Exact command to reproduce**: Nope\r\n\r\n### Describe the problem\r\nI believe there is currently no way to select which GPU device is used by TF Lite. Even though it is supposed to be used on mobile devices, we already have builds for Windows, Linux (server and embedded), Android, and platforms i686, x86_64, armv7 etc. So basically we use it as our main inference library everywhere, as it is much smaller compared to full TF (which we also integrated). \r\n\r\nI believe GPU selection is a feature desired by many. We are willing to put our resources into coding this feature and creating a push request. Could you point us to a proper place in code where the GPU selection should be integrated? Both for opengl and opencl. Please also suggest how to expose GPU selection in the public API, which structure to modify. We are mainly using the C API.\r\n\r\nIf we are just stupid and the feature is already present, please point us to it.\r\n\r\nThanks, r", "comments": ["Hi @svobora, can you talk a bit more about your use-case and need for desktop GPU acceleration?\r\n\r\nWhile TFLite supports many different platforms, including desktop platforms, our primary focus continues to be mobile/edge devices. Technically the GPU delegate can run using OpenCL, but we test and optimize primarily for mobile devices. If you can say more about what kind of workloads, models or use-cases you have in mind, we can try to find a better way to support you. Thanks.", "Hi, thanks for the reply.\r\n\r\nThe use case is to be able to select which GPU device will be used on systems with more than 1 GPU. This should not interfere with the primary goal of TF Lite. From public API perspective, this could mean a new member variable in TfLiteGpuDelegateOptionsV2, like int32 gpu_id, with default gpu_id = 0 (this is how it works now).\r\n\r\nSpecifically, I am not discussing builds for other platforms to be included, just the option to select GPU based on OpenCL/OpenGL.\r\n\r\nIn TfLiteGpuDelegateV2Create, code could be added to call opengl/opencl gpu selection routines.\r\n\r\n\r\n\r\n\r\n", "For the OpenCL part, the GPU can be selected in \r\n \r\ndelegates/gpu/cl/cl_device.cc \r\n\r\nwhere line \r\n\r\n<code>*result = CLDevice(devices[0], platform_id);</code>\r\n\r\nshould be modified to accept gpu_id instead of selecting device with index 0.\r\n\r\nThe access to this functions can be made via 4 layers,\r\n\r\ndelegate.cc:  absl::Status InitializeOpenClApi(*)\r\n\r\napi.cc absl::Status NewInferenceEnvironment(*)\r\n\r\napi.cc   absl::Status Init(*)\r\n\r\ncl_device.cc absl::Status CreateDefaultGPUDevice(*)\r\n\r\nwhere in the top layer the delegate options are available. In the delegate options, I suggest to add gpu_id variable.\r\n", "@impjdi @srjoglekar246 any thoughts on the proposal?"]}, {"number": 44306, "title": "Quantized MobileBERT and ALBERT models performing very poorly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nThe Model Maker tool provides an [easy to follow guide](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification) to train a custom text classifier by fine-tuning the pre-trained MobileBERT weights. The guide uses [this MobileBERT model](https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1) from TensorFlow Hub. \r\n\r\nNow, once should be able to directly use this TF-Hub model as a part of a Keras model (which is exactly what Model Maker does) and train it. I tried doing the same and I have been successful but when I evaluated the TFLite variant of the model on the test set, the performance of the model significantly drops from 90% to 49%. The same happens with the [ALBERT model](https://tfhub.dev/tensorflow/albert_lite_base/1) as well. \r\n\r\nI DID use the proper tokenizers for the respective models as one can see in the notebooks mentioned below. I fine-tuning on the SST-2 dataset as used in the Model Maker Text Classification guide. \r\n\r\nWhat's wrong here?\r\n\r\n**Describe the expected behavior**\r\n\r\nThe TFLite models should match the original model performance. \r\n\r\n**Standalone code to reproduce the issue**\r\n* [ALBERT model training and conversion notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/ALBERT_Keras.ipynb)\r\n* [TFLite ALBERT model evaluation](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/Evaluation_SST_2_ALBERT.ipynb)\r\n* [MobileBERT model training and conversion notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/MobileBERT_Keras.ipynb)\r\n* [TFLite MobileBERT model evaluation](https://colab.research.google.com/gist/sayakpaul/27f97101ca06b80c14d29ecf1afdccc8/evaluation_sst-2_albert.ipynb)", "comments": ["That's really strange since Model maker works properly (Yuqi probably can help with that).\r\n\r\nBTW, wonder how the tf performs? I can see only training accuracy, have you tried the inference accuracy?\r\n\r\nThanks!", "The validation accuracy is already present in the training notebooks mentioned above. ", "Any updates on this? ", "I tried to follow the exact same workflow with Hugging Face's MobileBERT and the accuracy is still terrible (~49%) - https://colab.research.google.com/gist/sayakpaul/9fb955c973b8fbfcc94a557d27445044/albert_keras.ipynb. \r\n\r\nPretty weird! :o \r\n\r\n@renjie-liu ", "@MeghnaNatraj would you be able to take a look at this issue or point me in the right direction?", "Here's a [modified notebook](https://colab.research.google.com/gist/MeghnaNatraj/c03f6da6078a30929bf9a56de71f5192/albert_keras.ipynb#scrollTo=2qOcNEU7BFoH). Looks like the TF model and TFLite float model predictions are the same. Could you again compare this and verify if this is true? Could you print some of the TF model outputs, and compare them to ground truth. And finally compare this to the TFlite outputs? It also looks like the model\u2019s last layer is (None, 128) - is it supposed to be (None, 1)? \r\n\r\nTo find what component caused an issue, refer to this:\r\n - If there is a discrepancy in TF vs TFLite float, then there is an issue with TFLite conversion\r\n- If there is a discrepancy in TFfloat vs TFLite quantized (any type), then there is an issue with TFLite quantization\r\n", "Thank you very much for looking into this @MeghnaNatraj. \r\n\r\n> It also looks like the model\u2019s last layer is (None, 128) - is it supposed to be (None, 1)?\r\n\r\nYes, it happens with Hugging Face's MobileBERT version as we are taking the last hidden state from the pooled output. \r\n\r\nBut when you see [my original notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/MobileBERT_Keras.ipynb) (that uses MobileBERT from Hub) it is `(None, 1)`. If you follow this notebook then it'd error out during the conversion step (TensorFlow 2.3.0) - \r\n\r\n```\r\nValueError: Input 3 of node StatefulPartitionedCall/functional_1/hub_keras_layer_v1v2/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/input/_3:0 incompatible with expected resource.\r\n```\r\n\r\nHence, I had used a nightly build to do the conversion but that exact version (`2.4.0-dev20200810`) isn't available anymore and subsequent ones aren't able to communicate with Colab GPU (needed during model training otherwise it'd take ~6 hours on a CPU :3) since the Colab CUDA version is 10.1 (refer to https://github.com/tensorflow/tensorflow/issues/42957).\r\n\r\nSo, the long story cut short is I couldn't verify your findings on the original MobileBERT notebook.", "Same drill with ALBERT - https://colab.research.google.com/gist/sayakpaul/fc63c91b1f6e38fc859a3e5c3d310e7d/albert_keras.ipynb. ", "Got it, if there is a way you can verify this with an older version of TF or until the issue is resolved - it would be helpful for us to figure out where the error originated and debug this. If this is not urgent, you can reply on this thread once you can verify.", "Sure. @MeghnaNatraj I will try. I am guessing TensorFlow 2.2 would produce the same error here. ", "@MeghnaNatraj so here are some more findings. I had to spin up a notebook on AI Platform that supported CUDA 11.0. So, here's my setup - \r\n\r\n![image](https://user-images.githubusercontent.com/22957388/100964656-21c24380-354f-11eb-80e5-265e66779921.png)\r\n\r\nI installed 2.5.0-dev20201202 of `tf-nightly`. As expected the conversion went seamlessly well. Now coming to your point on output mismatches here are some findings: \r\n\r\n## ALBERT\r\n\r\n* Original model predictions \r\n  ```\r\n  array([[0.45974356],\r\n       [0.40282163],\r\n       [0.50713634],\r\n       [0.4738442 ],\r\n       [0.39585397],\r\n       [0.45194846],\r\n       [0.432267  ],\r\n       [0.4871237 ],\r\n       [0.48969463],\r\n       [0.48001567],\r\n       [0.46036363],\r\n       [0.49563286],\r\n       [0.49220225],\r\n       [0.46842712],\r\n       [0.48277178],\r\n       [0.45274204],\r\n       [0.5044237 ],\r\n       [0.4519858 ],\r\n       [0.43670607],\r\n       [0.4987168 ]], dtype=float32)\r\n  ```\r\n\r\n* TFLite (float model) predictions\r\n  ```\r\n  [array([0.9806177], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32),\r\n\tarray([nan], dtype=float32)]\r\n  ```\r\n\r\nI have faced this `NaN` issue before (refer to https://github.com/tensorflow/tensorflow/issues/43719). When I tried to run inference with the dynamic-range quantized model the runtime crashed. \r\n\r\nHere's an interesting observation regarding the model training. The model training logs report a decent validation accuracy - \r\n```\r\n2807/2807 [==============================] - 1758s 623ms/step - loss: 0.4047 - accuracy: 0.7907 - val_loss: 0.3459 - val_accuracy: 0.8830\r\n```\r\n\r\nBut when I ran `model.evaluate()` separately I got surprised - \r\n\r\n```\r\nmodel.evaluate(train_ds)\r\n2807/2807 [==============================] - 610s 217ms/step - loss: 0.7304 - accuracy: 0.4458\r\n[0.7303553223609924, 0.44579726457595825]\r\n```\r\n\r\n```\r\nmodel.evaluate(valid_ds)\r\n37/37 [==============================] - 8s 205ms/step - loss: 0.6962 - accuracy: 0.5149\r\n[0.69621342420578, 0.5149082541465759]\r\n```\r\n\r\n## MobileBERT\r\n\r\n* Original TF model predictions\r\n\r\n  ```\r\n  array([[9.9999523e-01],\r\n       [3.5430368e-02],\r\n       [9.9836868e-01],\r\n       [9.9768424e-01],\r\n       [1.5873680e-02],\r\n       [9.9999309e-01],\r\n       [8.2542878e-01],\r\n       [9.6560270e-02],\r\n       [9.9333352e-01],\r\n       [1.7254837e-02],\r\n       [9.9999368e-01],\r\n       [5.3525418e-03],\r\n       [8.6974440e-04],\r\n       [4.6796936e-01],\r\n       [2.7280839e-03],\r\n       [9.9999714e-01],\r\n       [5.9698451e-01],\r\n       [9.9830139e-01],\r\n       [3.1085087e-03],\r\n       [9.9264004e-04]], dtype=float32)\r\n  ```\r\n\r\n* TFLite model predictions\r\n\r\n  ```\r\n   [array([0.7102443], dtype=float32),\r\n\t array([0.57573587], dtype=float32),\r\n\t array([0.70804507], dtype=float32),\r\n\t array([0.60090107], dtype=float32),\r\n\t array([0.45175412], dtype=float32),\r\n\t array([0.7718483], dtype=float32),\r\n\t array([0.655351], dtype=float32),\r\n\t array([0.5626731], dtype=float32),\r\n\t array([0.6212563], dtype=float32),\r\n\t array([0.22457898], dtype=float32),\r\n\t array([0.67261064], dtype=float32),\r\n\t array([0.56146646], dtype=float32),\r\n\t array([0.53055984], dtype=float32),\r\n\t array([0.5748798], dtype=float32),\r\n\t array([0.4939883], dtype=float32),\r\n\t array([0.69004154], dtype=float32),\r\n\t array([0.55181825], dtype=float32),\r\n\t array([0.9738283], dtype=float32),\r\n\t array([0.46502632], dtype=float32),\r\n\t array([0.3641715], dtype=float32)]\r\n  ```\r\n\r\nWhen I tried running inference with the DR quantized model TFLite model, runtime crashed. You would see a bit of discrepancy in between the model training logs and `scores` from `model.evaluate()`. \r\n\r\n**Original model training log**:\r\n\r\n```\r\n2807/2807 [==============================] - 628s 210ms/step - loss: 0.4848 - accuracy: 0.7825 - val_loss: 0.2525 - val_accuracy: 0.9117\r\n```\r\n\r\n**model.evaluate()**:\r\n\r\n```\r\nmodel.evaluate(train_ds)\r\n2807/2807 [==============================] - 217s 77ms/step - loss: 0.1372 - accuracy: 0.9493\r\n[0.13716447353363037, 0.9492791295051575]\r\n```\r\n\r\n```\r\nmodel.evaluate(valid_ds)\r\n37/37 [==============================] - 3s 76ms/step - loss: 0.2525 - accuracy: 0.9117\r\n[0.2524813711643219, 0.911697268486023]\r\n```\r\n\r\nTo help you reproduce these I am attaching the notebooks and the `SavedModel` files. \r\n\r\n## Important files\r\n\r\n* Notebooks\r\n  [Notebooks.zip](https://github.com/tensorflow/tensorflow/files/5633734/Notebooks.zip)\r\n* `SavedModel`\r\n  * https://storage.googleapis.com/demo-experiments/albert.tar.gz\r\n  * https://storage.googleapis.com/demo-experiments/mobilebert.tar.gz\r\n\r\nLet me know if you need more details. \r\n"]}, {"number": 44282, "title": "update grpc to v1.32.0 to fix val.val issues while compiling TFS from source", "body": "- This PR addresses issue in TFS https://github.com/tensorflow/serving/issues/1642 due to https://github.com/protocolbuffers/upb/issues/300.\r\n- The fix for upb has gone into grpc v1.32.x\r\n- This fix needs to go into TF from which TFS picks it up.\r\n- This PR uses protobuf version upgrade to v3.15.1 and grpc 1.32.2\r\n\r\n@mihaimaruseac @ncteisen : Could you please review this PR and see if this goes into 2.5 ?", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44282) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n@googlebot I signed it!", "From the failure logs, I see that it is not able to pull the file from https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/414bb8322de2411eee1f4e841ff29d887bec7884.tar.gz. Could you please add the grpc tar.gz file to the googleapis repository @mihaimaruseac ?", "This is weird. The build should pull from github if the mirror does not contain the hash. Let's try again.", "There are actually compile issues\r\n\r\n```\r\nExecution platform: @org_tensorflow//third_party/toolchains:rbe_ubuntu16.04-manylinux2010\r\nIn file included from external/com_github_grpc_grpc/src/compiler/python_generator_helpers.h:29:0,\r\n                 from external/com_github_grpc_grpc/src/compiler/protobuf_plugin.h:24,\r\n                 from external/com_github_grpc_grpc/src/compiler/python_plugin.cc:22:\r\nexternal/com_github_grpc_grpc/src/compiler/python_generator.h:47:12: error: 'uint64_t grpc_python_generator::PythonGrpcGenerator::GetSupportedFeatures() const' marked 'override', but does not override\r\n   uint64_t GetSupportedFeatures() const override;\r\n```", "should I try changing the grpc version ? grpc 1.33.x is the latest release.\r\n@mihaimaruseac any suggestions here?\r\n\r\n", "We can try.", "> We can try.\r\n\r\n@mihaimaruseac updated grpc to v1.33.2. ", "@satishpasumarthi  Can you please fix build failures ? Thanks!", "> @satishpasumarthi Can you please fix build failures ? Thanks!\r\n\r\n@mihaimaruseac looks like grpc v1.33.x also didn't fix the build faillures. ", "Yes, we get the same error\r\n\r\n```\r\nIn file included from external/com_github_grpc_grpc/src/compiler/python_generator_helpers.h:29:0,\r\n                 from external/com_github_grpc_grpc/src/compiler/protobuf_plugin.h:24,\r\n                 from external/com_github_grpc_grpc/src/compiler/python_plugin.cc:22:\r\nexternal/com_github_grpc_grpc/src/compiler/python_generator.h:47:12: error: 'uint64_t grpc_python_generator::PythonGrpcGenerator::GetSupportedFeatures() const' marked 'override', but does not override\r\n   uint64_t GetSupportedFeatures() const override;\r\n```\r\n\r\nSince this is a GRPC issue, we probably won't be able to update GRPC in TF.", "Is the protobuf version the probable issue ? https://github.com/grpc/grpc/issues/23311", "But the error does not seem to come from protobuf.", "> But the error does not seem to come from protobuf.\r\n\r\nHi @mihaimaruseac , as @vigbk mentioned , this looks like a protobuf version requirement for grpc. Please see https://github.com/grpc/grpc/pull/23324\r\n", "Can we attempt to upgrade both?", "@satishpasumarthi  Any update on this PR? Please. Thanks!", "> @satishpasumarthi Any update on this PR? Please. Thanks!\r\n\r\nI will do it today", "This PR will also fix #39467", "hi @mihaimaruseac , looks like it failed to download the protobuf? ", "@satishpasumarthi  Can you please fix build failures ? Thanks!", "> @satishpasumarthi Can you please fix build failures ? Thanks!\r\n\r\nHi @gbaned, I see the error -- \r\nDownload from https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/v3.13.0.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR:\u001bAn error occurred during the fetch of repository 'com_google_protobuf': \r\n\r\nIsn't it supposed to pull the zip file automatically if it's missing? I've verified that filepath, sha256 are correct. Not sure, what am I missing here. ", "@mihaimaruseac Can you please assist on above comments from @satishpasumarthi. Thanks!", "Investigating. The `tf_http_archive` seems broken. It requires 2 links one of which is the TF mirror but it should always fallback to the upstream link if the mirror is not available. Will see what broke and come back to this one.", "It seems the protobug patch no longer applies", "This still fails all builds :(", "@satishpasumarthi  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "> @satishpasumarthi Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!\r\n\r\nI see a failure with package 'six' (required by protobuf). But the package was not disturbed and there is no new version of it either. Does upgrading protobuf cause this issue? We've tried protobuf versions 3.12 and 3.13. Any suggestions @mihaimaruseac ?", "Let's try one more time in case the last error was transient", "There are a bunch of MLIR tests failing\r\n\r\n```\r\nblaze test //tensorflow/compiler/mlir/xla/tests/translate:import.hlotxt.test\r\n```\r\n\r\nYou can try testing the above.\r\n\r\nAlso, to fix sanity build, you have to add the `\"@com_github_google_re2//:LICENSE\",` license to `tensorflow/tools/lib_package/BUILD` and `tensorflow/tools/pip_package/BUILD`", "> There are a bunch of MLIR tests failing\r\n> \r\n> ```\r\n> blaze test //tensorflow/compiler/mlir/xla/tests/translate:import.hlotxt.test\r\n> ```\r\n> \r\n> You can try testing the above.\r\n> \r\n> Also, to fix sanity build, you have to add the `\"@com_github_google_re2//:LICENSE\",` license to `tensorflow/tools/lib_package/BUILD` and `tensorflow/tools/pip_package/BUILD`\r\n\r\nsure, thanks. I will take a look and try fix these errors.", "Could we wait for https://github.com/tensorflow/tensorflow/issues/44485#issuecomment-752183574 ?", "It's being worked on.", "@satishpasumarthi  Any update on this PR? Please. Thanks!", "Can we start to switch this to 3.15.0rc1 and then merge when 3.15 is released?", "If that passes CI, yes", "@satishpasumarthi Any update on this PR? Please. Thanks!", "We could switch this to the released `v3.15.1`", "@satishpasumarthi Can you upgrade the version?", "> @satishpasumarthi Can you upgrade the version?\r\n\r\nwill do.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@satishpasumarthi Any update on this PR? Please. Thanks!", "@gbaned @bhack sorry for the delay. Upgraded protobuf to v3.15.1", "@satishpasumarthi Can you please check @bhack's comments and keep us posted ? Thanks!\r\n", "Can we also try to align  protobuf to the last version?", "> Can we also try to align protobuf to the last version?\r\n\r\nDoes this hold good for latest protobuf versions as well? I see that the for loop logic changed from v3.13.x\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/protobuf/protobuf.patch#L39-L40", "FYI - there was an attempt to update protobuf to 3.14 in the last release, but it was reverted back to 3.9.x:\r\nhttps://github.com/tensorflow/tensorflow/commit/61fb67458eaadcc1873419a1d47c715599c3646d\r\nhttps://github.com/tensorflow/tensorflow/commit/bd1aa7a01c377341de5b89e5d168735dcb21017b\r\n\r\n@mihaimaruseac may have details as to why it was reverted. Protobuf is very sticky.", "> Approve just to trigger kokoro\r\n\r\n@mihaimaruseac looks like the CI build is not able to find the grpc binary. Ideally it should pull and place it in the repo, right?", "@satishpasumarthi  Can you please check build failures. Thanks!", "An updated protobuf version is probably required", "With the specific release tag is see this protobuf version constrain https://github.com/grpc/grpc/blob/v1.36.4/requirements.txt#L4", "> With the specific release tag is see this protobuf version constrain https://github.com/grpc/grpc/blob/v1.36.4/requirements.txt#L4\r\n\r\nThe current protobuf being used is v3.9.2 \r\nhttps://github.com/tensorflow/tensorflow/blob/5bed9c49c530d8f76056b5569a508adc2ead68fe/tensorflow/workspace2.bzl#L540\r\nI am not quite sure on how to proceed further on this.", "@mihaimaruseac  Can you please assist on above comments from @satishpasumarthi. Thanks!", "> An updated protobuf version is probably required\r\n...\r\n> The current protobuf being used is v3.9.2\r\n\r\nShould it be v3.12.x or greater? https://github.com/grpc/grpc/pull/23324", "@mihaimaruseac  Any update on this PR? Please. Thanks!\r\n", "@satishpasumarthi  Can you please fix build failures ? Thanks!", "@satishpasumarthi  Any update on this PR? Please. Thanks!", "> @satishpasumarthi Any update on this PR? Please. Thanks!\r\n\r\nI see this error in the logs `Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/v1.36.4.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found` \r\nLet me try to figure out what's going on here ", "Where are we at with this PR? TensorFlow so very much needs a grpc/protobuf update.", "> > @satishpasumarthi Any update on this PR? Please. Thanks!\r\n> \r\n> I see this error in the logs `Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/v1.36.4.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found`\r\n> Let me try to figure out what's going on here\r\n\r\nThe mirror 404s are not blocking the build, the Bazel setup will just go and download the artifact from the upstream location it the mirror fails.\r\n\r\nSanity failure here is\r\n\r\n```\r\n6. do_pip_package_licenses_check: pip: license check for external dependencies\r\n  FAIL\r\n7. do_lib_package_licenses_check: C library: license check for external dependencies\r\n  FAIL\r\n8. do_java_package_licenses_check: Java Native Library: license check for external dependencies\r\n  FAIL\r\n```\r\n\r\nwith details\r\n\r\n```\r\nFAIL: mismatch in packaged licenses and external dependencies\r\nMissing the licenses for the following external dependencies:\r\n@com_github_google_re2//\r\n@upb//\r\nPlease add the license(s) to //tensorflow/tools/pip_package:licenses.\r\n```\r\n\r\nhttps://source.cloud.google.com/results/invocations/68ea10e8-1a44-45d9-be9b-93f4b85d2998/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log\r\n\r\nI haven't checked the other builds.", "Yeah, I suspect those 404s will go away once this is merged and that tar is cached.\r\nCool fancy license checker. That problem should be easy enough to fix - I assume that just needs a new line the tf_http_archive stanza? ", "Yes, that is just checking that the files exists in BUILD rules. Unfortunately this is needed to ensure license compliance on all downstream projects of TF.", "> Yeah, I suspect those 404s will go away once this is merged and that tar is cached.\r\n> Cool fancy license checker. That problem should be easy enough to fix - I assume that just needs a new line the tf_http_archive stanza?\r\n\r\nHi @mihaimaruseac @jayfurmanek  Why is it complaining about missing license files when what I did was just version update?\r\nAlso, where should I include the license file (I am not very well versed with TF bazel build system) ? In `workspace2.bzl` ? The error log suggests adding the missing logs to `//tensorflow/tools/pip_package:licenses`", "@mihaimaruseac @jayfurmanek Can you please assist on above comments from @satishpasumarthi. Thanks!", "So the problem with the license sanity check is that the newer grpc includes some new things.\r\nFor one, `upb` is now living in the grpc code tree at `/third_party/upb` so the license checker is picking up on that\r\nThe new grpc also has a re2 submodule that is sligtly different to the one already included/vendored by TensorFlow.\r\n\r\nThe license checker just wants to know where the licenses are for these packages. I think maybe the more important question is if TensorFlow wants to vendor these differently and have them listed under `tensorflow/third_party`. I don't know if the potentially different versions of re2 would cause a problem.\r\nPerhaps @mihaimaruseac can advise.\r\n\r\nIn addition the license checker stuff, this PR is also causing a bunch of tests to fail, presumably because the newer grpc uses function from newer protobuf version than the 3.9 version in TF by default. @satishpasumarthi I think at one point you had a newer protobuf version attached to this PR as well, perhaps that needs to be added back.", "btw, the license checker expects license locations to be listed here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L181", "Thanks @jayfurmanek for finding the culprit.\r\n\r\nI think we can either try to use the versions vendored by grpc or those by TF, but definitely not both.\r\n\r\nGoing with the versions vendored by grpc requires updating license files and code paths as well as removing the other versions.\r\n\r\nGoing with the versions provided by TF would still require a change where we don't pull in the grpc deps, if possible.\r\n\r\nUnfortunately, I won't be able to take a look at this for a while, apologies for the delay", "@satishpasumarthi wondering if you are still actively working on this issue?", "> @satishpasumarthi wondering if you are still actively working on this issue?\r\n\r\nHi, I am waiting on @mihaimaruseac's feedback. ", "@mihaimaruseac Any update on this PR? Please. Thanks!", "#51923 is the last update. We had to roll it back as it caused issues with Windows"]}, {"number": 44247, "title": "why the scales in quantization are floats not in the format of m/2^n", "body": "on the hardware, while we are doing quantized model inference, we use featmap*m/2^n to simulate rescaling from one layer(int32) to another(int8).\r\nbut in the tensorflow lite source code, it seems that the scale is calculate by (max-min)/2^8, while max and min are floats.\r\ndue to m and n are all integer, scale(m/2^n) is not totally equal to scale((max-min)/255)\r\n\r\nFor example, max = 3.1415 and min=1.9276, the scale=0.00476...\r\nthere is no m and n can make this scale", "comments": []}, {"number": 44237, "title": "opcode 'SUB' version '3' in tfllite_runtime on Windows", "body": "**System information**\r\n- Ubuntu 18.04, Nvidia driver 430.4, CUDA 10.1\r\n- tf-nightly: 2.4.0-dev20201022, and\r\n- **tflite_runtime: 2.1.0.post1**\r\n\r\nConverted keras-model with tf-nightly. This seems to be incompatible with tflite_runtime.\r\n\r\nInference test script:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tflite_runtime.interpreter import Interpreter\r\n\r\n# interpreter = tf.lite.Interpreter(model_path=\"<path_to_wts_file>.tflite\")\r\ninterpreter = Interpreter(model_path=\"<path_to_wts_file>.tflite\")\r\n\r\ninput_details = interpreter.get_input_details()\r\n\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (1, 64, 64, 64, 1))\r\ninput_details = interpreter.get_input_details()\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\n\r\noutput_details = interpreter.get_output_details()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nThe above works fine when using tf-nightly with:\r\n```\r\nimport tensorflow as tf\r\ninterpreter = tf.lite.Interpreter(model_path=\"<path_to_wts_file>.tflite\")\r\n```\r\n\r\nWhen trying to execute the same with **tflite_runtime**, it fails to load the model:\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\ninterpreter = Interpreter(model_path=\"<path_to_wts_file>.tflite\")\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tfliteTest.py\", line 13, in <module>\r\n    interpreter = Interpreter(model_path=\"_<path_to_wts_file>_.tflite\")\r\n  File \"/home/venv/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 204, in __init__\r\n    model_path, self._custom_op_registerers))\r\nValueError: Didn't find op for builtin opcode 'SUB' version '3'\r\nRegistration failed.\r\n```\r\nIs there a nightly build of tflite_runtime?", "comments": ["You are required to use TF Lite runtime that supports `Select TF Ops`.\r\nYou can try updating build.gradle dependencies with\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}\r\n```\r\nSee https://www.tensorflow.org/lite/guide/ops_select#run_inference", "This option isn't applicable for the  [TensorFlow Lite Interpreter pip package](https://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter) version, right?\r\n\r\nIs there a TF Lite Runtime python version supporting `Select TF Ops` ?  We will need to build python wheel with TF OPS using `tflite_pip_with_flex` flag.", "when i run this command :\r\n\r\n`CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows`\r\n\r\nthere is an error:\r\n\r\n```\r\nERROR: C:/porsapp/tensorflow/tensorflow/tensorflow/core/common_runtime/BUILD:1651:1: Illegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/common_runtime:core_cpu_internal:\r\n//tensorflow:windows\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted:\r\n```\r\n\r\nbut when i run this:\r\n\r\n`tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh`\r\n\r\nthere is no error.\r\n\r\nI've checked out Tensorflow r.2.4. Do you know what the problem is?\r\n\r\n@nfisdnf986 ", "I finally build tflite_runtime successfully by removing `--config=noaws --config=nogcp --config=nohdfs --config=nonccl` options from `build_pip_package_with_bazel.sh`. The result file is about 26MB but still I got this error:\r\n\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexFusedBatchNormV3) failed to prepare.\r\nNode number 1 (IF) failed to prepare.\r\n```\r\n\r\nI think 'CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true' does not work. Because when I build the original tensorflow package and use `tensorflow.lite.python.interpreter` there is no error.", "@terryheo could you take a look at this issue?  It is related to bulding TFLite + Flex runtime at head.", "Verified the issue. Working on making a fix.", "Sorry about the delayed reply. @pourfard Did the above fix work for you?\r\n\r\n@terryheo Thanks for adding that change. I haven't been able to build master branch on Windows, r.2.3 and r2.4 build fine.\r\n\r\nAny suggestions for the following error?\r\n\r\n```\r\n\r\nERROR: C:/users//desktop/build-from-src/tensorflow/tensorflow/python/BUILD:5259:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users//_bazel_/poa7uci7/execroot/org_tensorflow\r\n...\r\n...\r\n...\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\ncl : Command line warning D9002 : ignoring unknown option '-fno-strict-aliasing'\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C3861: 'copysign': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "@nfisdnf986  Yes, it works now, [here is the resulted whl file ](https://drive.google.com/file/d/10mFqiyezP9UTq63rWOUDZ9mKjl54koov/view?usp=sharing)\r\n\r\nThanks a lot, @terryheo ", "Thank you for the wheel @pourfard. Are the extra flex-supported-operations working too?\r\nWhen I try to build with the same command `build_pip_package_with_bazel.sh windows`, it starts the script in MSYS and gets stuck at `setup_with_bazel.py`. Did you make any changes other than the `--config` ones mentioned about to the shell script? \r\n\r\n```\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users//_bazel_/hya7usi5/sandbox\r\n[4 / 231] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\nTarget //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper up-to-date (nothing to build)\r\nINFO: Elapsed time: 11.714s, Critical Path: 0.81s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\n+ cp /c/Users//Desktop/temp/build-from-src/tensorflow/tensorflow/lite/tools/pip_package/../../../../bazel-bin/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.pyd /c/Users//Desktop/temp/build-from-src/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ chmod u+w /c/Users//Desktop/temp/build-from-src/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.pyd\r\n+ cd /c/Users//Desktop/temp/build-from-src/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ [[ -n windows ]]\r\n+ [[ -n '' ]]\r\n+ python3 setup_with_bazel.py bdist bdist_wheel\r\n\r\n\r\n```", "> Sorry about the delayed reply. @pourfard Did the above fix work for you?\r\n> \r\n> @terryheo Thanks for adding that change. I haven't been able to build master branch on Windows, r.2.3 and r2.4 build fine.\r\n> \r\n> Any suggestions for the following error?\r\n> \r\n> ```\r\n> \r\n> ERROR: C:/users//desktop/build-from-src/tensorflow/tensorflow/python/BUILD:5259:1: C++ compilation of rule '//tensorflow/python:_tf_stack.so' failed (Exit 2): cl.exe failed: error executing command\r\n>   cd C:/users//_bazel_/poa7uci7/execroot/org_tensorflow\r\n> ...\r\n> ...\r\n> ...\r\n> cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\n> cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\n> cl : Command line warning D9002 : ignoring unknown option '-fno-strict-aliasing'\r\n> cl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\n> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C2039: 'copysign': is not a member of '`global namespace''\r\n> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29333\\include\\complex(354): error C3861: 'copysign': identifier not found\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> ```\r\n\r\nHaving the same issue trying to build tensorflow from master on windows. The problem seems to be about <complex> library, but I don't understand what's wrong. I have exactly the same error with the same version of MSVC.", "The `copysign` issue is caused by MSVC. See #46902, https://www.gitmemory.com/issue/OpenImageIO/oiio/2799/752372181, https://github.com/python/cpython/pull/23326"]}, {"number": 44235, "title": "Embedding layer with NaN input different behaviour on CPU vs GPU", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CUDA Version 10.1.243\r\n\r\n**Describe the current behavior**\r\n\r\nDifferent behaviour of embedding layer on CPU vs GPU. When running on CPU and a `nan` value is passed as input to the embedding layer, the code throws a correct `InvalidArgumentError`. Running the same code on GPU silently converts the `nan` value to 0 and does not throw any warning/exception.\r\n\r\n**Describe the expected behavior**\r\n\r\nMaybe an exception should be thrown (or at least a warning) if a `nan` value is passed to the embedding layer on GPU?\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis code breaks (as expected) running on CPU:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Embedding\r\nprint(tf.__version__)\r\n\r\n# Disable cuda devices\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \r\n\r\n# Make sure no cuda device is used\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nprint(gpus)\r\n\r\n# Simple toy model\r\nn_inputs      = 10\r\nembedding_dim = 2\r\n\r\ninput_tensor    = Input((1,))\r\nembedded_tensor = Embedding(n_inputs, embedding_dim, name = 'embedding_layer')(input_tensor)\r\n\r\nmodel = Model(inputs = input_tensor, outputs = embedded_tensor)\r\n\r\n# Predict which should break (and does break on CPU)\r\nmodel.predict([np.nan])\r\n```\r\nOutput (`nan` gets converted to minimum value of int32 when cast to tf.int32, thus triggering the out of bound error):\r\n```\r\nInvalidArgumentError:  indices[0,0] = -2147483648 is not in [0, 10)\r\n\t [[node functional_3/embedding_layer/embedding_lookup (defined at <ipython-input-7-7784f8517478>:25) ]] [Op:__inference_predict_function_295]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node functional_3/embedding_layer/embedding_lookup:\r\n functional_3/embedding_layer/embedding_lookup/287 (defined at /usr/lib/python3.6/contextlib.py:81)\r\n\r\nFunction call stack:\r\npredict_function\r\n```\r\nOn the other hand, this piece of code does not break, nor throws a warning at me (running on GPU):\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Embedding\r\nprint(tf.__version__)\r\n\r\n# Usa cuda device 0\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n\r\n# Simple toy model\r\nn_inputs      = 10\r\nembedding_dim = 2\r\n\r\ninput_tensor    = Input((1,))\r\nembedded_tensor = Embedding(n_inputs, embedding_dim, name = 'embedding_layer')(input_tensor)\r\n\r\nmodel = Model(inputs = input_tensor, outputs = embedded_tensor)\r\n\r\n# Predict which should break (and does break on CPU)\r\nprint(model.predict([np.nan]))\r\nprint(model.predict([0]))\r\n```\r\nOutput (`nan` output is same as 0th embedding):\r\n```\r\n2.3.1\r\n[[[ 0.02256938 -0.02229818]]]\r\n[[[ 0.02256938 -0.02229818]]]\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3.\r\n\r\nCode throws an `InvalidArgumentError` when running on [CPU](https://colab.research.google.com/gist/amahendrakar/f80b8916c0b553ef60f1a08a75bd9172/44235-cpu.ipynb), whereas no error is thrown when running on [GPU](https://colab.research.google.com/gist/amahendrakar/91482d85fc0a5a435ed4b0d6a96a68cf/44235-gpu.ipynb). Please find the attached gist. Thanks!", "Was able to reproduce the issue with TF 2.5.0-dev20210114\r\n\r\nCode throws an InvalidArgumentError when running on [CPU](https://colab.research.google.com/gist/Saduf2019/bd0e6d5de67539269898561f2a3f017a/untitled499.ipynb#scrollTo=_IpQoLNDHgyt), when running on GPU. Please find the attached [gist](https://colab.research.google.com/gist/Saduf2019/a571e4d483751ea92aa56c7e0b67e460/untitled499.ipynb). Thanks!", "Was able to reproduce your issue in [CPU](https://colab.research.google.com/gist/sachinprasadhs/6d3d409fd3d5f2081eba91f9ebaa7e8e/44235.ipynb) using Tf Nightly 2.6 version and on [GPU](https://colab.research.google.com/gist/sachinprasadhs/10a42a379754d620218dab14b580b562/44235.ipynb) using Tensorflow 2.5. "]}, {"number": 44214, "title": "tensor.numpy() from GPU model prediction very slow", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Colab)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: Colab\r\n- GPU model and memory: Colab has k80 I think\r\n\r\n**Describe the current behavior**\r\nI'm running inference with batch_size=1, calling a `tf.keras.Model` model, and when trying to convert the predictions to numpy array (moving the tensor from GPU to CPU), the inference time drops significantly. In the example given, I am just running a CNN with 10 conv2d layers. When running `.numpy()`, the inference time increases more than 2x. In the case mentioned the prediction has the same shape as the input, a tensor of shape (1,512,512,3).\r\n\r\n**Describe the expected behavior**\r\nOne would expect that copying just an image from the GPU won't take that much. Running the same network with `pytorch` confirms that moving the tensor to the CPU can be done way faster.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\ndef get_model(depth):\r\n  model = tf.keras.Sequential(\r\n      [\r\n       tf.keras.layers.Conv2D(64,3,padding=\"same\",input_shape=(512,512,3), name=\"input\")]+[\r\n       tf.keras.layers.Conv2D(64,3,padding=\"same\") if i < depth -1 else tf.keras.layers.Conv2D(64,3,padding=\"same\", name=\"output\")\r\n       for i in range(depth)\r\n       ],\r\n  )\r\n  return model\r\n\r\nmodel = get_model(10)\r\nmodel_fn = tf.function(model)\r\nimage = tf.random.normal((1, 512,512,3))\r\n# warm up\r\nfor i in range(50):\r\n    model_fn(image)\r\niters = 100\r\nimages = [tf.random.normal((1, 512,512,3)) for i in range(iters)]\r\n\r\ninit = time.time()\r\nfor image in images:\r\n    x = model_fn(image)\r\nend = time.time() - init\r\nprint(f\"FPS {1/(end/iters)}\") # FPS 28.559242310262988\r\nprint(f\"Time {end/iters}\") # Time 0.03501493453979492\r\n\r\ninit = time.time()\r\nfor image in images:\r\n    x = model_fn(image).numpy()\r\nend = time.time() - init\r\nprint(f\"FPS {1/(end/iters)}\") # FPS 13.259242909743344\r\nprint(f\"Time {end/iters}\") # Time 0.07541908740997315\r\n```\r\nThe colab is also here https://colab.research.google.com/gist/charlielito/c8a301da67b5bf1cf68a4da4afc24d8e/tf-numpy-issue.ipynb\r\n\r\n\r\n", "comments": ["@charlielito,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/35cfa05d9ede0de8e42d5f99dcd4ef84/44214.ipynb). However, with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/05e0fb69438a72346727f53a20f0d73a/44214-tf-nightly.ipynb), the `Time` is almost identical but the `FPS` is very low, please check the linked gist for reference. \r\n\r\nCould you please check if you are facing the same issue with TF-nightly too? Thanks!", "@amahendrakar  I think your version with `tf-nightly` is running on CPU... I tested `tf2.3` running on CPU and got similar times for your example with `tf-nightly`. Moreover if you run `tf.test.gpu_device_name()` for the nightly version, you got `''`, no detection of any GPU\r\nEdit: found this issue https://github.com/tensorflow/tensorflow/issues/42957 ...", "Same question!", "Could you please let us know the Numpy version?", "@charlielito Is this still an issue for you? \r\n\r\nI tried with `TF2.7` and I don't see that much difference as it had with `TF2.3`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/c8b3fb82636f21965d2d7b36b8b081fd/44214.ipynb) is a gist for reference. Thanks!", "> @charlielito Is this still an issue for you?\r\n> \r\n> I tried with `TF2.7` and I don't see that much difference as it had with `TF2.3`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/c8b3fb82636f21965d2d7b36b8b081fd/44214.ipynb) is a gist for reference. Thanks!\r\n\r\nFrom you gist, with tensorflow==2.3 and numpy==1.18.5 and Tesla K80:\r\n```\r\nFPS 10.215680644434615\r\nTime 0.09788872957229615\r\nFPS  with .nump() 6.69969223263249\r\nTime with .numpy() 0.14926058769226075\r\n```\r\nAverage overhead .nump() per call is ~50ms\r\n\r\nFrom you gist, with tensorflow==2.7 and numpy==1.19.5 and Tesla K80:\r\n```\r\nFPS 5.980206239141523\r\nTime 0.16721831321716307\r\nFPS with .numpy() 4.133535122740489\r\nTime with .numpy() 0.2419236731529236\r\n```\r\nAverage overhead .nump() per call is ~75ms\r\n\r\nSo I think this is still and issue. Moreover the 2.7 tf version seems to be very slow in comparison with 2.3\r\n"]}, {"number": 44191, "title": "mixed_float16 + example code = horrible performance on a GeForce GTX 1660 Ti", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker image (binary)\r\n- TensorFlow version (use command below): 2.3.0 (v2.3.0-rc2-23-gb36436b087)\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Nvidia GeForce GTX 1660 Ti (mobile)\r\n\r\n**Describe the current behavior**\r\nI went to the TensorFlow guide on mixed precision, here:\r\n\r\nhttps://www.tensorflow.org/guide/mixed_precision\r\n\r\n... and copied the code/model as best I could. (I will attach my code.)\r\n\r\nInference performance decreases by a factor of ~4 when mixed_float16 is turned on. (GeForce GTX 1660 Ti)\r\n\r\nI have tried several different input dimensions and model designs (including the model that I'm actually trying to optimize) and I can't find a combination where mixed_float16 doesn't decrease performance by a factor of 2-4x.\r\n\r\n**Describe the expected behavior**\r\nI understand that the 1660 Ti (chip name: TU116 SM) does not have Tensor Cores but it does have support for float16 and Nvidia advertises that its float16 throughput is twice that of float32, so I would expect some speedup from switching to mixed_float16. Instead I'm seeing enormous slowdowns.\r\n\r\n**Standalone code to reproduce the issue**\r\nI will attach my minimal-repro code to this issue.\r\n\r\n[test.py.txt](https://github.com/tensorflow/tensorflow/files/5413328/test.py.txt)\r\n\r\n", "comments": ["@motrek \r\nPlease share a simple stand alone code to replicate the issue faced or is possible share a colab gist with issue reported.", "> @motrek\r\n> Please share a simple stand alone code to replicate the issue faced or is possible share a colab gist with issue reported.\r\n\r\nPlease note that I attached a file called \"test.py.txt\" to this issue. This is a small Python script that demonstrates the issue. I had to add the \".txt\" extension to the file because the system would not allow me to upload a file with the extension \".py\".\r\n\r\nIf you rename the file and run it on any system that has TensorFlow installed, it will create a model (the model from TensorFlow's page on mixed precision), then create some fake input data, and then measure how fast the model can do inference on that data.\r\n\r\nThen uncomment the line that enables mixed_float16 and re-run the script. Note that the performance of inference decreases dramatically. (At least it does on a TU116 chip.)\r\n\r\nOn my 1660 Ti, the script does ~42,000 inferences per second when it's float32 and ~15,000 inferences when it's mixed_float16.", "In addition to the script with the model from the TensorFlow mixed precision page that I already posted, here's a similar file containing the model that I'm really trying to optimize. It's a pretty straightforward sequential model with some convolutional filters and residual blocks.\r\n\r\nSwitching to mixed_float16 (see the lines that are commented out) causes this script to run exactly 4x slower on my computer.\r\n\r\n[mymodel.py.txt](https://github.com/tensorflow/tensorflow/files/5417465/mymodel.py.txt)\r\n", "@motrek Pls give us a minimal example to reproduce this issue. Also did you try reproducing this issue on colab using gpu.. if not please try it and share the gist. Thanks!", "> @motrek Pls give us a minimal example to reproduce this issue. Also did you try reproducing this issue on colab using gpu.. if not please try it and share the gist. Thanks!\r\n\r\nI'm not sure what's going wrong here but you're the second person to ask me for a minimal repro.\r\n\r\nAre you guys not seeing that I attached a python script to the first post, called \"test.py.txt\"?\r\n\r\nJust rename the file (remove the .txt extension) and run it. That's the minimal repro. The problem should be immediately obvious.", "I cannot reproduce on a Titan V or a P100, the latter which does not have tensor cores. My guess is that this issue is exclusive to the Geforce 16 series. Unfortunately I do not have such a card to debug.\r\n\r\n@nluehr any ideas what the issue is? The compute capability of the GTX 1660 Ti is not listed on the [CUDA website](https://developer.nvidia.com/cuda-gpus) but according to [this review](https://www.anandtech.com/show/13973/nvidia-gtx-1660-ti-review-feat-evga-xc-gaming/2), the compute capability is 7.5. This card lacks tensor cores, so perhaps some code is getting confused and taking a slow path because it assumes tensor cores are present? The [mixed precision guide](https://www.tensorflow.org/guide/mixed_precision#supported_hardware) claims that all cards with compute capability of at least 7.0 greatly benefit from mixed precision; is this true of the GTX 1660 Ti?", "> I cannot reproduce on a Titan V or a P100, the latter which does not have tensor cores. My guess is that this issue is exclusive to the Geforce 16 series. Unfortunately I do not have such a card to debug.\r\n\r\nThank you so much for even trying to reproduce this. If I could get this problem fixed, it would save me literally weeks of compute time for a project I'm working on.", "Sorry, I don't know how labels work on these issues/threads but it seems like this issue is still \"awaiting tensorflower.\" Was the label removed in error?", "I'm not sure how significant labels are. I don't think they are very important, but I'll add the label back.", "Having the same issue here with a GTX 1660 (non Ti). Mixed precision performance is roughly 5 times slower than normal 32 bit.", "Same problem with 1660ti max-q gpu"]}, {"number": 44174, "title": "ESP32: Register_RESIZE_NEAREST_NEIGHBOR not a member ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Ubuntu20\r\n- TensorFlow installed from conda:\r\n- Tensorflow version 2.3\r\n- ESP32\r\n\r\n\r\nI try to run a cnn on esp32 cam. When defining the operators with \r\n```\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,\r\n    tflite::ops::micro::Register_RESIZE_NEAREST_NEIGHBOR(),\r\n    1, 4);\r\n```\r\nand compiling I receive the error \r\n`'Register_RESIZE_NEAREST_NEIGHBOR' is not a member of 'tflite::ops::micro'\r\n`\r\n\r\nThis is my code:\r\n```\r\n#include \"TensorFlowLite_ESP32.h\"\r\n#include \"tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h\"\r\n#include \"tensorflow/lite/experimental/micro/kernels/micro_ops.h\"\r\n\r\n#include \"tensorflow/lite/experimental/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/experimental/micro/micro_interpreter.h\"\r\n\r\n\r\n// Create a memory pool for the nodes in the network\r\nconstexpr int tensor_pool_size = 25 * 1024; // trial and erro approach to get the right size\r\nuint8_t tensor_pool[tensor_pool_size];\r\n\r\n// Define the model to be used\r\nconst tflite::Model* tflite_model;\r\n\r\n// Define the interpreter\r\ntflite::MicroInterpreter* interpreter;\r\n\r\n// Input/Output nodes for the network\r\nTfLiteTensor* input;\r\nTfLiteTensor* output;\r\n\r\n\r\n// define array that contains pixel values (gray scale, 0-255)\r\n#define HEIGHT 120\r\n#define WIDTH 160\r\n\r\nuint8_t img_array[HEIGHT * WIDTH] = { 0 };\r\n\r\nvoid setup() \r\n{\r\n\r\n\t// Load tflite model\r\n\tSerial.println(\"Loading Tensorflow model....\");\r\n\ttflite_model = tflite::GetModel(cnn_model);\r\n\tSerial.println(\"Model loaded!\");\r\n\r\n  // Pull in only needed operations (should match NN layers)\r\n  // Available ops:\r\n  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/micro_ops.h\r\n  static tflite::MicroMutableOpResolver micro_mutable_op_resolver;\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_FULLY_CONNECTED,\r\n    tflite::ops::micro::Register_FULLY_CONNECTED(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_MAX_POOL_2D,\r\n    tflite::ops::micro::Register_MAX_POOL_2D(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_SOFTMAX,\r\n    tflite::ops::micro::Register_SOFTMAX(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_RESHAPE,\r\n    tflite::ops::micro::Register_RESHAPE(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_CONV_2D,\r\n    tflite::ops::micro::Register_CONV_2D(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_QUANTIZE,\r\n    tflite::ops::micro::Register_QUANTIZE(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_DEQUANTIZE,\r\n    tflite::ops::micro::Register_DEQUANTIZE(),\r\n    1, 4);\r\n  micro_mutable_op_resolver.AddBuiltin(\r\n    tflite::BuiltinOperator_RESIZE_NEAREST_NEIGHBOR,\r\n    tflite::ops::micro::Register_RESIZE_NEAREST_NEIGHBOR(),\r\n    1, 4);\r\n    \r\n  // Instantiate the ErrorReporter and MicroErrorReporter\r\n\tstatic tflite::ErrorReporter* error_reporter;\r\n\tstatic tflite::MicroErrorReporter micro_error;\r\n\terror_reporter = &micro_error;\r\n\r\n\t// Instantiate the interpreter \r\n\tstatic tflite::MicroInterpreter static_interpreter(\r\n\t\ttflite_model, micro_mutable_op_resolver, tensor_pool, tensor_pool_size, error_reporter\r\n\t);\r\n\r\n\tinterpreter = &static_interpreter;\r\n\r\n\t// Allocate the the model's tensors in the memory pool that was created.\r\n\tSerial.println(\"Allocating tensors to memory pool\");\r\n\tif(interpreter->AllocateTensors() != kTfLiteOk) {\r\n\t\tSerial.println(\"There was an error allocating the memory...ooof\");\r\n\t\treturn;\r\n\t}\r\n\r\n\t// Define input and output nodes\r\n\tinput = interpreter->input(0);\r\n\toutput = interpreter->output(0);\r\n}\r\n```\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Change this line:\r\n`static tflite::MicroMutableOpResolver micro_mutable_op_resolver;`\r\n\r\nto\r\n\r\n`static tflite::MicroMutableOpResolver<10> micro_mutable_op_resolver;`  and it should work fine.\r\n\r\n10 is the number of OPS you need to register. Maybe it's different in your case."]}, {"number": 44173, "title": "Not able to convert model to c array after quantization", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution Ubuntu 20.04:\r\n- TensorFlow installed from source:\r\n- Tensorflow version 2.3:\r\n- Target platform Arduino Nano 33\r\n\r\n**Describe the problem**\r\nI am optimizing a model for the Arduino Nano 33. After training I quantized the model using the tf lite converter. The resulting tf-lite file is as expected much smaller, but I am not able convert it now using the xxd -i command. The non quantized model however, does work as expected and I am able to run inference with it on the microcontroller.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI trained a functional keras model with tf 2.3 and gpu acceleration. After that I used the tf-lite converter class to quantize the model with my validation set and also did a conversion without quantization. Subsequently, I used the xxd -i command to convert the tflite files to a .cc file. The one without quantization looks as expected and I can copy it into my arduino code. The quantized model on the other hand is not a byte array. The file is made of lines like this:\r\n\r\n```\r\n00000000: 2400 0000 5446 4c33 0000 0000 0000 0000  $...TFL3........\r\n00000010: 0000 1200 1c00 0400 0800 0c00 1000 1400  ................\r\n00000020: 0000 1800 1200 0000 0300 0000 fc17 0000  ................\r\n00000030: 7004 0000 5804 0000 3c00 0000 0400 0000  p...X...<.......\r\n00000040: 0100 0000 0c00 0000 0800 0c00 0400 0800  ................\r\n00000050: 0800 0000 0800 0000 1a00 0000 1300 0000  ................\r\n00000060: 6d69 6e5f 7275 6e74 696d 655f 7665 7273  min_runtime_vers\r\n```\r\n\r\nCan anyone provide information to this problem?\r\n\r\n\r\n", "comments": ["Short update: When quantizing the model with dynamic quantization (converter.optimizations = [tf.lite.Optimize.DEFAULT]), I am able to convert it to a c array with xxd. However, I can not load it onto my arduino, probably because mixed datatypes are not supported by the microcontroler?"]}, {"number": 44170, "title": "Missing compiler flags for some Cortex-M target architectures", "body": "@tensorflow/micro\r\n\r\nExact reproduction steps can not be shared here since they rely on internal systems. However, it is very likely that the specific problem that I am seeing can be reproduced on a sparkfun edge as well.\r\n\r\nHigh level steps:\r\n\r\n 1. Generated a static lib for an M4F with:\r\n     ```\r\n     make -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=cortex_m_generic TARGET_ARCH=cortex-m4+fp TAGS=cmsis-nn microlite\r\n    ```\r\n 1. Link into a binary and then attempt to run the binary (in my case via qemu)\r\n\r\n 1. qemu gives an error like this:\r\n    ```\r\n    qemu: uncaught target signal 4 (Illegal instruction) - core dumped\r\n    ```\r\n\r\nIn this case I isolated the problem to a missing\r\n\r\n```TARGET_SPECIFIC_FLAGS += -D__FPU_PRESENT=1 -mfpu=fpv4-sp-d16```.\r\n\r\nMore generally, it seems like there might be such omissions for the other target architectures as well.\r\n\r\n@mansnils @freddan80 would you be willing to set the correct flags for both arm-gcc and armclang for all the target architectures?", "comments": ["@advaitjain thanks for looking into this. we're trying to clarify this with out compiler experts", "tagging @mansnils @felix-johnny ", "@advaitjain It seems GCC7 does not add FP capabilities to CPU, not by default nor can you use extensions. Our goal is to update GCC to version x.y.z once we have verified it to be working."]}, {"number": 44162, "title": "cpu cores not fully utilized causing gpu bottleneck", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHi, I am finding that when running a step, only 3 out of 8 cpu cores are pegged at 100 percent and gpu is underutilized at 50 or 60 pct. other 5 cpu cores are at 0 pct. I am assuming that this may be causing cpu bottleneck in my model. \r\nI also see during backward process, one of compute process seems to run entire backward op (with tf profiler) but i see 3 cores getting utilized. \r\nI have tried setting these config but i don't see much benefit. Any pointers to if there is any config can utilize to provide more cpu parallelism.\r\nnum_threads = 8\r\ntf.config.threading.set_inter_op_parallelism_threads(2*num_threads)\r\ntf.config.threading.set_intra_op_parallelism_threads(2*num_threads)\r\nAs you see in below image there are lagger compute node where i see only 3 cores getting utilized and underutilized gpu. \r\n\r\nAlso seems like lagger Execute::process correponds to backward pass as i see Adam operator running for timeframe.\r\n\r\n<img width=\"1177\" alt=\"Screen Shot 2020-10-19 at 5 32 24 PM\" src=\"https://user-images.githubusercontent.com/1817689/96525925-1a5e0980-1231-11eb-980b-69fc66f12b17.png\">\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Vikas-kum \r\nCan you please let us know the tf version on which the issue is faced.", "> @Vikas-kum\r\n> Can you please let us know the tf version on which the issue is faced.\r\n\r\nTf 2.3 . \r\nI think the  operations especially tf.where calls seems to be not using all cores and that is causing cpu bottleneck. Is there a way to parallelize these calls over all cores?", "> > @Vikas-kum\r\n> > Can you please let us know the tf version on which the issue is faced.\r\n> \r\n> Tf 2.3 .\r\n> I think the operations especially tf.where calls seems to be not using all cores and that is causing cpu bottleneck. Is there a way to parallelize these calls over all cores?\r\n\r\n@Saduf2019 my bad, actually its tf 2.2 . sorry for confusion.\r\n", "@Vikas-kum Can you please try `tf-nightly` and let us know whether this is an issue even with latest `tf-nightly`? Thanks!"]}, {"number": 44153, "title": "bool disable_per_channel: per-channel vs. per-layer quantization", "body": "I want to switch the quantization scheme to either per-tensor or per-channel quantization e.g. [disable_per_channel](https://github.com/tensorflow/tensorflow/blob/762df3454fa85fa0b842fcf1a08429d093835bc5/tensorflow/compiler/mlir/lite/quantization/quantization_config.h#L53) set as true (no quantization per-channel) within from [quantization_config.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/quantization/quantization_config.h)\r\n```\r\n  // When set to true, quantization will be done per-tensor. Currently, this\r\n  // option is only valid when the quantization parameters need to be created by\r\n  // scanning the constant content (post-training quantization or QAT without\r\n  // weight FakeQuant).\r\n  bool disable_per_channel = false;\r\n\r\n```\r\n\r\nDo you have any example to switch between either per-channel or per-layer quantization with [Python API](https://www.tensorflow.org/lite/convert#python_api) or [Command Line Tool](https://www.tensorflow.org/lite/convert#cmdline) call?\r\n\r\nIs it possible somehow to use [`mlir_quantize`](https://github.com/tensorflow/tensorflow/blob/2396803aa12cff02dbc0aaad05c7614a4976843f/tensorflow/lite/python/convert.py#L126) function, if so, how?\r\n", "comments": []}, {"number": 44126, "title": "keras.models.load_model() checking for keras configuration mismatch", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No (depending on how time-consuming it ends up being)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, if the model is saved with the `\"image_data_format\": \"channels_first\"` option configured in keras.json it produces a shapes mismatch when loaded in an environment with `\"image_data_format\": \"channels_last\"` option. It's really difficult to debug since all issues found in Google point to mistakes in how the network is put together and results in hours wasted in looking for possible culprits.\r\n\r\nI suggest that relevant parameters from keras.json configuration can be saved together with the model's metadata and keras.models.load_model() function could check for mismatch that will prevent the model from being loaded or running correctly in the environment it is being loaded in.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nThe world\r\n\r\n**Any Other info.**\r\n", "comments": []}]