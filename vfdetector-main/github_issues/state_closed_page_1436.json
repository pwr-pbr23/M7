[{"number": 9888, "title": "fixing $PATH search when running ffmpeg.", "body": "Previous code bailed on first failure of realpath() which didn't proceed to the rest of $PATH", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Being a bug fix, this might ought to be merged rather to r1.1, but I didn't know if such changes are appropriate for there.  Fine with me either way."]}, {"number": 9887, "title": "Allow tensor as iou_threshold parameter to tf.image.non_max_suppression.", "body": "I have a use case where dynamically changing the iou_threshold parameter on a frozen graph is required. I'm not 100% positive this doesn't violate the backwards compatibility rules, and it does make the iou_threshold a required parameter. Feedback from the maintainers would be appreciated.", "comments": ["Can one of the admins verify this patch?", "@xodus7, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Since this changes the functionality of the op, I believe you have to do this by \r\n\r\n1) adding a new kernel named NonMaxSuppressionOpV2 \r\n2) migrate all producers (e.g. Python wrappers) to use that kernel instead.\r\n\r\nSee this for more details:\r\nhttps://www.tensorflow.org/programmers_guide/data_versions", "(Though you have to wait 3 weeks after checking in the new Op/OpKernel before changing the python code).", "Ok, I moved my changes into a new NonMaxSuppressionV2Op as suggested.", "@tensorflow-jenkins test this please"]}, {"number": 9886, "title": "[Docs] Fixing old model link", "body": "An addition to https://github.com/tensorflow/tensorflow/pull/9882", "comments": ["Can one of the admins verify this patch?", "Hi @Carmezim, is there any reason why your link points to a particular commit? By default I would want it to point to https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py.", "Hi @nealwu. I am so sorry. I was in the wrong branch, you are absolutely right. I wanted to point to r1.1. I fixed to point to master. Thanks for catching it!", "Can one of the admins verify this patch?", "Looks good!"]}, {"number": 9885, "title": "Temporarily disbale //tensorflow/contrib/factorization:gmm_test", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @rohan100jain to be potential reviewers."]}, {"number": 9884, "title": "No longer use openjdk8 PPA in Dockerfile.devel", "body": "Ubuntu 16.04 has an openjdk8 package, so use that instead of a PPA with a backported version, which was a holdover from when the base image was ubuntu 14.04", "comments": ["Can one of the admins verify this patch?", "@staticfloat, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @caisq to be potential reviewers.", "@staticfloat Thank you for the PR. The changes look good to me. But can you make the same changes in this file as well?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu", "@caisq I've done so.  Feel free to squash the two commits together when you merge, or if you prefer, I can do that when I get back to a computer with command-line `git`."]}, {"number": 9883, "title": "Delete .mention-bot file, not used anymore.", "body": "", "comments": []}, {"number": 9882, "title": "[Docs] Fixing broken repository links", "body": "Fixes #9873", "comments": ["Can one of the admins verify this patch?", "Looks good. Thanks for submitting!\r\n\r\n@caisq: I've previously submitted a bug internally for this (b/37962228) since we would like the internal links to point to the internal code. However, I'd say fixing these broken links for now is a higher priority.", "@nealwu, thanks for the review. Understood. Will merge this PR pending tests passing.", "@tensorflow-jenkins test this please.", "I couldn't add another commit here since I happened to have made this quickly on GH UI so I made \r\nanother PR to address one last old link #9886", "Just realized this PR was merged into the r1.1 branch rather than into master. I'm submitting another PR to revert this on the r1.1 branch as well: https://github.com/tensorflow/tensorflow/pull/9990", "@nealwu Thank you. I noticed that some fixes already present on master were not on tensorflow.org which apparently was following at least branch r1.1 so I decided at the time submit to the latter to reflect these changes on the website tutorials. ", "I see. In the future it's typically best to submit to master first and then ask us to cherry-pick the changes into the appropriate release branch. That way we avoid the situation where we merge into the release branch but end up forgetting to merge into master.", "Good to know, thank you for the insight!"]}, {"number": 9881, "title": "how can creat the module clstm((convolution Long short-term memory)) in tensorflow", "body": "I wish to implement clstm's network architecture in the tensorflow, to facilitate extension and modifications to the network. already create in module?", "comments": ["@jeansely this question is better suited for StackOverflow.", "Here's an implementation: https://github.com/carlthome/tensorflow-convlstm-cell", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9880, "title": "Building a Convolutional Neural Network", "body": "### Describe the problem\r\nI just completed the implementation of A Guide to TF Layers: Building a Convolutional Neural Network for the MNIST data set. The training model successfully ran and gave accuracy of 97.3%.\r\n\r\nHowever, the tutorial does not mention how to use this new trained model to supply own images and see the predictions. Does anyone know how to use the output of the training model to make predictions? I see in the tmp/mnist_convnet_model$ folder, there are some output files like  .pbtxt , meta files and index files. But I can't find instructions to use them for making predictions on my own images.\r\n", "comments": ["GitHub issues are for issues and not to request help. I would strongly suggest you to ask here http://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nYou might have better chance to get an answer.", "@rohit7033 is this resolved? If not please raise or link a Stack Overflow question, link it here and **close** the issue. I can come and help you if required. ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9879, "title": "Fix building r0.11 from source", "body": "Fixes a bunch of issues I had building from source on r0.11, including #9130 and #5143. With these changes I was able to successfully build tensorflow with cuda support with bazel 0.4.5.", "comments": ["Can one of the admins verify this patch?", "@vladfi1, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @mrry and @jart to be potential reviewers.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@kashif @aj-michael are you ok with my cherry-picking your commits here?", "This looks good, and thank you for the contribution, but help me understand why it's necessary? This branch is quite old. I'm wondering why no one else has run into problems with it.", "I suspect people are simply using the pre-built pip wheels (this included me until recently). These changes were only necessary to build from source.", "I am opposed to accepting PRs to old release branches.\r\nThe biggest reason is, bazel had a lot of backwards incompatible changes.\r\nWhen fixing the build for latest bazel version included in the dockerfiles in this branch.\r\n\r\nUnless anyone else has any objections, I will close this PR. "]}, {"number": 9878, "title": "Does tensorflow support forward multiple times then backward once like average_loss in Caffe ?", "body": "Hi, everyone. In my training tasks, the batch size is expected to be  large My training task is a little special that costs a lot of GPU memory. Now only set batch size = 6 almost run out of entire GPU memory 11GB GTX 1080Ti. However, I need to enlarge my batch size. In Caffe Platform, there is an **average_loss**  setting in solver.prototxt to perform forward multiple times and compute the average loss, then backward once. I scaned the documentation of tensorflow but there is no operation can implement such function. Does anyone have any ideas or how to use the existing op to implement this process?  Thanks\r\n\r\nFollowing is my system info:\r\nGTX1080Ti\r\nCUDA version: 8.0\r\ncuDNN:6.0\r\nTensorflow: r1.0.1\r\nUbuntu 16.04", "comments": ["Hi @YihangLou, this question is better suited for StackOverflow. Github issues are only for bugs and feature requests.\r\nThanks.", "I'm gonna close this. @YihangLou, please open a new issue if you can't find a good solution via  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) and would like to request a new feature."]}, {"number": 9877, "title": "gif decoder returns 4-D tensor, remove the first dim", "body": "What returned by the gif decoder returns is a 4-D tensor [frames, height, width, channels]. Since only the single frame gif will be used in this label_image, frames should be 1. So we can use Squeeze() to remove it. \r\n\r\nThe patch solve https://github.com/tensorflow/tensorflow/issues/8572", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 9876, "title": "Fix issue #9858.", "body": "Added some byte swapping code to solve endianness problem.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@rmlarsen like this?", "I added a test case for endianness. By the way, according to the documentation (https://www.tensorflow.org/versions/master/api_docs/python/tf/decode_raw), I think this op should behave consistently on platforms with different endianness, so I removed the endianness test in an existing test case. If the original behavior in that test case is expected, maybe you should make changes on the documentation.", "@tensorflow-jenkins test this please"]}, {"number": 9875, "title": "Fix contrib.seq2seq.BeamSearchDecoder #9855", "body": "Fixed Issue 9855 by shielding the `top_k` op's input with `min`.\r\nAlso fixed wrong `log_probs `shape at `_get_scores` method docstring. ", "comments": ["Can one of the admins verify this patch?", "scores_flat.shape[-1].value) might be None for dynamic batch size ? ", "@chenghuige That is true. It will be better to treat two values as tensor since top_k operation can handle `k` as tensor.\r\n```\r\nfrom tensorflow.python.ops import gen_math_ops\r\n\r\n...\r\n\r\nnext_beam_size = gen_math_ops.minimum(\r\n  ops.convert_to_tensor(\r\n    beam_width, dtype=tf.int32, name=\"beam_width\"),\r\n  ops.convert_to_tensor(\r\n    scores_flat.shape[-1], dtype=tf.int32, name=\"num_available\"))\r\n```\r\nWill this code be OK?", "@shuuki4 \r\n\r\nYes, that should work, but use math_ops.minimum, not gen_math_ops.minimum.", "@tensorflow-jenkins test this please", "@shuuki4 the unit test is failing, can you take a look?", "@rmlarsen Error was due to the unknown shape after cond operation. Fixed the problem.", "@tensorflow-jenkins test this please", "Thanks for the contribution!"]}, {"number": 9874, "title": "Branch 155393864", "body": "", "comments": ["Jenkins, test this please.", "@tensorflow-jenkins test this please", "The \"Linux CPU Test\" failed due to the following reason:\r\n```\r\nERROR: /workspace/bazel_pip/tensorflow/contrib/util/BUILD:65:1: Target '//tensorflow/python:framework' is not visible from target '//bazel_pip/tensorflow/contrib/util:util_py'. Check the visibility declaration of the former target if you think the dependency is legitimate.\r\nERROR: /workspace/bazel_pip/tensorflow/contrib/util/BUILD:65:1: Target '//tensorflow/python:platform' is not visible from target '//bazel_pip/tensorflow/contrib/util:util_py'. Check the visibility declaration of the former target if you think the dependency is legitimate.\r\nERROR: /workspace/bazel_pip/tensorflow/contrib/util/BUILD:65:1: Target '//tensorflow/python:util' is not visible from target '//bazel_pip/tensorflow/contrib/util:util_py'. Check the visibility declaration of the former target if you think the dependency is legitimate.\r\n```\r\n\r\nIs this related to this commit, @ringw ? https://github.com/tensorflow/tensorflow/pull/9874/commits/733bff53926717bb9583d4833ba062c58f27960futil", "Yes that must be the culprit--weird that our open source presubmits passed. Rolling back."]}, {"number": 9873, "title": "Can't not find the tutorial code for Convolutional Neural network", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThe tutorial here: https://www.tensorflow.org/tutorials/deep_cnn, the code can't find in github\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi @Freshield, sorry you couldn't find the code. It was moved to a different repository (Models). For now while it's fixed please access the code [here](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/). \r\nThanks for reporting.", "Thank you very much\n\n2017\u5e745\u670813\u65e5 13:26\uff0c\"Adriano Carmezim\" <notifications@github.com>\u5199\u9053\uff1a\n\n> Hi @Freshield <https://github.com/freshield>, sorry you faced issues\n> finding the code. It was moved to a different repository (Models), for now\n> while it's fixed please go here\n> <https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/>\n> to access the code.\n> Thanks for reporting.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9873#issuecomment-301262177>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOdqPvfkKU03AcHJm8U4ePrN6Gmjdkmaks5r5efRgaJpZM4NZ7Qv>\n> .\n>\n"]}, {"number": 9872, "title": "www.tensorflow.org/versions/ incorrectly lists 0.12 as the most recent stable branch", "body": "https://www.tensorflow.org/versions/ says that 0.12 is most recent stable branch, which is causing new users to install the older version --\r\nhttps://github.com/tensorflow/tensorflow/issues/9590#issuecomment-301211285", "comments": ["Duplicate of #8429\r\nWas raised a while ago but no traction just yet!", "Closing as duplicate."]}, {"number": 9871, "title": "errors in ipython sessions cause core dump or segfault", "body": "------------------------\r\n\r\n### System information\r\n\r\nPython 2.7.13 :: Anaconda custom (64-bit)\r\nipython :: 5.3.0\r\nlinux :: 3.16.0-4-amd64\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh gives:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 4.9.2-10) 4.9.2\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmsgpack-numpy (0.3.9)\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.0.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.0.1\r\ntf.GIT_VERSION = v1.0.0-65-g4763edf-dirty\r\ntf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri May 12 19:38:18 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    Off  | 0000:05:00.0      On |                  N/A |\r\n| 61%   67C    P0    36W /  75W |    145MiB /  1998MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN X (Pascal)    Off  | 0000:06:00.0     Off |                  N/A |\r\n| 54%   84C    P2    68W / 250W |  11530MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN X (Pascal)    Off  | 0000:09:00.0     Off |                  N/A |\r\n| 45%   78C    P2    85W / 250W |  11820MiB / 12189MiB |     35%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1518    G   /usr/bin/X                                     141MiB |\r\n|    1     20850    C   python                                       11527MiB |\r\n|    2     18363    C   python                                       11817MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n== cat /etc/issue ===============================================\r\nLinux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 4.9.2-10) 4.9.2\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux leto26 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2 (2017-04-30) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmsgpack-numpy (0.3.9)\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.0.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.0.1\r\ntf.GIT_VERSION = v1.0.0-65-g4763edf-dirty\r\ntf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri May 12 19:38:27 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    Off  | 0000:05:00.0      On |                  N/A |\r\n| 61%   67C    P0    36W /  75W |    145MiB /  1998MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN X (Pascal)    Off  | 0000:06:00.0     Off |                  N/A |\r\n| 54%   84C    P2    68W / 250W |  11530MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN X (Pascal)    Off  | 0000:09:00.0     Off |                  N/A |\r\n| 46%   78C    P2   155W / 250W |  11820MiB / 12189MiB |     38%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1518    G   /usr/bin/X                                     141MiB |\r\n|    1     20850    C   python                                       11527MiB |\r\n|    2     18363    C   python                                       11817MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n== cat /etc/issue ===============================================\r\nLinux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 4.9.2-10) 4.9.2\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmsgpack-numpy (0.3.9)\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.0.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.0.1\r\ntf.GIT_VERSION = v1.0.0-65-g4763edf-dirty\r\ntf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri May 12 19:40:02 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |\r\n| 22%   46C    P8    18W / 250W |      1MiB / 12206MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\r\n== cat /etc/issue ===============================================\r\nLinux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 4.9.2-10) 4.9.2\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux leto06 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmsgpack-numpy (0.3.9)\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.2.0)\r\ntensorflow-gpu (1.0.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.0.1\r\ntf.GIT_VERSION = v1.0.0-65-g4763edf-dirty\r\ntf.COMPILER_VERSION = v1.0.0-65-g4763edf-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/cudnn_v5.1:/Tmp/lisa/os_v5/nccl_8/lib:/Tmp/lisa/os_v5/lib:/Tmp/lisa/os_v5/lib64:/usr/local/lib:/usr/lib64/atlas:/Tmp/lisa/os_v5/lib32:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib:/u/kruegerd/.local/lib64/:/u/kruegerd/.local/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri May 12 19:40:54 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |\r\n| 22%   47C    P8    18W / 250W |      1MiB / 12206MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\r\n\r\n\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" gives\r\n('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n\r\n\r\n\r\n### Describe the problem\r\nWhen I run tensorflow code (e.g. keras's mnist_cnn.py) in ipython, errors and Ctrl+C often crashes the interactive ipython session with a core dump or segfault, e.g....\r\n\r\n\r\n### Source code / logs\r\n\r\nIn [1]: run mnist_cnn.py \r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nx_train shape: (60000, 1, 28, 28)\r\n60000 train samples\r\n10000 test samples\r\n/u/kruegerd/python_modules/keras/keras/backend/tensorflow_backend.py:2252: UserWarning: Expected no kwargs, you passed 1\r\nkwargs passed to function are ignored with Tensorflow backend\r\n  warnings.warn('\\n'.join(msg))\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/12\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\r\n23680/60000 [==========>...................] - ETA: 10s - loss: 0.5787 - acc: 0.8213^C---------------------------------------------------------------------------\r\nKeyboardInterrupt                         Traceback (most recent call last)\r\n/u/kruegerd/python_modules/keras/examples/mnist_cnn.py in <module>()\r\n     65           epochs=epochs,\r\n     66           verbose=1,\r\n---> 67           validation_data=(x_test, y_test))\r\n     68 score = model.evaluate(x_test, y_test, verbose=0)\r\n     69 print('Test loss:', score[0])\r\n\r\n/u/kruegerd/python_modules/keras/keras/models.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\r\n    863                               class_weight=class_weight,\r\n    864                               sample_weight=sample_weight,\r\n--> 865                               initial_epoch=initial_epoch)\r\n    866 \r\n    867     def evaluate(self, x, y, batch_size=32, verbose=1,\r\n\r\n/u/kruegerd/python_modules/keras/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\r\n   1499                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\r\n   1500                               callback_metrics=callback_metrics,\r\n-> 1501                               initial_epoch=initial_epoch)\r\n   1502 \r\n   1503     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\r\n\r\n/u/kruegerd/python_modules/keras/keras/engine/training.pyc in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\r\n   1153                 batch_logs['size'] = len(batch_ids)\r\n   1154                 callbacks.on_batch_begin(batch_index, batch_logs)\r\n-> 1155                 outs = f(ins_batch)\r\n   1156                 if not isinstance(outs, list):\r\n   1157                     outs = [outs]\r\n\r\n/u/kruegerd/python_modules/keras/keras/backend/tensorflow_backend.pyc in __call__(self, inputs)\r\n   2229         session = get_session()\r\n   2230         updated = session.run(self.outputs + [self.updates_op],\r\n-> 2231                               feed_dict=feed_dict)\r\n   2232         return updated[:len(self.outputs)]\r\n   2233 \r\n\r\n/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\n/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1020   def _do_call(self, fn, *args):\r\n   1021     try:\r\n-> 1022       return fn(*args)\r\n   1023     except errors.OpError as e:\r\n   1024       message = compat.as_text(e.message)\r\n\r\n/u/kruegerd/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1002         return tf_session.TF_Run(session, options,\r\n   1003                                  feed_dict, fetch_list, target_list,\r\n-> 1004                                  status, run_metadata)\r\n   1005 \r\n   1006     def _prun_fn(session, handle, feed_dict, fetch_list):\r\n\r\nKeyboardInterrupt: \r\n\r\nFatal Python error: GC object already tracked\r\nAborted (core dumped)\r\n\r\n", "comments": ["Thanks @capybaralet, noted.\r\n", "Closing due to lack of recent activity. Please create new issue using one of the issue templates, if you are still facing this problem. Thanks!\r\n\r\n"]}, {"number": 9870, "title": "Incorrect inference in convnet with batch size >65535", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0.44\r\n- **GPU model and memory**: GTX 1080, 16GB\r\n- **Exact command to reproduce**: See source code below\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen inference is performed on a convolutional NN with batch sizes larger than 65,535, the outputs on cases after 65,535 are incorrect.  The convolution operation appears to be important because I have not been able to reproduce this problem with a fully connected NN.\r\n\r\nIn the source code below, I am training a convnet with one convolutional layer to distinguish between sine waves of two periods.  If I perform inference on a batch size of 100,000, I find that the output for case number 65,535 is correct, but the output for case number 65,536 is zero.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nn_features = 80\r\ndef gen_data(size):\r\n  '''Generate sine waves of one of two periods.'''\r\n  labels = np.random.randint(0, 2, size)\r\n  periods = np.zeros(size)\r\n  periods[labels == 0] = 1\r\n  periods[labels == 1] = 10\r\n\r\n  x = np.arange(0, n_features)\r\n  data = np.zeros((size, n_features))\r\n  for i in range(size):\r\n    data[i] = np.sin(x / periods[i])\r\n\r\n    data = np.expand_dims(data, axis=2)\r\n    return (data, labels)\r\n\r\nX = tf.placeholder(tf.float32, [None, n_features, 1])\r\nY = tf.placeholder(tf.int32, [None])\r\n\r\nW_conv = tf.get_variable('W_conv', [8, 1, 16])\r\npreact = tf.nn.conv1d(X, W_conv, stride=1, padding='SAME')\r\npooled = tf.nn.pool(preact, window_shape=[2], pooling_type='MAX', padding='SAME', strides=[2])\r\nconv_output = tf.contrib.layers.flatten(tf.nn.relu(pooled))\r\n\r\nW_out = tf.get_variable('W_out', [int(n_features/2) * 16, 1])\r\nlogits = tf.squeeze(tf.matmul(conv_output, W_out))\r\n\r\nxentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\r\n    labels=tf.cast(Y, tf.float32), logits=logits))\r\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(xentropy)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n# Train a little while with small batches\r\nfor i in range(1000):\r\n  data, labels = gen_data(size=128)\r\n  sess.run(train_step, feed_dict={X: data, Y: labels})\r\n\r\n# Now try performing inference on a much larger batch\r\nlarge_batch = np.zeros((100000, n_features, 1))\r\nlarge_batch[2**16 - 1] = data[0]\r\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\r\n\r\n# This should be some non-zero number\r\nprint('Logit for input number 65,535:', logits_[2**16-1])\r\n\r\n# Now do the same thing, but putting the input in spot 65,536\r\nlarge_batch = np.zeros((100000, n_features, 1))\r\nlarge_batch[2**16] = data[0]\r\nlogits_ = sess.run(logits, feed_dict={X: large_batch})\r\n\r\n# This is now zero on my system\r\nprint('Logit for input number 65,536:', logits_[2**16])\r\n```", "comments": ["@andydavis1 can you take a look at this or redirect to someone else who can? Thanks!", "I forgot to add that the issue is with the GPU --- I don't see this problem when I assign the operations to the CPU.", "zheng-xq@ have you seen something like this before for large batches? Wondering if some code assumes batch size can always fit in 16 bits...", "Upgrading to version 8.0.61 of CUDA fixed this issue for me.", "Whoops, I spoke too soon!  I had accidentally assigned my test scripts to the CPU after upgrading CUDA, which made it look like it was working.  But the issue is still there when I run on the GPU.", "@zheng-xq, I think that andydavis1 accidentally flipped the @ sign with your username so you may never have gotten a notification for this.", "I have similar issues on testing a 70k train set on a small convnet. Get 99+% accuracy on first 64k data, while get about 50% accuracy on other remain data. No problem when testing them in small batch size.", "This issue is GPU only.\r\nset CUDA_VISIBLE_DEVICES=-1, then works fine.\r\n\r\nMy environment is Tensorflow 1.3.0 on Win7 sp1 x64, GTX 1080.\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Could be a Cudnn bug. Adding @yzhwang to investigate", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Same problem here with a small CNN, although using Keras with tensorflow. Batches larger than 65535 only seem to return correct results when using CPU. System info: Ubuntu 16.04, GTX 1080ti, tensorflow 1.3.0 and CUDA Version 8.0.61\r\n  \r\nAlso the following issue seems related:\r\nhttps://github.com/tensorflow/tensorflow/issues/13869", "I could reproduce this with CUDA 8 with cudnn 6 and I've verified that with CUDA9 + cudnn7, the issue is gone. I think this is a cudnn bug, please refer to: https://github.com/tensorflow/tensorflow/issues/13869#issuecomment-338433690\r\n\r\nCurrent OSS build from source as well as pip package for nightly (and 1.5 once it comes out) is CUDA9 + cudnn7, so expect this issue to be gone in these versions. For v1.4, it still uses CUDA8+cudnn6, so unfortunately, the bug will still exists if you use any version < v1.5."]}, {"number": 9869, "title": "Adding the benchmarks and performance models to 1.1", "body": "For doc gen.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Closing at the request of @tfboyd "]}, {"number": 9868, "title": "Chrome timeline for Keras?", "body": "I've been looking at the chrome timeline to profile my Keras models, but have been unable to find any documentation on how I could use it with my Keras models.\r\nI am running keras with a tf backend, and my sequential models are all built in keras. For instance, this is how my model is being built:\r\n\r\n\r\n    'model.fit_generator( \\\r\n\t\t\t\tgenerator= data_gen(args, 1), \\\r\n\t\t\t \tsteps_per_epoch=tr_steps, \\\r\n\t\t\t \tepochs=args.epochs, \\\r\n\t\t\t \tvalidation_data=data_gen(args, 2), \\\r\n\t\t\t \tvalidation_steps=val_steps, \\\r\n\t\t\t \tverbose=1, \\\r\n\t\t\t \tcallbacks=[checkpointer])`\r\n\r\n\r\n \r\n\r\n\r\nI am at a loss for how I would try to generate a timeline trace for this model, and wanted to know if there is any related documentation for how I can do this?", "comments": ["This does not seem to be supported.", "You could probably hack it by monkey patching 'tf.Session' to always create session with full statistics collection (as in https://github.com/tensorflow/tensorflow/issues/7794#issuecomment-281888201) and also monkey patching the \"run\" call to dump timeline file each time session.run gets called", "This was fixed in https://github.com/fchollet/keras/pull/6693\r\n\r\nA simple example:\r\n\r\n```python\r\nfrom tensorflow.python.client import timeline\r\n\r\n# Make your keras model\r\n# ...\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nmodel.compile(loss='MSE', optimizer='Adam', options=run_options, run_metadata=run_metadata)\r\n# Run model in your usual way\r\n# ...\r\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\nwith open('timeline.ctf.json', 'w') as f:\r\n    f.write(trace.generate_chrome_trace_format())\r\n```", "@al626 training model issues multiple run calls, does this save metadata for last run call or does it merge them?", "Saves the last run call - I figured if people run multiple times for warm-up, then only the last run is relevant.", "@al626 What if you want the metadata for all run calls?", "After finding the commit that added the ability to trace, I came here looking for an example implementation - maybe a good candidate to go into the docs somewhere?", "Is the commit in the latest version of Keras? Or, do I need to build from source? \r\n\r\nAs far as example implementation I am trying something like this, but would appreciate some feedback:\r\n\r\n```\r\n        import tensorflow as tf\r\n\r\n\tfrom keras import backend as K\r\n\t\r\n\tsess = tf.Session(config=tf.ConfigProto(\r\n                    intra_op_parallelism_threads=16, inter_op_parallelism_threads=2))\r\n\r\n\trun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n\trun_metadata = tf.RunMetadata()\r\n\r\n\tK.set_session(sess)\r\n\r\n\thistory = model.fit_generator(generator = training_generator,\r\n                    steps_per_epoch = 24800//BATCH_SIZE,\r\n                    validation_data = validation_generator,\r\n                    validation_steps = 6200//BATCH_SIZE,\r\n                    verbose=1, callbacks=[model_checkpoint])\r\n\r\n\tfrom tensorflow.python.client import timeline\r\n\r\n\t# Create the Timeline object, and write it to a json\r\n\ttl = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n\tctf = tl.generate_chrome_trace_format()\r\n\twith open('timeline.json', 'w') as f:\r\n\t    f.write(ctf)\r\n\r\n```", "Can this be done with inference too? I mean can you pass the run_metadata into the .predict()? Or when you load the model from a saved file?", "I am trying to modify imdb_fasttext.py as follows (4points added). \r\nBut the chrome shows simple timeline for timeline.keras.json.\r\nIs this correct usage for chrome timeline ?\r\n\r\n```Python\r\n'''This example demonstrates the use of fasttext for text classification\r\n\r\nBased on Joulin et al's paper:\r\n\r\nBags of Tricks for Efficient Text Classification\r\nhttps://arxiv.org/abs/1607.01759\r\n\r\nResults on IMDB datasets with uni and bi-gram embeddings:\r\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\r\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\r\n'''\r\n\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import Embedding\r\nfrom keras.layers import GlobalAveragePooling1D\r\nfrom keras.datasets import imdb\r\n# Add RunMetadata for timeline 1/4\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef create_ngram_set(input_list, ngram_value=2):\r\n    \"\"\"\r\n    Extract a set of n-grams from a list of integers.\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\r\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\r\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\r\n    \"\"\"\r\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\r\n\r\n\r\ndef add_ngram(sequences, token_indice, ngram_range=2):\r\n    \"\"\"\r\n    Augment the input list of list (sequences) by appending n-grams values.\r\n\r\n    Example: adding bi-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\r\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\r\n\r\n    Example: adding tri-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\r\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\r\n    \"\"\"\r\n    new_sequences = []\r\n    for input_list in sequences:\r\n        new_list = input_list[:]\r\n        for i in range(len(new_list) - ngram_range + 1):\r\n            for ngram_value in range(2, ngram_range + 1):\r\n                ngram = tuple(new_list[i:i + ngram_value])\r\n                if ngram in token_indice:\r\n                    new_list.append(token_indice[ngram])\r\n        new_sequences.append(new_list)\r\n\r\n    return new_sequences\r\n\r\n# Set parameters:\r\n# ngram_range = 2 will add bi-grams features\r\nngram_range = 1\r\nmax_features = 20000\r\nmaxlen = 400\r\nbatch_size = 32\r\nembedding_dims = 50\r\nepochs = 5\r\n\r\nprint('Loading data...')\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nprint(len(x_train), 'train sequences')\r\nprint(len(x_test), 'test sequences')\r\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nif ngram_range > 1:\r\n    print('Adding {}-gram features'.format(ngram_range))\r\n    # Create set of unique n-gram from the training set.\r\n    ngram_set = set()\r\n    for input_list in x_train:\r\n        for i in range(2, ngram_range + 1):\r\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\r\n            ngram_set.update(set_of_ngram)\r\n\r\n    # Dictionary mapping n-gram token to a unique integer.\r\n    # Integer values are greater than max_features in order\r\n    # to avoid collision with existing features.\r\n    start_index = max_features + 1\r\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\r\n    indice_token = {token_indice[k]: k for k in token_indice}\r\n\r\n    # max_features is the highest integer that could be found in the dataset.\r\n    max_features = np.max(list(indice_token.keys())) + 1\r\n\r\n    # Augmenting x_train and x_test with n-grams features\r\n    x_train = add_ngram(x_train, token_indice, ngram_range)\r\n    x_test = add_ngram(x_test, token_indice, ngram_range)\r\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nprint('Pad sequences (samples x time)')\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\nprint('x_train shape:', x_train.shape)\r\nprint('x_test shape:', x_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\n\r\n# we start off with an efficient embedding layer which maps\r\n# our vocab indices into embedding_dims dimensions\r\nmodel.add(Embedding(max_features,\r\n                    embedding_dims,\r\n                    input_length=maxlen))\r\n\r\n# we add a GlobalAveragePooling1D, which will average the embeddings\r\n# of all words in the document\r\nmodel.add(GlobalAveragePooling1D())\r\n\r\n# We project onto a single unit output layer, and squash it with a sigmoid:\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\n# Add RunMetadata for timeline 2/4\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'],\r\n# Add RunMetadata for timeline 3/4\r\n              options=run_options,\r\n              run_metadata=run_metadata)\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          validation_data=(x_test, y_test))\r\n\r\n# Add RunMetadata for timeline 4/4\r\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\nwith open('timeline.keras.json', 'w') as f:\r\n    f.write(trace.generate_chrome_trace_format())\r\n```", "@al626 I try to get a timeline but it creates a file that only contains:\r\n```\r\n  \"traceEvents\": [\r\n        {\r\n            \"name\": \"process_name\",\r\n            \"ph\": \"M\",\r\n            \"pid\": 0,\r\n            \"args\": {\r\n                \"name\": \"Allocators\"\r\n            }\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nI get this by running the following\r\n\r\n```\r\nimport time\r\nfrom tensorflow.python.client import timeline\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\nn=3\r\nnet.compile( optimizer='Adam', loss='categorical_crossentropy', loss_weights=[1.0, 0.4, 0.16], metrics=['categorical_accuracy'], run_metadata=run_metadata)\r\n\r\nstart=time.time()\r\npred=net.predict(np.zeros((n,2048,1024,3)))\r\nend=time.time()\r\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\nwith open('timeline.ctf.json', 'w') as f:\r\n    f.write(trace.generate_chrome_trace_format())\r\n# print(np.array(pred.shape)\r\nprint((end-start)/n)\r\n```\r\nwhat am i doing wrong?\r\n\r\nEDIT: I frogot to add options=run_options, to the compile. When i add that it works\r\n", "just after compile, run timeline.Timeline(step_stats=run_metadata.step_stats) . is ok?@al626", "Related, I've also opened https://github.com/tensorflow/tensorflow/issues/18732 to require an official documentation for the High Level API.", "tensorflow.keras.Model doesn't allow run_metadata correct?", "@kenfehling You are correct, see pull request #19932", "recording timelines appears to work with tf1.11 and keras 2.2.4. I agree to @bhack that some more docs are needed. \r\n\r\nBut the timeline appears surprisingly short. I did a simple CNN with 12 epochs which takes about 1.5 minutes to complete on a Titan Xp. But the timeline only shows a record up to 3.5s (the number of convolutions is also a bit off, 12 epochs on 60k samples with a batch size of 32 and I only see 14 Conv2D calls - is tf optimizing the compute graph that badly?). \r\n\r\nI put the timeline and the keras script that produced it into this [gist](https://gist.github.com/psteinb/d3315a065a90814b435ac024e9835aab) so that people can reproduce if needed.", "here is a screen shot of a `nvprof` recorded run of the same script. One can clearly see the 12 epochs, so I am inclined to say that the timeline is incomplete.\r\n![nvvp-screenshot_2018-10-09_14-11-39](https://user-images.githubusercontent.com/1465603/46668725-9bf41900-cbcd-11e8-9726-352a57450e7c.png)\r\nIt's also interesting that tf decided to use `maxwell_winograd` style convolutions on a Titan Xp which is a Pascal card. I need to dig into this.", "@psteinb :  Do you more info on why maxwell_scudnn style convolutions on Titan Xp?  I am also seeing the same for a different use-case.  Thanks", "@rohith14 : what do you mean? Do I have more info on this or do I want more details on this? I want more information on this, if anyone could share them.", "@psteinb I am looking for more information on this.  I am seeing maxwell_scudnn style convolutions on Titan Xp and not sure why.", "@rohith14 What cuda and tf version did you use?", "@mas-dse-greina have you found a way to use it during inference? Is it possible to do it?", "https://gist.github.com/tonyreina/5bbe050c2cfceae62a1dda7d9010b692\r\n\r\nYeah. I ended up creating this which just runs the Keras model in a regular TensorFlow session.  It seems to work.", "Hi @stalagmite7 ! \r\nWe are checking to see whether you still need help in this issue .Have you checked above[ comment](https://github.com/tensorflow/tensorflow/issues/9868#issuecomment-499695742) yet?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 9867, "title": "distributed runtime leaking memory on windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'unknown' 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: follow https://www.tensorflow.org/deploy/distributed\r\n\r\n### Describe the problem\r\nThe memory footprint for parameter server keep growing.\r\nThe cause is a memory leak in a windows specific path in grpc which is fixed here:\r\nhttps://github.com/grpc/grpc/commit/fa242cba900ece728d2910e7396d02ebab4ddb2c\r\nI filed the issue issue so others don't need to spend the time debugging it and as reason to update the grpc version.\r\n", "comments": ["@mrry Looks like we should update grpc if that hasn't already happened.", "Thanks for figuring out the problem, @guschmue!\r\n\r\nI think updating gRPC is blocked by #7466, which appears to be on hold because of a different(?) bug in gRPC on Windows: https://github.com/grpc/grpc/pull/9826.\r\n\r\nHanding this off to @jhseu, who is shepherding in #7466.", "https://github.com/tensorflow/tensorflow/pull/11768 - closing.\r\n"]}, {"number": 9866, "title": "Issue with tensor flow (undefined symbol: cuDevicePrimaryCtxRetain)", "body": "When I try to import tensorflow in python I get this issue:\r\nNote: It was working fine yesterday.  It is really strange that today it no longer works.\r\nAlso note: This is my first time to ever post a question online, as I prefer to find answers to my own problems.  The issue here is that there is no documentation on this problem online and I am hitting a wall on ideas for once for how to resolve it.  I am using TF without gpu support, but am hopeful the capable team on this git can at least tell me if the problem on my end or not.  I realize it is likely something on my end, but a response to verify that would be helpful, even if the problem is perceived as too simple to bother with.\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/student/miniconda3/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/student/miniconda3/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/student/miniconda3/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/student/miniconda3/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/student/miniconda3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary? If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Thank you for the response.  It appears to be an issue with Cuda and driver compatibility on an older laptop.  Closing ticket.", "How to solve it. I have get the same problem.", "Hi,\n\nI didn't actually solve it, other than moving to a newer computer with the\nlatest version of Linux and a newer gpu.  I would recommend if you are\ntrying to use cuda, making sure that it is the appropriate version for your\ngraphics card.  I have an nvidia 720 with Ubuntu 14, and had cuda 6.5\noperational, but when I tried to use cuda 8 all these problems popped up.\nCheck cuda docs and read very thoroughly about their recommended specs for\nyour gpu.  Also make sure you are removing the newer version of cuda before\nreplacing it with the older.  Hope that helps!\nLee\n\nOn May 18, 2017 9:00 PM, \"dahangli\" <notifications@github.com> wrote:\n\n> How to solve it. I have get the same problem.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9866#issuecomment-302588385>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB0YNvUWX0tnHm9cxW52nhr5GkGRPtE_ks5r7PfLgaJpZM4NZnNI>\n> .\n>\n", "I try nvidia 710 with Ubuntu 14.04,s ok!\r\nI try nvidia 1060 with Ubuntu 14.04, Fault! -- undefined symbol: cuDevicePrimaryCtxRetain\uff01\r\n\r\nNVIDIA-Linux-x86_64-375.66.run\r\ncuda_8.0.61_375.26_linux.run\r\nCuDNN5.1\r\nAnaconda3-4.3.1-Linux-x86_64.sh \uff08Python3.6\uff09\r\nTensorflowGPU1.1.0", "Wow!  That is strange.  Did you make sure Cuda is functional with some\nsample programs?\n\nOn May 18, 2017 9:17 PM, \"dahangli\" <notifications@github.com> wrote:\n\n> I try nvidia 710 with Ubuntu 14.04, and had cuda 8.0 is ok!\n> I try nvidia 1060 with Ubuntu 14.04, and had cuda 8.0 is Fault!\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9866#issuecomment-302590486>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB0YNljjxim9P5CtwFZwxOnVgMP1nGRtks5r7PuugaJpZM4NZnNI>\n> .\n>\n", "For example, check to make sure device Query (usr->local->cuda-8.0->samples->1_Utilities->deviceQuery->make) compiles and runs with Cuda.\n\n\nSent from my MetroPCS 4G LTE Android device-------- Original message --------From: dahangli <notifications@github.com> Date: 5/18/2017  9:17 PM  (GMT-06:00) To: tensorflow/tensorflow <tensorflow@noreply.github.com> Cc: Lee-L-Boyd <lee.l.boyd@gmail.com>, State change <state_change@noreply.github.com> Subject: Re: [tensorflow/tensorflow] Issue with tensor flow (undefined symbol:\n  cuDevicePrimaryCtxRetain) (#9866) \nNVIDIA-Linux-x86_64-375.66.run\n\ncuda_8.0.61_375.26_linux.run\n\nAnaconda3-4.3.1-Linux-x86_64.sh\n\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n  \n  \n\n\n\n\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@dahangli in #9866: NVIDIA-Linux-x86_64-375.66.run\\r\\ncuda_8.0.61_375.26_linux.run\\r\\nAnaconda3-4.3.1-Linux-x86_64.sh\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/9866#issuecomment-302590573\"}}}"]}, {"number": 9865, "title": "Issue with TensorFlow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 9864, "title": "MPI based communication path for tensor exchange operations", "body": "This pull request adds an additional communication path to TensorFlow that allows Tensors to be exchanged using MPI based Send and Recieve routines when running distributed TensorFlow. T\r\nThe used MPI implementation will pick the most optimal path available between the two communication points allowing the user to take advantage of high performance networks such as Infiniband. \r\nCertain MPI implementations allow direct data-transfers from GPU buffers (CUDA Aware / GPUDirect RDMA), this option can be enabled using an environment variable. See the README for more details.\r\n\r\nNote: Be aware of the known problems that results in unpredictable behavior for certain complex networks. Any help/insight on that from other MPI experts would be appreciated. \r\n", "comments": ["Can one of the admins verify this patch?", "I think @poxvoculi is more familiar with these changes, so I'll defer to him on the review.", "Jenkins test this please", "@jhseu can you look at the build/config files?", "./configure, .bzl, and BUILD changes LGTM", "@tensorflow-jenkins test this please", "@jbedorf please address the build file formatting errors reported here: https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4354/consoleFull", "@rmlarsen  Those errors should now be fixed.\r\n@poxvoculi  Fixed all indicated comments in the latest commit. ", "Jenkins test this please", "Jenkins test this please"]}, {"number": 9863, "title": "Bug in census_widendeep.py downloading of test data", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: nightly builds\r\n- **TensorFlow version (use command below)**: 1.1.0-rc2\r\n\r\n### Describe the problem\r\nThere seems to be an error in the downloading of the test data in the distributed training example `census_widendeep.py`. The test data is loaded from the file before the file is actually downloaded (lines 152-153):\r\n\r\n```python\r\ntest_file = open(test_file_path)\r\nurllib.urlretrieve(test_data_url, test_file_path)\r\n```\r\n", "comments": ["@caisq That does seem odd.  A slightly larger snippet of code is\r\n\r\n    # Retrieve data from disk (if available) or download from the web.\r\n    train_file_path = os.path.join(data_dir, \"adult.data\")\r\n    if os.path.isfile(train_file_path):\r\n      print(\"Loading training data from file: %s\" % train_file_path)\r\n      train_file = open(train_file_path)\r\n    else:\r\n      urllib.urlretrieve(train_data_url, train_file_path)\r\n\r\n    test_file_path = os.path.join(data_dir, \"adult.test\")\r\n    if os.path.isfile(test_file_path):\r\n      print(\"Loading test data from file: %s\" % test_file_path)\r\n      test_file = open(test_file_path)\r\n    else:\r\n      test_file = open(test_file_path)\r\n      urllib.urlretrieve(test_data_url, test_file_path)\r\n\r\nDo you know how this is supposed to work?", "friendly ping: are you still having issues @ancadumitrache ", "@itsmeolivia It worked after I shuffled the URL retrieve and file open commands (for both train and test data):\r\n\r\n```\r\n# Retrieve data from disk (if available) or download from the web.\r\ntrain_file_path = os.path.join(data_dir, \"adult.data\")\r\nif os.path.isfile(train_file_path):\r\n  print(\"Loading training data from file: %s\" % train_file_path)\r\n  train_file = open(train_file_path)\r\nelse:\r\n  urllib.urlretrieve(train_data_url, train_file_path)\r\n  train_file = open(train_file_path)\r\n\r\ntest_file_path = os.path.join(data_dir, \"adult.test\")\r\nif os.path.isfile(test_file_path):\r\n  print(\"Loading test data from file: %s\" % test_file_path)\r\n  test_file = open(test_file_path)\r\nelse:\r\n  urllib.urlretrieve(test_data_url, test_file_path)\r\n  test_file = open(test_file_path)\r\n```"]}, {"number": 9862, "title": "output of tf.nn.convolution is not consistent for bigger 3d nets", "body": "Code :\r\n```python\r\nimport tensorflow as tf\r\ndef Conv3D(x, filter_shape, stride = 1, padding = 'VALID', dilation_rate = (1,1,1)):\r\n      W = tf.get_variable(name = 'weights', shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\r\n      b = tf.get_variable(name = 'bias', shape = filter_shape[4], initializer = tf.constant_initializer(0.1))\r\n      conv_out = tf.nn.convolution(x, W, b, padding = padding, strides = [1,1,1], dilation_rate = dilation_rate)\r\n     ret_val = tf.nn.bias_add(conv_out,b)\r\n     return ret_val\r\n\r\ndef MaxPool3D(x, dilation_rate = (1,1,1)):\r\n      ret_val = tf.nn.pool(x, window_shape = [2,2,2], pooling_type = 'MAX', padding = 'VALID', dilation_rate = dilation_rate, strides = [1,1,1])\r\n      return ret_val\r\n\r\n\r\nlayer = {}\r\ndilation = 1\r\ninputs = tf.placeholder(tf.float32, (None, 80, 80, 80, 1))\r\nlayer[1] = Conv3D(inputs, [3,3,3,1,128], dilation_rate = (dilation, dilation, dilation))\r\nlayer[2] = Conv3D(layer[1], [3,3,3,128,64], dilation_rate = (dilation, dilation, dilation))\r\nlayer[3] = MaxPool3D(layer[2], dilation_rate = (dilation, dilation, dilation))\r\ndilation = 2*dilation\r\nlayer[4] = Conv3D(layer[3], [3,3,3,64,128], dilation_rate = (dilation, dilation, dilation))\r\nlayer[5] = MaxPool3D(layer[4], dilation_rate = (dilation, dilation, dilation))\r\ndilation = 2*dilation\r\nlayer[6] = Conv3D(layer[5], [3,3,3,128, 256], dilation_rate = (dilation, dilation, dilation))\r\nlayer[7] = MaxPool3D(layer[6], dilation_rate = (dilation, dilation, dilation))\r\ndilation = 2*dilation\r\nlayer[8] = Conv3D(layer[7], [3,3,3,256,64], dilation_rate = (dilation, dilation, dilation))\r\n\r\ninputs_np = np.random.randn(1,80,80,80,1)\r\nsess = tf.Session()\r\nlogits1 = sess.run(layer[8], feed_dict = {inputs : inputs_np})\r\nlogits1_duplicate = sess.run(layer[8], feed_dict = {inputs : inputs_np})\r\n\r\nsum_diff = np.sum(logits1 - logits1_duplicate)\r\nprint(' sum diff : ', sum_diff)\r\n\r\n```\r\nWhen I run this code, I get a non zero sum_diff. Why is the network's output not consistent with multiple runs ? I will appreciate some help", "comments": ["Closing issue because I am not able to reproduce this error"]}, {"number": 9861, "title": "Exporting and loading models with crossed_columns gives errors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.1.0\r\n\r\n### Describe the problem\r\nI trained a `LinearClassifier` that includes a `crossed_column`. When I export it and then load and run it again I get an error message: \"ValueError: No op named SparseFeatureCross in defined operations\".\r\n\r\n### Source code / logs\r\nTo train and export the model I used the following python script:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n\r\ndef input_fn():\r\n    features = {'a': tf.constant([[1],[2]]),\r\n                'b': tf.constant([[3],[4]]) }\r\n    labels = tf.constant([0, 1])\r\n    return features, labels\r\n\r\nfeature_a = tf.contrib.layers.sparse_column_with_integerized_feature(\"a\", bucket_size=10)\r\nfeature_b = tf.contrib.layers.sparse_column_with_integerized_feature(\"b\", bucket_size=10)\r\nfeature_c = tf.contrib.layers.crossed_column([feature_a, feature_b], hash_bucket_size=100)\r\nfeature_columns = [feature_a, feature_b, feature_c]\r\nmodel = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns)\r\nmodel.fit(input_fn=input_fn, steps=10)\r\n\r\nfeature_spec = tf.contrib.layers.create_feature_spec_for_parsing(feature_columns)\r\nserving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\r\nmodel.export_savedmodel('simple-cross/export', serving_input_fn)\r\n```\r\nTo load and run the model I used the following python script:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef _int_feature(value):\r\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\nwith tf.Session() as session:\r\n    model = tf.saved_model.loader.load(session, ['serve'], \"simple-cross/export/1494601566/\")\r\n    probs = tf.get_default_graph().get_tensor_by_name('linear/binary_logistic_head/predictions/probabilities:0')\r\n\r\n    feature_dict = {'a': _int_feature(value=0),\r\n                    'b': _int_feature(value=5)}\r\n    example = tf.train.Example(features=tf.train.Features(feature=feature_dict)).SerializeToString()\r\n    feed_dict = { 'input_example_tensor:0' : [example] }\r\n    print(session.run(probs, feed_dict=feed_dict))\r\n```\r\n(BTW: is this the best way to import/run a saved model? It feels like plugging in constants like `linear/binary_logistic_head/predictions/probabilities:0` isn't the way to go.)\r\n\r\nThis results in the following error:\r\n```\r\nValueError: No op named SparseFeatureCross in defined operations.\r\n```\r\n### Notes\r\nWhen I add the import\r\n```\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n```\r\nto the load/run script, it magically works.\r\n\r\nUnfortunately, I like to run the model from Java as well, and in Java there is no analogous workaround AFAIK (input_fn_utils doesn't exist there).\r\n", "comments": ["@ispirmustafa: Looks like a bug.  Can you take a look?", "It seems saved model load couldn't find the contrib ops.\r\n@sukritiramesh @davidsoergel do you have idea what's going on?", "The usage of the lower level of SavedModel APIs for loading from python/saved_model seems correct. @davidsoergel Could this be stemming from export_savedmodel(...)?", "Ops defined in `contrib` are built as shared libraries which need to be loaded into the address space before they are used. Currently, this load happens when the corresponding Python module is imported. Unfortunately, no other piece of code knows where to find the shared libraries for ops found in the graph.\r\n\r\nAs a workaround for now, add:\r\n\r\n```python\r\nfrom tensorflow.contrib.layers.python import ops\r\n```\r\n\r\nto your program before invoking `tf.saved_model.loader.load` (which triggers a [load library call](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/layers/python/ops/sparse_feature_cross_op.py#L30)), so something like:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers.python import ops\r\nwith tf.Session() as session:\r\n    model = tf.saved_model.loader.load(session, ['serve'], \"simple-cross/export/1494601566/\")\r\n```\r\n\r\nFWIW, you can see all the custom op libraries installed with the pip package using something like:\r\n\r\n```sh\r\nfind $(dirname $(python -c \"import tensorflow; print tensorflow.__file__\")) -name \"*.so\"\r\n```\r\n\r\nUnfortunately, loading custom op libraries from within Java or other language bindings is not working at the moment, primarily because the way custom op libraries are built right now requires the use of `RTLD_GLOBAL` when loading them. We [hack this up in Python right now](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/pywrap_tensorflow.py#L31).\r\n\r\nA believe a more correct solution would be to have the custom op libraries explicitly depend on the framework shared library and not rely on `RTLD_GLOBAL`, but that is a longer fix (FYI @allenlavoie)\r\n\r\nIn the mean time, possible workarounds for Java will require some source modification and compilation of `libtensorflow_jni.so`. Options include:\r\n\r\n- Changing [`load_library.cc`](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/core/platform/posix/load_library.cc#L27) to use `RTLD_GLOBAL` and then initiating a call to [`TF_LoadLibrary`](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/c/c_api.h#L1116) from Java\r\n- Statically linking the ops needed by [adding explicit dependencies in the BUILD rule](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/java/BUILD#L151)\r\n\r\nI'm sincerely sorry that things aren't in better shape at this time. Improving support for custom (contrib) ops in other languages is on our TODO list. A related aside, ops needed for the linear model used in wide-and-deep are slowly moving towards core anyway (e.g. commit 65283e269fac1306089303143daf550b7b1a6658). \r\n\r\nHope that helps.\r\n\r\nFYI @allenlavoie @jhseu @skye", "@asimshankar \r\n\r\n>  Improving support for custom (contrib) ops in other languages is on our TODO list. \r\n\r\nAny idea when this might happen ?", "@pavanky relatively soon! The code is written and tested, it just needs review and the ironing out of some final details. Mid September if things go according to schedule.", "For crossed_column, you can switch to core versions of feature_columns. tf.feature_column.*\r\nsuch as tf.feature_column.crossed_column. Exporting and loading should work if you use these ones. Also please use core version of estimator too such as tf.estimator.DNNLinearCombinedClassifier."]}, {"number": 9860, "title": "Branch 155393864", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 9859, "title": "Where is tf.layers.flatten?", "body": "I hope this question is considered suitable for asking here on GitHub rather than StackOverflow as it relates to a possibly missing feature of the TensorFlow API. I apologize beforehand if I am wasting your time.\r\n\r\nThis concerns TensorFlow 1.1.0 and `tf.layers`\r\n\r\nThe tutorial on `tf.layers` uses manual reshaping to get from 4-rank to 2-rank tensors (search for the word flatten):\r\n\r\nhttps://www.tensorflow.org/tutorials/layers\r\n\r\nThis is not very elegant and I would prefer to use a `flatten()` function.\r\n\r\nThe documentation for `tf.layers.dense()` says something about flattening the input, but it apparently does something else, as discussed in other threads https://github.com/tensorflow/tensorflow/issues/8175 and https://github.com/tensorflow/tensorflow/pull/9043\r\n\r\nThere does not seem to be any `tf.layers.flatten()` function, see e.g. the API docs:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/layers\r\n\r\nAlthough there is one for `tf.contrib.layers.flatten()` as shown here:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten\r\n\r\nI wonder if `flatten()` has been omitted for some reason when moving `layers` to TF core (why?) or perhaps it has been moved somewhere else, but I have searched and I cannot find it anywhere?\r\n\r\nFurthermore, I would like to ask if I can expect that `tf.layers` is going to be the standard builder API going forward? Or will you focus on Keras instead? I have previously used PrettyTensor. There is also tf.slim and other builder APIs. I don't want to change builder API every 6 months, so I'd like to know what the TF developers are betting on this time?", "comments": ["I too think that there too many movements within the APIs and it is now time that they are stablized.\r\n\r\n", "@fchollet for layers roadmap", "There will be a `tf.layers.flatten()` in the future for sure. We are still working on some layer refactoring at the moment, but adding more layers in the core layers module will be the step right after.", "Thanks for the quick answer.\r\n\r\nIs your recommendation to use `tf.contrib.layers.flatten()` for the time being? That is not very elegant.\r\n\r\nIt is a bit hard to understand why such a simple function is not in `tf.layers` already? Is there some technical reason?", "Only a single line is required to use all the high level ops from layers or its alias [slim](https://github.com/tensorflow/tensorflow/blob/5bb3d62fb2a28ef3cd9f02eeebde82560313905a/tensorflow/contrib/slim/__init__.py).\r\n```python\r\nlayers = tf.contrib.layers\r\nlayers.flatten()\r\nor\r\nslim = tf.contrib.slim\r\nslim.flatten()\r\n```\r\n", "@martinwicke, this issue comes up regularly. We should consider adding `tf.layers.flatten`, as previously discussed...", "Sigh. Alright.", "I have now upgraded to TensorFlow 1.3 and this is still missing. I will try my best to be polite, but it is frustrating that you still haven't completed the builder API for TensorFlow. It is more than a year ago that I pleaded with you to consolidate the many builder APIs so we could have a single API that was complete and well-documented. I am struggling to understand why this is not a priority for you. Perhaps we view software engineering differently. To me the API is an incredibly important aspect of any software library. Perhaps Keras is now the most complete API for TensorFlow and it seems there's a person dedicated to its development. Perhaps you should consider making Keras the main builder API then. But please make a decision for one API, stick with it and finish it. Thanks.", "Can't agree more! Why there is still no tf.layers.flatten? And the changes of tf over versions are so big that it is a mess when learning", "Any update?", "If this is extremely important to you, please do send us a PR. ", "There is in fact a flatten layer in TF 1.4 in `tf.layers`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/core.py#L347-L406\r\n\r\nIn general, feel free to use layers from `tf.keras` in your workflow.", "@fchollet THX. Found it. Just wondering if this two will be merged in future? Btw, using keras means one more hierarchy :)", "What's the difference of tf.layers and tf.contrib.slim by the way. they are so many interface are exactly same but with different behaviours, even same codes but totally different error messages, so confusing.", "Just use Keras. I have lost patience with all the other TensorFlow APIs and just use Keras now. It's not perfect and some features are missing or broken, but it's the best TensorFlow API I've found and it's still under serious development so we can expect it to improve in the future.\r\n"]}]