[{"number": 5694, "title": "FusedBatchNorm does not support 3D Filters", "body": "The current implementation of `tf.contrib.layers.batch_norm` is quite slow with the default parameters. Recent builds of TF support `fused=True` which forces the use of the faster `nn.fused_batch_norm`. However this method does not support 3D filters for computing the mean and variance. The normal (slower) variant with `fused=False` does not have this problem.\r\n\r\n**FusedBatchNorm**:\r\n```python\r\n# Following error is raised when trying to batch normalize 3D filters\r\nelif original_rank not in [2, 4]:\r\n      raise ValueError('Inputs %s has unsupported rank. \\\r\n          Expected 2 or 4 but got %d' % (inputs.name, original_rank))\r\n```\r\n\r\nAnd the default implementation without the fused version uses these axis to compute the `tf.nn.moments`:\r\n\r\n```python\r\naxis = list(range(inputs_rank - 1))\r\n```\r\n\r\nThis also works for 3D filters of course. See code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L483", "comments": ["@tomrunia, thanks for filing the issue! \r\n\r\nI believe 3D filters including both NCW and NWC could be supported in a similar way to that of convolution: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L766\r\n\r\nI'll label this as contribution welcome for now.", "As a work-around I now use `tf.reshape` to merge the WxH so the 5D tensor becomes 4D. Then I am able to use `tf.nn.fused_batch_norm()`. After the batch normalization I reshape the tensor again to get the 5D tensor back. This speeds up training time with 20% compared to using the non-fused version of batch normalization.", "@zhangyaobit I'm working on this issue.\r\n", "@zhangyaobit I investigated into the issue. The C++ implementation of `FusedBatchNorm` depends on [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/stream.cc#L280) in `stream.cc` which uses the underlying cuDNN implementation from what I can see. Thus, the solution you suggested can work for 1D filters but not for 3D. But as @tomrunia suggested we can have a workaround. Let me know if I should make changes only for 1D filters or write a special case for 3D filters as well.", "@siddharth-agrawal Please note that how the code currently handles the case when the input is 2D (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L233): the input is reshaped to a 4D shape [-1, 1, 1, channels].\r\n\r\nI believe this is essentially the same as the workaround by @tomrunia, where the 5D input is reshaped to 4D. For the 3D case, say NWC, you can do the same to reshape from [N, W, C] to [-1, 1, W, C]. \r\n\r\nI think you can add both the 5D and 3D case inside _fused_batch_norm (while @tomrunia is doing it outside _fused_batch_norm), in addition to the 2d case, which is currently already supported.\r\n\r\nPlease let me know if this works.", "Hello,\r\n\r\n\r\nHas there been any progress on getting fused_batch_norm working on 3D data?\r\n\r\nUp until now I was using tf.contrib.layers.batch_norm, but due to tf.nn.moments returning negative values for variance (as reported [here](https://github.com/tensorflow/tensorflow/issues/3290)),  I am forced to switch to fused_batch_norm.  I suspect many people dealing with 3D data will be running into similar problems.\r\n\r\nThanks.", "Could you use the workaround suggested by @tomrunia to reshape the data to 3D?", "Yes, that does seem to work for now.  Thank you.\r\n\r\nI was mainly curious as to whether progress had been made on being able to use fused_batch_norm without that reshaping business.", "This is currently marked as contributions welcome! \r\n\r\nEssentially, to do the same \"reshaping business\", but inside the fused_batch_norm function.", "Ah, I see that now!  If I have the time I will give it a try :)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I don't think the issue is fully resolved. And the lack of fused batch norm can be a critical performance issue for networks with 3D convolutions. I have done the modification in _fused_batch_norm as suggested by @zhangyaobit. Yet, I do see the [to-do comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/normalization.py#L144) in normalization.py, which is more fundamental. I am willing to send a PR if that to-do action will not be implemented any time soon. ", "@tomrunia how did you merge WxH? I tried\r\n\r\n```\r\n# net is the output of conv3d\r\norig_shape = tf.shape(net)\r\nworkaround_shape = orig_shape[0], orig_shape[1], orig_shape[2]*orig_shape[3], orig_shape[4]\r\nnet = tf.reshape(net, workaround_shape)\r\nnet = tf.layers.batch_normalization(net, training=True)\r\nnet = tf.reshape(net, orig_shape)\r\n```\r\nBut TensorFlow will complain at the `batch_normalization` line:\r\n```\r\nFile \"~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 253, in build\r\n    input_shape)\r\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(None)]))\r\n```\r\n\r\n**Update**: solved. I have to make sure [the axis/axes passed to batch_normalization having known size(s)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/normalization.py#L253). In my case, I know the images are of gray-scale, so just use \r\n\r\n`workaround_shape = orig_shape[0], orig_shape[1], orig_shape[2]*orig_shape[3], 1`\r\n", "@happyharrycn \r\nI'm working with 5D tensors and 3D convolutions with TF 1.7.0 and I want to used fused_batch_norm instead of tf.nn.moments\r\nHas this issue been resolved?", "@happyharrycn Also, if nn.moments were not returning negative variance, we could give axes as [0,1,2,3] for 5D tensors and 3D convolutions, right? Because nn.batch_normalization takes tensors of any size > 1"]}, {"number": 5693, "title": "install Pillow in Dockerfile", "body": "Hi team,\r\n\r\nWhen i used **tensorflow** in python, sometimes i try the code like \r\n`scipy.misc.imread('test.tif')`\r\nbut i get an error:\r\n`AttributeError: 'module' object has no attribute 'imread'. `\r\n\r\nAfter some digging, i find i need to install PIL from [the docs](https://docs.scipy.org/doc/scipy/reference/misc.html) on scipy.misc. i guess there may be some other developers using tensorflow in my way (e.g. [fast-style-transfer src/utils.py/#9](https://github.com/lengstrom/fast-style-transfer/blob/master/src/utils.py)). \r\n\r\nSo i would like to suggest adding Pillow into docker image build. \r\n\r\n", "comments": ["@foreverfaint, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @caisq and @vrv to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@foreverfaint , can you let me know the approximate docker size increase (`docker images`) due to this change?\n", "I have signed it. I used foreverfaint@gmail.com for signing. \nGoogle Individual CLA   Hao Nov 18, 2016 06:14 PST  Edit Contact Information\n\nbut when i submit this PR, my local git global config is wrong. It was _foreverfailt@gmail.com_ due to a typo on failt (should be faint)\n", "@caisq, it increases from 958.6 to 981.6. about 23MB up. Is it acceptable?\n\nREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\ngcr.io/tensorflow/tensorflow   Pillow              dff6ac3189bd        10 hours ago        981.6 MB\ngcr.io/tensorflow/tensorflow   latest              5547120ff897        9 days ago          959.6 MB\n", "Since it's a <2.5% increase, I think it is acceptable. For symmetry, can you also add it to the following file:\ntensorflow/tools/docker/Dockerfile.gpu\n", "no problem, will do it.\n\nquestion: @caisq, after i add it to Dockerfile.gpu, should i create a new PR?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "because i used a wrong author name (not CLA signed one), i can't pass CLA check. so i am going to incorprate the change mentioned by @caisq into Dockerfile.gpu with a right author name, and create a new PR for review. will at you again in new PR. @mention-bot @tensorflow-jenkins @googlebot @caisq \n\nclose this one now\n"]}, {"number": 5692, "title": "Update bazel.rc.template", "body": "This'll work better, trust me", "comments": ["@Christopher-Steel, thanks for your PR! By analyzing the history of the files in this pull request, we identified @davidzchen, @vrv and @nfiedel to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "This was an example for a student, please ignore it, and sorry\n"]}, {"number": 5691, "title": "change validation_metrics as  MetricSpec", "body": "tensorflow:Please specify metrics using MetricSpec. Using bare functions or (key, fn) tuples is deprecated and support for it will be removed on Oct 1, 2016.\r\nso run the previous code will get a mistake!", "comments": ["@BoyuanJiang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @MrQianJinSi to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 5690, "title": "Replace squeeze/split with unpack", "body": "In [Recurrent Neural Networks](https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html) tutorial file `ptb_word_lm.py` I've replaced the alternative version of the code with `unpack` in place of `squeeze` and `split` (as in [documentation](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#split)).", "comments": ["@kamilhryniewicz, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @benoitsteiner to be potential reviewers.\n", "Can one of the admins verify this patch?\n"]}, {"number": 5689, "title": "Ensure stochastic_tensor_test runs.", "body": "stochastic_tensor_test.py was missing a call to tf.test.main() so the\r\ntest wasn't actually running. This commit fixes that and the test\r\npasses.", "comments": ["Can one of the admins verify this patch?\n", "@darrengarvey, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo and @tensorflower-gardener to be potential reviewers.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5688, "title": "tensorflow crashes when using large image with 3d convolutional network", "body": "I'm trying to implement a 3d fully convolutional network on my GPU. But for some reason I get a crash.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04 LTS\r\nGPU: GeForce Titan X .\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5\r\n(attach the output of `ls -l /path/to/cuda/lib/libcud* \r\n[cud.filelist.txt](https://github.com/tensorflow/tensorflow/files/599236/cud.filelist.txt) )\r\n\r\nI installed tensorflow version 0.11.0rc2, and it also reproduce in docker installation (gcr.io/tensorflow/tensorflow:latest-gpu)\r\n\r\n### Example code\r\nThe following code reproduce the problem:\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n\r\n    graph = tf.Graph()\r\n\r\n    with graph.as_default():\r\n        tf_dataset = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))\r\n        tf_label = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))\r\n\r\n        layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))\r\n        layer1_bias = tf.Variable(tf.zeros(1))\r\n\r\n        conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n        logits = tf.nn.relu(conv+layer1_bias)\r\n\r\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_label))\r\n        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n\r\n    with tf.Session(graph=graph) as session:\r\n        tf.initialize_all_variables().run()\r\n        batchData = np.random.rand(1, 512, 512, 512, 1).astype(np.float32)\r\n        batchLabels = (np.random.rand(1, 512, 512, 512, 1)>0.5).astype(np.float32)\r\n        feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n        _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n\r\nwith the following output:\r\n\r\n> I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.68GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:2440] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED\r\n\r\n", "comments": ["Does it work with smaller images?  It vaguely sounds like running out of memory, but it doesn't seem like it should given the numbers and batch size 1.\n", "Can it be a memory issue? My GPU has 12GB. The size of the input is 0.25GB. Anyhow the following size works:  (32, 32, 512, 512, 1) which is larger in size, but smaller in one convolutional direction. \n", "@HggsHntr I'm just a simple computer scientist.  Your questions are reasonable, but it's hard for me to think beyond testing whether a smaller image works or not.\n", "I guess it might be memory issue after all. \nI set config.gpu_options.allow_growth = True and traced the memory consumption of the GPU. It raised to the maximum before it crashed. \nAlso, by changing the stride size to 4 in each direction, or reducing the image size the crash disappear.\nSo I wonder if this large memory consumption in done intentionally or is it a bug. \n", "@zheng-xq Is there any possible issue with GPU convolution scratch space?  (e.g. can the autotuner selects an algorithm based on available memory and then fail later on)", "I've encountered the same issue on `tensorflow==0.11.0rc2` with latest `keras`:\r\n\r\n```\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED\r\n...... 104 Aborted                 (core dumped) .......\r\n```\r\n\r\nI was feeding the CNN 10k 200x200x3 images per `fit`. When I gradually lower the number to 8k, the same issue no longer existed. On the other hand, with the 10k images per `fit`, the same CNN written in `tflearn` was just fine.\r\n\r\n", "can confirm a similar issue with large 3D convolutions:\r\n\r\n```python\r\ninshape = (2,258,258,34,1)  # tf img-order\r\nfilter_shape = (3,3,3,1,32)\r\nx = tf.placeholder('float32', shape=inshape, name='X')\r\nf = tf.placeholder('float32', shape=filter_shape, name='filter')\r\nc = tf.nn.conv3d(x, f, padding='VALID', strides=[1,1,1,1,1])\r\ngrads = tf.gradients(c,f)[0]\r\n\r\nxx = np.random.rand(*inshape)\r\nff = np.random.rand(*filter_shape)\r\n\r\nwith tf.Session().as_default():\r\n    Q = c.eval(feed_dict={x:xx, f:ff})\r\n    gradQ = grads.eval(feed_dict={x:xx, f:ff})\r\n```\r\nwill yield `tensorflow/stream_executor/cuda/cuda_dnn.cc:2674] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED`.\r\nIt runs fine if I set `inshape = (2,257,257,34,1)` instead, so there's some issue with the sizes. However I dont think its the usual GPU-out-of-memory issue:\r\n1. the same convolution runs perfectly fine in theano (there I can even increase to `(2,300,300,34,1)` without trouble)\r\n2. it doesnt respond to all dimension the same: decreasing the batchsize from 2->1 will still crash even though we just halfed the input\r\n\r\nTurns out that it's somehow **related to the gradient computations**, i.e. commenting the `gradQ =...` line, just evaluating the result of the convolutions works!\r\n\r\nCode was run on tensorflow 0.11.0rc2 (same happens for 0.12.1), Titan X, CUDA-8.0, cudnn 5\r\n\r\n\r\n\r\n", "@redst4r I have a similar issue. I too think there could be some issue when calculating the gradients. Any update on how you resolved it?", "@deepak09027 Unfortunately I dont have any solution yet. I was hoping that some googler picks it up here :)", "Can confirm this issue as well. I'm having exactly the same problem! Would be great if this could be resolved :)", "Similar problem here with deep 2d convolution network and huge batch size (because my input is [seq_length * batch_size, height, width] ).\r\n\r\n```bash\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.75GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a9e500\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 980 Ti\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 4.99GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:2684] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED\r\nAborted (core dumped)\r\n\r\n```\r\n\r\nChange batch size to a smaller one solves the issue. It'd great if it can throw a python exception : )", "@prb12, @zheng-xq any updates on this issue?", "I am having the same issue as well", "Same here. I can reproduce this issue with the example code above. Can confirm CUDA 8, CUDNN 5.1 with tf 1.0. \r\n\r\nI ran into an interesting aspect when determining the maximum size of the tensor. It run when tensors with shape less than [1, 256, 256, 256,1] are used. The example below will run and sz=256 will break. \r\n\r\nIs anyone looking into this?\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\n\r\nsz = 255\r\n\r\nwith graph.as_default():\r\n    tf_dataset = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\r\n    tf_label = tf.placeholder(tf.float32, shape=(1, sz, sz, sz, 1))\r\n\r\n    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))\r\n    layer1_bias = tf.Variable(tf.zeros(1))\r\n\r\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n    logits = tf.nn.relu(conv+layer1_bias)\r\n\r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n\r\nwith tf.Session(graph=graph) as session:\r\n    tf.initialize_all_variables().run()\r\n    batchData = np.random.rand(1, sz, sz, sz, 1).astype(np.float32)\r\n    batchLabels = (np.random.rand(1, sz, sz, sz, 1)>0.5).astype(np.float32)\r\n    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n    _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n```", "I've also run into the same issue, where a P100 with 16GB RAM would get the same error, but I know that my data is not even big enough to take up the entire GPU RAM.\r\n\r\nI'm using TF 0.12, CUDA 8.0, CuDNN 5.1.\r\n\r\nHas this been fixed in the newest version of TF?", "Mostly its not related to VRAM (I am using latest TF with keras)\r\nIn first layer for a VGG like architecture \r\n`Convoulution3D(NumChannels, 3,3,3,)(inputs)`\r\n\r\nif I set **NumChannels < 8**, it crashes with the above message. \r\nThis somehow contradicting with [waTeim ](https://github.com/tensorflow/tensorflow/issues/2190)", "Hmm, I am trying to run on tensors of shape (1,125,125,125,128) = 1 gig and getting the standard OOM error. I am wondering how much working memory the 3d conv needs. It seems like doing a conv of this size should work ok.", "This issue also goes away for me when I reduce my batch size. Seems like a memory issue that Tensorflow doesn't catch the way it does other OOM issues.", "Did anyone find any solution for this?\r\nI am also hitting this issue when trying to train a CNN. When I reduce the batch size to 2 its runs and at this point the GPU VRAM consumption is 217 MB/3072 MB and on increasing the batch size to 3, it gives the \"failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\" error. I doubt if this is due to low VRAM.\r\n\r\nTensorFlow's response/support has been very weak on the bugs being reported here. Really disappointing.", "@tfboyd Any ideas?", "same problem . \r\n\r\n`name: Tesla P100-PCIE-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.3285\r\npciBusID 0000:05:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.61GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0)\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:1989] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\r\n('Sample data and label: ', 50000, (3072,), array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]))\r\n('Training data: ', (35000, 3072), (35000, 10), '\\nValidation data: ', (15000, 3072), (15000, 10))\r\nTraining started !!!!!\r\n/cm/local/apps/slurm/var/spool/job109000/slurm_script: line 14: 38773 Aborted                 /home/ksrivastava/tensorflow/bin/python /home/ksrivastava/DL/CIFAR10/scripts/trainGPU.py`", "I will try to escalate this.  The issue has been open for a long time and reproduced many times based on my skimming of the thread.  ", "thanks @tfboyd !!", "Internal email thread started, looking for an owner to \"dig in\".  Sorry for the delays hopefully we get this moving forward.  ", "b/64718915  @yzhwang is working on it and thank you to @SrivastavaKshitij for following up via email and helping get this prioritized.  ", "Hi @SrivastavaKshitij could you let me know which version of tensorflow you are using? I tested with @HggsHntr 's example code on tf 1.2.1 with CUDA 8 and cudnn 5.1. This seems to be a duplicated issue with https://github.com/tensorflow/tensorflow/issues/11327, which I have already fixed in https://github.com/tensorflow/tensorflow/commit/db596594b5653b43fcb558a4753b39904bb62cbd. Unfortunately the fix is still not included in 1.3rc2. If you believe this is a separate issue, could you provide a reproducer? Thank you!", "Hi @yzhwang : I was able to solve the problem. I am working on a cluster and I was using an incompatible version of PGI+OpenMPI with cuda and cudnn version which was causing the problem. But as soon as I corrected it, everything worked fine. If you want I can give u the details.\r\n\r\nThanks", "If it is not too complicated, we would like you to provide a more detailed description, and how you solved it. I think it would help other users with the same configurations.\r\n\r\nAs for this issue, I will close it for now as I have a fix for this (https://github.com/tensorflow/tensorflow/commit/db596594b5653b43fcb558a4753b39904bb62cbd) and it has been verified both internally and on the head of OSS version. @HggsHntr if you believe the fix didn't solve the issue, please let us know so that we can reopen and investigate more.\r\n\r\n@SrivastavaKshitij Please do put more details if you like even when the issue is closed. Much appreciated!", "@yzhwang \r\nI have upgraded to **tensorflow 1.4** nightly build (windows-gpu) and when I run the following:\r\nimport numpy as np\r\nfrom keras.engine import Input, Model\r\nfrom keras.layers import Conv3D, Activation\r\ndim = 256\r\ninput_shape = [dim,dim,dim,1]\r\ninputs = Input(input_shape)\r\nconv1 = Conv3D(filters=1, kernel_size=(2, 2, 2), strides=(1,1,1), activation='relu', padding='same')(inputs)\r\nlogits = Activation('softmax')(conv1)\r\nmodel = Model(inputs=inputs, outputs=logits)\r\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\r\nbatchData = np.random.rand(1, dim, dim, dim, 1).astype(np.float32)\r\nbatchLabels = (np.random.rand(1, dim, dim, dim, 1)>0.5).astype(np.float32)\r\nmodel.fit(x=batchData, y=batchLabels)\r\n\r\nit crashes with an error about Conv3DBackpropFilterV2", "@yzhwang \r\nI get the same thing as @hadarpo . \r\nIs it the same bug or something else? ", "Hi @hadarpo and @HggsHntr , I cannot reproduce this on my linux box. I do not have a Windows machine to test this right now. I will ask my colleagues to help reproduce this. Meanwhile, if you could include a full error log here, I will start looking at it and see where we go from there. Thank you!", "Hi @yzhwang, thanks for your response. I have tried the latest nightly build (#55) with the original tester of @HggsHntr and got the following error output:\r\n```\r\n2017-10-02 11:30:17.785801: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\r\n2017-10-02 11:30:17.785922: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Internal: error destroying CUDA event in context 0000028FBE051ED0: CUDA_ERROR_LAUNCH_FAILED\r\n2017-10-02 11:30:17.787104: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Internal: error destroying CUDA event in context 0000028FBE051ED0: CUDA_ERROR_LAUNCH_FAILED\r\n2017-10-02 11:30:17.787561: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.787960: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.788460: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.788998: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.789578: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.790140: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.790606: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.790944: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.790987: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-02 11:30:17.791331: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1322     try:\r\n-> 1323       return fn(*args)\r\n   1324     except errors.OpError as e:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1301                                    feed_dict, fetch_list, target_list,\r\n-> 1302                                    status, run_metadata)\r\n   1303\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    466           compat.as_text(c_api.TF_Message(status.status)),\r\n--> 467           c_api.TF_GetCode(status.status))\r\n    468   # Delete the underlying status object from memory otherwise it stays alive\r\n\r\nNotFoundError: No algorithm worked!\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-1-0ef7131b03e5> in <module>()\r\n     22     batchLabels = (np.random.rand(1, 512, 512, 512, 1)>0.5).astype(np.float32)\r\n     23     feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n---> 24     _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n     25\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1119       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1120                              feed_dict_tensor, options, run_metadata)\r\n   1121     else:\r\n   1122       results = []\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1315     if handle is None:\r\n   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1317                            options, run_metadata)\r\n   1318     else:\r\n   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1334         except KeyError:\r\n   1335           pass\r\n-> 1336       raise type(e)(node_def, op, message)\r\n   1337\r\n   1338   def _extend_graph(self):\r\n\r\nNotFoundError: No algorithm worked!\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n\r\nCaused by op 'gradients/Conv3D_grad/Conv3DBackpropFilterV2', defined at:\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\Scripts\\ipython-script.py\", line 5, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\terminal\\ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\terminal\\interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\terminal\\interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-0ef7131b03e5>\", line 17, in <module>\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 343, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 88, in _Conv3DGrad\r\n    data_format=data_format)]\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 964, in conv3d_backprop_filter_v2\r\n    data_format=data_format, name=name)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3090, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'Conv3D', defined at:\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\Scripts\\ipython-script.py\", line 5, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n[elided 7 identical lines from previous traceback]\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-0ef7131b03e5>\", line 13, in <module>\r\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 846, in conv3d\r\n    padding=padding, data_format=data_format, name=name)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3090, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\hadar\\AppData\\Local\\conda\\conda\\envs\\hadar-tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): No algorithm worked!\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n```", "Hi @hadarpo , I'm trying to reproduce this on a Windows machine. Just a couple of things I want to make sure:\r\n1) Are you using the head of TensorFlow on our github repo?\r\n2) Are you using this reproducer (https://github.com/tensorflow/tensorflow/issues/5688#issuecomment-332832739) or the original reproducer (https://github.com/tensorflow/tensorflow/issues/5688#issue-190252047)?\r\n3) It would be nice if you could also provide us the CUDA version, cudnn version, and display driver version you use.\r\n\r\nThank you!", "Hi @yzhwang \r\n1. I've installed tensorflow from jenkins nightly build using the following wheel:\r\n[https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/55/artifact/cmake_build/tf_python/dist/tf_nightly_gpu-1.4.0.dev20170929-cp35-cp35m-win_amd64.whl](url)\r\n2. I am using the original tensorflow version of the reproducer (even though the keras one gives the same results)\r\n3. I'm using CUDA 8.0 and cudnn6, and the NVIDIA drivers are as follow:\r\n![image](https://user-images.githubusercontent.com/26375398/31111868-36b0a24e-a819-11e7-9a23-9412bc33d1ac.png)\r\nThank you!", "Hi @hadarpo I updated the original reproducer and ran it on a windows machine with the nightly build TF that you had and CUDA 8/cudnnv6, and I wasn't able to reproduce your error. Here's the result:\r\n\r\n```\r\nC:\\Users\\yangzihao\\Desktop>\"C:\\Program Files\\Anaconda3\\python.exe\" tfversion.py\r\n1.4.0-dev20170929\r\n\r\nC:\\Users\\yangzihao\\Desktop>\"C:\\Program Files\\Anaconda3\\python.exe\" run.py\r\n2017-10-03 20:41:13.569273: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\n2017-10-03 20:41:14.419846: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:04.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:14.560192: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 1 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:05.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:14.696383: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 2 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:06.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:14.830435: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 3 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:07.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:14.962369: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 4 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:08.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:15.093104: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 5 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:09.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:15.230357: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 6 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:0a.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:15.368208: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 7 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:0b.0\r\ntotalMemory: 11.18GiB freeMemory: 11.12GiB\r\n2017-10-03 20:41:15.369462: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:980] Device peer to peer matrix\r\n2017-10-03 20:41:15.371303: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] DMA: 0 1 2 3 4 5 6 7\r\n2017-10-03 20:41:15.371382: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 0:   Y N N N N N N N\r\n2017-10-03 20:41:15.371969: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 1:   N Y N N N N N N\r\n2017-10-03 20:41:15.372523: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 2:   N N Y N N N N N\r\n2017-10-03 20:41:15.373115: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 3:   N N N Y N N N N\r\n2017-10-03 20:41:15.373629: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 4:   N N N N Y N N N\r\n2017-10-03 20:41:15.374197: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 5:   N N N N N Y N N\r\n2017-10-03 20:41:15.374683: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 6:   N N N N N N Y N\r\n2017-10-03 20:41:15.375206: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:996] 7:   N N N N N N N Y\r\n2017-10-03 20:41:15.375761: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.376257: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.376797: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:00:06.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.377331: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:00:07.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.377845: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:4) -> (device: 4, name: Tesla K80, pci bus id: 0000:00:08.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.378384: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:5) -> (device: 5, name: Tesla K80, pci bus id: 0000:00:09.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.378910: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:6) -> (device: 6, name: Tesla K80, pci bus id: 0000:00:0a.0, compute capability: 3.7)\r\n2017-10-03 20:41:15.379449: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:7) -> (device: 7, name: Tesla K80, pci bus id: 0000:00:0b.0, compute capability: 3.7)\r\n```\r\n\r\nHere's the modified original reproducer (only interface changes to make it run on v1.4):\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    tf_dataset = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))\r\n    tf_label = tf.placeholder(tf.float32, shape=(1, 512, 512, 512, 1))\r\n\r\n    layer1_weights = tf.Variable(tf.truncated_normal((2, 2, 2, 1, 1), stddev=0.1))\r\n    layer1_bias = tf.Variable(tf.zeros(1))\r\n\r\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n    logits = tf.nn.relu(conv+layer1_bias)\r\n\r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_label))\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n\r\nwith tf.Session(graph=graph) as session:\r\n    tf.global_variables_initializer().run()\r\n    batchData = np.random.rand(1, 512, 512, 512, 1).astype(np.float32)\r\n    batchLabels = (np.random.rand(1, 512, 512, 512, 1)>0.5).astype(np.float32)\r\n    feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n    _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n```\r\nCould you try to run other conv-related tests (for example this one):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/conv_ops_3d_test.py\r\nAnd also other CUDA samples to make sure your CUDA/cudnn works correctly first?\r\n\r\nSince although it looks like the error was from conv3d_backprop, but the stream has already been polluted by earlier CUDA_ERROR_LAUNCH_FAILEDs in cuda_driver.cc and cuda_timer.cc.\r\n\r\n", "@hadarpo have you tried my suggestions? Since I cannot reproduce it and from the error log you posted I don't think it's related to conv3d_backprop, I will close this issue for now. Feel free to comment or open another issue. Thank you!", "Hi @yzhwang ,\r\nThank you for your work.\r\n \r\nI tested the tesnsor-flow 1.4 windows-gpu night build on two different machines:\r\nwindows server 2016 + Titan X + cuda 8 + cudnn 6 \r\nwindows 10 + GTX 1050 + cuda 8 + cudnn 6\r\nOn both the script _conv_ops_3d_test.py_ ran without any problem. \r\nOn both the modified reproducer code failed: titan X with  CUDA_ERROR_LAUNCH_FAILED and GTX 1050 with CUDA_ERROR_LAUNCH_TIMEOUT .\r\nOn the Titan X (12GB of memory) if I change in all places 512 to 202 it works perfectly well, and any number above it produces the same failure. \r\nOn the GTX 1050, (3GB of available memory) I can get as high as 255. Any number above it produces the following error:\r\n```\r\n2017-10-05 22:09:24.921703: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_TIMEOUT :: No stack trace available\r\n```\r\nI have no idea if these errors relates to the same problem or not. ", "Hi @HggsHntr , as I can't find any Windows server with Pascal/Maxwell architecture, could you try adding this line to the python reproducer:\r\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\r\nor type:\r\nset CUDA_LAUNCH_BLOCKING=1\r\nin command line?\r\nThis is to force CUDA kernels to synchronize after each call, and hopefully will show us more useful error logs. So that we can tell whether it is related to conv3d kernel or not. Thank you!\r\n", "Hi @yzhwang , It seems like I get the same error. No new information. please see below.\r\n```\r\n2017-10-09 20:46:28.203661: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2017-10-09 20:46:28.494815: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 12.00GiB freeMemory: 10.06GiB\r\n2017-10-09 20:46:28.494997: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2017-10-09 20:47:40.380734: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Internal: error destroying CUDA event in context 0000014DD584E910: CUDA_ERROR_LAUNCH_FAILED\r\n2017-10-09 20:47:40.380902: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Internal: error destroying CUDA event in context 0000014DD584E910: CUDA_ERROR_LAUNCH_FAILED\r\n2017-10-09 20:47:40.382666: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.383087: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.383572: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.384041: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.384500: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.384520: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.384919: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.385438: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.385884: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Invalid argument: input event cannot be null\r\n2017-10-09 20:47:40.386252: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Invalid argument: input event cannot be null\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1322     try:\r\n-> 1323       return fn(*args)\r\n   1324     except errors.OpError as e:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1301                                    feed_dict, fetch_list, target_list,\r\n-> 1302                                    status, run_metadata)\r\n   1303\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    466           compat.as_text(c_api.TF_Message(status.status)),\r\n--> 467           c_api.TF_GetCode(status.status))\r\n    468   # Delete the underlying status object from memory otherwise it stays alive\r\n\r\nNotFoundError: No algorithm worked!\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-1-7824d7b75ec7> in <module>()\r\n     26     batchLabels = (np.random.rand(1, n, n, n, 1)>0.5).astype(np.float32)\r\n     27     feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n---> 28     _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n     29\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1119       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1120                              feed_dict_tensor, options, run_metadata)\r\n   1121     else:\r\n   1122       results = []\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1315     if handle is None:\r\n   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1317                            options, run_metadata)\r\n   1318     else:\r\n   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1334         except KeyError:\r\n   1335           pass\r\n-> 1336       raise type(e)(node_def, op, message)\r\n   1337\r\n   1338   def _extend_graph(self):\r\n\r\nNotFoundError: No algorithm worked!\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n\r\nCaused by op 'gradients/Conv3D_grad/Conv3DBackpropFilterV2', defined at:\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\Scripts\\ipython-script.py\", line 5, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\terminal\\ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\terminal\\interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\terminal\\interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-7824d7b75ec7>\", line 21, in <module>\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 343, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 88, in _Conv3DGrad\r\n    data_format=data_format)]\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 964, in conv3d_backprop_filter_v2\r\n    data_format=data_format, name=name)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2936, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1464, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'Conv3D', defined at:\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\Scripts\\ipython-script.py\", line 5, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n[elided 7 identical lines from previous traceback]\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-7824d7b75ec7>\", line 17, in <module>\r\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 846, in conv3d\r\n    padding=padding, data_format=data_format, name=name)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2936, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1464, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): No algorithm worked!\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n```", "Hi @HggsHntr , thanks for the info! Since we do not have any Windows machine with Pascal/Maxwell card, I will provide some suggestions on how to further debugging the issue:\r\n1) os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'\r\nor type:\r\nset TF_CUDNN_USE_AUTOTUNE=0\r\nThis will disable the autotune on convolutions, and force all cudnn convolution calls to use the default algorithm.\r\nIf this doesn't solve the problem, that means it is not related to conv3d backprop op and must be other CUDA issues. I can tell you where to go from there. If this solves the problem, then it means that during the profiling of one of the internal cudnn kernels, there was some kernel launch error. I'm working on an extensive test suite for cudnn's convolution algorithms to help us spot which internal algorithm caused the problem. Before I release that tool, there is a hacky way to do it yourself though, you could try the following two things:\r\n1) Add cuda synchronization code and error checking code after each cudnnConvolutionBackwardFilter call:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L3398\r\nThe code can be something like this:\r\n```\r\ncudaThreadSynchronize();                                                                                                              \r\ncudaError_t err = cudaPeekAtLastError();\r\nif (err != cudaSuccess) {\r\n  LOG(FATAL) << \"kernel launch error: \" << err;\r\n}\r\n```\r\n\r\n2) Modify this function to get only one internal kernel for cudnnConvolutionBackwardFilter and test which one caused the failure:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2540\r\nEither way, if you manage to reproduce the bug in TensorFlow, I will write a standalone cudnn C++ reproducer according to your shape and internal algorithm used and file it to NVIDIA. Let me know if that sounds OK to you. Another way is to wait for the extensive cudnn test tool I'm working on. I should be able to finish it before November.", "Hi @yzhwang ,\r\nI set both env vars as you suggested. still no luck. It is still crashing on network sizes larger than roughly 200.  Let me know if you want to see the errors.  \r\nAbout adding some debugging code messages, I don't mind to give it a try but I don't have a building environment for tensorflow. Is there an instruction page on how to do it?\r\n\r\nthanks for all you effort. ", "@HggsHntr Please do let me see the error log as if no autotune is involved, there shouldn't be any profiling so I assume the error must came from a different place instead of cuda_timer.cc.", "@yzhwang \r\n```\r\n2017-10-11 21:48:30.115149: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2017-10-11 21:48:30.402802: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 12.00GiB freeMemory: 10.06GiB\r\n2017-10-11 21:48:30.402991: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2017-10-11 21:48:47.601437: E C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3387] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1322     try:\r\n-> 1323       return fn(*args)\r\n   1324     except errors.OpError as e:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1301                                    feed_dict, fetch_list, target_list,\r\n-> 1302                                    status, run_metadata)\r\n   1303\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    466           compat.as_text(c_api.TF_Message(status.status)),\r\n--> 467           c_api.TF_GetCode(status.status))\r\n    468   # Delete the underlying status object from memory otherwise it stays alive\r\n\r\nInternalError: cuDNN Backward Filter function launch failure : input shape([1,512,512,512,1]) filter shape([2,2,2,1,1])\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-1-436eb9c8573a> in <module>()\r\n     28     batchLabels = (np.random.rand(1, n, n, n, 1)>0.5).astype(np.float32)\r\n     29     feed_dict = {tf_dataset : batchData, tf_label : batchLabels}\r\n---> 30     _ = session.run((optimizer, ), feed_dict=feed_dict)\r\n     31\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1119       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1120                              feed_dict_tensor, options, run_metadata)\r\n   1121     else:\r\n   1122       results = []\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1315     if handle is None:\r\n   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1317                            options, run_metadata)\r\n   1318     else:\r\n   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1334         except KeyError:\r\n   1335           pass\r\n-> 1336       raise type(e)(node_def, op, message)\r\n   1337\r\n   1338   def _extend_graph(self):\r\n\r\nInternalError: cuDNN Backward Filter function launch failure : input shape([1,512,512,512,1]) filter shape([2,2,2,1,1])\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n\r\nCaused by op 'gradients/Conv3D_grad/Conv3DBackpropFilterV2', defined at:\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\Scripts\\ipython-script.py\", line 5, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\terminal\\ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\terminal\\interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\terminal\\interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-436eb9c8573a>\", line 23, in <module>\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 343, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 414, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 88, in _Conv3DGrad\r\n    data_format=data_format)]\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 964, in conv3d_backprop_filter_v2\r\n    data_format=data_format, name=name)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2936, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1464, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'Conv3D', defined at:\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\Scripts\\ipython-script.py\", line 5, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n[elided 7 identical lines from previous traceback]\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-436eb9c8573a>\", line 19, in <module>\r\n    conv = tf.nn.conv3d(tf_dataset, layer1_weights, (1, 1, 1, 1, 1), padding='SAME')\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 846, in conv3d\r\n    padding=padding, data_format=data_format, name=name)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2936, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\HggsHntr\\AppData\\Local\\conda\\conda\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1464, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cuDNN Backward Filter function launch failure : input shape([1,512,512,512,1]) filter shape([2,2,2,1,1])\r\n         [[Node: gradients/Conv3D_grad/Conv3DBackpropFilterV2 = Conv3DBackpropFilterV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_Placeholder_0_0/_1, gradients/Conv3D_grad/Shape_1, gradients/add_grad/tuple/control_dependency)]]\r\n```", "Hi,\r\nMight be helpful for some of the people that are having the same problem.\r\nA small workaround for this problem that I am using is changing the driver model from WDDM to TCC. \r\nDoing it by \r\n`nvidia-smi -dm 1`\r\n", "@HggsHntr \r\nThis solves your problem completely on windows 10?", "I am having problem with evaluation network with slim.conv3d_transpose(which is wrapper of convolution3d_transpose, which is computed by the backprop of conv3d, therefore is related to this?)\r\nWhat's interesting is that the error message does not occur all the time, almost like a coin flipping...\r\nIt happens on both:\r\nwin7 + py3.6 + QuadroM4000 8g + pip tensorflow-gpu 1.4\r\nwin10 + py3.5 + GTX980m 8g + pip tensorflow-gpu 1.4\r\napparently, it occurs more frequently on the win7 machine \r\n```\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1359, in convolution3d_transpose\r\n    outputs = layer.apply(inputs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 671, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 1588, in call\r\n    padding=self.padding.upper())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1428, in conv3d_transpose\r\n    name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1083, in conv3d_backprop_input_v2\r\n    data_format=data_format, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cuDNN Backward Data function launch failure : input shape([1,80,160,160,32]) filter shape([5,5,5,32,32])\r\n\t [[Node: head_block/conv0/conv3d_transpose = Conv3DBackpropInputV2[T=DT_FLOAT, data_format=\"NDHWC\", padding=\"SAME\", strides=[1, 1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](head_block/conv0/stack, head_block/conv0/weights/read/_4949, decode_block_0/Residual_Block/add)]]\r\n\t [[Node: componentwise_overlap_loss/sub/_4957 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3379_componentwise_overlap_loss/sub\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "@qianyizhang\r\nI am not sure about windows 10. I only tested it on windows server 2016. \r\n ", "I encountered the same issue, with 3D convolution, cuda 8.0, cudnn 6.0, tensorflow 1.4.0. I encountered this actually while debugging a more strange phenomenon with 1.5.0: the GPU memory is fine but consumption of memory in CPU keeps growing almost linearly with time until it uses up all the memory (e.g., I set the limit to 40 GB, and it keeps running until 40GB memory is used by the CPU). So I downgraded to 1.4.0, but encountered this new issue as reported by the users above."]}, {"number": 5687, "title": "tf.const now supports verification of a shape of values. ", "body": "If shape that is passed via the argument is not consistent with the value in the shape variable ValueError will be raised. Example:\r\n\r\n\"Expected Tensor's shape: (2, 1), got (1, 2)\"", "comments": ["Can one of the admins verify this patch?\n", "@b0noI, thanks for your PR! By analyzing the history of the files in this pull request, we identified @itsmeolivia, @keveman and @mrry to be potential reviewers.\n", "@vrv this looks like a useful feature to me. Do you agree that we should extend the API of constant?\n", "@tensorflow-jenkins test this please\n", "ERROR form the CI:\nFAIL: Found 2 non-whitelited pylint errors:\ntensorflow/python/framework/tensor_util.py:402: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\n\ntensorflow/python/framework/tensor_util.py:403: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 8\n\nlooks like style checks error, will fix till EOD.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5686, "title": "Automated rollback of change 139483354", "body": "Change: 139520847", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @benlee, @tensorflower-gardener and @zhangyaobit to be potential reviewers.\n", "Jenkins, test this please.\n", "The failures in PR testing (Linux CPU and MacOS CPU) are unrelated. I'm merging the PR now.\n"]}, {"number": 5685, "title": "Fix integer overflow error", "body": "This sample crashes if used on big text files (>2B words).\r\nChanging corpus_size_ to 64-bit int solves the problem.", "comments": ["@MikalaiDrabovich, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @a-dai and @vrv to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA submitted\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please\n"]}, {"number": 5684, "title": "Building the library on Raspberry Pi results in: undefined reference to `google::protobuf::internal::fixed_address_empty_string'", "body": "Hello,\r\n\r\nI was told to post this question here:\r\n\r\nI am trying to complete a fresh install of Tensorflow on my Raspberry Pi following a failed previous install. I figured it would be easier trying to start fresh so I could more easily document my progress, and what solutions worked or failed. Additionally the new guide is easier to follow than the old guide I was using.\r\n\r\nI am following the guide [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi) and have been able to complete all of the steps up until the `make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8 `command. NOTE: I did not install the example graph.\r\n\r\nHowever when attempting to run this command I am left with several errors. The full output is rather long, but the main recurring error is similar to:\r\n\r\n`test_log.pb.cc:(.text+0x784): undefined reference to `google::protobuf::internal::fixed_address_empty_string'`\r\nThis kind of error is repeated several times throughout the output for the `make -f command with test_log.pb.cc:(.text+0x784): ` being replaced with different designations. The output ends with the Following error state:\r\n\r\n```\r\nstep_stats_collector.cc:(.text+0x46c): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/pi/makevoicedemo/tf/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(debug_io_utils.o):debug_io_utils.cc:(.text+0x8e8): more undefined references to `google::protobuf::internal::fixed_address_empty_string' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:501: recipe for target '/home/pi/makevoicedemo/tf/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed\r\nmake: *** [/home/pi/makevoicedemo/tf/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\r\n```\r\nI feel like the problem is likely do to an error with protobuf, however I am using the latest version and had no errors installing it. I am using the most recent version of tensorflow and followed the Before you start section in the guide to make sure all dependencies were installed and up to date.\r\n\r\nWhat exactly might be causing this error and how would I go about fixing it?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n[http://stackoverflow.com/questions/38915709/problems-on-builiding-tensorflow-on-rpi-2](http://stackoverflow.com/questions/38915709/problems-on-builiding-tensorflow-on-rpi-2)\r\n\r\n\r\n\r\n### Environment info\r\nOperating System:\r\nRaspbian Jessie\r\n\r\nInstalled version of CUDA and cuDNN: \r\nN/A\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n`ls: cannot access /path/to/cuda/lib/libcud*: No such file or directory\r\n`\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n`a2d96ca09e9943b4bad65b674d29d380dc6a8327`\r\n\r\n2. The output of `bazel version`\r\n\r\n```\r\n...........................................................................\r\nBuild label: 0.2.1-2016-09-15 (@e7a95e5)\r\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Sep 16 00:46:39 2016 (1473986799)\r\nBuild timestamp: 1473986799\r\nBuild timestamp as int: 1473986799\r\n\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nSteps taken (in order) from pre-created directory /tf:\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n\r\ncd tensorflow\r\n\r\ntensorflow/contrib/makefile/download_dependencies.sh\r\n\r\nsudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8\r\n\r\ncd tensorflow/contrib/makefile/downloads/protobuf/\r\n\r\n./autogen.sh\r\n\r\n./configure\r\n\r\nmake\r\n\r\nsudo make install\r\n\r\nsudo ldconfig\r\n\r\ncd ../../../../..\r\n\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8\r\n```\r\nThe final command results in the error and is as far as I have been able to progress.\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI have not come across any applicable solutions to try. \r\n\r\nAny help would be appreciated.\r\n", "comments": ["@petewarden Can you take a look?\n", "Sorry you're hitting problems! This looks like a mismatch between the protobuf version that's installed in your /usr/local system folders, and the one that was used to create the .pb.cc/.h files. Can you try running this command to build a local copy of the libs?\n\n```\ntensorflow/contrib/makefile/compile_linux_protobuf.sh\n```\n", "Tried `tensorflow/contrib/makefile/compile_linux_protobuf.sh` then ran `make -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os\" CXX=g++-4.8`\n\nResulted in the same error as before. \n\nI'm going to retry `tensorflow/contrib/makefile/compile_linux_protobuf.sh` and then run `make -f tensorflow/contrib/makefile/Makefile clean` to clean the makefile before running the make -f command again just to be sure.\n\nEDIT: Second attempt using a clean makefile failed as well, same error.\n", "Sorry for taking so long to respond. Holiday season was rather busy.\r\n\r\nI was looking through the /usr/local/lib folder and I've compiled a list of all the protobuf related files below:\r\n\r\n```\r\nlibprotobuf.a\r\nlibprotobuf.la\r\nlibprotobuf-lite.a\r\nlibprotobuf-lite.la\r\nlibprotobuf-lite.so\r\nlibprotobuf-lite.so.11\r\nlibprotobuf-lite.so.11.0.0\r\nlibprotobuf.so\r\nlibprotobuf.so.11\r\nlibprotobuf.so.11.0.0\r\n```\r\nAlso, according to `protoc --version` I currently have `libprotoc 3.1.0` installed. Could this be causing the mismatch? Should I switch to a different version? As far as I know this is the most up to date version.\r\n", "I had this issue and it's probably due to compiling protobuf with a different compiler.\r\nSince for tensorflow we use gcc-4.8 explicitly, we should also compile protobuf with gcc-4.8 as well. This fixed my problem. So just use \"make CXX=g++-4.8\" when you build protobuf.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5683, "title": "Synchronous distributed training is too slow", "body": "While trying to train Imagenet using Inceptionv3, I noticed the training was proceeding too slow. I then tested with Alexnet and Resnet and I saw the same slow training speed. \r\n\r\nHere is the images/sec I'm able to achieve on various models:\r\n\r\n```\r\nInceptionv3:\r\n# GPU, Images/sec\r\n1,        17.525\r\n2,        34.568\r\n4,        62.580\r\n8,        94.832\r\n16,       99.072\r\n32,       140.704\r\n64,       183.488\r\n128,      322.816\r\n\r\nAlexnet:\r\n1,        92.464\r\n2,        189.736\r\n4,        344.336\r\n8,        532.40\r\n16,       710.224\r\n32,       878.944\r\n64,       848.128\r\n128,      874.624\r\n\r\nResnet 152:\r\n1,        10.567\r\n2,        20.224\r\n4,        32.332\r\n8,        37.952\r\n16,       42.416\r\n32,       66.016\r\n64,       95.168\r\n128,      152.192\r\n\r\n```\r\nIs it possible there is some bug causing this slow performance?\r\n\r\nI'm using EC2 P2 instances (p2.16xlarge). Each instance has 16 GPUs. Each GPU is a Tesla K80. I'm running one worker per GPU and one PS per host. Here is more info on P2 instances: https://aws.amazon.com/ec2/instance-types/p2/\r\n\r\nI use randomly generated synthetic data for images and label. I've written some scripts to reproduce this issue in case it helps: https://github.com/indhub/tfperftest/tree/master/perftest\r\n\r\n- Inception model is based on https://github.com/tensorflow/models/tree/master/inception. I've done some changes to use randomly generated synthetic data for images and labels. Modified code here: https://github.com/indhub/tfperftest/tree/master/inception\r\n- Resnet model is based on https://github.com/tensorflow/models/tree/master/resnet. I've done some changes to build Resnet 152. Modified code here: https://github.com/indhub/tfperftest/tree/master/resnet\r\n- Alexnet model is based on https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/image/alexnet/alexnet_benchmark.py. I've added the fully connected layers at the end. Modified code here: https://github.com/indhub/tfperftest/blob/master/alexnet/alexnet.py\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/40411597/synchronous-distributed-training-is-slow\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nubuntu@ip-172-31-52-161:~$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root 558720 Sep 14 23:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Sep 14 23:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 415432 Sep 14 23:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 775162 Sep 14 23:02 /usr/local/cuda/lib64/libcudart_static.a\r\n\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nubuntu@ip-172-31-52-161:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0rc2\r\n\r\n```\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nJust running https://github.com/tensorflow/models/tree/master/inception reproduces the problem. \r\n", "comments": ["@shlens Do these seem like aberrant numbers?\n", "@jmchen-g Would you be able to take a look?\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5682, "title": "Implement a method that returns amount of seconds since the epoch (time)", "body": "In order to add timestamp to the ```tf.Print``` (as per #2076 ) there should be a method that provides time (something like: SecondsAfterEpoch). Existing API ([env.h](https://github.com/tensorflow/tensorflow/blob/dd01113c2b9efb16c3c3ea0a4aaa340a0494696f/tensorflow/core/platform/env.h)) provides 2 method to obtain time:\r\n-  [NowSeconds](https://github.com/tensorflow/tensorflow/blob/dd01113c2b9efb16c3c3ea0a4aaa340a0494696f/tensorflow/core/platform/env.h#L217);\r\n- [NowMicros](https://github.com/tensorflow/tensorflow/blob/dd01113c2b9efb16c3c3ea0a4aaa340a0494696f/tensorflow/core/platform/env.h#L213);\r\n\r\nThis makes both of them unusable for the task of adding the timestamp to the logging output. ", "comments": []}, {"number": 5681, "title": "Windows build of the PIP package fails : error LNK1120 LNK2019 LNK2001", "body": "@mrry Thanks for all your contributions to the windows version of tensorflow. \r\n I'm building tensorflow on windows 7 with CMake 3.6.3, GIT 2.10.1, Python 3.5.2 (NOT anaconda), the MsBuilt of VS 2015 (v 14.0.252420.1) and swigwin 3.0.10 without the GPU.\r\n\r\nIt successfully built the tf_tutorials_example_trainer target. However the MsBuild fails to build the PIP package, and gives 113 errors!\r\nThis is CMake command I used:\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release  -DSWIG_EXECUTABLE=c:/swigwin-3.0.10/swig.exe  -DPYTHON_EXECUTABLE=\"c:/Program Files (x86)/Python35-32/python.exe\" -DPYTHON_LIBRARIES=\"c:/Program Files (x86)/Python35-32/libs/python35.lib\" \r\n\r\nwhich finished with no errors, and this is MsBuild command I'm trying to run:\r\n\r\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj /filelogger\r\n\r\nCan anyone tell me what is going wrong?\r\n\r\nThe error reads like:\r\n\r\n\r\n> \"C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n> \"C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj\" (default target) (3) ->\r\n> (Link target) -> \r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyBytes_FromStringAndSize referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::CopyStringToPyArrayElement(struct tagPyArrayObject *,void *,struct TF_Tensor *,__int64,__int64)\" (?CopyStringToPyArrayElement@?A0x2033429b@tensorflow@@YA?AVStatus@2@PEAUtagPyArrayObject@@PEAXPEAUTF_Tensor@@_J3@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2001: unresolved external symbol __imp_PyBytes_FromStringAndSize [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyBytes_FromStringAndSize [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyBytes_AsString referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyArrayDescr_to_TF_DataType(struct _PyArray_Descr *,enum TF_DataType *)\" (?PyArrayDescr_to_TF_DataType@?A0x2033429b@tensorflow@@YA?AVStatus@2@PEAU_PyArray_Descr@@PEAW4TF_DataType@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyBytes_AsString [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyBytes_AsStringAndSize referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyBytesArrayMap<class <lambda_61881d22c4163ba98e316eda8a26e1e2> >(struct tagPyArrayObject *,class <lambda_61881d22c4163ba98e316eda8a26e1e2>)\" (??$PyBytesArrayMap@V<lambda_61881d22c4163ba98e316eda8a26e1e2>@@@?A0x2033429b@tensorflow@@YA?AVStatus@1@PEAUtagPyArrayObject@@V<lambda_61881d22c4163ba98e316eda8a26e1e2>@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2001: unresolved external symbol __imp_PyBytes_AsStringAndSize [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyBytes_AsStringAndSize [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_AsUTF8AndSize referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyBytesArrayMap<class <lambda_61881d22c4163ba98e316eda8a26e1e2> >(struct tagPyArrayObject *,class <lambda_61881d22c4163ba98e316eda8a26e1e2>)\" (??$PyBytesArrayMap@V<lambda_61881d22c4163ba98e316eda8a26e1e2>@@@?A0x2033429b@tensorflow@@YA?AVStatus@1@PEAUtagPyArrayObject@@V<lambda_61881d22c4163ba98e316eda8a26e1e2>@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyDict_Next referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyArrayDescr_to_TF_DataType(struct _PyArray_Descr *,enum TF_DataType *)\" (?PyArrayDescr_to_TF_DataType@?A0x2033429b@tensorflow@@YA?AVStatus@2@PEAU_PyArray_Descr@@PEAW4TF_DataType@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyEval_SaveThread referenced in function \"void __cdecl tensorflow::TF_PRunSetup_wrapper(struct TF_DeprecatedSession *,class tensorflow::gtl::InlinedVector<char const *,8> const &,class tensorflow::gtl::InlinedVector<char const *,8> const &,class tensorflow::gtl::InlinedVector<char const *,8> const &,struct TF_Status *,char const * *)\" (?TF_PRunSetup_wrapper@tensorflow@@YAXPEAUTF_DeprecatedSession@@AEBV?$InlinedVector@PEBD$07@gtl@1@11PEAUTF_Status@@PEAPEBD@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyEval_SaveThread [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp_PyEval_RestoreThread referenced in function \"void __cdecl tensorflow::TF_PRunSetup_wrapper(struct TF_DeprecatedSession *,class tensorflow::gtl::InlinedVector<char const *,8> const &,class tensorflow::gtl::InlinedVector<char const *,8> const &,class tensorflow::gtl::InlinedVector<char const *,8> const &,struct TF_Status *,char const * *)\" (?TF_PRunSetup_wrapper@tensorflow@@YAXPEAUTF_DeprecatedSession@@AEBV?$InlinedVector@PEBD$07@gtl@1@11PEAUTF_Status@@PEAPEBD@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyEval_RestoreThread [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   tf_session_helper.obj : error LNK2019: unresolved external symbol __imp__Py_NoneStruct referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::TF_Tensor_to_PyObject(struct TF_Tensor *,struct _object * *)\" (?TF_Tensor_to_PyObject@?A0x2033429b@tensorflow@@YA?AVStatus@2@PEAUTF_Tensor@@PEAPEAU_object@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   cpp_shape_inference.obj : error LNK2001: unresolved external symbol __imp__Py_NoneStruct [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2001: unresolved external symbol __imp__Py_NoneStruct [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   cpp_shape_inference.obj : error LNK2019: unresolved external symbol __imp_PyList_Size referenced in function \"class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > __cdecl tensorflow::swig::RunCppShapeInference(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,struct _object *,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,struct TF_Status *)\" (?RunCppShapeInference@swig@tensorflow@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@4@AEBV34@PEAU_object@@1PEAUTF_Status@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2001: unresolved external symbol __imp_PyList_Size [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyList_Size [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   cpp_shape_inference.obj : error LNK2019: unresolved external symbol __imp_PyList_GetItem referenced in function \"class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > __cdecl tensorflow::swig::RunCppShapeInference(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,struct _object *,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,struct TF_Status *)\" (?RunCppShapeInference@swig@tensorflow@@YA?AV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@4@AEBV34@PEAU_object@@1PEAUTF_Status@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2001: unresolved external symbol __imp_PyList_GetItem [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyList_GetItem [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyObject_GetAttrString referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyObject_GetAttrString [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyCapsule_GetPointer referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyCapsule_GetPointer [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyErr_SetString referenced in function \"void __cdecl tensorflow::ImportNumpy(void)\" (?ImportNumpy@tensorflow@@YAXXZ) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyErr_SetString [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyErr_Format referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyErr_Print referenced in function \"void __cdecl tensorflow::ImportNumpy(void)\" (?ImportNumpy@tensorflow@@YAXXZ) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2001: unresolved external symbol __imp_PyErr_Print [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyImport_ImportModule referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyCapsule_Type referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyExc_AttributeError referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyExc_AttributeError [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyExc_ImportError referenced in function \"void __cdecl tensorflow::ImportNumpy(void)\" (?ImportNumpy@tensorflow@@YAXXZ) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   numpy.obj : error LNK2019: unresolved external symbol __imp_PyExc_RuntimeError referenced in function _import_array [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyExc_RuntimeError [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyType_IsSubtype referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::DoCallPyFunc(struct tensorflow::A0x053e5023::PyCall *)\" (?DoCallPyFunc@?A0x053e5023@tensorflow@@YA?AVStatus@2@PEAUPyCall@12@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyList_New referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::MakeArgTuple(struct tensorflow::A0x053e5023::PyCall *,struct _object * *)\" (?MakeArgTuple@?A0x053e5023@tensorflow@@YA?AVStatus@2@PEAUPyCall@12@PEAPEAU_object@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyList_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyList_SetItem referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::MakeArgTuple(struct tensorflow::A0x053e5023::PyCall *,struct _object * *)\" (?MakeArgTuple@?A0x053e5023@tensorflow@@YA?AVStatus@2@PEAUPyCall@12@PEAPEAU_object@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyGILState_Ensure referenced in function \"public: virtual void __cdecl tensorflow::PyFuncOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@PyFuncOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyGILState_Release referenced in function \"public: virtual void __cdecl tensorflow::PyFuncOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@PyFuncOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyErr_Occurred referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::DoCallPyFunc(struct tensorflow::A0x053e5023::PyCall *)\" (?DoCallPyFunc@?A0x053e5023@tensorflow@@YA?AVStatus@2@PEAUPyCall@12@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyErr_Occurred [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_Py_BuildValue referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::MakeArgTuple(struct tensorflow::A0x053e5023::PyCall *,struct _object * *)\" (?MakeArgTuple@?A0x053e5023@tensorflow@@YA?AVStatus@2@PEAUPyCall@12@PEAPEAU_object@@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_Py_BuildValue [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   py_func.obj : error LNK2019: unresolved external symbol __imp_PyEval_CallObjectWithKeywords referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::DoCallPyFunc(struct tensorflow::A0x053e5023::PyCall *)\" (?DoCallPyFunc@?A0x053e5023@tensorflow@@YA?AVStatus@2@PEAUPyCall@12@@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyType_Type referenced in function SwigPyClientData_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyBool_Type referenced in function _wrap_FileStatistics_is_directory_set [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_IndexError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_MemoryError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_OverflowError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_NotImplementedError referenced in function _wrap_new_Status [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_SyntaxError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_SystemError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_TypeError referenced in function SwigPyObject_append [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_ValueError referenced in function _wrap_StatSummarizer_ProcessStepStats [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_ZeroDivisionError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyExc_IOError referenced in function \"struct _object * __cdecl SWIG_Python_ErrorType(int)\" (?SWIG_Python_ErrorType@@YAPEAU_object@@H@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyType_Ready referenced in function SwigPyObject_TypeOnce [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_GetAttr referenced in function SWIG_Python_GetSwigThis [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_SetAttr referenced in function SWIG_Python_NewShadowInstance [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_GenericGetAttr referenced in function SwigPyObject_TypeOnce [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_IsTrue referenced in function SwigPyObject_own [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_Py_DecRef referenced in function SwigPyObject_repr [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_Malloc referenced in function SwigPyObject_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_Free referenced in function SwigPyObject_dealloc [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_Init referenced in function SwigPyObject_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp__PyObject_New referenced in function SWIG_Python_NewPointerObj [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_FromStringAndSize referenced in function _wrap_CheckpointReader_get_variable_to_shape_map [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_FromString referenced in function SWIG_Python_DestroyModule [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_FromFormat referenced in function SwigPyObject_repr [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_DecodeUTF8 referenced in function _wrap_StatSummarizer_GetOutputString [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_AsUTF8String referenced in function _wrap_TF_PRun [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyUnicode_Concat referenced in function SwigPyObject_repr [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_FromLong referenced in function _wrap_FileStatistics_length_get [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_AsLong referenced in function _wrap_DoQuantizeTrainingOnGraphDefHelper [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2001: unresolved external symbol __imp_PyLong_AsUnsignedLong [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_FromVoidPtr referenced in function SwigPyObject_long [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_FromLongLong referenced in function _wrap_FileStatistics_length_get [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_FromUnsignedLongLong referenced in function _wrap_PyRecordReader_offset [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_AsLongLong referenced in function _wrap_FileStatistics_length_set [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyLong_AsUnsignedLongLong referenced in function _wrap_PyRecordReader_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyBool_FromLong referenced in function _wrap_IsDirectory [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyTuple_New referenced in function SwigPyClientData_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyTuple_SetItem referenced in function SwigPyClientData_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyDict_New referenced in function _wrap_CheckpointReader_get_variable_to_shape_map [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyDict_SetItem referenced in function _wrap_CheckpointReader_get_variable_to_shape_map [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyDict_SetItemString referenced in function PyInit__pywrap_tensorflow [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyModule_GetDict referenced in function PyInit__pywrap_tensorflow [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyInstanceMethod_New referenced in function SWIG_PyInstanceMethod_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyCapsule_New referenced in function SWIG_InitializeModule [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyCapsule_Import referenced in function SWIG_InitializeModule [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyErr_Clear referenced in function _wrap_FileStatistics_length_set [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyErr_Fetch referenced in function SwigPyObject_dealloc [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyErr_Restore referenced in function SwigPyObject_dealloc [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyErr_WriteUnraisable referenced in function SwigPyObject_dealloc [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyArg_ParseTuple referenced in function _wrap_PyRecordWriter_Close [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyArg_UnpackTuple referenced in function SwigPyObject_own [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyModule_AddObject referenced in function SWIG_InitializeModule [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyModule_Create2 referenced in function PyInit__pywrap_tensorflow [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyImport_AddModule referenced in function SWIG_InitializeModule [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_Call referenced in function SWIG_Python_NewShadowInstance [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_CallFunctionObjArgs referenced in function SWIG_Python_ConvertPtrAndOwn [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_Size referenced in function _wrap_new_Status [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_GetIter referenced in function \"bool __cdecl tf_vector_input_helper<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(struct _object *,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > *,bool (__cdecl*)(struct _object *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * const))\" (??$tf_vector_input_helper@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@@YA_NPEAU_object@@PEAV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@P6A_N0QEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@@Z@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyIter_Next referenced in function \"bool __cdecl tf_vector_input_helper<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >(struct _object *,class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > *,bool (__cdecl*)(struct _object *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > * const))\" (??$tf_vector_input_helper@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@@YA_NPEAU_object@@PEAV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@P6A_N0QEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@@Z@Z) [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp_PyObject_IsInstance referenced in function SwigPyClientData_New [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   pywrap_tensorflow.obj : error LNK2019: unresolved external symbol __imp__Py_NotImplementedStruct referenced in function SwigPyObject_richcompare [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n>   C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\Release\\pywrap_tensorflow.dll : fatal error LNK1120: 90 unresolved externals [C:\\Temp\\tensorflow\\tensorflow\\contrib\\cmake\\release\\pywrap_tensorflow.vcxproj]\r\n> \r\n>     355 Warning(s)\r\n>     113 Error(s)\r\n> \r\n> Time Elapsed 00:34:22.63\r\n", "comments": ["Hmm, it sure does look like the `python35.lib` library is missing - all the unresolved externals seem to be Python API functions and types. Is it possible that `\"c:/Program Files (x86)/Python35-32/libs/python35.lib\"` is not the correct path to your Python import library?\n", "Well, that's the only location where all the *.lib files exist. Here's the the list of libraries there:\n_tkinter.lib\npython3.lib\npython3_d.lib\npython35.lib\npython35_d.lib\n\nMaybe its path is too long. I'll try another folder see if it works or not.\n", "Hey,\r\n\r\nFor some reason the python35.lib file was corrupted (maybe a mismatch version). I reinstalled it using the [python 3.5.2 amd 64 bit](https://www.python.org/ftp/python/3.5.2/python-3.5.2-amd64.exe). Now it works. Thanks for your hint. I'll see if I can get a 32bit version working :)", "That's great to hear - thanks for following up!\r\n\r\nI'll close this issue now, but please feel free to open another one if you run into any more problems."]}, {"number": 5680, "title": "Anyway to clear/reset the static shape to None?", "body": "`Tensor.set_shape()` doesn't set shape but merge shape. If I don't want the statically-inferred shape, could there be a method to clear it?\r\n\r\nThe use case is as following: I have:\r\n```python\r\nx = tf.random_uniform([128,10])\r\n# other operations involving x\r\n```\r\nin the graph, but I want to be able to run it (in inference time) with `feed_dict={x: value}` where `value` is a tensor of shape [1, 10]. It would work perfectly if I could somehow set the shape of x to [None, 10].\r\nCurrently I'm using `tf.shape()` as argument of `random_uniform` to avoid shape inference and `set_shape` later, which is not a very nice solution.", "comments": ["There's deliberately no way to make a tensor's shape _less_ specific. This makes it safe for TensorFlow to use the current static shape of a tensor in later shape inference, and also to specialize the graph for a specific shape (see e.g. the implementation of `tf.shape()`, which returns a constant if the shape is known).\n\nHowever, your feeding use case is supported, using the [`tf.placeholder_with_default()`](https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html#placeholder_with_default) op, which lets you define a feed point with both (i) a default value when you don't feed it, and (ii) a less specific static shape than the default value. You could use it in your case as follows:\n\n``` python\nx_default = tf.random_uniform([128,10])\nx = tf.placeholder_with_default(x_default, shape=[None, 10])\n# other operations involving x\n```\n", "Thanks! `placeholder_with_default` works for my case.\n", "HI, I'm also facing a similar issue that requires to modify the static shape to None. But it cannot be handled by placeholder_with_default. I'm loading a pre-trained model/graph that has a static shape. \r\n\r\nThe thing is that I'm using the Inception Score, which is a very important evaluation metric in generative models, especially the GANs community.  \r\n\r\nIt involves the pre-trained model http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz. But it has a static batch size 1, which makes the inference extremely slow. It was used to be handled by the following code, i.e. changing the tensor shape via tensor._shape = new_shape.\r\n\r\n```\r\n ops = self.inception_graph.get_operations()\r\n        for op_idx, op in enumerate(ops):\r\n            for o in op.outputs:\r\n                shape = o.get_shape()\r\n                shape = [s.value for s in shape]\r\n                new_shape = []\r\n                for j, s in enumerate(shape):\r\n                    if s == 1 and j == 0:\r\n                        new_shape.append(None)\r\n                    else:\r\n                        new_shape.append(s)\r\n                # o._shape = tf.TensorShape(new_shape)\r\n```\r\nThe above operation works for tensorflow 1.5 and earlier version. But for the latter version such as tensorflow 1.9, o._shape = tf.TensorShape(new_shape) is no longer allowed, and set_shape only merges shape, but not support modifying known to None.\r\n\r\nResetting shape seems necessary here.  So I'm looking forward to a update_shape operation. I understand that \"there's deliberately no way to make a tensor's shape less-specific.\" But any idea to solve the above-mentioned problem, any help? The current forces me to use tensorflow up to version 1.5.\r\n\r\nThanks a lot!", "You can use the `input_map` argument in `tf.import_graph_def()` to rebind the input tensor to a `tf.placeholder()` created in the target graph. "]}, {"number": 5679, "title": "Branch 139498839", "body": "Internal push.", "comments": []}, {"number": 5678, "title": "update batch norm layer decay param documentation", "body": "Update `decay` parameter documentation in contrib.layers.batch_norm, suggest users lower `decay` value if experiencing reasonable training performance but poor reference performance. This PR is associated with the content discussed in [issue #1122 ](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-259762584)", "comments": ["Can one of the admins verify this patch?\n", "@zhongyuk, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zhangyaobit, @tensorflower-gardener and @martinwicke to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed a Contributor License Agreement (CLA)!\n\nOn Thu, Nov 17, 2016 at 4:50 PM, googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5678#issuecomment-261380636,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGlNU9zDIAoM8z-ZheAxO9p_c8yn5GVzks5q_MwugaJpZM4K12pQ\n> .\n\n## \n\n_Best regards,_\n_Zhongyu KUANG_\n_dream.kuang@gmail.com_ dream.kuang@gmail.com\n_zhongyu@envsci.rutgers.edu_ zhongyu@envsci.rutgers.edu\n_Tel: 732-789-9178_\n_Add: 14 College Farm Road, New Brunswick, New Jersey_\n_Atmospheric Science, the Department of Environmental Science, *_Rutgers*\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 5677, "title": "run bazel build tensorflow/examples/label_image/...  fail", "body": "I just sync the **latest code** and build from source.\r\n\r\nhttps://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html\r\n\r\ni run **bazel build tensorflow/examples/label_image/...** get error:\r\n\r\nERROR: /home/scopeserver/RaidDisk/tensorflow/tensorflow/examples/label_image/BUILD:10:1: no such package 'tensorflow/contrib/quantization/kernels': BUILD file not found on package path and referenced by '//tensorflow/examples/label_image:label_image'.\r\nERROR: Analysis of target '//tensorflow/examples/label_image:label_image' failed; build aborted.\r\nINFO: Elapsed time: 0.116s\r\n", "comments": ["but i run **bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package** with no errors. \n", "@civilmanxx You said you synced to the latest code, but you linked to 0.11.  Which version are you using?\n", "The link points out the source of the cmd, but not for code version.\n\nI want to build the file for single image prediction. the source code is still the latest one.\n", "@civilmanxx I don't think you're using up to date TensorFlow.  The version of `label_image/BUILD` that I see makes no mention of quantization: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/BUILD\n", "yeah, I just check there are 2 quantization related lines in the build file. I do not why, though I pull the latest code.\n"]}, {"number": 5676, "title": "UnicodeDecodeError when reading from tfrecords", "body": "All I'm really trying to do is read a large array from a tfrecords file. I can do it so long that it's below a 8million ints but above that I get a unicode error, which doesn't seem to make much sense to me. Also, it worth noting that this script works using r0.9. I have about 64GB of ram on my computer so I doubt it's a memory issue. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\nLinux Mint 18 Sarah (GNU/Linux 4.4.0-47-generic x86_64)\r\n\r\nInstalled version of CUDA and cuDNN: \r\nnot installed\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndim_size = 9000000\r\n\r\n# Create the array\r\narray = np.zeros([dim_size], dtype=int)\r\n\r\nexample = tf.train.Example(\r\n    features = tf.train.Features(\r\n        feature = {\r\n            'array': tf.train.Feature(\r\n                bytes_list= tf.train.BytesList(value=[array.tostring()])\r\n            )\r\n        }\r\n    )\r\n)\r\n\r\n# Write the example to disk\r\nwriter = tf.python_io.TFRecordWriter('temp.tfrecords')\r\nwriter.write(example.SerializeToString())\r\nwriter.close()\r\n\r\n# Define input pipeline\r\nqueue = tf.train.string_input_producer(['temp.tfrecords'], shuffle=False, num_epochs=1)\r\nreader = tf.TFRecordReader()\r\n_, se = reader.read(queue)\r\n\r\n#Define the parsed dict\r\nparsed = tf.parse_single_example(\r\n    serialized=se,\r\n    features={\r\n        \"array\":tf.FixedLenFeature([], tf.string),\r\n    }\r\n)\r\n\r\n# Decode the example\r\ntens = tf.decode_raw(parsed['array'], tf.int64)\r\n\r\n# Define the init op.\r\ninit_op = tf.group(\r\n    tf.initialize_all_variables(),\r\n    tf.initialize_local_variables()\r\n)\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n    # Define coordinator and threads\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n    # Run the reading operation\r\n    print sess.run([tens])\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nThis works using r0.9 and it also works if you lower the dim_size below 800000. So it seems to be an issue with the size and r0.11. I also get the same results if I use a float array as opposed to an int. \r\n\r\n### Logs or other output that would be helpful\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Could not parse example input, value: '\r\n?\u012a\"\r\n?\u012a\"\r\narray?\u012a\"\r\n?\u012a\"\r\n?\u012a\"\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 52, in <module>\r\n    print sess.run([tens])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 972, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 954, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors.py\", line 462, in raise_exception_on_not_ok_status\r\n    compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.py\", line 82, in as_text\r\n    return bytes_or_text.decode('utf-8')\r\n  File \"/usr/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x9b in position 40: invalid start byte\r\n```\r\n", "comments": ["@mrry Should we be escaping strings in error messages?  I vaguely recall having part of that discussion before.\n\n@CRSilkworth The unicode error is secondary.  Your immediate problem is the `Could not parse example input` error.  @martinwicke Is this the dreaded protobuf limit?  The error is from https://github.com/tensorflow/tensorflow/blob/dfc5cd48a095b133ece9caff663e3cc512e8a268/tensorflow/core/kernels/example_parsing_ops.cc#L326.\n", "@girving Yes, I think we should be escaping strings here. I don't remember what the outcome of the earlier discussion was (in particular, where the encoding should happen) but it seems like we use `str_util::CEscape(binary_string)` in a few kernels, so perhaps we should use it here?\n", "@ebrevdo Care to add escaping to that error message?  Assigning to you for now.\n", "Yes on Monday.\n\nOn Fri, Nov 18, 2016 at 11:12 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo Care to add escaping to that error\n> message? Assigning to you for now.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5676#issuecomment-261615834,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim1N3sy6ABDqCbq0fmAo7lqUZtyYOks5q_fiZgaJpZM4K1yxn\n> .\n", "Did we ever determine if the real issue is this protobuf limit? Am I able to avoid this some how or will I have to revert back to 0.9?", "I don't know why there would be a difference between 0.9 and 0.11 then.\nOtherwise it looks plausible. Sadly the real error is hidden by the Unicode\nproblem. Did you install the >64MB version of protobuf? Assuming that that\nis used and nothing else (hard to tell), the limit shouldn't trigger at 8M\nints.\n\nOn Wed, Nov 23, 2016 at 05:00 Christopher Silkworth <\nnotifications@github.com> wrote:\n\n> Did we ever determine if the real issue is this protobuf limit? Am I able\n> to avoid this some how or will I have to revert back to 0.9?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5676#issuecomment-262506458,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_QA2w6TLQ8cqVLuaZvIdF6vOX8Iaks5rBDj_gaJpZM4K1yxn\n> .\n", "Hmm I'm not sure if my protobuf version is > 64MB, I just installed the one that tensorflow as a dependency (protobuf==3.0.0). \r\n\r\nIs there anything I can do to help out in tracking this down? ", "There's a protobuf version on our website with the 64MB limit removed: \r\nhttps://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#protobuf-library-related-issues\r\n\r\nCan you try installing one of those packages? Make sure to uninstall protobuf beforehand.", "I met the same problem and verified installing new protobuf library doesn't help.\r\n\r\nmy version is 0.12, installed through anaconda2", "I have the same problem. Using the other protobuf library doesn't help.", "Is this still an issue in current TF? \r\n\r\n@ebrevdo did you ever fix the error message?", "Replicated only part of the error in python3:\n\n```\nwith open('temp.tfrecords', 'rb') as v:\n    s = v.read()\nz = tf.parse_single_example(serialized=s, features={\n\n        \"array\":tf.FixedLenFeature([], tf.string),\n    })\ns = tf.Session()\ns.run(z)\n```\n\nreturns\n\n```\n2017-03-27 16:35:53.868045: W tensorflow/core/framework/op_kernel.cc:1150]\nInvalid argument: Could not parse example input, value: ' \ufffdJ\nERROR - failed to write data to stream: <_io.TextIOWrapper name='<stdout>'\nmode='w' encoding='UTF-8'>\n```\n\n\nOn Mon, Mar 27, 2017 at 11:17 AM, Martin Wicke <notifications@github.com>\nwrote:\n\n> Is this still an issue in current TF?\n>\n> @ebrevdo <https://github.com/ebrevdo> did you ever fix the error message?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5676#issuecomment-289538813>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-BoKP_X6_XVB_j2D9DRPlogg6Lzks5rp_1EgaJpZM4K1yxn>\n> .\n>\n", "That said, running:\n\n\nreturns\n\nDecodeError: Unexpected end-group tag.\n\nin other words, looks like the proto is invalid once written?  weird.\n looks like an error by the TFRecordWriter?\n\nthis works just fine:\n\n```\nsess.run(\n  tf.parse_single_example(serialized=example.SerializeToString(),\n    features={\n\n        \"array\":tf.FixedLenFeature([], tf.string),\n    })\n```\n\nso looks like the issue is somewhere in the TFRecordWriter or\nTFRecordReader.\n\nOn Mon, Mar 27, 2017 at 4:36 PM, Eugene Brevdo <ebrevdo@gmail.com> wrote:\n\n> Replicated only part of the error in python3:\n>\n> ```\n> with open('temp.tfrecords', 'rb') as v:\n>     s = v.read()\n> z = tf.parse_single_example(serialized=s, features={\n>\n>         \"array\":tf.FixedLenFeature([], tf.string),\n>     })\n> s = tf.Session()\n> s.run(z)\n> ```\n>\n> returns\n>\n> ```\n> 2017-03-27 16:35:53.868045: W tensorflow/core/framework/op_kernel.cc:1150]\n> Invalid argument: Could not parse example input, value: ' \ufffdJ\n> ERROR - failed to write data to stream: <_io.TextIOWrapper name='<stdout>'\n> mode='w' encoding='UTF-8'>\n> ```\n>\n>\n> On Mon, Mar 27, 2017 at 11:17 AM, Martin Wicke <notifications@github.com>\n> wrote:\n>\n>> Is this still an issue in current TF?\n>>\n>> @ebrevdo <https://github.com/ebrevdo> did you ever fix the error message?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/5676#issuecomment-289538813>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim-BoKP_X6_XVB_j2D9DRPlogg6Lzks5rp_1EgaJpZM4K1yxn>\n>> .\n>>\n>\n>\n", "@saxenasaurabh I saw you were working on the reader/writer a bit, can you take a look at this?", "I also have this problem. I generated the tfrecords on Windows system, and the problem occured. But it's fine to parse on Ubuntu system using the same parsing code.", "@martinwicke Can it be that #5676 #7311 and #13575 are all relating to the same issue which is still present on windows?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this looks like a duplicate of [#13575](https://github.com/tensorflow/tensorflow/issues/13575) which I have been unable to reproduce. Please re-try and re-open [#13575](https://github.com/tensorflow/tensorflow/issues/13575) if this is still an issue."]}, {"number": 5675, "title": "Fix gpu-mac test build issues with test_installation script.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan, @ebrevdo and @martinwicke to be potential reviewers.\n", "Here is a link to the experimental job https://ci.tensorflow.org/job/experimental-gpu-mac-pip-yifeif/2/console\n"]}, {"number": 5674, "title": "run ./configure error ", "body": "I just sync the latest code and try to build from source. CUDA version 8, cuDNN 5.1.5, Ubuntu 16.04\r\n\r\n\r\nrun ./configure  and get the error below:\r\n\r\n\r\nERROR: /home/scopeserver/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:76:1: **no such package '@org_tensorflow//tensorflow/core':** error loading package 'external': The repository named **'org_tensorflow' could not be resolved and referenced by '//tensorflow/models/syntaxnet/syntaxnet:base'.**\r\nERROR: /home/scopeserver/tensorflow/tensorflow/models/inception/inception/slim/BUILD:41:1: no such package 'inception': BUILD file not found on package path and referenced by '//tensorflow/models/inception/inception/slim:losses'.\r\nERROR: /home/scopeserver/tensorflow/tensorflow/models/inception/inception/slim/BUILD:43:12: no such package 'inception': BUILD file not found on package path (this is usually caused by a missing package group in the package-level visibility declaration).\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n", "comments": ["need assign 8.0 and 5.1.5 for cuda config.\n"]}, {"number": 5673, "title": "Possible typo in docstring of embedding_rnn_seq2seq and embedding_attention_seq2seq?", "body": "So in the docstring of embedding_rnn_seq2seq:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296\r\n\r\nIt says:\r\n\r\n```\r\n  This model first embeds encoder_inputs by a newly created embedding (of shape\r\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\r\n```\r\n\r\nShould that instead read:\r\n\r\n```\r\n  [embedding_size x input_size]). Then it runs an RNN to encode\r\n```\r\n\r\nSince if you are creating an embedding it is to do dimensionality reduction (and so you usually want your embedding size to be smaller than the number of encoder symbols). Also in the code:\r\n\r\n```\r\n    # Encoder.\r\n    encoder_cell = rnn_cell.EmbeddingWrapper(\r\n        cell, embedding_classes=num_encoder_symbols,\r\n        embedding_size=embedding_size)\r\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\r\n```\r\n\r\nIt looks like the encoder inputs are mapped to vectors of size embedding_size.\r\n\r\nLooking forward to hearing the community thoughts (this is also the case in the docstring of embedding_attention_seq2seq).", "comments": ["Yep, looks like this should be fixed.\n", "shuvrobiswas@: you ask if it the shape of the embedding is [embedding_size x input_size], right?\n\nThe answer is no. It's [num_encoder_symbols x embedding_size] -- because every symbol (e.g., word) needs to be embedded into a dense vector of size embedding_size. It's dimensionality reduction only in a certain sense -- most importantly, it's going from a sparse space of size num_encoder_symbols to  dense vectors of embedding_size floats. And \"embedding\" here means the embedding matrix, every single symbol will be embedded into a vector of size [embedding_size]. Hope that helps!\n", "@lukaszkaiser Thanks for the response, I agree the shape of the embedding is of course [num_encoder_symbols x embedding_size]\n\nIn the docstring it says \"model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size])\". I interpreted that as talking about taking encoder_inputs and then getting the corresponding dense vector representation of those encoder_inputs which would be of dimension [input_size x embedding_size] or the transpose [embedding_size x input_size]?\n"]}, {"number": 5672, "title": "[CMake] Avoid common pitfalls in CMake build.", "body": "Fixes a couple of issues that have cropped up:\r\n* Fixes #5576 by not looking for pthreads on WIN32 where we don't need it.\r\n* Fixes #5670 by rebuilding the SWIG wrapper on each build.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ageron, @danmane and @guschmue to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "GCE issue in GPU test, that should not be affected by this PR at all.\nmerging.\n"]}, {"number": 5671, "title": "'tensorflow.core.framework.types_pb2' has no attribute 'DT_RESOURCE'", "body": "I successfully built the PIP package (at HEAD) on Windows with cmake.  When I tried to import it, I met several errors that all boil down to the following:\r\n\r\n```\r\nAttributeError: module 'tensorflow.core.framework.types_pb2' has no attribute 'DT_RESOURCE'\r\nAttributeError: module 'tensorflow.core.framework.types_pb2' has no attribute 'DT_RESOURCE_REF'\r\n```", "comments": ["Maybe because I had a previous build, git pulled and pb was using the old files?\n", "This sounds like a similar problem to #5670. In this case, `protoc` isn't re-running, I'm guessing you've done an earlier build based on an old version of `tensorflow\\core\\framework\\types.proto`, and the PIP package contains a mixture of code based on that old version and new code that expects to see the new types.\n\nYour best bet (in the short term) is probably to delete the entire `tf_python` directory to force a rebuild.\n", "@mrry Thank you, sir. I can confirm it's all good!\n"]}, {"number": 5670, "title": "Windows cmake build errors", "body": "@mrry I tried to build the HEAD and got the errors below:\r\n\r\n```\r\n\"C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n\"C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj\" (default target) (3) ->\r\n(ClCompile target) ->\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(3823): error C2440: 'return': cannot convert from 'tensorflow::Status' to 'bool' [C:\\sr\r\nc\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(3827): error C2440: 'return': cannot convert from 'tensorflow::Status' to 'bool' [C:\\sr\r\nc\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(5227): error C2660: 'TF_NewSession': function does not take 2 arguments [C:\\src\\tensorf\r\nlow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(5343): error C2664: 'void TF_ExtendGraph(TF_DeprecatedSession *,const void *,size_t,TF_\r\nStatus *)': cannot convert argument 1 from 'TF_Session *' to 'TF_DeprecatedSession *' [C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxpr\r\noj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(5544): error C2664: 'void tensorflow::TF_Run_wrapper(TF_DeprecatedSession *,const TF_Bu\r\nffer *,PyObject *,const tensorflow::NameVector &,const tensorflow::NameVector &,TF_Status *,tensorflow::PyObjectVector *,TF_Buffer *)': cannot convert argument\r\n 1 from 'TF_Session *' to 'TF_DeprecatedSession *' [C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(5709): error C2664: 'void tensorflow::TF_PRunSetup_wrapper(TF_DeprecatedSession *,const\r\n tensorflow::NameVector &,const tensorflow::NameVector &,const tensorflow::NameVector &,TF_Status *,const char **)': cannot convert argument 1 from 'TF_Session\r\n *' to 'TF_DeprecatedSession *' [C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(5804): error C2664: 'void tensorflow::TF_PRun_wrapper(TF_DeprecatedSession *,const char\r\n *,PyObject *,const tensorflow::NameVector &,TF_Status *,tensorflow::PyObjectVector *)': cannot convert argument 1 from 'TF_Session *' to 'TF_DeprecatedSession\r\n *' [C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n  c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc(7501): error C2660: 'tensorflow::swig::RunCppShapeInference': function does not take 4\r\narguments [C:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj]\r\n\r\n    341 Warning(s)\r\n    8 Error(s)\r\n```", "comments": ["@mrry Any more information needed?\n", "Can you try deleting `c:\\src\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.cc` and trying again? (My guess is that you've built at a previous revision, checked out a newer revision that changes aspects of the SWIGged API, and then tried to build again in the same directory.)\n", "you are right about my having previous build! let me try.\n", "Worked! My bad. Thank you!\n", "Thanks for trying out the build! We could try adding a rule that causes the file to always be regenerated and avoid this issue. (Alternatively we could track the set of headers and `.i` files that the code generator depends on, but I suspect that'll be brittle....)\n"]}, {"number": 5669, "title": "Small typo in documentation", "body": "In the code example in https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#replica_device_setter  `tf.replica_device_setter` is used instead of `tf.train.replica_device_setter`.", "comments": ["Thanks!  Want to submit a quick PR?\n", "This issue has been fixed. It can be closed. :)"]}, {"number": 5668, "title": "Fix building for iOS #4640", "body": "Seems like the reason why compilation fails does not directly related to TensorFlow or even Protobuf:\r\n\r\nThe `configure` script of Protobuf (generated by `autogen`) has non trivial logic for detecting cross-compilation (currently there checks can be found in lines 3918\u20133976 at `configure` file). It tries to compile some simple C programs into executable and run it during the configuration process. It works fine except the case when we build for x86_64 iOS simulator: one the one hand we get native code for build platform one the other hand we can not execute test program because it is based on iOS sdk. As result the `configure` script generates error.\r\n\r\nFrankly speaking I still do not fully understand the logic behind cross-compilation check, however I've found workaround for building Protobuf and TensorFlow for all platforms including x86_64 iOS simulator. The workaround is simple: just remove `--build=x86_64-apple-${OSX_VERSION} \\` string for x86_64 in `compile_ios_protobuf.sh` file. Note, for consistency I also removed same strings for all other platforms (x386, armv7, armv7s, arm64), however it doesn't seems necessary.\r\n\r\nMy platform: macOS Sierra 10.12.1, Xcode 8.1, autoconf 2.69 (installed via homebrew).", "comments": ["@Gubarev, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @tensorflower-gardener and @petewarden to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I am the CTO of CVisionLab company. Our CEO have signed CLA recently and added me to special contrib@cvisionlab.com mailto:contrib@cvisionlab.com google group.\n\n> On 17 Nov 2016, at 18:01, googlebot notifications@github.com wrote:\n> \n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd Please visit https://cla.developers.google.com/ https://cla.developers.google.com/ to sign.\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n> \n> If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA data https://cla.developers.google.com/clas and verify that your email is set on your git commits https://help.github.com/articles/setting-your-email-in-git/.\n> If you signed the CLA as a corporation, please let us know the company's name.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/pull/5668#issuecomment-261269190, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOX_YUS0yMODaakyCMKKkJIKKZMooLfks5q_GxWgaJpZM4K1aEj.\n", "Please make sure that whatever email is used in the CLA is associated with your github account (doesn't have to be primary, but is has to be one of the emails on your account, and also, that it is set as the email on the commits. @willnorris do you see the CLA? Or can I check this somehow?\n", "I didn\u2019t signed CLA and as I understand I don\u2019t have to. I am employee of CVisionLab company. Our CEO (Alexander) have recently signed CLA and added my email gubarev@cvisionlab.com to google group contrib@cvisionlab.com with list of employees allowed to contribute.\n\n> On 17 Nov 2016, at 22:47, Martin Wicke notifications@github.com wrote:\n> \n> Please make sure that whatever email is used in the CLA is associated with your github account (doesn't have to be primary, but is has to be one of the emails on your account, and also, that it is set as the email on the commits. @willnorris https://github.com/willnorris do you see the CLA? Or can I check this somehow?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub https://github.com/tensorflow/tensorflow/pull/5668#issuecomment-261349277, or mute the thread https://github.com/notifications/unsubscribe-auth/AAOX_Yn_dSGWD9XqJbAy2KtpteLaf3H_ks5q_K9kgaJpZM4K1aEj.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "The failing test is a flake. I'm pushing a fix for it.\n"]}, {"number": 5667, "title": "Documentation shows None default instead of actual op attr default", "body": "This is not a bug but a small mistake in the API:\r\nThe [documentation ](https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#assign) says \"use_locking\" defaults to _None_ in **tf.assign**.\r\nThat parameter is a boolean, so _None_ would not make a lot of sense. In any case, the source code makes it clear the default value is actually _False_ (file: tensorflow/tensorflow/python/ops/variables.py)\r\n\r\nI do not know if this is the case with other boolean default arguments in other functions, but I wanted to point it out.", "comments": ["@Antonio95 Yep, this is a problem with our doc generator.  The place that correctly says `False` is a different thing: it's a convenience method of `Variable` that calls down to the underlying op.\n\nThis would be great to fix, but we may not get to it soon.  Contributions would be very welcome!\n\nMore background: this documentation is for a Python wrapper automatically generated based on the C++ op definition.  The Python wrappers use `None` as defaults.  Is there a good reason for that, @josh11b (I vaguely recall having the discussion but don't remember the result)?  In any case, once the Python wrapper has a `None` default, the generated docs do too since they pull the defaults out of the Python function definition.\n", "I'd be very happy to help (I'm writing my final degree project on TF (python API)), if you tell me how to get started.\n", "Thank you!\n\nHow to proceed depends on whether @josh11b and @vrv think we should fix the Python wrapper generator or the doc generator.  Fixing the Python wrapper generator is more natural, but there may be a subtle reason for the `None`s that I do not recall.\n", "I was under the impression that there was an overhaul to our doc generation / website coming and that's why this wasn't being fixed ...  @martinwicke ?\n", "As soon as you make a decision I'll be happy to try and learn how to solve the issue", "To re-iterate what girving@ said:\r\n* The documentation you link to does not come from tensorflow/python/ops/variables.py, but from the generated code (gen_state_ops.py, imported by tensorflow/python/ops/state_ops.py).\r\n* I'm a bit foggy on the specifics of why we did generated the code the way we did (the documentation just comes from the generated code), but I can hazard a few guesses:\r\n - It avoid duplicating the places where we specify the default, reducing friction if we ever want to change the default (especially since sometimes people copy-paste the generated code as a starting point for a wrapper around it).\r\n - We'd have to write code to render the default as legal Python in the generated code, which was extra code we didn't really need for anything else.\r\n - There is a gotcha if the default value is considered mutable by Python, see e.g.: http://docs.quantifiedcode.com/python-anti-patterns/correctness/mutable_default_value_as_argument.html\r\n\r\nInstead we put the defaults in the doc string (search for the string \"Defaults to\").  We talked about various fixes (for example changing the rendering from \"arg=None\" to something else that indicates it is optional without specifying what the default is, perhaps \"[arg]\"), but no one solution made everybody happy so we left it as-is.  Another (tricky) possible solution: get the default out of the doc string, but that seems like it might be a bit fragile (e.g. could be disturbed by word wrapping, or a dot inside the default value).", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5666, "title": "about  this error when i try to run the existing wide_n_deep_tutorial.py", "body": "Hello,\r\nI get this error when i try to run the existing wide_n_deep_tutorial.py, any ideas on this?\r\n\r\n(tensorflow) xx@ubuntu:~$ python wide_n_deep_tutorial.py --model_type=wide_n_deep\r\n\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_tutorial.py\", line 208, in <module>\r\n    tf.app.run()\r\n  File \"/xxxxxxx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv))\r\n  File \"wide_n_deep_tutorial.py\", line 204, in main\r\n    train_and_eval()\r\n  File \"wide_n_deep_tutorial.py\", line 196, in train_and_eval\r\n    m = build_estimator(model_dir)\r\n  File \"wide_n_deep_tutorial.py\", line 80, in build_estimator\r\n    gender = tf.contrib.layers.sparse_column_with_hash_bucket(\r\nAttributeError: 'module' object has no attribute 'sparse_column_with_hash_bucket'\r\n\r\n\r\nI have no problems with TF installation, and my tensorflow version is 0.9 ,python 2.7  .Last week,the text_cnn.py  examples run just fine.\r\nThanks!\ufeff", "comments": ["@yidan216home TensorFlow 0.9 is very old, and probably doesn't have that function.  Is your copy of the `wide_n_deep_tutorial.py` also from 0.9?\n", "@girving Thanks! Works like charm!\nThe latest version is  tensorflow 0.11, but  fisrt   i used  0.7,then update to  0.8  and 0.9  according to the old operation on the Internet,the error  was still not solved.\n Now  i update to 0.11, it works.\n"]}, {"number": 5665, "title": "Is there a bug in embedding_attention_seq2seq?", "body": "Hi, all,\r\n\r\nI think the code of embedding_attention_seq2seq is confusing. And I can't run this code.\r\n\r\n```python\r\nif output_projection is None:\r\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n...\r\n\r\n      x = linear([inp] + attns, input_size, True)\r\n      # Run the RNN.\r\n      cell_output, state = cell(x, state)\r\n\r\n...\r\n\r\nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\r\n        output = linear([cell_output] + attns, output_size, True)\r\n\r\n```\r\n\r\nI don't know what's the meaning of \"AttnOutputProjection\". The attention information has been used in \"x = linear([inp] + attns, input_size, True)\", why adding it again?\r\n\r\nAnd meanwhile what is the meaning of \"[cell_output] + attns\", \"cell_output\" is equal with num_symbols, so why add attens?\r\n\r\nIn my experiment, the num_decoder_symbols is 10000 and the hidden size is 32, and I get a matrix 10032*10000 at \"AttnOutputProjection\". It will be out of memory.\r\n10032 is a confusing number, I don't know the physical meaning of this number.\r\n\r\nSo I don't know if it's a bug. If it's a bug, I'm really pleasure to make a PR.\r\nThanks so much.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["This question is better suited to StackOverflow, since it is a combination of asking how a model works and trying to run too large of a network.  Github issues are for bug reports and feature requests.\n", "Hi @girving ,\r\nI think this is a bug in the code and you did not understand @Syndrome777 's question correctly. The code here does not implement any attention mechanism I know. \r\n\r\n```\r\ncell_output, state = cell(x, state) # cell_output has the size of batch_size x num_symbols while the correct shape is batch_size x hidden_size \r\nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\r\n        output = linear([cell_output] + attns, output_size, True) # so the linear op here has the weight matrix of (num_symbols + attn_vec_size) x num_symbols. The correct size shoulde be (hidden_size + attn_vec_size) x num_symbols\r\n```\r\nThe output of the cell() must be the hidden state, not the projection to the vocabulary. I think this bug was created because the attention model code was derived from the normal seq2seq code in which the output of the decoder is projected directly to the vocabulary space.\r\n\r\nI quick fixed it by removing the OutputProjectWrapper in embedding_attention_seq2seq method. But this is not a good fix as it cannot fix the case where `output_projection is not None`. I hope for better fix from tensorflow community.\r\n```\r\n\r\noutput_size = None\r\n    if output_projection is None:\r\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L827\r\n\r\n\r\n", "@lukaszkaiser Can you comment?", "Indeed, there is a bug there, see #4938 for details (and how to work around it). We've decided not to correct it as the whole static seq2seq is going away anyway.", "As for the new version, see #4686 -- it's coming along (no attention at first, but we'll get there)."]}]