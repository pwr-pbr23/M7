[{"number": 10590, "title": "Saving embeddings or output evaluations in parallel to training", "body": "Feature request to create an async embedding saving buffer, which runs on the processor while the code trains on a GPU\r\n\r\nThe aim would be make sure that training is not affected by saving the model. That anyway takes a single thread, therefore it is a process that is done using an async buffer, it could optimize the training of models.\r\n------------------------\r\n\r\n### System information\r\n- Linux Ubuntu 16.04:\r\n- Installed from source with GPU implementation:\r\n- 1.0.1:\r\n- CUDA, cuDNN : 7.5\r\n- GPU: Nividia Titan X\r\n\r\nThere was a **57x** slowdown to in line saving. \r\n\r\n", "comments": ["You should be able to just spawn your own thread and run a saver op there.  I don't think this needs any TensorFlow changes, since multiple `session.run` calls already execute concurrently."]}, {"number": 10589, "title": "R1.1", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please clarify what you're intending with the change. Closing out for now."]}, {"number": 10588, "title": "Simplify getting variable by name", "body": "Suppose we created a variable `foo/v`\r\n```\r\nwith tf.variable_scope(\"foo\"):\r\n    v = tf.get_variable(\"v\", [1])\r\n```\r\nTo get this variable from its name one currently needs to write two lines:\r\n```\r\nwith tf.variable_scope(\"foo\", reuse=True):\r\n    v1 = tf.get_variable(\"v\")\r\n```\r\nInstead, it would be much more intuitive to just write:\r\n```\r\nv1 = tf.get_variable(\"foo/v\", reuse=True)\r\n```\r\n\r\n", "comments": ["You can do `v = sess.graph.get_tensor_by_name(\"foo/v:0\")`", "@Androbin : that's an interesting option I wasn't aware about, thanks! But it might still make sense to simplify the standard tf.get_variable call as suggested above.", "It should be possible to split the name and then place the scope as a hack.", "Seems like `tf.get_variable` should already do that:\r\n\r\n> This function prefixes the name with the current variable scope\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L1069", "@wielandbrendel Can you confirm that `tf.get_variable` accepts this `name` layout?", "@Androbin : Sorry, but could you explain in more detail?", "The documentation suggests that at least one of these should work:\r\n`v = tf.get_variable(\"foo/v\")`\r\n`v = tf.get_variable(\"foo/v:0\")`", "```\r\nwith tf.variable_scope(\"foo\"):\r\n    v = tf.get_variable(\"v\", [1])\r\n    \r\nv1 = tf.get_variable(\"foo/v\")\r\n```\r\nthrows a ValueError\r\n```\r\nValueError: Variable foo/v already exists, disallowed. \r\nDid you mean to set reuse=True in VarScope? \r\n```", "@wielandbrendel `get_variable` doesn't return old variables unless `reuse=True` in order to prevent subtle bugs.  It seems like the behavior you want is already available either via `reuse=True` or `sess.graph.get_tensor_by_name`.", "Get tensor by name gets a tensor, not a variable object.  It would be nice to add a reuse keyword to tf.get_variable so we could grab variables directly.  Obviously to avoid subtle bugs it should not be the default behavior."]}, {"number": 10587, "title": "TensorFlow is failing on ci.bazel.io with Bazel@HEAD", "body": "http://ci.bazel.io/job/TensorFlow/BAZEL_VERSION=HEAD,PLATFORM_NAME=linux-x86_64/870/console\r\n\r\n```\r\n==== bazel version ====\r\n...........\r\nBuild label: \r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Jun 8 20:25:30 2017 (1496953530)\r\nBuild timestamp: 1496953530\r\nBuild timestamp as int: 1496953530\r\n\r\n\r\n+ ./tensorflow/tools/ci_build/builds/configured CPU\r\n~/workspace/TensorFlow/BAZEL_VERSION/HEAD/PLATFORM_NAME/linux-x86_64 ~/workspace/TensorFlow/BAZEL_VERSION/HEAD/PLATFORM_NAME/linux-x86_64\r\nYou have bazel  installed.\r\nPlease upgrade your bazel installation to version 0.4.5 or higher to build TensorFlow!\r\nExiting...\r\n```\r\nThe reason is Bazel@HEAD doesn't generate a version number, the new way of version check introduced in 88d648f3beedfa6123efabb756be372f69382983 fails.\r\n\r\nMaybe skip the check if the version number is empty? Because this also means users cannot use their custom bazel to build TF unless they add `--embed_label ${version_number}` as a build option.\r\n\r\n", "comments": ["@martinwicke Can you take a look?", "I am OK with having the version check succeed if the version string returned is empty", "Agreed. @meteorcloudy do you want to fix the configure script, or @yifeif are you going to do it as part of pythonification?", "yea, but will send a fix for this now to unblock.", "Thanks for the fix!"]}, {"number": 10586, "title": "Minor fix typo", "body": "minor fix typo in `tf.contrib.learn`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10585, "title": "Run convert_graphdef_memmapped_format fail", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    OS X EI Caption 10.11.6\r\n- **TensorFlow installed from (source or binary)**:\r\n    binary (pip install)\r\n- **TensorFlow version (use command below)**:\r\n    TensorFlow 1.2.0-rc1 CPU Only\r\n- **Bazel version (if compiling from source)**:\r\n    Build label: 0.4.5-homebrew\r\n    Build target: bazel-out/local-\r\n    opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Thu Mar 16 13:37:54 2017 (1489671474)\r\n    Build timestamp: 1489671474\r\n    Build timestamp as int: 1489671474\r\n- **CUDA/cuDNN version**:\r\n    CPU Only\r\n\r\n### Describe the problem\r\nBecause the buffers holding the model weight values are 77MB in size, the memory needed to load these into the app can crash in Android, even before the model is run. So I want to run `convert_graphdef_memmapped_format` to map them into memory.So I build it.When I run it ,it get me a error`tensorflow/contrib/util/convert_graphdef_memmapped_format.cc:61] Unknown argument \u2013-in_graph=/Users/liba/Desktop/OptimizeCTNModel.pb\r\n`\r\n\r\n### Source code / logs\r\nlog:\r\n``` \r\nZHANGSH7-MP:tensorflow liba$ bazel-bin/tensorflow/contrib/util/convert_graphdef_memmapped_format \u2013-in_graph=/Users/liba/Desktop/OptimizeCTNModel.pb \u2013-out_graph=/Users/liba/Desktop/MemmappedCTNModel.pb\r\n2017-06-09 14:59:12.633589: E tensorflow/contrib/util/convert_graphdef_memmapped_format.cc:61] Unknown argument \u2013-in_graph=/Users/liba/Desktop/OptimizeCTNModel.pb\r\nusage: bazel-bin/tensorflow/contrib/util/convert_graphdef_memmapped_format\r\nFlags:\r\n\t--in_graph=\"\"                    \tstring\tinput graph\r\n\t--out_graph=\"\"                   \tstring\toutput graph\r\n\t--min_conversion_tensor_size=10000\tint32\tconstants with tensors that have less than this number elements won't be converted into ImmutableConst (be memmapped)\r\n\r\n```\r\n", "comments": ["@andrewharp Can you take a look?  The error does seem inconsistent with the usage information it spits out.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Any progress on this?. I am also facing this issue...."]}, {"number": 10584, "title": "Added uint8 registration for addition operation (Fixes #10447)", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The failures in tests/build seem unrelated to the changes made by this pull request", "Jenkins, test this please.", "I suspect that there is a bug in `tensorflow/python/kernel_tests/sparse_reshape_op_test.py`. This test seems to fail almost always even when the changes are completely unrelated #10673.", "I agree. It's unrelated."]}, {"number": 10583, "title": "fix iOS example README", "body": "fix iOS example README\r\n\r\nThe iOS example got moved to a different directory. This PR fixes that.\r\n\r\nTest plan:\r\n\r\n- [x] test it by following the readme to set up the new project for iOS.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "ping", "CLAs look good, thanks!\n\n<!-- ok -->", "ping", "@mhyttsten, does that look right to you?"]}, {"number": 10582, "title": "executor.cc:334 Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Tshape' not in Op", "body": "I got this error message when I used this setting inside TensorFlowImageListener.java\r\n\r\n```\r\n  private static final int NUM_CLASSES = 1001;\r\n  private static final int INPUT_SIZE = 299;\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128;\r\n  private static final String INPUT_NAME = \"Mul:0\";\r\n  private static final String OUTPUT_NAME = \"final_result:0\";\r\n```\r\n\r\nError message\r\n\r\n```\r\n06-09 09:09:49.880 27618-27659/my.xxxxx E/native: executor.cc:334 Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool_3, pool_3/_reshape/shape)\r\n                                                                                  \t [[Node: pool_3/_reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](pool_3, pool_3/_reshape/shape)]]\r\n                                                                                  \r\n                                                                                  [ 06-09 09:09:49.890 27618:27685 E/         ]\r\n                                                                                  [android_ws] Format: 5, Width: 1080, Height: 1620\r\n```\r\n\r\nMy machine setting\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin Mohammads-MacBook-Air.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 23 18:25:34 PDT 2016; root:xnu-3248.60.10~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 7.3.0 (clang-703.0.31)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Mohammads-MacBook-Air.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 23 18:25:34 PDT 2016; root:xnu-3248.60.10~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.1.0\r\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\nPlease advice. Thank you.", "comments": ["@asimshankar Any ideas?  The `Reshape` definitely has a `Tshape` attr, and has since 91ce95d497ec2957535b2ce6a965cd8269d723e5 which should precede 1.1.", "@datomnurdin : Could you describe in more detail how you built your Android application - in particular what state the git repository that you built the Android demo from? (The details of the machine are pointing to TensorFlow 1.1 as the version installed for Python, but if I understand correctly, the error you're getting is in the Android sample - I'm wondering what the source of that is).\r\n\r\nAlso, please share the full stacktrace if possible.\r\n\r\n(FYI @andrewharp )", "@asimshankar actually already change the whole demo source code. This is my repository, http://bit.ly/2r44xUk. \r\n\r\nTensorFlowImageListener.java\r\n\r\n```\r\n/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\npackage my.intellij.androidtensorflowbirdexample;\r\n\r\nimport android.content.res.AssetManager;\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.Bitmap.Config;\r\nimport android.graphics.Canvas;\r\nimport android.graphics.Matrix;\r\nimport android.media.Image;\r\nimport android.media.Image.Plane;\r\nimport android.media.ImageReader;\r\nimport android.media.ImageReader.OnImageAvailableListener;\r\nimport android.os.Handler;\r\nimport android.os.Trace;\r\n\r\nimport junit.framework.Assert;\r\n\r\nimport org.tensorflow.demo.TensorFlowClassifier;\r\nimport org.tensorflow.demo.env.ImageUtils;\r\nimport org.tensorflow.demo.env.Logger;\r\n\r\nimport java.util.List;\r\n\r\n/**\r\n * Class that takes in preview frames and converts the image to Bitmaps to process with Tensorflow.\r\n */\r\npublic class TensorFlowImageListener implements OnImageAvailableListener {\r\n  private static final Logger LOGGER = new Logger();\r\n\r\n  private static final boolean SAVE_PREVIEW_BITMAP = false;\r\n\r\n  // These are the settings for the original v1 Inception model. If you want to\r\n  // use a model that's been produced from the TensorFlow for Poets codelab,\r\n  // you'll need to set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128,\r\n  // INPUT_NAME = \"Mul:0\", and OUTPUT_NAME = \"final_result:0\".\r\n  // You'll also need to update the MODEL_FILE and LABEL_FILE paths to point to\r\n  // the ones you produced.\r\n  private static final int NUM_CLASSES = 1001;\r\n  private static final int INPUT_SIZE = 299;\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128;\r\n  private static final String INPUT_NAME = \"Mul:0\";\r\n  private static final String OUTPUT_NAME = \"final_result:0\";\r\n\r\n  private static final String MODEL_FILE = \"file:///android_asset/tensorflow_inception_graph.pb\";\r\n  private static final String LABEL_FILE =\r\n      \"file:///android_asset/imagenet_comp_graph_label_strings.txt\";\r\n\r\n  private Integer sensorOrientation;\r\n\r\n  private final TensorFlowClassifier tensorflow = new TensorFlowClassifier();\r\n\r\n  private int previewWidth = 0;\r\n  private int previewHeight = 0;\r\n  private byte[][] yuvBytes;\r\n  private int[] rgbBytes = null;\r\n  private Bitmap rgbFrameBitmap = null;\r\n  private Bitmap croppedBitmap = null;\r\n\r\n  private boolean computing = false;\r\n  private Handler handler;\r\n\r\n  private RecognitionScoreView scoreView;\r\n\r\n  public void initialize(\r\n      final AssetManager assetManager,\r\n      final RecognitionScoreView scoreView,\r\n      final Handler handler,\r\n      final Integer sensorOrientation) {\r\n    Assert.assertNotNull(sensorOrientation);\r\n    tensorflow.initializeTensorFlow(\r\n        assetManager, MODEL_FILE, LABEL_FILE, NUM_CLASSES, INPUT_SIZE, IMAGE_MEAN, IMAGE_STD,\r\n        INPUT_NAME, OUTPUT_NAME);\r\n    this.scoreView = scoreView;\r\n    this.handler = handler;\r\n    this.sensorOrientation = sensorOrientation;\r\n  }\r\n\r\n  private void drawResizedBitmap(final Bitmap src, final Bitmap dst) {\r\n    Assert.assertEquals(dst.getWidth(), dst.getHeight());\r\n    final float minDim = Math.min(src.getWidth(), src.getHeight());\r\n\r\n    final Matrix matrix = new Matrix();\r\n\r\n    // We only want the center square out of the original rectangle.\r\n    final float translateX = -Math.max(0, (src.getWidth() - minDim) / 2);\r\n    final float translateY = -Math.max(0, (src.getHeight() - minDim) / 2);\r\n    matrix.preTranslate(translateX, translateY);\r\n\r\n    final float scaleFactor = dst.getHeight() / minDim;\r\n    matrix.postScale(scaleFactor, scaleFactor);\r\n\r\n    // Rotate around the center if necessary.\r\n    if (sensorOrientation != 0) {\r\n      matrix.postTranslate(-dst.getWidth() / 2.0f, -dst.getHeight() / 2.0f);\r\n      matrix.postRotate(sensorOrientation);\r\n      matrix.postTranslate(dst.getWidth() / 2.0f, dst.getHeight() / 2.0f);\r\n    }\r\n\r\n    final Canvas canvas = new Canvas(dst);\r\n    canvas.drawBitmap(src, matrix, null);\r\n  }\r\n\r\n  @Override\r\n  public void onImageAvailable(final ImageReader reader) {\r\n    Image image = null;\r\n    try {\r\n      image = reader.acquireLatestImage();\r\n\r\n      if (image == null) {\r\n        return;\r\n      }\r\n\r\n      // No mutex needed as this method is not reentrant.\r\n      if (computing) {\r\n        image.close();\r\n        return;\r\n      }\r\n      computing = true;\r\n\r\n      Trace.beginSection(\"imageAvailable\");\r\n\r\n      final Plane[] planes = image.getPlanes();\r\n\r\n      // Initialize the storage bitmaps once when the resolution is known.\r\n      if (previewWidth != image.getWidth() || previewHeight != image.getHeight()) {\r\n        previewWidth = image.getWidth();\r\n        previewHeight = image.getHeight();\r\n\r\n        LOGGER.i(\"Initializing at size %dx%d\", previewWidth, previewHeight);\r\n        rgbBytes = new int[previewWidth * previewHeight];\r\n        rgbFrameBitmap = Bitmap.createBitmap(previewWidth, previewHeight, Config.ARGB_8888);\r\n        croppedBitmap = Bitmap.createBitmap(INPUT_SIZE, INPUT_SIZE, Config.ARGB_8888);\r\n\r\n        yuvBytes = new byte[planes.length][];\r\n        for (int i = 0; i < planes.length; ++i) {\r\n          yuvBytes[i] = new byte[planes[i].getBuffer().capacity()];\r\n        }\r\n      }\r\n\r\n      for (int i = 0; i < planes.length; ++i) {\r\n        planes[i].getBuffer().get(yuvBytes[i]);\r\n      }\r\n\r\n      final int yRowStride = planes[0].getRowStride();\r\n      final int uvRowStride = planes[1].getRowStride();\r\n      final int uvPixelStride = planes[1].getPixelStride();\r\n      ImageUtils.convertYUV420ToARGB8888(\r\n          yuvBytes[0],\r\n          yuvBytes[1],\r\n          yuvBytes[2],\r\n          rgbBytes,\r\n          previewWidth,\r\n          previewHeight,\r\n          yRowStride,\r\n          uvRowStride,\r\n          uvPixelStride,\r\n          false);\r\n\r\n      image.close();\r\n    } catch (final Exception e) {\r\n      if (image != null) {\r\n        image.close();\r\n      }\r\n      LOGGER.e(e, \"Exception!\");\r\n      Trace.endSection();\r\n      return;\r\n    }\r\n\r\n    rgbFrameBitmap.setPixels(rgbBytes, 0, previewWidth, 0, 0, previewWidth, previewHeight);\r\n    drawResizedBitmap(rgbFrameBitmap, croppedBitmap);\r\n\r\n    // For examining the actual TF input.\r\n    if (SAVE_PREVIEW_BITMAP) {\r\n      ImageUtils.saveBitmap(croppedBitmap);\r\n    }\r\n\r\n    handler.post(\r\n        new Runnable() {\r\n          @Override\r\n          public void run() {\r\n            final List<Classifier.Recognition> results = tensorflow.recognizeImage(croppedBitmap);\r\n\r\n            LOGGER.v(\"%d results\", results.size());\r\n            for (final Classifier.Recognition result : results) {\r\n              LOGGER.v(\"Result: \" + result.getTitle());\r\n            }\r\n            scoreView.setResults(results);\r\n            computing = false;\r\n          }\r\n        });\r\n\r\n    Trace.endSection();\r\n  }\r\n}\r\n```\r\n\r\nPlease advice.", "@datomnurdin : I was looking for more detail on the error message.\r\n\r\nAlso, it seems you're including some binary files there (e.g., https://github.com/datomnurdin/AndroidTensorFlowBirdExample/tree/master/app/libs/armeabi-v7a)\r\nWhat is the source of those files?\r\n\r\n(The error message suggests that the TensorFlow native code that you're using might be too old - so I'm trying to determine how you got that native code).\r\n\r\nIdeally, you shouldn't have to include the binary `.so` files in your repository but can instead pick the TensorFlow runtime from jcenter. See:  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android\r\n\r\nI would recommend removing the native libraries from your repository and using the release binaries from jcenter (by adding to your `build.gradle` files) as mentioned in that README.\r\n\r\nLet us know how that goes.\r\n", "@asimshankar btw the .so file I compile it myself on my machine using this jni library, https://github.com/datomnurdin/AndroidTensorFlowBirdExample/tree/master/app/src/main/jni.", "I'm sorry, I don't quite follow how exactly the shared library came into being - what version of the TensorFlow sources were used, how it was compiled etc. Without that information, it's hard to determine what's going on.\r\n\r\nIs it possible for you to use the recommended process mentioned in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android instead of building your own JNI library? Or if you're basing this off the example  - then see these instructions: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#building-in-android-studio-using-the-tensorflow-aar-from-jcenter\r\n", "I already used your methods to generate **libtensorflow_demo.so** library but it gave same error message.", "@datomnurdin : Detailed instructions to reproduce the problem would help, I'm having trouble doing so. That said, it does seem your code is based on a very old version of the demo (`TensorFlowImageListener.java` was removed in November last year, before TensorFlow 1.0 - https://github.com/tensorflow/tensorflow/commit/4930f42dde4c02e034eccd66500a0ddede7ae499) - please try with an updated version.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10581, "title": "Run quantize_graph error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    OS X EI Caption 10.11.6\r\n- **TensorFlow installed from (source or binary)**:\r\n    binary (pip install)\r\n- **TensorFlow version (use command below)**:\r\n    TensorFlow 1.2.0-rc1 CPU Only\r\n- **Bazel version (if compiling from source)**:\r\n    Build label: 0.4.5-homebrew\r\n    Build target: bazel-out/local-\r\n    opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Thu Mar 16 13:37:54 2017 (1489671474)\r\n    Build timestamp: 1489671474\r\n    Build timestamp as int: 1489671474\r\n- **CUDA/cuDNN version**:\r\n    CPU Only\r\n\r\n### Describe the problem\r\nBecause of the buffers holding the model weight values are 77MB in size, the memory needed to load these into the app can put a lot of pressure on RAM in Android even before the model is run.  \r\nSo, I need to further compress the model and round the weights,Even at the expense of accuracy.  \r\nSo I build `/tensorflow/tools/quantization/quantize_graph`.There are something wrongs when I run `quantize_graph` .\r\n### Source code / logs\r\nSource code:\r\n``` \r\nbazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n\u2013input=/Users/liba/Desktop/OptimizeCTNModel.pb \\\r\n\u2013output=/Users/liba/Desktop/RoundedCTNModel.pb \\ \r\n\u2013output_node_names=output/outputs \\\r\n\u2013mode=weights_rounded\r\n```\r\n\r\nlog:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1301, in <module>\r\n    app.run()\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1267, in main\r\n    data = f.read()\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 125, in read\r\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))\r\n  File \"/Users/liba/anaconda/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: .\r\n\r\n```\r\n\r\n", "comments": ["This `quantize_graph` tool is outdated. You may try the new `transform_graph` tool:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#eight-bit-calculations", "@petewarden This seems like a bad error message even if the tool is deprecated.  Could you take a look?", "@Androbin  I try.But it have a error.\r\nlog:\r\n``` \r\nZHANGSH7-MP:tensorflow liba$ bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph =/Users/liba/Desktop/OptimizeCTNModel.pb --out_graph =/Users/liba/DesktoRouddndedOptimizeCTodel.pb  --inputs = 'inputs/X' --outputs = 'output/outputs' --transforms = 'strip_unused_nodes\uff08type = float\uff0cshape =\u201c1,256,256,1\u201d\uff09fold_constants\uff08ignore_errors = true) fold_batch_norms fold_old_batch_norms round_weights\uff08num_steps = 256\uff09'\r\n2017-06-12 10:36:01.153946: E tensorflow/tools/graph_transforms/transform_graph.cc:164] Unknown argument --in_graph.\r\nusage: bazel-bin/tensorflow/tools/graph_transforms/transform_graph\r\nFlags:\r\n\t--in_graph=\"\"                    \tstring\tinput graph file name\r\n\t--out_graph=\"\"                   \tstring\toutput graph file name\r\n\t--inputs=\"\"                      \tstring\tinputs\r\n\t--outputs=\"\"                     \tstring\toutputs\r\n\t--transforms=\"\"                  \tstring\tlist of transforms\r\n\t--output_as_text=false           \tbool\twhether to write the graph in text protobuf format\r\n\r\nTransforms are:\r\nadd_default_attributes\r\nbackport_concatv2\r\nbackport_tensor_array_v3\r\nfold_batch_norms\r\nfold_constants\r\nfold_old_batch_norms\r\nfreeze_requantization_ranges\r\nfuse_pad_and_conv\r\nfuse_resize_and_conv\r\nfuse_resize_pad_and_conv\r\ninsert_logging\r\nmerge_duplicate_nodes\r\nobfuscate_names\r\nquantize_nodes\r\nquantize_weights\r\nremove_attribute\r\nremove_device\r\nremove_nodes\r\nrename_attribute\r\nrename_op\r\nrewrite_quantized_stripped_model_for_hexagon\r\nround_weights\r\nset_device\r\nsort_by_execution_order\r\nsparsify_gather\r\nstrip_unused_nodes\r\n```", "It can't work!Thanks.", "Hello,\r\n@JcmeLs Can you please tell me the solution you found of the above problem?\r\n"]}, {"number": 10580, "title": "contrib.keras TypeError with HDF5Matrix and validation_split", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: v8.0/v5.1\r\n- **GPU model and memory**: NVIDIA M4000, 8GB\r\n- **Exact command to reproduce**: https://gist.github.com/droidicus/4a55c83e522d90b103b81bf5fb63e610\r\n\r\n### Describe the problem\r\nWhen using tf.contrib.keras.HDF5Matrix as an input to model.fit, a TypeError is thrown if validation_split is used. If no validation_split is used then the model.fit command proceeds as expected.\r\n\r\n### Source code / logs\r\nA gist of a minimal example is available here: https://gist.github.com/droidicus/4a55c83e522d90b103b81bf5fb63e610\r\n\r\nThe trace of the error is as follows:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-bdaf6d0caaa7> in <module>()\r\n     13 # This call to fit uses a validation_split, and causes:\r\n     14 # TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n---> 15 model.fit(X_train, y_train, validation_split=0.1, shuffle='batch')\r\n \r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_rc\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\r\n    842         class_weight=class_weight,\r\n    843         sample_weight=sample_weight,\r\n--> 844         initial_epoch=initial_epoch)\r\n    845\r\n    846   def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\r\n \r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_rc\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\r\n   1436       do_validation = True\r\n   1437       split_at = int(len(x[0]) * (1. - validation_split))\r\n-> 1438       x, val_x = (_slice_arrays(x, 0, split_at), _slice_arrays(x, split_at))\r\n   1439       y, val_y = (_slice_arrays(y, 0, split_at), _slice_arrays(y, split_at))\r\n   1440       sample_weights, val_sample_weights = (_slice_arrays(\r\n \r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_rc\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py in _slice_arrays(arrays, start, stop)\r\n    395       return [x[start] for x in arrays]\r\n    396     else:\r\n--> 397       return [x[start:stop] for x in arrays]\r\n    398   else:\r\n    399     if hasattr(start, '__len__'):\r\n \r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_rc\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py in <listcomp>(.0)\r\n    395       return [x[start] for x in arrays]\r\n    396     else:\r\n--> 397       return [x[start:stop] for x in arrays]\r\n    398   else:\r\n    399     if hasattr(start, '__len__'):\r\n \r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_rc\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\utils\\io_utils.py in __getitem__(self, key)\r\n     81   def __getitem__(self, key):\r\n     82     if isinstance(key, slice):\r\n---> 83       if key.stop + self.start <= self.end:\r\n     84         idx = slice(key.start + self.start, key.stop + self.start)\r\n     85       else:\r\n \r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n```", "comments": ["The provided script runs without any error messages on linux using tensorflow compiled from source using the code from latest commit (e9de087fa) but fails when I run using the code from about 8 days ago. It seems that the issue has already been fixed. You can try building tensorflow from source.", "Good to know it has been fixed. Unfortunately I do not have the dev tools for windows set up currently (it is a lot less simple than under Linux, and not currently supported for end users), so I cannot build from source.\r\n\r\nAny chance this can be cherrypicked for the v1.2.0 release?", "The fix appears to be this commit to io_utils.py: https://github.com/tensorflow/tensorflow/commit/d21bf7d7502f447e5f967a479282b32b5845ba8b#diff-cffdb24185e822dcdc7ae20dcc4c7879\r\n", "This has been fixed previously."]}, {"number": 10579, "title": "Sort out confusion around usage of selective registration", "body": "`--config=android_arm` is defined in [`tensorflow/BUILD`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD)\r\nwhere it resolved to `--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a`\r\nseems like this cannot be referenced here", "comments": ["Can one of the admins verify this patch?", "Don't merge this yet!\r\n`--config=android_arm` gives `WARNING: Config values are not defined in any .rc file: android_arm`\r\n`--crosstool_top=//tensorflow/contrib/android:crosstool --cpu=armeabi-v7a` gives\r\n```\r\nERROR: /home/androbin/.cache/bazel/_bazel_androbin/0614ddb178f91f25438b47fee5f001b7/external/protobuf/BUILD:113:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command \r\n  (cd /home/androbin/.cache/bazel/_bazel_androbin/0614ddb178f91f25438b47fee5f001b7/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3.5 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=6 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /bin/false -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION -MD -MF bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/dynamic_message.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/dynamic_message.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/dynamic_message.cc -o bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/dynamic_message.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\n```", "Now, I am a bit confused about all the flags ...", "Andrew, could you take a look at this and comment on the flags?", "@andrewharp any luck?", "It seems like some tools related to building the inference library are kinda messing up some configs. Building it the standard way only worked for me by omitting `host_crosstool_top` but keeping `crosstool_top`.\r\n\r\nAt least somee of it seems to be due to BUILD files not in expected location and/or invalid toolchain names.\r\n\r\n```\r\nERROR: No toolchain found for cpu 'armeabi-v7a'. Valid cpus are: [\r\n  k8,\r\n  piii,\r\n  arm,\r\n  darwin,\r\n  ppc,\r\n].\r\n```\r\n\r\nThe next best command that manages that builds with selective registration:\r\n```\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n--copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n//tensorflow/contrib/android:libtensorflow_inference.so \\\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--cpu=arm\r\n```", "The nightly build of `libtensorflow_inference.so` for `armeabi-v7a` is 9.7 MB and has issue #8897.\r\nThe builds with `-DTENSORFLOW_DISABLE_META` and/or `-DSELECTIVE_REGISTRATION` are 120 MB - 180 MB and are not found by `System.loadLibrary(\"tensorflow_inference\")` even under the exact same conditions.", "Context for this is #10299, building the inference library yourself and doing selective registration is really nasty and hard to do without opening X new issues. Why is such a central tool so error-prone and poorly documented? Most should be covered if the various prepared bazel commands would just work.", "@tensorflow-jenkins test this please", "@vrv What about the erroneous flag?", "@vrv Could you replace the command by\r\n```\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n--copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n//tensorflow/contrib/android:libtensorflow_inference.so \\\r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n--cpu=arm\r\n```", "@Androbin I'm afraid I have no idea how this system works -- @andrewharp do you understand? \r\n\r\n@Androbin should I revert this PR, did I merge by mistake?", "Which ARM level does --cpu=arm target? Almost all Android devices out there should be v7 or above, so that seems like a reasonable default to leave it at.", "@vrv It seems like neither the previous nor the current version works. The only command that runs without error is the last one I posted here. This may be committed for now instead.\r\n\r\n@andrewharp Most scripts I have seen here do indeed use that without further note. And according to some BUILD files, `arm` should just resolve to `armeabi-v7a`"]}, {"number": 10578, "title": "Testing branch 1.2", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 10577, "title": "Fix typo in bazel command", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10576, "title": "OOM error after some number of steps ( Inputsize remains constant)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom Code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux version 3.10.0-229.11.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) )\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled from sources\r\n\r\n- **TensorFlow version (use command below)**:\r\n'v1.1.0-rc2-1003-g3792dd9' 1.1.0-rc2\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\n  CUDA 8.0\r\n: CuDNN 5.1\r\n- **GPU model and memory**:\r\n: NVidia GTX 1080 (Pascal)\r\n\r\n### Describe the problem\r\nWhile running training on a six layer CNN on 2 GPUs, the training starts fine, but after about 450 iterations, the OOM error comes up. During the whole time, the input size remains the same `[2, 221, 221, 3]` . I am unable to find any anomaly in the tensorboard or in my data.  Complete output and error log below\r\n\r\n### Source code / logs\r\nattached with the post\r\n\r\n[stderr.txt](https://github.com/tensorflow/tensorflow/files/1062484/stderr.txt)\r\n[stdout.txt](https://github.com/tensorflow/tensorflow/files/1062485/stdout.txt)\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10575, "title": "Testing 1.2 branch.", "body": "", "comments": []}, {"number": 10574, "title": "Changed default `evalution_master` to also use cluster information", "body": "When using the standard Estimator during evaluation in distributed setting (a `cluster_spec` was supplied) I encountered the following error message:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation \r\n'save/RestoreV2_28': Operation was explicitly assigned to \r\n/job:ps/replica:0/task:0/device:CPU:0 but available devices are\r\n[ /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0 ]. \r\nMake sure the device specification refers to a valid device.\r\n```\r\n```\r\n[[Node: save/RestoreV2_28 = RestoreV2[dtypes=[DT_FLOAT], \r\n_device=\"/job:ps/replica:0/task:0/device:CPU:0\"](save/Const, save/RestoreV2_28/tensor_names, \r\nsave/ResyoreV2_28/shape_and_slices)]]\r\n```\r\nIn short:\r\nThe evaluation graph is looking for Variables on ` /job:localhost` but they are on ` /job:ps`, as is intended. \r\n\r\nTo fix this I changed the default settings of placing the `evaluation_master` on `localhost` to the same as for `master`. This means that when instancing a RunConfig with no `evaluation_master` given it will check if there is a `cluster_spec`. If so it will get the `evaluation_master` based on the `cluster_spec`. \r\n\r\nThis fixes the problem for me and leads to correct behaviour.", "comments": ["Can one of the admins verify this patch?", "Sorry, I cannot approve this pull request. \r\n\r\nBy design, the evaluation will be executed based on the model parameters from checkpoint. This ensures that the whole evaluation, which could be multiple steps, has a consistent snapshot/view of the model parameters and will not change (and also avoid the race condition) if there is a training happening in parallel (so, ps gets updated). \r\n\r\nThis is why evaluation_master is '' by default. And in most of the use cases, this is a only correct behavior. If there is any cutting edge research exploring new evaluation approach or user strongly wants the customized evaluation_master to be the same as master (though I am not fully convinced this is correct), user can set the evaluation_master explicitly. The default setting should not be changed.\r\n", "All right, thanks! I did not think about it this way."]}, {"number": 10573, "title": "Partly revert #10533", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10572, "title": "Various Bash Improvements", "body": "Sorry about the previous spamming.\r\nI didn't expect it to sum up that quickly.", "comments": ["Can one of the admins verify this patch?", "Thanks for merging!\r\n\r\nJenkins, test this please", "Jenkins, test this please.", "Jenkins, test this please"]}, {"number": 10571, "title": "Remove an unused typedef", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 10570, "title": "Branch 158391996", "body": "", "comments": ["Jenkins, test this please"]}, {"number": 10569, "title": "[Bash] Move multiple parameters out of shebang", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2096", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10568, "title": "[Bash] read with -r to not mangle backslashes", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2162", "comments": ["Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10567, "title": "[Bash] Use $(...) instead of legacy `...`", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2006", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10566, "title": "[Bash] Use $(...) instead of legacy `...`", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2006", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Please feel free to pack these changes into one Pull request.\r\nAs they are mostly one liners, reviewing them once rather than going through a few pull requests would be much easier."]}, {"number": 10565, "title": "[Bash] Use $(...) instead of legacy `...`", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2006", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10564, "title": "[Bash] Use cd ... || exit in case cd fails", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2164", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10563, "title": "Fix AttributeError in resnet.py", "body": "There is no function tf.softmax() in Tensorflow 1.x.\r\n\r\nWhen running the old code, Python interpreter complains:\r\n\r\nFile \"resnet.py\", line 152, in res_net_model\r\nprediction, loss = res_net(x, y)\r\nFile \"resnet.py\", line 148, in res_net\r\nreturn tf.softmax(logits), loss\r\nAttributeError: 'module' object has no attribute 'softmax'", "comments": ["Can one of the admins verify this patch?", "I submitted a pull request once but cla did not works:\r\nhttps://github.com/tensorflow/tensorflow/pull/10465\r\n\r\nSo I request again and this time cla works.\r\n\r\nThank for your time!", "@tensorflow-jenkins test this please"]}, {"number": 10562, "title": "[Bash] Moving multiple parameters out of shebang", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2096", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10561, "title": "[Bash] Fix misspellings of \"architecture\"", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins test this please."]}]