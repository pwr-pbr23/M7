[{"number": 2834, "title": "Defining shape of 1-D variable-size placeholder with tuple doesn't enforce the actual shape", "body": "I noticed that defining a 1-D batch-sized placeholder with a tuple instead of a list doesn't enforce its shape:\n\n```\nph = tf.placeholder(tf.int32, (None))\nop = 2 * ph\nx = np.random.randint(0, 9, (5, 5))\nsess.run(op, feed_dict={ph: x})\n```\n\nThis outputs:\n\n```\narray([[ 0, 10, 10, 12,  4],\n       [12, 12, 16,  2,  2],\n       [ 4, 14, 10, 14,  6],\n       [ 6,  8,  2, 10, 10],\n       [ 4, 14,  6,  6,  4]], dtype=int32)\n```\n\nWhile with `[None]`:\n\n```\nph = tf.placeholder(tf.int32, [None])\nop = 2 * ph\nsess.run(op, feed_dict={ph: x})\n```\n\nresults in `ValueError: Cannot feed value of shape (5, 5) for Tensor u'Placeholder_5:0', which has shape '(?,)'`, as I would expect.\n\nIs the behavior with the tuple intended to work like this? I really don't think it's a good idea (I spent a few hours tracking a bug in my code caused by feeding a 2-D array to a supposedly 1-D placeholder). Anyway, the documentation on placeholders says the following about the argument `shape`: _The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape_.\n", "comments": ["The problem is that `(None)` evaluates to `None`, and not a `tuple` containing a single element that is `None`. To define a single element tuple, you need to type `(None,)`. \n\nThis is due to Python's syntax, not the placeholder implementation:\n\n``` python\n>>> print type(None)\n<type 'NoneType'>\n>>> print type((None))\n<type 'NoneType'>\n>>> print type((None,))\n<type 'tuple'>\n>>> print type([None])\n<type 'list'>\n>>> print type([None,])\n<type 'list'>\n```\n"]}, {"number": 2833, "title": "Second try at testing removal of gtest flag", "body": "", "comments": []}, {"number": 2832, "title": "Branch 124731683", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Ignoring CLA, all contributions came from internal.\n"]}, {"number": 2831, "title": "Fixes to TensorArray and Functional ops", "body": "Fixes #2787.\n", "comments": []}, {"number": 2830, "title": "syntaxnet bazel test failed", "body": "I ran `bazel test syntaxnet/... util/utf8/...` and it gave a few errors . It gave me this output:\n\n```\nFAIL: //syntaxnet:parser_trainer_test (see /home/me/.cache/bazel/_bazel_rushat/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log).\nINFO: Elapsed time: 2179.396s, Critical Path: 1623.00s\n//syntaxnet:arc_standard_transitions_test                                PASSED in 0.7s\n//syntaxnet:beam_reader_ops_test                                         PASSED in 24.1s\n//syntaxnet:graph_builder_test                                           PASSED in 14.6s\n//syntaxnet:lexicon_builder_test                                         PASSED in 6.1s\n//syntaxnet:parser_features_test                                         PASSED in 5.8s\n//syntaxnet:reader_ops_test                                              PASSED in 9.4s\n//syntaxnet:sentence_features_test                                       PASSED in 0.2s\n//syntaxnet:shared_store_test                                            PASSED in 41.7s\n//syntaxnet:tagger_transitions_test                                      PASSED in 5.2s\n//syntaxnet:text_formats_test                                            PASSED in 6.1s\n//util/utf8:unicodetext_unittest                                         PASSED in 0.4s\n//syntaxnet:parser_trainer_test                                          FAILED in 0.5s\n  /home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log\n\nExecuted 12 out of 12 tests: 11 tests pass and 1 fails locally.\nThere were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\n```\n\nIf you want the output of `--test_verbose_timeout_warnings` then please ask\n\nI have no idea what these mean...please help me :pray:\n\nPS: If this is the wrong place to ask, then please direct me where to post this(but kindly suggest an answer)\n", "comments": ["Could you please attach the log?\n\n/home/me/.cache/bazel/_bazel_rushat/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log\n", "This is probably better filed at tensorflow/models, since this is a different repo.\n", "@sherrym Output of`/home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/testlogs/syntaxnet/parser_trainer_test/test.log`\n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\n+ BINDIR=/home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet\n+ CONTEXT=/home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/testdata/context.pbtxt\n+ TMP_DIR=/tmp/syntaxnet-output\n+ mkdir -p /tmp/syntaxnet-output\n+ sed s=OUTPATH=/tmp/syntaxnet-output=\n+ sed s=SRCDIR=/home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles= /home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/testdata/context.pbtxt\nsed: can't read /home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/testdata/context.pbtxt: No such file or directory\n+ PARAMS=128-0.08-3600-0.9-0\n+ /home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/parser_trainer --arg_prefix=brain_parser --batch_size=32 --compute_lexicon --decay_steps=3600 --graph_builder=greedy --hidden_layer_sizes=128 --learning_rate=0.08 --momentum=0.9 --output_path=/tmp/syntaxnet-output --task_context=/tmp/syntaxnet-output/context --training_corpus=training-corpus --tuning_corpus=tuning-corpus --params=128-0.08-3600-0.9-0 --num_epochs=12 --report_every=100 --checkpoint_every=1000 --logtostderr\nsyntaxnet/parser_trainer_test: line 36: /home/me/.cache/bazel/_bazel_me/cc4d67663fbe887a603385d628fdf383/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/parser_trainer: No such file or directory\n\n```\n", "any updates on this. I am getting a similar error : \n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\n+ BINDIR=/home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/bazel-out/local-opt/bin/syntaxnet/p\narser_trainer_test.runfiles/syntaxnet\n+ CONTEXT=/home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/bazel-out/local-opt/bin/syntaxnet/\nparser_trainer_test.runfiles/syntaxnet/testdata/context.pbtxt\n+ TMP_DIR=/tmp/syntaxnet-output\n+ mkdir -p /tmp/syntaxnet-output\n+ sed s=SRCDIR=/home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/bazel-out/local-opt/bin/synta\nxnet/parser_trainer_test.runfiles= /home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/bazel-out\n/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/testdata/context.pbtxt\n+ sed s=OUTPATH=/tmp/syntaxnet-output=\nsed: can't read /home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/bazel-out/local-opt/bin/synt\naxnet/parser_trainer_test.runfiles/syntaxnet/testdata/context.pbtxt: No such file or directory\n+ PARAMS=128-0.08-3600-0.9-0\n+ /home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/bazel-out/local-opt/bin/syntaxnet/parser_t\nrainer_test.runfiles/syntaxnet/parser_trainer --arg_prefix=brain_parser --batch_size=32 --compute_lexicon --decay_steps=3600 --graph_builder=greedy --hidden_layer\n_sizes=128 --learning_rate=0.08 --momentum=0.9 --output_path=/tmp/syntaxnet-output --task_context=/tmp/syntaxnet-output/context --training_corpus=training-corpus \n--tuning_corpus=tuning-corpus --params=128-0.08-3600-0.9-0 --num_epochs=12 --report_every=100 --checkpoint_every=1000 --logtostderr\nsyntaxnet/parser_trainer_test: line 36: /home/evolveconsulting_an/.cache/bazel/_bazel_evolveconsulting_an/4551312cf21d769fe1be4c588e97e362/execroot/syntaxnet/baze\nl-out/local-opt/bin/syntaxnet/parser_trainer_test.runfiles/syntaxnet/parser_trainer: No such file or directory\n```\n\nseems like it can't read some files \n", "@vrv please reopen as another person has same issue\n", "@skyprince999 Follow this [here](http://stackoverflow.com/questions/37804158/syntaxnet-bazel-test-failed) and [here](https://github.com/tensorflow/models/issues/196). Please tell me if you have found an answer!!\n", "@machomachopadre no, I am still confounded!!! However I have been able to run the Dockerfile and build it using the docker service. \n", "I was also facing the same issue. The problem was with bazel version that, I have been using.\nhttps://github.com/bazelbuild/bazel/releases  <- Download bazel-0.2.2.deb for you configuration.\nrun it using `sudo dpkg -i filename.deb`. \nCheck for the bazel version by typing: `bazel version` , it must say 0.2.2.\nRest follow the tutorials. You must be good to go.\n", "@prakhar21 my bazel version is 0.2.3, but tensorflow(not syntaxnet) had built normally and is functioning\n", "@skyprince999 got some vague response [here](http://stackoverflow.com/questions/37804158/syntaxnet-bazel-test-failed)(by kristina)\n", "@machomachopadre syntaxtnet doesn't work with bazel 0.2.3 check the installation instructions over [here](https://github.com/tensorflow/models/tree/master/syntaxnet)\nThanks @prakhar21 for pointing it out. I had the latest build installed on my VM. uninstalled it & loaded the 0.2.2b version. works fine now!\n", "@skyprince999 How did you uninstall your previous bazel version? I tried `rm -fr .cache/bazel` following recommendation [here](http://stackoverflow.com/questions/37804158/syntaxnet-bazel-test-failed#comment63168021_37845800), but it didn't work\n", "Uninstalling bazel seems to be a pain! My installation was at that location, you could try `rm -f ~/.bazel  ~/.bazelrc` \nYou can check the treads over [here](https://github.com/bazelbuild/bazel/issues/838) & [here](https://github.com/bazelbuild/bazel/issues/962)  \n", "Successfully uninstalled! Syntaxnet working!!\n"]}, {"number": 2829, "title": "Can not completely disable GPU & CUDA support. ", "body": "This only applies to `TensorFlow 0.9`.\n\nEven if you disable GPU support when configuring `TensorFlow`, you will still need CUDA to build it. Indeed, `Eigen::half` (half floats) should only be used when running on a GPU, it's not CPU-compatible. \n\nBut they are always used, and this causes exceptions to be thrown if our `Eigen`version does not contains the `Eigen/src/arch/CUDA` folder and headers. (=You don't have CUDA installed).\n\nThanks for fixing this as fast as possible. If you need some other infos, just ask me.\n", "comments": ["@MrSlayer02 Eigen::half runs on both CPU and GPU. The code resides in a directory called CUDA purely for historical reasons. Moreover tensorflow 0.9 pulls a recent version of Eigen that contains the CUDA directory whether or not TensorFlow is compiled with CUDA support.\n\nCan you document what you're trying to do as well as the symptoms that you experience ?\n", "I was trying to install `TensorFlow 0.9` on a Raspberry Pi 3, but I got an error that told me g++ could not `static_cast` from `Eigen::half` to `float`. The header Half.h is not there in the pulled eigen version. \nActually it seems TensorFlow declares `struct half` in `namespace Eigen` (header `strcat.h`) but does not instantiates it. Maybe this is normally done in Half.h...\n\nI also tried to pull eigen myself and change the `#include` path, but it didn't changed anything.\n", "This could be because the makefile used to compile TensorFlow on Raspberry Pi isn't up to date, so we end up pulling a very old version of Eigen instead of the correct one. Can you send me the command(s) you use to launch your build ? I'll take a look.\n", "After installing bazel and protobuf, I used : \n- `git clone --recurse-submodules https://github.com/tensorflow/tensorflow`\n- `grep -Rl 'lib64'| xargs sed -i 's/lib64/lib/g'`\n- `sudo ./configure` ; I configured no Google Platform Support and no GPU support\n- `sudo bazel build -c opt --local_resources 1024,1.0,1.0 --verbose_failures --ignore_unsupported_sandboxing --copt=\"-mfpu=neon\" tensorflow/tools/pip_package:build_pip_package`\n", "It looks like the issue is with the Neon GCC compiler instruction. I was able to build TensorFlow 0.9rc0 today by simply taking out `--copt=\"-mfpu=neon\"`. I'm not an expert in the GCC instructions for ARM, but I'll continue to look into options for various optimizations that may be possible without messing up the compiler.\n\n@MrSlayer02 linked [his error log](http://pastebin.com/zUfP4xsP) in the parallel issue thread on the TF on RPi repo- you can see it's complaining about converting to a Neon type.\n"]}, {"number": 2828, "title": "MNIST tutorial incorrectly described a file", "body": "The file \"input_data.py\" from MNIST tutorial does not download any data, as previously stated.\n\nRelated to #2733.\n", "comments": ["Can one of the admins verify this patch?\n", "Text updated according to @vrv comment.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 2827, "title": "_reverse_seq in rnn.py", "body": "I can't figure out the usage of _reverse_seq in rnn.py\n\nfollowings are my code\n\n``` javascript\n\nencoder_inputs = []\nfor i in range(3):\n            encoder_inputs.append(tf.placeholder(tf.int32, shape = [None,3], name = 'encoder{}'.format(i)))\n\ntrain_encoder_batches, train_decoder_batches, train_target_batches, train_target_weight_batches = train_data\ninput_feed = dict()\nencoder_data = np.array([[[1,2,3],[1,2,3]],[[1,2,3],[1,2,3]],[[0,0,0],[0,0,0]]])\nfor i in xrange(3):\n    input_feed[encoder_inputs[i].name] = encoder_data[i]\n\noutput_feed = [rnn._reverse_seq(encoder_inputs,tf.Variable([2,2]))]\nprint np.asarray([[1,2,3],[1,2,3]])\n\nsess = tf.InteractiveSession()\nprint sess.run(output_feed,input_feed)\n```\n\ntf.Variable([2,2]) means that (batch size : 2) each length of elements in batch is 2 \ndata : vector size 3, total length 3, useful length 2\n\nerrors are as follows :\n\n``` javascript\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-28-b8ae6f496e18> in <module>()\n      1 sess = tf.InteractiveSession()\n----> 2 print sess.run(output_feed,input_feed)\n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    338     try:\n    339       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 340                          run_metadata_ptr)\n    341       if run_metadata:\n    342         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    521 \n    522     # Validate and process fetches.\n--> 523     processed_fetches = self._process_fetches(fetches)\n    524     unique_fetches = processed_fetches[0]\n    525     target_list = processed_fetches[1]\n\n/opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _process_fetches(self, fetches)\n    491           raise TypeError('Fetch argument %r of %r has invalid type %r, '\n    492                           'must be a string or Tensor. (%s)'\n--> 493                           % (subfetch, fetch, type(subfetch), str(e)))\n    494         except ValueError as e:\n    495           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n\nTypeError: Fetch argument [<tf.Tensor 'unpack_4:0' shape=(2, 3) dtype=int32>, <tf.Tensor 'unpack_4:1' shape=(2, 3) dtype=int32>, <tf.Tensor 'unpack_4:2' shape=(2, 3) dtype=int32>] of [<tf.Tensor 'unpack_4:0' shape=(2, 3) dtype=int32>, <tf.Tensor 'unpack_4:1' shape=(2, 3) dtype=int32>, <tf.Tensor 'unpack_4:2' shape=(2, 3) dtype=int32>] has invalid type <type 'list'>, must be a string or Tensor. (Can not convert a list into a Tensor or Operation.)\n```\n", "comments": ["_reverse_seq returns a list already. So remove [] from output_feed:\n\noutput_feed = rnn._reverse_seq(encoder_inputs,tf.Variable([2,2]))\n", "I got it. THANKS A LOT\n"]}, {"number": 2826, "title": "A 1D and 2D Gaussian implementation in TensorFlow", "body": "Feature request:\n\nMixture Density networks, attention models and variational inference getting more popular, we are defining more and more Gaussians in our model. My feature request would be to have a native Gaussian implementation in TensorFlow.\n\nUp to now, I re-use [this](https://github.com/hardmaru/write-rnn-tensorflow/blob/master/model.py) code:\n\n```\ndef tf_2d_normal(x1, x2, mu1, mu2, s1, s2, rho):\n  # eq # 24 and 25 of http://arxiv.org/abs/1308.0850\n  norm1 = tf.sub(x1, mu1)\n  norm2 = tf.sub(x2, mu2)\n  s1s2 = tf.mul(s1, s2)\n  z = tf.square(tf.div(norm1, s1))+tf.square(tf.div(norm2, s2))-2*tf.div(tf.mul(rho, tf.mul(norm1, norm2)), s1s2)\n  negRho = 1-tf.square(rho)\n  result = tf.exp(tf.div(-z,2*negRho))\n  denom = 2*np.pi*tf.mul(s1s2, tf.sqrt(negRho))\n  result = tf.div(result, denom)\n  return result\n```\n\nHowever, it might be nice for Tensorflow to implement it in the library. They might be able to apply it more computationally efficient? Also, they might go for N-dimensional Gaussians, not only two.\n\nWhat do you think? Is this feasible?\n", "comments": ["I'll do it, implement 1D and 2D Gaussian. I'll be grateful if anyone can give me guidance.\n\nI think we should first register ops in tensorflow/core/ops/math_ops.cc, then implement in tensorflow/core/kernels/, then implement python api in tensorflow/python/ops/math_ops.py.\n", "See tf.contrib.distributions for normal and multivariate normal distribution classes.\n"]}, {"number": 2825, "title": "Initialize moving averages by the first applied value rather than 0", "body": "Currently hidden moving averages are initialized by 0. If the decay is large, it takes a lot of time for the moving average variable to forget about initial zero value. It would be much more reasonable to initialize by the first obtained value of the observed variable, and then start to apply decay. In this case the initial initialization is not required anymore, so the variable can be removed from the \"assert_variables_initialized\" list\n", "comments": ["I can't reproduce what you're seeing. Could you add a small code snippet to repro? \n\n```\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\n\nx = tf.Variable(3.0)\nema = tf.train.ExponentialMovingAverage(0.9)\nmaintain_averages_op = ema.apply([x])\n\nsess.run(tf.initialize_all_variables())\nsess.run(x.assign(x+1.0))\nsess.run(maintain_averages_op)\nprint 'x', sess.run(x)\nprint 'avg', sess.run(ema.average(x))\n\n```\n\nThis returns:\n\n```\nx 4.0\navg 3.1\n```\n", "@sdemyanov is probably using a Tensor in the moving average rather than a Variable, since the documentation for `tf.train.ExponentialMovingAverage.apply` specifies that \"For Tensor objects, the shadow variables are initialized to 0.\"\n", "Yes, I have the same problem with @sdemyanov. Doing batch normalization using tf.nn.moments and initializing to zero as a big problem in my case.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "the same question.\n", "@tarikarici, @sdemyanov, @CosmosShadow, did you try @golmschenk's suggestion? Could one of you provide a small reproducing snippet if you want us to look at it further.\n", "Closing due to lack of activity.", "I am running into the same problem. Here is a minimum example:\r\n\r\n```\r\ntf.reset_default_graph()\r\nsess = tf.InteractiveSession()\r\n\r\nx = tf.identity(tf.Variable(3.0))\r\nema = tf.train.ExponentialMovingAverage(0.9)\r\nmaintain_averages_op = ema.apply([x])\r\n\r\nsess.run(tf.initialize_all_variables())\r\nsess.run(maintain_averages_op)\r\nprint('x', sess.run(x))\r\nprint('avg', sess.run(ema.average(x)))\r\n```\r\nThis returns\r\n```\r\nx 3.0\r\navg 0.3\r\n```", "Guys, the issue was resolved by zero_debias=True parameter. Just use that one.", "@sdemyanov Thanks! I completely misunderstood this parameter.", "`zero_debias=True` mitigates this issue after a couple of hundred iterations, but has almost no effect after few iterations."]}, {"number": 2824, "title": "Shrink iOS binaries", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Internal commits cherry-picked from master. Ignoring CLAbot.\n"]}, {"number": 2823, "title": "Implement variable length sequences for seq2seq", "body": "Now, seq2seq uses bucketing to deal with variable length sequences. Which need to define many models and require manually tuning.  While in RNN, some models implement variable length sequences with a param `sequence_length`, which I think is a better method. So how about use the same method in RNN to implement variable length sequences for seq2seq?\n", "comments": ["This is on the near-future TODO list.\n", "Where can I see the TODO list? I'm interesting in implement this.\n", "We already have interest from about 2 folks internally to implement this.\nPlease be patient and we'll have something \"soonish\".\n\nOn Sun, Jun 12, 2016 at 8:20 PM, Bin Wang notifications@github.com wrote:\n\n> Where can I see the TODO list? I'm interesting in implement this.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2823#issuecomment-225481836,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtimzwFpoEiihHDl58C8jZYsD0u64Kbks5qLMxhgaJpZM4Iz71c\n> .\n", "Thanks. Looking forward it.\n", "Hello, is there advance on this issue?\nI'm not rushing you; I'm using seq2seq with pleasure with bucketing and padding technique. :)\n", "Not trying to sound pushy at all, I was just wondering how the process on this is moving along? ", "There exists quite good progress on #4686!\r\nIt's not merged yet but I think most of the functionalities are ready to use, and I'm actually using it.", "That's great news!\n\nOn Nov 26, 2016 12:08 AM, \"Jihun Choi\" <notifications@github.com> wrote:\n\n> There exists quite good progress on #4686\n> <https://github.com/tensorflow/tensorflow/pull/4686>!\n> It's not be merged yet but I think most of the functionalities are ready\n> to use, and I'm actually using it.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2823#issuecomment-263044674>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ARW5mmOi8lWa9I_twabbCHLg2SKqSd4Tks5rB77TgaJpZM4Iz71c>\n> .\n>\n"]}, {"number": 2822, "title": "R0.9", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "We're not quite done with r0.9, we'll merge it when we publish the non-RC.\n"]}, {"number": 2821, "title": "max_pool_with_argmax index calculation", "body": "max_pool_with_argmax returns an argmax array which doesn't seem to account for the batch dimension. That is, the op returns the flattened index as: (height \\* width + x) \\* channels + c instead of ((b \\* height + y) \\* width + x) \\* channels + c as indicated in the API\n", "comments": ["Could you please cut and paste the code that you believe is doing this incorrectly?\n\nCould you please also include a unit test demonstrating the problem? Thanks.\n", "Automatically closing because there was no response. Please reopen if it is still an issue.\n"]}, {"number": 2820, "title": "[tf.learn] Allow Attention in RNNClassifier and Regressor", "body": "1. Added attention wrappers in rnn model\n2. Added test on additions and moved tests to a separate rnn_test.py\n3. Allow attention params in rnn estimators\n", "comments": ["Without having checked -- the RNN interface has recently evolved a bit, did you use the new cell types (or can a user use those)?\n", "@martinwicke Do you mean state_is_tuple change? If so, I specified a TODO there but haven't got a chance to look into differences yet. \n", "I'll wait for @ilblackdragon to comment. Thanks!\n", "I actually never used this kind of attention, usually using attention in seq2seq models or attentive reader / memory network ones. Would be interesting to see how results are different between plain lstm/gru and this on some non-sentiment analysis (paper only mentioned sentiment analysis task).\n", "@ilblackdragon I think I've addressed your comments. Thanks!\n"]}, {"number": 2819, "title": "Fixed bug in train_test_split operation.", "body": "If scikit-learn is installed, tensorflow attempts to import the operation train_test_split from scikit-learn. Otherwise it provides a custom implementation. The custom implementation correctly computes the position to split the dataset, but returns the same set of indices for both the training set and the test set. Effectively this operations returns the training set for both the training and test sets. \n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 2818, "title": "tf.gradients casts away imaginary part of result when differentiating by a real value ", "body": "This is continued from #2255:\n\nDifferentiating a complex expression by a real value with `tf.gradients` should yield a complex value, but this doesn't seem to be the way that `tf.gradients` works at the moment.\nIf I run\n\n```\nimport tensorflow as tf\nimport numpy as np\n\ns = tf.Session()\nx = tf.placeholder(tf.float32)\n\nz = tf.complex(0., x)\n\nprint('full', s.run(tf.gradients(z, [x]), feed_dict={x: 1}))\nprint('real', s.run(tf.gradients(tf.real(z), [x]), feed_dict={x: 1}))\nprint('imag', s.run(tf.gradients(tf.imag(z), [x]), feed_dict={x: 1}))\n```\n\nThe output is\n\n```\nfull [0.0]\nreal [0.0]\nimag [1.0]\n```\n\nwhile I would have expected\n\n```\nfull [0.0 + 1.0j]\nreal [0.0]\nimag [1.0]\n```\n\nI suspect that `tf.gradients` always casts the final result to the type of the variable that we differentiate with respect to, which makes sense in many circumstances, but leads to incorrect behaviour in this case.\n", "comments": ["I've thought about this some more and realized that tensorflow expression aren't necessarily holomorphic, which means returning a complex number doesn't always make sense.\nMaybe it would be better to throw an error if the user tries to differentiate a complex expression?\nThe user can always insert a `tf.real` to make it work the way it currently does, and this would prevent accidental mistakes.\n", "Throwing an error is an option.  For consistency I'd prefer to throw an error if the tensor isn't scalar as well.  Unfortunately, absorbing either change would require verifying that all the downstream code doesn't (weirdly) depend on this behavior, which makes it hard to do from the outside.\n", "If not an error, a warning at least would be useful.\n", "I am trying to compute the gradients of a complex function in Tensorflow, but I have some trouble. \nCould it be the same problem described by @ibab ?\nHere is my code:\n\n```\nimport numpy as np\nimport tensorflow as tf\n\ndef CompSEQ(rho0):\n\n    def EvolveRHO(prev, input):\n        return tf.matmul(rho0,prev)\n\n    rhos = tf.scan(EvolveRHO, np.array([1, 2, 3]), initializer=rho0)\n    return tf.real(tf.trace(tf.unpack(tf.slice(rhos,[2,0,0],[1,2,2]))[0]))\n\nnp.random.seed(1234)\nN = 2\n\nx_r = tf.Variable(tf.zeros([N,N], dtype=tf.float32))\nx_i = tf.Variable(tf.zeros([N,N], dtype=tf.float32))\nfun = CompSEQ(tf.complex(x_r,x_i))\n#grad = tf.gradients([fun],[x_r,x_i])    # THIS LINE CAUSES THE PROBLEM!!!\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nre = np.random.rand(N,N).astype(np.float32)\nim = np.random.rand(N,N).astype(np.float32)\nprint('fun',sess.run(fun, feed_dict={x_r:re, x_i:im}))\nprint('grad',sess.run(grad, feed_dict={x_r:re, x_i:im}))\n```\n\nI use 2 input float32 vectors x,y that will be transformed into a complex matrix. All the computations are performed using complex numbers and at the end all the result is transformed into a float32 by taking the real part. If I comment the call to tf.gradients (as in the code) everything works fine, but when I try to compute the gradients I get the following error:\n\n```\nTraceback (most recent call last):\n  File \"errorS.py\", line 18, in <module>\n    grad = tf.gradients([fun],[x_r,x_i])    # THIS LINE CAUSES THE PROBLEM!!!\n  File \"/Users/tamburin/Library/Python/2.7/lib/python/site-packages/tensorflow/python/ops/gradients.py\", line 486, in gradients\n    _VerifyGeneratedGradients(in_grads, op)\n  File \"/Users/tamburin/Library/Python/2.7/lib/python/site-packages/tensorflow/python/ops/gradients.py\", line 264, in _VerifyGeneratedGradients\n    dtypes.as_dtype(inp.dtype).name))\nValueError: Gradient type float32 generated for op name: \"scan/while/Switch_1\"\nop: \"Switch\"\ninput: \"scan/while/Merge_1\"\ninput: \"scan/while/LoopCond\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_COMPLEX64\n  }\n}\nattr {\n  key: \"_class\"\n  value {\n    list {\n      s: \"loc:@scan/while/Merge_1\"\n    }\n  }\n}\n does not match input type complex64\n```\n\nConverting all the computations into float32 variables (replacing tf.complex with tf.add and deleting tf.real) resolves the problem, but I need to maintain the computation on complex variables (this is a very simplified example w.r.t. my real problem).\n", "Same problem when running dynamic_rnn with complex hidden states. \r\n```\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 264, in _VerifyGeneratedGradients\r\n    dtypes.as_dtype(inp.dtype).name))\r\nValueError: Gradient type float32 generated for op name: \"RNN/while/Switch_2\"\r\nop: \"Switch\"\r\ninput: \"RNN/while/Merge_2\"\r\ninput: \"RNN/while/LoopCond\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_COMPLEX64\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@RNN/while/Merge_2\"\r\n    }\r\n  }\r\n}\r\n does not match input type complex64\r\n```", "The linked commit means TensorFlow now raises an exception for the complex case.  There were too many uses of the nonscalar real case, so that persists.", "Raising an exeception is not the best option, people will still be confused about why my code raises an exeception, tensorflow had better to make the definition of gradients clear.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/20613\r\n", "> This is continued from #2255:\r\n> \r\n> Differentiating a complex expression by a real value with `tf.gradients` should yield a complex value, but this doesn't seem to be the way that `tf.gradients` works at the moment.\r\n> If I run\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import numpy as np\r\n> \r\n> s = tf.Session()\r\n> x = tf.placeholder(tf.float32)\r\n> \r\n> z = tf.complex(0., x)\r\n> \r\n> print('full', s.run(tf.gradients(z, [x]), feed_dict={x: 1}))\r\n> print('real', s.run(tf.gradients(tf.real(z), [x]), feed_dict={x: 1}))\r\n> print('imag', s.run(tf.gradients(tf.imag(z), [x]), feed_dict={x: 1}))\r\n> ```\r\n> \r\n> The output is\r\n> \r\n> ```\r\n> full [0.0]\r\n> real [0.0]\r\n> imag [1.0]\r\n> ```\r\n> \r\n> while I would have expected\r\n> \r\n> ```\r\n> full [0.0 + 1.0j]\r\n> real [0.0]\r\n> imag [1.0]\r\n> ```\r\n> \r\n> I suspect that `tf.gradients` always casts the final result to the type of the variable that we differentiate with respect to, which makes sense in many circumstances, but leads to incorrect behaviour in this case.\r\n\r\nIf you check the definition of the gradient I posted in [stackoverflow](https://stackoverflow.com/questions/57108959/how-does-tf-gradients-manages-non-holomorphic-functions/), the result you are having makes perfect sense.\r\n\r\nAs said, I still don't know WHY tensorflow uses that definition of the gradient but makes perfect sense for real scalar function wtf of complex variables.\r\n\r\n\r\n\r\n> Raising an exeception is not the best option, people will still be confused about why my code raises an exeception, tensorflow had better to make the definition of gradients clear.\r\n> \r\n> #20613\r\n\r\nI totally agree! I have been asking for months, even posted a 100 reputation reward to that stackoverlow question and got no answer. I would love to have a formal mathematical definition on the documentation of what tensorflow does for complex-valued gradients."]}, {"number": 2817, "title": "[learn] update TensorFlowEstimator's functions (fit, partial_fit, predict)", "body": "- update `fit`, `partial_fit` and `predict` functions according to the last changes in `Estimator`\n- update `Esitmator`'s functions to accept `logdir` and `n_classes` args\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2816, "title": "Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed", "body": "Hi,\nInstalling TensorFlow from source fails on my system. There seems to be some Python-related error when building 'half_plus_two'.\nSee (hopefully) all relevant output below.\n### Environment info\n\nOperating System:\n\n> lsb_release -a\n> No LSB modules are available.\n> Distributor ID: Ubuntu\n> Description:    Ubuntu 16.04 LTS\n> Release:    16.04\n> Codename:   xenial\n> \n> uname -a\n> Linux the-beast 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n> ls -l /usr/local/cuda-8.0/lib64/libcud*\n> -rw-r--r-- 1 root root   560184 Jun 12 16:53 /usr/local/cuda-8.0/lib64/libcudadevrt.a\n> lrwxrwxrwx 1 root root       16 Jun 12 16:53 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\n> lrwxrwxrwx 1 root root       19 Jun 12 16:53 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n> -rwxr-xr-x 1 root root   394472 Jun 12 16:53 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n> -rw-r--r-- 1 root root   737516 Jun 12 16:53 /usr/local/cuda-8.0/lib64/libcudart_static.a\n> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54 /usr/local/cuda-8.0/lib64/libcudnn.so\n> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\n> -rw-r--r-- 1 root root 68709594 Jun 12 16:54 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from sources, provide the commit hash:\nbf83048d5f28b7fa0f39df799bd01a8fc581f5cf\n### Steps to reproduce\n1. Clone TensorFlow and follow instructions to build from source:\n   https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installation-for-linux\n2. Execute command\n   bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n   which ultimately fails.\n### Logs or other output that would be helpful\n\nThis should be the relevant part:\n\nmichael@the-beast:~/devel/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nWARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/protobuf/WORKSPACE:1: Workspace name in /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /home/michael/devel/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nWARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/highwayhash/WORKSPACE:1: Workspace name in /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/re2/WORKSPACE:1: Workspace name in /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nINFO: Found 1 target...\nERROR: /home/michael/devel/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command \n  (cd /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/home/michael/torch/install/bin:/home/michael/devel/jdk1.8.0_77/bin:/home/michael/devel/bazel/output:/usr/local/cuda-8.0/bin:/home/michael/devel/gocode/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; rm -rf /tmp/half_plus_two; /usr/bin/python bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two; cp -r /tmp/half_plus_two/\\* bazel-out/local_linux-opt/genfiles/tensorflow/contrib/session_bundle/example/half_plus_two'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\nTraceback (most recent call last):\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\", line 32, in <module>\n    import tensorflow as tf\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\x62\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 16.639s, Critical Path: 8.93s\n", "comments": ["I think I\"ve seen this error when \"git pull\" overwrote files like CROSSTOOL\nthat ./configure sets up, can you try again after ./configure?\n\nOn Sun, Jun 12, 2016 at 9:07 AM, Michael Hofmann notifications@github.com\nwrote:\n\n> Hi,\n> Installing TensorFlow from source fails on my system. There seems to be\n> some Python-related error when building 'half_plus_two'.\n> See (hopefully) all relevant output below.\n> Environment info\n> \n> Operating System:\n> \n> lsb_release -a\n> No LSB modules are available.\n> Distributor ID: Ubuntu\n> Description: Ubuntu 16.04 LTS\n> Release: 16.04\n> Codename: xenial\n> \n> uname -a\n> Linux the-beast 4.4.0-24-generic #43\n> https://github.com/tensorflow/tensorflow/issues/43-Ubuntu SMP Wed Jun 8\n> 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n> \n> Installed version of CUDA and cuDNN:\n> (please attach the output of ls -l /path/to/cuda/lib/libcud*):\n> \n> ls -l /usr/local/cuda-8.0/lib64/libcud*\n> -rw-r--r-- 1 root root 560184 Jun 12 16:53\n> /usr/local/cuda-8.0/lib64/libcudadevrt.a\n> lrwxrwxrwx 1 root root 16 Jun 12 16:53\n> /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\n> lrwxrwxrwx 1 root root 19 Jun 12 16:53\n> /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n> -rwxr-xr-x 1 root root 394472 Jun 12 16:53\n> /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n> -rw-r--r-- 1 root root 737516 Jun 12 16:53\n> /usr/local/cuda-8.0/lib64/libcudart_static.a\n> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54\n> /usr/local/cuda-8.0/lib64/libcudnn.so\n> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54\n> /usr/local/cuda-8.0/lib64/libcudnn.so.5\n> -rwxr-xr-x 1 root root 78065952 Jun 12 16:54\n> /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\n> -rw-r--r-- 1 root root 68709594 Jun 12 16:54\n> /usr/local/cuda-8.0/lib64/libcudnn_static.a\n> \n> If installed from sources, provide the commit hash:\n> bf83048\n> https://github.com/tensorflow/tensorflow/commit/bf83048d5f28b7fa0f39df799bd01a8fc581f5cf\n> Steps to reproduce\n> 1. Clone TensorFlow and follow instructions to build from source:\n>    https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installation-for-linux\n> 2. Execute command bazel build -c opt --config=cuda\n>    //tensorflow/tools/pip_package:build_pip_package which ultimately fails.\n> \n> Logs or other output that would be helpful\n> \n> This should be the relevant part:\n> \n> michael@the-beast:~/devel/tensorflow$ bazel build -c opt --config=cuda\n> //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n> WARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/protobuf/WORKSPACE:1:\n> Workspace name in\n> /home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/external/protobuf/WORKSPACE\n> (@__main_) does not match the name given in the repository's definition (\n> @protobuf https://github.com/protobuf); this will cause a build error\n> in future versions.\n> WARNING: /home/michael/devel/tensorflow/util/python/BUILD:11:16: in\n> includes attribute of cc_library rule //util/python:python_headers:\n> 'python_include' resolves to 'util/python/python_include' not in\n> 'third_party'. This will be an error in the future.\n> WARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/highwayhash/WORKSPACE:1:\n> Workspace name in\n> /home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/external/highwayhash/WORKSPACE\n> (@__main_) does not match the name given in the repository's definition\n> (@highwayhash); this will cause a build error in future versions.\n> WARNING: /home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/external/re2/WORKSPACE:1:\n> Workspace name in\n> /home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/external/re2/WORKSPACE\n> (@__main_) does not match the name given in the repository's definition\n> (@re2); this will cause a build error in future versions.\n> INFO: Found 1 target...\n> ERROR:\n> /home/michael/devel/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1:\n> Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two\n> failed: bash failed: error executing command\n> (cd /home/michael/.cache/bazel/\n> \n> _bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow && \\\n> exec env - \\\n> PATH=/home/michael/torch/install/bin:/home/michael/devel/jdk1.8.0_77/bin:/home/michael/devel/bazel/output:/usr/local/cuda-8.0/bin:/home/michael/devel/gocode/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/home/michael/torch/install/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n> \\ /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh;\n> rm -rf /tmp/half_plus_two; /usr/bin/python\n> bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two;\n> cp -r /tmp/half_plus_two/_\n> bazel-out/local_linux-opt/genfiles/tensorflow/contrib/session_bundle/example/half_plus_two'):\n> com.google.devtools.build.lib.shell.BadExitStatusException: Process exited\n> with status 1. I tensorflow/stream_executor/dso_loader.cc:108] successfully\n> opened CUDA library libcublas.so.8.0 locally I\n> tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA\n> library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:108]\n> successfully opened CUDA library libcufft.so.8.0 locally I\n> tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA\n> library libcuda.so locally I tensorflow/stream_executor/dso_loader.cc:108]\n> successfully opened CUDA library libcurand.so.8.0 locally Traceback (most\n> recent call last): File\n> \"/home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\",\n> line 32, in import tensorflow as tf File\n> \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/__init*.py\",\n> line 23, in\n> from tensorflow.python import *\n> File \"/home/michael/.cache/bazel/\n> *bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/python/__init_*.py\",\n> line 52, in\n> from tensorflow.core.framework.graph_pb2 import *\n> File \"/home/michael/.cache/bazel/\n> \n> _bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\",\n> line 16, in from tensorflow.core.framework import attr_value_pb2 as\n> tensorflow_dot_core_dot_framework_dot_attr__value__pb2 File\n> \"/home/michael/.cache/bazel/bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/attr_value_pb2.py\",\n> line 16, in from tensorflow.core.framework import tensor_pb2 as\n> tensorflow_dot_core_dot_framework_dot_tensor__pb2 File\n> \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/tensor_pb2.py\",\n> line 16, in from tensorflow.core.framework import tensor_shape_pb2 as\n> tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2 File\n> \"/home/michael/.cache/bazel/_bazel_michael/09c785d214849e49e64c5959b4c31911/execroot/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/org_tensorflow/tensorflow/core/framework/tensor_shape_pb2.py\",\n> line 22, in\n> serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02\n> \\x03(\\x0b\\x32\n> .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03\n> \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01\n> \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02\n> \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\x62\\x06proto3')\n> TypeError: __init_() got an unexpected keyword argument 'syntax'\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> INFO: Elapsed time: 16.639s, Critical Path: 8.93s\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2816, or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHLTP5M78Q1FjR7RiIt_HwW-wGriNks5qLC7EgaJpZM4Iz0xs\n> .\n", "The error still occurs after a fresh repository clone, a new call to ./configure, and also cleaning the Bazel cache (~/.cache/bazel).\n", "I delete the old version(2.6.1) protobuf and it works\nTF need protobuf 3.0\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl\n", "Indeed, I verified that this works in a virtualenv environment. Compilation then succeeds with protobuf-3.0.0b2. Thanks!\n\nThe current documentation (https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#protobuf-library-related-issues) does not make it clear that installing a custom protobuf library is necessary for compilation. Instead, it is mentioned that \"Install the above package after you have installed TensorFlow via pip, as the standard pip install tensorflow would install the python only pip package.\"\nThe \"Installing from sources\" section does not link to the \"Protobuf library related issues\" section.\n\nIt would be great if either the documentation could be amended, or (even better) this precondition could be checked before starting compilation, and a meaningful error message could be emitted.\n"]}, {"number": 2815, "title": "glibc detected *** python3: invalid fastbin entry (free)", "body": "Hello ,I am try to run my code on TensorFlow. When I run it, it works fine. But after 30 minutes, I get this error:\n\n```\n*** glibc detected *** python3: invalid fastbin entry (free): 0x00007fae00288af0 ***\n======= Backtrace: =========\n/lib64/libc.so.6[0x3ba6276166]\n/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6Tensor16CopyFromInternalERKS0_RKNS_11TensorShapeE+0x109)[0x7fae8eebecf9]\n/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x1cdb96e)[0x7fae8ee2796e]\n/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x1cd0cf5)[0x7fae8ee1ccf5]\n/gpfs/home/chaowei/development/python/python3_tf_cpu_vir/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPool4Impl10WorkerLoopEv+0x17b)[0x7fae8ef4639b]\n/gpfs/home/chaowei/software/gcc-6.1.0/lib64/libstdc++.so.6(+0xbe232)[0x7faebd7b4232]\n/lib64/libpthread.so.0[0x3ba66079d1]\n/lib64/libc.so.6(clone+0x6d)[0x3ba62e8b6d]\n======= Memory map: ========\n00400000-007f8000 r-xp 00000000 00:15 3515267                            /gpfs/home/chaowei/development/python/python3_tf_cpu_vir/bin/python3.4\n009f8000-00a69000 rw-p 003f8000 00:15 3515267                            /gpfs/home/chaowei/development/python/python3_tf_cpu_vir/bin/python3.4\n00a69000-00a87000 rw-p 00000000 00:00 0 \n010b3000-2970f000 rw-p 00000000 00:00 0                                  [heap]\n3ba5a00000-3ba5a20000 r-xp 00000000 08:05 18612623                       /lib64/ld-2.12.so\n3ba5c1f000-3ba5c20000 r--p 0001f000 08:05 18612623                       /lib64/ld-2.12.so\n3ba5c20000-3ba5c21000 rw-p 00020000 08:05 18612623                       /lib64/ld-2.12.so\n3ba5c21000-3ba5c22000 rw-p 00000000 00:00 0 \n3ba5e00000-3ba5e02000 r-xp 00000000 08:05 18612630                       /lib64/libdl-2.12.so\n3ba5e02000-3ba6002000 ---p 00002000 08:05 18612630                       /lib64/libdl-2.12.so\n3ba6002000-3ba6003000 r--p 00002000 08:05 18612630                       /lib64/libdl-2.12.so\n3ba6003000-3ba6004000 rw-p 00003000 08:05 18612630                       /lib64/libdl-2.12.so\n3ba6200000-3ba638b000 r-xp 00000000 08:05 18612624                       /lib64/libc-2.12.so\n3ba638b000-3ba658a000 ---p 0018b000 08:05 18612624                       /lib64/libc-2.12.so\n3ba658a000-3ba658e000 r--p 0018a000 08:05 18612624                       /lib64/libc-2.12.so\n3ba658e000-3ba658f000 rw-p 0018e000 08:05 18612624                       /lib64/libc-2.12.so\n3ba658f000-3ba6594000 rw-p 00000000 00:00 0 \n3ba6600000-3ba6617000 r-xp 00000000 08:05 18612626                       /lib64/libpthread-2.12.so\n3ba6617000-3ba6817000 ---p 00017000 08:05 18612626                       /lib64/libpthread-2.12.so\n3ba6817000-3ba6818000 r--p 00017000 08:05 18612626                       /lib64/libpthread-2.12.so\n3ba6818000-3ba6819000 rw-p 00018000 08:05 18612626                       /lib64/libpthread-2.12.so\n3ba6819000-3ba681d000 rw-p 00000000 00:00 0 \n3ba6a00000-3ba6a83000 r-xp 00000000 08:05 18612635                       /lib64/libm-2.12.so\n3ba6a83000-3ba6c82000 ---p 00083000 08:05 18612635                       /lib64/libm-2.12.so\n3ba6c82000-3ba6c83000 r--p 00082000 08:05 18612635                       /lib64/libm-2.12.so\n3ba6c83000-3ba6c84000 rw-p 00083000 08:05 18612635                       /lib64/libm-2.12.so\n3ba6e00000-3ba6e07000 r-xp 00000000 08:05 18612627                       /lib64/librt-2.12.so\n3ba6e07000-3ba7006000 ---p 00007000 08:05 18612627                       /lib64/librt-2.12.so\n3ba7006000-3ba7007000 r--p 00006000 08:05 18612627                       /lib64/librt-2.12.so\n3ba7007000-3ba7008000 rw-p 00007000 08:05 18612627                       /lib64/librt-2.12.so\n3ba7200000-3ba7215000 r-xp 00000000 08:05 18612638                       /lib64/libz.so.1.2.3\n3ba7215000-3ba7414000 ---p 00015000 08:05 18612638                       /lib64/libz.so.1.2.3\n3ba7414000-3ba7415000 r--p 00014000 08:05 18612638                       /lib64/libz.so.1.2.3\n3ba7415000-3ba7416000 rw-p 00015000 08:05 18612638                       /lib64/libz.so.1.2.3\n3ba7600000-3ba763b000 r-xp 00000000 08:05 5769740                        /usr/lib64/libxslt.so.1.1.26\n3ba763b000-3ba783b000 ---p 0003b000 08:05 5769740                        /usr/lib64/libxslt.so.1.1.26\n3ba783b000-3ba783d000 rw-p 0003b000 08:05 5769740                        /usr/lib64/libxslt.so.1.1.26\n3ba7e00000-3ba7e02000 r-xp 00000000 08:05 18612339                       /lib64/libutil-2.12.so\n3ba7e02000-3ba8001000 ---p 00002000 08:05 18612339                       /lib64/libutil-2.12.so\n3ba8001000-3ba8002000 r--p 00001000 08:05 18612339                       /lib64/libutil-2.12.so\n3ba8002000-3ba8003000 rw-p 00002000 08:05 18612339                       /lib64/libutil-2.12.so\n3ba8200000-3ba8203000 r-xp 00000000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0\n3ba8203000-3ba8402000 ---p 00003000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0\n3ba8402000-3ba8403000 r--p 00002000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0\n3ba8403000-3ba8404000 rw-p 00003000 08:05 18612558                       /lib64/libgpg-error.so.0.5.0\n3bab600000-3bab610000 r-xp 00000000 08:05 18612329                       /lib64/libbz2.so.1.0.4\n3bab610000-3bab80f000 ---p 00010000 08:05 18612329                       /lib64/libbz2.so.1.0.4\n3bab80f000-3bab811000 rw-p 0000f000 08:05 18612329                       /lib64/libbz2.so.1.0.4\n3bace00000-3bacf48000 r-xp 00000000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6\n3bacf48000-3bad147000 ---p 00148000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6\n3bad147000-3bad151000 rw-p 00147000 08:05 5769052                        /usr/lib64/libxml2.so.2.7.6\n3bad151000-3bad152000 rw-p 00000000 00:00 0 \n7fadb0000000-7fadb3ff7000 rw-p 00000000 00:00 0 \n7fadb3ff7000-7fadb4000000 ---p 00000000 00:00 0 \n7fadb4000000-7fadb4ac5000 rw-p 00000000 00:00 0 Aborted (core dumped)\n\n```\n\nI don't see this error before when I use TF. It is the first time to see it on running TF and I am not sure weather it's the problem of TF or not. BTW, I am trying to run my code again in order to see if the error can appear again.\n\nThe version of TF is 0.8 and the version of gcc is 6.1\n", "comments": ["FWIW I just saw something similar while training an LSTM through `keras`; of 8 runs of the same model w/ different hyperparameters, 7 completed and one stopped after a few hours with:\n\n```\n*** Error in `python': invalid\n fastbin entry (free): 0x00007fa6eec0f830 ***\n======= Backtrace: =========\n/lib64/libc.so.6(+0x7d023)[0x7fa84b5af023]\n/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(\n_ZN10tensorflow6Tensor16CopyFromInternalERKS0_RKNS_11TensorShapeE+0xe6)[0x7fa8356c17d6]\n/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(\n+0x1701e9e)[0x7fa835595e9e]\n/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(\n+0x16f7460)[0x7fa83558b460]\n/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(\n_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEj+0x388)[0\nx7fa835795428]\n/home/bnaul/miniconda3/envs/deep/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so(\n_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEU\nlvE_E9_M_invokeERKSt9_Any_data+0x22)[0x7fa835795082]\n/lib64/libstdc++.so.6(+0xb5220)[0x7fa8337cd220]\n/lib64/libpthread.so.0(+0x7dc5)[0x7fa84c20bdc5]\n/lib64/libc.so.6(clone+0x6d)[0x7fa84b62828d]\n======= Memory map: ========\n00400000-00401000 r-xp 00000000 00:32 518803                             /home/bnaul/miniconda3/envs/\ndeep/bin/python3.5\n00601000-00602000 rw-p 00001000 00:32 518803                             /home/bnaul/miniconda3/envs/\ndeep/bin/python3.5\n0204f000-4eddd000 rw-p 00000000 00:00 0                                  [heap]\n7fa630000000-7fa63015f000 rw-p 00000000 00:00 0\n7fa63015f000-7fa634000000 ---p 00000000 00:00 0\n7fa638000000-7fa6387f6000 rw-p 00000000 00:00 0\n...\n7fa80e98a000-7fa80e98e000 r-xp 00000000 00:32 93983454                   /home/bnaul/miniconda3/envs/\ndeep/lib/python3.5/lib-dynload/_lsprof.cpython-35m-x86_64-linux-gnu.so\n7fa80e98e000-7fa80eb8d000 ---p 00004000 00:32 93983454                   /home/bnaul/miniconda3/envs/\ndeep/lib/python3.5/lib-dynload/_lsprof.cpython-35m-x86_64-linux-gnu.so\n7fa80eb8d000-7fa80eb8e000 rw-p 00003000 00:32 93983454                   /home/bnaul/miniconda3/envs/\ndeep/lib/python3.5/lib-dynload/_lsprof.cpython-35m-x86_64-linux-gnu.so\n7fa80eb8e000-7fa80ec8f000 rw-p 00000000 00:00 0\n7fa80ec8f000-7fa80ec99000 r-xp 00000000 00:32 93983517                   /home/bnaul/miniconda3/envs/\ndeep/lib/python3.5/lib-dynload/array.cpython-35m-x86_64-linux-gnu.sobash: line 1: 24245 Aborted\n           (core dumped) python period.py 128 2 0.0\n```\n", "@bnaul Wow, I am also use lstm and of 4 runs of same model in different  parameter. But when I run again, this error disappear.\n", "Where does the tensorflow binary that you use come from? Did you build it from source? If so, what's the git hash? Or did you download a pip file and install it? If so, which version? \n", "@caisq  I downloaded it from github and built it from source. But I am sorry I don't know how to check git hash.\n", "@caisq I'm running `tensorflow==0.9.0rc0` under Python 3.5, Linux+GPU (installed from the wheel).\n", "I'm closing this issue due to inactivity. It's likely that the OP is on CentOS 7 and the bug is due to https://bugzilla.redhat.com/show_bug.cgi?id=1305406. A workaround might be to use tcmalloc, but we don't know yet. It's actively being discussed in #4960.\n"]}, {"number": 2814, "title": "Define new ops with pure python", "body": "AFAIK, new ops can't be created with python alone. However, such feature would be quite helpful when dealing with not-so-common IO, e.g. reading an LMDB, or when experimenting with new algorithms.\n", "comments": ["There's the `tf.py_func` op which lets you insert Python functions into the computational graph: https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html\nIs this close to what you had in mind?\n", "That's exactly what I'm looking for. @ibab Thanks!\n"]}, {"number": 2813, "title": "Broken link for tensorboard", "body": "The link [https://www.tensorflow.org/versions/tensorboard/README.html](https://www.tensorflow.org/versions/tensorboard/README.html) for README file in [tensorboard](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html) page is broken. \n", "comments": ["@danmane  This link still appears broken... \n", "This was fixed in the tensorboard page. The link https://www.tensorflow.org/versions/tensorboard/README.html was actually wrong. The correct is https://www.tensorflow.org/code/tensorflow/tensorboard/README.md\n", "Actually, there was a second Readme link on the page that I missed. I'll fix that.\n", "Fixed by 6b08d99, #3733 and #3734.\n", "(The fix won't propagate to the website until the next rebuild. )\n"]}, {"number": 2812, "title": "Branch 124648897", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2811, "title": " ERROR when building pip package: undeclared function related to swig", "body": "I was able to build the cc:tutorials_example_trainer , but failed to build the  pip package. Could anyone suggest how I can fix it? Thanks a lot!\n### Environment info\n\nOperating System:\nCentOS 6.5\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\ncuda-7.5\n### Steps to reproduce\n1. I used python 3.5.1  installled by pyenv\n2. I installed swig from source\n### Error Log\n\nERROR: /csproject/dygroup2/czeng/downloads/tensorflow/tensorflow/python/BUILD:978:1: C++ compilation of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\n  (cd /csproject/dygroup2/czeng/.cache/bazel/_bazel_czeng/211970d9a1065a07eafbeb7dcc542b10/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/project/dygroup2/czeng//.pyenv/shims:/project/dygroup2/czeng//.pyenv/bin:/project/dygroup2/czeng/venv/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/cuda-7.5/bin:/usr/local/cuda-7.5/nvvm/bin:/csproject/dygroup2/czeng/venv/cudnnv5 \\\n    TMPDIR=/tmp/2110270.1.all.q \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -fPIC -DGPR_BACKWARDS_COMPATIBILITY_MODE -iquote . -iquote bazel-out/host/genfiles -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/host/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/host/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote external/nanopb_git -iquote bazel-out/host/genfiles/external/nanopb_git -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/host/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/host/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/host/genfiles/external/png_archive/libpng-1.2.53 -isystem external/highwayhash -isystem bazel-out/host/genfiles/external/highwayhash -isystem external/re2 -isystem bazel-out/host/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/host/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-5f86b31739cd -isystem bazel-out/host/genfiles/external/eigen_archive/eigen-eigen-5f86b31739cd -isystem third_party/gpus/cuda/include -isystem bazel-out/host/genfiles/third_party/gpus/cuda/include -isystem third_party/py/numpy/numpy_include -isystem bazel-out/host/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/host/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/host/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/extras/CUPTI/include -isystem bazel-out/host/genfiles/third_party/gpus/cuda/extras/CUPTI/include -isystem external/grpc/include -isystem bazel-out/host/genfiles/external/grpc/include -isystem external/grpc -isystem bazel-out/host/genfiles/external/grpc -Wno-self-assign -Wno-write-strings -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c bazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/host/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'swig_module_info\\* SWIG_Python_GetModule()':\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:2452:51: error: 'PyCObject_Import' was not declared in this scope\n         (char_)\"type_pointer\" SWIG_TYPE_TABLE_NAME);\n                                                   ^\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'void SWIG_Python_SetModule(swig_module_info_)':\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:2521:92: error: 'PyCObject_FromVoidPtr' was not declared in this scope\n   PyObject _pointer = PyCObject_FromVoidPtr((void *) swig_module, SWIG_Python_DestroyModule);\n                                                                                            ^\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:2512:22: warning: unused variable 'swig_empty_runtime_method_table' [-Wunused-variable]\n   static PyMethodDef swig_empty_runtime_method_table[] = { {NULL, NULL, 0, NULL} };/_ Sentinel _/\n                      ^\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'swig_type_info_ SWIG_Python_TypeQuery(const char_)':\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:2544:60: error: 'PyCObject_AsVoidPtr' was not declared in this scope\n     descriptor = (swig_type_info *) PyCObject_AsVoidPtr(obj);\n                                                            ^\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:2549:51: error: 'PyCObject_FromVoidPtr' was not declared in this scope\n       obj = PyCObject_FromVoidPtr(descriptor, NULL);\n                                                   ^\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject_ _wrap_GetMatchingFiles(PyObject_, PyObject_)':\nbazel-out/host/bin/tensorflow/python/pywrap_tensorflow.cc:5392:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (int i = 0; i < converted.size(); ++i) {\n                       ^\nAt global scope:\ncc1plus: warning: unrecognized command line option \"-Wno-self-assign\"\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n", "comments": ["Which version of SWIG are you using? Can you try 3.0.8+? \n", "Hi @caisq \n\nThanks for your reply. I tried SWIG Version 3.0.8 and installed to my virtualenv directory but another error occured, saying that libpcre.so.1 not found\n", "Same happens for 3.0.10\n`(cd /root/.cache/bazel/_bazel_root/2b02cca0edf7f3a977d1db2d1af2e0e1/execroot/tensorflow-0.9.0 && \\\n  exec env - \\\n    PATH=/software/gcc/6.1.0/login/bin:/software/tensorflow/0.9.0/:/software/python/3.4.5/login/bin:/software/intel/xe2015/composer_xe_2015.1.133/bin/intel64:/software/intel/xe2015/composer_xe_2015.1.133/bin/intel64_mic:/software/intel/xe2015/composer_xe_2015.1.133/debugger/gui/intel64:/software/CUDA/7.5.18/bin:/software/bazel/0.3.0:/software/java/1.8.0_25//bin:/usr/pbs/bin:.:/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin \\\n  /software/gcc/6.1.0/login/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/software/gcc/6.1.0/login/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o' -fPIC -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/protobuf -iquote bazel-out/local-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/local-py3-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/local-py3-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/grpc -iquote bazel-out/local-py3-opt/genfiles/external/grpc -iquote external/nanopb_git -iquote bazel-out/local-py3-opt/genfiles/external/nanopb_git -isystem external/protobuf/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local-py3-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local-py3-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/highwayhash -isystem bazel-out/local-py3-opt/genfiles/external/highwayhash -isystem external/re2 -isystem bazel-out/local-py3-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local-py3-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-d02e6a705c30 -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive/eigen-eigen-d02e6a705c30 -isystem third_party/py/numpy/numpy_include -isystem bazel-out/local-py3-opt/genfiles/third_party/py/numpy/numpy_include -isystem util/python/python_include -isystem bazel-out/local-py3-opt/genfiles/util/python/python_include -isystem third_party/gpus/cuda -isystem bazel-out/local-py3-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local-py3-opt/genfiles/third_party/gpus/cuda/include -isystem external/grpc/include -isystem bazel-out/local-py3-opt/genfiles/external/grpc/include -isystem external/grpc -isystem bazel-out/local-py3-opt/genfiles/external/grpc -Wno-self-assign -Wno-write-strings -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -MD -MF bazel-out/local-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.d -c bazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc -o bazel-out/local-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'swig_module_info* SWIG_Python_GetModule()':\nbazel-out/local-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc:2452:51: error: 'PyCObject_Import' was not declared in this scope\n         (char*)\"type_pointer\" SWIG_TYPE_TABLE_NAME);\n`\n"]}, {"number": 2810, "title": "CUDA_ERROR_MISALIGNED_ADDRESS on MNIST example ", "body": "## Summary\n\nWhat might be causing this error when running **python tensorflow/models/image/mnist/convolutional.py**?\n\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\n### Environment info\n\n**Operating System:**\nLinux Lounge 4.5.6-200.fc23.x86_64 #1 SMP Wed Jun 1 21:28:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\n**Installed version of CUDA and cuDNN**:\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw-r--r--. 1 root root   322936 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx. 1 root root       16 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root root       19 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root root   383336 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r--. 1 root root   720192 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\n-rwxr-xr-x. 1 root root 61453024 Jun 11 12:35 /usr/local/cuda-7.5/lib64/libcudnn.so\n-rwxr-xr-x. 1 root root 61453024 Jun 11 12:35 /usr/local/cuda-7.5/lib64/libcudnn.so.4\n-rwxr-xr-x. 1 root root 61453024 Jun 11 12:35 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rwxr-xr-x. 1 root root 59909104 Jun 11 12:35 /usr/local/cuda-7.5/lib64/libcudnn.so.5\n-rwxr-xr-x. 1 root root 59909104 Jun 11 12:35 /usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\n-rw-r--r--. 1 root root 62025862 Jun 11 12:35 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n\n**1. Which pip package you installed.**\n\n> export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n> pip install --upgrade $TF_BINARY_URL\n\n**2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.**\npython -c \"import tensorflow; print(tensorflow.**version**)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1  python tensorflow/models/image/mnist/convolutional.py.\n2. Observe errror CUDA_ERROR_MISALIGNED_ADDRESS\n3. Scratch head\n### What have you tried?\n1. Searching the internet for clues, none found\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nResults of cuda-memcheck and dmesg\n[error.txt](https://github.com/tensorflow/tensorflow/files/310536/error.txt)\n", "comments": ["Swapping my action with Benoit. All the misaligned memory reads came from Eigen kernels in the attached error messages. \n", "Thanks for your help, but I'm still lost. I'm a user, not a programmer. I'm a statistician. So I'm not sure what to do to fix this problem. Does \"All the misaligned memory reads came from Eigen kernels\" mean I've done something wrong? If so, what have I done wrong? If not, then what can I do to get tensorflow working on my machine?\n", "@johnfrombluff GREETINGS. I don't know what misaligned memory reads came from Eigen kernels\n\nDo you have a basic block size like (or such as) 256 , 512, 1024\n", "Sorry, I'm still confused. What block size are you referring to? Which file(s) should I look at to find what you're talking about?\n\nI'm trying to run example code that comes with the tensorflow distribution. Shouldn't that code run on all supported architectures? Maybe GNU/Linux or my GPU is not supported, but I haven't noticed that in the documentation?\n\nAnd thank you for your attempt to help me!\n", "@johnfrombluff, my earlier comment only meant to point out which part of the program is triggering the error to my colleague. It didn't imply you did something wrong. \n\nYour GPU is GTX 750 Ti, which is gm107. It is supported in theory. But it is a low-end GPU, out of which you might not see a very big speedup. \n\nSince we never saw this problem before, and were unable to reproduce it, the only way to root cause is to ask you to run experiments. However, some steps are not the easiest for users who are not familiar with GPU programming. \n\nAlternatively, you can try a different GPU. Both Titan-X and GTX 1080 are very popular choices. If you still see the same problem with the latest Cuda driver, Cuda SDK and more recent GPU, we would definitely like to investigate. \n", "@zheng-xq, I have a similar setup on Ubuntu 14.04.4 LTS: Cuda v7.5, Cudnn v4, `/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl` with GTX750 Ti and  see the same issue on the mnist example:\n`E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS`.\n\nI went one step further and tried another tf examples, such as alexnet, imagenet, cifar10_multi_gpu_train. It seems they run OK, see attached log, and the problem is in the code of `convolutional.py`.\n\n[CUDA_ERROR_MISALIGNED_ADDRESS.txt](https://github.com/tensorflow/tensorflow/files/313792/CUDA_ERROR_MISALIGNED_ADDRESS.txt)\n", "@zheng-xq See the same error when running MNIST test.\n`E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS`\n\nAlso on Ubuntu 14.04, Cuda v7.5, Cudnn v4.  Use the nvidia-docker using [this image](https://github.com/NVIDIA/nvidia-docker/blob/master/ubuntu-14.04/cuda/7.5/devel/cudnn4/Dockerfile).  \n\nThis is using a GTX 960M (use it for sanity checks before spinning up servers).\n\nI'm calling via [Keras MNIST example](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py).  Same example works fine using Theano backend (via Keras configuration).\n\n[cuda-memcheck.txt](https://github.com/tensorflow/tensorflow/files/315774/cuda-memcheck.txt)\n[environment.txt](https://github.com/tensorflow/tensorflow/files/315767/environment.txt)\n", "I get the same a similar error from running `tf.nn.softmax()`.\nI can give more details if this seems relevant.\n\n#### To reproduce:\n\n`import tensorflow as tf`\n`logits = tf.random_normal((10, 2))`\n`y = tf.nn.softmax(logits)`\n\n`with tf.Session() as sess:`\n    `print(y.eval())`\n\n#### Result:\n\n`I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Quadro K2200\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:03:00.0\nTotal memory: 3.99GiB\nFree memory: 3.20GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2200, pci bus id: 0000:03:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_MISALIGNED_ADDRESS :: No stack trace available\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)`\n", "Same error trying to run basic MNIST example.  \nLinux Y15MATE 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nOUTPUT:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce 840M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:06:00.0\nTotal memory: 2.00GiB\nFree memory: 1.84GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 840M, pci bus id: 0000:06:00.0)\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\n\nProcess finished with exit code 134\n", "I tried running the mnist example after I installed TensorFlow in virtualenv and I got the same error, Ubuntu 16, gcc 5.3.1, python 3.5.1, Driver Version: 361.42, cuda 7.5, this time with a GTX960 with 4GiB, which should be more than enough for this network model:\n\n`python -m tensorflow.models.image.mnist.convolutional`\n\n``` sh\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.33GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\nInitialized!\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\n[1]    25066 abort (core dumped)  python -m tensorflow.models.image.mnist.convolutional\n```\n\n_edit:_ Running `cifar10` model seems to be working just fine...\n", "I've run into the same problem exactly as described by floringogianu except w/ Ubuntu 16.04 and gcc 4.9. Also, i used the --override flag when installing cuda toolkit via the .run script, which may or may not be relevant. cifar10 runs fine. \n", "I've been able to circumvent the issue by following [this setup](https://devtalk.nvidia.com/default/topic/936429/-solved-tensorflow-with-gpu-in-anaconda-env-ubuntu-16-04-cuda-7-5-cudnn-/), which includes switching to python 3.x. and the associated binary. \n\nI also noted that the default build does not target nvidia compute capability 5.0 (that of the GTX 960M), but it works anyways. I attempted to build from source myself but ran into some linking errors which I don't have time to address at the moment. Others have had trouble building for Ubuntu 16.04 as documented in [this thread](https://github.com/tensorflow/tensorflow/issues/2306), with some success. I'd be interested in knowing if anyone else succeeds at this and having that issue closed.\n", "Same problem here. Downgraded to gcc 4.9 to use theano, now tensorflow is broken with CUDA_MISALIGNED_ADDRESS. (bottleneck generation is rapidly improved though)\n", "@johnfrombluff, @tsitsilin, @acowlikeobject, @kalleknast, @dzupin, @floringogianu, @MartianWearables, sorry that we cannot reproduce this problem on our side. I will try to guess where the problem is and see whether it could be fixed. \n\nAmong folks who encountered this problem, what is common is that all used gm107 and gm108 based GPUs. That is compute capability 5.0. TensorFlow binary by default carries compute capability 3.5 and 5.2. The Cuda driver will extract the compute 3.5 PTX and JIT compile into compute 5.0 SASS upon the first run. Given the error message is \"Invalid **local** read of size 16\", my current guess is that the JIT compiler in the Cuda driver is generating wrong code for tf.nn.softmax on GPUs with compute capability 5.0.\n\nHere are a number of things to try:\n1. Enable compute capability 5.0 directly when building from the source code. It is part of the \"configure\". This would enable SASS 5.0 from the static Cuda compiler, and bypasses the JIT Cuda compiler in the Cuda driver. \n2. Install the latest driver from NVIDIA. \n\nIf #1 still fails, we can dump the SASS code from your binary and see what goes wrong. \n", "From an offline conversation, we can confirm that this problem goes away: \n1. Build from source while explicitly setting 5.0 build target. \n2. Or install the latest graphics driver 367.27. \n\nSo it does seem like a JIT compiler issue that goes away the latest driver. \n", "To expand @zheng-xq fix:\n- Get your current driver version: `nvidia-smi | grep \"Driver Version\"` (or by using the GUI: `nvidia-settings`)\n- Go to http://www.nvidia.com/Download/index.aspx and get your driver (`nvidia-smi` might be handy to find your GPU)\n- [Install Tensorflow from sources](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installing-from-sources). A few notes for Ubuntu 16.04 users:\n  - You will have to install: `sudo apt-get install gcc-4.9 g++-4.9`\n  - [Install CUDA](http://askubuntu.com/q/799184/10425) via the script from nvidia, not via repository.\n\nedit: Updating the driver seems not to be that easy (see [ask.SE question](http://askubuntu.com/q/798932/10425)). @zheng-xq Could you please add some details how to build tensorflow setting it explicitly to 5.0? Is it possible to build tensorflow when one installed CUDA via `apt-get` (and thus does not have one cuda folder)?\n", "@MartinThoma, you can set compute version for 5.0 via \"configure\". \n\n> > Please specify a list of comma-separated Cuda compute capabilities you want to\n> > build with. You can find the compute capability of your device at:\n> > https://developer.nvidia.com/cuda-gpus.\n> > Please note that each additional compute capability significantly increases your\n> > build time and binary size. [Default is: \\\"3.5,5.2\\\"]: 3.5\n\nThe same thing applies to Cuda and Cudnn paths. \n\n> > Please specify the location where CUDA 7.5 toolkit is installed. Refer to\n> > README.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda\n\nWhat does the structure of the Cuda binaries look like when you do apt-get? You can download and install directly from NVIDIA. If that is not possible, if the file directory is similar, you can pass that directory to \"configure\". If the file directory is completely different for some reason, I guess you can build another directory with symlinks that mimics the downloaded Cuda directory.\n\nhttps://developer.nvidia.com/cuda-downloads\n\nHope that helps. \n", "> What does the structure of the Cuda binaries look like when you do apt-get?\n\nHere are some of the files:\n\n```\n/usr/bin/nvcc\n/usr/bin/nvidia-smi\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\n/usr/include/cudnn.h\n```\n\nWhen `configure` of tensorflow asks me where the cuda folder is, I pointed to `/usr/lib/x86_64-linux-gnu/`. But then it complained that there is no `/usr/lib/x86_64-linux-gnu/lib64` (or something similar).\n\nI think I'll just install it manually, because I run into those problems quite regularly.\n", "If those are not symlinks, I would recommend you to manually reinstall that. \n", "Was facing same problem, get \"CUDA_ERROR_MISALIGNED_ADDRESS\" with MNIST samples. Below are the environment versions.\nUbuntu - 16.04\nDriver - 361.45 or 364.19\nCUDA - 7.5\nCUDNN - 4.0\nTF - 0.9\ngcc - 4.9\n\nDowngraded tensorflow to 0.8 and this error does not show up. However, started facing a new problem. TF would just hang, nvidia-smi shows temperature at 68 C and the process stops responding. It should probably the driver issue. Installed all the latest versions (except TF) and its all fine now.\nUbuntu - 16.04\nDriver - 367.35\nCUDA - 8.0 RC\nCUDNN - 5.0\nTF - 0.8\ngcc - 5.41 (default that comes with Ubuntu 16.04 install)\n", "I have the same error:\r\n`017-09-07 11:20:05.454046: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-09-07 11:20:05.454119: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n2017-09-07 11:20:05.454124: E tensorflow/stream_executor/cuda/cuda_blas.cc:365] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2017-09-07 11:20:05.454183: W tensorflow/stream_executor/stream.cc:1601] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n^[tools/jumbo/bin/python4.8: line 11: 32638 Aborted                 /opt/compiler/gcc-4.8.2/lib/ld-linux-x86-64.so.2 --library-path $SCRIPTPATH/../lib:/opt/compiler/gcc-4.8.2/lib:$LD_LIBRARY_PATH $SCRIPTPATH/python \"$@\"`\r\n\r\nEnv:\r\nGPU: Tesla P40\r\nDriver: 375.20\r\ncudnn\uff1a 5.1.10\r\ngcc: 4.8.2\r\nos: centos 4.3\r\n"]}, {"number": 2809, "title": "Stopgap measure to fix session_bundle genrule for py3.5", "body": "", "comments": []}, {"number": 2808, "title": "CLOSED Discard PR", "body": "Greetings @tensorflow-jenkins. This is my first contribution to a Google open source project and I'd like to read your Contributor License Agreement (CLA) before I sign it\n", "comments": []}, {"number": 2807, "title": "Slow quantized graph", "body": "1. On Ubuntu 15.10 with CUDA 7.5, cuDNN 7.0, tensorflow-0.9.0rc0, ran \"tensorflow/examples/label_image/\" application by taking inception-v3 graph and roughly measure the elapsed time. \n2. Then take \"tensorflow/contrib/quantization/tools:quantize_graph\" to quant inception-v3, rebuilt application by giving \n   \n   ```\n   \"//tensorflow/contrib/quantization:cc_ops\",\n   \"//tensorflow/contrib/quantization/kernels:quantized_ops\",\n   ```\n\ninto \"tensorflow/examples/label_image/BUILD\" and redo the same classification and measure the time. \n\nBefore/After quantization, elapsed time were 6 seconds vs. 17 seconds, i.e. quantization doubled the inference time? \n\nThe results looks ok as below so I think I was running it correctly. \nBefore\n- military uniform (866): 0.647299\n- suit (794): 0.0477195\n- academic gown (896): 0.0232407\n- bow tie (817): 0.0157355\n- bolo tie (940): 0.0145023\n\nAfter\n- military uniform (866): 0.703474\n- suit (794): 0.0248454\n- bow tie (817): 0.0171362\n- bolo tie (940): 0.0171362\n- academic gown (896): 0.0164432\n\nMy tensor flow was built as CPU only. Have also tried to enable GPU while the timing didn't change. Do we know what the expected performance would be? \n", "comments": ["Maybe it's possible that the reference (slow) implementations are used when not running on Android/ARM.  @petewarden would know more.\n", "Thanks for the response, vrv. \n\nI am running on Intel i7-3770 with Nvidia GM200 [GeForce GTX TITAN X] GPU (I am profiling CPU only anyways). The profiling is very rough as I am just taking bash timer. The complete script, which actually comes from Pete's blog is as following.\n\n```\nbazel build tensorflow/contrib/quantization/tools:quantize_graph\nbazel build tensorflow/examples/label_image:label_image\n\ncurl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz\ntar xzf /tmp/inceptionv3.tgz -C ./tensorflow/examples/label_image/data/\n\necho \"Quantizing Incepton-v3 graph as 8bits mode\"\nbazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \\\n--input=./tensorflow/examples/label_image/data/classify_image_graph_def.pb \\\n--output_node_names=\"softmax\" --output=./tensorflow/examples/label_image/data/quantized_graph_eightbit.pb \\\n--mode=eightbit\n\necho \"Original classify_image_graph_def.pb\"\nSTARTTIME=$(date +%s)\n./bazel-bin/tensorflow/examples/label_image/label_image \\\n--graph=./tensorflow/examples/label_image/data/classify_image_graph_def.pb\nENDTIME=$(date +%s)\necho \"It takes $(($ENDTIME - $STARTTIME)) seconds to complete this task...\\n\\n\"\n\n\necho \"Quantized as 8 bits\"\nSTARTTIME=$(date +%s)\n#command block that takes time to complete...\n#........\n./bazel-bin/tensorflow/examples/label_image/label_image \\\n--graph=./tensorflow/examples/label_image/data/quantized_graph_eightbit.pb \\\n--input_width=299 \\\n--input_height=299 \\\n--input_mean=128 \\\n--input_std=128 \\\n--input_layer=\"Mul:0\" \\\n--output_layer=\"softmax:0\"\nENDTIME=$(date +%s)\necho \"It takes $(($ENDTIME - $STARTTIME)) seconds to complete this task...\\n\\n\"\n```\n", "By doing this, \n\n```\nmodel_filename = os.path.join(\n        \"./tensorflow/tensorflow/examples/label_image/data\", 'quantized_graph_eightbit.pb')\n    print model_filename\n    with gfile.FastGFile(model_filename, 'rb') as f:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(f.read())\n      writer=tf.train.write_graph(graph_def, './tensorflow/tensorflow/examples/label_image/data', 'quantized_graph.pb')\n```\n\nI print out the graph and load into tensorboard to visually check it and looks  2  pool layers\n- pool between conv_2 and conve_3\n- pool_1 between conv_4 and mixed\n  \n  goes away in the \"quantized\" version graph. \n\nIs there any reason? I am quite new to tensor flow so there could be something wrong on my side. \n\nThanks.\n", "I just tried giving -pg as\n\n```\nbazel build --copt=\"-pg\" --cxxopt=\"-pg\" tensorflow/examples/label_image:label_image\n```\n\nand rerun the quantized graph after rebuild. It then took 245 seconds to finish the classification. I did not find gprof log file produced and results even did not match which confused me:\n- military uniform (866): 0.579775\n- suit (794): 0.0243188\n- bolo tie (940): 0.0200896\n- academic gown (896): 0.0179139\n- Windsor tie (935): 0.010099\n\nAny recommended way to do proper profiling? \n\nThanks.\n", "What are the results running on GPU?\n", "After quant, on GPU:\nmilitary uniform (866): 0.703474\nsuit (794): 0.0248454\nbow tie (817): 0.0171362\nbolo tie (940): 0.0171362\nacademic gown (896): 0.0164432\n\nElapsed time is actually the same as CPU only build, which is 6 and 17 seconds before/after quant respectively so it quite confused me. \n", "Can you try using https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark and let me know what you get? There are some known optimizations for eight-bit CPU we're working on, but it would be useful to see where the time is going from the profiling logs you'll get from that tool.\n", "sure. The machine has launched some over weekend testing so I will do it on coming Monday. Thanks. \n", "`\n[quant_inceptionv3.txt](https://github.com/tensorflow/tensorflow/files/323300/quant_inceptionv3.txt)\n\n`\nAttached output from benchmarking on quantized inception v3. \n", "I have the same problem and my operation is similary to \"changyun79\". I test inception_v3 on my computer and the results were: \n         Before/After quantization, elapsed time were 36 seconds vs. 108 seconds per 10 images\n         Before/After quantization, my trained inception_v3 size were 92M vs. 22M\n", "my result is \uff08for one image\uff09\n- inception-v3 : 8s\n- inception-v3-quantized : 20s\n- inception-v3-stripped : 7s\n\nwith \n- tf0.9.0-gpu\n- TeslaK40-cuda7.5-cudnn5.0\n- ubuntu1404 LTS\n", "I tried a small network on GPU and get similar results.  ( https://stackoverflow.com/questions/38775675/why-my-tensorflow-network-becomes-slower-after-applying-the-quantization-tools-o\n )  It may because  GeForce GTX  has less integer computing cores? \n", "i get the same result, any one had solved?\n", "I do get the same slow down on a single CPU. I ran Intel vtune profiler (for CPUs) on the same test image and it seems that its just not optimized. There needs to be better optimization for gemmlowp. Comparing it with the FP32 implementation that is optimized to a certain extent does better.\n", "Has anybody verified that quantized computations are taking place on their GPU? I have not been able to get them to run on the GPU. Everything runs okay - GPU memory is being reserved and blocked off, but the computation is still taking place on the CPU. \n\n@petewarden it appears that maybe the quantized ops don't have GPU support, or that it is not optimized quite yet. Any insight on this, or anything in the works on this front?\n", "Quantized ops currently only work on the CPU, because most GPUs don't support eight-bit matrix multiplications natively. I have just seen that the latest TitanX Pascal cards offer eight-bit support though, so I'm hoping we will be able to use that in the future.\n", "@petewarden Should we expect the quantized model to speed up inference as of `0.11.0rc1` on mobile? I just tried one locally and it seems slower as also mentioned in #4434. Thanks\n", "@petewarden If I quantize the graph and run it on iOS (CPU), I too get about 3 times worse performance than running the unquantized version.", "I met the same problem, the quantized model is much slower than the unquantized one, are there any solution now?", "Anyone tried on TitanX? \r\n\r\n@lihungchieh did you try compiling the latest code with `--config=opt`? It should have `-march=native`, perhaps that helps on CPU.", "@petewarden I ran the benchmark mentioned above on 32-bit ARM using the google inception model.But I got the result below, it shows that the 8-bit quantized graph is much slower than the 32-bit float one. Is this result the reasonable one and what's the performance of the latest version on 32-bit ARM. This really bothered me a lot. Looking forward to your response. Thanks.\r\n\r\n**fp result:**\r\n\r\nnative : stat_summarizer.cc:392 ============================== Top by Computation Time ==============================\r\nnative : stat_summarizer.cc:392                      [node type]          [start]         [first]        [avg ms]            [%]          [cdf%]          [mem KB]      [Name]\r\nnative : stat_summarizer.cc:392                           Conv2D          124.403         274.567     165.023    14.748%         14.748%          2408.448      conv2d2_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D          472.443         103.640      84.950     7.592%         22.340%           602.112      mixed3b_3x3_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D            0.298         133.612      74.971     6.700%         29.041%          3211.264      conv2d0_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D          913.577          55.950      47.739     4.267%         33.307%           250.880      mixed4e_3x3_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D          366.220          55.346      45.836     4.096%         37.403%           401.408      mixed3a_3x3_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D          841.525          50.361      38.759     3.464%         40.867%           225.792      mixed4d_3x3_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D          769.854          41.145      33.240     2.971%         43.838%           200.704      mixed4c_3x3_pre_relu/conv\r\nnative : stat_summarizer.cc:392                           Conv2D          567.045          40.916      31.938     2.854%         46.692%           301.056      mixed3b_5x5_pre_relu/conv\r\nnative : stat_summarizer.cc:392                              LRN          300.748          25.291      25.403     2.270%         48.963%          2408.448      localresponsenorm1\r\nnative : stat_summarizer.cc:392                           Conv2D          706.635          29.413      24.777     2.214%         51.177%           175.616      mixed4b_3x3_pre_relu/conv\r\nnative : stat_summarizer.cc:392\r\nnative : stat_summarizer.cc:392 ============================== Summary by node type ==============================\r\nnative : stat_summarizer.cc:392                      [Node type]          [count]         [avg ms]      [avg %]     [cdf %]       [mem KB]\r\nnative : stat_summarizer.cc:392                           Conv2D               57          900.237      80.466%     80.466%      12904.640\r\nnative : stat_summarizer.cc:392                          MaxPool               13           76.670       6.853%     87.319%       5666.752\r\nnative : stat_summarizer.cc:392                          BiasAdd               58           56.362       5.038%     92.357%      12908.672\r\nnative : stat_summarizer.cc:392                              LRN                2           34.084       3.047%     95.403%       3211.264\r\nnative : stat_summarizer.cc:392                             Relu               57           21.780       1.947%     97.350%      12904.640\r\nnative : stat_summarizer.cc:392                           MatMul                1           18.691       1.671%     99.020%          4.032\r\nnative : stat_summarizer.cc:392                           Concat                9            8.364           0.748%         99.768%       4939.200\r\nnative : stat_summarizer.cc:392                            Const              118            1.951       0.174%     99.942%          0.000\r\nnative : stat_summarizer.cc:392                          AvgPool                1            0.387       0.035%     99.977%          4.096\r\nnative : stat_summarizer.cc:392                          Softmax                1            0.128       0.011%     99.988%          4.032\r\nnative : stat_summarizer.cc:392                               <>                1            0.108       0.010%     99.998%          0.000\r\nnative : stat_summarizer.cc:392                          Reshape                1            0.021       0.002%    100.000%          0.000\r\nnative : stat_summarizer.cc:392\r\nnative : stat_summarizer.cc:392 Timings (microseconds): count=50 first=1460227 curr=1078211 min=1051457 max=1460227 avg=1.11893e+06 std=84555\r\nnative : stat_summarizer.cc:392 Memory (bytes): count=50 curr=52547328(all same)\r\nnative : stat_summarizer.cc:392 370 nodes defined 319 nodes observed\r\n\r\n**quantized result**\r\n\r\nnative : stat_summarizer.cc:392 ============================== Top by Computation Time ==============================\r\nnative : stat_summarizer.cc:392                      [node type]          [start]         [first]    [avg ms]        [%]          [cdf%]          [mem KB]      [Name]\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D          236.817         246.238     240.406    14.122%         14.122%          2408.456      conv2d2_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D          779.869         125.167     126.172     7.412%         21.534%           602.120      mixed3b_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D            8.386         119.831         104.379         6.131%         27.665%          3211.272      conv2d0_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D         1431.287          70.142      69.877     4.105%         31.770%           250.888      mixed4e_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D          628.304          59.150      59.561     3.499%         35.268%           401.416      mixed3a_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D         1291.839          57.775      56.634     3.327%         38.595%           225.800      mixed4d_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D         1191.274          44.160      44.362     2.606%         41.201%           200.712      mixed4c_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D          929.076          41.119      41.609     2.444%         43.645%           301.064      mixed3b_5x5_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D         1102.009          34.599      34.248     2.012%         45.657%           175.624      mixed4b_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D         1650.254          30.406      31.038     1.823%         47.480%            75.272      mixed5b_3x3_pre_relu/conv_eightbit_quantized_conv\r\nnative : stat_summarizer.cc:392 ============================== Summary by node type ==============================\r\nnative : stat_summarizer.cc:392                      [Node type]          [count]         [avg ms]      [avg %]     [cdf %]       [mem KB]\r\nnative : stat_summarizer.cc:392                  QuantizedConv2D               57         1217.166      71.515%     71.515%      12905.096\r\nnative : stat_summarizer.cc:392                       Requantize              116          191.868      11.273%     82.788%       6455.264\r\nnative : stat_summarizer.cc:392                 QuantizedBiasAdd               58           95.586       5.616%     88.405%      12909.136\r\nnative : stat_summarizer.cc:392         \t        QuantizedMaxPool               13           42.846       2.517%     90.922%       1416.792\r\nnative : stat_summarizer.cc:392                  QuantizedConcat                9           34.559       2.031%     92.953%       1234.872\r\nnative : stat_summarizer.cc:392              RequantizationRange              116           33.522       1.970%     94.922%          0.928\r\nnative : stat_summarizer.cc:392                              LRN                2           31.049           1.824%         96.746%       3211.264\r\nnative : stat_summarizer.cc:392                  QuantizedMatMul                1           26.198       1.539%     98.286%          4.040\r\nnative : stat_summarizer.cc:392                       QuantizeV2                3            9.296       0.546%     98.832%        953.368\r\nnative : stat_summarizer.cc:392                    QuantizedRelu               57            9.132       0.537%     99.368%       3226.616\r\nnative : stat_summarizer.cc:392                            Const              356            4.124       0.242%     99.611%          0.000\r\nnative : stat_summarizer.cc:392                       Dequantize                3            2.933       0.172%     99.783%       3215.296\r\nnative : stat_summarizer.cc:392                              Min                3            1.459       0.086%     99.869%          0.024\r\nnative : stat_summarizer.cc:392                              Max                3            1.315       0.077%     99.946%          0.024\r\nnative : stat_summarizer.cc:392                 QuantizedAvgPool                1            0.454       0.027%     99.973%          1.032\r\nnative : stat_summarizer.cc:392                               <>                1            0.216       0.013%     99.985%          0.000\r\nnative : stat_summarizer.cc:392                          Softmax                1            0.122       0.007%     99.993%          4.032\r\nnative : stat_summarizer.cc:392                          Reshape                3            0.092       0.005%     99.998%          0.000\r\nnative : stat_summarizer.cc:392                 QuantizedReshape                1            0.033       0.002%    100.000%          0.008\r\nnative : stat_summarizer.cc:392\r\nnative : stat_summarizer.cc:392 Timings (microseconds): count=50 first=1739035 curr=1690331 min=1684873 max=1782461 avg=1.70236e+06 std=20766\r\nnative : stat_summarizer.cc:392 Memory (bytes): count=50 curr=45537792(all same)\r\nnative : stat_summarizer.cc:392 812 nodes defined 804 nodes observed", "I have the similar problem. I quantify my own cnn model, before quantization, the size of model is 11M, and inference takes 72 ms per 128 small images; after quantization, size become 2.8M, but takes 236ms. I run the experiment in OS X ,2.6 GHz Intel Core i5. ", "I had the similar problem. Finally, I found that there are some comments in ```tensorflow/tensorflow/core/kernels/quantized_conv_ops.cc```\r\n\r\n```C++\r\n// This means that multiple ops can't be run simultaneously on different\r\n    // threads, because we have a single shared resource. The platforms this is\r\n    // aimed at have intra-op parallelism as their focus though, so it shouldn't\r\n    // be an issue.\r\n    mutex_lock lock_buffer(im2col_buffer_resource->mu);\r\n    core::ScopedUnref unref_buffer(im2col_buffer_resource);\r\n    T1* im2col_buffer = im2col_buffer_resource->data;\r\n```\r\nSo, to get a good performance, you should set ```inter_op_parallelism_threads=1``` when you use ```QuantizedConv2D``` op.\r\nBut, even you do that, you will find that although the conv ops get faster because of the use of ```gemmlowp```, quantization will add some new ops like ```RequantizationRange```, ```Requantize``` to your model, which are very time-consuming. So, the quantized model will still be slower than the original one.", "I have same issue on Inception V3.", "The quantization is aimed at mobile performance, so most of the optimizations are for ARM not x86. We're hoping to get good quantization on Intel eventually, but we don't have anyone actively working on it yet.", "Any progress? After quantize, network is 3 times slower with bad performance at accuracy", "Same issue described by @clumsydzd here.", "what's the expected performance of inception-v3 on arm-v7? @petewarden ", "@petewarden\r\nAny progress for GPU support?", "@drpngx,\r\nAny example to use quantized graph with GPU for inference?", "/CC @aselle \r\n\r\n@nealwu do we have something in the models repo?", "Not that I know of. Maybe @suharshs would know.", "Not yet. I am not aware of progress on GPU support. ", "Is there any news whether GPUs support eight-bit operations? Or any other optimization suggestions from tensorflow side?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Does anyone know which GPUs are optimised for 8 bit calculations? The best info I have found so far is [this article](https://www.microway.com/knowledge-center-articles/comparison-of-nvidia-geforce-gpus-and-nvidia-tesla-gpus/) which compares the floating point performance between 64 bit and 16 bit operations. I'm sort of extrapolating that GPUs which had a decrease in performance on 16 bits would have a further decrease with 8 bits, and those with an increase *might* have a further increase on 8 bits. I hope this helps, but I would appreciate confirmation from someone :)", "We are focusing our eight-bit efforts on TF Lite (visible at tensorflow/contrib/lite), so we aren't expecting TensorFlow's quantized performance to improve in cases where it's not currently fast. These tend to be on x86 platforms (we're concentrating on ARM performance for mobile), and for models that use ops that we don't have quantized implementations for (which is most models outside a few vision-related ones we've optimized for).\r\n\r\nSince we're not likely to see changes in this area soon, I'm closing this as infeasible. Pull requests or other help in this area would be very welcome of course!", "As far as I understand from this entire thread is there are no plans to put 8bit on x86 or desktop CPUs. I tried with transform graph as well as building and running TfLite on windows but still its very slow when it comes to quantized model. And if there no plans to support that, what should we users of tensorflow do? How do we get this thing supported?"]}, {"number": 2806, "title": "Building TF with custom GCC requires hardcoded ld,nm and as", "body": "Hi! I am using Fedora 23 to build TensorFlow with GPU support.\n\nGPU support requires a specific GCC version, 4.9 for successful compilation. Since this version is not available from the Fedora Package repositories, it has to be compiled from source. Unfortunately, there are a few obstacles using this version with bazel.\n\nOne of those is the following:\nAfter configuring TensorFlow with the self-compiled GCC, and running bazel, the compilation stops with the following message:\n\n`gcc: error trying to exec 'as': execvp: No such file or directory`\nor\n`gcc: error trying to exec 'nm': execvp: No such file or directory`\nor\n`gcc: error trying to exec 'ld': execvp: No such file or directory`\n\nTo work around this, one can compile GCC by hardcoding the paths to those tools, by adding this to the configuration line of GCC:\n`--with-ld=/bin/ld --with-nm=/bin/nm --with-as=/usr/bin/as`\n\nThis does seem rather strange, however, because those programs are on the path, and the custom GCC without bazel is capable of producing working binaries just fine. I feel that in a well-working build system, this kind of hacks should not be necessary.\n\nThis is probably a bazel bug, but since I am compiling TensorFlow with it and I cannot be sure that it's not a TF issue, I am creating this here.\nPlease ping bazel devs as required.\n", "comments": ["@davidzchen\n\nIt seems that you have taken to fixing TF builds with a custom compiler? If so, than you might also be interested in this oddity. Thanks for your work!\n", "I believe this is indeed a bazel problem. I wouldn't know what we can do inside TensorFlow to address this. I would file an issue for bazel.\n", "I created Issue https://github.com/bazelbuild/bazel/issues/1713 in bazel for this.\n", "I have got the same problem gcc: error trying to exec 'as': execvp: No such file or directory.\nand I solved it by copy and paste the /usr/bin/as to the same folder as gcc 5.3(/usr/local/bin/gcc)\n"]}, {"number": 2805, "title": "Update iOS Eigen version", "body": "Update Eigen version in iOS projects to 5f86b31739cd\n", "comments": ["Can one of the admins verify this patch?\n", "Superseded by Pull Request #2841.\n"]}]