[{"number": 3648, "title": "SKFlow - RNN - Tuple does not have dtype", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: gcr docker image running in Ubuntu 16.04\n### Steps to reproduce\n\nContact me via private message or twitter (@Data4Bots) and I can give you access to my Python Notebook to replicate full issue and let you see the entire thing.\n### What have you tried?\n1.  Not really sure where to begin with this one, it seems pretty odd on the surface to me.  I'll be digging in and taking suggestions.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n$$$$$$$$$\n$ Output $\n$$$$$$$$$\nAttributeErrorTraceback (most recent call last)\n<ipython-input-9-ad53b65c4640> in <module>()\n     22                         continue_training=True)\n     23 \n---> 24 classifier.fit(zip(_train)[0], zip(_test)[1])\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in fit(self, X, y, monitor, logdir)\n    216         self._data_feeder = setup_train_data_feeder(X, y,\n    217                                                     self.n_classes,\n--> 218                                                     self.batch_size)\n    219 \n    220         if monitor is None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.pyc in setup_train_data_feeder(X, y, n_classes, batch_size)\n     97                              \"streaming learning to work.\")\n     98         data_feeder_cls = StreamingDataFeeder\n---> 99     return data_feeder_cls(X, y, n_classes, batch_size)\n    100 \n    101 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.pyc in **init**(self, X, y, n_classes, batch_size, random_state)\n    187 \n    188     def **init**(self, X, y, n_classes, batch_size, random_state=None):\n--> 189         x_dtype = np.int64 if X.dtype == np.int64 else np.float32\n    190         y_dtype = np.int64 if n_classes > 1 else np.float32\n    191         self.X = check_array(X, dtype=x_dtype)\n\nAttributeError: 'tuple' object has no attribute 'dtype'\n\n$$$$$$$$$$$$$$$\n$ Data Snapshot $\n$$$$$$$$$$$$$$$\nLabeled\n\n [array([128, 128, 128, ..., 128, 128, 128], dtype=uint8) 1]\n [array([128, 128, 128, ..., 128, 128, 128], dtype=uint8) 3]\n [array([128, 128, 128, ..., 128, 128, 128], dtype=uint8) 3]\n [array([128, 128, 128, ..., 128, 128, 128], dtype=uint8) 2]\n\nSplit using zip(_data)[0] to grab the list of arrays and zip(_data)[1] to grab the labels\n\n$$$$$$$$$$$$\n$ Some Code $\n$$$$$$$$$$$$$\nindices = np.random.permutation(data)\nvalid_cnt = int(len(data) \\* 0.25)\n\ntest, train = indices[:valid_cnt], indices[valid_cnt:]\n\ndef listify(x):\n    return [x]\n\nnum_classes = 3\n# Hyper Params\n\nstate_size = 3\nnum_layers = 4\nsteps = 100\nlearningRate = 0.01\n# One line model\n\nclassifier = skflow.TensorFlowRNNClassifier(\n                        rnn_size=state_size, \n                        n_classes = num_classes, \n                        cell_type='rnn',\n                        input_op_fn = listify,\n                        num_layers = num_layers,\n                        steps = steps,\n                        optimizer = 'Adam',\n                        learning_rate = learningRate,\n                        continue_training=True)\n\nclassifier.fit(zip(_train)[0], zip(_test)[1])\n\n$$$$$$$$$$$$\n$ Final Notes $\n$$$$$$$$$$$$\nI'm just learning this thing, so bear with it.  Let me know what I can do to help.  Again, get hold of me and I'll open up the Jupyter notebook to you.\n", "comments": ["Solved the issue.  The data needs to be provided to SKFLow as an ndarray and not a list or tuple or anything else.  Simply convert the data using np.array(data) for data.  Labels appear to be OK without this treatment.\n"]}, {"number": 3647, "title": "Mismatch in gradient of 'abs' for complex values", "body": "... in the `def abs(x, name=None):` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py)\n\nThe abs correctly returns complex abs if the input is complex, but the gradient always returns `math_ops.sign(x)`. There seems to be a `_ComplexAbsGrad` though, so maybe that should be used?\n\nAnyone have time for this? Clearly it's not that urgent (how many people are using complex valued tensors anyway), but maybe it should at least be flagged in the code...\n", "comments": ["The gradient registry is based on the names of C++ ops, not the names of Python functions.  The code is correct.\n"]}, {"number": 3646, "title": "Added libcupti path to Linux GPU support documentation", "body": "Added path to libcupti in the documentation for enabling Linux GPU support.\n\nWithout this library being in LD path certain tutorials (e.g. https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html) crash with message \"tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcupti.so.\"; if installed under these instructions and run under e.g. Jupyter.\n\nThis is also referenced in tensorflow#2626\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please\n", "@tensorflow-jenkins, test this please\n", "@martinwicke @vrv Should be okay to merge assuming approval from @zheng-xq . The lingering unfinished \"Linux GPU PIP\" job is due to a job name change that I made and can be ignored.\n"]}, {"number": 3645, "title": "TensorShape dimensions size should be of type uint64, not int64", "body": "According to core/framework/tensor_shape.cc:171:\n\n``` cpp\nvoid TensorShape::AddDim(int64 size) {\n    CHECK_GE(size, 0);\n```\n\nTensorShape dimensions size can only be greater or equal to 0. **Thus, its type should be uint64_t.**\n\nI even wonder if **size_t** would not be more appropriate.\n", "comments": ["Use of this type is deliberate, and reflects Google coding guidelines.  Signed int types are to be preferred since overflow/underflow errors are more likely to be caught.  Loss of one bit of range is not of concern in this case. \n"]}, {"number": 3644, "title": "Multiple tasks in the same server may cause OOM in distributed mode with GPUs", "body": "Following the [official tutorial](https://www.tensorflow.org/versions/master/how_tos/distributed/index.html) of distributed TensorFlow, we find that the ps and works will use the first GPU by default which may cause OOM in distributed mode.\n\nIf we don't set `CUDA_VISIBLE_DEVICES`, all the ps tasks may see all the GPUs and use the first one by default. And when I start all the ps and worker processes, OOM occurs and ps uses most of GPU's memory even though no job running.\n\nIs that possible to optimize the algorithm of scheduler for more intelligence, such as using CPU for ps task and place operations in different GPUs instead of the first one?\n\nOne of the solution is specifying `CUDA_VISIBLE_DEVICES` for each task. Or we can use `with tf.device()` which may be only use for model parallel in distributed mode.\n### Environment info\n\nOperating System: \n\n```\nUbuntu 14.04\n```\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n# ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 Jun 30 04:17 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 Jun 30 04:17 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 Jun 30 04:17 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Jun 30 04:17 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n\n```\n0.9.0\n```\n", "comments": ["Your observation is correct.  It is a deliberate design choice that a TF process will, unless instructed otherwise, use visible GPUs in order, and attempt to use all of the memory on each device.  This approach can work well when processes sharing a server are in a virtual environment that makes visible only the GPUs that the process is permitted to use.  As you note, setting a different value for `CUDA_VISIBLE_DEVICES` in each process is a similar solution.\n\nUnfortunately, there's no single best solution for distribution, and TF does not attempt to provide one.   Packing multiple ps shards and worker shards onto a single server is a convenient way of distributing, particularly if one has a large virtualized server farm, but if you really care about maximizing performance it may be necessary to customize your model to a particular server architecture, and run one process per server, where each process is aware of all the GPUs on that server and potentially takes advantage of local communication between them.  This kind of approach would involve the `with tf.device()` construct you noted.\n\nIt's probably feasible to do a somewhat more effective job of scheduling Ops onto devices than TF does at the moment, but doing so is actually a pretty hard problem.  Although we've looked into this, we've continued to find that a little bit of attention from the programmer when setting up the model and execution plan is usually sufficient for a good solution.\n", "Thanks for confirming and detailed explaination \ud83d\ude03 \n\nTensorFlow is flexible enough with `CUDA_VISIBLE_DEVICES` and `with tf.device()` to archive any architecture. We are also looking forward to the optimization of scheduling. Maybe adding the rule of using CPU for ps and placing operations randomly in all GPUs for worker could help, especially for beginners.\n"]}, {"number": 3643, "title": "Remove some signed/unsigned integer comparison warnings", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Adding @keveman for review. \n\nHowever, I think TensorFlow in general prefers signed int over unsigned int. But I'll leave it up to @keveman. \n", "Changes mainly look like the ones from this pull request, accepted at the time: https://github.com/tensorflow/tensorflow/pull/1099\n", "@tensorflow-jenkins test this please.\n", "@sdenel it looks like some of these were already fixed. Can you please rebase your PR? In general, this PR looks fine to me apart from the change to ExecutorState.\n", "@rmlarsen I just rebased my PR.\n", "@tensorflow-jenkins test this please\n", "@sdenel It looks like you have to rebase again to pick up the changes I pushed earlier to day, sorry about the duplicate work.\n", "@rmlarsen done!\n", "@sdenel I'm still seeing a bunch of changes to the shape inference code in array_ops.cc. did you fail to resolve a merge conflict?\n", "@rmlarsen It looks like I failed my first rebase :) Sorry for the mess, it should ok now.\n", "@tensorflow-jenkins test this please\n\n@sdenel Thanks! \n", "@tensorflow-jenkins test this please.\n", "@sdenel thanks for the cleanup!\n"]}, {"number": 3642, "title": "fix tanh inconsistent", "body": "#2234\n1. use tag dispatching to dispatch complex number and non-complex number, because no max/min operator for complex number, if not use dispatching, build will fail \n2. packetOp is inherited from scalar_tanh_op in eigen\n3. complex number can't be packet, so just use numext::tanh(std::tanh) to compute\n", "comments": ["Can one of the admins verify this patch?\n", "Adding @benoitsteiner to review. \n\nIt is generally not a good idea to put so much operator specific logic in a common header file. \n", "@zheng-xq we can write two more scalar_tanh_op_google template for complex<double> and complex<float> specifically, so we can remove  tag dispatching\n", "Taking over sync rotation for the week. Should we wait for the eigen PR to go through and then update / merge this afterwards? Or can this PR be closed if we solve the issue in eigen?\n", "The fix is now merged in Eigen. All that remains is to upgrade TensorFlow to pull the latest version of Eigen. This should happen in the next few days.\n", "OK, closing this PR per @benoitsteiner's comment.\n"]}, {"number": 3641, "title": "1171 (reopen)", "body": "undefined reference to symbol 'ceil@@GLIBC_2.2.5'\nI get the same and a great many Info/Warnings.\nthis issue was patched and merged, but I simply follow the syntaxnet instructions (from the readme.md):\n\ngit clone --recursive https://github.com/tensorflow/models.git\ncd models/syntaxnet/tensorflow\n./configure\ncd ..\nbazel test syntaxnet/... util/utf8/...\n\nit still gives:\n/home/tf/.cache/bazel/_bazel_sam/5cd71b2b91989f3dd022ee2c43ab916c/external/org_tensorflow/tensorflow/tools/proto_text/BUILD:31:1: Linking of rule '@org_tensorflow//tensorflow/tools/proto_text:gen_proto_text_functions' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/proto_text/gen_proto_text_functions -pthread -no-canonical-prefixes -B/usr/bin -B/usr/bin -pass-exit-codes '-Wl,--build-id=md5' ... (remaining 12 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/usr/bin/ld: bazel-out/host/bin/external/org_tensorflow/tensorflow/core/liblib_internal.a(numbers.o): undefined reference to symbol 'ceil@@GLIBC_2.2.5'\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\n\nis this a Bazel issue? that's a fairly obscure build system, what should I do?\n(Ubuntu 16.04, w cudnn and cuda 8 bazel versionBuild label: 0.2.2b)\n", "comments": ["Looks like a bazel issue.  @martinwicke ?\n", "The fix elsewhere fixes it, need to add -pthread and -lm to\n~/models/syntaxnet/tensorflow/tensorflow/tools/proto_text$ ls\nBUILD <-- this file's cc_lib structure\n"]}, {"number": 3640, "title": "Output for tfrecords files is not reproducible", "body": "### Environment info\n\nOperating System:\nOS: Debian testing stretch  \nKernel: x86_64 Linux 4.5.0-1-amd64\n\nTensorflow\nVersion: 0.9.0\n### Steps to reproduce\n\nRunning this script gives a ~/test.tfrecords file. \nRunning the script repeatedly gives different hashes for the file. \n\n``` python\nimport os\nimport argparse\n\nimport tensorflow as tf\nimport numpy as np\n\nMAXIMUM_SEQUENCE_LENGTH = 1000\n\ndef export(data, name, args):\n    num_examples = len(data)\n\n    fh_output = os.path.join(os.path.expandvars(args.output_directory), name + '.tfrecords')\n    print('Writing', fh_output)\n    writer = tf.python_io.TFRecordWriter(fh_output)\n    for index in range(num_examples):\n        ## prepare things to save\n        seq_x1 = data[index][0].tostring()\n        seq_x2 = data[index][1].tostring()\n\n        example = tf.train.Example(features=tf.train.Features(feature={\n            'x1': _bytes_feature(seq_x1),\n            'x2': _bytes_feature(seq_x2),\n            }))\n        writer.write(example.SerializeToString())\n    writer.close()\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\nif __name__ == '__main__':\n    tf.set_random_seed(11)\n    np.random.seed(11)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output_directory', type=str, default='$HOME/', help='Directory to write the converted result')\n    # parser.add_argument('--validation_size', type=int, default=3000, help='Number of examples to separate from the training data for the validation set.')\n    args = parser.parse_args()\n\n    train = [( np.zeros(1000), np.ones(1000), 500 )]\n\n    ## Write data to training file\n    export(train, 'test', args)\n```\n\nI wonder why I cannot produce the same binary file again, if I rerun the script. Does this mean the content is different? I didn't find any option to set a seed either. \n", "comments": ["When I run this program I get the same result every time, with identical md5sum hashes.  What happens if you print the protobuf, instead of using SerializeToString(), are they different?  \n", "I am very sorry, in my haste, I seem to have given you a bad toy example. Very strangely, in my setup if I execute a slightly modified example from above, I get different hashes: \n`\nexample = tf.train.Example(features=tf.train.Features(feature={\n'x1': _bytes_feature(seq_x1),\n'x2': _bytes_feature(seq_x2),\n'x3': _bytes_feature(seq_x2),\n}))\n`\nIt seems that the protobuf is identical though, I have attached an example of the printed output of example (with x1,x2,x3), for which I obtain identical hashes for the protobuf in all runs. \n`print(example) > test.txt`\n[test.txt](https://github.com/tensorflow/tensorflow/files/402820/test.txt)\n\nmd5sum test.tfrecords test.txt\n1630f4da9a52856ac31e84fd482aebe0  test.tfrecords\n9e9cda7528deb2e7aa358b39c56b2f1d  test.txt\n\nmd5sum test.tfrecords test.txt\ne0af3c621d636f147e91582a00cc0834  test.tfrecords\n9e9cda7528deb2e7aa358b39c56b2f1d  test.txt\n\nThe hashes for the tfrecords file start to repeat themselves as well. I am not entirely sure, whether a similar pattern happens for the x1, x2 case, with lower frequency, though. \n\nCan this be a problem in the following line then? \n`writer.write(example.SerializeToString())`\n\nCan you reproduce the issue with the updated example? \nI could also try with a more up-to-date version of tensorflow alternatively. \n", "Sorry for the latency, just got a chance to run your updated program, with the three examples.  I ran it 6 times and got identical output files each time.  \n\nYou say you always see the same values printed by\n  print(example)\nHave you verified that by taking a checksum of that printed output?\n\nIf that is true, yet the example.SerializeToString() output varies, I would suspect a problem not with TensorFlow, but with the python implementation of protocol buffers that you're using, or maybe the filesystem.  It would be good to track down exactly which bytes are varying, and whether they're semantically significant (i.e. do they effect the reparsed value).\n", "Ok. I managed to reproduce the issue on a digital ocean droplet. I have attached all the files, hope that helps to clarify things. \n\nSystem info: \n4.4.0-31-generic #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016\nUbuntu 16.04 xenia\n\nSteps to reproduce: \nsee command_info and issue.txt\n\nI have also included the tfrecord files in the zip folder. \n\n[command_info.txt](https://github.com/tensorflow/tensorflow/files/404971/command_info.txt)\n[issue.txt](https://github.com/tensorflow/tensorflow/files/404974/issue.txt)\n[issue.zip](https://github.com/tensorflow/tensorflow/files/404975/issue.zip)\n", "Doh!  Sorry for not seeing the issue here sooner.  tf.train.example is a protocol message whose internal representation uses some kind of map.  The order in which the map entries are stored in serializied-format is non-deterministic, so that's where your checksum differences come from, even though the ASCII serializations are identical.  This should be clear enough if you squint at your example1.tfrecords and example2.tfrecords: You can see three records inside which look approximately the same, but are in different order.\n\nThe non-deterministic ordering of maps inside protobufs is a feature of the protobuf implementation, not TensorFlow.\n", "Did you find a solution for reproducibility of TFRecords creation, especially when it is multithreaded ? \r\nThanks ! "]}, {"number": 3639, "title": "TensorFlow master build failing:  error: 'Packet4f' does not name a type", "body": "I am trying to build TensorFlow master on big endian architecture and I am facing build issues related to EIGEN_VECTORIZE_SSE2 as mentioned below:\n\n```\nIn file included from tensorflow/core/kernels/sparse_matmul_op.cc:20:0:\n./tensorflow/core/kernels/sparse_matmul_op.h:46:26: error: 'Packet4f' does not name a type\n\u00a0EIGEN_DEVICE_FUNC inline Packet4f pexpand_bf16_l(const Packet4f& from) {\n\n./tensorflow/core/kernels/sparse_matmul_op.h:59:26: error: 'Packet4f' does not name a type\n\u00a0EIGEN_DEVICE_FUNC inline Packet4f pexpand_bf16_u(const Packet4f& from) {\n\n./tensorflow/core/kernels/sparse_matmul_op.h:117:21: error: 'Packet4f' does not name a type\n\u00a0EIGEN_STRONG_INLINE Packet4f pload4bf16<Packet4f>(const float* from) {\n\n./tensorflow/core/kernels/sparse_matmul_op.h:128:21: error: 'Packet4f' does not name a type\n\u00a0EIGEN_STRONG_INLINE Packet4f pload2bf16<Packet4f>(const float* from) {\n\n```\n\nLooks like this issue is caused due to a recent commit [fix build for PPC](https://github.com/tensorflow/tensorflow/pull/2911/commits/a1de95f38b442b0534f1f8871bc5cb16390725ae)\nThis commit includes fix for PPC however its breaking build for other non SSE platform. \n### Environment info\n\nOperating System: **Ubuntu/Red Hat**\n\nInstalled version of CUDA and cuDNN: **Not installed**\n\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.  **master** \n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)   [98d63de](https://github.com/tensorflow/tensorflow/commit/98d63de3bb2bab7c9a81f83c8ca864741399300c)  \n2. The output of `bazel version`  **0.3.0**\n### Steps to reproduce\n1. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n### What have you tried?\n1. Could build tensorflow master by removing condition for `#ifndef EIGEN_VECTORIZE_SSE2` from tensorflow/core/kernels/sparse_matmul_op.h file. \n", "comments": ["A fix is on its way. I'm hoping to get it merged by tomorrow.\n"]}, {"number": 3638, "title": "tf.gather produces zeros for invalid indices on GPU", "body": "### Environment info\n\nOperating System: Ubuntu 16.04 LTS (64 bit)\n\n```\n$ dpkg -l | grep cuda | grep ^ii\nii  libcuda1-361                                361.42-0ubuntu2                                             amd64        NVIDIA CUDA runtime library\nii  libcudart7.5:amd64                          7.5.18-0ubuntu1                                             amd64        NVIDIA CUDA Runtime Library\nii  libcudnn5                                   5.0.5-1+cuda7.5                                             amd64        cuDNN runtime libraries\nii  libcudnn5-dev                               5.0.5-1+cuda7.5                                             amd64        cuDNN development libraries and headers\nii  libcudnn5-doc                               5.0.5-1+cuda7.5                                             amd64        cuDNN documents and samples\nii  nvidia-cuda-dev                             7.5.18-0ubuntu1                                             amd64        NVIDIA CUDA development files\nii  nvidia-cuda-doc                             7.5.18-0ubuntu1                                             all          NVIDIA CUDA and OpenCL documentation\nii  nvidia-cuda-gdb                             7.5.18-0ubuntu1                                             amd64        NVIDIA CUDA Debugger (GDB)\nii  nvidia-cuda-toolkit                         7.5.18-0ubuntu1                                             amd64        NVIDIA CUDA development toolkit\n```\n\n```\n$ find /usr/lib -name libcud\\*\n/usr/lib/i386-linux-gnu/libcuda.so.1\n/usr/lib/i386-linux-gnu/libcuda.so.361.42\n/usr/lib/i386-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n/usr/lib/x86_64-linux-gnu/libcudnn.so\n/usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.0.5\n/usr/lib/x86_64-linux-gnu/libcudart.so\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5\n/usr/lib/x86_64-linux-gnu/stubs/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudart_static.a\n```\n\n```\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0rc0\n```\n### Steps to reproduce\n\n```\nIn [23]: x = tf.constant([1.1,2.2,3.3])\n\nIn [24]: a = tf.constant(123,dtype=tf.int32)\n\nIn [25]: tf.gather(x,a,validate_indices=True).eval()\nGather_8: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Gather_8: /job:localhost/replica:0/task:0/gpu:0\nConst_8: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Const_8: /job:localhost/replica:0/task:0/gpu:0\nConst_7: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] Const_7: /job:localhost/replica:0/task:0/gpu:0\nOut[25]: 0.0\n\nIn [26]: \n```\n", "comments": ["You are correct.  The validation is always performed, since overhead was negligible.\n", "To clarify: it works on CPU but not on GPU, right?  Unfortunately, while we always perform the checks on GPU, we never report those errors back to the user:\n\nhttps://github.com/tensorflow/tensorflow/blob/c5f94b10bbb30e525fa3ca313e7ccb173040c90a/tensorflow/core/kernels/gather_op_gpu.cu.cc#L40\n\nIt would be great to fix this, but we didn't know how to do it in a performant fashion.\n", "So what does the CPU version do? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_op.cc\n\nOn CPU I get a exception independent on what I'm using (True or False):\n\n```\nInvalidArgumentError: indices = 123 is not in [0, 3)\n```\n\nI think the GPU version at least could return a NaN if its out of bounds. The flag could probably be removed entirely from the API since neither CPU nor GPU returns/does anything different on either branches.\n", "@hholst80 The CPU gives you an exception independent of what you're using (True or False).  This is to avoid crashing.  NaN would be reasonable, but the real fix would be to get an actual exception to bubble back up to the CPU from the GPU.\n", "I neither have the context for this bug, nor am I going to be able to get to this in a reasonable time frame.\n", "I don't know enough about GPUs to solve this one.  @zheng-xq Can you triage?\n", "This is an area that is contribution welcome. We cannot have GPU synchronize with CPU. There are a few ways: \n1. Trigger a \"trap\" instruction on GPU, which triggers an interrupt on the host CPU. \n2. Allocated a pinned memory on CPU, and write the error status from GPU in the kernel. During the launch, also schedule an asynchrous functor to check the status through \"ThenExecute\".\n", "I'd love to see GPU doing the same thing as the CPU code is doing, but as I said, the flag is unnecessary since it does not do anything on neither GPU nor CPU (and this behavior is undocumented as well).\n\nIf the exception thingy from GPU is too hard to do right now just put a NaN here instead of T(0)?\nhttps://github.com/tensorflow/tensorflow/blob/c5f94b10bbb30e525fa3ca313e7ccb173040c90a/tensorflow/core/kernels/gather_op_gpu.cu.cc#L41\n\nSilent and undetectable failure is dangerous...\n", "Here's an older issue that can be closed once this one is fixed: https://github.com/tensorflow/tensorflow/issues/3067\n", "@hholst80: You can only put `NaN` into the output if `T` is a floating point type, so I'm afraid that the idea can't work for all types :(\n\n@zheng-xq: Do you know whether it's possible to somehow signal an error in TensorFlow after `OpKernel::Compute()` has already finished? It seems that this might not be possible through the `OpKernelContext`, as it's only guaranteed to live roughly until the end of `OpKernel::Compute()`.\nFrom reading the `Executor` code at https://github.com/tensorflow/tensorflow/blob/00700f00fdf71baec1342d1afd7849e16fbd2a33/tensorflow/core/common_runtime/executor.cc#L1210 it doesn't seem like there's an opportunity to pass a `Status` from our asynchronous functor to the `Executor`, but I might be wrong here.\n", "If someone really wants to have this kernel report errors, here are some more details:\n1. Add non-default attribute so the kernel can optional synchronize with GPU and check errors. \n2. A more TensorFlow way is to change the kernel AsyncOpKernel, and check the errors and call \"done\" through ThenExecute. That way, the current thread is released to do other work. But ops following gather still cannot issue. \n3. Still use the sync OpKernel, schedule ThenExecute to check for the error. The down-side is the context is long gone at that point. So you can print out enough error message and crash the process with LOG(FATAL). \n\nI will leave it up to the contributor/developer to choose from those. But it is important that by default, we don't synchronize and hold off other kernel launches unnecessarily. \n", "Thanks!\nI think the checking shouldn't be optional, otherwise users will keep running into problems, so that would rule out option 1.\nOption 2 still blocks ops that depend on gather. I think we should try to avoid this if we can.\nOption 3 sounds good, but a `LOG(FATAL)` would crash the interpreter that the user might be running TensorFlow in, which isn't very nice. (But maybe this is easy to fix?)\n\nI've also thought of writing a `CheckGather` op, which performs the index checking separately from `Gather`. It could be launched alongside `Gather` from a Python wrapper.\nThis could either always run on the CPU, or synchronize with the GPU, possibly as an AsyncOpKernel as you've explained.\n\nMaybe the cleanest option would be to modify `Executor` slightly to add support for asynchronous error reporting.\nFor example, we could add a `Status` protected by a mutex to `Executor` that's specifically designed for asynchronous error reporting. The op's `Compute` method could then pass the `Status` into a callback that's executed as part of the CUDA stream. The `Executor` would then check the `Status` after it synchronized the computation.\nI'm not sure how useful this change would be for ops other than `Gather`, though.\n", "I've talked to my colleagues about modifying executor. It is trickier than we would like. \n\nAt this point, I think option 3 with LOG(FATAL) is the path with the least potential problems. I agree that it is not clean. But there are plenty of CHECK in TensorFlow, which does essentially the same thing. So if we put debugging information in that error message, perhaps it is more useful to people than it is today. \n", "@zheng-xq Actually we've done quite a bit of work to remove `LOG(FATAL)` uses that can be triggered by the user.  The policy is that they should all be unreachable.  It would be a shame if `gather` did this.\n", "In that case, maybe we could explore the async error reporting as @ibab mentioned. The tricky part is that we need to hold executor to wait for all the error checks to finish at the end. Otherwise, the run could finish  successfully before the async error checking. And we need to be very careful that we don't cause a deadlock. \n", "I ran into this issue today (on v1.0). \r\n\r\nIf no fix is on the horizon, I suggest either removing the `validate_indices` option or mentioning in the API docs that it works only on CPU for now, as that helps with debugging (otherwise people will think their problem lies elsewhere, trusting that the indices must be valid).", "@Fenugreek Interested in contributing a doc fix?", "@girving Yes. A bit busy today but will get back by early next week.", "@girving I feel a little stupid but after 15+ minutes of looking I can't find where the docs for `tf.gather` live. They're not in `python/ops/array_ops.py`. Any tips?", "tensorflow/core/ops/array_ops.cc", "Cool, thanks for the pointer @josh11b . I am hoping to submit a pull request for the doc fix in a day or so, looking to be consistent with docs elsewhere Re: GPU vs CPU differences/limitations.", "This is my proposed addition to the `tf.gather` doc in array_ops.cc:\r\n\r\n---\r\n\r\n`validate_indices`: DEPRECATED. If this operation is assigned to CPU, values in \r\n`indices` are always validated to be within range. If assigned to GPU,\r\nout-of-bound values in `indices` currently result in output values of `0`.\r\n\r\n---\r\nGiven @hholst80 's comment on Aug 5, 2016 above, and confirming by looking at the CPU and GPU .cc source, the above seems the most complete doc. We may prefer to avoid saying that current fallback value on GPU is `0`, in case people start depending on it.\r\n\r\nThe above fix is in my fork at `github.com/Fenugreek/tensorflow`. I expect there will be comments resulting in changes, so I am holding off making a pull request.", "Looks good.  Maybe change the GPU sentence to\r\n\r\n> If assigned to GPU, out-of-bound indices result in unspecified behavior (currently the result is 0, but this may become an error in future).", "Ok, changed it as so and put in the associated pull request #8646 .", "Related bug with the same \"how to signal errors on GPU\" problem: https://github.com/tensorflow/tensorflow/issues/2594.", "What about unifying the GPU and CPU implementations of gather by always returning 0 for out-of-bound indices, then having a separate op which checks indices with a CPU-only implementation? That way you get consistent behavior, error propagation if desired and a clean interface.\r\n\r\nThat would also allow \"conditional gather\"-type operations relatively easily, where a default value is used for indices that are out of range, such as:\r\n\r\ntf.gather(xs, indices) + (indices >= tf.shape(xs)[0]) * 2.0\r\n\r\nAt the moment performing conditional gather operations like this are tricky on the CPU, since error checking is always done (unless this can be done with another op I'm not aware of?)", "@MattShannon Well, your idea seems legit, but I don't get how you would differentiate between a \"real\" zero value and a \"fake\" zero value. Although this would be a reasonable fallback behavior for out-of-range indices, it should always be a conscious decision, else we end up with silent errors.\r\n\r\nIdeally, we would have a \"special value\" other than zero:\r\n* one available for all dtypes OR one for each dtype\r\n* presumably unused by any \"non-special behavior\" code\r\n\r\nThen we could wrap it with handlers like the ones you brought up.", "My idea was that there are two ops. The gather op is changed to be forgiving, i.e. for both GPU and CPU it returns zero whenever an index is out-of-bounds. A separate op (say \"check_indices\") with only a CPU implementation determines whether the gather is \"valid\" (i.e. no out-of-bounds lookups, in the same way that the current CPU implementation does, iiuc).\r\n\r\nFor some applications, forgiving gather is exactly what you want. It is simple to turn this into something which uses a user-specified default value instead of 0, using something like the code snippet above. Afaict, forgiving gather is currently quite hard to effect in tensorflow.\r\n\r\ntf.gather can be implemented in terms of forgiving gather and check_indices. This preserves the current CPU behavior, and extends that to the GPU case, fixing this bug.", "> A separate op (say \"check_indices\") with only a CPU implementation ...\r\n\r\nIf this op is forced to be run on CPU, it could slow down the computation due to additional data transfers between CPU and GPU. I can't assess the impact this might have, but this effect should be considered.", "Yeah, I agree that's something to bear in mind. The motivation for my suggestion was just that: (a) I think it's uniformly better than the current situation where the GPU implementation is nominally \"incorrect\" (doesn't check indices); and (b) I think the current GPU behavior (\"forgiving gather\") is actually a great op to have.\r\n\r\nThe procedure to check indices could consist of an op which outputs a boolean (whether the indices were valid) and has a GPU implementation, and a CPU-only op which takes that and actually raises the error. This avoids any large GPU to CPU transfer.", "I am currently using this quirk of the gather op because I want to have identically 0 tensors if the index is out of bounds. Is this bad from a performance perspective?", "Hi @hholst80! \r\nWe are checking to see whether you still need help in this issue . It will raise error in CPU and render zero in GPU .Attaching [Gist ](https://colab.research.google.com/gist/mohantym/6676d460c5572bdce10857982b99a856/github_3638.ipynb)and [Reference ](https://www.tensorflow.org/api_docs/python/tf/gather#args).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/3638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/3638\">No</a>\n"]}, {"number": 3637, "title": "Fix word2vec_optimized excepction when words contain unicode.", "body": "When word2vec_optimized deal with Korean language, it will break out below exception.\n\n```\nTraceback (most recent call last):\n  File \"word2vec_optimized.py\", line 432, in <module>\n    tf.app.run()\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"word2vec_optimized.py\", line 417, in main\n    model = Word2Vec(opts, session)\n  File \"word2vec_optimized.py\", line 146, in __init__\n    self.save_vocab()\n  File \"word2vec_optimized.py\", line 242, in save_vocab\n    opts.vocab_counts[i]))\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)\n```\n\nI fix this exception by adding encode('utf-8') , and then it successful run word2vec_optimized model.\n", "comments": ["Can one of the admins verify this patch?\n", "I have done the same job in #3365 sub-commit.\nBut it still pending.\n", "There is the same job in #3365 @raix852 mentioned.\n"]}, {"number": 3636, "title": "Inception retraining / transfer learning can only utilize one CPU core and one GPU", "body": "The retraining code is example code, but it does seem like there are many people experimenting with retraining and improving performance could make the example code much more useful as a launchpad for others if it can scale across GPUs and multiple CPU cores.\n\nAt the moment, running\n`bazel-bin/tensorflow/examples/image_retraining/retrain --num_gpus=2 --image_dir /images`\nwill only result in a single GPU performing computations in `nvidia-smi`, even though both GPUs are running `python` in nvidia-smi.  GPU #2 will see zero memory/power utilization during the computation beyond idle levels.  GPU #1 will utilize all memory TensorFlow and an additional 30 watts or so.\n\nThe other related issue is that, during training, a single CPU core will be maxed out at 100% running python; given the capabilities of the card, I'm guessing removing this bottleneck would increase retraining speed by perhaps 200-400%.  `nmon` shows that disk IO is not a bottleneck.\n\nThanks for your time and for making TensorFlow available to everyone!\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 8.0 and 5 with two GTX 1080 cards\n\nretrain.py: `train_batch_size` is 20000 and `learning_rate` is 0.5\n1. The commit hash (`git rev-parse HEAD`): r0.9.0\n2. The output of `bazel version`: Build label: 0.2.3\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Tue May 17 14:21:13 2016 (1463494873)\n   Build timestamp: 1463494873\n   Build timestamp as int: 1463494873\n", "comments": ["This retraining program was written to be a relatively simple example, not a high-performance distributed program.  That said, it does seem to be a very popular application and a multi-gpu version would be nice.  It's not as simple as just adding a flag and running on a machine with more gpus; the model needs to be modified so that it can effectively use more gpus in parallel.  TF is able to use multiple cpu cores in parallel because large operations like matrix multiply can easily be split across multiple cores by libraries like BLAS.  But a single TF Op can't (yet) automatically be split over multiple gpus.  \n\nMaking available a multi-gpu version of image re-training would be a nice community contribution.\n", "Not sure what the single core being pegged at 100% issue is; I'm not a python expert but I've heard it's not very good a doing multithreading, and this program has some image preprocessing done in python.  For a real high-performance application, that stuff should be ported to a more effective, distributable framework.\n", "If parallelizing the training is difficult could we at least parallelize the computation of bottlenecks ? The should be no dependencies between the computation of two bottleneck, so their computation could scale to multiple GPU/CPU.", "It seems really odd that the one core is being stressed.  Given you have two GTX 1080s I am assuming you have at least a 4 core 8 thread setup.  \r\n\r\nUsing our benchmark code I have not had an issue feeding 1080s.  I 100% recognize that my code is not the same as this code.  \r\n\r\nI will leave this open a contributions welcome.  I looked over the example and I can see where it could use some help that might speed it up.  It does not actually define a tf.device so it is very possible that part of the image input pipeline is happening on CPU and even something crazy like some ops running on CPU could be happening (although less likely).  \r\n\r\nI will pick this up but if I do not do anything in the next few weeks it is something that will likely not get updated with out community support.   \r\n\r\nIf you wanted to do it yourself  you just need to wrap the input pipeline with\r\n```\r\nwith tf.device(\":/CPU\"):\r\n```\r\n\r\nAnd the model aspects with \r\n\r\n```\r\nwith tf.device(\":/GPU:0\")\r\n```\r\n\r\nIt is slightly more complicated than that.  These examples were written to give people a place to start and unfortunately did not age well and people did not think about all the scenarios.  \r\n\r\n", "Can anyone provide the multi-GPU version of image re-training would be great !!!!", "If you are not looking to use TF-Lite, then there are a lot more models that can be use for fine tuning in the [SLIM examples](https://github.com/tensorflow/models/tree/master/research/slim), which I have confirmed by asking the author do support multi-GPU.  I knew the SLIM examples were multi-GPU and I confirmed that also works for fine tuning.  \r\n\r\nLeaving open until I add some info to the example README or someone says I cannot.  :-)\r\n  \r\n  ", "Thanks for tfboyd.\r\nCan you please suggest or guide me any link which will support the multi-gpu for Tensorflow for poets retraining. \r\n\r\n\r\nPart of my code :\r\n\r\n_**with tf.device('/device:GPU:0'):**\r\n      **with tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=True)) as sess:**\r\n        jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(\r\n            model_info['input_width'], model_info['input_height'],\r\n            model_info['input_depth'], model_info['input_mean'],\r\n            model_info['input_std'])\r\n    \r\n        if do_distort_images:\r\n          # We will be applying distortions, so setup the operations we'll need.\r\n          (distorted_jpeg_data_tensor,\r\n           distorted_image_tensor) = add_input_distortions(\r\n               FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\r\n               FLAGS.random_brightness, model_info['input_width'],\r\n               model_info['input_height'], model_info['input_depth'],\r\n               model_info['input_mean'], model_info['input_std'])\r\n        else:\r\n          # We'll make sure we've calculated the 'bottleneck' image summaries and\r\n          # cached them on disk.\r\n          cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\r\n                            FLAGS.bottleneck_dir, jpeg_data_tensor,\r\n                            decoded_image_tensor, resized_image_tensor,\r\n                            bottleneck_tensor, FLAGS.architecture)\r\n    \r\n        # Add the new layer that we'll be training.\r\n        (train_step, cross_entropy, bottleneck_input, ground_truth_input,\r\n         final_tensor) = add_final_training_ops(\r\n             len(image_lists.keys()), FLAGS.final_tensor_name, bottleneck_tensor,\r\n             model_info['bottleneck_tensor_size'])\r\n    \r\n        # Create the operations we need to evaluate the accuracy of our new layer.\r\n        evaluation_step, prediction = add_evaluation_step(\r\n            final_tensor, ground_truth_input)\r\n    \r\n        # Merge all the summaries and write them out to the summaries_dir\r\n        merged = tf.summary.merge_all()\r\n        train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\r\n                                             sess.graph)\r\n    \r\n        validation_writer = tf.summary.FileWriter(\r\n            FLAGS.summaries_dir + '/validation')\r\n    \r\n        # Set up all our weights to their initial default values.\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n    \r\n        # Run the training for as many cycles as requested on the command line.\r\n        **for i in range(FLAGS.how_many_training_steps):**\r\n             # Get a batch of input bottleneck values, either calculated fresh every\r\n          # time with distortions applied, or from the cache stored on disk.\r\n          if do_distort_images:\r\n            (train_bottlenecks,\r\n             train_ground_truth) = get_random_distorted_bottlenecks(\r\n                 sess, image_lists, FLAGS.train_batch_size, 'training',\r\n                 FLAGS.image_dir, distorted_jpeg_data_tensor,\r\n                 distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\r\n          else:\r\n            (train_bottlenecks,\r\n             train_ground_truth, _) = get_random_cached_bottlenecks(\r\n                 sess, image_lists, FLAGS.train_batch_size, 'training',\r\n                 FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\r\n                 decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\r\n                 FLAGS.architecture)\r\n          # Feed the bottlenecks and ground truth into the graph, and run a training\r\n          # step. Capture training summaries for TensorBoard with the `merged` op.\r\n          train_summary, _ = sess.run(\r\n              [merged, train_step],\r\n              feed_dict={bottleneck_input: train_bottlenecks,\r\n                         ground_truth_input: train_ground_truth})\r\n          train_writer.add_summary(train_summary, i)\r\n    \r\n          # Every so often, print out how well the graph is training.\r\n          is_last_step = (i + 1 == FLAGS.how_many_training_steps)\r\n          if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\r\n            train_accuracy, cross_entropy_value = sess.run(\r\n                [evaluation_step, cross_entropy],\r\n                feed_dict={bottleneck_input: train_bottlenecks,\r\n                           ground_truth_input: train_ground_truth})\r\n            tf.logging.info('%s: Step %d: Train accuracy = %.1f%%' %\r\n                            (datetime.now(), i, train_accuracy * 100))\r\n            tf.logging.info('%s: Step %d: Cross entropy = %f' %\r\n                            (datetime.now(), i, cross_entropy_value))\r\n            validation_bottlenecks, validation_ground_truth, _ = (\r\n                get_random_cached_bottlenecks(\r\n                    sess, image_lists, FLAGS.validation_batch_size, 'validation',\r\n                    FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\r\n                    decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\r\n                    FLAGS.architecture))\r\n            # Run a validation step and capture training summaries for TensorBoard\r\n            # with the `merged` op.\r\n            validation_summary, validation_accuracy = sess.run(\r\n                [merged, evaluation_step],\r\n                feed_dict={bottleneck_input: validation_bottlenecks,\r\n                           ground_truth_input: validation_ground_truth})\r\n            validation_writer.add_summary(validation_summary, i)\r\n            tf.logging.info('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\r\n                            (datetime.now(), i, validation_accuracy * 100,\r\n                             len(validation_bottlenecks)))\r\n    \r\n          # Store intermediate results\r\n          intermediate_frequency = FLAGS.intermediate_store_frequency\r\n    \r\n          if (intermediate_frequency > 0 and (i % intermediate_frequency == 0)\r\n              and i > 0):\r\n            intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\r\n                                      'intermediate_' + str(i) + '.pb')\r\n            tf.logging.info('Save intermediate result to : ' +\r\n                            intermediate_file_name)\r\n            save_graph_to_file(sess, graph, intermediate_file_name)\r\n    \r\n        # We've completed all our training, so run a final test evaluation on\r\n        # some new images we haven't used before.\r\n        test_bottlenecks, test_ground_truth, test_filenames = (\r\n            get_random_cached_bottlenecks(\r\n                sess, image_lists, FLAGS.test_batch_size, 'testing',\r\n                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\r\n                decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\r\n                FLAGS.architecture))\r\n        test_accuracy, predictions = sess.run(\r\n            [evaluation_step, prediction],\r\n            feed_dict={bottleneck_input: test_bottlenecks,\r\n                       ground_truth_input: test_ground_truth})\r\n        tf.logging.info('Final test accuracy = %.1f%% (N=%d)' %\r\n                        (test_accuracy * 100, len(test_bottlenecks)))\r\n    \r\n        if FLAGS.print_misclassified_test_images:\r\n          tf.logging.info('=== MISCLASSIFIED TEST IMAGES ===')\r\n          for i, test_filename in enumerate(test_filenames):\r\n            if predictions[i] != test_ground_truth[i].argmax():\r\n              tf.logging.info('%70s  %s' %\r\n                              (test_filename,\r\n                               list(image_lists.keys())[predictions[i]]))\r\n    \r\n        # Write out the trained graph and labels with the weights stored as\r\n        # constants.\r\n        save_graph_to_file(sess, graph, FLAGS.output_graph)\r\n        with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\r\n          f.write('\\n'.join(image_lists.keys()) + '\\n')_\r\n\r\nHere I am creating sess object with tf.device('/device:GPU:0'),how can i ran my training on multiple GPU's", "@tfboyd -- is the README change mentioned above completed?", "Nope not yet.", "+1", "Anyone ever make progress on this? Using retrain.py with a Tesla K80 and noticed only half the GPU is used since it appears as two devices.\r\n\r\nCan I switch to the slim retraining code without adjusting anything else? Using inceptionV3 from the TF for poets post and really would like to fully use the GPU. Appreciate any help.", "Nagging Assignee @tfboyd: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please use the documentation on slim for retraining inception model on multiple gpus. Readme reflects to use slim models. I am closing this as the original issue has been resolved. Please create a new one if you still run into issues..", "I am having similar problem running multiple trained Inception V3 models on different GPUs due to out of memory\r\n\r\n(failed to allocate 5.15G (5525864448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory \r\n\r\nfollowed by\r\n\r\n2019-02-26 15:25:59.123826: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-02-26 15:25:59.128184: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-02-26 15:25:59.130136: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-02-26 15:25:59.130791: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED)\r\n\r\n\r\nGPU configs- \r\nGeForce GTX 1060 6GB major\r\ntotalMemory: 5.93GiB freeMemory: 5.69GiB  memoryClockRate(GHz): 1.7715\r\n\r\nGeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392 =>ignored\r\ntotalMemory: 3.94GiB freeMemory: 3.89GiB\r\n\r\nCODE\r\nUsed thread.start to run multiple threads each doing one run of Inception as per\r\nhttps://www.tensorflow.org/hub/tutorials/image_retraining\r\nfrom the \"Using the retrained model\" section.\r\n"]}, {"number": 3635, "title": "Fixed inconsistency of outputs collection", "body": "Because naming logic for utils.collect_named_outputs in max_pool2d and convolution2d is different, collection is getting filled with inconsistent names.\ncode to reproduce:\n\n```\ngraph = tf.Graph()\n\nnode_collection = 'nodes'\nwith graph.as_default():\n    x = tf.placeholder(tf.float32, (1, 20, 20, 3))\n\n    with tf.name_scope('name_scope'):\n        conv = tf.contrib.layers.conv2d(\n            x, num_outputs=8, kernel_size=4,\n            scope='conv', outputs_collections=node_collection)\n        max_pool = tf.contrib.layers.max_pool2d(\n                conv, kernel_size=2, stride=2, scope='max_pool',\n                outputs_collections=node_collection)\n\nprint([nt.name for nt in graph.get_collection(node_collection)])\n=============================================\n['conv', 'name_scope/max_pool']\n```\n\nExpected output is ['name_scope/conv', 'name_scope/max_pool'].\n\nIt is also important for those who use tf.name_scope to distinguish processing of different inputs with the same layer, like in siamese neural network. Currently, they will get identical names.\nCode to reproduce:\n\n```\ngraph = tf.Graph()\n\nnode_collection = 'nodes'\nwith graph.as_default():\n  input_a = tf.placeholder(tf.float32, (1, 20, 20, 3))\n  input_b = tf.placeholder(tf.float32, (1, 20, 20, 3))\n  with tf.variable_scope('feature_extractor') as vs:\n    for name_scope, inputs in [('a', input_a), ('b', input_b)]:\n      with tf.name_scope(name_scope):\n        conv = tf.contrib.layers.conv2d(\n          inputs, num_outputs=8, kernel_size=4,\n          scope='conv', outputs_collections=node_collection)\n        max_pool = tf.contrib.layers.max_pool2d(\n          conv, kernel_size=2, stride=2, scope='max_pool',\n          outputs_collections=node_collection)\n      vs.reuse_variables()\n\nprint([nt.name for nt in graph.get_collection(node_collection)])\n=============================================\n['feature_extractor/conv', 'feature_extractor/a/max_pool', 'feature_extractor/conv', 'feature_extractor/b/max_pool']\n```\n", "comments": ["Can one of the admins verify this patch?\n", "ping for @sguada \n", "Jenkins, test this please.\n", "@Egor-Krivov could you add a test for it?\n", "OK, I'll add some tests. Can I just add new methods to classes in tensorflow/contrib/layers/python/layers/layers_test.py?\n", "Jenkins, test this please.\n", "@Egor-Krivov yes you can add more methods to existing tests.\n", "@Egor-Krivov please review the test that are failing.\n", "@sguada \nSeems like a problem with testing system. All problems in some Java libraries. There are similar problems here https://github.com/kubernetes/kubernetes/issues/26822\n", "@tensorflow-jenkins test this please\n", "I am going to add tests in an hour\n", "@sguada \nI've added tests for fully connected and conv2d\n", "@tensorflow-jenkins test this please\n", "@rmlarsen Please merge.\n"]}, {"number": 3634, "title": "Tutorial updates for r0.10 release", "body": "Hi TF team,\n\nWould it be possible to fold these tutorial doc updates from master into the r0.10 release branch?\n\nThanks,\nSanders\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Hi Sanders,\n\nPlease sign the CLA and then we can move forward with the review process.\n\nThanks!\n", "alternatively, change the git commit email to your google address\n", "@sandersk, can you sign the cla? \n", "@benoitsteiner I signed it. Let me know if there's still any issues.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 3633, "title": "Branch 129255791", "body": "Merging internal changes\n", "comments": ["@tensorflow-jenkins, test this please\n", "The build failure on GPU is due to the newly added \"ios_tensorflow_lib\" target in core/BUILD. I've left notes for the CL's author.\n", "Abandoning this pull request. \n"]}, {"number": 3632, "title": "Not finding cuda library for jobs that do NOT require GPU", "body": "I am having this error when I use the slurm (http://slurm.schedmd.com/) workload manager. When I run some tensorflow python scripts, sometimes it results in an error (attached). It seems that it can't find cuda library installed but I am running scripts that do **not** require GPUs.  Therefore, I find it very confusing why cuda would be an issue at all. Why is cuda installation an issue if I don't need it?\n\n---\n\nOperating System:   CentOS Linux release 7.2.1511\n\nInstalled version of CUDA and cuDNN (calling ls -l /usr/lib/libcuda.so.352.63): \n-rwxr-xr-x 1 root root 14272428 Dec 15  2015 /usr/lib/libcuda.so.352.63\n\nIf installed from binary pip package, provide:\ntensorflow (0.9.0)\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. running this seems to produce different results from the error I expected. what it outputs:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /cm/shared/openmind/cuda/7.5/lib64:/cm/shared/openmind/cuda/7.5/lib\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:2092] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n```\n\n the error I expected:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /cm/shared/openmind/cuda/7.5/lib64:/cm/shared/openmind/cuda/7.5/lib\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:2092] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: node047\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: node047\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.63.0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n```\n### Steps to reproduce\n1. run the sbatch command in slurm and dispatch about 1000 jobs running an idential tensorflow script. Returns that error.\n### What have you tried?\n1. I've tried logging into the node directly throwing the error and run tensorflow jobs but it seems when I do that there are no errors.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /cm/shared/openmind/cuda/7.5/lib64:/cm/shared/openmind/cuda/7.5/lib\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:2092] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: node047\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: node047\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:347] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)\n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 352.63.0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n```\n", "comments": ["TensorFlow has separate builds for CPU and GPU binaries (http://ci.tensorflow.org/view/Nightly/) You seem to be running a GPU build on a machine that has no GPUs, hence the warning. However, it should still work regardless, this message is purely informational.\n", "@yaroslavvb however, the observed behavior is that the job just stalls and does not move on. Is there some way to go around this? I never requested it to run on GPUs in the first place so its confusing that its doing this.\n", "Job stalling is unrelated to the message. GPU initialization happens when TensorFlow library is loaded, regardless of whether you actually need GPU or not.\n", "I also made a stackoverflow question for it: http://stackoverflow.com/questions/38727375/why-do-jobs-in-slurm-freeze-indefinitely-when-they-are-tensorflow-scripts and currently put a bounty of 100 on it. \n\nI'd love to be able to solve this and why its stalling. Not sure if its tensorflow or the computers/nodes themselves. \n", "Just to add more information to the discussion, in slurm one can explicitly say to assign jobs to nodes with GPU's. I've observed that the apparent \"stalling\" only happens when jobs are sent to machine that are suppose to be CPU only. \n\nIf the machine is artificially restricted to only use CPU but tensorflow detects GPUs, can tensorflow accidentally get stuck?\n", "Is there anything I can do to help get this resolved? Thanks! :) \n", "@brando90 I've added some suggestions on the stackoverflow thread (remove queues, add session timeouts, use cpu-only binary, try to match env to reproduce locally, isolate exactly which line gets stuck). From the description it's not clear that there's anything wrong with tensorflow (ie, maybe it's stuck waiting for input)\n", "It's possible there's a bug with the nvidia driver -- I've seen reports of people updating their driver and solving some issues like these.  In particular, TF shouldn't be doing anything with your GPUs after trying to initialize them if you never use them.  I'm going to close this for now unless there's more evidence that there's something in TF we're doing wrong (we can't really reproduce this, so you'd have to do some debugging to help us out, not sure what other help we can offer).\n", "I was checkpointing my tensorflow models and once I stopped doing that the stalling issue stopped (for the moment). Not sure if that was it but it might be some way locking works with tensorflow interferes with slurm. \n"]}, {"number": 3631, "title": "Unable to build from source for TensorFlow r0.10 C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command", "body": "I have followed this tutorial (https://github.com/samjabrahams/tensorflow-on-raspberry-pi) trying to install the latest tensorflow with quantization enabled, on raspberry pi 3. So I followed every step in it except for the tensorflow version, instead of Tensorflow 0.9 I used the latest 0.10. \n\nBut I ran into the following error\n\n```\nERROR: /home/pi/tf2/bazel/tensorflow/tensorflow/core/BUILD:826:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command \n  (cd /home/pi/.cache/bazel/_bazel_pi/b1a0eb2b93fc558ac1cbf83f2cf3a5f1/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games:/opt/hadoop/bin:/opt/hadoop/sbin:/home/pi/opt/Cypress/arm-2013.11/bin:/home/pi/opt/Cypress/eclipse:/home/pi/opt \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-mfpu=neon' '-std=c++0x' -DHAVE_CONFIG_H -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/gif_archive -iquote bazel-out/local_linux-opt/genfiles/external/gif_archive -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-opt/genfiles/external/zlib_archive -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/gif_archive/giflib-5.1.4/lib -isystem bazel-out/local_linux-opt/genfiles/external/gif_archive/giflib-5.1.4/lib -isystem external/highwayhash -isystem bazel-out/local_linux-opt/genfiles/external/highwayhash -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem external/eigen_archive -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/zlib_archive/zlib-1.2.8 -isystem bazel-out/local_linux-opt/genfiles/external/zlib_archive/zlib-1.2.8 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/png/png_io.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/png/png_io.pic.d -fPIC -c tensorflow/core/lib/png/png_io.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/lib_internal/tensorflow/core/lib/png/png_io.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nIn file included from ./tensorflow/core/lib/png/png_io.h:39:0,\n                 from tensorflow/core/lib/png/png_io.cc:27:\n./tensorflow/core/platform/png.h:26:2: error: #error Define the appropriate PLATFORM_<foo> macro for this platform\n #error Define the appropriate PLATFORM_<foo> macro for this platform\n  ^\nIn file included from tensorflow/core/lib/png/png_io.cc:27:0:\n./tensorflow/core/lib/png/png_io.h:48:3: error: 'png_structp' does not name a type\n   png_structp png_ptr;\n   ^\n./tensorflow/core/lib/png/png_io.h:49:3: error: 'png_infop' does not name a type\n   png_infop info_ptr;\n   ^\n./tensorflow/core/lib/png/png_io.h:50:3: error: 'png_uint_32' does not name a type\n   png_uint_32 width, height;\n   ^\n./tensorflow/core/lib/png/png_io.h: In constructor 'tensorflow::png::DecodeContext::DecodeContext()':\n./tensorflow/core/lib/png/png_io.h:57:21: error: class 'tensorflow::png::DecodeContext' does not have any field named 'png_ptr'\n   DecodeContext() : png_ptr(NULL), info_ptr(NULL) {}\n                     ^\n./tensorflow/core/lib/png/png_io.h:57:36: error: class 'tensorflow::png::DecodeContext' does not have any field named 'info_ptr'\n   DecodeContext() : png_ptr(NULL), info_ptr(NULL) {}\n                                    ^\n./tensorflow/core/lib/png/png_io.h: At global scope:\n./tensorflow/core/lib/png/png_io.h:78:25: error: 'png_bytep' was not declared in this scope\n bool CommonFinishDecode(png_bytep data, int row_bytes, DecodeContext* context);\n                         ^\n./tensorflow/core/lib/png/png_io.h:78:41: error: expected primary-expression before 'int'\n bool CommonFinishDecode(png_bytep data, int row_bytes, DecodeContext* context);\n                                         ^\n./tensorflow/core/lib/png/png_io.h:78:69: error: expected primary-expression before '*' token\n bool CommonFinishDecode(png_bytep data, int row_bytes, DecodeContext* context);\n                                                                     ^\n./tensorflow/core/lib/png/png_io.h:78:71: error: 'context' was not declared in this scope\n bool CommonFinishDecode(png_bytep data, int row_bytes, DecodeContext* context);\n                                                                       ^\n./tensorflow/core/lib/png/png_io.h:78:78: error: expression list treated as compound expression in initializer [-fpermissive]\n bool CommonFinishDecode(png_bytep data, int row_bytes, DecodeContext* context);\n                                                                              ^\ntensorflow/core/lib/png/png_io.cc:77:19: error: variable or field 'ErrorHandler' declared void\n void ErrorHandler(png_structp png_ptr, png_const_charp msg) {\n                   ^\ntensorflow/core/lib/png/png_io.cc:77:19: error: 'png_structp' was not declared in this scope\ntensorflow/core/lib/png/png_io.cc:77:40: error: 'png_const_charp' was not declared in this scope\n void ErrorHandler(png_structp png_ptr, png_const_charp msg) {\n                                        ^\ntensorflow/core/lib/png/png_io.cc:85:21: error: variable or field 'WarningHandler' declared void\n void WarningHandler(png_structp png_ptr, png_const_charp msg) {\n                     ^\ntensorflow/core/lib/png/png_io.cc:85:21: error: 'png_structp' was not declared in this scope\ntensorflow/core/lib/png/png_io.cc:85:42: error: 'png_const_charp' was not declared in this scope\n void WarningHandler(png_structp png_ptr, png_const_charp msg) {\n                                          ^\ntensorflow/core/lib/png/png_io.cc:89:19: error: variable or field 'StringReader' declared void\n void StringReader(png_structp png_ptr, png_bytep data, png_size_t length) {\n                   ^\ntensorflow/core/lib/png/png_io.cc:89:19: error: 'png_structp' was not declared in this scope\ntensorflow/core/lib/png/png_io.cc:89:40: error: 'png_bytep' was not declared in this scope\n void StringReader(png_structp png_ptr, png_bytep data, png_size_t length) {\n                                        ^\ntensorflow/core/lib/png/png_io.cc:89:56: error: 'png_size_t' was not declared in this scope\n void StringReader(png_structp png_ptr, png_bytep data, png_size_t length) {\n                                                        ^\ntensorflow/core/lib/png/png_io.cc:398:1: error: expected '}' at end of input\n }  // namespace tensorflow\n ^\ntensorflow/core/lib/png/png_io.cc:398:1: error: expected '}' at end of input\ntensorflow/core/lib/png/png_io.cc:398:1: error: expected '}' at end of input\ntensorflow/core/lib/png/png_io.cc:48:13: warning: 'void tensorflow::png::{anonymous}::Convert8to16(const uint8*, int, int, int, int, tensorflow::uint16*, int)' defined but not used [-Wunused-function]\n static void Convert8to16(const uint8* p8, int num_comps, int p8_row_bytes,\n             ^\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n```\n\nmy gcc version is 4.9.2\n\nI appreciate any help and suggestion!\n", "comments": ["looks like libpng-dev is missing.\n", "sorry for the delay, but it seems libpng-dev is already installed\n\n```\n$ sudo apt-get install libpng-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nNote, selecting 'libpng12-dev' instead of 'libpng-dev'\nlibpng12-dev is already the newest version.\nlibpng12-dev set to manually installed.\n0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n```\n", "@petewarden Is `IS_MOBILE_PLATFORM` defined for Raspberry Pi?  If so, what's the appropriate `PLATFORM_<foo>` macro?\n", "Is this the makefile build?", "Closing due to lack of response.  Please reopen if necessary."]}, {"number": 3630, "title": "TF works in python3.4 for some users but not for others under RedHat7 ", "body": "We have a RedHat7 machine where our sysadmin, per my request, has installed TF 0.9.0 for both python2.7 and python3.4. Both versions of python are installed centrally under /usr/bin. \n\nWhen installing, he followed the \"pip install\" instructions from the TF page for CPU-only 64-bit 2.7 and 3.4 installs.\n\nFor both me and him, importing tenserflow in python2.7 works fine.\n\nFor him, importing TF in python3.4 works fine as well.\n\nBut for me, it generates an error. For some reason, it attempts to load the 2.7 version of the TF package. For him, it loads the 3.4 version as expected.\n\nWhen I ran the command `python3 -v -c 'import tensorflow'`, I get, among other things, this:\n\n`# /usr/lib/python2.7/site-packages/tensorflow/__pycache__/__init__.cpython-34.pyc matches /usr/lib/python2.7/site-packages/tensorflow/__init__.py`\n`# code object from '/usr/lib/python2.7/site-packages/tensorflow/__pycache__/__init__.cpython-34.pyc'`\n`# /usr/lib/python2.7/site-packages/tensorflow/python/__pycache__/__init__.cpython-34.pyc matches /usr/lib/python2.7/site-packages/tensorflow/python/__init__.py`\n`# code object from '/usr/lib/python2.7/site-packages/tensorflow/python/__pycache__/__init__.cpython-34.pyc'`\n\nWhen he runs the same command, he gets:\n\n`# /usr/lib/python3.4/site-packages/tensorflow/__pycache__/__init__.cpython-34.pyc matches /usr/lib/python3.4/site-packages/tensorflow/__init__.py`\n`# code object from '/usr/lib/python3.4/site-packages/tensorflow/__pycache__/__init__.cpython-34.pyc'`\n`# /usr/lib/python3.4/site-packages/tensorflow/python/__pycache__/__init__.cpython-34.pyc matches /usr/lib/python3.4/site-packages/tensorflow/python/__init__.py`\n`# code object from '/usr/lib/python3.4/site-packages/tensorflow/python/__pycache__/__init__.cpython-34.pyc'`\n\n`PYTHONPATH` is defined for neither of us. \n\nWhy is there this difference and how can it be fixed?\n\nThank you!\n", "comments": ["There seems to be a misconfiguration on this machine causing python3 to look for Python code in the 2.7 directories. It's hard for us to help with this, as it is probably not related to TensorFlow specifically. I will close this issue. If you find evidence that there is a problem with TensorFlow, feel free to reopen it.\n"]}, {"number": 3629, "title": "Problems in \"Implement the gradient in Python\" docs", "body": "Referring to the docs [here](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html#implement-the-gradient-in-python)\n\nFirst of all, the chain rule is completely wrong. Also the `ZeroOut` example is not a very good one IMO... would be much better to give e.g. a simple derivative of a polynomial or something. Just a FYI.\n\nEDIT: I'd like to emphasize that the chain rule is _embarrasingly_ wrong...\n", "comments": ["@harpone It's only embarrassingly wrong if one is irrationally embarrassed by making mistakes!\n", "@harpone To clarify: by \"embarrassingly wrong\" all you mean is that it should be `dL/dx = dL/dy dy/dx` to avoid implying that `d/dy` is differentiating `dy/dx`?\n", "Yup, exactly! Was a bit tired yesterday so maybe I overreacted a bit though :)\n\nAnyway, for example the abs + it's gradient would probably make for a better example?\n", "Oops actually maybe `abs` is not a good example, see [this](https://github.com/tensorflow/tensorflow/issues/3647) (possible) bug.\n", "@harpone Are you happy with this chain rule fix?  https://github.com/girving/tensorflow/commit/05e7bc317317cc868710db2762acaab87b8c17a4\n", "Sure, that's much better!\n\nOn Thu, Aug 4, 2016, 22:27 Geoffrey Irving notifications@github.com wrote:\n\n> @harpone https://github.com/harpone Are you happy with this chain rule\n> fix? girving@05e7bc3\n> https://github.com/girving/tensorflow/commit/05e7bc317317cc868710db2762acaab87b8c17a4\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3629#issuecomment-237657178,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AE4ECBBf55lWbBIFhyDFPsUhjk_DbMrPks5qcj0ygaJpZM4Jb8ob\n> .\n", "So the PR to fix the chain rule mention is https://github.com/tensorflow/tensorflow/pull/3650.  As for the example, @harpone: do you want to personally fix it?  It's a nontrivial amount of work for a marginal gain, since the goal of the tutorial is to teach people how to use the API, not to teach them calculus.\n", "I'm afraid I don't have the time right now, but I may submit a PR related\non another thing (log abs det gradient etc.) some time in the future, so I\ncould also do this in the same PR? No promises however, it could take a\nwhile...\n\nOn Fri, Aug 5, 2016 at 1:22 AM Geoffrey Irving notifications@github.com\nwrote:\n\n> So the PR to fix the chain rule mention is #3650\n> https://github.com/tensorflow/tensorflow/pull/3650. As for the example,\n> @harpone https://github.com/harpone: do you want to personally fix it?\n> It's a nontrivial amount of work for a marginal gain, since the goal of the\n> tutorial is to teach people how to use the API, not to teach them calculus.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3629#issuecomment-237701078,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AE4ECPvMnsLv3IHQVEMvTasNEBb_tacKks5qcmYlgaJpZM4Jb8ob\n> .\n", "@harpone That sounds like it should be a separate PR.  I'll close this one for now.\n"]}, {"number": 3628, "title": "Unable to import frozen graph with batchnorm", "body": "Error when loading the frozen graph with [tensorflow.contrib.layers.python.layers.batch_norm](https://github.com/tensorflow/tensorflow/blob/88d9bc16d6a16e5b660cda548b74944f27ddcd1b/tensorflow/contrib/layers/python/layers/layers.py)\n`\nValueError: graph_def is invalid at node u'BatchNorm/cond/AssignMovingAvg/Switch': Input tensor 'BatchNorm/moving_mean:0' Cannot convert a tensor of type float32 to an input of type float32_ref\n`\n[freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py)  doesn't seem to store moving_mean and moving_variance properly\n", "comments": ["An ugly way to get it working:\nmanually replace the wrong node definitions in the frozen graph\n`\nRefSwitch --> Switch + add '/read' to the input names\n`\n`\nAssignSub --> Sub + remove use_locking attributes \n`\n", "@petewarden Do you know how freeze_graph.py handle conditional assignments?\n", "@sguada Any update with this? Without this fixed I don't think exporting a frozen graph (for use in C++) is possible when using Batch Norm. \n", "I also have the same problem. @petewarden \n", "I think I have the same issue.  Pasting my output below in case it helps:\n\n``` python\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/Users/pkmital/.pyenv/versions/3.4.0/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\n    354             # pylint: disable=protected-access\n--> 355             op._add_input(source_tensor, dtype=input_type)\n    356             # pylint: enable=protected-access\n\n/Users/pkmital/.pyenv/versions/3.4.0/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/framework/ops.py in _add_input(self, tensor, dtype)\n   1333             \"Cannot convert a tensor of type %s to an input of type %s\"\n-> 1334             % (tensor.dtype.name, dtype.name))\n   1335     self._inputs.append(tensor)\n\nTypeError: Cannot convert a tensor of type float32 to an input of type float32_ref\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-48-371a235edd1f> in <module>()\n----> 1 tf.import_graph_def(net['graph_def'])\n      2 g = tf.get_default_graph()\n      3 [op.name for op in g.get_operations()]\n\n/Users/pkmital/.pyenv/versions/3.4.0/Python.framework/Versions/3.4/lib/python3.4/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\n    357           except TypeError as te:\n    358             raise ValueError(_InvalidNodeMessage(\n--> 359                 node, 'Input tensor %r %s' % (input_name, te)))\n    360 \n    361       # pylint: disable=protected_access\n\nValueError: graph_def is invalid at node 'encoder/0/bn/ExponentialMovingAverage/AssignMovingAvg': Input tensor 'encoder/0/bn/encoder/0/bn/moments/moments_1/mean/ExponentialMovingAverage:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\n```\n\nYou can also try the model for yourself here: https://s3.amazonaws.com/cadl/models/celeb_vaegan.tfmodel\n", "Bump\n", "This is hitting me too; it's a bad bug that makes it hard to use BatchNorm in production settings.", "I'm also experiencing this. It seems that moving_averages._zero_debias() uses scopes in a peculiar way.\r\n\r\nFor example, if you pass assign_moving_average a variable created within a variable scope, e.g with name \"scope1/var\", then _zero_debias() creates its variables \"biased\" and \"local_step\" within the variable scope \"scope1/var\", but because we're already within the scope \"scope1\" the variables end up with names \"scope1/scope1/var/biased\" etc.\r\n\r\nThis may not be the problem, but it seems like something that needs tidying up!\r\n", "@pavelgonchar Your suggestion didn't work for me:\r\n\r\n```python\r\n# read graph definition\r\nf = tf.python.platform.gfile.FastGFile(model_path)\r\ngd = graph_def = tf.GraphDef()\r\ngraph_def.ParseFromString(f.read())\r\n\r\n# fix nodes\r\nfor node in graph_def.node:\r\n  if node.op == 'RefSwitch':\r\n    node.op = 'Switch'\r\n    for index in xrange(len(node.input)):\r\n      node.input[index] = node.input[index] + '/read'\r\n  elif node.op == 'AssignSub':\r\n    node.op = 'Sub'\r\n    if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n# import graph into session\r\ntf.import_graph_def(graph_def, name='')\r\n```\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 357, in import_graph_def\r\n    % (input_name,)))\r\nValueError: graph_def is invalid at node u'conv1/BatchNorm/cond/AssignMovingAvg/Switch': Input tensor 'conv1/BatchNorm/cond/pred_id/read:0' not found in graph_def..\r\n```", "@pavelgonchar  This has worked for me:\r\n\r\n```python\r\n# read graph definition\r\nf = tf.python.platform.gfile.FastGFile(model_path)\r\ngd = graph_def = tf.GraphDef()\r\ngraph_def.ParseFromString(f.read())\r\n\r\n# fix nodes\r\nfor node in graph_def.node:\r\n  if node.op == 'RefSwitch':\r\n    node.op = 'Switch'\r\n    for index in xrange(len(node.input)):\r\n      if 'moving_' in node.input[index]:\r\n        node.input[index] = node.input[index] + '/read'\r\n  elif node.op == 'AssignSub':\r\n    node.op = 'Sub'\r\n    if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n# import graph into session\r\ntf.import_graph_def(graph_def, name='')\r\n```\r\n\r\nI've only changed the inputs related to the \"moving_variance\" and \"moving_mean\".", "The full script I use to convert a checkpoint model to a protobuf graph is below, in case more people using batch norm layers find it useful.\r\n\r\n```python\r\n\"\"\"\r\nConvert model.ckpt to model.pb\r\n\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\n\r\n# create a session\r\nsess = tf.Session()\r\n\r\n# import best model\r\nsaver = tf.train.import_meta_graph('model.ckpt.meta') # graph\r\nsaver.restore(sess, 'model.ckpt') # variables\r\n\r\n# get graph definition\r\ngd = sess.graph.as_graph_def()\r\n\r\n# fix batch norm nodes\r\nfor node in gd.node:\r\n  if node.op == 'RefSwitch':\r\n    node.op = 'Switch'\r\n    for index in xrange(len(node.input)):\r\n      if 'moving_' in node.input[index]:\r\n        node.input[index] = node.input[index] + '/read'\r\n  elif node.op == 'AssignSub':\r\n    node.op = 'Sub'\r\n    if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n# generate protobuf\r\nconverted_graph_def = graph_util.convert_variables_to_constants(sess, gd, [\"logits_set\"])\r\ntf.train.write_graph(converted_graph_def, '/path/to/save/', 'model.pb', as_text=False)\r\n```\r\n", "@barbolo is that still true of recent versions of TF?\r\n@gunan in case we have to cherry-pick.\r\n@zheng-xq FYI.", "@drpngx I use TensorFlow 0.12.0rc0. I can check the exact commit tomorrow if you need that information.", "Thanks!", "The script @barbolo provided failed for me on an ExponentialMovingAverage node; I added `AssignAdd` just like `AssignSub`, then it worked.", "Might this issue be related to / the same as #4044? ", "I am running TensorFlow 0.12. I tried the solution provided by @barbolo and @sunsided, but it didn't solve the problem. I still get this error:\r\n\r\n`\r\nValueError: graph_def is invalid at node 'InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/cond/AssignMovingAvg/Switch': Input tensor 'InceptionResnetV1/Conv2d_1a_3x3/BatchNorm/moving_mean:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n`", "@drpngx @gunan - I think the state machine got confused here.  @barbolo replied with his version but no tensorflower picked up the ball.", "@petewarden this seems to be a `freeze_graph` issue.\r\n\r\n@barbolo if you have an end-to-end script to exhibit the behavior, I could take a look.", "@drpngx I'll prepare a script that can be shared and that presents this issue.\r\n\r\n", "Thanks!\n\nOn Mar 14, 2017 4:50 AM, \"Rafael Barbolo\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> I'll prepare a script that can be\n> shared and that presents this issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-286398196>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbewczCw0D8L0Yuaow9XQnusc1_YSks5rln8LgaJpZM4Jb7rY>\n> .\n>\n", "I meet the same problem when deeling with BN in resnet, it seems that the function  ```inference_graph = extract_sub_graph(input_graph_def, output_node_names)``` in ```graph_util.convert_variables_to_constants()```cuts off the node of  moving_mean and moving_variance, I solve it in this way: save the model in train mode, then load the model in eval or test mode, run and  save again, freeze the last model, you will find the  moving_mean and moving_variance node. BTW, my goal is to get the variables such as mean, variance, and weight in the frozen model, I don't need to load it again.", "I'm using `tf.cond` and also got this error:\r\n\r\n```ValueError: graph_def is invalid at node 'RNN1/cond/Assign/Switch': Input tensor 'RNN1/Variable:0' Cannot convert a tensor of type float32 to an input of type float32_ref.```", "The workaround of @barbolo  worked for me (for python 3 change `xrange` to `range`. But would be nice if native tensorflow would allow to freeze a graph with batch norm without these kind of workarounds!", "I encountered an error \r\n`ValueError: graph_def is invalid at node 'conv3_1/BatchNorm/AssignMovingAvg': Input tensor 'conv3_1/BatchNorm/moving_mean:0' Cannot convert a tensor of type float32 to an input of type float32_ref.` \r\nwhen using BatchNorm layer in slim. Not sure how to solve it...", "@ebrevdo Is this related to tf.cond dependencies?", "It seems like the (local?) variable moving_mean is not being stored properly as a ref type.  @sguada does tf.contrib.layers.batch_norm work with the new resource variables?  If so, try creating your initial graph inside a tf.variable_scope(..., use_resource=True) and see if it'll properly save/restore.", "@ebrevdo Thanks for your suggestion. `tf.variable_scope(..., use_resource=True)` does not help.", "I am also experiencing the same problem with reading BatchNorm variance values. I tried to fix with script provided by @barbolo but it doesn't seem to work for me(TF version 1.2.0). Any help or inputs regarding the same?", "I was recently able to freeze a graph using batch norm (via `tf.layers.batch_normalization`) and optimize it, although I had to use the [graph transform tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) to do the latter part and not the old optimization tool.", "I am facing similar issue but for GRU. I am using tensorflow 1.1.0 and I tried dumping the model in different ways:\r\na) saver = tf.train.Saver(tf.global_variables())\r\nmodel_exporter = exporter.Exporter(saver)\r\n\r\n    # Restore variables from training checkpoint\r\n    # TODO: This restores the most recent checkpoint, but if we use validation to counterract\r\n    #       over-fitting, we may want to restore an earlier checkpoint.\r\n    checkpoint = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\r\n    checkpoint_path = checkpoint.model_checkpoint_path\r\n    saver.restore(session, checkpoint_path)\r\n    log_info('Restored checkpoint at training epoch %d' % (int(checkpoint_path.split('-')[-1]) + 1))\r\n\r\n    # Initialise the model exporter and export the model\r\n    model_exporter.init(session.graph.as_graph_def(),\r\n                        named_graph_signatures = {\r\n                            'inputs': exporter.generic_signature(\r\n                                { 'input': input_tensor,\r\n                                  'input_lengths': seq_length}),\r\n                            'outputs': exporter.generic_signature(\r\n                                { 'outputs': decoded})})\r\n    if FLAGS.remove_export:\r\n        actual_export_dir = os.path.join(FLAGS.export_dir, '%08d' % FLAGS.export_version)\r\n        if os.path.isdir(actual_export_dir):\r\n            log_info('Removing old export')\r\n            shutil.rmtree(actual_FLAGS.export_dir)\r\n    try:\r\n        # Export serving model\r\n        model_exporter.export(FLAGS.export_dir, tf.constant(FLAGS.export_version), session)\r\n\r\n        # Export graph\r\n        input_graph_name = 'input_graph.pb'\r\n        tf.train.write_graph(session.graph, FLAGS.export_dir, input_graph_name, as_text=False)\r\n\r\n        # Freeze graph\r\n        input_graph_path = os.path.join(FLAGS.export_dir, input_graph_name)\r\n        input_saver_def_path = ''\r\n        input_binary = True\r\n        output_node_names = 'output_node'\r\n        restore_op_name = 'save/restore_all'\r\n        filename_tensor_name = 'save/Const:0'\r\n        output_graph_path = os.path.join(FLAGS.export_dir, 'output_graph.pb')\r\n        clear_devices = False\r\n        freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\n                                  input_binary, checkpoint_path, output_node_names,\r\n                                  restore_op_name, filename_tensor_name,\r\n                                  output_graph_path, clear_devices, '')\r\nb) output_graph_def = graph_util.convert_variables_to_constants(session, session.graph.as_graph_def(), ['output_node'])\r\nwith gfile.FastGFile('./data/ldc93s1/output_graph2.pb', 'wb') as f:\r\nf.write(output_graph_def.SerializeToString())\r\n\r\nbut for both the dump I get the following error: -\r\n****  File \"converter.py\", line 22, in <module>\r\n    graphdef_to_pbtxt('./data/ldc93s1/output_graph.pb')\r\n  File \"converter.py\", line 17, in graphdef_to_pbtxt\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"/home/prashant/miniconda2/envs/mozilla/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 388, in import_graph_def\r\n    node, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node u'rnn/while/multi_rnn_cell/cell_0/gru_cell/gates/r/cond/rnn/while/multi_rnn_cell/cell_0/gru_cell/gates/r/strided_slice/_assign/RefEnter': Input tensor 'rnn/multi_rnn_cell/cell_0/gru_cell/gates/r/pop_mean:0' Cannot convert a tensor of type float32 to an input of type float32_ref**\r\n\r\nAny solution so far?", "The same problem in tf1.2.\r\nIs there anyone solve the problem?", "same problem encountered...", "I have sovled the problem, just remove some node used when training but not used when inference.\r\n\r\nIt worked for me!", "@tangyanlin Can you tell me how you made it work? How you identified unused node and removed it?", "Have you tried to blacklist the problematic variables (e.g. BatchNorm/moving_mean:0 in the initial error message) with the --variable_names_blacklist argument?\r\n\r\nI had the same issue with my graph, which were solved this way.\r\n\r\nThe problematic variables were used in model to transfer the states of rnn cells from one iteration to another, these really had to remain variables and must not be converted to constants.", "@asoehlke  Can you elaborate more on how you solved this problem. I haven't found a solution to it.", "@prashantmaheshwari94: I added a more detailed description to #14452. \r\nIt seems that we run into trouble as soon as a variable is involved which is modified within the model and thus must not be converted to a constant. \r\nA GRU cell also has the internal state variables like the LSTM cell that I use, so there should be similar problems.", "The code in https://github.com/davidsandberg/facenet/issues/161 works for me.", "hope this bug be solved in a shorter time , cause this bug is in c++ core . \r\ni only use c++ core and cross compile tensor to various devices , \r\nany way , here's my c++ \"ugly\" version for this bug ..\r\n\r\n```\r\nvoid MyTensorflow::SolveIssue_MovingAverage(GraphDef & graph_def)\r\n{\r\n\r\n    int i;\r\n    int j;\r\n    for (i = 0; i < graph_def.node_size(); i++)\r\n    {\r\n        if (graph_def.mutable_node(i)->op() == \"RefSwitch\")\r\n        {\r\n            graph_def.mutable_node(i)->set_op(\"Switch\");\r\n            for (j = 0; j < graph_def.mutable_node(i)->input_size(); j++)\r\n            {\r\n                if (string::npos != graph_def.mutable_node(i)->mutable_input(j)->find(\"moving_\"))\r\n                {\r\n                    graph_def.mutable_node(i)->set_input(j, graph_def.mutable_node(i)->input(j) + \"/read\");\r\n                }\r\n            }\r\n        }\r\n        else if (graph_def.mutable_node(i)->op() == \"AssignSub\")\r\n        {\r\n            graph_def.mutable_node(i)->set_op(\"Sub\");\r\n\r\n            auto it = graph_def.mutable_node(i)->mutable_attr()->end();\r\n\r\n            for (it = graph_def.mutable_node(i)->mutable_attr()->begin(); it != graph_def.mutable_node(i)->mutable_attr()->end(); it++)\r\n            {\r\n                if (it->first == \"use_locking\")\r\n                {\r\n                    break;\r\n                }\r\n            }\r\n\r\n            if (it != graph_def.mutable_node(i)->mutable_attr()->end())\r\n            {\r\n                graph_def.mutable_node(i)->mutable_attr()->erase(it);\r\n            }\r\n        }\r\n    }\r\n\r\n}\r\n```", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Plz. give me a solution\r\n:cry: ", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm facing the same problem using models provided by [slim.](https://github.com/tensorflow/models/tree/master/research/slim/nets) on TensorFlow 1.4.0 and training using the Estimator API\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary - from pip\r\n- TensorFlow version: release 1.4.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): - Not applicable\r\n- GCC/Compiler version (if compiling from source): - Not applicable\r\n- CUDA/cuDNN version: 8/6\r\n- GPU model and memory: GTX 770, 4GB\r\n\r\nThe workaround that worked for me:\r\n\r\n1. Create the  graph from the source code  \r\n2. Create my own input placeholder (as opposed to the batch iterator from estimator API)\r\n3. Restore trained checkpoint \r\n4. Save a new checkpoint\r\n5. Freeze graph\r\n6. Quantize (optinal)\r\n\r\nThe code below is the one I'm currently using for steps 1-4\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nfrom cnn_architecture import cnn_architecture # Define your architecture here\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n# Set default flags for the output directories\r\nFLAGS = tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_string(\r\n    flag_name='checkpoint_path', default_value='',\r\n    docstring='Checkpoint path')\r\ntf.app.flags.DEFINE_string(\r\n    flag_name='output_checkpoint_path', default_value='',\r\n    docstring='Checkpoint path')\r\ntf.app.flags.DEFINE_integer(flag_name='image_size',\r\n                            default_value=200, docstring=\"Image size\")\r\n\r\ndef load_and_save_ckpt():\r\n\r\n    # Create placeholders\r\n    X = tf.placeholder(dtype=tf.float32, shape=(\r\n        None, FLAGS.image_size, FLAGS.image_size, 3), name='input')\r\n\r\n    # Load net architecture\r\n    endpoint = cnn_architecture(X, is_training=False)\r\n\r\n    # Add softmax layer\r\n    sm_endpoint = tf.nn.softmax(endpoint, name=\"sm_endpoint\")\r\n\r\n    saver = tf.train.Saver()\r\n\r\n    # Open session\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, FLAGS.checkpoint_path)\r\n        saver.save(sess, FLAGS.output_checkpoint_path)\r\n\r\nif __name__ == \"__main__\":\r\n    load_and_save_ckpt()\r\n```", "This is still an issue, and it seems to be related to the way we handle references in TensorFlow when freezing. I haven't had time to investigate, but it's come up repeatedly so it seems worth keeping alive. I'm cc-ing Suharsh since he's looking at related areas around graph freezing.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Not convert  ALL nodes to constants.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "same problem...", "@barbolo your answer works for me!!!! \r\nyou really save my project..\r\nso many thanks!", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@pavelgonchar I want to know what I should do with Java API ??\r\nThat is to say, I donnot know how to transfer the following code with JAVA:\r\n# fix batch norm nodes\r\nfor node in gd.node:\r\n  if node.op == 'RefSwitch':\r\n    node.op = 'Switch'\r\n    for index in xrange(len(node.input)):\r\n      if 'moving_' in node.input[index]:\r\n        node.input[index] = node.input[index] + '/read'\r\n  elif node.op == 'AssignSub':\r\n    node.op = 'Sub'\r\n    if 'use_locking' in node.attr: del node.attr['use_locking']", "@barbolo  I want to know what I should do with Java API ??\r\nThat is to say, I donnot know how to transfer the following code with JAVA:\r\nfor node in gd.node:\r\nif node.op == 'RefSwitch':\r\nnode.op = 'Switch'\r\nfor index in xrange(len(node.input)):\r\nif 'moving_' in node.input[index]:\r\nnode.input[index] = node.input[index] + '/read'\r\nelif node.op == 'AssignSub':\r\nnode.op = 'Sub'\r\nif 'use_locking' in node.attr: del node.attr['use_locking']", "I found another work-around for this. Our implementation of batch norm was using tf.cond() to distinguish between training-time and test-time behavior. At training time, the variables in batch norm have to be updated. This causes an error when those variables are converted to constants. \r\n\r\nWhen freezing a graph for inference only, the update operations are still present in the frozen graph because tf.cond() chooses the behavior at run-time, not compile-time. The easiest solution for me was to generate two graphs that share all of their variables, one for training and one for testing. This way you can eliminate the call to tf.cond() and distinguish behaviors at compile time. Then Tensorflow correctly removes all the update operations when calling tf.graph_util.convert_variables_to_constants() on the inference output.\r\n\r\nAs other users have pointed out, one could also fix this using the 'blacklist' option in tf.graph_util.convert_variables_to_constants(). The downside of this is that unneeded ops are still present in the frozen graph.\r\n\r\nIs there going to be a more comprehensive patch for this any time soon?  I am surprised this issue has been open for almost a year with no action. This seems like a very big issue, since batch norm is so useful for training large, complex networks. It is not always practical to re-train the network without batch norm for deployment. Users should not be relying on hacks that edit the graph after the fact.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@barbolo . i did your suggestion to delete Some Nodes and I saved the model correctly as pb format,\r\nbut unfortunately, when I run the model in c++ I got the same prediction results. furthermore, I used TFlearn to create my model in Tensorflow.\r\nthe layers that i used in TFlearn are (tflearn.layers.normalization.batch_normalization, max_pool_2d, conv_2d, dropout)\r\n\r\n# **The code in python:** \r\n\r\ndef freeze_graph(model):\r\noutput_node_names = \"FullyConnected_2/Softmax\"# \"FullyConnected_2\" #input/X,\r\ngraph = model.net.graph\r\n\r\nsess = model.session\r\n\r\nfor v in sess.graph.get_operations():\r\n  print(v.name)\r\n\r\ngd = sess.graph.as_graph_def()\r\n\r\nfor node in gd.node:            \r\n    if node.op == 'RefSwitch':\r\n        node.op = 'Switch'\r\n        for index in range(len(node.input)):\r\n            if 'moving_' in node.input[index]:\r\n                node.input[index] = node.input[index] + '/read'\r\n    elif node.op == 'AssignSub':\r\n        node.op = 'Sub'\r\n        if 'use_locking' in node.attr: del node.attr['use_locking']\r\n    elif node.op == 'AssignAdd':\r\n        node.op = 'Add'\r\n        if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\noutput_graph_def = graph_util.convert_variables_to_constants(\r\n    sess, # The session is used to retrieve the weights\r\n    gd, # The graph_def is used to retrieve the nodes \r\n    output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n) \r\n\r\nwith tf.gfile.GFile(output_graph, \"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())\r\nprint(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n\r\n#  Run the session in c++\r\n\r\nstring input_layer = \"input/X:0\";//\"input/X:0\";\r\nstring output_layer = \"FullyConnected_2/Softmax:0\";\r\n\r\nStatus status = m_session->Run({{input_layer,resized_tensor}},\r\n{output_layer}, {}, &outputs);\r\nstd::cout << outputs[0].DebugString() << \"\\n\"; ----> the output is constant for all images", "This code didn't work for me. None of my ops were converted into const ops and the resulting graph was empty.\r\n\r\nI found a solution by using code from Modified from https://gist.github.com/moodoki/e37a85fb0258b045c005ca3db9cbc7f6", "thanks, @andrewginns for your answer. \r\nthe problem with me in the suggestion code to freeze the model is (importing meta graph), that causes me this error:\r\nKeyError: \"The name 'Adam' refers to an Operation not in the graph.\"\r\n\r\nthis is the reason why I used the trained model directly after the training phase to create a protobuf file, instead of saving the model as a meta file and from that creating a protobuf pb. \r\n\r\n\r\n", "Nagging Assignees @petewarden, @suharshs: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@barbolo Thanks, your solution works for me! ", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since @barbolo 's solution seems to work.\r\n\r\nIn general the best route is to create a separate eval graph with is_training=False for batchnorm, freeze the training checkpoint into that graph. \r\n\r\nThanks!", "@barbolo  I used the API tf.layers . I printed the operation in my net. I didn't found RefSwitch, So I can't resolve the question through your suggestion. The error ValueError: graph_def is invalid at node 'conv1/kernel/Assign': Input tensor 'conv1/kernel:0' Cannot convert a tensor of type float32 to an input of type float32_ref. Please Do you any suggestion?\r\n\r\n\r\nconv1/kernel/Initializer/random_uniform/shape\r\nconv1/kernel/Initializer/random_uniform/min\r\nconv1/kernel/Initializer/random_uniform/max\r\nconv1/kernel/Initializer/random_uniform/RandomUniform\r\nconv1/kernel/Initializer/random_uniform/sub\r\nconv1/kernel/Initializer/random_uniform/mul\r\nconv1/kernel/Initializer/random_uniform\r\nconv1/kernel\r\nconv1/kernel/Assign\r\nconv1/kernel/read\r\nconv1/bias/Initializer/zeros\r\nconv1/bias\r\nconv1/bias/Assign\r\nconv1/bias/read\r\nconv1/dilation_rate\r\nconv1/Conv2D\r\nconv1/BiasAdd\r\nconv1/Relu\r\nbatch_normalization/gamma/Initializer/ones\r\nbatch_normalization/gamma\r\nbatch_normalization/gamma/Assign\r\nbatch_normalization/gamma/read\r\nbatch_normalization/beta/Initializer/zeros\r\nbatch_normalization/beta\r\nbatch_normalization/beta/Assign\r\nbatch_normalization/beta/read\r\nbatch_normalization/moving_mean/Initializer/zeros\r\nbatch_normalization/moving_mean\r\nbatch_normalization/moving_mean/Assign\r\nbatch_normalization/moving_mean/read\r\nbatch_normalization/moving_variance/Initializer/ones\r\nbatch_normalization/moving_variance\r\nbatch_normalization/moving_variance/Assign\r\nbatch_normalization/moving_variance/read\r\nbatch_normalization/cond/Switch\r\nbatch_normalization/cond/switch_t\r\nbatch_normalization/cond/switch_f\r\nbatch_normalization/cond/pred_id\r\nbatch_normalization/cond/Const\r\nbatch_normalization/cond/Const_1\r\nbatch_normalization/cond/FusedBatchNorm/Switch\r\nbatch_normalization/cond/FusedBatchNorm/Switch_1\r\nbatch_normalization/cond/FusedBatchNorm/Switch_2\r\nbatch_normalization/cond/FusedBatchNorm\r\nbatch_normalization/cond/FusedBatchNorm_1/Switch\r\nbatch_normalization/cond/FusedBatchNorm_1/Switch_1\r\nbatch_normalization/cond/FusedBatchNorm_1/Switch_2\r\nbatch_normalization/cond/FusedBatchNorm_1/Switch_3\r\nbatch_normalization/cond/FusedBatchNorm_1/Switch_4\r\nbatch_normalization/cond/FusedBatchNorm_1\r\nbatch_normalization/cond/Merge\r\nbatch_normalization/cond/Merge_1\r\nbatch_normalization/cond/Merge_2\r\nbatch_normalization/ExpandDims/input\r\nbatch_normalization/ExpandDims/dim\r\nbatch_normalization/ExpandDims\r\nbatch_normalization/ExpandDims_1/input\r\nbatch_normalization/ExpandDims_1/dim\r\nbatch_normalization/ExpandDims_1\r\nbatch_normalization/Reshape/shape\r\nbatch_normalization/Reshape\r\nbatch_normalization/Select\r\nbatch_normalization/Squeeze\r\nbatch_normalization/AssignMovingAvg/read\r\nbatch_normalization/AssignMovingAvg/Sub\r\nbatch_normalization/AssignMovingAvg/Mul\r\nbatch_normalization/AssignMovingAvg\r\nbatch_normalization/AssignMovingAvg_1/read\r\nbatch_normalization/AssignMovingAvg_1/Sub\r\nbatch_normalization/AssignMovingAvg_1/Mul\r\nbatch_normalization/AssignMovingAvg_1", "@petewarden this is still a problem. severely limits the ability to put models in production with batch norm (which are most models...)", "same problem here, I trained my model and tried to import the frozen graph with batch norm nodes: \r\nfollowing the Complete Traceback : \r\n\r\n`---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nc:\\users\\shivam.agarwal\\pycharmprojects\\audioapi\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    417         results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 418             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    419         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: Node 'Test_Net/convolution/normalize_input/Test_Net/convolution/normalize_input/mean/ExponentialMovingAverage/read' expects to be colocated with unknown node 'Test_Net/convolution/normalize_input/mean'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-5efe49866b97> in <module>()\r\n----> 1 read_graph_proto(as_constants=True)\r\n\r\n<ipython-input-5-e1593c13b4b1> in read_graph_proto(proto_dir, batch_number, as_constants, new_filename_queue)\r\n     25                     tf.import_graph_def(graph_def=graph_def)\r\n     26             else:\r\n---> 27                 tf.import_graph_def(graph_def=graph_def)\r\n     28             print([node for node in self.session.graph_def.node])\r\n     29     else:\r\n\r\nc:\\users\\shivam.agarwal\\pycharmprojects\\audioapi\\venv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n    452                 'in a future version' if date is None else ('after %s' % date),\r\n    453                 instructions)\r\n--> 454       return func(*args, **kwargs)\r\n    455     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    456                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\nc:\\users\\shivam.agarwal\\pycharmprojects\\audioapi\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    420       except errors.InvalidArgumentError as e:\r\n    421         # Convert to ValueError for backwards compatibility.\r\n--> 422         raise ValueError(str(e))\r\n    423 \r\n    424     # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: Node 'Test_Net/convolution/normalize_input/Test_Net/convolution/normalize_input/mean/ExponentialMovingAverage/read' expects to be colocated with unknown node 'Test_Net/convolution/normalize_input/mean'\r\n`\r\n\r\nHow can I sort out this ? I have been stuck since months now because of this one issue. ", "Same problem\r\nI trined GooglNet model and get the error when importing a frozen graph\r\n\r\n`ValueError: Input 0 of node save/Assign_41 was passed float from auxiliary_classifier_1/classifier/biases/Adam_1:0 incompatible with expected float_ref.\r\n`\r\n\r\nNone of the answers I found on the net do not answer this specific situation, where the problem is in the save op and Adam optimizer\r\n", "> @barbolo I used the API tf.layers . I printed the operation in my net. I didn't found RefSwitch, So I can't resolve the question through your suggestion. The error ValueError: graph_def is invalid at node 'conv1/kernel/Assign': Input tensor 'conv1/kernel:0' Cannot convert a tensor of type float32 to an input of type float32_ref. Please Do you any suggestion?\r\n> \r\n> conv1/kernel/Initializer/random_uniform/shape\r\n> conv1/kernel/Initializer/random_uniform/min\r\n> conv1/kernel/Initializer/random_uniform/max\r\n> conv1/kernel/Initializer/random_uniform/RandomUniform\r\n> conv1/kernel/Initializer/random_uniform/sub\r\n> conv1/kernel/Initializer/random_uniform/mul\r\n> conv1/kernel/Initializer/random_uniform\r\n> conv1/kernel\r\n> conv1/kernel/Assign\r\n> conv1/kernel/read\r\n> conv1/bias/Initializer/zeros\r\n> conv1/bias\r\n> conv1/bias/Assign\r\n> conv1/bias/read\r\n> conv1/dilation_rate\r\n> conv1/Conv2D\r\n> conv1/BiasAdd\r\n> conv1/Relu\r\n> batch_normalization/gamma/Initializer/ones\r\n> batch_normalization/gamma\r\n> batch_normalization/gamma/Assign\r\n> batch_normalization/gamma/read\r\n> batch_normalization/beta/Initializer/zeros\r\n> batch_normalization/beta\r\n> batch_normalization/beta/Assign\r\n> batch_normalization/beta/read\r\n> batch_normalization/moving_mean/Initializer/zeros\r\n> batch_normalization/moving_mean\r\n> batch_normalization/moving_mean/Assign\r\n> batch_normalization/moving_mean/read\r\n> batch_normalization/moving_variance/Initializer/ones\r\n> batch_normalization/moving_variance\r\n> batch_normalization/moving_variance/Assign\r\n> batch_normalization/moving_variance/read\r\n> batch_normalization/cond/Switch\r\n> batch_normalization/cond/switch_t\r\n> batch_normalization/cond/switch_f\r\n> batch_normalization/cond/pred_id\r\n> batch_normalization/cond/Const\r\n> batch_normalization/cond/Const_1\r\n> batch_normalization/cond/FusedBatchNorm/Switch\r\n> batch_normalization/cond/FusedBatchNorm/Switch_1\r\n> batch_normalization/cond/FusedBatchNorm/Switch_2\r\n> batch_normalization/cond/FusedBatchNorm\r\n> batch_normalization/cond/FusedBatchNorm_1/Switch\r\n> batch_normalization/cond/FusedBatchNorm_1/Switch_1\r\n> batch_normalization/cond/FusedBatchNorm_1/Switch_2\r\n> batch_normalization/cond/FusedBatchNorm_1/Switch_3\r\n> batch_normalization/cond/FusedBatchNorm_1/Switch_4\r\n> batch_normalization/cond/FusedBatchNorm_1\r\n> batch_normalization/cond/Merge\r\n> batch_normalization/cond/Merge_1\r\n> batch_normalization/cond/Merge_2\r\n> batch_normalization/ExpandDims/input\r\n> batch_normalization/ExpandDims/dim\r\n> batch_normalization/ExpandDims\r\n> batch_normalization/ExpandDims_1/input\r\n> batch_normalization/ExpandDims_1/dim\r\n> batch_normalization/ExpandDims_1\r\n> batch_normalization/Reshape/shape\r\n> batch_normalization/Reshape\r\n> batch_normalization/Select\r\n> batch_normalization/Squeeze\r\n> batch_normalization/AssignMovingAvg/read\r\n> batch_normalization/AssignMovingAvg/Sub\r\n> batch_normalization/AssignMovingAvg/Mul\r\n> batch_normalization/AssignMovingAvg\r\n> batch_normalization/AssignMovingAvg_1/read\r\n> batch_normalization/AssignMovingAvg_1/Sub\r\n> batch_normalization/AssignMovingAvg_1/Mul\r\n> batch_normalization/AssignMovingAvg_1\r\n\r\nHi, I have met the same problem with you. Did you solved it?", "@XiaodanLi001 since I started using TensorFlow Estimator API I never had this problem again.\r\nHowever, there is still a trick. You do must define your training op within `tf.control_dependencies` context, like the snippet bellow, because although the batch_norm layers allocates some variables, they are not trainable, but updatable. So you do need to ensure that they are being updated at every training step.\r\nI hope it helps\r\n\r\n```python\r\n # You DO must get this collection in order to perform updates on batch_norm variables\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_op = optimizer.minimize(\r\n    loss=total_loss, global_step=tf.train.get_global_step())\r\n```\r\n\r\n*OBS:* I use `tf.layers` too.", "> This is hitting me too; it's a bad bug that makes it hard to use BatchNorm in production settings.\r\n\r\nyeah , Fuck tensorflow !!!", "I solved this problem by using the tf.layers.batch_normalization rather the tf.contrib.layers.batch_norm.", "# fix batch norm nodes\r\n        for node in input_graph_def.node:\r\n            if node.op == 'RefSwitch':\r\n                node.op = 'Switch'\r\n                for index in range(len(node.input)):\r\n                    if 'moving_' in node.input[index] and \"Switch\" not in node.input[index]:\r\n                        node.input[index] = node.input[index] + '/read'\r\n            elif node.op == 'AssignSub':\r\n                node.op = 'Sub'\r\n                if 'use_locking' in node.attr: del node.attr['use_locking']\r\n            elif node.op == 'AssignAdd':\r\n                node.op = 'Add'\r\n                if 'use_locking' in node.attr: del node.attr['use_locking']\r\ni add \"if 'moving_' in node.input[index] and \"Switch\" not in node.input[index]:\" and i have solved my problem, thanks!", "```\r\nwith tf.Session() as sess:\r\n    saved_model_dir = \"saved_model_dir_signature\"\r\n    meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], '')\r\n    for node in sess.graph_def.node:\r\n      if node.op == 'RefEnter':\r\n        node.op = 'Enter'\r\n        for index in range(len(node.input)):\r\n          if 'moving_' in node.input[index]:\r\n            node.input[index] = node.input[index] + '/read'\r\n      if node.op == 'RefSwitch':\r\n        node.op = 'Switch'\r\n        for index in range(len(node.input)):\r\n          if 'moving_' in node.input[index]:\r\n            node.input[index] = node.input[index] + '/read'\r\n      elif node.op == 'AssignSub':\r\n        node.op = 'Sub'\r\n        if 'use_locking' in node.attr: del node.attr['use_locking']\r\n      elif node.op == 'AssignAdd':\r\n        node.op = 'Add'\r\n        if 'use_locking' in node.attr: del node.attr['use_locking']\r\n```\r\nHow can i modify the graph_def in session? if i do it in this way, the model saved by sess didn't change from RefSwitch to Switch.\r\nCan someone tell me how to modify the graph_def in sess? thanks.", "> The full script I use to convert a checkpoint model to a protobuf graph is below, in case more people using batch norm layers find it useful.\r\n> \r\n> ```python\r\n> \"\"\"\r\n> Convert model.ckpt to model.pb\r\n> \"\"\"\r\n> \r\n> from __future__ import absolute_import\r\n> from __future__ import division\r\n> from __future__ import print_function\r\n> \r\n> import tensorflow as tf\r\n> from tensorflow.python.framework import graph_util\r\n> \r\n> # create a session\r\n> sess = tf.Session()\r\n> \r\n> # import best model\r\n> saver = tf.train.import_meta_graph('model.ckpt.meta') # graph\r\n> saver.restore(sess, 'model.ckpt') # variables\r\n> \r\n> # get graph definition\r\n> gd = sess.graph.as_graph_def()\r\n> \r\n> # fix batch norm nodes\r\n> for node in gd.node:\r\n>   if node.op == 'RefSwitch':\r\n>     node.op = 'Switch'\r\n>     for index in xrange(len(node.input)):\r\n>       if 'moving_' in node.input[index]:\r\n>         node.input[index] = node.input[index] + '/read'\r\n>   elif node.op == 'AssignSub':\r\n>     node.op = 'Sub'\r\n>     if 'use_locking' in node.attr: del node.attr['use_locking']\r\n> \r\n> # generate protobuf\r\n> converted_graph_def = graph_util.convert_variables_to_constants(sess, gd, [\"logits_set\"])\r\n> tf.train.write_graph(converted_graph_def, '/path/to/save/', 'model.pb', as_text=False)\r\n> ```\r\nnot bad\r\n", "> The full script I use to convert a checkpoint model to a protobuf graph is below, in case more people using batch norm layers find it useful.\r\n> \r\n> ```python\r\n> \"\"\"\r\n> Convert model.ckpt to model.pb\r\n> \"\"\"\r\n> \r\n> from __future__ import absolute_import\r\n> from __future__ import division\r\n> from __future__ import print_function\r\n> \r\n> import tensorflow as tf\r\n> from tensorflow.python.framework import graph_util\r\n> \r\n> # create a session\r\n> sess = tf.Session()\r\n> \r\n> # import best model\r\n> saver = tf.train.import_meta_graph('model.ckpt.meta') # graph\r\n> saver.restore(sess, 'model.ckpt') # variables\r\n> \r\n> # get graph definition\r\n> gd = sess.graph.as_graph_def()\r\n> \r\n> # fix batch norm nodes\r\n> for node in gd.node:\r\n>   if node.op == 'RefSwitch':\r\n>     node.op = 'Switch'\r\n>     for index in xrange(len(node.input)):\r\n>       if 'moving_' in node.input[index]:\r\n>         node.input[index] = node.input[index] + '/read'\r\n>   elif node.op == 'AssignSub':\r\n>     node.op = 'Sub'\r\n>     if 'use_locking' in node.attr: del node.attr['use_locking']\r\n> \r\n> # generate protobuf\r\n> converted_graph_def = graph_util.convert_variables_to_constants(sess, gd, [\"logits_set\"])\r\n> tf.train.write_graph(converted_graph_def, '/path/to/save/', 'model.pb', as_text=False)\r\n> ```\r\n\r\nHi.\r\nI don't have gpu and i can't run it.can you help me?\r\n", "Just an idea, since freeze works only with simple mathematical operations we can convert the important part of the graphs including batch norm to simple mathematical operation and freeze the graph", "Traceback (most recent call last):\r\n  File \"convert_TFLite.py\", line 90, in <module>\r\n    convert_from_savedModel(savedModelDir, TFLiteFile, quan=True, integerOnly=True)\r\n  File \"convert_TFLite.py\", line 50, in convert_from_savedModel\r\n    TFLiteModel = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/python/lite.py\", line 459, in convert\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 707, in convert_variables_to_constants_v2_as_graph\r\n    frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 406, in _construct_concrete_function\r\n    new_output_names)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 633, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 611, in wrap_function\r\n    collections={}),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 86, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 92, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/wrap_function.py\", line 631, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node conv21/pointwise/BatchNorm/cond/Assign/Switch was passed float from conv21/pointwise/BatchNorm/moving_mean:0 incompatible with expected float_ref.\r\n\r\nwhen I use \"tf.lite.TFLiteConverter.from_saved_model()\" to convert checkpoint to tflite\uff0cI got above err about BatchNorm, please help me, ths."]}, {"number": 3627, "title": "Tensorflow inability to kill processes using more than 1 GPU", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \nCUDA 7.5\nCUDNNv4\n\nGPUs:\n2x Tesla K40x\n2x GeForce GTX K40c\n\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n\nThis problem is specific to running tensorflow with multiple GPUs. When I try to restart the notebook in jupyter, if I have more than one GPU visible via `os.environ[\"CUDA_VISIBLE_DEVICES\"]`, the notebook freezes up, and I can no longer run nvidia-smi. \n\nThis problem does not exist when using a single GPU or no GPU on the same network.\n", "comments": ["@timsainb, I am not sure we are aware of this problem before. Could you share the simplest python code that has the problem? And if you run it through regular python, will it have the same problem? \n", "Closing due to lack of activity.  Please feel free to reopen if you have further information.\n"]}, {"number": 3626, "title": "contrib/makefile:  error: conflicting return type", "body": "### Environment info\n\nOS:   I'm running on Docker with an Ubuntu 14.04.4 LTS x86_64 (gcc/g++ version is 4.8.4)\nNo CUDA or cuDNN installed\n\ninstalled from source\n1. The commit hash (`git rev-parse HEAD`)\n   $ git rev-parse HEAD\n   27eeb441bad8bcaa1bcba42a4b4ee49fb50ea0d3\n   (note: I also tried with branch  r0.9 and  r0.10 with the same issue about conflicting return type)\n### Steps to reproduce\n\n```\n$ mkdir /opt/tensor-build/\n$ cd /opt/tensor-build/\n$ git clone https://github.com/tensorflow/tensorflow\n$ cd tensorflow\n$ ./tensorflow/contrib/makefile/download_dependencies.sh\n$ sudo apt-get install autoconf automake libtool curl make g++ unzip\n$ pushd .\n$ cd tensorflow/contrib/makefile/downloads/protobuf\n$ ./autogen.sh\n$ ./configure\n$ make\n$ make check\n$ sudo make install\n$ sudo ldconfig # refresh shared library cache\n$ popd\n```\n\nSo far, no issue, then:\n\n```\n$ make -f tensorflow/contrib/makefile/Makefile\nPROTOC = \"protoc\"\nCC_PREFIX = \"\"\nprotoc  tensorflow/core/util/test_log.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/util/saved_tensor_slice.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/util/memmapped_file_system.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/util/event.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/protobuf/tensorflow_server.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/protobuf/saver.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/protobuf/queue_runner.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/protobuf/named_tensor.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/protobuf/meta_graph.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/protobuf/config.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/lib/core/error_codes.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/versions.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/variable.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/types.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/tensor_slice.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/tensor_shape.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/tensor_description.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/tensor.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/summary.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/step_stats.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/op_def.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/log_memory.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/kernel_def.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/graph.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/function.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/device_attributes.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/cost_graph.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/attr_value.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/framework/allocation_description.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/example/feature.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/example/example.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/proto/\nprotoc  tensorflow/core/util/test_log.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/util/saved_tensor_slice.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/util/memmapped_file_system.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/util/event.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/protobuf/tensorflow_server.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/protobuf/saver.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/protobuf/queue_runner.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/protobuf/named_tensor.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/protobuf/meta_graph.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/protobuf/config.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/lib/core/error_codes.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/versions.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/variable.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/types.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/tensor_slice.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/tensor_shape.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/tensor_description.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/tensor.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/summary.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/step_stats.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/op_def.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/log_memory.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/kernel_def.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/graph.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/function.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/device_attributes.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/cost_graph.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/attr_value.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/framework/allocation_description.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/example/feature.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\nprotoc  tensorflow/core/example/example.proto --cpp_out /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/tools/proto_text/gen_p\nroto_text_functions_lib.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/tools/proto_text/gen_proto_text_functions_lib.\no\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/tools/proto_text/gen_p\nroto_text_functions.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/tools/proto_text/gen_proto_text_functions.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/tracing.\ncc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/tracing.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/tensor_c\noding.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/tensor_coding.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/protobuf\n_util.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/protobuf_util.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/posix/po\nsix_file_system.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/posix/posix_file_system.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/posix/po\nrt.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/posix/port.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/posix/en\nv.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/posix/env.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/load_lib\nrary.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/load_library.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/file_sys\ntem.cc -o /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/file_system.o\ngcc --std=c++11 -I. -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/downloads/ -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/download\ns/eigen-eigen-b4fa9622b809 -I/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/usr/local/include -c tensorflow/core/platform/env.cc -\no /opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o\ntensorflow/core/platform/env.cc:304:9: error: conflicting return type specified for 'virtual tensorflow::int64 tensorflow::{anonymous}::FileStream::ByteCount\n() const'\n   int64 ByteCount() const override { return pos_; }\n         ^\nIn file included from ./tensorflow/core/platform/default/protobuf.h:26:0,\n                 from ./tensorflow/core/platform/protobuf.h:31,\n                 from ./tensorflow/core/platform/file_system.h:28,\n                 from ./tensorflow/core/platform/env.h:27,\n                 from tensorflow/core/platform/env.cc:23:\n/usr/include/google/protobuf/io/zero_copy_stream.h:172:17: error:   overriding 'virtual google::protobuf::int64 google::protobuf::io::ZeroCopyInputStream::By\nteCount() const'\n   virtual int64 ByteCount() const = 0;\n                 ^\nmake: *** [/opt/tensor-build/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o] Error 1\n```\n", "comments": ["I think this recent change to `protobuf` might be causing the problem: https://github.com/google/protobuf/commit/a17367f44a3b592993d2ac5c074ed58fb0ce784c. (I noticed the same issue when attempting to build TensorFlow with the 3.0.0 release.) A stop-gap solution would be to change [`download_dependencies.sh`](https://github.com/tensorflow/tensorflow/blob/27eeb441bad8bcaa1bcba42a4b4ee49fb50ea0d3/tensorflow/contrib/makefile/download_dependencies.sh#L41) so that it uses a known-good version of `protobuf`. For example, something like the following should work:\n\n``` bash\ngit clone https://github.com/google/protobuf.git ${DOWNLOADS_DIR}/protobuf\npushd ${DOWNLOADS_DIR}/protobuf\ngit checkout d5fb408ddc281ffcadeb08699e65bb694656d0bd  # hash for 3.0.0-beta-2\npopd\n```\n", "Thanks, look like the hash d5fb408ddc281ffcadeb08699e65bb694656d0bd works well for protobuf and now I've been able to compile Tensorflow.\n"]}, {"number": 3625, "title": "iOS CameraExample KVO mismatching keys", "body": "I got a KVO error when switching views and noticed: \n\nIn CameraExampleViewController.mm, `setupAVCapture`uses\n\n```\n[stillImageOutput\n     addObserver:self\n     forKeyPath:@\"capturingStillImage\"\n     options:NSKeyValueObservingOptionNew\n     context:(void *)(AVCaptureStillImageIsCapturingStillImageContext)];\n```\n\nwhile `teardownAVCapture` uses\n\n```\n[stillImageOutput removeObserver:self forKeyPath:@\"isCapturingStillImage\"];\n```\n\n `forKeyPath:@\"capturingStillImage\"` and `forKeyPath:@\"isCapturingStillImage\"` do not match.\n", "comments": ["@petewarden can you take a look at this?\n", "@jesseseales, is this still an issue for you?\n", "Closing due to lack of response.\n"]}, {"number": 3624, "title": "Basic Element-wise Complex Number Calculations Not Available On GPU", "body": "Basic element-wise addition, subtraction, multiplication or division for any Tensor of type tf.complex64 is not implemented on GPU.\n### Environment info\n\nOperating System: Centos 7,  3.10.0-327.22.2.el7.x86_64\n\nInstalled version of CUDA and cuDNN:  CUDA 7.5 and cuDNN 7.0-v4\n-rw-r--r--. 1 root root 189170 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudadevrt.a\nlrwxrwxrwx. 1 root root     16 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root root     19 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root root 311596 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n-rw-r--r--. 1 root root 558020 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart_static.a\n\nTensorflow installed from source: \n1. Commit hash 00700f00fdf71baec1342d1afd7849e16fbd2a33\n2. Bazel information:\nBuild label: 0.3.0-2016-07-22 (@ca36b06)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 22 19:23:10 2016 (1469215390)\nBuild timestamp: 1469215390\nBuild timestamp as int: 1469215390\n### Steps to reproduce\n1. Add, subtract, multiply or divide any Tensor of type tf.complex64. A code example is shown here for element-wise addition:\n\n```\nimport tensorflow as tf\n\nif __name__ == '__main__':\n\n    with tf.device('/gpu:0'):\n        N = 100\n        a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        c = a + b\n\n        with tf.Session() as sess:\n            c = sess.run(c)\n```\n\nThe code returns the following output if run on GPU (works well on CPU):\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.90GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x5168890\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GT 610\nmajor: 2 minor: 1 memoryClockRate (GHz) 1.62\npciBusID 0000:01:00.0\nTotal memory: 1023.19MiB\nFree memory: 396.98MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:814] Ignoring gpu device (device: 1, name: GeForce GT 610, pci bus id: 0000:01:00.0) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.5.\nE tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: add = Add[T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex, Complex_1)]]\nTraceback (most recent call last):\n  File \"test_div_gpu_prob.py\", line 12, in <module>\n    c = sess.run(c)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: add = Add[T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex, Complex_1)]]\nCaused by op u'add', defined at:\n  File \"test_div_gpu_prob.py\", line 9, in <module>\n    c = a + b\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 755, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 70, in add\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n### What have you tried?\n1.  Implementation using builtin Tensorflow functions works, if the real and imaginary parts are separated. See the code below:\n\n```\nimport numpy as np\nimport tensorflow as tf\n\ndef complex_add(x, y):\n    xr, xi = tf.real(x), tf.imag(x)\n    yr, yi = tf.real(y), tf.imag(y)\n    return tf.complex(xr + yr, xi + yi)\n\ndef complex_sub(x, y):\n    xr, xi = tf.real(x), tf.imag(x)\n    yr, yi = tf.real(y), tf.imag(y)\n    return tf.complex(xr - yr, xi - yi)\n\ndef complex_mul(x, y):\n    xr, xi = tf.real(x), tf.imag(x)\n    yr, yi = tf.real(y), tf.imag(y)\n    return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)\n\ndef complex_div(x, y):\n    xr, xi = tf.real(x), tf.imag(x)\n    yr, yi = tf.real(y), tf.imag(y)\n    d = tf.square(yr) + tf.square(yi)\n    return tf.complex((xr*yr+xi*yi)/d, (xi*yr-xr*yi)/d)\n\nif __name__ == '__main__':\n\n    with tf.device('/gpu:0'):\n        N = 100\n        a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n\n        with tf.Session() as sess:\n\n            a_, b_, c = sess.run([a,b,complex_add(a,b)])\n            assert np.allclose(c, a_ + b_)\n\n            a_, b_, c = sess.run([a,b,complex_sub(a,b)])\n            assert np.allclose(c, a_ - b_)\n\n            a_, b_, c = sess.run([a,b,complex_mul(a,b)])\n            assert np.allclose(c, a_ * b_)\n\n            a_, b_, c = sess.run([a,b,complex_div(a,b)])\n            assert np.allclose(c, a_ / b_)\n```\n\nIt would be nice to have such functions transparent with the built-in CPU implementations.\n", "comments": ["Note: implementations using built-in Tensorflow functions as show above doesn't solve gradient issues caused by the handling of complex numbers:\n\n```\nimport tensorflow as tf\n\ndef complex_mul(x, y):\n    xr, xi = tf.real(x), tf.imag(x)\n    yr, yi = tf.real(y), tf.imag(y)\n    return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)\n\nif __name__ == '__main__':\n\n    with tf.device('/gpu:0'):\n        N = 100\n        a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        c = complex_mul(a, b)\n\n        grad = tf.gradients([c], [a])\n\n        with tf.Session() as sess:\n            grad = sess.run(grad)\n```\n\nThis code will fail with the following error:\n\nE tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: gradients/Shape = Shape[T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex_2)]]\nTraceback (most recent call last):\n  File \"test_div_gpu_grad_prob.py\", line 19, in <module>\n    grad = sess.run(grad)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n     [[Node: gradients/Shape = Shape[T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex_2)]]\nCaused by op u'gradients/Shape', defined at:\n  File \"test_div_gpu_grad_prob.py\", line 16, in <module>\n    grad = tf.gradients([c], [a])\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 367, in gradients\n    grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 230, in _DefaultGradYs\n    array_ops.shape(y),\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 131, in shape\n    return gen_array_ops.shape(input, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1922, in shape\n    result = _op_def_lib.apply_op(\"Shape\", input=input, name=name)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n", "It seems that support for complex64 types is piecemeal, by op-type and device-type.  Bringing in @martinwicke for a comment on the policy.\n", "Yes, this is a good feature request bordering on a bug. Please check the Op registrations of the affected ops, and you'll probably find that the templates of many of them are not specialized for complex data types. It is a relatively simple thing to fix, and I'd love PRs that do it.\n", "@sbrodeur: Are you currently working on this?\nIf not, I could go ahead and attempt a fix.\n", "@ibab: I did not yet attempt a fix. I've looked a at little at Eigen:\n\"_By default, Eigen currently supports standard floating-point types (float, double, std::complex<float>, std::complex<double>, long double), as well as all native integer types (e.g., int, unsigned int, short, etc.), and bool._\"\n[https://eigen.tuxfamily.org/dox-devel/TopicCustomizingEigen.html]\n\nThus, for the simple calculations here, should I expect Eigen to provide compatible functors, e.g. :\n\n```\ntemplate <typename T>\nstruct add : base<T, Eigen::internal::scalar_sum_op<T> > {\n  static const bool use_bcast_optimization = true;\n};\n```\n\nThis code is in file _cwise_ops.h_\n\nDoes this means the fix is similar to #2263, i.e. just adding the complex64 type when we register the kernels?\n", "Yes, you won't have to implement the operations themselves, you just need to enable them.\nFor example, you can look at the supported types for the addition op here:\nhttps://github.com/tensorflow/tensorflow/blob/32bd3d024f33e920a67a1081bc0ae0048350fdee/tensorflow/core/kernels/cwise_op_add.cc#L22\nYou would need to add `complex64` and `complex128` to the macro (and change it into `REGISTER6`).\n\nYou should make sure that the GPU tests are enabled for `complex64` and `complex128` for each op that has been extended, for example here: https://github.com/tensorflow/tensorflow/blob/32bd3d024f33e920a67a1081bc0ae0048350fdee/tensorflow/python/kernel_tests/cwise_ops_test.py#L362.\n", "Thanks for the information @ibab! I will attempt a fix myself and send a PR soon!\n", "So far, I can make it work with some operations (add, sub) by simply adding the complex data types when registering the kernels: e.g. https://github.com/tensorflow/tensorflow/blob/32bd3d024f33e920a67a1081bc0ae0048350fdee/tensorflow/core/kernels/cwise_op_add.cc#L22\n\n```\nDEFINE_BINARY6(add, Eigen::half, float, double, int64, complex64, complex128);\n```\n\nCompilation errors however occur for multiplication (and division), as seen below.\nSearching the web, I found here that CUDA may not support std::complex because of STL incompatibilities:\nhttps://forum.kde.org/viewtopic.php?f=74&t=123919\n\nIt seems to solve this problem, people have been using reimplementations of the std:complex type (e.g. from thrust, cuda_complex or cusp) so that it can be used in device code:\nhttps://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h\nhttps://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp\nhttps://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h\n\nWould the Eigen library implementing something similar to what thrust uses solve the issue in Tensorflow?\n\n### Compilation output\n\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nIn file included from /usr/local/cuda-7.5/include/host_config.h:161:0,\n                 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\nIn file included from /usr/local/cuda-7.5/include/host_config.h:161:0,\n                 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\nIn file included from /usr/local/cuda-7.5/include/host_config.h:161:0,\n                 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,\n                 from <command-line>:0:\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\n    ^\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n\n12 errors detected in the compilation of \"/tmp/tmpxft_0000430f_00000000-12_cwise_op_gpu_mul.cu.compute_35.cpp2.i\".\n", "Strange, your errors seem to be caused by the fact that Eigen is trying to assign values from an int `Tensor` into a complex `Tensor`.\nI don't think that's supposed to happen usually.\n\nI've tried enabling complex `mul` and `div` just now, and it successfully compiled and ran when I restricted the ops to run on the GPU.\nI'm also using CUDA 7.5, not sure what else could be different between our setups.\n", "Here is my configuration:\n\nGPU: Tesla K40c\nOperating System: CentOS Linux 7 (Core)\nKernel: Linux 3.10.0-327.22.2.el7.x86_64\nArchitecture: x86-64\nC Compiler: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\nCUDA Compiler: Cuda compilation tools, release 7.5, V7.5.17\n\nI will try with a more recent gcc (e.g. 4.9.2) to see if the compilation problem disappears.\n", "I'm also using `gcc 4.8.5` and`Cuda compilation tools, release 7.5, V7.5.17`.\nNot sure what could be causing this if these two are the same.\nI've uploaded my changes to https://github.com/ibab/tensorflow/commit/8c3baae08bd449d25f80e9af8ae4830eb7ae2670, so you might want to compare these with yours.\nIf that doesn't help we should try rebasing to the same TensorFlow commit. \n", "Sadly, I obtain the same errors if I clone and compile the fork ibab@8c3baae without any modifications.\n@ibab - What is your Linux distribution?\n", "I'm running Scientific Linux 6, which should be virtually identical to Red Hat 6.\nI get my compiler toolchain from anaconda, though.\nI'll try to make sure it's not something weird on my end.\n", "On my side, I'll try a build on my laptop which runs the latest Debian 8 (Jessie). I don't have a Nvidia GPU but I should nevertheless be able to compile with CUDA.\n", "Okay, I've rebuilt tensorflow after a `bazel clean --expunge` to make extra sure that I'm using the right Eigen version, as I've changed it around a few times previously, but it still built successfully.\n\n**Edit:** Btw, do you also get compiler warnings about calling `__host__` functions from device code as in the link you posted above?\n", "I do not get compiler warnings about calling **host** functions from device code.\n\nI just tried to build on my laptop (Debian 8, up-to-date) with configuration;\ngcc (Debian 4.9.2-10) 4.9.2\nCuda compilation tools, release 7.5, V7.5.17\n\nI obtained the same errors, so it does not seem related to gcc or distribution. I also tried to build with the latest eigen (3782cd1de9c4) on the Centos 7 machine, and that did not help either. I will try building with CUDA 8, after which I will be clueless about those compilation issues.\n\n**Edit**:  same errors with CUDA 8.\n", "I've tried compiling with different compute capabilities, but it still compiled without errors.\nThe fact that you reproduced it on two different systems makes me think that it's a problem with my setup, though.\nUnfortunately `nvcc` doesn't give us a lot of information in the error message (like which template instantiations we are dealing with) :(\n", "@ibab , @sbrodeur , thank you so much for working on this. I think it would really speed up one of my projects. Is there any new progress? Are you planning to include this on the next release of TensorFlow? What about basic math functions such as `tf.exp()`, `tf.complex_abs()`?\n\nThanks again!\n", "@iportillo - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.\n\ntf.complex_abs is easy to implement on GPU right now:\n\n```\ndef complex_abs(x):\n    return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))\n```\n\nBy tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):\n\n```\ndef atan2(y, x):\n    angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)\n    return angle\n\ndef complex_arg(x):\n    return atan2(tf.imag(x), tf.real(x))\n```\n\nIt's not optimized but works well on GPU.\n", "@benoitsteiner: We're having some problems with implementing the product and div ops for `std::complex` using the Eigen Tensor library.\nDo you see why we would get an error like the following when enabling them in TensorFlow?\n\n```\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n```\n\n```\n$ c++filt _ZNSt7complexIfE9_ComplexTE\nstd::complex<float>::_ComplexT\n```\n\nMaybe we would need to switch to something like `thrust::complex` as @sbrodeur suggested?\n", "I made some progress! I can make multiplication and division ops work for complex numbers if I specialized the templates in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432\n\n```\ntemplate <typename T>\nstruct mul : base<T, Eigen::internal::scalar_product_op<T> > {};\n\ntemplate <typename T>\nstruct multiply_complex {\n  typedef std::complex<T> result_type;\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex<T> a,\n                                                               std::complex<T> b) const {\n    return std::complex<T>(a.real()*b.real() - a.imag()*b.imag(),\n                           a.real()*b.imag() + a.imag()*b.real());\n  }\n};\n\ntemplate <>\nstruct mul<std::complex<float> > : base<std::complex<float>, multiply_complex<float> > {};\n\ntemplate <>\nstruct mul<std::complex<double> > : base<std::complex<double>, multiply_complex<double> > {};\n\n```\n\nIt seems more like a hack, but it doesn't involve changes in Eigen for now.\n\nNot sure what is wrong with nvcc using scalar_product_op in Eigen for complex numbers:\nhttps://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76\n\nHowever, it seems tightly related to using built-in \\* and / operators for std:complex types.\nFor instance, this fails with the same errors as in the previous posts:\n\n```\ntemplate <typename T>\nstruct mul : base<T, Eigen::internal::scalar_product_op<T> > {};\n\ntemplate <typename T>\nstruct multiply_complex {\n  typedef std::complex<T> result_type;\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex<T> a,\n                                                               std::complex<T> b) const {\n    return a*b;\n  }\n};\n\ntemplate <>\nstruct mul<std::complex<float> > : base<std::complex<float>, multiply_complex<float> > {};\n\ntemplate <>\nstruct mul<std::complex<double> > : base<std::complex<double>, multiply_complex<double> > {};\n\n```\n", "I can confirm that with the above trick, I can make work a lot of very useful functions for complex numbers on GPU (e.g. square, neg, div, mul, abs) This brings support for complex gradient computation on GPU:\n\n```\nimport tensorflow as tf\n\nif __name__ == '__main__':\n\n    with tf.device('/gpu:0'):\n        N = 100\n        a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n        c = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n\n        d = c * tf.neg(tf.square(a + b))\n\n        grad = tf.gradients([d], [a])\n\n        with tf.Session() as sess:\n            grad = sess.run(grad)\n```\n\nShould I make a PR or should we investigate further the handling of std::complex by nvcc?\n", "I'm not a maintainer, but I think a PR would definitely be a good idea \ud83d\udc4d \nMaybe you can split it into two PRs, one that requires the extra `scalar_prod_op` specialization, and one that doesn't?\nIn the long term, it would probably be best to get the specialization into Eigen itself, or to find another fix (like switching to `thrust::complex`).\n", "I've been privately writing GPU-based complex-valued ops for TF and decided to make my repository public. I think that more general support for computation of complex numbers on the GPU will be valuable to the community. However since my repository is in the early stages and isn't well tested, I think I'd like to develop it as a separate project and then port it as a TF pull request when it's more mature.   Feel free to make contributions and/or suggestions.\n\nhttps://github.com/woodshop/complex_tf\n", "In C++14,  std::complex methods are marked as constexpr. This will ensure that they can be used inside cuda kernels even though they're not marked as `__device__` functions provided that we compile with the --relaxed-constexpr flag (which TensorFlow has been doing for some time now).\n\nUnfortunately nvcc doesn't yet support c++14, but we can ask nvidia to start adding partial support for it starting with complex numbers.\n", "@iportillo ComplexAbs (and a few others) added here: https://github.com/tensorflow/tensorflow/commit/f21642013092b53186491064335053a9e02ce010\nand the corresponding Eigen change:\nhttps://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed\n", "After adding a workaround to Eigen:\nhttps://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344\n\nWe were able to enable addition, subtraction, division, and multiplication kernels for complex types on GPU: https://github.com/tensorflow/tensorflow/commit/93f15d4cde2f08057819f1194e5a4771f0d391ff \n", "@sbrodeur Does TensorFlow now support all the operations you need on complex, or are there additional improvements we need to make ?\n", "@benoitsteiner Tensorflow now supports everything I need for handling complex numbers.\n", "Thanks, closing the issue.\n", "Is it possible to calculate a complex number divide a float number without type cast?"]}, {"number": 3623, "title": "Adding a new gate to LSTM", "body": "I am trying to reimplement this paper [Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems](http://mi.eng.cam.ac.uk/~thw28/papers/EMNLP15.pdf), in which they add a gate to the LSTM cell and change how the state is computed.\n\nHow can I do this in tensorflow? Do I need to add a new OP ?\n", "comments": ["This sort of question is better asked at stackoverflow, tagged 'tensorflow'.\n"]}, {"number": 3622, "title": "Missing doc: tf.contrib.layers.embedding_column", "body": "Tutorial https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html refers to tf.contrib.layers.embedding_column.\n\n No documentation for this is available at API docs https://www.tensorflow.org/versions/master/api_docs/python/contrib.layers.html\n\nThe same applies for tf.contrib.learn.DNNLinearCombinedClassifier used later in that tutorial.\n", "comments": []}, {"number": 3621, "title": "layer normalization", "body": "recently Hinton published layer normalization on [arxiv layer normalization](https://arxiv.org/pdf/1607.06450v1.pdf?), any plan to add an Op?\n", "comments": ["I see c++ implement of batch_normalization is deprecated.\n", "I've got a patch that adds a layer_norm function to to the contrib.layers module. I'm just finishing up some testing before submitting a pull request.\n", "@xodus7 great\n", "@suiyuan2009 Have you ever tried this layer_norm op with most recent TF (1.3 or 1.4)? \r\n#3671 #14562 "]}, {"number": 3620, "title": "Building Android version on Ubuntu 16.04 failed.", "body": "Error message as below:\n\n``` gcc\ntensorflow/core/platform/env.cc:304:9:\nerror: conflicting return type specified for \u2018virtual tensorflow::int64 tensorflow::{anonymous}::FileStream::ByteCount() const\u2019\n   int64 ByteCount() const override { return pos_; }\n         ^\nIn file included from ./tensorflow/core/platform/default/protobuf.h:26:0,\n                 from ./tensorflow/core/platform/protobuf.h:31,\n                 from ./tensorflow/core/platform/file_system.h:28,\n                 from ./tensorflow/core/platform/env.h:27,\n                 from tensorflow/core/platform/env.cc:23:\n/usr/local/include/google/protobuf/io/zero_copy_stream.h:172:17: error:   overriding \u2018virtual google::protobuf::int64 google::protobuf::io::ZeroCopyInputStream::ByteCount() const\u2019\n   virtual int64 ByteCount() const = 0;\n```\n\nI was trying to build an Android version Tensorflow on Ubuntu 16.04 according to   [Tensorflow's docs](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) .\n\nThis error happened, after I executed this:\n`make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID` \n", "comments": ["env.cc: 304 is like this:\n\n``` c\n  int64 ByteCount() const override { return pos_; }\n```\n\nSo I added the protobuf namespace:\n\n``` c\ngoogle::protobuf::int64 ByteCount() const override { return pos_; }\n```\n\nNow, it works fine.\n", "PR #3614 fixed this.\n\nClose it now.\n"]}, {"number": 3619, "title": "Running Quantized model in iOS failed: Couldn't load model: Not found: Op type not registered 'Dequantize'", "body": "### Environment info\n\nOperating System:\nBuilding on Mac OS X\nRunning on iOS 9.2.1\n1. The commit hash (`git rev-parse HEAD`)\n   27eeb441bad8bcaa1bcba42a4b4ee49fb50ea0d3 \n2. The output of `bazel version`\n\n```\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Jul 12 11:11:47 2016 (1468321907)\nBuild timestamp: 1468321907\nBuild timestamp as int: 1468321907\n```\n### Steps to reproduce\n1. I compile tf for iOS with the makefile\n2. I follow https://www.tensorflow.org/versions/master/how_tos/quantization/index.html to quantize a model which run perfectly in iOS\n3. Replace the frozen model file with the quantized one, iOS throw the Error\n\n```\nF /Users/yjmade/Documents/code/labs/tensorflow/tf_master1/tensorflow/contrib/ios_examples/camera_object_detect/CameraExampleViewController.mm:436] Couldn't load model: Not found: Op type not registered 'Dequantize'\n```\n\nI think it maybe is the same problem with #3543 \n", "comments": ["Sorry to drop another one on you @petewarden but maybe this is your area?\n", "I'm still working on a proper fix for this one, but a hacky workaround for now is to add tensorflow/contrib/quantization/{ops,kernels}/*.cc to tf_op_files.txt and recompile. I hope to have more documentation and a script fix for this soon.\n", "@petewarden I added all the *.cc files in tensorflow/contrib/quantization/ops and tensorflow/contrib/quantization/kernels to tf_op_files.txt and then run `tensorflow/contrib/makefile/build_all_ios.sh` but got the following error:\n\n```\nIn file included from tensorflow/contrib/quantization/kernels/dequantize_op.cc:20:\n./tensorflow/contrib/quantization/kernels/quantization_utils.h:28:10: fatal error: 'public/gemmlowp.h' file not found\n#include \"public/gemmlowp.h\"\n         ^\n```\n\nI then changed the Makefile by adding the path to gemmlowp to HOST_INCLUDES:\n\n```\nHOST_INCLUDES := \\\n-I. \\\n-I$(MAKEFILE_DIR)/downloads/ \\\n-I$(MAKEFILE_DIR)/downloads/eigen-eigen-$(EIGEN_VERSION) \\\n-I$(MAKEFILE_DIR)/downloads/gemmlowp \\\n-I$(HOST_GENDIR)\n```\n\nbut I got the same compile error. I do have all the files of gemmlowp downloaded in the tensorflow/contrib/makefile/downloads/gemmlowp/ directory. \n", "Update: `-I$(MAKEFILE_DIR)/downloads/gemmlowp \\` should be added to INCLUDES (around line 130) instead of HOST_INCLUDES. Also the path to gtest also needs to be added to INCLUDES:\n\n```\n-I$(MAKEFILE_DIR)/downloads/gemmlowp \\\n-I$(MAKEFILE_DIR)/downloads/protobuf/gmock/gtest/include \\\n```\n\nThat fixed my compiler error. Now I'm having some linker error.. \n", "More Update: I found a \"hacky workaround\" to bypass the linker error by removing all the tensorflow/contrib/quantization/kernels/*_test.cc files from tf_op_files.txt and now build_all_ios.sh finally completes successfully with the added quantization files in tf_op_files.txt:\n\n```\ntensorflow/contrib/quantization/ops/array_ops.cc\ntensorflow/contrib/quantization/ops/math_ops.cc\ntensorflow/contrib/quantization/ops/nn_ops.cc\ntensorflow/contrib/quantization/kernels/dequantize_op.cc\ntensorflow/contrib/quantization/kernels/quantization_utils.cc\ntensorflow/contrib/quantization/kernels/quantize_down_and_shrink_range.cc\ntensorflow/contrib/quantization/kernels/quantize_op.cc\ntensorflow/contrib/quantization/kernels/quantized_activation_ops.cc\ntensorflow/contrib/quantization/kernels/quantized_batch_norm_op.cc\ntensorflow/contrib/quantization/kernels/quantized_bias_add_op.cc\ntensorflow/contrib/quantization/kernels/quantized_concat_op.cc\ntensorflow/contrib/quantization/kernels/quantized_conv_ops.cc\ntensorflow/contrib/quantization/kernels/quantized_matmul_op.cc\ntensorflow/contrib/quantization/kernels/quantized_pooling_ops.cc\n```\n\nBut after I linked the newly built libtensorflow-core.a file in the iOS simple project and started the app, tap on Run Model generates the following error:\n\n```\ntensorflow/contrib/ios_examples/simple/RunModelViewController.mm:268] Running model failed: Not found: FetchOutputs node FetchOutputs: not found\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedMaxPool\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedAvgPool\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedBatchNormWithGlobalNormalization\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedRelu6\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QINT32 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedRelu6\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizeDownAndShrinkRange\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QINT32 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Dequantize\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Dequantize\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Dequantize\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Dequantize\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Dequantize\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AddN\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AddN\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedConv2D\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"Tfilter\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ArgMin\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"dimension\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ArgMin\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"dimension\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ArgMax\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"dimension\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ArgMax\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"dimension\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AvgPoolGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"orig_input_shape\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AvgPool\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AvgPool\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_HALF } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedConcat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT8 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedConcat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT32 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BatchNormWithGlobalNormalizationGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BroadcastGradientArgs\" device_type: \"GPU\" host_memory_arg: \"s0\" host_memory_arg: \"s1\" host_memory_arg: \"r0\" host_memory_arg: \"r1\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BroadcastGradientArgs\" device_type: \"CPU\" host_memory_arg: \"s0\" host_memory_arg: \"s1\" host_memory_arg: \"r0\" host_memory_arg: \"r1\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BiasAddGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BiasAddGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BiasAddV1\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BiasAddV1\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_HostCast\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedRelu\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QINT32 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedRelu\" device_type: \"CPU\" constraint { name: \"Tinput\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"CheckNumerics\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ConcatOffset\" device_type: \"GPU\" host_memory_arg: \"concat_dim\" host_memory_arg: \"shape\" host_memory_arg: \"offset\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ConcatOffset\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT8 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT8 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT16 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT16 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT32 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Concat\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } } host_memory_arg: \"concat_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Placeholder\" device_type: \"GPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Placeholder\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ZerosLike\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ZerosLike\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Fill\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"dims\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Fill\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"dims\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Abort\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ControlTrigger\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"LoopCond\" device_type: \"GPU\" host_memory_arg: \"input\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefNextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefNextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefNextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefNextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NextIteration\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefNextIteration\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NextIteration\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefExit\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefExit\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Exit\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Exit\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Exit\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Exit\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefEnter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefEnter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefEnter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefEnter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefEnter\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Enter\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Merge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Merge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Merge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"inputs\" host_memory_arg: \"output\" host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Merge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"inputs\" host_memory_arg: \"output\" host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Merge\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSelect\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSelect\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSwitch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"pred\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSwitch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"pred\" host_memory_arg: \"output_false\" host_memory_arg: \"output_true\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSwitch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"data\" host_memory_arg: \"pred\" host_memory_arg: \"output_false\" host_memory_arg: \"output_true\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSwitch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"pred\" host_memory_arg: \"output_false\" host_memory_arg: \"output_true\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Switch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"pred\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Switch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"pred\" host_memory_arg: \"output_false\" host_memory_arg: \"output_true\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Switch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"data\" host_memory_arg: \"pred\" host_memory_arg: \"output_false\" host_memory_arg: \"output_true\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Switch\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"pred\" host_memory_arg: \"output_false\" host_memory_arg: \"output_true\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2DBackpropFilter\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } label: \"custom\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2DBackpropFilter\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"CTCBeamSearchDecoder\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"CTCGreedyDecoder\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Add\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Div\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Div\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Exp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Greater\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Inv\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"IsFinite\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Cast\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Less\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2DBackpropInput\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } label: \"eigen_tensor\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Log\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Maximum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Minimum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Neg\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Rsqrt\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SigmoidGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Sigmoid\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Sqrt\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Square\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SquaredDifference\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefMerge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefMerge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefMerge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"inputs\" host_memory_arg: \"output\" host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefMerge\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"inputs\" host_memory_arg: \"output\" host_memory_arg: \"value_index\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TanhGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AssignSub\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AssignSub\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BiasAdd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BiasAdd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AssignAdd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"AssignAdd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Assign\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Assign\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DynamicPartition\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DynamicPartition\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DynamicStitch\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"indices\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DynamicStitch\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"indices\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ParseExample\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Gather\" device_type: \"CPU\" constraint { name: \"Tparams\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Gather\" device_type: \"CPU\" constraint { name: \"Tparams\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Gather\" device_type: \"CPU\" constraint { name: \"Tparams\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Gather\" device_type: \"CPU\" constraint { name: \"Tparams\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefIdentity\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefIdentity\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_HostRecv\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"InTopK\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"InTopK\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"LRN\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } label: \"eigen\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } label: \"eigen\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyAdadelta\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyAdadelta\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPush\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPush\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"handle\" host_memory_arg: \"elem\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPush\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"handle\" host_memory_arg: \"elem\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MirrorPadGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"paddings\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MirrorPadGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"paddings\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MatMul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MaxPool\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MaxPool\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_HALF } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MirrorPad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"paddings\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MirrorPad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"paddings\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Const\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NoOp\" device_type: \"GPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SoftmaxCrossEntropyWithLogits\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StridedSlice\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"begin\" host_memory_arg: \"end\" host_memory_arg: \"strides\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StridedSlice\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"begin\" host_memory_arg: \"end\" host_memory_arg: \"strides\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StridedSlice\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } } host_memory_arg: \"begin\" host_memory_arg: \"end\" host_memory_arg: \"strides\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"NoOp\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Pack\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Pack\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SoftsignGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SoftsignGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ParseSingleSequenceExample\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Relu6Grad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Relu6Grad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Elu\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Relu6\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Relu6\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Reshape\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"shape\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeBilinearGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"PlaceholderWithDefault\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Select\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Select\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_HostSend\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeBilinear\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeBilinear\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeNearestNeighborGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeNearestNeighborGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeNearestNeighbor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ResizeNearestNeighbor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SquaredDifference\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"x\" host_memory_arg: \"y\" host_memory_arg: \"z\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ReverseSequence\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ReverseSequence\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Relu\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Relu\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Stack\" device_type: \"GPU\" host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ShardedFilespec\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyProximalAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyProximalAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ShardedFilename\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SaveSlices\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedMatMul\" device_type: \"CPU\" constraint { name: \"T1\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"T2\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"Toutput\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Save\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"LoopCond\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Restore\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"LinSpace\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"start\" host_memory_arg: \"stop\" host_memory_arg: \"num\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_HostRecv\" device_type: \"GPU\" host_memory_arg: \"tensor\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StopGradient\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2DBackpropInput\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } label: \"custom\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_Recv\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Identity\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Identity\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_Send\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"LinSpace\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"start\" host_memory_arg: \"stop\" host_memory_arg: \"num\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Range\" device_type: \"CPU\" host_memory_arg: \"start\" host_memory_arg: \"limit\" host_memory_arg: \"delta\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"InvertPermutation\" device_type: \"GPU\" host_memory_arg: \"x\" host_memory_arg: \"y\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DeleteSessionTensor\" device_type: \"GPU\" host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DeleteSessionTensor\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Sub\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Sub\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionTensor\" device_type: \"GPU\" constraint { name: \"dtype\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionTensor\" device_type: \"GPU\" constraint { name: \"dtype\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionTensor\" device_type: \"GPU\" constraint { name: \"dtype\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Min\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Min\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizeV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizeV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT8 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizeV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QUINT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizeV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_QINT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Identity\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StopGradient\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StopGradient\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionTensor\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Tanh\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"EluGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Prod\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Prod\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefMerge\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Pad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"paddings\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Pad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"paddings\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Enter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Enter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Enter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Enter\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } host_memory_arg: \"data\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefIdentity\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_Recv\" device_type: \"GPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionHandle\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionHandle\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionHandle\" device_type: \"GPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Squeeze\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TemporaryVariable\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ExpandDims\" device_type: \"CPU\" host_memory_arg: \"dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Shape\" device_type: \"CPU\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Exit\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Slice\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"begin\" host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Slice\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"begin\" host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Slice\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } } host_memory_arg: \"begin\" host_memory_arg: \"size\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Mul\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Softmax\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Equal\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Softplus\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Softplus\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedBiasAdd\" device_type: \"CPU\" constraint { name: \"T1\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"T2\" allowed_values { list { type: DT_QUINT8 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"QuantizedBiasAdd\" device_type: \"CPU\" constraint { name: \"T1\" allowed_values { list { type: DT_QINT8 } } } constraint { name: \"T2\" allowed_values { list { type: DT_QINT8 } } } constraint { name: \"out_type\" allowed_values { list { type: DT_QINT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Softsign\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Softsign\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BOOL } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseToDense\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_STRING } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackClose\" device_type: \"GPU\" host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_HALF } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_HALF } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackClose\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_Send\" device_type: \"GPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPush\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Rank\" device_type: \"CPU\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Reshape\" device_type: \"CPU\" host_memory_arg: \"shape\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPop\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Stack\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Reverse\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"dims\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Reverse\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"dims\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefExit\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StridedSliceGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"shape\" host_memory_arg: \"begin\" host_memory_arg: \"end\" host_memory_arg: \"strides\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StridedSliceGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"shape\" host_memory_arg: \"begin\" host_memory_arg: \"end\" host_memory_arg: \"strides\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StridedSliceGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } } host_memory_arg: \"shape\" host_memory_arg: \"begin\" host_memory_arg: \"end\" host_memory_arg: \"strides\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Mean\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Mean\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TileGrad\" device_type: \"CPU\" host_memory_arg: \"multiples\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TopKV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TopKV2\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Max\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Max\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPop\" device_type: \"GPU\" constraint { name: \"elem_type\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"handle\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPop\" device_type: \"GPU\" constraint { name: \"elem_type\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"handle\" host_memory_arg: \"elem\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"StackPop\" device_type: \"GPU\" constraint { name: \"elem_type\" allowed_values { list { type: DT_BOOL } } } host_memory_arg: \"handle\" host_memory_arg: \"elem\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TopK\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"TopK\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_HostSend\" device_type: \"GPU\" host_memory_arg: \"tensor\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ShapeN\" device_type: \"CPU\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyRMSProp\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SoftplusGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SoftplusGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyAdam\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2D\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyMomentum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyMomentum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyMomentum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Size\" device_type: \"CPU\" host_memory_arg: \"output\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"InvertPermutation\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyFtrl\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2DBackpropInput\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyProximalGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyProximalGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyAdagrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Switch\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"pred\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Switch\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"pred\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MaxPoolGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"MaxPoolGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_HALF } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSwitch\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"pred\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RefSwitch\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"pred\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"BatchNormWithGlobalNormalization\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ReluGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ReluGrad\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ImmutableConst\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyAdadelta\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ControlTrigger\" device_type: \"GPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"ApplyGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Variable\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Split\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"split_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Split\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"split_dim\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"LogSoftmax\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"GetSessionHandle\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"IsVariableInitialized\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Tile\" device_type: \"CPU\" host_memory_arg: \"multiples\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Sum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Sum\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Where\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyFtrl\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyFtrl\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Transpose\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } } host_memory_arg: \"perm\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Transpose\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } host_memory_arg: \"perm\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Transpose\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_BFLOAT16 } } } host_memory_arg: \"perm\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"DestroyTemporaryVariable\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"RestoreSlice\" device_type: \"CPU\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"SparseApplyProximalGradientDescent\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_DOUBLE } } } constraint { name: \"Tindices\" allowed_values { list { type: DT_INT64 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Conv2DBackpropFilter\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } } label: \"eigen_tensor\"')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Unpack\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"Unpack\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_FLOAT } } }')\nI tensorflow/core/framework/op_kernel.cc:802] OpKernel ('op: \"_HostCast\" device_type: \"GPU\" host_memory_arg: \"x\" host_memory_arg: \"y\"')\n```\n\n@petewarden any ideas how to fix that? Thanks!\n", "for FetchOutputs issue \nI think you must quantize you pb file with --output_node_names=\"output\"\nlike this:\nbazel-bin/tensorflow/contrib/quantization/tools/quantize_graph --input=tensorflow_inception_graph.pb --output_node_names=\"output\" --print_nodes=true --output=tensorflow/contrib/ios_examples/camera/data/quantized_tensorflow_inception_graph.pb --mode=eightbit --logtostderr\n\nHi @petewarden \nI run the quantized-pb on camera example of ios \nIt's well, but I get info:\n\nZero is not representable in the quantized range used by the input. This means QuantizedConv2d has to fall back to a slow implementation, since the border of zero values can't be represented easily. You should try to construct graphs that avoid this situation.\n\nHow to avoid this info?\n", "@lcwyylcwyy Thanks for the info. Very helpful! The only thing I had to change is \"mode=weights\" instead of \"mode=eightbit\", then I can run the simple app with the quantized Inception v1 model on my iPhone 6 device without crash. And I don't see any performance issue. Maybe you can try the simple project to compare? I'll try the camera one later too.\n", "@jeffxtang Ok, I will try. \nI knows why! I measured the inference procedure, because the quantized network have more layer,so every inference slower than origin network. \n", "Closing due to inactivity. I gather from your last comment that you figured it out.", "@petewarden @jeffxtang I'm facing the issue \r\n```\r\nDidn't find custom op for name 'Dequantize'\r\nDidn't find custom op for name 'ReorderAxes'\r\nRegistration failed.\r\n```\r\nNow that the directories have changed in tensorflow, can someone help me where I can find `tensorflow/contrib/quantization/{ops,kernels}/*.cc` as suggested by @petewarden "]}]