[{"number": 21298, "title": "Error messages when using Dataset.from_generator in debug mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.3\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA (CPU)\r\n- **GPU model and memory**: NA (CPU)\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n  for val in range(10):\r\n    yield (str(val),)\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, (tf.string,))\r\n\r\niter = dataset.make_one_shot_iterator()\r\nnext = iter.get_next()\r\n\r\nsess = tf.Session()\r\n\r\nfor ii in range(10):\r\n  print(sess.run(next))\r\n```\r\n\r\n### Describe the problem\r\nWhen using `Dataset.from_generator()` in debug mode (PyCharm) error messages () are printed to the console. The code seems to run as expected. Might be some finalization issues. When not in debug mode everything seems quiet.\r\n\r\n### Source code / logs\r\n\r\nError message:\r\n```\r\nException ignored in: <generator object _yield_value at 0x181a3d7830>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/aaa/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\", line 100, in _yield_value\r\n    yield value\r\nSystemError: error return without exception set\r\nException ignored in: <generator object _yield_value at 0x181a3d7830>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/aaa/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\", line 100, in _yield_value\r\n    yield value\r\nSystemError: error return without exception set\r\nException ignored in: <generator object _yield_value at 0x181a3d7830>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/aaa/lib/python3.6/site-packages/tensorflow/python/data/util/nest.py\", line 100, in _yield_value\r\n    yield value\r\n.\r\n.\r\n.\r\n```\r\n", "comments": ["I have similar problem with a bit different output error message. It only happens during the debug mode of pycharm. It does not happen during run. Though it is really annoying. Please indicate if there is any fix/bypass.\r\n\r\n", "This appears to affect other projects than TensorFlow when run under the debugger in PyCharm debugger, according to an issue on their tracker: https://youtrack.jetbrains.com/issue/PY-30036\r\n\r\nThe issue seems to involve the debugging of Python generators and a potential mishandling of the exception state.\r\n\r\nPlease follow up on that thread with your reproduction and that should help to narrow down the problem. The thread also suggests a potential workaround of setting the environment variable `PYDEVD_USE_FRAME_EVAL=NO`.", "Setting `PYDEVD_USE_FRAME_EVAL=NO` does the trick. I'm following up on the PyCharm thread, will update.\r\n\r\n", "Hi Derek;\n\n \n\nsetting the PYDEVD_USE_FRAME_EVAL=NO stops the error output. Tks for input.\n\n \n\nRegards,\n\n \n\nJian\n\n \n\nFrom: Derek Murray <notifications@github.com> \nSent: Wednesday, August 1, 2018 8:46 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Jian Xu <jianxucsm@gmail.com>; Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Error messages when using Dataset.from_generator in debug mode (#21298)\n\n \n\nThis appears to affect other projects than TensorFlow when run under the debugger in PyCharm debugger, according to an issue on their tracker: https://youtrack.jetbrains.com/issue/PY-30036\n\nThe issue seems to involve the debugging of Python generators and a potential mishandling of the exception state.\n\nPlease follow up on that thread with your reproduction and that should help to narrow down the problem. The thread also suggests a potential workaround of setting the environment variable PYDEVD_USE_FRAME_EVAL=NO.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/21298#issuecomment-409796317> , or mute the thread <https://github.com/notifications/unsubscribe-auth/AVCfkvX0BQRnzDZ8SLETv9UiI1YTkBHTks5uMnYTgaJpZM4Vp8RN> .  <https://github.com/notifications/beacon/AVCfkiZjuQWwbrzNc3s8r-aBfgZvdroiks5uMnYTgaJpZM4Vp8RN.gif> \n\n", "Hi Albert;\n\n \n\nSetting the environment variable works on ubuntu but not on windows. Does anyone have experience it with windows as well?\n\n \n\nRegards,\n\n \n\nJian\n\n \n\nFrom: Albert <notifications@github.com> \nSent: Wednesday, August 1, 2018 11:24 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Jian Xu <jianxucsm@gmail.com>; Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Error messages when using Dataset.from_generator in debug mode (#21298)\n\n \n\nSetting PYDEVD_USE_FRAME_EVAL=NO does the trick. I'm following up on the PyCharm thread, will update.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/21298#issuecomment-409819149> , or mute the thread <https://github.com/notifications/unsubscribe-auth/AVCfkluQFfNquVKIhnkY7GqPlSzhGVcpks5uMpr8gaJpZM4Vp8RN> .  <https://github.com/notifications/beacon/AVCfkosi4oS1Utj1Bm1hiFlExHCjetHJks5uMpr8gaJpZM4Vp8RN.gif> \n\n", "Didn't try on Windows (and no plans to do so). My exact setup is actually PyCharm @ OSX but interpreter is running in Docker container Ubuntu 16.04 (tensorflow/tensorflow:1.9.0-devel-py3) ", "@Xyand while I cannot guarantee that this specific piece of tensorflow code causes the issue on Windows, I can guarantee that the underlying problem (PyCharm debugger using evaluation frames with generators) does cause an issue on Windows (I'm using it, and I get the issue with sympy)", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm closing this issue because (as far as I can tell) there's no action item on the TensorFlow team's part. Please feel free to reopen if this changes.", "This problem still exists. setting PYDEVD_USE_FRAME_EVAL might be a fix on the user side, but obviously it should be fixed on the source side", "@danielbraun89 As far as we understand, the fix would need to be in PyCharm, and it's not clear that there's anything the TensorFlow team can do here. See https://youtrack.jetbrains.com/issue/PY-30036 for the PyCharm bug."]}, {"number": 21297, "title": "kernel register bug fix", "body": "If the right kernel found, don't set `was_attr_mismatch` true.", "comments": ["@case540 @petewarden Please give a review or reassign."]}, {"number": 21296, "title": "Support constant value tensor shape for tf.constant", "body": "\r\nThis fix tries to address the issue raised in #21267 where\r\nit was not possible to provide a tensor to the shape argument\r\nof tf.constant:\r\n```\r\ntf.constant(3, shape=(3, 1, tf.constant(3)))\r\n```\r\nwill throw out ValueError.\r\n\r\nThis fix is an attempt to address the issue so that at least\r\nthe constant value tensor shape like `shape=(3, 1, tf.constant(3))`\r\nor `shape=tf.constant([3, 1, 3])` is supported.\r\nIt may not be a complete fix but an improvement.\r\n\r\nThis fix fixes #21267.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Sorry for delay looking at this! According to @asimshankar comment for #21267, it is better to use `tf.fill` in this case.\r\n\r\nIs there another usecase for this PR? Would it be ok to use `tf.fill` in all such cases?", "@yongtang gentle ping to check review comments", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 21295, "title": "support 3d convolution with float64", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom pip install\r\n\r\n- **TensorFlow version (use command below)**:\r\ntensorflow 1.9.0\r\n\r\n\r\n\r\n- **Python version**:\r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\ncuDNN 9.0 v7, CUDA9.1\r\n\r\n- **GPU model and memory**:\r\nGTX980Ti 6GB, Geforce Titan X 12GB\r\n\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI'm working on some convolutional calculation on the Tensorflow and got some strange errors with the Double Precision numbers.\r\nI needed to calculate those tensors in float64 because of the precision issues.\r\n\r\nThe conv3d works fine with me on float32, but on the Double precision, it only works on CPU and on the GPU it causes error.\r\n\r\nSo is there any way I can get access to this operation?\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith tf.device('/device:GPU:0'):\r\n\r\n    a = tf.constant(2 * np.ones([1, 200, 200, 200, 9], dtype=np.float64), dtype=tf.float64)\r\n\r\n    b = tf.constant(np.ones([3, 3, 3, 9, 9], dtype=np.float64), dtype=tf.float64)\r\n\r\n\r\n    sess = tf.Session()\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    sess.run(init)\r\n\r\n\r\n    c = tf.nn.conv3d(a, b, strides=[1, 1, 1, 1, 1], padding='SAME')\r\n    print(a)\r\n    print(b)\r\n    print(c)\r\n    for i in range(1000):\r\n        print(sess.run(c))\r\n\r\nthe output says\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Conv3D': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\n [[Node: Conv3D = Conv3D[T=DT_DOUBLE, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/device:GPU:0\"](Const, Const_1)]]\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Hi, I create a PR #21356 to fix the issue.", "Thanks!", "Nagging Assignee @robieta: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Eshonar sorry for the delay. The PR has been merged in master branch. I wish it could be useful for you :-)"]}, {"number": 21294, "title": "tfconvert float to float_ref valueError ", "body": "\r\n **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NA\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:tensorflow-gpu 1.9\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:0.15\r\n- **GCC/Compiler version (if compiling from source)**:5.4\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**: geforce 1070/8G\r\n- **Exact command to reproduce**:\r\nUsing the command:\r\n tflite_convert  --graph_def_file=./opt.pb --output_format=TFLITE   --output_file=./model.tflite --inference_type=FLOAT  --input_arrays=inputs   --output_arrays=softmax --input_shapes=1,5000,1,1\r\nYou can collect some of this information using our environment capture script:\r\n\r\n### Describe the problem\r\nConverting a trained CNN+RNN  model which was freezed by the freeze_graph and optimized  command gives the valueError message:\r\nInput 0 of node save/Assign was passed float from beta1_power:0 incompatible with expected float_ref\r\n\r\nI guess the issue was caused by the Adam optimizer \r\n\r\n### Source code / logs\r\n### The parameters saved after trainning process like this\r\n![screenshot from 2018-08-01 10-06-34](https://user-images.githubusercontent.com/32789540/43496953-a018118a-9572-11e8-8aaf-c3001378a205.png)\r\n\r\n\r\n\r\nWhat can I do now?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks for your prompt help, and sorry to lack the information in the issue template, cause I think it has little relevance in my case. I got the same issue when using tensorflow version of 0.12 and cuda8/dunddn6.\r\nI have updated the issue information above, please help.", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Can you provide the code you are trying to run, and the relevant error traces?", "Sorry, maybe I should tell you my work now, I just want to convert my own defined tensorflow model into a tflite model which works in mobile phones.\r\n\r\nIn my code I use following codes to create an adam optimizer for trainning process\r\n \r\n```\r\n#Optimizer that use different learning rates for each part of the network\r\n            train_op, grads_and_vars_op = adam_clipping_list_lr(\r\n                loss=train_net.loss_op,              \r\n                list_lrs=[0.00001, 0.0001],\r\n                list_train_vars=[train_vars1, train_vars2],\r\n                clip_value=10.0\r\n            )\r\n```\r\n\r\nFirst I freezed the model by the freeze_graph.py in tensorflow reposite and  and optimized the model by the optimize_for_inference.py command.\r\nThe I  use the following command to produce a tflite mobile model.\r\n`**tflite_convert --graph_def_file=./opt.pb --output_format=TFLITE --output_file=./model.tflite --inference_type=FLOAT --input_arrays=inputs --output_arrays=softmax --input_shapes=1,5000,1,1`\r\nBut I got the following message:**\r\n\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n2018-08-03 14:13:55.136839: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-08-03 14:13:55.241106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-03 14:13:55.241540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.873\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.27GiB\r\n2018-08-03 14:13:55.241554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-08-03 14:13:55.417519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-03 14:13:55.417569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-08-03 14:13:55.417575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-08-03 14:13:55.417775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7020 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:07:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 121, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 309, in convert\r\n    allow_custom_ops=self.allow_custom_ops)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 225, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 107, in toco_convert_protos\r\n    (stdout, stderr))\r\n**RuntimeError: TOCO failed see console for info.**\r\nb'2018-08-03 14:13:58.069666: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.069846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.070054: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.070240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.070338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.070471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.070627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.070771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.074949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Reciprocal\\n2018-08-03 14:13:58.075008: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-03 14:13:58.099706: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 322 operators, 491 arrays (0 quantized)\\n2018-08-03 14:13:58.103204: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 322 operators, 491 arrays (0 quantized)\\n2018-08-03 14:13:58.103368: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] **RandomUniform op outputting \"rssEEG/l1_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy).** **Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this**\\n2018-08-03 14:13:58.104557: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l5_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.105416: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l9_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.138101: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85]` RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/FW/FW/MultiRNNCell/Cell0/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). ``Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 `14:13:58.142571: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/FW/FW/MultiRNNCell/Cell1/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.164082: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/BW/BW/MultiRNNCell/Cell0/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.168823: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/BW/BW/MultiRNNCell/Cell1/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.168978: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l12_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.171340: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 277 operators, 425 arrays (0 quantized)\\n2018-08-03 14:13:58.171902: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l12_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.171985: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/BW/BW/MultiRNNCell/Cell1/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.172107: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/BW/BW/MultiRNNCell/Cell0/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.172222: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/FW/FW/MultiRNNCell/Cell1/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.172334: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l11_bi_gru/BiRNN/FW/FW/MultiRNNCell/Cell0/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.172541: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l9_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.172844: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l5_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.173163: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"rssEEG/l1_dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\\n2018-08-03 14:13:58.175469: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 277 operators, 425 arrays (0 quantized)\\n2018-08-03 14:13:58.178267: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 256128 bytes, theoretical optimal value: 256000 bytes.\\n2018-08-03 14:13:58.179561: F tensorflow/contrib/lite/toco/tflite/export.cc:330] `**`Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If`**` you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  **you will need custom implementations: RandomUniform, Reciprocal, TensorFlowSqrt, TensorFlowSquare, TensorFlowSum, Unpack.\\nAborted (core dumped)\\n'**\r\nNone\r\n\r\n\r\nThen I adoped the detailed conveting process, when I use the quantize_graph command below:\r\n`sudo bazel-bin/tensorflow/tools/quantization/quantize_graph  --input=./op.pb --output_node_names=\"rssEEG/l13_softmax/Add\" --output=./model.pb --mode=eightbit`\r\n\r\nI got the metioned error message:\r\nValueError: Input 0 of node save/Assign was passed float from beta1_power:0 incompatible with expected float_ref.\r\n\r\nDetailed message here:\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\ntf.estimator package not installed.\r\nTraceback (most recent call last):\r\n  File \"/media/joyice/ssdata/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node save/Assign was passed float from beta1_power:0 incompatible with expected float_ref.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/media/joyice/ssdata/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1302, in <module>\r\n    app.run()\r\n  File \"/media/joyice/ssdata/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/media/joyice/ssdata/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1273, in main\r\n    importer.import_graph_def(tf_graph, input_map={}, name=\"\")\r\n  File \"/media/joyice/ssdata/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/media/joyice/ssdata/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 422, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node save/Assign was passed float from beta1_power:0 incompatible with expected float_ref.\r\n\r\n\r\n\r\n\r\n", "Nagging Assignee @miaout17: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Regarding the error message occurring from `tflite_convert`.\r\n\r\n> Here is a list of operators for which you will need custom implementations: RandomUniform, Reciprocal, TensorFlowSqrt, TensorFlowSquare, TensorFlowSum, Unpack. Aborted (core dumped)\r\n\r\nThe list of supported ops are available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md). The two recommended solutions are:\r\n1. Add a custom op as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md).\r\n2. Only use the ops that are currently supported by TFLite in your model.\r\n\r\nAdditionally, we currently have limited support for converting RNN models. You noted your model was a CNN+RNN. While we have implemented the RNN ops (e.g. RNN, LSTM, UNIDIRECTIONAL_LSTM...etc) in TensorFlow Lite, there is no easy way to author the ops currently. We're working on it. Stay tuned. \r\n\r\n@suharshs Can you comment on the issue with running the model through `quantize_graph`.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21293, "title": "Support connected components", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 21292, "title": "Allow for python3.5 and 3.6 builds for MacOS.", "body": "PiperOrigin-RevId: 206831440", "comments": []}, {"number": 21291, "title": "Add new Dockerfile assembler based on partials", "body": "This change adds a new suite of TensorFlow dockerfiles. The dockerfiles come from an assembler controlled by a yaml spec, and are based on a set of re-usable partial dockerfiles. You can build any of the generated Dockerfiles to see how they work. See the new README.md for more information.\r\n\r\nI'd recommend cloning from angersson:angerson-docker if you want to try this out.\r\n\r\nThis PR is the implementation for [this design document](https://github.com/tensorflow/community/pull/8).\r\n\r\nThe assembler and spec include conveniences like spec validation, references to other images and specs for minimizing repetition, and arg expansion.", "comments": ["Love it! When can I add MKL?", "@claynerobison Hopefully soon! If you don't mind starting early (at the risk of running into design changes later), you could fork my repo and add MKL there, to be sent after this design gets approved and merged. It would be a fantastic way to test out the system and see how it feels to use (which would be really helpful feedback for the design review's sake).", "@gunan Thanks! Please take a look at my latest changes. Which files should I add the license header to? The other dockerfiles and such don't have one, so I'm not sure which I should add it to aside from the assembler.py script.", "@gunan I addressed your comments (I think I got them all?). Can you take one more look?", "Woot!\r\n"]}, {"number": 21290, "title": "This adds a new way to manage our complex docker images.", "body": null, "comments": ["Whoops. This isn't totally ready yet."]}, {"number": 21289, "title": "Add extra log for failing to load variables", "body": "There is a flaky in tensorflow/serving. There is a chance for loading a new model version to fail and then following version would all fail with the `The specified SavedModel has no variables; no checkpoints were restored` \r\nrelated to this https://github.com/tensorflow/serving/issues/1027\r\n\r\nWhich would cause the serving model have non-initialized weights error.\r\n\r\nAdding this extra log would help us understand more on why it's failing ", "comments": ["Can you add a line break before ` << variables_index_path;`? It's above the 80 character limit, and causing the linter to complain. \r\n\r\n@yifeif how is clang-format looking?", "there is an error with `ImportError: cannot import name 'gen_collective_ops'` \r\n\r\ni don't see how they are connected "]}, {"number": 21288, "title": "fix the function name in the gide of custom estimator", "body": "I changed the name of some variables from `my_model` to `my_model_fn` because the model functions in this guide are defined as `my_model_fn`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21287, "title": "Possible bug in dynamic_rnn when training on TPU for iterations_per_loop > 1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud Platform (Linux Debian)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA (TPU training)\r\n- **GPU model and memory**: NA (TPU training)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTraining an RNN (constructed with `dynamic_rnn`) on TPU gives largely different loss values for `iterations_per_loop=1` and `iterations_per_loop=100`. The loss when training on TPU with `iterations_per_loop=1` is very close to the loss when training on CPU, but the loss for `iterations_per_loop=100` case is orders of magnitude different.\r\n\r\nSee below for the code to reproduce this issue. I also tested it with BasicRNNCell (instead of GRU) and observed the same issue. For easier debugging, I have made the runs deterministic (all the random ops are seeded, repeated runs produce the exact same values).\r\n\r\nNote that if I replace my model_fn with a simple linear model containing only matrix multiplication (instead of `dynamic_rnn`) the loss for any value of `iterations_per_loop` will be the same which is as expected. So I suspect there is a bug in using `dynamic_rnn` with TPU. \r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\r\nimport subprocess\r\nimport os\r\n\r\nSEED = 10010\r\n\r\nUSE_TPU = True\r\n\r\ndef make_data(params):\r\n    # make training and validation data: sinusoids with random phases\r\n    np.random.seed(SEED)\r\n    num_samp_tr = 1000\r\n    num_samp_val = 1000\r\n    ramps_tr = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_tr, params['dims'], 100)), (0, 2, 1))\r\n    rand_phase = np.transpose(np.tile(np.random.randn(num_samp_tr, params['dims']), (100,1,1)), (1, 0, 2))\r\n    ramps_val = np.transpose(np.broadcast_to(0.1*np.arange(0,100), (num_samp_val, params['dims'], 100)), (0, 2, 1))\r\n    rand_phase_val = np.transpose(np.tile(np.random.randn(num_samp_val, params['dims']), (100,1,1)), (1, 0, 2))\r\n    data = {'train_data': np.sin(ramps_tr + rand_phase),\r\n            'valid_data': np.sin(ramps_val + rand_phase_val)}\r\n    return data\r\n\r\ndef input_fn(data_dict, mode):\r\n    def data_fn(params):\r\n        batch_size = params['batch_size']\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32)).cache().repeat().shuffle(buffer_size=10000, seed=SEED)\r\n        else:\r\n            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32)).cache().repeat()\r\n        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n        return dataset\r\n    return data_fn\r\n\r\ndef model_fn(features, mode, params):\r\n    #tf.set_random_seed(SEED) # Use for BasicRNNCell, it does not get an initializer\r\n    batch_size=params['batch_size']\r\n    ts = features.get_shape().as_list()[1]\r\n    seq_len = ts * np.ones([batch_size,])\r\n    with tf.variable_scope('encoder'):\r\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\r\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\r\n        _, output_latent = tf.nn.dynamic_rnn(cell=cell, inputs=features, sequence_length=seq_len, dtype=tf.float32)\r\n    with tf.variable_scope('decoder'):\r\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n        #cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\r\n        cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\r\n        z_inps = tf.zeros([batch_size, ts, 1])\r\n        output_recon, _ = tf.nn.dynamic_rnn(cell=cell, inputs=z_inps, initial_state=output_latent, sequence_length=seq_len, dtype=tf.float32)    \r\n\r\n    winit = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None, weights_initializer=winit)\r\n    loss = tf.losses.mean_squared_error(features, output_recon)\r\n\r\n    global_step = tf.train.get_global_step()\r\n    opt = tf.train.AdamOptimizer(0.01)\r\n    if USE_TPU:\r\n        opt = tf.contrib.tpu.CrossShardOptimizer(opt)\r\n    train_op = opt.minimize(loss, global_step)\r\n    def metric_fn(labels, rec):\r\n        return {\r\n        'MSE': tf.metrics.mean_squared_error(labels, rec),\r\n        }\r\n    tpu_eval_metrics = (metric_fn, [features, output_recon])\r\n\r\n    return tpu_estimator.TPUEstimatorSpec(mode=mode,\r\n                                      loss=loss,\r\n                                      train_op=train_op,\r\n                                      eval_metrics=tpu_eval_metrics,\r\n                                      )\r\n\r\ndef train_model(num_steps, iterations_per_loop, num_shards=1):\r\n    if USE_TPU:\r\n        my_project = subprocess.check_output([\r\n            'gcloud','config','get-value','project'])\r\n        my_zone = subprocess.check_output([\r\n            'gcloud','config','get-value','compute/zone'])\r\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n                tpu=[os.environ['TPU_NAME']],\r\n                )\r\n        tpu_cluster_resolver = tpu_cluster_resolver\r\n    else:\r\n        tpu_cluster_resolver = None\r\n\r\n    #tf.logging.set_verbosity(tf.logging.INFO)\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=True)\r\n\r\n    run_config = tf.contrib.tpu.RunConfig(\r\n        save_checkpoints_steps=400,\r\n        cluster=tpu_cluster_resolver,\r\n        keep_checkpoint_max=1,\r\n        model_dir = 'gs://test-bucket/runs',\r\n        session_config=config,\r\n        tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=iterations_per_loop, num_shards=num_shards))\r\n    \r\n    params = {'dims': 5}\r\n    data = make_data(params)\r\n\r\n    train_input = input_fn(data, tf.estimator.ModeKeys.TRAIN)\r\n    eval_input = input_fn(data, tf.estimator.ModeKeys.EVAL)\r\n\r\n    model = tf.contrib.tpu.TPUEstimator(model_fn=model_fn, params=params, config=run_config,\r\n        use_tpu=USE_TPU, train_batch_size=100, eval_batch_size=100)\r\n    model.train(train_input, steps=num_steps)\r\n    \r\n    valid_costs = model.evaluate(eval_input, name='valid_data', steps=2)\r\n    print('==== Evaluation:')\r\n    print(valid_costs)\r\n    return valid_costs\r\n\r\n\r\n\r\nprint(\"==================== Training with iterations_per_loop = 1\")\r\nrun1 = train_model(num_steps=100, iterations_per_loop=1, num_shards=1)\r\n\r\n# remove checkpoints\r\nsubprocess.call(\"gsutil -m rm -r gs://test-bucket/runs/*\", shell=True)\r\n\r\nprint(\"==================== Training with iterations_per_loop = 100\")\r\nrun2 = train_model(num_steps=100, iterations_per_loop=100, num_shards=1)\r\n\r\nprint('Summary:')\r\nprint('====== iterations_per_loop = 1 :')\r\nprint(run1)\r\n\r\nprint('====== iterations_per_loop = 100 :')\r\nprint(run2)\r\n```\r\n\r\n### Output (multiple runs):\r\n**CPU Run:**\r\n```{'loss': 0.2408253, 'MSE': 0.24082531, 'global_step': 100}```\r\n\r\n**TPU Runs:**\r\niterations_per_loop=1\r\nRun1:\r\n```{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}```\r\nRun2:\r\n```{'loss': 0.24119371, 'MSE': 0.2411936, 'global_step': 100}```\r\n\r\niterations_per_loop=100\r\nRun1:\r\n```{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}```\r\nRun2:\r\n```{'loss': 29.255905, 'MSE': 29.25589, 'global_step': 100}```", "comments": ["@ebrevdo any ideas what the issue is?", "Taking a look today", "Do you see the same difference when using static_rnn?\n\nOn Wed, Aug 1, 2018, 10:56 AM ebrevdo <notifications@github.com> wrote:\n\n> Taking a look today\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21287#issuecomment-409664626>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxwoeJTam-G0dQ0nWFlHznYGokthks5uMevfgaJpZM4VpIVD>\n> .\n>\n", "I am trying it out with static_rnn, and will update you ASAP.", "Seems that I cannot even get `static_rnn` to work with TPU while it works okey on CPU (and it gave me similar costs as `dynamic_rnn` on CPU). \r\n\r\nHere is the model_fn code with static_rnn:\r\n```\r\ndef model_fn(features, mode, params):\r\n    tf.set_random_seed(SEED) # Use for BasicRNNCell, it does not get an initializer\r\n    batch_size=params['batch_size']\r\n    ts = features.get_shape().as_list()[1]\r\n    seq_len = ts * np.ones([batch_size,])\r\n    with tf.variable_scope('encoder'):\r\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n        cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\r\n        #cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\r\n        rnn_input = list(tf.split(features, ts, axis=1))\r\n        rnn_input = [tf.squeeze(t) for t in rnn_input]\r\n        _, output_latent = tf.nn.static_rnn(cell=cell, inputs=rnn_input, sequence_length=seq_len, dtype=tf.float32)\r\n        print(output_latent)\r\n    with tf.variable_scope('decoder'):\r\n        init_kern = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n        cell = tf.contrib.rnn.BasicRNNCell(num_units=20)\r\n        #cell = tf.contrib.rnn.GRUCell(num_units=20, kernel_initializer=init_kern)\r\n        z_inps = [tf.zeros([batch_size, 1])] * ts\r\n        output_recon, _ = tf.nn.static_rnn(cell=cell, inputs=z_inps, initial_state=output_latent,\r\n                                            sequence_length=seq_len, dtype=tf.float32)\r\n        output_recon = tf.stack(output_recon, 1)\r\n        print(output_recon)\r\n\r\n    winit = tf.random_normal_initializer(0.0, 0.1, dtype=tf.float32, seed=SEED)\r\n    output_recon = tf.contrib.layers.fully_connected(inputs=output_recon, num_outputs=params['dims'], activation_fn=None, weights_initializer=winit)\r\n    loss = tf.losses.mean_squared_error(features, output_recon)\r\n\r\n    global_step = tf.train.get_global_step()\r\n    opt = tf.train.AdamOptimizer(0.01)\r\n    if USE_TPU:\r\n        opt = tf.contrib.tpu.CrossShardOptimizer(opt)\r\n    train_op = opt.minimize(loss, global_step)\r\n    def metric_fn(labels, rec):\r\n        return {\r\n        'MSE': tf.metrics.mean_squared_error(labels, rec),\r\n        }\r\n    tpu_eval_metrics = (metric_fn, [features, output_recon])\r\n\r\n    return tpu_estimator.TPUEstimatorSpec(mode=mode,\r\n                                      loss=loss,\r\n                                      train_op=train_op,\r\n                                      eval_metrics=tpu_eval_metrics,\r\n                                      )\r\n```\r\n\r\nAnd here is the error I get using static_rnn on TPU:\r\n```\r\n    model.train(train_input, steps=num_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1119, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1135, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1336, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 577, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1053, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1144, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1129, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1201, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 981, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Compilation failure: Mismatched parent frames for 932: while_context vs\r\n\tTPU compilation failed\r\n\t [[Node: tpu_compile_succeeded_assert/_25 = TPUCompileSucceededAssert[_device=\"/job:worker/replica:0/task:0/device:TPU_SYSTEM:0\"](TPUReplicate/_compile/_24)]]\r\n\t [[Node: encoder/rnn/basic_rnn_cell/bias/Adam/switch_val/_96_G105 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:TPU:0\", send_device_incarnation=3404073267463702744, tensor_name=\"edge_225_encoder/rnn/basic_rnn_cell/bias/Adam/switch_val/_96\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "We're looking into the error with static_rnn first.  After that's fixed you will be able to rerun and we can compare again.", "I can reproduce this externally (Cloud TPU, TF 1.9), but when I tried to reproduce this internally (with dynamic_rnn) but I'm seeing identical loss and evaluation metrics for iterations_per_loop={1, 100}.\r\n\r\n@saeta could there be a version issue here?  Do we have a TF 1.10 version available we can test with?", "Simple solution: try using dynamic_rnn using a tensorflow nightly version.", "Thanks @ebrevdo. Any pointers on how I can upgrade TF on the cloud TPU kernel to tf-nightly? \r\n(When I use tfnightly on the VM only I see a binary incompatibility error since the cloud TPU kernel is running on TF1.9 and I cannot get that updated to tfnightly). ", "Looping in Brennan.\n\nOn Thu, Aug 2, 2018, 1:40 PM Mohammad Reza Keshtkaran <\nnotifications@github.com> wrote:\n\n> Thanks @ebrevdo <https://github.com/ebrevdo>. Any pointers on how I can\n> upgrade TF on TPU kernel to tf-nightly?\n> (When I use tfnightly on the VM only I see a binary incompatibility error).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21287#issuecomment-410060748>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9BFlhEhz2BWLUcfsGr0YNJFdusnks5uM2O0gaJpZM4VpIVD>\n> .\n>\n", "Hi @mrezak we have nightly builds available to Googlers and whitelisted projects. TF 1.10 RC1 is broken with Cloud TPUs, but RC2 should be fixed. Once that drops, we'll open up access (assuming there's no other underlying issues still to be discovered). If you're super eager (and are willing to put up with nightly sometimes being broken), please email me your GCP Project ID (`saeta@google.com`), and I'll see if we can get you white listed.\r\n\r\nAll the best,\r\n-Brennan\r\n", "Thanks @saeta! Sent you an email with our GCP Project information.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "After much delay (sorry about that!), @mrezak your GCP project should be whitelisted. Please do re-open if this is not the case. Thanks!\r\n\r\n(For others reading this thread, TF 1.10 has encountered a couple delays that we are working on fixing. It should be out soon. Thanks for your patience!)", "Thanks a lot @saeta! I could use the nightly build on TPU and got consistent results using dynamic_rnn! "]}, {"number": 21286, "title": "Improve estimator.py doc-strings", "body": "* Fixed typos\r\n* Warped python terms with ` characters\r\n* A lot of tf terms are now links to their docs", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the cleanup! Much appreciated."]}, {"number": 21285, "title": "add debian stretch Dockerfile (CPU)", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "We use the Dockerfiles under `ci_build` mostly for internal release architecture. If you're submitting this for general use, you'll need to place it in `tools/docker`, where none of these helper scripts are available.\r\n\r\nAlso, we are currently reviewing https://github.com/tensorflow/community/pull/8, which would totally restructure how *those* Dockerfiles are maintained (because they are widespread and messy). Please take a look at that doc if you're interested in improving the Dockerfiles.", "Nagging Assignee @case540: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21284, "title": "TensorFlow 1.9.0 deadlocks on libc6 2.19?", "body": "### System information\r\n- **Have I written custom code**: Yes, reproduction repo: https://github.com/ravwojdyla/tf190bug\r\n- **OS Platform and Distribution**: Ubuntu 14.04, Container-Optimized OS\r\n- **TensorFlow installed from (source or binary)**:  binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Exact command to reproduce**: see repo https://github.com/ravwojdyla/tf190bug\r\n- **Mobile device**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n### Describe the problem\r\nIt seems like TF 1.9.0 deadlocks when used concurrently and linked to libc 2.19 (libpthread?)  (Ubuntu 14.04 and COS to name a some). See the reproduction repo. This is specifically a problem if TF 1.9.0 is used on Google Dataflow worker which  currently ship with libc 2.19. The libc/libpthread is currently just a theory, that said I validated that the reproduction code works fine with libc 2.24 (Ubuntu 16.04).\r\n\r\nI might continue investigating this issue, but wanted to double check if this is a known problem?\r\n\r\n### Source code / logs\r\n\r\nReproduction repo: https://github.com/ravwojdyla/tf190bug\r\n\r\nStack of one of the threads:\r\n\r\n```\r\n\"Thread-9\" #18 prio=5 os_prio=0 tid=0x00007fa568149800 nid=0x4a runnable [0x00007fa55244e000]\r\n   java.lang.Thread.State: RUNNABLE\r\n        at org.tensorflow.Tensor.allocateScalarBytes(Native Method)\r\n        at org.tensorflow.Tensor.create(Tensor.java:150)\r\n        at org.tensorflow.Tensor.create(Tensor.java:115)\r\n        at org.tensorflow.Tensors.create(Tensors.java:257)\r\n        at sh.rav.TestTF.createTensor(TestTF.java:11)\r\n        at sh.rav.TestTF$1.run(TestTF.java:18)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\nThose threads are all in runnable state, and in fact they do burn CPU, I checked the native frames as well, it seems like some form of a starvation.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "hi @shivaniag,\r\nAny update on this? Should I look deeper into this?", "Nagging Assignee @shivaniag: It has been 92 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this is resolved."]}, {"number": 21283, "title": "Improve readability of Tensor::CheckType error output", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21282, "title": "Merging the r1.10 branch back to master.", "body": "", "comments": []}, {"number": 21281, "title": "Add Cloud Bigtable user docs for TF 1.10 release", "body": "This PR pulls in 2 existing commits from the `master` branch into the 1.10 release branch:\r\n\r\n* [[tf.data / Bigtable] Document use of the Cloud Bigtable API](https://github.com/tensorflow/tensorflow/commit/2279279fd15369e361a02fb09a1df41e08a34aae) by @saeta \u2014 this provides the critical documentation for getting started with TF + Cloud Bigtable integration\r\n* [[tf.data / Bigtable] Renamed BigTable class to BigtableTable for clarity](https://github.com/tensorflow/tensorflow/commit/162304f9da4114f5ed3f0e4c27929413e7abc965) by @mbrukman \u2014 this is a simple renaming change, but it cleans up the documentation and removes confusion on similar names of \"BigTable\" + \"Bigtable\", so I highly recommend including this as well\r\n\r\nWith these changes, the final TF 1.10 release will be much more usable to Cloud Bigtable users.\r\n\r\nWithout these changes, TF 1.10 will ship without any docs for the Cloud Bigtable, and we'll have to manually refer users to [`README.md` in GitHub](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/bigtable) instead of the official TF docs, which is cumbersome. Also, any TF users who independently discover the Cloud Bigtable integration will have no way of knowing how to use the two together, and will have to wait until TF 1.11 instead.", "comments": ["LGTM.", "@saeta \u2014 thanks for the review!\r\n\r\n@av8ramit \u2014 looks like all 17 checks have passed, so I presume it's OK to merge?", "Thanks, @av8ramit!"]}, {"number": 21280, "title": "Update libpng to v1.6.35", "body": "This fix updates libpng to the latest version of v1.6.35 that was released on 07/2018.\r\n(The last version of v1.6.34 was released on 09/2017)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the PR, but what does this update fix?", "Thanks @case540. The development of libpng has not been very active recently so there is not a lot of change compared with the last version.\r\n\r\nThe last version was released almost 1 year ago though, so I thought it might be ok to update.\r\n\r\nSince this is not very imperative, let me close the PR for now."]}, {"number": 21279, "title": "Add complex128 support for tf.as_string", "body": "The tf.as_string supports most of the data types (including `complex64`) but not `complex128`.\r\n\r\nThis fix add `complex128` support for tf.as_string.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks!"]}, {"number": 21278, "title": "Long awaited update", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Please add more details in the desc, fix some issues (such as I see commented out lines), an rebase on latest master commit (merge conflict)"]}, {"number": 21277, "title": "Using TensorFlow's Datasets API causes process to hang in session destructor", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nMacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nSource (but happens with the binary version as well)\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n\r\n- **Python version**:\r\n\r\nPython 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n0.10.1\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nN/A\r\n\r\n- **GPU model and memory**:\r\n\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n\r\nUnfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).\r\n\r\n### Describe the problem\r\n\r\n**Summary:**\r\n\r\nWe are using TensorFlow's Datasets API. More specifically, we're using `tf.data.Dataset.from_generator` to create a dataset based on a generator function.\r\n\r\nWhen Python comes to garbage collect our `tf.Session` object its destructor makes a call into TensorFlow to delete the session (`tf_session.TF_DeleteSession`). This call hangs because it's trying to execute a `tf.py_func` function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the \"finalize\" function from our dataset.\r\n\r\nThis looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.\r\n\r\n**More Details:**\r\n\r\nWhen our `tf.Session` object is garbage collected in Python, its destructor (`__del__` method) hangs indefinitely. The problem appears to be this call in `BaseSession`:\r\n\r\n    tf_session.TF_DeleteSession(self._session)\r\n\r\nRunning lldb shows the following stack trace:\r\n\r\n    * thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\r\n      * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n        frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n        frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n        frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283\r\n        frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423\r\n        frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49\r\n        frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67\r\n        frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649\r\n        frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97\r\n        frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n        frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105\r\n        frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n        frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43\r\n        frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169\r\n        frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14\r\n        frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301\r\n        frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192\r\n        frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104\r\n        frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14\r\n        frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63\r\n        frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205\r\n        frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546\r\n        frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14\r\n        frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931\r\n        frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122\r\n        frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568\r\n        frame #26: 0x00000001008443e4 Python`call_function + 612\r\n        frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n        frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\r\n        frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\r\n        frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\r\n        frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180\r\n        frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121\r\n        frame #33: 0x000000010089b18a Python`collect + 1418\r\n        frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99\r\n        frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119\r\n        frame #36: 0x000000010087b0e0 Python`Py_Exit + 16\r\n        frame #37: 0x000000010087ef4c Python`handle_system_exit + 252\r\n        frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437\r\n        frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125\r\n        frame #40: 0x00000001008992a4 Python`Py_Main + 1812\r\n        frame #41: 0x0000000100000dfe Python\r\n        frame #42: 0x0000000100000c34 Python\r\n\r\nIt appears that the session's destructor is waiting for an op to complete. The culprit seems to be `PyFuncOp`, which doesn't get past this line:\r\n\r\n    py_threadstate = PyGILState_Ensure();\r\n\r\nSo it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the \"finalize\" function for the dataset (from `_GeneratorDataset`).\r\n\r\nMy assumption is that when Python calls `tf_session.TF_DeleteSession(self._session)` that the GIL should be released, and so `PyFuncOp` should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.\r\n\r\nUnfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem.", "comments": ["Thanks for the report and for diving into the details... this definitely sounds like a bug, although my understanding is the same as yours: `tf_session.TF_DeleteSession(self._session)` should release the GIL because of this SWIG code block here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2826d123a017bc5f1a2cc7b969e275c5a63c326c/tensorflow/python/client/tf_session.i#L106-L111\r\n\r\nIn terms of a reproduction, would you be able to capture a dump of *all* thread stacks when the problem occurs?  One thing I've found useful in debugging this kind of problem is to set `config=tf.ConfigProto(inter_op_parallelism=1, intra_op_parallelism=1)` when creating the session. This makes the set of threads less unwieldy, and can sometimes tease out deadlock bugs that are less likely to happen with larger threadpools.", "Sure, I've included a backtrace of all native threads below. This is with inter_op_parallelism_threads = 1 and intra_op_parallelism_threads = 1.\r\n\r\nI've also included a thread dump from Python as well, in case it's interesting. It doesn't look to me like any of the Python threads should be holding the GIL either. The only interesting one looks like 0x00007000025b4000, which is performing an IO operation (consuming from a multiprocessing queue) that I believe should also have caused the GIL to be released.\r\n\r\n**Native threads:**\r\n\r\n```\r\n(lldb) thread backtrace all\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\r\n  * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x00000001127b665b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283\r\n    frame #4: 0x00000001127b2ed7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423\r\n    frame #5: 0x00000001127b3641 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49\r\n    frame #6: 0x000000010909d4b3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67\r\n    frame #7: 0x0000000109d69869 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649\r\n    frame #8: 0x0000000109d1ba81 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97\r\n    frame #9: 0x0000000109d1bbee _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n    frame #10: 0x0000000109d196c9 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105\r\n    frame #11: 0x0000000109d1973e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n    frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43\r\n    frame #13: 0x0000000109d265d9 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169\r\n    frame #14: 0x0000000109d2665e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14\r\n    frame #15: 0x0000000112289cad libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301\r\n    frame #16: 0x0000000112289eb0 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192\r\n    frame #17: 0x0000000109d285b8 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104\r\n    frame #18: 0x0000000109d2877e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14\r\n    frame #19: 0x000000011228325f libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63\r\n    frame #20: 0x000000011228415d libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205\r\n    frame #21: 0x000000010b89cba2 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546\r\n    frame #22: 0x000000010b89d0ee _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14\r\n    frame #23: 0x000000010937a0b2 _pywrap_tensorflow_internal.so`TF_DeleteSession + 290\r\n    frame #24: 0x000000010902319a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122\r\n    frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568\r\n    frame #26: 0x00000001008443e4 Python`call_function + 612\r\n    frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\r\n    frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\r\n    frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\r\n    frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180\r\n    frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121\r\n    frame #33: 0x000000010089b18a Python`collect + 1418\r\n    frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99\r\n    frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119\r\n    frame #36: 0x000000010087b0e0 Python`Py_Exit + 16\r\n    frame #37: 0x000000010087ef4c Python`handle_system_exit + 252\r\n    frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437\r\n    frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125\r\n    frame #40: 0x00000001008992a4 Python`Py_Main + 1812\r\n    frame #41: 0x0000000100000dfe Python\r\n    frame #42: 0x0000000100000c34 Python\r\n  thread #2\r\n    frame #0: 0x00000001018566da libsystem_kernel.dylib`__workq_kernreturn + 10\r\n    frame #1: 0x000000010188c06a libsystem_pthread.dylib`_pthread_wqthread + 1035\r\n    frame #2: 0x000000010188bc4d libsystem_pthread.dylib`start_wqthread + 13\r\n  thread #3\r\n    frame #0: 0x0000000000000000\r\n  thread #4\r\n    frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x0000000100896738 Python`PyThread_acquire_lock_timed + 312\r\n    frame #3: 0x000000010089ca89 Python`acquire_timed + 137\r\n    frame #4: 0x000000010089cbdd Python`lock_PyThread_acquire_lock + 61\r\n    frame #5: 0x00000001007bb545 Python`_PyCFunction_FastCallDict + 245\r\n    frame #6: 0x00000001008443e4 Python`call_function + 612\r\n    frame #7: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #8: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #9: 0x0000000100843fab Python`fast_function + 219\r\n    frame #10: 0x00000001008443cb Python`call_function + 587\r\n    frame #11: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #12: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #13: 0x0000000100843fab Python`fast_function + 219\r\n    frame #14: 0x00000001008443cb Python`call_function + 587\r\n    frame #15: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #16: 0x000000010084412e Python`fast_function + 606\r\n    frame #17: 0x00000001008443cb Python`call_function + 587\r\n    frame #18: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #19: 0x000000010084412e Python`fast_function + 606\r\n    frame #20: 0x00000001008443cb Python`call_function + 587\r\n    frame #21: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #22: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\r\n    frame #23: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\r\n    frame #24: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\r\n    frame #25: 0x000000010075f5b3 Python`PyObject_Call + 99\r\n    frame #26: 0x000000010089c587 Python`t_bootstrap + 71\r\n    frame #27: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340\r\n    frame #28: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377\r\n    frame #29: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13\r\n  thread #5\r\n    frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278\r\n    frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828\r\n    frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568\r\n    frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n    frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n    frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340\r\n    frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377\r\n    frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13\r\n  thread #6\r\n    frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278\r\n    frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828\r\n    frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568\r\n    frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n    frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n    frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340\r\n    frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377\r\n    frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13\r\n  thread #7\r\n    frame #0: 0x0000000101857592 libsystem_kernel.dylib`read + 10\r\n    frame #1: 0x0000000100895402 Python`_Py_read + 82\r\n    frame #2: 0x000000010089f9b9 Python`os_read + 89\r\n    frame #3: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568\r\n    frame #4: 0x00000001008443e4 Python`call_function + 612\r\n    frame #5: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #6: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #7: 0x0000000100843fab Python`fast_function + 219\r\n    frame #8: 0x00000001008443cb Python`call_function + 587\r\n    frame #9: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #10: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #11: 0x0000000100843fab Python`fast_function + 219\r\n    frame #12: 0x00000001008443cb Python`call_function + 587\r\n    frame #13: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #14: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #15: 0x0000000100843fab Python`fast_function + 219\r\n    frame #16: 0x00000001008443cb Python`call_function + 587\r\n    frame #17: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #18: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #19: 0x0000000100843fab Python`fast_function + 219\r\n    frame #20: 0x00000001008443cb Python`call_function + 587\r\n    frame #21: 0x0000000100848aa0 Python`_PyEval_EvalFrameDefault + 17056\r\n    frame #22: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #23: 0x0000000100843fab Python`fast_function + 219\r\n    frame #24: 0x00000001008443cb Python`call_function + 587\r\n    frame #25: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #26: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #27: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95\r\n    frame #28: 0x000000010079344a Python`function_call + 186\r\n    frame #29: 0x000000010075f5b3 Python`PyObject_Call + 99\r\n    frame #30: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457\r\n    frame #31: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #32: 0x0000000100843fab Python`fast_function + 219\r\n    frame #33: 0x00000001008443cb Python`call_function + 587\r\n    frame #34: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #35: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #36: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95\r\n    frame #37: 0x000000010079344a Python`function_call + 186\r\n    frame #38: 0x000000010075f5b3 Python`PyObject_Call + 99\r\n    frame #39: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457\r\n    frame #40: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #41: 0x0000000100843fab Python`fast_function + 219\r\n    frame #42: 0x00000001008443cb Python`call_function + 587\r\n    frame #43: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #44: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #45: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95\r\n    frame #46: 0x000000010079344a Python`function_call + 186\r\n    frame #47: 0x000000010075f5b3 Python`PyObject_Call + 99\r\n    frame #48: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457\r\n    frame #49: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720\r\n    frame #50: 0x0000000100843fab Python`fast_function + 219\r\n    frame #51: 0x00000001008443cb Python`call_function + 587\r\n    frame #52: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #53: 0x000000010084412e Python`fast_function + 606\r\n    frame #54: 0x00000001008443cb Python`call_function + 587\r\n    frame #55: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #56: 0x000000010084412e Python`fast_function + 606\r\n    frame #57: 0x00000001008443cb Python`call_function + 587\r\n    frame #58: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892\r\n    frame #59: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828\r\n    frame #60: 0x000000010075f984 Python`_PyObject_FastCallDict + 356\r\n    frame #61: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208\r\n    frame #62: 0x000000010075f5b3 Python`PyObject_Call + 99\r\n    frame #63: 0x000000010089c587 Python`t_bootstrap + 71\r\n    frame #64: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340\r\n    frame #65: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377\r\n    frame #66: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13\r\n  thread #8\r\n    frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278\r\n    frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828\r\n    frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568\r\n    frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n    frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n    frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340\r\n    frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377\r\n    frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13\r\n  thread #9\r\n    frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278\r\n    frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828\r\n    frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568\r\n    frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n    frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n    frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340\r\n    frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377\r\n    frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13\r\n```\r\n\r\n**Python threads:**\r\n\r\n```\r\nThread 0x0000700002637000 (most recent call first):\r\n\r\nThread 0x00007000025b4000 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379 in _recv\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407 in _recv_bytes\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216 in recv_bytes\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 94 in get\r\n<REDACTED>\r\n\r\n\r\nThread 0x0000700001fab000 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 299 in wait\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 551 in wait\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 1180 in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916 in _bootstrap_inner\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 884 in _bootstrap\r\n\r\nThread 0x0000700001aa8000 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 299 in wait\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 551 in wait\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 1180 in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916 in _bootstrap_inner\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 884 in _bootstrap\r\n\r\nThread 0x0000700000f19000 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 299 in wait\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/queue.py\", line 173 in get\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/watchdog/observers/api.py\", line 360 in dispatch_events\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/watchdog/observers/api.py\", line 199 in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916 in _bootstrap_inner\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 884 in _bootstrap\r\n\r\nThread 0x0000700000a16000 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/selectors.py\", line 577 in select\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py\", line 1389 in _run_once\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py\", line 421 in run_forever\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 864 in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916 in _bootstrap_inner\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 884 in _bootstrap\r\n\r\nCurrent thread 0x0000000101895340 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 707 in __del__\r\n```", "Hmm, I think the Python thread in `multiprocessing/connection.py` (0x00007000025b4000) maps to native thread 7, which is in `_Py_read()` and that drops the GIL as well. \r\n\r\nAs far as I can tell though, none of the native threads is blocked trying to acquire the GIL:\r\n* Thread 1 is waiting on a `tensorflow::Notification` for the end of the finialize function.\r\n* Thread 2 isn't TF-related.\r\n* Thread 3 is ???\r\n* Thread 4 is a Python thread that's waiting on a Python lock. I don't think this is TF-related.\r\n* Threads 5, 6, 8, and 9 are idle TF/Eigen threadpool threads that are waiting for work. I'm a little surprised to see 4 of them rather than `inter_op_parallelism + intra_op_parallelism = 2` threads, but they don't seem to be doing anything concerning.\r\n* Thread 7 is (probably) Python thread 0x00007000025b4000, blocked on a multiprocessing queue.\r\n\r\nIt would be interesting to know if the PyFunc op in finalization actually started and/or finished. One way to do this is to set the environment variable `TF_CPP_MIN_VLOG_LEVEL=1` (which triggers very verbose logging, including each op invocation and completion). Could you try that and capture the part of the log that is produced *after* the `tf.Session` destructor begins?", "Yes, I was also confused by the fact that none of the native threads appear to be waiting to acquire the GIL. I haven't tried `TF_CPP_MIN_VLOG_LEVEL=1` yet, but if I surround the GIL lock acquisition with a couple of log statements as follows:\r\n\r\n```\r\nneil-mac-2:tensorflow nferguson$ git diff\r\ndiff --git a/tensorflow/python/lib/core/py_func.cc b/tensorflow/python/lib/core/py_func.cc\r\nindex 22317a348c..d52b5e4a00 100644\r\n--- a/tensorflow/python/lib/core/py_func.cc\r\n+++ b/tensorflow/python/lib/core/py_func.cc\r\n@@ -469,7 +469,9 @@ class PyFuncOp : public OpKernel {\r\n     }\r\n\r\n     PyGILState_STATE py_threadstate;\r\n+    LOG(ERROR) << \"About to ensure GIL\";\r\n     py_threadstate = PyGILState_Ensure();\r\n+    LOG(ERROR) << \"Got GIL!\";\r\n     bool log_on_error;\r\n     Status s = DoCallPyFunc(&call, &log_on_error);\r\n```\r\n\r\nI can observe it printing:\r\n\r\n`About to ensure GIL`\r\n\r\nbut not:\r\n\r\n`Got GIL!`\r\n\r\nWhich is why I mentioned that it was getting to `PyGILState_Ensure()`, but no further.\r\n\r\nAnyway, I will try with `TF_CPP_MIN_VLOG_LEVEL=1`.", "I've set `TF_CPP_MIN_VLOG_LEVEL=1` and attached the resulting log from after the call to `TF_DeleteSession`. Note that this includes my added log line \"About to ensure GIL\" at the end (see my previous comment). And then the program hangs.\r\n\r\n```\r\n2018-07-31 17:22:11.229247: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229282: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229298: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229311: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229329: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229338: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229356: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229375: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229396: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229413: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229426: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229452: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229462: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229474: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229495: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229503: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229515: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229534: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229543: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229917: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229930: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229961: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229970: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229990: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.229998: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230006: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230014: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230022: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230030: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230066: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230074: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230093: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230101: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230109: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230138: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230155: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230219: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230336: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230346: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230354: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230362: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230370: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230384: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.230401: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.231726: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.231838: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n2018-07-31 17:22:11.232569: I tensorflow/core/common_runtime/function.cc:584] Graph Initial #nodes 5 #edges 6\r\n2018-07-31 17:22:11.232583: I tensorflow/core/common_runtime/function.cc:584] Graph Before #nodes 5 #edges 6\r\n2018-07-31 17:22:11.232599: I tensorflow/core/common_runtime/constant_folding.cc:571] No constant foldable nodes found\r\n2018-07-31 17:22:11.232644: I tensorflow/core/common_runtime/function.cc:584] Graph ReCopy #nodes 5 #edges 7\r\n2018-07-31 17:22:11.232673: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: _SOURCE = NoOp[]()\r\n2018-07-31 17:22:11.232701: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: _SINK = NoOp[]()\r\n2018-07-31 17:22:11.232724: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: arg0 = _Arg[T=DT_INT64, index=0]()\r\n2018-07-31 17:22:11.232763: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=\"pyfunc_2\"](arg0)\r\n2018-07-31 17:22:11.232802: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: pyfunc_RetVal = _Retval[T=DT_INT64, index=0](PyFunc)\r\n2018-07-31 17:22:11.232861: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 0 step -6231738800936566896 _SOURCE = NoOp[]() is dead: 0\r\n2018-07-31 17:22:11.232880: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 2 step -6231738800936566896 arg0 = _Arg[T=DT_INT64, index=0]() is dead: 0\r\n2018-07-31 17:22:11.232906: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -6231738800936566896 kernel_name: \"arg0\" tensor { dtype: DT_INT64 shape { } allocation_description { requested_bytes: 8 allocator_name: \"cpu\" } } }\r\n2018-07-31 17:22:11.232926: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 3 step -6231738800936566896 PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=\"pyfunc_2\"](arg0) is dead: 0\r\n2018-07-31 17:22:11.232938: E tensorflow/python/lib/core/py_func.cc:472] About to ensure GIL\r\n```", "It definitely looks like it's failing to acquire the lock :). Is there some way you could log the thread ID on which the PyFunc op is trying to acquire the GIL so we can associate it with the stack trace?\r\n\r\nAlso, the presence of `multiprocessing` is slightly suspicious, because depending on when things are forked the TensorFlow runtime might end up in an illegal state (essentially, the process isn't forkable once a `tf.Session` has been created). Is it possible to reproduce the problem in a process that doesn't use `multiprocessing`?", "I've logged the thread that is executing the PyFunc op. Here's the output:\r\n\r\n```\r\n2018-08-01 15:37:06.264162: E tensorflow/python/lib/core/py_func.cc:473] About to ensure GIL in thread: 0x700011a15000\r\n2018-08-01 15:37:06.264168: E tensorflow/python/lib/core/py_func.cc:476] Native thread ID is : 4923929\r\n```\r\n\r\nThe native thread ID is the one we're interested in (4923929). This corresponds to **0x4B2219** in hex.\r\n\r\nHere's what lldb tells us about our threads:\r\n\r\n```\r\n(lldb) thread info all\r\nthread #1: tid = 0x4b1ea2, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\r\n\r\nthread #5: tid = 0x4b1f91, 0x00000001018566da libsystem_kernel.dylib`__workq_kernreturn + 10\r\n\r\nthread #7: tid = 0x4b21c4, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n\r\nthread #8: tid = 0x4b21c6, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n\r\nthread #9: tid = 0x4b2214, 0x0000000101857592 libsystem_kernel.dylib`read + 10\r\n\r\nthread #10: tid = 0x4b2218, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n\r\nthread #12: tid = 0x4b232d, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n```\r\n\r\nHowever, 0x4B2219 is not there! However, if we look at our threads just before the session is destroyed we can see that this thread did _previously_ exist:\r\n\r\n`thread #11: tid = 0x4b2219, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10`\r\n\r\nAs might be expected, this is one of the Eigen threads:\r\n\r\n```\r\nthread #11\r\n    frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732\r\n    frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x0000000111b605f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278\r\n    frame #4: 0x0000000111b6026c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828\r\n    frame #5: 0x0000000111b5f898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568\r\n    frame #6: 0x0000000111b5f55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n```\r\n\r\nSo it looks like this thread has tried to get the GIL, hasn't managed, and then has stopped or been killed.\r\n\r\nIs it possible that the thread encounters an error when getting the GIL (which is not logged for some reason), and then is killed?\r\n\r\nRegarding the multiprocessing stuff, we are using Python's \"spawn\" multiprocessing context (`multiprocessing.get_context('spawn')`), so as I understand it the limitations around spawning processes after sessions have been created do not apply (but in any case we don't spawn any processes after session creation anyway). Having said that, both of the scenarios where this happens in our application _do_ use multiprocessing, so I'm not able to rule this out as a cause.\r\n\r\nI was also wondering why there are 4 Eigen threads (in fact there are 6 before session destruction time), so just to double-check our config I dumped it out. Here it is:\r\n\r\n```\r\nintra_op_parallelism_threads: 1\r\ninter_op_parallelism_threads: 1\r\ngpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n  allow_growth: true\r\n}\r\nallow_soft_placement: true\r\n\r\n```", "It looks like an error may be occurring, which is killing the thread. Stepping through the `PyGILState_Ensure` function in lldb, I can see the following happening:\r\n\r\n```\r\nProcess 71453 stopped\r\n* thread #11, stop reason = instruction step over\r\n    frame #0: 0x000000010087d794 Python`PyGILState_Ensure + 100\r\nPython`PyGILState_Ensure:\r\n->  0x10087d794 <+100>: movl   $0x0, 0x88(%rbx)\r\n    0x10087d79e <+110>: jmp    0x10087d75a               ; <+42>\r\n    0x10087d7a0 <+112>: leaq   0x8e1d1(%rip), %rdi       ; \"Couldn't create thread-state for new thread\"\r\n    0x10087d7a7 <+119>: callq  0x10087a510               ; Py_FatalError\r\n```\r\n\r\nThis _looks_ to me like the `Py_FatalError` function is being called with \"Couldn't create thread-state for new thread\". Soon after this happens the thread dies.\r\n\r\nThis seems to correspond to these lines from `PyGILState_Ensure` (looking at the code here: https://github.com/python/cpython/blob/e42b705188271da108de42b55d9344642170aa2b/Python/pystate.c):\r\n\r\n```\r\ntcur = PyThreadState_New(_PyRuntime.gilstate.autoInterpreterState);\r\n        if (tcur == NULL)\r\n            Py_FatalError(\"Couldn't create thread-state for new thread\");\r\n```\r\n\r\nBut it's not obvious to me why this would be failing.", "Is the process terminating when you see the hang? Looking the the `PyGILState_Ensure()` code, as far as I can tell the only situation in which we'd hit this path is if `malloc()` returned null. I can't think why that might be happening, but I'd be less surprised if we were in some rarely-hit exit path.", "Yes, the process is terminating when we see the hang. In fact we can see from the original thread dump that the destructor appears to be called as a consequence of a garbage collection which happens as part of shut-down:\r\n\r\n```\r\nframe #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180\r\n    frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121\r\n    frame #33: 0x000000010089b18a Python`collect + 1418\r\n    frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99\r\n    frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119\r\n    frame #36: 0x000000010087b0e0 Python`Py_Exit + 16\r\n    frame #37: 0x000000010087ef4c Python`handle_system_exit + 252\r\n    frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437\r\n```\r\n\r\nThe docs for `Py_FinalizeEx` say \"Undo all initializations made by Py_Initialize() and subsequent use of Python/C API functions\" so this could well be why `PyThreadState_New` subsequently fails. \r\n\r\nI think this is may be why my simplified test case does not reproduce the problem: it looks like the destructor is called as part of a \"regular\" GC in this test case.", "Adding a call to `gc.collect()` just before the process starts shutting-down seems to fix the problem for us, so it does look like \"normal\" garbage collections are OK, but when the Python VM is shutting-down calling `BaseSession`'s destructor is problematic.", "Aha, that makes sense. Can you try patching the fix in 8cd2d6fe9389e93a4182ae9287f2f8325913fe6c and see if that fixes the problem without having to call `gc.collect()`?", "Yes, that fixes the issue, and I get a log message saying:\r\n\r\n```\r\n2018-08-02 16:58:17.950407: W tensorflow/core/kernels/data/generator_dataset_op.cc:129] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=\"pyfunc_2\"](arg0)]]\r\n```\r\n\r\nThanks very much for your assistance on this.", "Thank *you* very much for digging into the details and providing such a useful report!"]}, {"number": 21276, "title": "Add WeightNorm wrapper layer", "body": "Adds a wrapper for Weight Normalization as requested in #14070 & #10125 .\r\nContains optional data dependent initialization for eager execution, and works on both keras.layers and tf.layers\r\n\r\nI struggled to figure out where to place this Wrapper, as no other layers in contrib appear to subclass anything from tf.Keras, but going forward I believe this is the direction TF is headed.  Please advise if it should go somewhere else (maybe a wrappers module in contrib?)\r\n\r\nCollab Example:\r\nhttps://colab.research.google.com/drive/1nBQSAA78oUBmi9fhnHJ_zWhHq2NXjwIc#scrollTo=au25bSP75hdr\r\n\r\nWrapped graph:\r\n![selection_002](https://user-images.githubusercontent.com/18154355/43461842-7794a942-94a2-11e8-9d28-21955273c317.png)", "comments": ["+1\r\n\r\nCan this be reviewed/merged?", "Nagging Assignee @case540: It has been 105 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "+1\r\n\r\nI'm already using this, it would be nice to see it in an official release. Thanks for all the work from contributors and reviewers.", "This will be moved to a PR for [tensorflow/addons](https://github.com/tensorflow/addons).\r\n\r\nClosing for now to stop the nagging, but I'll reference this issue within a couple of weeks.", "a weight normalization wrapper is available in https://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/wrappers.py"]}, {"number": 21275, "title": "Release RC to jcenter", "body": "I want to use the latest release candidates.\r\nHowever they are not published to Jcenter: https://bintray.com/google/tensorflow/tensorflow-lite", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi @PaulWoitaschek We will publish 1.10.0 to bintray.com when it becomes available. Meanwhile, there are two alternatives:\r\n  - use 0.0.0-nightly\r\n  - build the AAR from source, based on TensorFlow's r10 branch.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Okay thanks. I was hoping you could create a snapshot repo."]}, {"number": 21274, "title": "Using a tuple of Numpy arrays as validation_data fails when fitting a tf.keras model with a tf.data.Dataset", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.6\r\n- **CUDA/cuDNN version**:9.2\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n```\r\nmodel.fit(train_dataset,\r\n          steps_per_epoch=100,\r\n          validation_data=(val_samples, valid_targets),\r\n          epochs=10)\r\n```\r\n### Describe the problem\r\nUsecase: train a tf.keras.Model using the `fit` method. I for training data I have a `tf.data.Dataset` but for  `validation_data` I'd like to have a prefetched tuple `(x_val, y_val)` of Numpy arrays. At the end of the epoch it fails with the error `TypeError: float() argument must be a string or a number, not 'NoneType'. `\r\n\r\nIt seems it's trying to calculate `validation_steps` but it fails. Passing `validation_steps` will avoid the exception but it's unnecessary and i'm not sure what the `fit` model does with it.\r\n\r\n### Source code / logs\r\n\r\n```\r\n\r\n#### train Dataset\r\n\r\nwith tf.device('/cpu:0'):\r\n    dataset = tf.data.Dataset.from_tensor_slices(\r\n    (train_df.file.values, train_targets))\r\n    dataset = dataset.apply(\r\n        tf.contrib.data.shuffle_and_repeat(len(train_df)))\r\n    dataset = dataset.apply(tf.contrib.data.map_and_batch(\r\n    map_func=proces_audio, batch_size=batch_size,\r\n        num_parallel_calls=64))\r\n\r\n#### model definition\r\nx_logml = tf.keras.Input(shape=(timesteps,input_dim))\r\nx = get_conv_layers(x_logml)  # returns a stack of conv/batch_norm/max_pool layers\r\nx = tf.keras.layers.Dense(128, activation = 'relu')(x) \r\nx = tf.keras.layers.Dense(len(classes), activation = 'softmax')(x)\r\nmodel = tf.keras.Model(inputs = x_logml, outputs = x)\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \r\n              loss=['categorical_crossentropy'],\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(dataset,\r\n          steps_per_epoch=100,\r\n          validation_data=(val_samples,valid_targets),\r\n#               validation_steps=10,  #including this line will avoid the exception, but not sure what's the point\r\n         epochs=10)\r\n```\r\n\r\nas a smoke test, I used `val_samples` and `valid_targets` as training data (`x` and `y`) and it runs fine. So the problem seems to be when using a `tf.data.Dataset` and `(x_val, y_val)`. Using a `tf.data.Dataset` as `validation_data` (with `validation_steps`) also works fine.\r\n\r\n\r\n\r\nerror log at the end of epoch:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-22-b8d33816aaa6> in <module>()\r\n      9               validation_data=(val_samples,valid_targets),#val_dataset,\r\n     10 #               validation_steps=len(valid_df)//16,\r\n---> 11              epochs=100)\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1346           initial_epoch=initial_epoch,\r\n   1347           steps_per_epoch=steps_per_epoch,\r\n-> 1348           validation_steps=validation_steps)\r\n   1349 \r\n   1350   def evaluate(self,\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    218             batch_size=batch_size,\r\n    219             steps=validation_steps,\r\n--> 220             verbose=0)\r\n    221         if not isinstance(val_outs, list):\r\n    222           val_outs = [val_outs]\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in test_loop(model, inputs, targets, sample_weights, batch_size, verbose, steps)\r\n    459         outs[i] /= steps\r\n    460   else:\r\n--> 461     batches = make_batches(num_samples, batch_size)\r\n    462     index_array = np.arange(num_samples)\r\n    463     for batch_index, (batch_start, batch_end) in enumerate(batches):\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in make_batches(size, batch_size)\r\n    465       A list of tuples of array indices.\r\n    466   \"\"\"\r\n--> 467   num_batches = int(np.ceil(size / float(batch_size)))\r\n    468   return [(i * batch_size, min(size, (i + 1) * batch_size))\r\n    469           for i in range(0, num_batches)]\r\n\r\nTypeError: float() argument must be a string or a number, not 'NoneType'\r\n```", "comments": ["You are getting this error because the batch_size of validation_data is None. Since you are passing a dataset for training you don't need to specify a batch_size for that. As you don't specify the batch_size and pass arrays as input for validation data, it fails. \r\n\r\nI would suggest using tf.data.Dataset for both training and validation or as you did by passing validation steps as a parameter. I hope this will unblock you.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@yashk2810 thanks, it feels like a broken experience though specially since the docstring for `fit` says:\r\n```\r\n            batch_size: Integer or `None`.\r\n                Number of samples per gradient update.\r\n                If unspecified, `batch_size` will default to 32.\r\n```\r\nfollowing your suggestion, is there a right value to set for `validation_steps` or is everything worked out based on `validation_steps = val_set_size / batch_size` relationship?\r\n", "@olix20  The docsting also specifies that if you use tf.data dataset, then the batch_size parameter won't be taken into consideration. \r\n\r\n`batch_size: Integer or `None`.\r\n            Number of samples per gradient update.\r\n            If unspecified, `batch_size` will default to 32.\r\n            Do not specify the `batch_size` if your data is in the\r\n            form of symbolic tensors, datasets, or dataset iterators\r\n            (since they generate batches).\r\n`\r\n\r\nIf you are creating a tf.data dataset for training, a tf.data dataset for testing and validation makes the most sense(it is only 1-2 lines of code), as everything is consistent and makes it easier to understand the flow. If you are using validation steps then it will be calculated according to the formula you mentioned above. "]}, {"number": 21273, "title": "error occurring while running my motion detectioncode on jupyter notebook", "body": "usage: ipykernel_launcher.py [-h] [-v VIDEO] [-a MIN_AREA]\r\nipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\HP\\AppData\\Roaming\\jupyter\\runtime\\kernel-fb666fe6-3260-4858-b7c8-b37fb9759c42.json\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 2", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21272, "title": "bad performance when build with jemalloc", "body": "I try to build tf r1.8 with jemalloc.\r\nI test the perfermance of r1.8 with jemalloc, found that the model use more sys use. the result as follow:\r\n\r\n```\r\ntf r1.8 with jemalloc:\r\n    virturl memory: 29581 MB,real memory: 16185 MB, user cpu: 751.623, sys cpu: 145.979\r\ntf r1.8 without  jemalloc:\r\n    virturl memory: 24491 MB,real memory: 14520 MB, user cpu: 748.362, sys cpu: 18.4359\r\n```\r\nI tested several models and found that they all have this phenomenon.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@gunan do you know the state of the jemalloc build? Does anyone maintain it?", "@jhseu may know.\r\nI thought we moved to tcmalloc, which is nowadays faster than jemalloc?", "It's actually known that there are situations where jemalloc performs worse, but on average it performs better.\r\n\r\nSwitching to tcmalloc is blocked at the moment.", "@gunan @jhseu I test tcmalloc\u3001jemalloc\u3001ptmalloc performance with 60 threads,found that when alloc small memory,tcmalloc is a lillter beter than jemalloc, ptmalloc  has the worst performance.but when alloc \r\nlarge memory, ptmalloc is the best.  the reason may be that when alloc small memory,jemalloc and tcmalloc \r\nuse thread cache memory. Can we use tcmalloc when applying for small memory, and use a self define memory pool when applying for large memory?\r\n\r\nthe performance result as follow:\r\n```\r\nwhen alloc memmory <= 32 KB  :  tcmalloc > jemalloc > ptmalloc\r\nwhen alloc 32KB < memmory <= 256 KB  :  tcmalloc > ptmalloc > jemalloc\r\nwhen alloc > 256 KB :  ptmalloc > jemalloc > tcmalloc\r\n```\r\n\r\nthe code as follow:\r\n```\r\n#include <iostream>\r\n#include <ctime>\r\n#include <cstdlib>\r\n#include <thread>\r\n\r\nvoid malloc_test(int time, int size) {\r\n    char *p = NULL;\r\n    for (int i = 0; i < time; i++) {\r\n        p = (char*)malloc(size);\r\n        free(p);\r\n    }\r\n}\r\n\r\nint main(int argc, char *argv[]) {\r\n    int num_thread = atoi(argv[1]);\r\n    int time = atoi(argv[2]);\r\n    int size = atoi(argv[3]);\r\n    std::cout << \"num_thread is \" << num_thread << \" ,malloc time is \" << time << \" ,malloc size is \" << size << std::endl;\r\n    clock_t start_time = clock();\r\n    std::thread threads[num_thread];\r\n    for (int i = 0; i < num_thread; i++) {\r\n        threads[i] = std::thread(malloc_test, time, size);\r\n    }\r\n\r\n    for(auto& t: threads) {\r\n        t.join();\r\n    }\r\n\r\n    clock_t finish_time = clock();\r\n\r\n    std::cout << \"total time (s) is  \" << (finish_time - start_time) / CLOCKS_PER_SEC << std::endl;\r\n    return 0;\r\n}\r\n```", "Nagging Assignee @skye: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21271, "title": "tf.nn.softmax_cross_entropy_with_logits_v2 returns wrong value with soft labels", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Linux CentOS 7.4\r\n- **Mobile device if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**:  GeForce 940MX, 4GB\r\n- **Exact command to reproduce**: save the codes below as a .py file, and run it with command-line something like `python3 test.py`.\r\n\r\n### Describe the problem\r\nI use soft labels (for example, [0.2, 0.8] instead of [0, 1]) in a CNN model, in which I use `tf.nn.softmax_cross_entropy_with_logits_v2` for loss computing. But when I trained the model, the loss became +inf in 10 steps, so I debugged the codes and found that the problem was caused by `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\nSo I implemented the softmax and cross_entropy process separately, then the returned value seemed to make sense. \r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ntf.executing_eagerly()\r\n\r\nlogits = [[-4885.4614, 4878.027], [-5188.321, 5179.7915], [-4121.558, 4114.9995], [-5165.612, 5157.5044], [-4152.7183, 4145.978], [-5175.428, 5167.603], [-4514.224, 4506.477], [-4752.5854, 4745.2524], [-5580.9463, 5572.2275], [-5164.766, 5156.6685], [-4273.686, 4266.31], [-4886.724, 4879.5757], [-5216.2935, 5208.269], [-5411.082, 5402.344], [-6057.239, 6048.3647], [-5314.882, 5306.708], [-5674.2505, 5664.6436], [-5650.7827, 5642.1997], [-4301.4194, 4294.5957], [-5156.4683, 5148.283], [-5032.6797, 5024.821], [-5072.1533, 5064.013], [-4129.488, 4123.4355], [-4915.1147, 4907.8643], [-5256.5747, 5248.351], [-5297.694, 5289.386], [-4979.939, 4971.691], [-4895.983, 4887.4897], [-4757.732, 4749.2886], [-4871.5654, 4863.604], [-4772.0356, 4763.9414], [-4528.853, 4521.4424],]\r\nlabels = [[9.6226019e-01, 3.7739828e-02], [9.5367432e-07, 9.9999905e-01], [9.9017835e-01, 9.8216468e-03], [8.3446503e-07, 9.9999917e-01], [9.9999952e-01, 4.9012118e-07], [3.6466300e-02, 9.6353370e-01], [3.2732385e-01, 6.7267615e-01], [2.1918458e-01, 7.8081542e-01], [3.4707606e-02, 9.6529239e-01], [1.3036132e-03, 9.9869639e-01], [4.1835755e-01, 5.8164245e-01], [5.0599152e-01, 4.9400848e-01], [9.9629784e-01, 3.7021455e-03], [1.9747615e-03, 9.9802524e-01], [8.2850456e-06, 9.9999171e-01], [8.1463850e-01, 1.8536153e-01], [7.6112747e-03, 9.9238873e-01], [9.4729370e-01, 5.2706327e-02], [9.4496381e-01, 5.5036198e-02], [3.3008814e-02, 9.6699119e-01], [1.0292470e-02, 9.8970753e-01], [9.9998099e-01, 1.9039073e-05], [7.5116873e-01, 2.4883127e-01], [9.9973243e-01, 2.6756502e-04], [3.1858683e-04, 9.9968141e-01], [3.6358833e-05, 9.9996364e-01], [9.3631679e-01, 6.3683234e-02], [7.9292524e-01, 2.0707479e-01], [9.9999642e-01, 3.6055078e-06], [4.9336654e-01, 5.0663346e-01], [6.9030523e-03, 9.9309695e-01], [9.9974275e-01, 2.5725813e-04],]\r\n\r\nlogits = np.array(logits, dtype=np.float32)\r\nlabels = np.array(labels, dtype=np.float32)\r\n\r\n# Check each row of labels is a valid probability distribution.\r\nlabels_sum = tf.reduce_sum(labels, axis=-1)\r\nprint(labels_sum)\r\n\r\n# Cross entropy computed by tf.nn.softmax_cross_entropy_with_logits_v2\r\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\r\nprint(cross_entropy_mean)\r\n\r\n# Cross entropy computed separately\r\nsoftmax = tf.nn.softmax(logits)\r\ncross_entropy_mean = tf.reduce_mean(-tf.reduce_sum(softmax * tf.log(labels), axis=-1))\r\nprint(cross_entropy_mean)\r\n```\r\n\r\n**Output:**\r\n```\r\ntf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(32,), dtype=float32)\r\ntf.Tensor(4538.922, shape=(), dtype=float32)\r\ntf.Tensor(2.621081, shape=(), dtype=float32)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce\nMobile device", "@tensorflowbutler OK, updated.", "My apologies, the cross entropy formula I used above is wrong.\r\nAfter I correct the error, it returns inf, I think this problem is already solved by `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\nSo I read some article which describe the implementation ([Streaming Log-sum-exp Computation](http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html)), and implement it.\r\nThen the result is the same as `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\nSo the problem is from me, not the TensorFlow code.\r\n\r\n\r\n**Source code:**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ntf.executing_eagerly()\r\n\r\nlogits = [[-4885.4614, 4878.027], [-5188.321, 5179.7915], [-4121.558, 4114.9995], [-5165.612, 5157.5044], [-4152.7183, 4145.978], [-5175.428, 5167.603], [-4514.224, 4506.477], [-4752.5854, 4745.2524], [-5580.9463, 5572.2275], [-5164.766, 5156.6685], [-4273.686, 4266.31], [-4886.724, 4879.5757], [-5216.2935, 5208.269], [-5411.082, 5402.344], [-6057.239, 6048.3647], [-5314.882, 5306.708], [-5674.2505, 5664.6436], [-5650.7827, 5642.1997], [-4301.4194, 4294.5957], [-5156.4683, 5148.283], [-5032.6797, 5024.821], [-5072.1533, 5064.013], [-4129.488, 4123.4355], [-4915.1147, 4907.8643], [-5256.5747, 5248.351], [-5297.694, 5289.386], [-4979.939, 4971.691], [-4895.983, 4887.4897], [-4757.732, 4749.2886], [-4871.5654, 4863.604], [-4772.0356, 4763.9414], [-4528.853, 4521.4424],]\r\nlabels = [[9.6226019e-01, 3.7739828e-02], [9.5367432e-07, 9.9999905e-01], [9.9017835e-01, 9.8216468e-03], [8.3446503e-07, 9.9999917e-01], [9.9999952e-01, 4.9012118e-07], [3.6466300e-02, 9.6353370e-01], [3.2732385e-01, 6.7267615e-01], [2.1918458e-01, 7.8081542e-01], [3.4707606e-02, 9.6529239e-01], [1.3036132e-03, 9.9869639e-01], [4.1835755e-01, 5.8164245e-01], [5.0599152e-01, 4.9400848e-01], [9.9629784e-01, 3.7021455e-03], [1.9747615e-03, 9.9802524e-01], [8.2850456e-06, 9.9999171e-01], [8.1463850e-01, 1.8536153e-01], [7.6112747e-03, 9.9238873e-01], [9.4729370e-01, 5.2706327e-02], [9.4496381e-01, 5.5036198e-02], [3.3008814e-02, 9.6699119e-01], [1.0292470e-02, 9.8970753e-01], [9.9998099e-01, 1.9039073e-05], [7.5116873e-01, 2.4883127e-01], [9.9973243e-01, 2.6756502e-04], [3.1858683e-04, 9.9968141e-01], [3.6358833e-05, 9.9996364e-01], [9.3631679e-01, 6.3683234e-02], [7.9292524e-01, 2.0707479e-01], [9.9999642e-01, 3.6055078e-06], [4.9336654e-01, 5.0663346e-01], [6.9030523e-03, 9.9309695e-01], [9.9974275e-01, 2.5725813e-04],]\r\n\r\nlogits = np.array(logits, dtype=np.float32)\r\nlabels = np.array(labels, dtype=np.float32)\r\n\r\n# Check each row of labels is a valid probability distribution.\r\nlabels_sum = tf.reduce_sum(labels, axis=-1)\r\nprint(labels_sum)\r\n\r\n# Cross entropy computed by tf.nn.softmax_cross_entropy_with_logits_v2\r\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\r\nprint(cross_entropy_mean)\r\n\r\n# Cross entropy computed separately\r\nsoftmax = tf.nn.softmax(logits)\r\ncross_entropy_mean = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(softmax), axis=-1))\r\nprint(cross_entropy_mean)\r\n\r\n# Cross entropy computed with batched log-sum-exp method\r\nalpha = tf.reduce_max(logits, axis=-1, keepdims=True)\r\nlog_sum_exp = tf.log(tf.reduce_sum(tf.exp(logits - alpha), axis=-1, keepdims=True)) + alpha\r\ncross_entropy = -tf.reduce_sum((logits - log_sum_exp) * labels, axis=-1)\r\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\r\nprint(cross_entropy_mean)\r\n```\r\n\r\n**Output:**\r\n```\r\ntf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(32,), dtype=float32)\r\ntf.Tensor(4538.922, shape=(), dtype=float32)\r\ntf.Tensor(inf, shape=(), dtype=float32)\r\ntf.Tensor(4538.922, shape=(), dtype=float32)\r\n```"]}, {"number": 21270, "title": "Add the optional param to base64 encode estimator inputs", "body": "This may resolved https://github.com/tensorflow/tensorflow/issues/21234 .\r\n\r\nUsing base64 to encode TensorFlow serialized example byte arrays makes it possible to serve the estimator models with RESTful servers.", "comments": ["It is well tested with the following script and `TensorFlow Serving`.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n\r\ndef input_fn():\r\n  features = {'a': tf.constant([[\"1\"], [\"2\"]]), 'b': tf.constant([[3], [4]])}\r\n  labels = tf.constant([0, 1])\r\n  return features, labels\r\n\r\nfeature_a = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n    \"a\", hash_bucket_size=1000)\r\nfeature_b = tf.contrib.layers.real_valued_column(\"b\")\r\nfeature_columns = [feature_a, feature_b]\r\nmodel = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns)\r\nmodel.fit(input_fn=input_fn, steps=10)\r\nfeature_spec = tf.contrib.layers.create_feature_spec_for_parsing(\r\n    feature_columns)\r\n\r\n#serving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\r\nserving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec, base64_encode_example=True)\r\nsavedmodel_path = \"./savedmodel\"\r\nmodel.export_savedmodel(savedmodel_path, serving_input_fn)\r\n```", "Can you help to review this? @karmel ", "Thanks @tobegit3hub . In the original issue, I did not realize you were referring to the contrib.learn utility. As that code is deprecated, we do not make ongoing changes. Have you tried with `tf.estimator.export.build_parsing_serving_input_receiver_fn`? Does that util have the same problem?", "Thanks @karmel for reminding. I have check `tf.estimator.export.build_parsing_serving_input_receiver_fn` and it has the same problem. Refer to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/export/export.py#L267 .\r\n\r\nSince `tensorflow.contrib.learn.python.learn.utils.input_fn_utils.build_parsing_serving_input_fn` is deprecated, I would like to change the PR to fix the code in `tf.estimator.export. build_parsing_serving_input_receiver_fn ` as well.\r\n\r\nPlease help to review with the new commit.", "Just wanted to cross-list my comment here: https://github.com/tensorflow/tensorflow/issues/21234#issuecomment-409650072 , @tobegit3hub , to make sure you saw it--", "Close since we can use the `b64` API in `TensorFlow Serving`."]}, {"number": 21269, "title": "Compiling TF for Ubuntu: Failed", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nChecked out tag origin/r1.9\r\n\r\n- **TensorFlow version (use command below)**:\r\nr1.9.0\r\n\r\n- **Python version**:\r\n3.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.15.2\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 17 12:22:40 2018 (1531830160)\r\nBuild timestamp: 1531830160\r\nBuild timestamp as int: 1531830160\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) \r\n\r\n- **CUDA/cuDNN version**:\r\nn/a; compiling for non-GPU machine\r\n\r\n- **GPU model and memory**:\r\nn/a\r\n\r\n- **Exact command to reproduce**:\r\n`bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\n```\r\n(tensorflow_p36) ubuntu:/efs/tensorflow/tools$ bash tf_env_collect.sh \r\nCollecting system information...\r\n/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\r\n  (fname, cnt))\r\n/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\r\n  (fname, cnt))\r\n2018-07-31 10:32:11.836897: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-07-31 10:32:12.013176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-31 10:32:12.013518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-07-31 10:32:12.013553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nWrote environment to tf_env.txt. You can review the contents of that file.\r\nand use it to populate the fields in the github issue template.\r\n\r\ncat tf_env.txt\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHere is the error message.  It doesnt say much:\r\n```\r\nexternal/curl/lib/vtls/openssl.c: At top level:\r\ncc1: warning: unrecognized command line option \"-Wno-string-plus-int\" [enabled by default]\r\nINFO: Elapsed time: 450.149s, Critical Path: 21.18s\r\nINFO: 1372 processes: 1372 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nHere are the 100 lines that precede it:\r\n```\r\nIn file included from external/curl/lib/vtls/openssl.c:72:0:\r\n/home/ubuntu/anaconda3/envs/tensorflow_p36/bin/../lib/gcc/../../include/openssl/ocsp.h:567:34: error: unknown type name 'OCSP_RESPONSE'\r\n int OCSP_RESPONSE_print(BIO *bp, OCSP_RESPONSE *o, unsigned long flags);\r\n                                  ^\r\nexternal/curl/lib/vtls/openssl.c: In function 'verifystatus':\r\nexternal/curl/lib/vtls/openssl.c:1262:3: error: unknown type name 'OCSP_RESPONSE'\r\n   OCSP_RESPONSE *rsp = NULL;\r\n   ^\r\nexternal/curl/lib/vtls/openssl.c:1267:3: warning: implicit declaration of function 'SSL_get_tlsext_status_ocsp_resp' [-Wimplicit-function-declaration]\r\n   long len = SSL_get_tlsext_status_ocsp_resp(connssl->handle, &p);\r\n   ^\r\nexternal/curl/lib/vtls/openssl.c:1275:3: warning: implicit declaration of function 'd2i_OCSP_RESPONSE' [-Wimplicit-function-declaration]\r\n   rsp = d2i_OCSP_RESPONSE(NULL, &p, len);\r\n   ^\r\nexternal/curl/lib/vtls/openssl.c:1275:7: warning: assignment makes pointer from integer without a cast [enabled by default]\r\n   rsp = d2i_OCSP_RESPONSE(NULL, &p, len);\r\n       ^\r\nexternal/curl/lib/vtls/openssl.c:1282:3: warning: implicit declaration of function 'OCSP_response_status' [-Wimplicit-function-declaration]\r\n   ocsp_status = OCSP_response_status(rsp);\r\n   ^\r\nexternal/curl/lib/vtls/openssl.c:1290:3: warning: implicit declaration of function 'OCSP_response_get1_basic' [-Wimplicit-function-declaration]\r\n   br = OCSP_response_get1_basic(rsp);\r\n   ^\r\nexternal/curl/lib/vtls/openssl.c:1290:6: warning: assignment makes pointer from integer without a cast [enabled by default]\r\n   br = OCSP_response_get1_basic(rsp);\r\n      ^\r\nexternal/curl/lib/vtls/openssl.c:1374:3: warning: implicit declaration of function 'OCSP_RESPONSE_free' [-Wimplicit-function-declaration]\r\n   OCSP_RESPONSE_free(rsp);\r\n   ^\r\nexternal/curl/lib/vtls/openssl.c: In function 'ossl_connect_step1':\r\nexternal/curl/lib/vtls/openssl.c:2061:5: warning: implicit declaration of function 'SSL_set_tlsext_status_type' [-Wimplicit-function-declaration]\r\n     SSL_set_tlsext_status_type(connssl->handle, TLSEXT_STATUSTYPE_ocsp);\r\n     ^\r\nexternal/curl/lib/vtls/openssl.c: In function 'get_cert_chain':\r\nexternal/curl/lib/vtls/openssl.c:2390:9: warning: passing argument 1 of 'X509_get0_signature' from incompatible pointer type [enabled by default]\r\n         X509_get0_signature(&psig, &palg, x);\r\n         ^\r\nIn file included from external/boringssl/src/include/openssl/pem.h:66:0,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/x509.h:761:21: note: expected 'const struct ASN1_BIT_STRING **' but argument is of type 'struct ASN1_BIT_STRING **'\r\n OPENSSL_EXPORT void X509_get0_signature(const ASN1_BIT_STRING **psig,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2390:9: warning: passing argument 2 of 'X509_get0_signature' from incompatible pointer type [enabled by default]\r\n         X509_get0_signature(&psig, &palg, x);\r\n         ^\r\nIn file included from external/boringssl/src/include/openssl/pem.h:66:0,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/x509.h:761:21: note: expected 'const struct X509_ALGOR **' but argument is of type 'struct X509_ALGOR **'\r\n OPENSSL_EXPORT void X509_get0_signature(const ASN1_BIT_STRING **psig,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2455:11: warning: passing argument 2 of 'RSA_get0_key' from incompatible pointer type [enabled by default]\r\n           RSA_get0_key(rsa, &n, &e, &d);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:95:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_key(const RSA *rsa, const BIGNUM **out_n,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2455:11: warning: passing argument 3 of 'RSA_get0_key' from incompatible pointer type [enabled by default]\r\n           RSA_get0_key(rsa, &n, &e, &d);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:95:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_key(const RSA *rsa, const BIGNUM **out_n,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2455:11: warning: passing argument 4 of 'RSA_get0_key' from incompatible pointer type [enabled by default]\r\n           RSA_get0_key(rsa, &n, &e, &d);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:95:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_key(const RSA *rsa, const BIGNUM **out_n,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2456:11: warning: passing argument 2 of 'RSA_get0_factors' from incompatible pointer type [enabled by default]\r\n           RSA_get0_factors(rsa, &p, &q);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:100:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_factors(const RSA *rsa, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2456:11: warning: passing argument 3 of 'RSA_get0_factors' from incompatible pointer type [enabled by default]\r\n           RSA_get0_factors(rsa, &p, &q);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:100:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_factors(const RSA *rsa, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2457:11: warning: passing argument 2 of 'RSA_get0_crt_params' from incompatible pointer type [enabled by default]\r\n           RSA_get0_crt_params(rsa, &dmp1, &dmq1, &iqmp);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:107:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_crt_params(const RSA *rsa, const BIGNUM **out_dmp1,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2457:11: warning: passing argument 3 of 'RSA_get0_crt_params' from incompatible pointer type [enabled by default]\r\n           RSA_get0_crt_params(rsa, &dmp1, &dmq1, &iqmp);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:107:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_crt_params(const RSA *rsa, const BIGNUM **out_dmp1,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2457:11: warning: passing argument 4 of 'RSA_get0_crt_params' from incompatible pointer type [enabled by default]\r\n           RSA_get0_crt_params(rsa, &dmp1, &dmq1, &iqmp);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:82:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/rsa.h:107:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void RSA_get0_crt_params(const RSA *rsa, const BIGNUM **out_dmp1,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2500:11: warning: passing argument 2 of 'DSA_get0_pqg' from incompatible pointer type [enabled by default]\r\n           DSA_get0_pqg(dsa, &p, &q, &g);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:74:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dsa.h:101:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DSA_get0_pqg(const DSA *dsa, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2500:11: warning: passing argument 3 of 'DSA_get0_pqg' from incompatible pointer type [enabled by default]\r\n           DSA_get0_pqg(dsa, &p, &q, &g);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:74:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dsa.h:101:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DSA_get0_pqg(const DSA *dsa, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2500:11: warning: passing argument 4 of 'DSA_get0_pqg' from incompatible pointer type [enabled by default]\r\n           DSA_get0_pqg(dsa, &p, &q, &g);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:74:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dsa.h:101:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DSA_get0_pqg(const DSA *dsa, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2501:11: warning: passing argument 2 of 'DSA_get0_key' from incompatible pointer type [enabled by default]\r\n           DSA_get0_key(dsa, &pub_key, &priv_key);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:74:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dsa.h:96:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DSA_get0_key(const DSA *dsa, const BIGNUM **out_pub_key,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2501:11: warning: passing argument 3 of 'DSA_get0_key' from incompatible pointer type [enabled by default]\r\n           DSA_get0_key(dsa, &pub_key, &priv_key);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:74:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dsa.h:96:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DSA_get0_key(const DSA *dsa, const BIGNUM **out_pub_key,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2533:11: warning: passing argument 2 of 'DH_get0_pqg' from incompatible pointer type [enabled by default]\r\n           DH_get0_pqg(dh, &p, &q, &g);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:73:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dh.h:102:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DH_get0_pqg(const DH *dh, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2533:11: warning: passing argument 3 of 'DH_get0_pqg' from incompatible pointer type [enabled by default]\r\n           DH_get0_pqg(dh, &p, &q, &g);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:73:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dh.h:102:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DH_get0_pqg(const DH *dh, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2533:11: warning: passing argument 4 of 'DH_get0_pqg' from incompatible pointer type [enabled by default]\r\n           DH_get0_pqg(dh, &p, &q, &g);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:73:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dh.h:102:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DH_get0_pqg(const DH *dh, const BIGNUM **out_p,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2534:11: warning: passing argument 2 of 'DH_get0_key' from incompatible pointer type [enabled by default]\r\n           DH_get0_key(dh, &pub_key, &priv_key);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:73:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dh.h:92:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DH_get0_key(const DH *dh, const BIGNUM **out_pub_key,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c:2534:11: warning: passing argument 3 of 'DH_get0_key' from incompatible pointer type [enabled by default]\r\n           DH_get0_key(dh, &pub_key, &priv_key);\r\n           ^\r\nIn file included from external/boringssl/src/include/openssl/x509.h:73:0,\r\n                 from external/boringssl/src/include/openssl/pem.h:66,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/curl/lib/urldata.h:86,\r\n                 from external/curl/lib/vtls/openssl.c:41:\r\nexternal/boringssl/src/include/openssl/dh.h:92:21: note: expected 'const struct BIGNUM **' but argument is of type 'struct BIGNUM **'\r\n OPENSSL_EXPORT void DH_get0_key(const DH *dh, const BIGNUM **out_pub_key,\r\n                     ^\r\nexternal/curl/lib/vtls/openssl.c: At top level:\r\ncc1: warning: unrecognized command line option \"-Wno-string-plus-int\" [enabled by default]\r\nINFO: Elapsed time: 450.149s, Critical Path: 21.18s\r\nINFO: 1372 processes: 1372 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["```\r\ninclude/openssl/ocsp.h:567:34: error: unknown type name 'OCSP_RESPONSE'\r\n```\r\nThis is a duplicate of this issue:\r\nhttps://github.com/tensorflow/serving/issues/626\r\n\r\nIt is related to anaconda's openssl. Please see the workaround on the other issue."]}]