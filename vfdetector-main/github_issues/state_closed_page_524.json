[{"number": 38014, "title": "ImportError: DLL load failed: The specified module could not be found.   Failed to load the native TensorFlow runtime.", "body": "\r\n- windows 10 home \r\n- TensorFlow installed from source :\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.2\r\n- Installed using  conda:\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda9.0/cudnn-9.0\r\n- GPU model and memory: Nvidea 1050 ti\r\n\r\n\r\n\r\n**Describe the problem**\r\ncant import tensorflow after its installation \r\nimport tensorflow as tf\r\nand give the following error\r\n\r\n\r\n**Any other info / logs**\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"D:\\programms\\teleuniv ques\\untitled0.py\", line 1, in <module>\r\n    from keras.preprocessing import image\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "not rectified its already done\r\n", "@sathwikreddy56,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) comment from a similar issue and let us know if it works? Thanks!", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38014\">No</a>\n"]}, {"number": 38013, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Traceback (most recent call last):\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"D:\\programms\\teleuniv ques\\untitled0.py\", line 1, in <module>\r\n    from keras.preprocessing import image\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sathw\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"G:\\anaconda\\envs\\machinelearning\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@sathwikreddy56, Please provide the tensorflow version. From the error trace, it looks like you are trying to install Tensorflow-gpu version, if yes, Please provide the details about the CUDA and cuDNN version. Thanks!", "10.1.263 (cuda)\n7.6 (cudnn)\n\nOn Mon, Mar 30, 2020 at 11:15 AM gadagashwini <notifications@github.com>\nwrote:\n\n> @sathwikreddy56 <https://github.com/sathwikreddy56>, Please provide the\n> tensorflow version. From the error trace, it looks like you are trying to\n> install Tensorflow-gpu version, if yes, Please provide the details about\n> the CUDA and cuDNN version. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38013#issuecomment-605794276>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKTWHN6KD5FPCDZM77ECPZDRKAWWHANCNFSM4LV3VVXQ>\n> .\n>\n", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204"]}, {"number": 38012, "title": "module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n\r\nthe function populate_dict_with_module_objects() is missed in the 2.2.0rc2\r\n\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@flydragon2018 \r\nplease update the template for us to help you resolve the issue, we will not be able to help with out the version, simple stand alone code before which the error was faced.", "on basis of error shared, please refer to [this link](https://github.com/tensorflow/probability/issues/255) and let us know if it helps", "the code is simply not there compared with the code on github.\r\nI means that the codes installed by \"  pip install tensorflow==2.2.0rc2\"", "@flydragon2018 \r\n \r\ncould you please share exact sequence of steps before this issue occurred", "@flydragon2018\r\n\r\ncould you please update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38012\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38012\">No</a>\n", "I have the same problem:\r\n\r\nAttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'\r\n\r\nAny fix for this error?", "I am also experiencing this issue. I am still very new to tf. I was running throught some tutorials. VS Code stops at the 'import tensorflow' line and returns the error:\r\n\r\nmodule 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'\r\n\r\nThis exact script was working BEFORE I installed CUDA drivers, but sent a warning saying it could not find the 'cuart64_101.dll'. After installing the drivers, it states that the dll was found, then immediately errors at the import. \r\n\r\nAll software is the most recent versions:\r\n\r\ntensorflow 2.2.0rc4\r\ncuda 10.2.89\r\n", "I am also experiencing the same error when importing tensorflow after installing tensorflow 2.2.0rc4.\r\n` AttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'`", "I have the same problem with tensorflow 2.2.0rc4\r\ncuda 10.2.89\r\ncudnn-10.2-linux-x64-v7.6.5.32\r\nTensorRT-7.0.0.11\r\nUbuntu 1804LTS\r\n", "It worked for me after uninstalling the `tf-nightly` packages.\r\n\r\n```bash\r\npip list | grep tf\r\n```\r\n\r\nThen reinstall tensorflow\r\n```bash\r\npip install tensorflow --upgrade --force-reinstall\r\n```\r\n", "> \r\n> \r\n> It worked for me after uninstalling the `tf-nightly` packages.\r\n> \r\n> ```shell\r\n> pip list | grep tf\r\n> ```\r\n> \r\n> Then reinstall tensorflow\r\n> \r\n> ```shell\r\n> pip install tensorflow --upgrade --force-reinstall\r\n> ```\r\n\r\nThis worked for me!! Thank you! Not sure what tf-nightly is or if I will want it later, but for now 'weeee'", "I have the same issue since tried the NMT which is required ```ft_nightly```, so it mays conflict with tf. First uninstall ```tf_nightly``` and then force reinstall tf.\r\n```\r\npip uninstall tf-nightly\r\npip install tensorflow --upgrade --force-reinstall\r\n```", "I'm having this issue and uninstalling tf-nightly\r\nand forcing reinstall of tensorflow is not helping.\r\n\r\nAny other ideas please folks?\r\n\r\nEdit: \r\nThere may be more than one \"tf-nightly\" module.\r\nAfter this step:  \r\n\r\n`pip list | grep tf`\r\n\r\ncheck the list, on my machine there was a module named tf-nightly, and also one called tf-estimate-nightly\r\nafter pip uninstalling both modules, the method mentioned above worked", "I have tried to force upgrade **tensorflow 2.2.0** and I am landing in same issue. \r\n**Keras version 2.3.1** \r\n\r\nAny help here is appreciated to help fix this issue.\r\n \r\n\r\nTraceback (most recent call last):\r\n  File \"image_classification.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 84, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/models.py\", line 24, in <module>\r\n    from tensorflow.python.keras import metrics as metrics_module\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\", line 37, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 51, in <module>\r\n    from tensorflow.python.keras import initializers\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/__init__.py\", line 127, in <module>\r\n    populate_deserializable_objects()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/__init__.py\", line 85, in populate_deserializable_objects\r\n    generic_utils.populate_dict_with_module_objects(\r\nAttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'\r\n", "Please copy \"populate_dict_with_module_objects\" function from this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py) (line 827 to 832) and add it to \"generic_utils.py\"", "I am having the same problem.\r\n\r\nIt worked fine, then I downloaded CUDA, CuDNN and tensorflow gpu and now i get this error.\r\nI do not have tf-nightly installed.\r\n\r\n", "The solution above from @danial880 worked for me", "@danial880 it worked on Tensorflow 2.3.1.", "> Please copy \"populate_dict_with_module_objects\" function from this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py) (line 827 to 832) and add it to \"generic_utils.py\"\r\n\r\nThe solution by @danial880 worked for me on Tensorflow 2.3.1", "Still an issue: https://github.com/keras-team/keras/issues/14632", "You simply need to reinstall tensorflow as shown by @lequan above. Works for me.", "> Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\ngo fuck yourself ", "@danial880 it worked for me. Tensorflow 2.5.0. actually after I copied and pasted \"populate_dict_with_module_objects\" function, another error message popped up which said I need \"to_snake_case\" function. So, I copied and pasted \"to_snake_case\" from the same page.\r\nThen another error popped up and said it needs \"re\" package. So, I wrote the code \"import re\" at the top of \"generic_utils.py.\" And finally, it works.\r\nPython version 3.7.7, tensorflow version 2.5.0", "> @danial880 it worked for me. Tensorflow 2.5.0. actually after I copied and pasted \"populate_dict_with_module_objects\" function, another error message popped up which said I need \"to_snake_case\" function. So, I copied and pasted \"to_snake_case\" from the same page.\r\n> Then another error popped up and said it needs \"re\" package. So, I wrote the code \"import re\" at the top of \"generic_utils.py.\" And finally, it works.\r\n> Python version 3.7.7, tensorflow version 2.5.0\r\n\r\nThat worked for me. Thanks", "> @danial880 it worked for me. Tensorflow 2.5.0. actually after I copied and pasted \"populate_dict_with_module_objects\" function, another error message popped up which said I need \"to_snake_case\" function. So, I copied and pasted \"to_snake_case\" from the same page.\r\n> Then another error popped up and said it needs \"re\" package. So, I wrote the code \"import re\" at the top of \"generic_utils.py.\" And finally, it works.\r\n> Python version 3.7.7, tensorflow version 2.5.0\r\n\r\nThis also worked for me, Python 3.8.10. Thanks!", "%tensorflow_version 1.x", "> @danial880 it worked for me. Tensorflow 2.5.0. actually after I copied and pasted \"populate_dict_with_module_objects\" function, another error message popped up which said I need \"to_snake_case\" function. So, I copied and pasted \"to_snake_case\" from the same page.\r\n> Then another error popped up and said it needs \"re\" package. So, I wrote the code \"import re\" at the top of \"generic_utils.py.\" And finally, it works.\r\n> Python version 3.7.7, tensorflow version 2.5.0\r\n\r\nthanx a lot it worked for me", "> Please copy \"populate_dict_with_module_objects\" function from this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py) (line 827 to 832) and add it to \"generic_utils.py\"\r\n\r\nit is already there in my case still facing the same issue", "> @danial880 it worked for me. Tensorflow 2.5.0. actually after I copied and pasted \"populate_dict_with_module_objects\" function, another error message popped up which said I need \"to_snake_case\" function. So, I copied and pasted \"to_snake_case\" from the same page.\r\n> Then another error popped up and said it needs \"re\" package. So, I wrote the code \"import re\" at the top of \"generic_utils.py.\" And finally, it works.\r\n> Python version 3.7.7, tensorflow version 2.5.0\r\n\r\nit worked for me,Python version 3.8.10,tensorflow version 2.5.0,thanks!", "The issue is **not occuring** in tensorflow==2.6.0rc0 and python 3.9.6 for me."]}, {"number": 38011, "title": "Notebook problem while contributing", "body": "# issue\r\nI'm contributing to Tensorflow/lucid notebooks. But while changing anything in the notebook I'm getting message in github \" The diff is too large to display\". And when I clicked to see the differences in the branch I'm seeing there are a lot of things is changed in the notebook automatically\r\n# Problem\r\nIt's very hard for the reviewer to see what changes I've made.\r\n* I've tried into vs code having python extension\r\n* Also tried in jupyter notebook\r\nBut I'm getting the same problem in all these\r\n## Example notebook\r\nhttps://github.com/tensorflow/lucid/blob/master/notebooks/activation-atlas/activation-atlas-adversarial.ipynb", "comments": ["Hey, @MarkDaoust  / @lamberta / @ravikyram / @mihaimaruseac / @colah . How you guys solve this problem? ", "I'm working in this issue https://github.com/tensorflow/lucid/issues/236", "@abhinavsp0730 You can try making changes in Google colab. I personally use \"Open in Colab\" chrome extension to open any notebook in colab from github. Make sure you're opening the notebook from your fork(not from upstream repo). After you're done with changes use \"Save a copy in Github\" option in File menu. ", " @ManishAradwad    thank you for your reply. I just only want to clear one thing after doing the changes, does it only reflects the changes I made in the notebook and solve my problem?", "Yes, you can try it once. You'll see the difference.", "What it does it deleted the original file and created a file with the same name. Not updating the original file.", "@ManishAradwad please see https://github.com/tensorflow/lucid/pull/238 this pull request is it correct?", "@abhinavsp0730 Something doesn't seem right. Can you tell me what steps are you actually doing to make a PR. See this [PR](https://github.com/tensorflow/examples/pull/141). Also make sure you're checking the changes in ReviewNB.  ", "\r\nHey, @ManishAradwad here are my steps:\r\nMy steps that you've suggested:\r\n1. I open the notebook in the colab from my forked repo.\r\n2. I have done the changes and clicked on the option save a copy in github .\r\n3. Then I've selected my forked repo directory where the .ipnyb file is present and clicked on save button.\r\n4. But what has happened is the file was replaced with the new one having the same name as the original one.", "@ManishAradwad I've checked the PR also and find that you just changed the window size to 20 to 30.\r\nBut when I saw the changes in the file there are a lot of things has changed. Making the reviwer hard to find what exactly I've changed . \r\n![Screenshot (94)](https://user-images.githubusercontent.com/43638955/77851440-d359e980-71f6-11ea-9976-bce28e5bef9c.png)\r\n![Screenshot (95)](https://user-images.githubusercontent.com/43638955/77851447-d523ad00-71f6-11ea-8c87-7f75ebb79dc7.png)\r\n\r\n", "Check them on the review notebook app(see the comment in your PR). It is used to make it easy for reviewers to see the changes. Also I'm not sure why it is creating a new file in your PR rather than showing changes in original one. I think you should wait till others reply.", "@ManishAradwad , thanks for your help.\r\n\r\nYes. \r\n\r\nOne other trick we have on tensorflow/docs is this [nbfmt.py](https://github.com/tensorflow/docs/blob/master/tools/nbfmt.py), which you can use to standardize notebook formatting for situations like this. \r\n\r\nBut it only helps if the repository is already solling that standard. \r\nAnd viewing the github diff directly will never work if there are inline images."]}, {"number": 38010, "title": "Update activations.py", "body": "improve documentation of sigmoid function, softmax, softplus and swish activation functions", "comments": ["@mihaimaruseac **Off topic doubt**  what does `keras_export()` do ? Once take a look at this https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/image.py file only some of the functions or classes have proper documentation can we add examples and description for the rest of the functions at the bottom of the page?", "`keras_export` is similar to `tf_export`. Both are exporting a Python function to the public API. For example\r\n\r\n```python\r\ntf_export(\"foo\")\r\ndef foo_impl(arg, *args, **kwargs):\r\n  \"\"\"Some function and documentation.\"\"\"\r\n  pass\r\n```\r\n\r\nThis means that TF API has a symbol `tf.foo` which when called results in calling the `foo_impl` function. `keras_export` exports to `keras` instead of to `tensorflow` (which becomes `tf` due to `import tensorflow as tf`). See https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/util/tf_export.py;l=400;drc=1cea2490cb1fb1e930694caa04c36a3049491535\r\n\r\nThe lines you mention are just exports for functions brought in from different modules. If the functions are implemented in Python, then documentation should be added to that Python function. If not, then we should create a wrapping function to wrap these symbols and then attach documentation to the wrapping function.", "@mihaimaruseac once review the changes", "@rohanreddych Can you please address Ubuntu Sanity errors? Thanks!", "@mihaimaruseac changed the output for `.numpy()` as the test was failing.", "There are conflicts with master branch. Can you rebase please?", "I fixed the conflict manually", "@rohanreddych Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned @mihaimaruseac it says `Internal CI build failed` what does it mean?", "The build failed. The `Details` link on the right of the build shows you the log of the build. Scroll to the end,\r\n\r\n```\r\n==== Summary of sanity check results ====\r\n1. do_configure_test: Run ./configure\r\n  PASS\r\n2. do_pylint: Python 3 pylint\r\n  FAIL\r\n3. do_check_futures_test: Check that python files have certain __future__ imports\r\n  PASS\r\n4. do_buildifier: buildifier check\r\n  PASS\r\n5. do_bazel_nobuild: bazel nobuild\r\n  PASS\r\n6. do_bazel_deps_query: bazel query\r\n  PASS\r\n...\r\n```\r\n\r\nwhich means pylint failed. Download the log, look for the pylint section and there is a list of pylint errors:\r\n\r\n```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/keras/activations.py:296: [C0301(line-too-long), ] Line too long (82/80)\r\n```", "@yifeif  Can you please take a look on cla/google test failure? Thanks!", "Fixed cla by playing with the CLA labels. Now this should be imported internally."]}, {"number": 38009, "title": "Keras conversion works in python, but gives strange results in Anroid", "body": "**System information**\r\n- Platform (where I'm experiencing issues)\r\n   android\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```tflite_convert --keras_model_file=best-model-03-0.00.h5 --output_file=new_model.tflite```\r\n\r\n\r\n**The output from the converter invocation**\r\n```\r\n2020-03-28 15:34:27.528903: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-28 15:34:27.562893: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb8e8f21fd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-28 15:34:27.562928: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-03-28 15:34:28.585694: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-03-28 15:34:28.590807: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-28 15:34:28.604941: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-28 15:34:28.604967: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.238ms.\r\n2020-03-28 15:34:28.604972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-03-28 15:34:28.707516: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-03-28 15:34:28.707602: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-28 15:34:28.734064: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-28 15:34:28.734111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 71 nodes (-32), 102 edges (-32), time = 15.415ms.\r\n2020-03-28 15:34:28.734122: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 71 nodes (0), 102 edges (0), time = 3.223ms.```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nHere is the .h5 file: https://drive.google.com/file/d/1Zaj02NDmxG4A7_Mkl81W0puMXAekEEmR/view?usp=sharing\r\n\r\nHere is the .tflite file: https://drive.google.com/file/d/1k_RJU7vm54gVtM_S_n7s4UAE3oekL51M/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nUsing the command above and using the .h5 file linked above, I am able to successfully convert to a .tflite file and perform inference on that .tflite file in Python. \r\n\r\nHowever, when I use this tflite model file in my Android code, I always get outputs that like this: \r\n```\r\n[{\"label\": \"4\", \"prob\": \"0.9999292\"}, {\"label\": \"2\", \"prob\": \"5.891328E-5\"}, {\"label\": \"3\", \"prob\": \"9.082261E-6\"}, {\"label\": \"0\", \"prob\": \"2.7710482E-6\"}, {\"label\": \"1\", \"prob\": \"3.9977402E-10\"}]\r\n```\r\n\r\n(this model has 5 outputs, 0-4)\r\n\r\nNo matter what image I pass, I get that \"4\" is the output with very similar (but not exactly the same) probabilities. These same images in Python are correctly analyzed and give probabilities that are more reasonable.  \r\n\r\n**Any other info / logs**\r\n\r\n- The input image is a grayscale image\r\n- this is a float32 tflite model\r\n- Here is the relevant android code: \r\n\r\n```\r\npublic class TensorFlowModule extends ReactContextBaseJavaModule {\r\n\r\n  private static final int DIM_BATCH_SIZE = 1;\r\n  private static final int DIM_PIXEL_SIZE = 1;\r\n  static final int DIM_IMG_SIZE_X = 300;\r\n  static final int DIM_IMG_SIZE_Y = 100;\r\n  private static final int BYTE_SIZE_OF_FLOAT = 4;\r\n\r\n  private final String TAG = this.getClass().getSimpleName();\r\n\r\n  // THIS IS THE \"JAVA BUFFER\"\r\n  protected ByteBuffer imgData = ByteBuffer.allocateDirect(BYTE_SIZE_OF_FLOAT * DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);\r\n  \r\n  private Interpreter tflite;\r\n  private List<String> labelList;\r\n  private ByteBuffer inputBuffer = null;\r\n  private float[][] labelProbArray = null;\r\n  private static final int RESULTS_TO_SHOW = 5;\r\n  private static final float IMAGE_MEAN = 0f;\r\n  private static final float IMAGE_STD = 255f;\r\n\r\n  private static final String MODEL_PATH = \"best_model.tflite\";\r\n  private static final String LABEL_PATH = \"best-model-03242020_dict.txt\";\r\n  private int[] intValues = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\n\r\n  public TensorFlowModule(ReactApplicationContext reactContext) {\r\n    super(reactContext);\r\n  }\r\n\r\n  @Override\r\n  public String getName() {\r\n    return \"TensorFlow\";\r\n  }\r\n\r\n  @ReactMethod void predictFromExactAssayWindow(String base64Image, final Promise promise) {\r\n    imgData.order(ByteOrder.nativeOrder());\r\n    labelProbArray = new float[1][RESULTS_TO_SHOW];\r\n\r\n    try {\r\n\r\n      labelList = loadLabelList();\r\n    } catch (Exception ex) {\r\n      ex.printStackTrace();\r\n    }\r\n\r\n    byte[] decodedString = Base64.decode(base64Image, Base64.DEFAULT);\r\n    Bitmap old_bitmap = BitmapFactory.decodeByteArray(decodedString, 0, decodedString.length);\r\n    Bitmap bitmap = Bitmap.createScaledBitmap(old_bitmap, DIM_IMG_SIZE_X, DIM_IMG_SIZE_Y, true);\r\n\r\n    convertBitmapToByteBuffer(bitmap);\r\n\r\n      \r\n    try {\r\n      tflite = new Interpreter(loadModelFile());\r\n    } catch (Exception ex) {\r\n      Log.w(\"FIND_exception in loading tflite\", \"1\");      \r\n    }\r\n\r\n    tflite.run(imgData, labelProbArray);\r\n    promise.resolve(getResult());\r\n    \r\n  }\r\n\r\n  private WritableNativeArray getResult() {\r\n    WritableNativeArray result = new WritableNativeArray();\r\n\r\n    //ArrayList<JSONObject> result = new ArrayList<JSONObject>();\r\n    try {\r\n\r\n      for (int i = 0; i < RESULTS_TO_SHOW; i++) {\r\n          WritableNativeMap map = new WritableNativeMap();\r\n\r\n          map.putString(\"label\", labelList.get(i));\r\n          float output = labelProbArray[0][i];\r\n          map.putString(\"prob\", String.valueOf(output));\r\n          result.pushMap(map);\r\n\r\n          Log.w(\"FIND_label \", labelList.get(i));\r\n          Log.w(\"FIND_prob \", String.valueOf(labelProbArray[0][i]));\r\n      }\r\n    } catch (Exception ex) {\r\n      ex.printStackTrace();\r\n    }\r\n\r\n    return result;\r\n  }\r\n\r\n  private List<String> loadLabelList() throws IOException {\r\n    Activity activity = getCurrentActivity();\r\n      List<String> labelList = new ArrayList<String>();\r\n      BufferedReader reader =\r\n              new BufferedReader(new InputStreamReader(activity.getAssets().open(LABEL_PATH)));\r\n      String line;\r\n      while ((line = reader.readLine()) != null) {\r\n          labelList.add(line);\r\n      }\r\n      reader.close();\r\n      return labelList;\r\n  }\r\n\r\n  private void convertBitmapToByteBuffer(Bitmap bitmap) {\r\n      if (imgData == null) {\r\n          return;\r\n      }\r\n      imgData.rewind();\r\n      // Log.w(\"FIND_bitmap width \", String.valueOf(bitmap.getWidth()));\r\n      // Log.w(\"FIND_bitmap height \", String.valueOf(bitmap.getHeight()));\r\n\r\n      bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n      // Convert the image to floating point.\r\n      int pixel = 0;\r\n      //long startTime = SystemClock.uptimeMillis();\r\n      for (int i = 0; i < DIM_IMG_SIZE_X; ++i) {\r\n          for (int j = 0; j < DIM_IMG_SIZE_Y; ++j) {\r\n              final int val = intValues[pixel++];\r\n              float rChannel = (val >> 16) & 0xFF;\r\n              float gChannel = (val >> 8) & 0xFF;\r\n              float bChannel = (val) & 0xFF;\r\n              float gray = (rChannel * 0.299f + gChannel * 0.587f + bChannel * 0.114f);\r\n\r\n              // float pixelValue = (rChannel + gChannel + bChannel) / 3 ;\r\n              Log.w(\"FIND_pixelValue\", \"(\" + i + \", \" + j + \") - \" + String.valueOf(gray));\r\n\r\n              imgData.putFloat(gray);\r\n\r\n          }\r\n      }\r\n  }\r\n\r\n  private MappedByteBuffer loadModelFile() throws IOException {\r\n    Activity activity = getCurrentActivity();\r\n    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_PATH);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n}\r\n```\r\n\r\nWould love some insight into why my outputs are  always a \"4\" and the probabilities for everything except \"4\" are so, so small. Thanks ahead of time", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 38008, "title": "How to use Google AutoML Tensorflow Container export in Python?", "body": "I've trained a model on Google AutoML and exported it as Container. It gave me a `.pb` file. I want to use it offline with Python. Here is I use in this code:\r\n```\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nsys.path.append(\"..\")\r\n\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\nMODEL_NAME = 'inference_graph'\r\nIMAGE_NAME = 'test8.jpg'\r\nCWD_PATH = os.getcwd()\r\n\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','optik.pbtxt')\r\nPATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\r\nNUM_CLASSES = 1\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\nimage = cv2.imread(PATH_TO_IMAGE)\r\nimage_expanded = np.expand_dims(image, axis=0)\r\n\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=8,\r\n    min_score_thresh=0.80)\r\n\r\ncv2.imshow('Object detector', image)\r\ncv2.imwrite('testres.jpg', image)\r\n\r\ncv2.waitKey(0)\r\ncv2.destroyAllWindows()\r\n```\r\nI've put the `.pb` file to inference_graph folder with `frozen_inference_graph.pb` name. I've setted the .pbtxt file on my own. But it gives this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"Object_detection_image.py\", line 67, in <module>\r\n    od_graph_def.ParseFromString(serialized_graph)\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```", "comments": ["@sundowatch, Provide the Tensorflow version that you are using. Thanks", "> @sundowatch, Provide the Tensorflow version that you are using. Thanks\r\n\r\n@gadagashwini thanks for your response.\r\nI'm using tensorflow-gpu 1.13.2 . I've tried on tensorflow-gpu 2.1, I'm getting the same error.\r\nIn addition to these, I've exported the model from Google AutoML Container.", "Ok According to the Google Cloud Documentation suggestion this code worked for me:\r\n\r\n`docker run --rm --name CONTAINER_NAME -p 8501:8501 -v %cd%/model:/tmp/mounted_model/0001 -t gcr.io/automl-vision-ondevice/gcloud-container-1.14.0:latest`"]}, {"number": 38007, "title": "TF 2 ignores one of 2 GPUs", "body": " Dear community,\r\n\r\nI have a problem regarding tensorflow calculation on 2 GPUs connected via SLI technology: only one of them is working and second one is not, although both GPUs are recognized by TF.\r\n\r\nSetup:\r\n- Ubuntu 18.04\r\n- Python 3\r\n- Tensorflow 2.1\r\n- Cuda 10.1\r\n- Nvidia drivers (officials) 440.64\r\n- AMD Ryzen 2700\r\n- Asus x470 prime\r\n- Two GPUs of GTX 1070 connected via SLI techno.\r\n\r\nI have already tested many things that I had found in internet. Concretely:\r\n\r\n1. I started with Tensorflow 2.0, it did not work, so I updated it to TF 2.1. The problem remains\r\n\r\n2. Purged and reinstalled the Nvidia drivers 430.50. Updated them to 440.64. The problem remains\r\n\r\n3. I verified each of my GPUs separately. I removed physically one of them, and launched code on the remaining. It worked and it seems that the GPUs are OK.\r\n\r\n4. I verified each of the GPU's ports on my motherboard separately. It worked and it means that each of the ports are fine.\r\n\r\n5. I inserted two GPUs with and without hardware SLI connection and launched the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.applications import Xception\r\nimport numpy as np\r\n\r\nnum_samples = 100\r\nheight = 224\r\nwidth = 224\r\nnum_classes = 50\r\n\r\nstrategy = tf.distribute.MirroredStrategy(devices=['/GPU:0', '/GPU:1'])\r\nwith strategy.scope():\r\n    parallel_model = Xception(weights=None,\r\n                              input_shape=(height, width, 3),\r\n                              classes=num_classes)\r\n    parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\r\n\r\n### Works only for the first GPU of the\r\n# parallel_model = Xception(weights=None,\r\n#                           input_shape=(height, width, 3),\r\n#                           classes=num_classes)\r\n# parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\n# Generate dummy data.\r\nx = np.random.random((num_samples, height, width, 3))\r\ny = np.random.random((num_samples, num_classes))\r\n\r\nparallel_model.summary()\r\n# This `fit` call will be distributed on 8 GPUs.\r\n# Since the batch size is 256, each GPU will process 32 samples.\r\nparallel_model.fit(x, y, epochs=20, batch_size=16)\r\n```\r\n\r\nAs a result, when ```strategy = tf.distribute.MirroredStrategy(devices=['/GPU:0'])```, the code is running fine.\r\nHowever, when ```devices=['/GPU:1']``` or ```devices=['/GPU:0', '/GPU:1']```, the nvidia-smi shows some process on the 2nd GPU, but the code execution is stacked at line\r\n\r\n```\r\n2020-03-28 21:51:14.891325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7162 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-03-28 21:51:14.891805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-28 21:51:14.892399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1),\r\n```\r\nso I have to reboot the computer, because it s dead.\r\n\r\n6. Initially, my X11 configuration (xorg.conf) was not configured for SLI:\r\n```\r\nSection \"Device\"\r\n    Identifier     \"Device0\"\r\n    Driver         \"nvidia\"\r\n    VendorName     \"NVIDIA Corporation\"\r\nEndSection\r\n\r\nSection \"Device\"\r\n    Identifier     \"Device1\"\r\n    Driver         \"nvidia\"\r\n    VendorName     \"NVIDIA Corporation\"\r\nEndSection\r\n\r\nSection \"Screen\"\r\n    Identifier     \"Screen0\"\r\n    Device         \"Device0\"\r\n    Monitor        \"Monitor0\"\r\n    DefaultDepth    24\r\n    SubSection     \"Display\"\r\n        Depth       24\r\n    EndSubSection\r\nEndSection\r\n```\r\nAfter google search, I played with ```sudo nvidia-xconfig -sli=on```; ```sudo nvidia-xconfig -sli=auto```, etc\r\n\r\nAs a result, after reboot, I obtain a bootloop with 2 lines:\r\n```\r\nrecovering journal\r\n/dev/nume0n1p2: clean, XXX/XXX files, XXX/XXX blocks\r\n```\r\nEvery ~3 sec the screen becomes black and then these 2 lines show again.\r\nImpossible to access to TTY, because it is in bootloop as well. I looked everything that I could find on this subject, nothing worked. So, I kept the previous X11 config without SLI\r\n\r\nIf you experienced such type of problem, do not hesitate to share it. Any advice would help.\r\n\r\nThanks! ", "comments": ["> so I have to reboot the computer, because it s dead.\r\n\r\nSo once execution reaches that point you cannot just kill the TF process, you *have* to reboot?  If so, this seems like it could be a problem with the NVIDIA driver.\r\n\r\n@chsigg Do you know of any extra logging we can enable on the CUDA driver to help triage this further?", "`CUDA_VISIBLE_DEVICES` might make debugging simpler without having to remove one of the cards. But the fact that you're now stuck in a bootloop seems to indicate that there is a more fundamental problem with your system. You could also try to write a little test program, along these lines:\r\n\r\n```\r\nsize_t buffer_size = 1024;\r\ncudaSetDevice(0);\r\nvoid* dev0_buffer;\r\ncudaMalloc(&dev0_buffer, buffer_size);\r\nvoid* dev1_buffer;\r\ncudaSetDevice(1);\r\ncudaMalloc(&dev1_buffer, buffer_size);\r\ncudaMemcpy(dev1_buffer, dev0_buffer, buffer_size, cudaMemcpyDefault);\r\n```\r\n", "Thanks for your reply.\r\n@sanjoy \r\n> So once execution reaches that point you cannot just kill the TF process, you _have_ to reboot? If so, this seems like it could be a problem with the NVIDIA driver.\r\nYes, it is impossible to kill the process, so I have to reboot the PC. I thought that it was the problem of the drivers as well. I reinstalled it, but the problem remains.\r\n\r\n@chsigg \r\nI run this code, but obtain the following errors:\r\n```\r\nhello_world.cpp:2:14: error: expected constructor, destructor, or type conversion before \u2018(\u2019 token\r\n cudaSetDevice(0);\r\n              ^\r\nhello_world.cpp:4:11: error: expected constructor, destructor, or type conversion before \u2018(\u2019 token\r\n cudaMalloc(&dev0_buffer, buffer_size);\r\n           ^\r\nhello_world.cpp:6:14: error: expected constructor, destructor, or type conversion before \u2018(\u2019 token\r\n cudaSetDevice(1);\r\n              ^\r\nhello_world.cpp:7:11: error: expected constructor, destructor, or type conversion before \u2018(\u2019 token\r\n cudaMalloc(&dev1_buffer, buffer_size);\r\n           ^\r\nhello_world.cpp:8:11: error: expected constructor, destructor, or type conversion before \u2018(\u2019 token\r\n cudaMemcpy(dev1_buffer, dev0_buffer, buffer_size, cudaMemcpyDefault);\r\n           ^\r\n```\r\n\r\n", "Small update:\r\nThe both gpu's are functional when I run the following test:\r\n```\r\nimport tensorflow as tf\r\n\r\nn = 12345\r\ndtype = tf.float32\r\nprint(2 * n*n*32/8 / 1.e9)\r\nwith tf.device(\"/gpu:1\"): # /gpu:0\r\n    for i in range(100):\r\n        matrix1 = tf.Variable(tf.random.uniform((n, n), dtype=dtype))\r\n        matrix2 = tf.Variable(tf.random.uniform((n, n), dtype=dtype))\r\n        product = tf.norm(tf.matmul(matrix1, matrix2))\r\n        print(i, product)\r\n```\r\nIt means that when `gpu=/gpu:0`, I obtain a good execution of code with\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:08:00.0  On |                  N/A |\r\n|  0%   43C    P8    13W / 180W |    7989MiB /  8116MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |\r\n|  0%   43C    P8     6W / 180W |      2MiB /  8119MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\nWhen `gpu=/gpu:1`, I obtain\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:08:00.0  On |                  N/A |\r\n|  0%   39C    P2    37W / 180W |    552MiB /  8116MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |\r\n|  0%   42C    P2   175W / 180W |   7855MiB /  8119MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```", "Another test provides very interesting results. Here is the code:\r\n```\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n# strategy = tf.distribute.MirroredStrategy()\r\n# with strategy.scope():\r\nwith tf.device(\"/gpu:1\"):\r\n    model = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n    ])\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                  metrics=['accuracy'])\r\n\r\n    model.fit(x_train, y_train, epochs=5)\r\n```\r\nThis code works (it means that it arrives at the end), but yields a numerically-strange result:\r\n```\r\n2020-04-07 14:00:39.080936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:00:39.081553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:00:39.082173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:00:39.082777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7259 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-04-07 14:00:39.083251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:00:39.084005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2020-04-07 14:00:40.645991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n60000/60000 [==============================] - 4s 68us/sample - loss: 2.3016 - accuracy: 0.1116\r\nEpoch 2/5\r\n60000/60000 [==============================] - 3s 56us/sample - loss: 2.3013 - accuracy: 0.1124\r\nEpoch 3/5\r\n60000/60000 [==============================] - 3s 54us/sample - loss: 2.3014 - accuracy: 0.1124\r\nEpoch 4/5\r\n60000/60000 [==============================] - 3s 53us/sample - loss: 2.3013 - accuracy: 0.1124\r\nEpoch 5/5\r\n60000/60000 [==============================] - 3s 53us/sample - loss: 2.3013 - accuracy: 0.1124\r\n\r\nProcess finished with exit code 0\r\n```\r\nAs you can see, the accuracy goes never up to ~0.11 (I run this test multiple times). \r\nAnother interesting thing is that even if I precise that GPU=GPU:/1, it seems that TF calls both of the GPUs:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:08:00.0  On |                  N/A |\r\n|  0%   50C    P2    46W / 180W |   7830MiB /  8116MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    Off  | 00000000:09:00.0 Off |                  N/A |\r\n|  0%   49C    P2    36W / 180W |   7855MiB /  8119MiB |     13%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\nI removed SMI hardware connection, but result repeated.\r\n\r\nAnd the 3rd interesting behavior is that when you put line `model.fit(x_train, y_train, epochs=5)` out of the `with tf.device(\"/gpu:1\"):` section, it seems that gpu is not used at all. One iteration takes a lot of time and loss is nan (or strangely big number).\r\n\r\nAt the end, I add that when you put `with tf.device(\"/gpu:0\"):`, everything that I ve tested finishes up to a good result:\r\n```\r\n2020-04-07 14:09:28.088388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:09:28.089664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:09:28.090964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:09:28.091713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7263 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-04-07 14:09:28.092022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-07 14:09:28.092459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2020-04-07 14:09:28.751855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n60000/60000 [==============================] - 3s 56us/sample - loss: 0.2543 - accuracy: 0.9271\r\nEpoch 2/5\r\n60000/60000 [==============================] - 3s 51us/sample - loss: 0.1128 - accuracy: 0.9670\r\nEpoch 3/5\r\n60000/60000 [==============================] - 3s 52us/sample - loss: 0.0763 - accuracy: 0.9772\r\nEpoch 4/5\r\n60000/60000 [==============================] - 3s 52us/sample - loss: 0.0573 - accuracy: 0.9822\r\nEpoch 5/5\r\n60000/60000 [==============================] - 3s 51us/sample - loss: 0.0451 - accuracy: 0.9858\r\n\r\nProcess finished with exit code 0\r\n```", "> Another interesting thing is that even if I precise that GPU=GPU:/1, it seems that TF calls both of the GPUs:\r\n\r\nThat may not have sufficient to completely disable access to gpu:0.  Can you try using [set_visible_devices](https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices) instead?", "> That may not have sufficient to completely disable access to gpu:0. Can you try using [set_visible_devices](https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices) instead?\r\n\r\n@sanjoy Thanks for the advice, you were right. I integrated `set_visible_devices` into the code and managed to disable the gpu:0.\r\n\r\nThe code is the following one:\r\n```\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(physical_devices)\r\n# tf.config.set_visible_devices(physical_devices[0], 'GPU')\r\ntf.config.set_visible_devices(physical_devices[1], 'GPU')\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(10)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n```\r\n\r\nTo sum up: I manage to train MNIST on each of the GPUs using `set_visible_devices` and nvidia-smi shows that only one of the GPUs is working.\r\n\r\nHowever, the initial problem remains: it is still not possible to train on multiple GPUs. One of the codes that I used is the following one:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nprint(physical_devices)\r\ntf.config.set_visible_devices(physical_devices, 'GPU')\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10)])\r\n    model.compile(optimizer='adam',\r\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                  metrics=['accuracy'])\r\n    model.fit(x_train, y_train, epochs=5)\r\n```\r\n\r\nVerbose that I get:\r\n```\r\n2020-04-11 22:45:54.156999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7245 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-04-11 22:45:54.157457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-11 22:45:54.158053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2020-04-11 22:45:59.774139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n```\r\n\r\nAnd from this moment nothing happens, so that I have to kill the process / reboot PC.\r\n\r\nDo you have any ideas?\r\nThanks!", "> And from this moment nothing happens, so that I have to kill the process / reboot PC.\r\n\r\nDo you have to kill the process or reboot the PC?  As I stated above, if you have to reboot the PC then it is likely a driver or a hardware issue.", "> Do you have to kill the process or reboot the PC? As I stated above, if you have to reboot the PC then it is likely a driver or a hardware issue.\r\n\r\nIf I am fast enough (I did not time it, but i am talking about first 5-10 sec of code execution), I can kill it. When I stop, I get the following message:\r\n\r\n```\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n   32/60000 [..............................] - ETA: 1:33:29Traceback (most recent call last):\r\n  File \"/home/ivan/Work/Code/diagnolly/datascience/gpu_testing.py\", line 55, in <module>\r\n    model.fit(x_train, y_train, epochs=5)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 694, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 201, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 363, in join\r\n    while any(t.is_alive() for t in threads) and not self.wait_for_stop(1.0):\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 311, in wait_for_stop\r\n    return self._stop_event.wait(timeout)\r\n  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\r\n    signaled = self._cond.wait(timeout)\r\n  File \"/usr/lib/python3.6/threading.py\", line 299, in wait\r\n    gotit = waiter.acquire(True, timeout)\r\nKeyboardInterrupt\r\n\r\nProcess finished with exit code 137 (interrupted by signal 9: SIGKILL)\r\n```\r\n\r\nWhen I wait too much time, the process completely and I have to reboot PC", "Small update:\r\nI just reinstalled everything (including Ubuntu), but the problem remains...\r\n\r\nTo sum up:\r\n- Each of the GPUs working well using `tf.config.set_visible_devices` and ables to train correctly MNIST\r\n- When try to use both of the GPUs simultaneously using `tf.distribute.MirroredStrategy()`\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model.compile(...)\r\n    model.fit(...)\r\n```\r\nThe execution stops at\r\n```\r\n2020-04-11 22:45:54.156999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7245 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1)\r\n2020-04-11 22:45:54.157457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-11 22:45:54.158053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7624 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n2020-04-11 22:45:59.774139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n```\r\nAnd the process gets stacked. If we try to stop this process, we obtain:\r\n```\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n   32/60000 [..............................] - ETA: 1:33:29Traceback (most recent call last):\r\n  File \"/home/ivan/Work/Code/diagnolly/datascience/gpu_testing.py\", line 55, in <module>\r\n    model.fit(x_train, y_train, epochs=5)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 694, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 201, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 363, in join\r\n    while any(t.is_alive() for t in threads) and not self.wait_for_stop(1.0):\r\n  File \"/home/ivan/Work/Code/pyG/lib/python3.6/site-packages/tensorflow_core/python/training/coordinator.py\", line 311, in wait_for_stop\r\n    return self._stop_event.wait(timeout)\r\n  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\r\n    signaled = self._cond.wait(timeout)\r\n  File \"/usr/lib/python3.6/threading.py\", line 299, in wait\r\n    gotit = waiter.acquire(True, timeout)\r\nKeyboardInterrupt\r\n\r\nProcess finished with exit code 137 (interrupted by signal 9: SIGKILL)\r\n```", "I have a similar issue.. though i did not go that deep as @mlexpert .. in short I am not able to train using MirroredStrategy .\r\nMy Config is roughly similar to @mlexpert \r\nThings I tried : \r\n\r\n- Run sample Mnist code for distributed training in a docker instance of tensorflow nightly and latest gpu containers.\r\n\r\n- It stalls precisely at this line and no progress is seen further. \r\n`INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).`\r\n\r\nMy configuration : Amd Ryzen 3900x 32gb Ram, Ubuntu 20.04 , Gtx 1070 X 2 connected via SLI \r\n\r\nFYI : Single GPU training is working fine.", "Ok i managed to resolve this by turning off IOMMU in my system bios.\r\nApparently this has to do something with pcie acs.", "Hi @jogiji,\r\n\r\nThanks for your input. In fact, I am not sure that I have an option of enable/disable of IOMMU for my motherboard.\r\n\r\nSo, for the moment the problem ramains...\r\n\r\nAny advice would help,\r\nThanks.", "@mlexpert atleast i had that option.. if not then try searching about turning acs off via command line immediatley after system reboot.\r\nFor more info https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/troubleshooting.html", "Dear @jogiji,\r\n\r\nI updated my BIOS and then found and **turned off IOMMU**.\r\nAnd it worked!\r\n\r\nSome more interesting behavior that could be helpful for someone in a similar situation: \r\nI installed NCCL with tests and tried to launch them. On 1 GPU `./build/all_reduce_perf -b 8 -e 128M -f 2 -g 1` everything worked goog, however, on 2 GPUs\r\n`./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2` the tests got stacked.\r\n\r\nThe same story about CUDA native tests in `1_Utilities`. They did not work.\r\n\r\nSo, anyone who has a problem with multi-gpu (TF, CUDA, or NCCL) try to turn off the IOMMU.\r\n\r\nThnaks a lot, @jogiji \r\nHave a nice day all!\r\nBR\r\nP.S. problem may be treated as solved\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38007\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38007\">No</a>\n"]}, {"number": 38006, "title": "_ConstantValue can now see through tf.identity ops", "body": "Fix for #37716\r\n\r\n`_ConstantValue` (and thus `get_static_value`) now know that the static value of the result of `tf.identity` is the same as the static value of the original.", "comments": ["It seems the test failures are caused by tensorflow now being able to raise `InvalidArgumentError` in some more situations, which means that the confusion matrix tests that expect the op to assert fail with this exception. How should I proceed? ", "> It seems the test failures are caused by tensorflow now being able to raise `InvalidArgumentError` in some more situations, which means that the confusion matrix tests that expect the op to assert fail with this exception. How should I proceed?\r\n\r\n@alextp Can you please assist on above comments from @ngc92 . Thanks!", "You can make changes to the tests to make them pass.", "@ngc92 Can you please check @alextp comments and resolve conflicts?. Thanks!"]}, {"number": 38005, "title": "Cleaning up old codes in colab notebooks.", "body": "## Cleaning up the codes in the colab notebooks.\r\nFrom taking the reference from this [commit](https://github.com/tensorflow/examples/commit/5351bd4e1d4067e2a007b410495200c4bcf3fe61). I've see that there is celaning of codes going on.\r\n* from __future__ import absolute_import, division, print_function, unicode_literals these lines are unnecessary. \r\n*   ``` \r\n         % try tensorflow 2.x\r\n             except:\r\n                     pass \r\n     ```\r\nAs now colab already supports tensorflow 2.x by default\r\n\r\n# Pull Request\r\nI'll love to address this issue. Kindly assign me this task.\r\n## Repositories\r\n1. Tensorflow/community\r\n\r\nP.S :- I'll keep on adding repo in this list after discussing with the community.", "comments": ["Hey, @MarkDaoust / @lamberta  / @mihaimaruseac what are your thoughts in this?", "I handled everything except `community/` in 036117caae91ede4783cb0797273128d203e56aa.\r\n\r\nIf you want to do that to `community/` too that would be great.", "You're talking about [this directory](https://github.com/tensorflow/examples/tree/master/community/en) right?", "> \r\n> \r\n> I handled everything except `community/` in 036117caae91ede4783cb0797273128d203e56aa.\r\n>\r\n>If you want to do that to `community/` too that would be great.\r\n\r\nYah I'd love to do that in `community/` but I'm not getting what this means 036117caae91ede4783cb0797273128d203e56aa", "Thanks for fixing this @abhinavsp0730.\r\n\r\n(Reviewing/merging/email-notifications may have been a bit simpler if you had stacked the commits into a single PR, but NBD) \r\n\r\n>  I'm not getting what this means 036117caae91ede4...\r\n\r\nI meant for it to be a link to the commit, but apparently that doesn't work across repositories.\r\n\r\nhttps://github.com/tensorflow/examples/commit/036117caae91ede4783cb0797273128d203e56aa", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38005\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38005\">No</a>\n"]}, {"number": 38004, "title": "KerasClassifier.score is ... broken!?", "body": "I am using the scikit_learn wrapper to wrap a keras model and train / evaluate it in scikit learn. Calling `KerasClassifer.score` should return the accuracy of the classifier; however, no matter what I do, it just doesn't.\r\n\r\nLooking at the source the code does two things:\r\n\r\n1. In case of sparse labels it converts them to a OneHot matrix (lines 296 - 300)\r\n2. It calles `Sequential.evaluate` and then hopes to find a metric called `acc` or `accuracy` which it treats as the accuracy of the model. (lines 302 - 307)\r\n\r\nIf it doesn't manage to find a named metric with the right name, it raises an exception.\r\n\r\nI don't understand how this could possibly work (and it doesn't work for me). Given that the target labels are OneHot encoded the correct metric to use is `CategoricalAccuracy`; however, it is named \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/metrics.py#L758\r\n\r\nLogically `KerasClassifer.score` raises an exception. Worse, the error message suggests to add the `Accuracy` metric to the model. This can be misleading, as it makes the error disappear and returns a value, but that value is not ... accurate (pun intended).\r\n\r\nI suggest renaming `accuracy` in \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/wrappers/scikit_learn.py#L306\r\n\r\nto `categorical_accuracy`, and, while at it, I suggest to add `_estimator_type = \"classifier\"` as a class variable. Scikit learn checks for it to identify `KerasClassifier` as a classifier, and without it a lot of functionality doesn't work as intended.\r\n\r\nIf there is agreement for this change I can submit a PR.\r\n\r\n", "comments": ["@FirefoxMetzger,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Sure: `tf.__version__ == '2.0.0'`\r\nMinimal (not) working example to demonstrate the problem:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sklearn.preprocessing\r\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier \r\n\r\n## --- LOAD DATA ---\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n\r\nencoder = sklearn.preprocessing.OneHotEncoder(dtype=np.float32)\r\nencoder.fit(Y_train.reshape((-1, 1)))\r\n\r\nY_train_encoded = encoder.transform(Y_train.reshape((-1, 1))).toarray()\r\nY_test_encoded = encoder.transform(Y_test.reshape((-1, 1))).toarray()\r\n\r\n\r\n## --- SETUP MODEL ---\r\ndef setupModel():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Input(shape=(28,28)))\r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(10))\r\n    model.add(tf.keras.layers.Softmax())\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-5),\r\n                  loss=tf.keras.losses.CategoricalCrossentropy(),\r\n                  metrics=[tf.keras.metrics.CategoricalAccuracy()]\r\n                 )\r\n    return model\r\n\r\n# this model now uses the sklearn API instead of the Keras API\r\n# it still trains using tensorflow under the hood\r\nmodel = KerasClassifier(build_fn=setupModel,\r\n                        epochs=2, # show that training executes\r\n                        batch_size=256,\r\n                       )\r\n\r\n# train the model\r\nmodel.fit(X_train, Y_train_encoded)\r\n\r\n# evaluate the test accuracy\r\ntest_accuracy = model.score(X_test, Y_test_encoded, verbose=0)\r\nprint(f\"The test accuracy is {test_accuracy * 100:.2f}%\")\r\n```\r\n\r\nMinimal not working example to demonstrate the `Accuracy` metric problem:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sklearn.preprocessing\r\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier \r\n\r\n## --- LOAD DATA ---\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n\r\nencoder = sklearn.preprocessing.OneHotEncoder(dtype=np.float32)\r\nencoder.fit(Y_train.reshape((-1, 1)))\r\n\r\nY_train_encoded = encoder.transform(Y_train.reshape((-1, 1))).toarray()\r\nY_test_encoded = encoder.transform(Y_test.reshape((-1, 1))).toarray()\r\n\r\n\r\n## --- SETUP MODEL ---\r\ndef setupModel():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Input(shape=(28,28)))\r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(10))\r\n    model.add(tf.keras.layers.Softmax())\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-5),\r\n                  loss=tf.keras.losses.CategoricalCrossentropy(),\r\n                  metrics=[tf.keras.metrics.Accuracy()]\r\n                 )\r\n    return model\r\n\r\n# this model now uses the sklearn API instead of the Keras API\r\n# it still trains using tensorflow under the hood\r\nmodel = KerasClassifier(build_fn=setupModel,\r\n                        epochs=2, # show that training executes\r\n                        batch_size=256,\r\n                       )\r\n\r\n# train the model\r\nmodel.fit(X_train, Y_train_encoded)\r\n\r\n# evaluate the test accuracy\r\ntest_accuracy = model.score(X_test, Y_test_encoded, verbose=0)\r\nprint(f\"The test accuracy is {test_accuracy * 100:.2f}%\")\r\n\r\nY_pred = model.model.predict(X_test)\r\nacc = tf.keras.metrics.CategoricalAccuracy()\r\nacc.update_state(Y_test, Y_pred)\r\nprint(f\"The (true) categorical accuracy is {acc.result().numpy()*100:.2f}%\")\r\n```\r\nOutput: \r\n```\r\nTrain on 60000 samples\r\nEpoch 1/2\r\n60000/60000 [==============================] - 1s 13us/sample - loss: 67.0612 - accuracy: 0.6470\r\nEpoch 2/2\r\n60000/60000 [==============================] - 1s 9us/sample - loss: 29.1524 - accuracy: 0.7125\r\nThe test accuracy is 72.88%\r\nThe (true) categorical accuracy is 10.40%\r\n```\r\nThe reason for this discrepancy is described in the opening comment.", "Was able to reproduce the issue with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/2e9acbeb5bad8ac79bd38142705635fd/38004.ipynb), [TF v2.1](https://colab.research.google.com/gist/amahendrakar/26bc1e24371e6d3c36a3b2ca25b1f82d/38004-2-1.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/75c55005e743f9cc3456b71a91b5e6a8/38004-tfnightly.ipynb). Please find the attached gist. Thanks!", "@FirefoxMetzger As mentioned in the error, I changed metric in the compile from `metrics=[tf.keras.metrics.CategoricalAccuracy()]` to `metrics=['accuracy']. With that change everything worked as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d710e8f57df61a6446f1845eebdd43a5/untitled54.ipynb).\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "You are correct. It does resolve the exception, as I mentioned in my issue. It does **not** solve the underlying issue though. It simply doesn't crash anymore and instead silently computes the wrong accuracy; arguably much worse, because easier to miss.\r\n\r\nUsing `metrics=['accuracy']` is, as far as I am aware, a shorthand for `metrics=[tf.keras.metrics.Accuracy()]`, and - as demonstrated in my reply to @amahendrakar - the metric is unsuited for OneHot encoded data. To demonstrate this, the test case I posted above run on @jvishnuvardhan 's modified code produces the following output:\r\n```\r\nEpoch 1/2\r\n235/235 [==============================] - 1s 3ms/step - loss: 51.1440 - accuracy: 0.3992\r\nEpoch 2/2\r\n235/235 [==============================] - 1s 3ms/step - loss: 25.3406 - accuracy: 0.5925\r\nThe test accuracy is 61.51%\r\nThe (true) categorical accuracy is 10.04%\r\n```\r\nBoth accuracy numbers should be identical. Both represent the fraction of the test dataset that has been assigned the correct label by the model, and both numbers use the same model and same test data.\r\n\r\nFor reference, here is the code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sklearn.preprocessing\r\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier \r\n\r\n## --- LOAD DATA ---\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n\r\nencoder = sklearn.preprocessing.OneHotEncoder(dtype=np.float32)\r\nencoder.fit(Y_train.reshape((-1, 1)))\r\n\r\nY_train_encoded = encoder.transform(Y_train.reshape((-1, 1))).toarray()\r\nY_test_encoded = encoder.transform(Y_test.reshape((-1, 1))).toarray()\r\n\r\n\r\n## --- SETUP MODEL ---\r\ndef setupModel():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Input(shape=(28,28)))\r\n    model.add(tf.keras.layers.Flatten())\r\n    model.add(tf.keras.layers.Dense(10))\r\n    model.add(tf.keras.layers.Softmax())\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-5),\r\n                  loss=tf.keras.losses.CategoricalCrossentropy(),\r\n                  metrics = ['accuracy'] #metrics=[tf.keras.metrics.CategoricalAccuracy()]\r\n                 )\r\n    return model\r\n\r\n# this model now uses the sklearn API instead of the Keras API\r\n# it still trains using tensorflow under the hood\r\nmodel = KerasClassifier(build_fn=setupModel,\r\n                        epochs=2, # show that training executes\r\n                        batch_size=256,\r\n                       )\r\n\r\n# train the model\r\nmodel.fit(X_train, Y_train_encoded)\r\n\r\n# evaluate the test accuracy\r\ntest_accuracy = model.score(X_test, Y_test_encoded, verbose=0)\r\nprint(f\"The test accuracy is {test_accuracy * 100:.2f}%\")\r\n\r\n# test case to compare metrics\r\nY_pred = model.model.predict(X_test)\r\nacc = tf.keras.metrics.CategoricalAccuracy()\r\nacc.update_state(Y_test, Y_pred)\r\nprint(f\"The (true) categorical accuracy is {acc.result().numpy()*100:.2f}%\")\r\n```", "Please take a look at issue #38596, linked above. I believe this is the underlying issue for why the Keras `model.evaluate()` method computes the wrong loss.", "> Please take a look at issue #38596, linked above. I believe this is the underlying issue for why the Keras `model.evaluate()` method computes the wrong loss.\r\n\r\n@FirefoxMetzger,\r\nPlease take a look at @bentyeh's comment and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hey @amahendrakar I checked out the comment, and it, unfortunately, doesn't resolve the issue. \r\n\r\nIt really seems to be the naming of the desired metric in line\r\nhttps://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/wrappers/scikit_learn.py#L306\r\nas I quoted in the opening post.\r\n\r\nIf you check the keras metrics, you can find the metric with `name='accuracy'` at\r\nhttps://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/metrics.py#L662\r\nwhereas the desired metric is defined at\r\nhttps://github.com/tensorflow/tensorflow/blob/e24331bf116f5efc8d42bc888a0dbd271aa92aab/tensorflow/python/keras/metrics.py#L758\r\nunder the name `categorical_accuracy`.\r\n\r\nThat is what I believe to be the issue, unless I misunderstand how the wrapper is supposed to work.", "Was able to reproduce the issue with [TF v2.2](https://colab.sandbox.google.com/gist/amahendrakar/78a28dd6b5318c2b13ceed64e7f09f95/38004.ipynb) and [TF-nightly](https://colab.sandbox.google.com/gist/amahendrakar/9838683e8aed3e8afa8622da15629b27/38004-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/drive/1OnKnoDXOcZJSpmViPbB4ZjWWlz77PO10?resourcekey=0-Vo6nWmCqjSLuukP5d-5VrQ#scrollTo=F7KiZ-XbcGF1). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38004\">No</a>\n"]}, {"number": 38003, "title": "AttributeError: module 'tensorflow' has no attribute 'layers'", "body": "   model = tf.layers.Sequential([\r\n        tf.keras.layers.Conv2D(64, (10, 10), activation = 'relu', input_shape = input_shape, kernel_initializer = initialize_weights(), kernel_regularizer = tf.keras.regularizers.l2(2e-4)),\r\n        tf.keras.layers.MaxPool2D(),\r\n        tf.keras.layers.Conv2D(128, (7, 7), activation='relu', kernel_initializer = initialize_weights(), bias_initializer = initialize_bias(), kernel_regularizer=tf.keras.regularizers.l2(2e-4)),\r\n        tf.keras.layers.MaxPool2D(),\r\n        tf.keras.layers.Conv2D(128, (4, 4), activation='relu', kernel_initializer= initialize_weights(), bias_initializer=initialize_bias(), kernel_regularizer=tf.keras.regularizers.l2(2e-4)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Conv2D(256, (4, 4), activation='relu', kernel_initializer=initialize_weights(), bias_initializer=initialize_bias(), kernel_regularizer=tf.keras.regularizers.l2(2e-4)),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(4096, activation='sigmoid',kernel_regularizer=l2(1e-3), kernel_initializer=initialize_weights(), bias_initializer=initialize_bias())\r\n\r\n    ])\r\n\r\n\r\n### **Error**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\shane\\OneDrive\\Desktop\\Project new\\signature_verification-master 3\\core\\model.py\", line 61, in <module>\r\n    model = get_model((105, 105, 1))\r\n  File \"C:\\Users\\shane\\OneDrive\\Desktop\\Project new\\signature_verification-master 3\\core\\model.py\", line 28, in get_model\r\n    model = tf.layers.Sequential([\r\nAttributeError: module 'tensorflow' has no attribute 'layers'\r\n>>> ", "comments": ["@shanethomas1029, Use `tf.keras.Sequential` instead of `tf.layers.Sequential`.", "As per the documentation at https://www.tensorflow.org/api_docs/python/tf/keras/Sequential,\r\n\r\n![image](https://user-images.githubusercontent.com/41635766/77829675-64708800-7145-11ea-8f35-6a14aee0c72d.png)\r\n\r\nIt is `tf.keras.Sequential`.\r\nPlease close this issue.", "@shanethomas1029, Did you try the @khimraj  and @Rubix982  solution. \r\nAs mentioned in the [Tenosrflow doc](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=nightly), use `tf.keras.Sequential `instead of `tf.layers.Sequential`. Thanks", "@shanethomas1029, Is this still an issue?", "Still working on it  Thank you !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38003\">No</a>\n"]}, {"number": 38002, "title": "Is retracing inevitable when working with variously-sized inputs?", "body": "I'm beginning to develop some models for token classification with varying-length documents, and it's looking like using variable-length inputs with one-document batches is likely going to give better-results than padding, given the extreme heterogeneity in document lengths.\r\n\r\nHere's a simplified version of my model:\r\n\r\n```\r\ninp = Input(shape=(None, input_dims))\r\nlay = Conv1D(filters=100, kernel_size=5, padding='same', activation='relu')(inp)\r\noutlayers = [Conv1D(filters=3, kernel_size=5, activation=\"softmax\", padding='same', name=i)(lay) for i in out_varnames]\r\nmodel = Model(inp, outlayers)\r\n```\r\n\r\nThe inputs vary in size from `(100,input_dims)` to `(8000,input_dims)`, hence the choice to use variable-length inputs.  \r\n\r\nMy training function is fairly generic:\r\n```\r\n@tf.function\r\ndef train(x, y):\r\n    with tf.GradientTape() as g:\r\n        pred = model(x)\r\n        loss = loss_object(y, pred)\r\n        gradients = g.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n\r\nWhen I loop the documents through the train function, I get the following warning:\r\n\r\n> WARNING:tensorflow:5 out of the last 5 calls to <function train at 0x7f954a804d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n\r\nReading and github issues elsewhere, I get the impression that retracing is caused by the fact that the varying input lengths require tensorflow to rejigger some stuff under the hood.  \r\n\r\nMy questions:  \r\n(1) is this accurate?  Is retracing caused by the variably-sized inputs?\r\n(2) is there a way to avoid retracing when working with variably-sized inputs?\r\n(3) what are some ways to optimize performance when working with variably-sized inputs?", "comments": ["@cranedroesch \r\ncould you please share the tensor flow version on which the error is faced, along with a simple standalone code for us to replicate the issue", "TF = 2.0.0\r\nPython = 3.7.6\r\nCUDA = 10.1.243\r\ncuDNN = 7.6.4.38\r\n\r\nI am also having the somewhat same problem as @cranedroesch, but my input-size is not variable. \r\nI am using a custom training loop with GradientTape to train the model. Everything is working as expected but instead of retracing. A new graph is built on every new training step and that leads to memory leak and my CPU's RAM is accumulating memory with every epoch(and also with every step_per_epoch). Following is my generic training script. \r\n\r\n```\r\nmodel = YoloV3(yolo_size, training=True, classes=num_classes)\r\noptimizer = tf.keras.optimizers.Adam(lr=learning_rate)\r\nloss = [YoloLoss(anchors[mask], classes=num_classes)\r\n            for mask in anchor_masks]\r\n\r\n#both training and validation datasets are python generator.\r\ntrain_dataset()#returns tf.Tensor( array of batch of [3-D Images], shape=(1, 416, 416, 3), dtype=float32)\r\nval_dataset()#returns tuple(<tf.Tensor: id=1809, shape=(1, 13, 13, 3, 6), dtype=float32, numpy=array of labels , dtype=float32)>, <tf.Tensor: id=1810, shape=(1, 26, 26, 3, 6), dtype=float32, numpy=array of labels , dtype=float32)>,<tf.Tensor: id=1811, shape=(1, 52, 52, 3, 6), dtype=float32, numpy=array of labels , dtype=float32))>)\r\n#val_dataset is actually a tuple of three eager tensors e.g tuple(has three eager tensors)\r\n\r\navg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\r\navg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\r\n\r\n# train graph\r\n# also tried with def train(model, optimizer, loss, avg_loss, images, labels):\r\n#as mentioned here https://www.tensorflow.org/api_docs/python/tf/function that you can avoid retracing if you use tensors as the input parameters to the graph and by using input_signature. So also used \r\n# @tf.function (input_signature=[tf.TensorSpec(shape=[None, None, None, None], dtype=tf.float32),\r\n#                               (tf.TensorSpec(shape=[None, None, None, None,None], dtype=tf.float32),\r\n#                                 tf.TensorSpec(shape=[None, None, None, None,None], dtype=tf.float32),\r\n#                                 tf.TensorSpec(shape=[None, None, None, None,None], dtype=tf.float32)\r\n#                                )])\r\nBut it still got the RAM acuumulation issue. \r\n\r\n@tf.function\r\ndef train(images, labels):\r\n    with tf.GradientTape() as tape:\r\n        outputs = model(images, training=True)\r\n        regularization_loss = tf.reduce_sum(model.losses)\r\n        pred_loss = []\r\n        for output, label, loss_fn in zip(outputs, labels, loss):\r\n            pred_loss.append(loss_fn(label, output))\r\n        total_loss = tf.reduce_sum(pred_loss) + regularization_loss\r\n    grads = tape.gradient(total_loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    avg_loss.update_state(total_loss)\r\n\r\n\r\n# val graph\r\n@tf.function\r\n# def val(model, loss, avg_val_loss, images, labels):\r\ndef val(images, labels):\r\n    # print('val is called')\r\n    outputs = model(images, training=False)\r\n    regularization_loss = tf.reduce_sum(model.losses)\r\n    pred_loss = []\r\n    for output, label, loss_fn in zip(outputs, labels, loss):\r\n        pred_loss.append(loss_fn(label, output))\r\n    total_loss = tf.reduce_sum(pred_loss) + regularization_loss\r\n    avg_val_loss.update_state(total_loss)\r\n\r\n#Graph retracing and RAM accumulation issue with the following training loop\r\n# training loop: Also updating the progress bar and writing tensorboard summary in the loop that is not shown here.\r\n\r\nfor epoch in range(epochs):\r\n    avg_loss.reset_states()\r\n    avg_val_loss.reset_states()\r\n\r\n    for train_images, train_labels in train_dataset:\r\n        if train_steps_per_epoch != steps_per_epochs:\r\n            train(train_images, train_labels)\r\n            train_loss_value = [('loss', avg_loss.result().numpy())]\r\n            train_steps_per_epoch += 1\r\n            if train_steps_per_epoch == steps_per_epochs:\r\n                break\r\n        else:\r\n            break\r\n\r\n    val_step_train = train_steps_per_epoch\r\n    train_steps_per_epoch = 0\r\n\r\n    for val_images, val_labels in val_dataset:\r\n        if val_steps_per_epoch != validation_steps:\r\n            val(val_images, val_labels)\r\n            val_loss_value = [('val_loss', avg_val_loss.result().numpy())]\r\n            val_steps_per_epoch += 1\r\n            if val_steps_per_epoch == validation_steps:\r\n                break\r\n        else:\r\n            break\r\n    val_steps_per_epoch = 0\r\n    model.save_weights()\r\n\r\n#The only that is working for me currently is resetting the default graph on every step_per_epoch. Although this, I think, is not the solution but the workaround. Following is the training loop that does not accumulate the CPU's RAM on every training step.\r\n\r\nfor epoch in range(epochs):\r\n    avg_loss.reset_states()\r\n    avg_val_loss.reset_states()\r\n\r\n    for train_images, train_labels in train_dataset:\r\n        tf.compat.v1.reset_default_graph()\r\n        if train_steps_per_epoch != steps_per_epochs:\r\n            train(train_images, train_labels)\r\n            train_loss_value = [('loss', avg_loss.result().numpy())]\r\n            train_steps_per_epoch += 1\r\n            if train_steps_per_epoch == steps_per_epochs:\r\n                break\r\n        else:\r\n            break\r\n\r\n    val_step_train = train_steps_per_epoch\r\n    train_steps_per_epoch = 0\r\n\r\n    for val_images, val_labels in val_dataset:\r\n        tf.compat.v1.reset_default_graph()\r\n        if val_steps_per_epoch != validation_steps:\r\n            val(val_images, val_labels)\r\n            val_loss_value = [('val_loss', avg_val_loss.result().numpy())]\r\n            val_steps_per_epoch += 1\r\n            if val_steps_per_epoch == validation_steps:\r\n                break\r\n        else:\r\n            break\r\n    val_steps_per_epoch = 0\r\n    model.save_weights()\r\n```\r\n\r\nI tried to debug the memory leak without resetting the default graph but no success. It would be very nice if we can debug the problem.", "@cranedroesch \r\ncode shared is incomplete for us to replicate the issue faced, please find [gist](https://colab.sandbox.google.com/gist/Saduf2019/898af8475ecd8114e1692647901a921f/38002.ipynb) of error faced.", "@cranedroesch\r\nplease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38001, "title": "Update README.md", "body": "Add new [Tensorflow 2 course from Coursera](https://www.coursera.org/learn/getting-started-with-tensor-flow2)", "comments": ["Let's not do a single line update to README and not make it overly complicated."]}, {"number": 38000, "title": "Fix TFE_OpReset doesn't clear inputs completely", "body": "https://github.com/tensorflow/tensorflow/pull/37803", "comments": []}, {"number": 37999, "title": "Fix context usage in dlpack functions", "body": "Fix broken usage in dlpack functions(https://github.com/tensorflow/tensorflow/issues/37986)\r\n\r\nnote: only `from_dlpack` part uses `TFE_Context`\r\n", "comments": ["This is a very high priority change. We need to cherry-pick this into 2.2 if we still can (it's a bugfix). cc @goldiegadde ", "@VoVAllen: Could you please put up a cherry-pick into the 2.2 branch?", "Sure", "Cherry-pick pr at https://github.com/tensorflow/tensorflow/pull/38088\r\n@jaingaurav @alextp "]}, {"number": 37998, "title": "[tf.data.experimental.cardinality] not working on FlatMapDataset ? [2.2.0-rc1]", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: - GPU model and memory: NA \r\n\r\n**Describe the current behavior**\r\nI am making some test with 2.2.0-rc1. When using data from tensorflow dataset tf.data.experimental.cardinality is returning the number of even \r\n\r\n```\r\nprint(data['train'])\r\nprint(tf.data.experimental.cardinality(data['train'])) \r\n```\r\n\r\n```\r\n<DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\r\ntf.Tensor(67349, shape=(), dtype=int64)\r\n```\r\nIn such case I have a `DatasetV1Adapter`\r\n\r\nNow when I am using Huggingface transformer that modify  the structure of the data:\r\n\r\n```\r\ntrain_dataset = glue_convert_examples_to_features(data['train'], \r\n                                                  tokenizer, \r\n                                                  max_length=128, \r\n                                                  task='sst-2')\r\n```\r\n\r\n```print(train_dataset)\r\nprint(tf.data.experimental.cardinality(train_dataset))\r\n```\r\n\r\n```\r\n<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\r\ntf.Tensor(-2, shape=(), dtype=int64)\r\n```\r\n\r\nIn this case this is a `FlatMapDataset ` and tf.data.experimental.cardinality is not able tor eturn the number of event ? Is this expected ? In which case is tf.data.experimental.cardinality working ?\r\n\r\nThe documentation for TF 2.1.0 just said:\r\n`dataset: A tf.data.Dataset for which to determine cardinality.`\r\n https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality\r\nFor me (as a naive user) FlatMapDataset/DatasetV1Adapter are tf.data.Dataset \r\n\r\nThe way the transformer is modifying the data is by using tf.data.Dataset.from_generator\r\n\r\n**Describe the expected behavior**\r\nBe able to return the total number of entry after the data being transformer using tf.data.Dataset.from_generator\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\ndata, info = tensorflow_datasets.load(name='glue/sst2',\r\n                                      data_dir=data_dir,\r\n                                      with_info=True)\r\n# recap input dataset\r\nprint(data['train'])\r\nprint(tf.data.experimental.cardinality(data['train']))\r\nprint(len(list(data['train']))\r\n\r\n# Prepare data for BERT\r\ntrain_dataset = glue_convert_examples_to_features(data['train'], \r\n                                                  tokenizer, \r\n                                                  max_length=128, \r\n                                                  task='sst-2')\r\n\r\nprint(train_dataset)\r\nprint(tf.data.experimental.cardinality(train_dataset))\r\nprint(len(list(train_dataset)))\r\n```\r\nA full notebook can be found here:\r\nhttps://github.com/tarrade/proj_multilingual_text_classification/blob/master/notebook/00-Test/08_SST2_Huggingface_model.ipynb\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["The answer is most likely that the `glue_convert_examples_to_features` method (for which I do not see source code) is doing something that prevents `tf.data.experimental.cardinality` from statically inferring the size of the output dataset.\r\n\r\nLike shape inference, `tf.data.experimental.cardinality` will only produce a known answer if the answer can be determined by static analysis (i.e. without executing the input pipeline). When the input pipeline contains a `flat_map`, it is generally not possible to statically determine what will be the cardinality of the output from the cardinality from the input; for instance, the inputs to `flat_map` could be filenames that `flat_map` uses to create `TFRecordDataset` (that do not have statically known cardinality).\r\n\r\nIf you know the relationship between the input and output cardinality of your `flat_map`, you can provide it to the runtime using `tf.data.experimental.assert_cardinality`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37998\">No</a>\n", "Hi @jsimsa,\r\n\r\nThanks for the follow up. I will check with the Huggingface's team. The code is here:\r\nhttps://github.com/huggingface/transformers/blob/master/src/transformers/data/processors/glue.py\r\n\r\nThanks"]}, {"number": 37997, "title": "1.15.2: _pywrap_tensorflow_internal.so differs between builds", "body": "**System information**\r\n- OS Platform and Distribution: openSUSE-Tumbleweed-20200324\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: built with rpm [spec](https://github.com/bmwiedemann/openSUSE/blob/master/packages/t/tensorflow/tensorflow.spec#L449)\r\n- Bazel version (if compiling from source): 0.24\r\n- GCC/Compiler version (if compiling from source): gcc9 and gcc10\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhile working on reproducible builds for openSUSE, I found that\r\nour tensorflow-1.15.2 package varied across builds.\r\n\r\nSee https://reproducible-builds.org/ for why this matters.\r\n\r\nThe variations do not occur when disabling ASLR for the build.\r\n\r\nThe previous 1.13.2 version built with python-3.7 still did build reproducibly.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbuild tensorflow twice from scratch:\r\nosc checkout openSUSE:Factory/tensorflow && cd $_\r\nosc build --noservice --keep-pkg=RPMS\r\n\r\nand compare resulting _pywrap_tensorflow_internal.so content\r\n\r\n**Any other info / logs**\r\n\r\n`/usr/lib64/python3.8/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so differs in assembler output`\r\n\r\nby building without Link Time Optimization (LTO), I could see that exactly one .o file differed in the build environment\r\nBinary files /var/tmp/build-root.10/.mount/home/abuild/rpmbuild/SOURCES/BAZEL/_bazel_abuild/089fd2236bcbfcbcf994cdf39cd6bcb6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/lite/_objs/tensorflow_lite_legalize_tf/prepare_tf.pic.o and /var/tmp/build-root.10b/.mount/home/abuild/rpmbuild/SOURCES/BAZEL/_bazel_abuild/089fd2236bcbfcbcf994cdf39cd6bcb6/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/lite/_objs/tensorflow_lite_legalize_tf/prepare_tf.pic.o differ\r\n\r\nalso the asm diff contained\r\n+       lea    offset(%rip),%rsi        #   <_ZTSZN4mlir16PassRegistrationINS_3TFL12_GLOBAL__N_129PrepareCompositeFunctionsPassEEC4EN4llvm9StringRefES6_EUlvE_ + ofs>\r\n\r\nthat comes from\r\ntensorflow-1.15.2/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc\r\nwhich is very close to the prepare_tf.cc file used to create the differing .o file\r\n\r\nIt is possible that the nondeterminism comes from within gcc9 and gcc10 triggered by some special feature used in prepare_tf.cc but to prove that, I would need a preprocessed version of that compilation. Due to the size and complexity of the build process, I did not manage to get that yet.\r\n", "comments": ["extra info: our openSUSE tensorflow2-2.1.0 package is also not affected", "tensorflow-1.15.3 still has these variations in `tensorflow_lite_legalize_tf/prepare_tf.pic.o`", "looking at the diff of build trees again, I found this diff\r\n\r\n```diff\r\n+++ org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/compiler/mlir/lite/transforms/generated_prepare_tf.inc        \r\n@@ -289,7 +289,7 @@\r\n */\r\n struct GeneratedConvert0 : public RewritePattern {\r\n   GeneratedConvert0(MLIRContext *context)\r\n-      : RewritePattern(\"tf.FusedBatchNorm\", {\"tf.Add\", \"tf.Mul\", \"tf.Rsqrt\", \"tf.Sub\", \"tf.Const\"}, 1, context) {}\r\n+      : RewritePattern(\"tf.FusedBatchNorm\", {\"tf.Const\", \"tf.Rsqrt\", \"tf.Add\", \"tf.Mul\", \"tf.Sub\"}, 1, context) {}\r\n```\r\n\r\ngenerated by a bazel call of `/bin/bash\r\n-c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/local_config_mlir/mlir-tblgen -I external/local_config_mlir/include -I external/org_tensorflow -I $(dirname tensorflow/compiler/mlir/lite/transforms/legalize_patterns.td)  -gen-rewriters tensorflow/compiler/mlir/lite/transforms/legalize_patterns.td -o bazel-out/k8-opt/genfiles/tensorflow/compiler/mlir/lite/transforms/generated_legalize_tf.inc`\r\n\r\nfrom `tensorflow-1.15.3/third_party/mlir/tools/mlir-tblgen/mlir-tblgen.cpp` - so rather not a gcc bug.", "Can you check against master please? We won't be able to fix this on patch releases since it seems to be a significant amount of change.", "There seems to be a large difference from v1.15.3..master and master branch lacks the `third_party/mlir/tools/mlir-tblgen` dir. I have trouble building that as a package.\r\nWas mlir-tblgen dropped? Or does some magic only pull it in at release time?", "`third_party/mlir` is now moved to be a part of LLVM. The location inside TF repo was only temporary.\r\n\r\nIt is expected that there are differences between master and `r1.15`, but the question is is the current builds are reproducible or if we should work towards making them be (likely). Since we cannot add major changes to release branches after final release, we know that `r1.15` based builds are not reproducible.", "I built that llvm master mlir-tblgen and when omitting the `--gen-rewriters` param, it even created reproducible output. Otherwise it was\r\n\r\n```\r\nllvm-project/build/bin/mlir-tblgen -I $(pwd)/inc -o generated_legalize_tf.inc --gen-rewriters legalize_patterns.td ; md5sum generated_legalize_tf.inc\r\nIncluded from legalize_patterns.td:21:\r\nSOMEPATH/inc/tensorflow/compiler/mlir/tensorflow/ir/tf_ops.td:66:1: error: Record `TF_ConstOp' does not have a field named `successors'!            \r\n\r\ndef TF_ConstOp : TF_Op<\"Const\", [NoSideEffect]> {\r\n^\r\n```\r\n\r\nI tried to compare it to the mlir-tblgen from 1.15.3 but that coredumped on the same call.\r\nBut this one worked with `--gen-rewriters` and was also reproducible now. so something else probably missing in this test setup.\r\n\r\n\r\nso it seems I know too little to properly debug that.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37997\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37997\">No</a>\n"]}, {"number": 37996, "title": "Documentation corresponding to Arguments for Pre-Trained Models inside <tf.keras.applications> is missing", "body": "## URL(s) with the issue: The information related to Arguments corresponding to the Pre-Trained Models defined under  https://www.tensorflow.org/api_docs/python/tf/keras/applications is missing.\r\n\r\nSome examples links are shown below:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNet\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16\r\n\r\n## Description of issue (what needs changing): The information corresponding to Arguments should be specified like that it is Specified in [Keras Website](https://keras.io/applications/#vgg16).\r\n\r\nFor example, why should someone use this method? How is it useful? : If someone want to know what Arguments should be passed while trying to use these Pre-Trained Models, information is lacking in TF.Org site and the Developers should go to Keras Website. The information is not available in the Source Code corresponding to those TF Pre-Trained Models.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : Yes\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : NO\r\n\r\n### Submit a pull request? : No\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Hello, I would like to work on this. Add docs and examples for the models, starting from the mobilenet.", "You can click on see nightly on the website to look at the arguments.", "@maxkaustav,\r\nThank you for your suggestion. But shouldn't the arguments visible in Stable Version as well, because, \r\n\r\n1. Many people refer the documentation of Stable Version (it will be open by default)\r\n2. Documentation of Nightly Version will not be very neat, with Stricken Off lines, etc..", "@rakeshmothukuru1,\r\nHi, I think the stricken off lines reflect the changes to the stable version, meaning it was deleted and will not be in the stable version when deployed. However, the documentation could still use some examples I believe", "I will also begin with the documentation for VGG16. But what files should I edit?? Can someone plz help?", "To whom it may concern, @gadagashwini (tagging because of the assignee label)\r\n\r\nTo prevent duplicated efforts, just notifying that I am working on adding examples and listing the raises for the models. I expect to finish within 48 hours for all the pretrained models. I'm adding to the docs in nightly. \r\n\r\nI'll also notify if I won't be able to complete it within the timeframe :) ", "I created a pull request for the examples I added at here [#38048](https://github.com/tensorflow/tensorflow/pull/38048)", "@rakeshmothukuru1 I am closing this issue as the related PR https://github.com/tensorflow/tensorflow/pull/38048 was merged. Please feel free to reopen if the issue was not resolved. Thanks!", "@jvishnuvardhan,\r\nI cannot Reopen the issue. Can you please reopen it because,\r\n\r\n1. The PR has been raised and merged and some examples have been added in the PR but the same is not reflected in the [TF Site](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) yet.\r\n\r\n2. Information about the Arguments to be used for a particular Pre-Trained Model is still missing, corresponding to Stable Version.  ", "@rakeshmothukuru1 Can Please mention what arguments are missing so that some one will work on updating the docs. Thanks!", "The PR, that had basic examples, was merged. However, I can see that it was rolled back in the master by a member of TensorFlow team.\r\nI guess they might already have been working on the docs but haven't put them out yet. I expect comprehensive documentation including the arguments for particular pre-trained models.", "FYI, nightly docs become the stable docs on next release", "Hi @rakeshmothukuru1! Concerned links in template has been updated with description of arguments in TF 2.6 , Please check the links from here. [MobileNet,](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/MobileNet)[InceptionV3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/InceptionV3),[ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50),[VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16) .Closing this issue as it seems to be resolved in TF 2.6. Thanks!"]}, {"number": 37995, "title": "Bazel is not able to detect CUDA 10.2 header files when building from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): MSVC 2019 (Visual Studio 2019)\r\n- CUDA/cuDNN version: CUDA Version: 10.2, cuDNN Version: 7\r\n- GPU model and memory: NVIDIA Geforce GTX 940MX 2GB VRAM with Cuda Compatibility 5.0 (Lenovo IdeaPad Flex 5)\r\n\r\n\r\n\r\n**Describe the problem**\r\nBazel is not able to detect the CUDA library files while building tensorflow, even though everything is included in the PATH, and the configure script is also detecting all relevant CUDA library files and includes.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. I was following the steps building from source as the official Documentation showed. While it mentioned CUDA 10.1, I wanted to test my installation with CUDA 10.2, since I have issues with working with CUDA 10.1.\r\n2. I ensured that I set all relevant Bazel PATH environment variables before running the configure script `configure.py`.\r\n3. I successfully configured bazel using `./config.py`.\r\n4. Now, I ran the following command,\r\n```\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nThis gave me different errors, which I initially resolved using the following links:\r\n1. I had the same issue as the poster in this link, which is resolved using [this ](https://github.com/tensorflow/tensorflow/issues/35253#issuecomment-570895670)patch.\r\nNow, I'm stuck with the following error. Apparently, bazel isn't able to find the CUDA header files, even though the relevant path is included in the `.tf_configure.bazelrc` file.\r\n\r\nFor reference, here is the `.tf_configure.bazelrc` file:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Users/$USER/AppData/Local/Programs/Python/Python36/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Users/$USER/AppData/Local/Programs/Python/Python36/lib/site-packages\"\r\nbuild --python_path=\"C:/Users/$USER/AppData/Local/Programs/Python/Python36/python.exe\"\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"5.0\"\r\nbuild --config=cuda\r\nbuild:opt --copt=/arch:AVX\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --define=override_eigen_strong_inline=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nHere is the bazel command I used for buliding:\r\n*Build Command*:\r\n```\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nThe log is mentioned below.\r\n\r\n**Any other info / logs**\r\n```\r\nE:\\Tensorflow\\tensorflow-master\\tensorflow-master>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/srira/AppData/Local/Programs/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/srira/AppData/Local/Programs/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=C:/Users/srira/AppData/Local/Programs/Python/Python36/lib/site-packages --python_path=C:/Users/srira/AppData/Local/Programs/Python/Python36/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:windows in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file e:\\tensorflow\\tensorflow-master\\tensorflow-master\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl:1243:18):\r\n - E:/tensorflow/tensorflow-master/tensorflow-master/tensorflow/workspace.bzl:91:5\r\n - E:/tensorflow/tensorflow-master/tensorflow-master/WORKSPACE:26:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1213\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1075, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1076, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 365, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 365, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nrealpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include': No such file or directory\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1213\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1075, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1076, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 365, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 365, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nrealpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include': No such file or directory\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1213\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1075, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 1076, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 365, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/gpus/cuda_configure.bzl\", line 365, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow-master/tensorflow-master/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nrealpath: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include': No such file or directory\r\nINFO: Elapsed time: 2.717s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\n**Additional Information**\r\nCUDA 10.2 works perfectly fine on my system, and I've tested programs using CUDA. Pytorch 1.4 with CUDA also runs fine on my system. I have Visual Studio 2019 installed, and I've included all relevant paths for both CUDA and Bazel.\r\n", "comments": ["It seems that the I only have the Linux `patch` command as a subset of the WSL `bash`, and not from a separate Linux Environment. At first, I thought that the issue could be resolved by converting the Unix style paths back into Windows, but then I realised that Bazel needs Unix pathnames anyway.\r\n\r\nSo the solution was to install `patch` from a separate Linux environment. One of the recommendations from the official Docs was the Arch environment for Windows, *MSYS*. I set it up according to the docs, and now the problems seem to have been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37995\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37995\">No</a>\n", "> It seems that the I only have the Linux `patch` command as a subset of the WSL `bash`, and not from a separate Linux Environment. At first, I thought that the issue could be resolved by converting the Unix style paths back into Windows, but then I realised that Bazel needs Unix pathnames anyway.\r\n> \r\n> So the solution was to install `patch` from a separate Linux environment. One of the recommendations from the official Docs was the Arch environment for Windows, _MSYS_. I set it up according to the docs, and now the problems seem to have been resolved.\r\n\r\n----------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nI've got the same issue even though I have both ```patch``` and ```realpath``` installed with msys2 (using ```pacman -S patch realpath```) and they're on my path as well as msys2.exe. Just to be sure I even removed the wsl versions but sadly to no avail."]}, {"number": 37994, "title": "Fixed make_*_function method name in *_step docstrings", "body": "`_make_train_function` became `make_train_function` in 4a8f4803f65007809bd94da66fc590cfc64c1cca. Same for the `test` and `predict` variants. This is now reflected in the docstrings of the `*_step` methods.", "comments": []}, {"number": 37993, "title": "can't install tensorflow-gpu==2.0.0b1 in Google Colaboratory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Google Colaboratory\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/__init__.py in <module>()\r\n     31 from tensorflow.python.keras.saving.save import load_model\r\n     32 from tensorflow.python.keras.saving.save import save_model\r\n---> 33 from tensorflow.python.keras.saving.saved_model import export_saved_model\r\n     34 from tensorflow.python.keras.saving.saved_model import load_from_saved_model\r\n     35 from tensorflow.python.keras.saving.saving_utils import trace_model_call\r\n\r\nImportError: cannot import name 'export_saved_model'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n!pip install tensorflow-gpu==2.0.0-beta1\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\ntensorflow==2.0.0b1 does work. I've also tried to uninstall any other version of tensorflow, but it doesn't solve the problem\r\n", "comments": ["Hi @abpalaciot !\r\n\r\nI copied `!pip install tensorflow-gpu==2.0.0-beta1` as you said to install the required version for TF's GPU builds in your `exac sequence of commands`, and it works perfectly fine for me.\r\n\r\n![image](https://user-images.githubusercontent.com/41635766/77828967-a945f000-7140-11ea-990e-95ff9b77336f.png)\r\n\r\nIs it possible that you can share your code or the steps that you took to reproduce them? It maybe that the problem lies in the process rather then that single line of code.\r\n\r\nHere is a [link](https://colab.research.google.com/drive/1lzYn5PXVJaYD3i_OxMn3ojEOLyBO7ULO) ( I've set the permissions to edit it ) for the above shown jupyter notebook file hosted on google colab, so you can see what settings I've set - it is the default as of now .\r\n\r\nHope to hear again from you soon! ", "Hi @Rubix982 !\r\n\r\n![image](https://user-images.githubusercontent.com/54606640/77833528-d680a700-711c-11ea-9d01-32e71567b165.png)\r\n\r\nThe error produces when I make the import. Thankfully I was able to solve it by previously uninstalling tf.", "Also when I start training produces this error/warning:\r\n\r\nWARNING:tensorflow:Entity <function train_step at 0x7f0725bfd730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x7f0725bfd730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function train_step at 0x7f0725bfd730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train_step at 0x7f0725bfd730>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method CNN_Encoder.call of <__main__.CNN_Encoder object at 0x7f0944685be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CNN_Encoder.call of <__main__.CNN_Encoder object at 0x7f0944685be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method CNN_Encoder.call of <__main__.CNN_Encoder object at 0x7f0944685be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CNN_Encoder.call of <__main__.CNN_Encoder object at 0x7f0944685be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x7f09d56178c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7f09d56178c8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function standard_gru at 0x7f09d56178c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7f09d56178c8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7f09d5617b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7f09d5617b70>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7f09d5617b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7f09d5617b70>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x7f09d56178c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7f09d56178c8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function standard_gru at 0x7f09d56178c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7f09d56178c8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7f09d5617b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7f09d5617b70>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7f09d5617b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7f09d5617b70>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x7f09441fae48>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function standard_gru at 0x7f09d56178c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7f09d56178c8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function standard_gru at 0x7f09d56178c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_gru at 0x7f09d56178c8>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function cudnn_gru at 0x7f09d5617b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7f09d5617b70>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function cudnn_gru at 0x7f09d5617b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_gru at 0x7f09d5617b70>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method RNN_Decoder.call of <__main__.RNN_Decoder object at 0x7f0944685ba8>>: AttributeError: module 'gast' has no attribute 'Num'\r\n\r\n![image](https://user-images.githubusercontent.com/54606640/77833597-69b9dc80-711d-11ea-9f0b-2128ecda6216.png)\r\n\r\nThis is working in the Image Captioning tutorial by TF in Colaboratory using tensorflow-gpu==2.0.0b1\r\n\r\n\r\n", "Are you referring to https://www.tensorflow.org/tutorials/text/image_captioning this by \"Image Captioning Tutorial\"?", "@abpalaciot \r\n\r\nCan you please provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> Are you referring to https://www.tensorflow.org/tutorials/text/image_captioning this by \"Image Captioning Tutorial\"?\r\n\r\nYes, that's the one.", "This is the Colab:\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\r\n\r\nThe only modification is done before making any import, run:\r\n\r\n!pip install tensorflow-gpu==2.0.0-beta1\r\n\r\n", "@abpalaciot \r\n\r\nIs there any specific reason you want to use 2.0.0-beta1. You can use 2.x stable versions TF 2.0, 2.1.0,'2.2.0-rc1' instead.And let us know if you face any problems with latest stable 2.x releases?", "Because it's the latest version of TensorFlow that works in Google Cloud Functions. The newest versions can't be installed beacuse pip was not updated.", "I have tried with TF 2.0.0-beta1 version and able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1d2c869abde58415689c62a1e1cd1e75/untitled758.ipynb). Thanks!", "There is a tracking bug for with Google Cloud Function. \r\nTo know more https://github.com/tensorflow/tensorflow/issues/34707#issuecomment-564266913\r\nClosing this issue since switching to latest stable TF version (2.1) solves this problem. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37993\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37993\">No</a>\n", "When i connect colab with hosted runtime,tensorflow installed and work successfully but when i connect it to local runtime it either shows 'no module tensorflow' or run infinitely when i enter command\r\n!pip install tensorflow\r\n\r\nkindly help with this"]}, {"number": 37992, "title": "2.2-rc2 cherry-pick request: Delegate _aggregate_gradients in LossScaleOptimizer.", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/37765 for real. This time I actually tested it fixes it by running the code example in the first post.", "comments": []}, {"number": 37990, "title": "[tf.keras] Model.metrics_names bug in TensorFlow 2.2.0", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Mac OS\r\n- TensorFlow version (use command below): 2.2.0rc0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nModel.metrics_names returns an empty list (see below example) for a compiled model. This is new unexpected behavior as of TensorFlow 2.2.0 (not the case in TensorFlow 2.1.0)\r\n\r\nThese metrics names are important at compile time because they can be used to check against monitored quantities in callback. E.g. if a `ModelCheckpoint` callback is trying to monitor `val_lss` we can easily catch the typo before calling `model.fit` or finishing the first epoch of training.\r\n\r\n**Describe the expected behavior**\r\n\r\nModel.metric returns metrics and loss names (see below example).\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ninputs = tf.keras.layers.Input(shape=(3,))\r\noutputs = tf.keras.layers.Dense(2, name=\"out_1\")(inputs)\r\nnet = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\nnet.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\r\n\r\nnet.metrics_names = []  # TensorFlow 2.2.0rc2\r\nnet.metrics_names = [\"loss\", \"mae\"] # TensorFlow 2.1.0\r\n```\r\n", "comments": ["@briannemsick, In TF 2.2 , you need to call `fit` or `train_on_batch` before you call  `metrics_names`.", "This seems like a design regression. The new behavior is less functional and acts unexpectedly - the model is compiled...\r\n\r\nAcessing the metrics names before calling model.fit can prevent wasted compute by checking the monitored metrics in callbacks against the models metrics.", "Firstly we need to make proper layer in say sequential neural network as if before training the model if done the layering for doing the particular job we can easily use `metrics_names` as after the fit function or transform function can be used ", "This change also breaks several callbacks that rely on being able to read the `metrics `and `metrics_names` with the `on_train_begin()` hook. In my case, it breaks the [TensorFlow Addons](https://github.com/tensorflow/addons) `TQDMProgressBar()` [callback](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/callbacks/tqdm_progress_bar.py).\r\nAny idea as to how I can implement a work-around to this issue?\r\n\r\nI had to upgrade to TF-2.2.0rc1 to fix a memory leak bug introduced in TF-2.1 and now I'm stuck with this instead. ", "@briannemsick Thank you for pointing this out. This is definitely a breaking change but intended. Sorry about the lack of information on this on release notes and docs. We have fixed that now. \r\n\r\nWe refactored compile/fit quite a bit and with that we made functional and subclassed model behaviors consistent. Because of this you will be able to use some of these model properties only after your model has been trained/evaluated on actual data.", "In this case, a warning or exception when these attributes are accessed before train/eval feels warranted.", "@pavithrasv Can I ask what is the reasoning behind this change? You mention refactoring to make things consistent, but how was the previous behavior inconsistent?", "@briannemsick We are looking into adding a warning as well. Thank you.\r\n\r\n@bjtho08 Earlier you will be able to see metrics_name after compile call for functional model but not for subclassed model.", "@briannemsick there are constraints because of which we will not be able to add a warning but we have updated the docs with a note and examples to clarify the behavior.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37990\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37990\">No</a>\n"]}, {"number": 37989, "title": "TFLite slower than Tensorflow on Desktop?", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Unbutu 18.04, Windows 10\r\n- TensorFlow installed from (source or binary): binary (tf-nightly)\r\n- TensorFlow version (use command below): v1.12.1-28195-g6609461732 2.2.0-dev20200327\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nOn desktop, TFLite infers 2x slower than the regular tensorflow in python (running on unbutu, installed from pip tf-nightly), and 5x slower in C (running on Windows10, with DLLs compiled from source)\r\nI tested with the UNet model found here: https://github.com/deezer/spleeter/blob/master/spleeter/model/functions/unet.py\r\nWithout GPU enabled.\r\n\r\n**Describe the expected behavior**\r\nTFLite is supposed to be optimized to be more efficient for prediction-only, but with this model it's not the case.\r\nIs this because TFLite is only optimized for mobile?\r\nOr is it a specific operator in the model that slows down the current build of TFLite?\r\n\r\nPS: I didn't have access to test on mobile, I only tested on desktop.\r\nPS2: I did try to play with SetNumThread, but it didn't change the results.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\n#!/usr/bin/env python\r\n# coding: utf8\r\n\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninterpreter = tf.lite.Interpreter(model_path='model.tflite')\r\ninterpreter.allocate_tensors()\r\ndata = np.zeros((1, 512, 1024, 2), dtype=np.float32)\r\ninterpreter.set_tensor(0, data)\r\nfor i in range(1, 10):\r\n    start = time.time()\r\n    interpreter.invoke()\r\n    end = time.time()\r\n    print(\"tflite:\", end-start)\r\n\r\nloaded = tf.saved_model.load('saved_model')\r\ninfer = loaded.signatures[\"serving_default\"]\r\ninput = tf.convert_to_tensor(data, np.float32)\r\nfor i in range(1, 10):\r\n    start = time.time()\r\n    infer(input)\r\n    end = time.time()\r\n    print(\"tensorflow:\", end-start)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n> python3 profile.py\r\ntflite: 3.574683427810669\r\ntflite: 1.5097410678863525\r\ntflite: 1.486119031906128\r\ntflite: 1.5448968410491943\r\ntflite: 1.4875497817993164\r\ntflite: 1.4804818630218506\r\ntflite: 1.543769121170044\r\ntflite: 1.5214331150054932\r\ntflite: 1.4932630062103271\r\ntensorflow: 1.282404899597168\r\ntensorflow: 0.6814086437225342\r\ntensorflow: 0.7910218238830566\r\ntensorflow: 0.6581625938415527\r\ntensorflow: 0.620100736618042\r\ntensorflow: 0.7165799140930176\r\ntensorflow: 0.6456854343414307\r\ntensorflow: 0.6069018840789795\r\n```", "comments": ["@sclavel \r\nplease share exact sequence of steps before you encounter the issue", "@sclavel\r\nplease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 37988, "title": "AttributeError with TF2.2.0-rc1 using `keras.model.train_on_batch` inside `tf.function`", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (both CPU and GPU runtime \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): From default Colab TF2.x version\r\n- TensorFlow version (use command below): v2.2.0-rc1-0-gacf4951a2f (2.2.0-rc1)\r\n- Python version: 3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen trying to train a keras model using `train_on_batch` inside a `tf.function`:\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32),\r\n                              tf.TensorSpec(shape=(None, 10), dtype=tf.int32)\r\n                              ])\r\ndef train(inp, extra):\r\n  expected = calc_expected(inp, extra)\r\n  return model_1.train_on_batch((inp, extra), expected)\r\n```\r\nTensorFlow raises an `AttributeError`:\r\n```\r\nAttributeError: in user code:\r\n\r\n    <ipython-input-6-49c8760b225c>:6 train  *\r\n        return model_1.train_on_batch((inp, extra), expected)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1287 train_on_batch  **\r\n        logs = tf_utils.to_numpy_or_python_type(logs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:523 to_numpy_or_python_type\r\n        return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 map_structure\r\n        structure[0], [func(*x) for x in entries],\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 <listcomp>\r\n        structure[0], [func(*x) for x in entries],\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:519 _to_single_numpy_or_python_type\r\n        x = t.numpy()\r\n\r\n    AttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe code works fine in eager execution, and should not raise an error when run in a `tf.function`.\r\nThis also works fine with TensorFlow 2.1, so it looks to be a recent regression.\r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://colab.research.google.com/drive/1uy1awEQE4_t_0uZ4MzGXvfeeRnG_EJVQ\r\n\r\n**Other info / logs**\r\n<details>\r\n<summary>Full backtrace copied from Colab</summary>\r\n<p>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-28-c04cb8faf387> in <module>()\r\n      2 b = tf.random.uniform(shape=(2, 10), maxval=10, dtype=tf.int32)\r\n      3 \r\n----> 4 train(a, b)\r\n\r\n8 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625       # This is the first call of __call__, so we have to initialize.\r\n    626       initializers = []\r\n--> 627       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    628     finally:\r\n    629       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504     self._concrete_stateful_fn = (\r\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 506             *args, **kwds))\r\n    507 \r\n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444       args, kwargs = None, None\r\n   2445     with self._lock:\r\n-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2447     return graph_function\r\n   2448 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n-> 2777       graph_function = self._create_graph_function(args, kwargs)\r\n   2778       self._function_cache.primary[cache_key] = graph_function\r\n   2779       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    <ipython-input-6-49c8760b225c>:6 train  *\r\n        return model_1.train_on_batch((inp, extra), expected)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1287 train_on_batch  **\r\n        logs = tf_utils.to_numpy_or_python_type(logs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:523 to_numpy_or_python_type\r\n        return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 map_structure\r\n        structure[0], [func(*x) for x in entries],\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:617 <listcomp>\r\n        structure[0], [func(*x) for x in entries],\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:519 _to_single_numpy_or_python_type\r\n        x = t.numpy()\r\n\r\n    AttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\n</p>\r\n</details>", "comments": ["@jwlawson ,  I think `tf.function` is not executed eagerly for performance purposes. So you need to remove the `@tf.function` decorator  for `.numpy()` to work. For your reference link of gist is [here](https://gist.github.com/khimraj/465004a32f5a64dbf4a78a2e441c4ac2).", "I guess my ticket wasn't clear. I know that this works when you remove the `tf.function` annotation, however I would expect it to work fine inside a `tf.function` as well.\r\n\r\nIn TF 2.1 you could use `Model.train_on_batch` inside a `tf.function`. Are you saying that this crash is now expected behaviour?\r\n\r\nIf the crash is now expected behaviour then it is a breaking change and should be documented (and should probably give a better error message than referencing a function deep inside keras).", "@jwlawson Thanks for the issue!\r\n\r\nModel.train_on_batch runs a single step of the Model.fit logic, it's a high-level endpoint that's not safe to run in a tf.function since it creates an Iterator, converts results to NumPy, etc. Probably the best thing to use is Model.train_step, which is just the logic of the training step and is safe to tf.function", "@jwlawson Can you please check @omalleyt12 comments and let us know whether it was resolved for you or not. Please close the issue if this was resolved for you. Thanks!", "I", "I'll close this, but I do think that there is a missing level of abstraction here that `train_on_batch` looked like it filled until these changes.\r\n\r\n* `model.fit` is very high level and handles everything for the user - including splitting up a dataset and sharding across distributed strategies etc\r\n* `model.train_step` is very low level and doesn't handle anything to do with strategies or datasets.\r\n\r\nWhat I wanted from `model.train_on_batch` was a middle ground that still handled the strategy sharding, but didn't require a dataset. It looked like this was the case prior to TF 2.2. If the only answer if to use the very low level function then as a user I need to care much more about the low level aspects that I would like keras to handle for me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37988\">No</a>\n"]}, {"number": 37987, "title": "Cadence: HiFi4 Neural Network (NN) source download and Fix issue with\u2026", "body": "\u2026 Softmax\r\n\r\nHiFi4 Neural Network (NN) source download\r\nFixed compilation issue with softmax.cc\r\n\r\nSigned-off-by: Prasad Nikam pnikam@cadence.com\r\nSigned-off-by: Niranjan Yadla nyadla@cadence.com", "comments": []}, {"number": 37986, "title": "[Bug] DLPack tensor seg faults when calling `tensor.device`", "body": "After https://github.com/tensorflow/tensorflow/pull/37944, the bug below happens. As I tested, if remove the added `TFE_DeleteContext(ctx);`, the codes below work fine. Do you have any idea what's the proper way to use TFE_Context here? @sanjoy @alextp \r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.constant([1])\r\ndlpack_arr = tf.experimental.dlpack.to_dlpack(a)\r\ntf_tensor = tf.experimental.dlpack.from_dlpack(dlpack_arr)\r\nprint(tf_tensor) # This works fine\r\nprint(tf_tensor.device) # Segment fault here. Works well without #37944 \r\n```\r\n\r\n\r\n**System information** \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 16.04\r\n- TF Version: latest nightly build or installed from source", "comments": ["Oh I completely misread the PR code. The code is broken.\r\n\r\nWh are we creating a new TFE_Context? We should pass the one from python in (so tf.experimental.dlpack.to_dlpack should capture context.context()._handle and pass it to the C++ function)", "Got it. I'm not familiar with tf codebases but just follows the code in unit tests when writing those functions. \ud83d\ude02"]}, {"number": 37985, "title": "Unable to successfully build a devel-cpu.Dockerfile for arm32v7/arm64v8", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 and 19.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 B (4 GB)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.1\r\n- Python version: 3.6 (Ubuntu 18.04) and 3.7 (Ubuntu 19.10)\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nThere are no crossbuild compile instructions for Raspberry Pi 4 armhf or aarch64--the latter being of much more interest--so I figured I'd follow the devel-cpu.Dockerfile (in tensorflow/tools/dockerfiles/dockerfiles) and devel-cpu-ppc64le.Dockerfile (in tensorflow/tools/dockerfiles/dockerfiles/ppc64le), and attempt to create both a devel-cpu-arm32v7.Dockerfile and devel-cpu-arm32v7.Dockerfile.  However, both fail to build.  Specifically, they fail at the building Bazel step.  I have attempted this from docker on an amd64 host using qemu, as well as from docker on arm32v7 and arm64v8, and finally directly on arm32v7 and arm64v8 without docker.  I have tried installing potential dependancies, i.e., pip install zypper, but still have had no success.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSee attachment for devel-cpu-arm64v8.Dockerfile.  Simply replace \"arm64v8\" with \"arm32v7\" to create the devel-cpu-arm32v7.Dockerfile.\r\n[devel-cpu-arm64v8.Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/4395428/devel-cpu-arm64v8.Dockerfile.txt)\r\n\r\ndocker build --build-arg UBUNTU_VERSION=19.10 --build-arg CHECKOUT_TF_SRC=1 --build-arg USE_PYTHON_3_NOT_2=1 -f ./devel-cpu-arm32v7.Dockerfile -t arm32v7/tensorflow:latest-devel-py3 .\r\ndocker build --build-arg UBUNTU_VERSION=19.10 --build-arg CHECKOUT_TF_SRC=1 --build-arg USE_PYTHON_3_NOT_2=1 -f ./devel-cpu-arm64v8.Dockerfile -t arm64v8/tensorflow:latest-devel-py3 .\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /bazel/src/BUILD:105:1: PythonZipper src/create_embedded_tools.zip failed (Exit 255)\r\nTarget //src:bazel_nojdk failed to build\r\nINFO: Elapsed time: 292.985s, Critical Path: 199.34s\r\nINFO: 438 processes: 438 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nERROR: Could not build Bazel\r\nThe command '/bin/sh -c cd /bazel &&     env BAZEL_JAVAC_OPTS=\"-J-Xmx2g -J-Xms200m\" EXTRA_BAZEL_ARGS=\"--host_javabase=@local_jdk//:jdk\" bash ./compile.sh' returned a non-zero code: 1\r\n", "comments": ["FYI, in a way creating a working devel-cpu Dockerfile is a prerequisite of [#36672](https://github.com/tensorflow/tensorflow/issues/36672).", "I managed to successfully build a devel-cpu Dockerfile for aarch64 and submitted the necessary changes as PR #39695", "@settle \r\nAs we have the latest stable version 2.6.0, Can you try building TF 2.6.0 and let us know if the issue still persists?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37985\">No</a>\n"]}, {"number": 37984, "title": "no detection on custom trained model", "body": "I have made a custom trained model using yolov2 and after 1500 iteration i tried to test it but no detection boxes the output image as the input any help please", "comments": ["@mhmedeng, One of reason may be that your model is not learned significantly in 1500 iteration. You can try decreasing threshold value for probability predictions and see the output image to check for any box.", "> @mhmedeng, One of reason may be that your model is not learned significantly in 1500 iteration. You can try decreasing threshold value for probability predictions and see the output image to check for any box.\r\n\r\nThank you for your reply and now my trained model raised to 3200 iterations, I tested it and decreased threshold to 0.01 but nothing happened.", "@mhmedeng, \r\nplease share tensorflow version and simple standalone code for us to replicate the issue", "> @mhmedeng,\r\n> please share tensorflow version and simple standalone code for us to replicate the issue\r\n\r\nthank you i solved the problem", "@mhmedeng,\r\nmoving this issue to closed status as its resolved"]}]