[{"number": 19076, "title": "why ldd  libtensorflow_cc.so can not find library related to tensorrt ?", "body": "I have build tensorflow c++ library from source code of tensorflow1.7-release vision, but when i ldd this so, i can not find any information about tenosrrt, why? And when i configure, i have designated the TRT library's path, and i use bazel to build ,the command is like this:\r\nbazel build --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @sgzqc, \r\n\r\nRight now tensorrt module behaves like a plugin and will become available to the system when you import the python module. In order to not to put it as a requirement on to the users who want to do pip install tensorflow-gpu. If you want to use TF-TRT in c++ you need to link your application to libraries produced by tensorrt package.\r\n\r\nSami\r\n", "@tensorflowbutler  So sorry for some information loss, the install environment is like below:\r\n\uff35\uff42\uff55\uff4e\uff54\uff55 14.04 + tensorflow1.7 + cuda8.0 + cudnnv7 + TensorRT3.0.4 + bazel10 + py2 + 1080Ti\r\n\r\nthe build command is like this:\r\ncd tensorflow-1.7.0\r\n./configure   at this step i have  designated the CUDA CUDNN  TRT path.\r\nbazel build --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so\r\n\r\nNow i have build libtensorflow_cc.so success, and i run the  cc example is also success! \r\nJust  as https://devblogs.nvidia.com/tensorrt-integration-speeds-tensorflow-inference/  , the TRT have integration into TF.   So i think libtensorflow_cc.so  should linked to the TRT library , and after i ldd this library the output is as below:\r\n![default](https://user-images.githubusercontent.com/15881893/39658299-ab61ae48-5044-11e8-835a-4670887b7224.png)\r\nAs you see above , we can see the cuda and cudnn library, however can not find TRT,  and why?\r\n\r\n", "@sgzqc,\r\n\r\nAs I mentioned above, tensorflow-trt integration is designed as plugin and not linked to tensorflow library, even if you do monolithic builds.\r\n\r\nSami\r\n", "@samikama  Thanks! If i want to use TF-TRT to do c++ inference, i should link both  two librarys as you say above?", "Yes, you need to link against the libraries in the contrib/tensorrt as well. converters for the conversion and kernels for the ops.", "I am closing this issue, since it's not a bug or feature request -- if you have additional installation or build issues, please ask a question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), which has more features to help everyone share problems and solutions. Thanks!\r\n\r\n@samikama, thanks for helping out!"]}, {"number": 19075, "title": "fix issue #18643 TensorFlow 1.8.0-rc1 fails on aarch64 platforms", "body": "fix issue #18643 TensorFlow 1.8.0-rc1 fails on aarch64 platforms\r\nReference to #18643 \r\n\r\nTested on arm64 Nvidia Jetson Tx2 and x64 ubuntu. Not sure this PR will effect other arm devices or not. Currently, tensorflow don't have aarch64 config option.", "comments": ["It seems this issue already get another fix \r\nhttps://github.com/tensorflow/tensorflow/commit/f0a506f67fe316c3adb282b58b7087e11d7c493f"]}, {"number": 19074, "title": "Tensorboard is down after upgrading to the tensorflow 1.8.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n\r\n- **Python version**: \r\n2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\nNo\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\ncuDNN 7.0\r\n\r\n- **GPU model and memory**:\r\nNvidia Geforece GTX 1080\r\n\r\n- **Exact command to reproduce**:\r\ntype \r\n\r\n> sudo pip install tensorboard\r\n\r\nand then get\r\n\r\n> tensorflow 1.4.1 requires tensorflow-tensorboard<0.5.0,>=0.4.0rc1, which is not installed.\r\n\r\nBut my tensorflow or tensorflow object detection api works.\r\n\r\n### Describe the problem\r\nCannot install tensorboard from pip.\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["Something seems amiss with your installation, since the error message says `tensorflow 1.4.1`, but it seems you're installing 1.8.0?\r\n\r\nCan you try upgrading both with:\r\n\r\n```sh\r\npip install --upgrade tensorflow tensorboard\r\n```\r\n", "Hello @asimshankar ,\r\n\r\nSorry for the late response.\r\n\r\nwhen I type `pip install --upgrade tensorflow tensorboard`,\r\n\r\nI got the following messages and the same error message:\r\n\r\n> Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python2.7/dist-packages (1.8.0)\r\nRequirement already up-to-date: tensorboard in /usr/local/lib/python2.7/dist-packages (1.8.0)\r\nRequirement not upgraded as not directly required: mock>=2.0.0 in /home/garmin/.local/lib/python2.7/site-packages (from tensorflow-gpu) (2.0.0)\r\nRequirement not upgraded as not directly required: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.11.0)\r\nRequirement not upgraded as not directly required: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.1.6)\r\nRequirement not upgraded as not directly required: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.14.3)\r\nRequirement not upgraded as not directly required: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement not upgraded as not directly required: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.0.post1)\r\nRequirement not upgraded as not directly required: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.2.0)\r\nRequirement not upgraded as not directly required: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.31.0)\r\nRequirement not upgraded as not directly required: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.11.0)\r\nRequirement not upgraded as not directly required: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.2.0)\r\nRequirement not upgraded as not directly required: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (3.5.2.post1)\r\nRequirement not upgraded as not directly required: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.6.2)\r\nRequirement not upgraded as not directly required: bleach==1.5.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.5.0)\r\nRequirement not upgraded as not directly required: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard) (3.2.0)\r\nRequirement not upgraded as not directly required: html5lib==0.9999999 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.9999999)\r\nRequirement not upgraded as not directly required: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (2.6.11)\r\nRequirement not upgraded as not directly required: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.14.1)\r\nRequirement not upgraded as not directly required: funcsigs>=1; python_version < \"3.3\" in /home/garmin/.local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu) (1.0.2)\r\nRequirement not upgraded as not directly required: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu) (4.0.2)\r\nRequirement not upgraded as not directly required: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow-gpu) (39.1.0)\r\ntensorflow 1.4.1 requires tensorflow-tensorboard<0.5.0,>=0.4.0rc1, which is not installed.\r\n\r\nwhen typing `python -c 'import tensorflow as tf; print(tf.__version__)'`\r\nit shows 1.8.0\r\n\r\nPlease let me know if any other information you need. Thank you.", "I'm confused about why the error message talks about TensorFlow 1.4.1.\r\nPerhaps you have conflicting installs for tensorflow and tensorflow-gpu?\r\nCould you try `pip list` to see what all you have installed and remove all TensorFlow packages and then reinstall?", "Hello @asimshankar ,\r\n\r\nThe 1.4.1 error is because I have tensorflow-gpu 1.8.0 and tensorflow 1.4.1 installed.\r\n\r\nAfter uninstalling tensorflow 1.4.1, the error is gone.\r\n\r\nAnd here is the output after `pip install --upgrade tensorflow-gpu tensorboard`\r\n\r\n> Requirement already up-to-date: tensorflow-gpu in /usr/local/lib/python2.7/dist-packages (1.8.0)\r\nRequirement already up-to-date: tensorboard in /usr/local/lib/python2.7/dist-packages (1.8.0)\r\nRequirement not upgraded as not directly required: mock>=2.0.0 in /home/garmin/.local/lib/python2.7/site-packages (from tensorflow-gpu) (2.0.0)\r\nRequirement not upgraded as not directly required: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.11.0)\r\nRequirement not upgraded as not directly required: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.1.6)\r\nRequirement not upgraded as not directly required: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.14.3)\r\nRequirement not upgraded as not directly required: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement not upgraded as not directly required: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.0.post1)\r\nRequirement not upgraded as not directly required: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.2.0)\r\nRequirement not upgraded as not directly required: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.31.0)\r\nRequirement not upgraded as not directly required: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (1.11.0)\r\nRequirement not upgraded as not directly required: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.2.0)\r\nRequirement not upgraded as not directly required: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (3.5.2.post1)\r\nRequirement not upgraded as not directly required: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu) (0.6.2)\r\nRequirement not upgraded as not directly required: bleach==1.5.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (1.5.0)\r\nRequirement not upgraded as not directly required: futures>=3.1.1; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorboard) (3.2.0)\r\nRequirement not upgraded as not directly required: html5lib==0.9999999 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.9999999)\r\nRequirement not upgraded as not directly required: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (2.6.11)\r\nRequirement not upgraded as not directly required: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard) (0.14.1)\r\nRequirement not upgraded as not directly required: funcsigs>=1; python_version < \"3.3\" in /home/garmin/.local/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu) (1.0.2)\r\nRequirement not upgraded as not directly required: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu) (4.0.2)\r\nRequirement not upgraded as not directly required: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow-gpu) (39.1.0)\r\n\r\nHowever,\r\n\r\nwhen trying to use tensorboard, I got \r\n\r\n> tensorboard: command not found\r\n\r\nIn /usr/local/lib/python2.7/dist-packages\r\n\r\nI have \r\n\r\n> tensorboard\r\ntensorboard-1.8.0.dist-info\r\ntensorflow\r\ntensorflow_gpu-1.8.0.dist-info\r\n\r\nAnd I cannot find main.py in tensorboard, like this [discussion feed](https://stackoverflow.com/questions/45095820/tensorboard-command-not-found?rq=1&utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa).\r\n\r\nAny suggestion?\r\n\r\nThank you for helping!", "Hello @asimshankar \r\n\r\nAfter\r\n\r\n1. \r\n\r\n> sudo pip uninstall tensorflow-gpu\r\n\r\n2. \r\n\r\n> pip install tensorflow-gpu\r\n\r\nBoth tensorflow and tensorboard work.\r\n\r\nThank you for helping."]}, {"number": 19073, "title": "Tensorflow-GPU costing 3130% cpu resouces not for computation", "body": "My host has 16 core cpu (with x2 hyper-thread), when I run resnet50 (weights & sync on gpu) benchmark for Tensorflow CUDA version, it actually not need CPU to do any heavy job since training and weights merge are done at gpu device. But it costs nearly full CPU resources not for computing but for long pulling queries on `cuEventQuery`, and 99% of `cuEventQuery` replies NOT_FINISHED, this is extremely bad for cpu utilization, and block all CPU cores from doing other tasks.\r\n\r\nAccording to my profiling, for Resnet50, Tensorflow costs only 14840 time of model forward and backward, but it gathers all threads to do full-time queries, and should cost 23564854 times to do useless `cuEventQuery` due to not ready, which is incredibly heavy and triggers 3130% CPU cost on Tensorflow GPU computational version.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "**Have I written custom code:** No\r\n**Source scripts:** https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks\r\n**Exact command to reproduce:** `./tf_cnn_benchmarks/tf_cnn_benchmarks.py --model resnet50 --batch_size 64 --num_batches 50000 --data_format=NCHW --local_parameter_device gpu --data_dir=/usr/imagenet`\r\n**OS Platform and Distribution:** Ubuntu 16.04 (64bit)\r\n**TensorFlow installed from:** pip3 install tensorflow-gpu==1.4.2\r\n**TensorFlow version:** 1.4.2\r\n**Bazel version:** pre-built from pip\r\n**CUDA/cuDNN version**: CUDA8 + CUDNN6\r\n**GPU model and memory:** Tesla P100 (x4GPU, each 16GMem) + NVlink", "Any comments?", "@reedwm , thoughts on the proper parameterization here?", "I cannot really reproduce. Adding a log on [this line](https://github.com/tensorflow/tensorflow/blob/714f3c4f2f901e865bfcbf830485adafb92dca48/tensorflow/stream_executor/cuda/cuda_driver.cc#L1053) and running for five steps, cuEventQuery was run only 10,000 times. When you said \"and should cost 23564854 times to do useless cuEventQuery due to not ready\", do you mean cuEventQuery was run 23564854 times? Was that over 50,000 steps?\r\n\r\nNote that the CPU should be doing a fair amount of work for the input pipeline. You can run without the input pipeline by not specifying `--data_dir`. Do you still have the problem even when you do not specify `--data_dir`?", "@reedwm Yes, if I do not specify --data-dir, the cpu occupation can be reduced, but it is meaningless when training a dataset. If I reduce the inter-thread parallel number, the cpu can be reduced as well but the model benchmark will be very bad.\r\n\r\nBy using 32-cpu inter-thread parallel number, I can get 800 image/sec using 4 P100 GPUs (costing 3130% cpus);\r\n\r\nHowever, if I use 8-cpu inter-thread parallel number, I can only get 370 image/sec using same 4 P100 GPUs (costing 800% cpus which is decent).", "The number of cuEventQuery depends on the GPU performance, if GPU is normal, the GPU function execution will be slow, which will highly increase the the amount of cuEventQuery.", "I am still not convinced the high CPU utilization is due to cuEventQuery. The CPU should be high utilized due to the input pipeline that is used when `--data_dir` is set. As you said, training is meaningless without `--data_dir`, but for real training, the CPU needs to do a lot of work for the input pipeline and so will have high utilization. Do you have any evidence that the CPU has high utilization because of cuEventQuery?\r\n\r\nI'm not sure what you mean by you last comment: \"The number of cuEventQuery depends on the GPU performance, if GPU is normal, the GPU function execution will be slow, which will highly increase the the amount of cuEventQuery.\" Can you clarify?\r\n\r\nAlso, @yzhwang, do you have any thoughts/comments?", "I just profiled the reason, and here is my explanation:\r\n\r\nBy default, the `num_inter_threads` is 0, and my machine has 32 cores, and I found that `num_inter_threads` actually selects 32 by tensorflow's recommendation. Thus during the whole benchmark of tf_cnn_benchmark, I found 41 threads created by tensorflow in total: 1 thread for boot, 8 thread for num_intra_threads, and 32 thread for num_inter_threads.\r\n\r\nHere we only consider 32 threads for num_inter_threads which is too heavy, and what I found is that every thread within these 32 num_inter_threads are occupied with cuEventQuery calls, which seems like everyone is eager to feed data to a available cuda stream as much as possible but every thread are doing cuEventQuery all the time, so this makes quantities of cpus are wasted on each thread calling cuEventQuery.\r\n\r\nThis issue can be slightly reduced by setting num_inter_threads to a small number manually since the default tensorflow recommendation is not well which makes ~3200% cpus occupation.\r\n\r\nBTW, I don't see CPU did a really heavy pipeline for input data expect for just a single batch of input data for each term of mini-batch which should be lite in terms of the number of calls of `cuMemcpyHtoDAsync`, so what makes CPU occupation 2800% larger?\r\n", "By using synthetic data source, the cuEventQuery is very lite, and CPU occupation is ~400%.\r\n\r\nAfter giving `--data-dir`, it actually uses JUST ~10 additional `cuMemcpyHtoDAsync` calls for every mini-batch which triggers 3200% cpu occupation and this is definitely not a expected thing.", "@reedwm \r\nBy using synthetic data source, here is the CUDA call counts:\r\n```sh\r\n...\r\ncuEventQuery  x 23957788 times\r\n...\r\ncuMemcpyHtoDAsync_v2 x 590 times\r\ncuMemcpyDtoHAsync_v2 x 60 times\r\ncuMemcpyDtoDAsync_v2 x 63300 times\r\n...\r\ncudnnBatchNormalizationBackward x 12720\r\n...\r\n```\r\nIt only costs ~400% cpus which is quick;\r\n\r\nHowever, by using real `--data-dir`, here is the CUDA call counts:\r\n```sh\r\n...\r\ncuEventQuery  x 22843932 times\r\n...\r\ncuMemcpyHtoDAsync_v2 x 830 times\r\ncuMemcpyDtoHAsync_v2 x 60 times\r\ncuMemcpyDtoDAsync_v2 x 63300 times\r\n...\r\ncudnnBatchNormalizationBackward x 12720\r\n...\r\n```\r\nIt displays that 99% CUDA calls are the same for 2 cases above, just except for `cuMemcpyHtoDAsync_v2`, in which the case of using `--data-dir` will have 240 (= 830 - 590) more of `cuMemcpyHtoDAsync_v2` (all these 240 additional calls are exactly 240 times of NCHW batch inputs for every mini-batch), but the CPU occupation increased to ~3200% CPUs. Can you explain what all these CPUs are doing in this case? e.g. Shuffling, argumentation? (even though, it still not necessary to cost so much CPU resources)\r\n", "@ghostplant this post explains some of the optimizations we have on input pipeline: https://www.tensorflow.org/performance/datasets_performance\r\n\r\n@mrry Could you comment on the high CPU occupation or point to some resource to help @ghostplant better understand? Thanks!", "As @reedwm already pointed out, the higher CPU utilization arises because there is more work for the CPU to do per batch, including file I/O, protocol buffer parsing, JPEG decoding, distortions, and so on.\r\n\r\nI'm not sure what your basis is for the statement \"it still not necessary to cost so much CPU resources\". Almost all of the low hanging fruit has been picked, although you might want to upgrade from TensorFlow 1.4.2 to TensorFlow 1.8, as the team has improved many of the operations in the input pipeline, and I'd expect CPU utilization to decrease with an upgrade. Nevertheless, the baseline for utilization will inherently be higher than the synthetic data case. ", "Thanks, I'll try on version 1.8.", "TF1.8 is actually even slower than TF1.4 to process tensorflow/benchmark:tf_cnn_bencmark.py.", "Nagging Assignee @reedwm: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks, all. I will close this PR. I think TF using CPU to do image preprocessing is limited by slow python runtime, and using an independent C-based preprocessor instead of TF runtime preprocessing solves my problem, not only faster but also less CPU occupation."]}, {"number": 19072, "title": "Branch 195301913", "body": "", "comments": []}, {"number": 19071, "title": "MobileNet v2 vs MobileNet v1 - bug with retrain.py and label_image.py examples from Tensorflow for Poets v2 transfer learning example!", "body": "### System information\r\nWe have reproduced this on Mac OSX, a windows box running a Linux VM, and on a Canadian research cluster all with the same result.\r\n - mac was v1.7 recompiled to take advantage of CPU: ('v1.7.0-1321-gd82b2f71b6', '1.7.0')\r\n - stock tensorflow code and examples\r\n\r\n### Description\r\nWe are using Tensorflow v1.7.  We train **MobileNet v2** (from tfHub - feature/1 version) by running the code Tensorflow provided in **retrain.py** (from examples). When training is done the validation set is run (for us about 700 images) and it reports say **80% validation accuracy**.   Then it saves the model.  We load the saved model using **label_image.py** (also from tensorflow examples) and see how it's doing on a small validation set that we've withheld.  Instead of seeing the validation of 80% roughly confirmed, we instead see a validation accuracy of about 35%.  The clincher is that when use the same code and data and instead use **MobileNet v1** we do see that training validation is confirmed by validation reported using label_image.py . \r\n\r\nWhen calling label_image we change the input_layer from \"input\" for MobileNet v1 to \"Placeholder\" for MobileNet v2\r\n\r\nCan you confirm this? What is going on?\r\n\r\n### Logs:\r\nFrom retrain.py:\r\n...\r\nINFO:tensorflow:Initialize variable module/MobilenetV2/expanded_conv_9/project/weights:0 from checkpoint /var/folders/x8/f10sgc052q75r1thg5hlw4dm0000gn/T/tfhub_modules/33f8428fe83945b8b3d46d79168a0e2818e65e8a/variables/variables with MobilenetV2/expanded_conv_9/project/weights\r\nINFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint\r\nINFO:tensorflow:Final test accuracy = 74.0% (N=696)\r\n...\r\nthen from using label_image.py:\r\n\r\nit gets 35% is correct\r\n\r\nHave I written custom code: no\r\nOS Platform and Distribution: macosx latest\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.7\r\nBazel version: Build label: 0.11.1-homebrew\r\n   Build target: bazel-out/darwin- \r\n   opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n   Build time: Sun May 31 16:57:39 +50150 (1520426998659)\r\n   Build timestamp: 1520426998659\r\n   Build timestamp as int: 1520426998659\r\n\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: see above\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "\u200bI've added it... But in fact all that information was there when I\r\noriginally posted, to the extent that it is relevant in this case.  Considering we're working on making more capable tech here, you are a bit disappointing... just sayin!\r\n\r\nThanks\u200b\r\n\r\nOn Fri, May 4, 2018 at 5:42 AM, Alfred Sorten Wolf <notifications@github.com\r\n> wrote:\r\n\r\n> Thank you for your post. We noticed you have not filled out the following\r\n> field in the issue template. Could you update them if they are relevant in\r\n> your case, or leave them as N/A? Thanks.\r\n> Have I written custom code\r\n> OS Platform and Distribution\r\n> TensorFlow installed from\r\n> TensorFlow version\r\n> Bazel version\r\n> CUDA/cuDNN version\r\n> GPU model and memory\r\n> Exact command to reproduce\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/19071#issuecomment-386588724>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AAQh9iDGbCwJK4d23BB5bnEL6ljpPzVaks5tvEysgaJpZM4Tx_WA>\r\n> .\r\n>\r\n", "I've been able to replicate this issue with TensorFlow 1.7 on a CentOS distribution running a 12 GB Tesla P100, CUDA version 8.0. Namely,  downloaded MobileNet V1 and V2 from their respective tfhub modules and retrained on roughly 7000 images using the retrain.py script, default parameters. Final test accuracy for both the v1 and v2 models is roughly 74%. When then going back and using label_image.py to validate on a separate set of roughly 20 images with the same properties as the original training set, the v1 model gives comparable performance (~75%), but the v2 model inexplicably drops to 33%.", "Any news from the front?  In case anyone cares, and I think y'all ought to, these files are used in the Flower Transfer Learning example also known as \"**Tensorflow for Poets v2**\"  ... You'd think it would be good if that example actually worked on tfhub hosted models...", "Hi, It has been 21 days with no activity and the awaiting response label was assigned. Is this still an issue?", "I too was wondering if how to adapt this to mobilenet v2, or even mobilenet v2-SSD", "@jgberg Is this still an open issue?", "yes\n\nOn Wed, Aug 1, 2018 at 1:07 PM achowdhery <notifications@github.com> wrote:\n\n> @jgberg <https://github.com/jgberg> Is this still an open issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19071#issuecomment-409703666>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAQh9sdI7Mr7jMqG0L3pCmOxoG25n_aSks5uMgp_gaJpZM4Tx_WA>\n> .\n>\n", "+1 interested in a fix for this - the MobileNetv2 speed performance seems much better than v1, but with this issue, the inference performance is just too low.", "Can confirm this is still an issue. Even validating on the very set of images the stock MobileNetV2 model is trained on leads to inexplicably lower accuracy. Is the problem in the validation scripts provided or the way the models are loaded?", "Same problem here", "My guess is that there's something wrong with the input image preprocessing in this case, in particular the input_mean and input_std values (though if it's working for v1, it should be similar for v2). Since this is now a bit old, closing, but please reopen with details on the exact command lines if you're still hitting the problem."]}, {"number": 19070, "title": "Fix minor typos", "body": "Just a couple minor typos ", "comments": []}, {"number": 19069, "title": "Variable initialization under estimator + dynamic_rnn + MirroredStrategy (DistributionStrategy)", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 and Windows 10\r\nTensorFlow installed from (source or binary): Binary (pip)\r\nTensorFlow version (use command below): 1.8.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: GTX 1080 8 GiB (Windows) or two 8 GiB Tesla M60 (Ubuntu 16.04)\r\nExact command to reproduce: See python code (below)\r\n\r\n**Describe the problem**\r\nI am trying to transition my TensorFlow sequence2sequence codebase (RNN) into using the Estimator API, using the new tf.contrib.distribute.DistributionStrategy API. My provided runnable code snippet (below) works for a tf.contrib.distribute.OneDeviceStrategy, but not for a tf.contrib.distribute.MirroredStrategy.\r\n\r\nThe problem lies with initialization of the variables associated with RNN layers and the way these variables are handled/initialized using dynamic_rnn. \r\nThe first tower seems to work fine, as it correctly uses the default callable initializer. The problem seems to be that the following towers are initialized directly with the tensors resulting from the initializers called on the first tower, which the dynamic_rnn (tf.nn.bidirectional_dynamic_rnn) API does not like. \r\n\r\nI have found three possible issues:\r\n\r\n- (Possibly major?) Unable to initialize RNN parameters/variables, beyond the first tower, using tf.nn.bidirectional_dynamic_rnn and tf.contrib.rnn.LSTMCell together with a tf.contrib.distribute.MirroredStrategy.\r\n-  (Minor) Optimizer API, such as tf.train.AdamOptimizer, seems to return a tensor containing the updated global_step tensor instead of a no_op, as it normally does, when running under a distribution strategy. (Seems this is an easy fix, e.g. by using tf.group (see below))\r\n- (Minor) Error message: Would it not be more meaningful if the below error message \"... use a lambda as the initializer\" said something along the lines of: \"... use a callable, e.g. an Initializer (tf.keras.initializers.Initializer) or a lambda, as the initializer\" ?\r\n\r\nIs there something i might have missed?\r\n \r\n**Source code / logs**\r\n\r\n    import tensorflow as tf\r\n\r\n    class RNNModel(object):\r\n    \r\n      def __init__(self, hparams):\r\n        return\r\n    \r\n      def __call__(self, features, labels, mode, params):\r\n    \r\n        inputs = features[0]\r\n    \r\n        print(inputs)\r\n    \r\n        cell_fw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\r\n          300)\r\n        cell_bw = tf.contrib.rnn.LSTMCell(  # tf.contrib.rnn.BasicLSTMCell\r\n          300)\r\n    \r\n        (outputs, output_states) = tf.nn.bidirectional_dynamic_rnn(\r\n          cell_fw,\r\n          cell_bw,\r\n          inputs,\r\n          sequence_length=tf.convert_to_tensor([2, 2]),\r\n          initial_state_fw=None,\r\n          initial_state_bw=None,\r\n          dtype=tf.float32,\r\n          parallel_iterations=None,\r\n          swap_memory=False,\r\n          time_major=False,\r\n          scope=None\r\n        )\r\n    \r\n        model_output = tf.concat(outputs, axis=-1)\r\n        mock_loss = 10 - tf.reduce_sum(model_output)\r\n    \r\n        train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mock_loss,\r\n                                                                             global_step=tf.train.get_or_create_global_step())\r\n        train_op = tf.group(train_op)\r\n    \r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n          return tf.estimator.EstimatorSpec(mode, loss=mock_loss, train_op=train_op)\r\n        elif mode == tf.estimator.ModeKeys.EVAL:\r\n          return tf.estimator.EstimatorSpec(mode, loss=mock_loss, eval_metric_ops=None)\r\n        elif mode == tf.estimator.ModeKeys.PREDICT:\r\n          predictions = {\r\n            'mock': 1,\r\n          }\r\n          return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n    \r\n    \r\n    def train(hparams):\r\n      model = RNNModel(hparams)\r\n    \r\n      distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\n      #distribution_strategy = tf.contrib.distribute.OneDeviceStrategy(tf.DeviceSpec(device_type=\"GPU\", device_index=0))\r\n    \r\n      checkpointing_config = tf.estimator.RunConfig(\r\n        save_checkpoints_secs=20 * 60,  # Save checkpoints every 20 minutes.\r\n        keep_checkpoint_max=10,  # Retain the 10 most recent checkpoints.\r\n        train_distribute=distribution_strategy\r\n      )\r\n    \r\n      estimator = tf.estimator.Estimator(\r\n        model_fn=model,\r\n        model_dir=\"out/\",\r\n        params=hparams,\r\n        config=checkpointing_config,\r\n      )\r\n    \r\n      def create_mock_dataset():\r\n        mock_data = tf.convert_to_tensor([[[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]],\r\n                                          [[0.0, 1.0, 2.0, 3.0, 4.0], [0.0, 1.0, 2.0, 3.0, 4.0]]])\r\n    \r\n        mock_data_set = tf.data.Dataset.from_tensors(mock_data)\r\n        mock_data_set = mock_data_set.map(lambda mock_data: (((mock_data,)), ()) )\r\n    \r\n        return mock_data_set\r\n    \r\n      num_train_steps = 100\r\n      estimator.train(create_mock_dataset, hooks=None,\r\n                      steps=num_train_steps)\r\n    \r\n    if __name__ == '__main__':\r\n      train({\"mock\": \"mock\"})\r\n\r\nThe code produces the following  output:\r\n```\r\nTraceback (most recent call last):\r\nFile \"C:\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n\"main\", mod_spec)\r\nFile \"C:\\Python36\\lib\\runpy.py\", line 85, in _run_code\r\nexec(code, run_globals)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 105, in \r\ntf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n_sys.exit(main(argv))\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 96, in main\r\nrun_main(FLAGS, default_hparams, train_fn, inference_fn, hparams_creator)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\run_seq2seq.py\", line 88, in run_main\r\ntrain_fn(hparams)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\train_estimator.py\", line 470, in s2s_train\r\nestimator.train(input_fn=train_input_fn, hooks=train_hooks,steps=num_train_steps)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 363, in train\r\nloss = self._train_model(input_fn, hooks, saving_listeners)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 841, in _train_model\r\nreturn self._train_model_distributed(input_fn, hooks, saving_listeners)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 884, in _train_model_distributed\r\nself.config)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 756, in call_for_each_tower\r\nreturn self._call_for_each_tower(fn, *args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 254, in _call_for_each_tower\r\ncoord.join(threads)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 389, in join\r\nsix.reraise(*self._exc_info_to_raise)\r\nFile \"C:\\Python36\\lib\\site-packages\\six.py\", line 693, in reraise\r\nraise value\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\nyield\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 465, in run\r\nself.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 831, in _call_model_fn\r\nmodel_fn_results = self._model_fn(features=features, **kwargs)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\s2s_base_model.py\", line 157, in call\r\nres = self.build_graph(hparams, features, labels)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 83, in build_graph\r\nencoded_outputs, encoded_state = self._build_encoder(hparams, features)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 202, in _build_encoder\r\nreturn self._build_flat_encoder(hparams, features)\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 250, in _build_flat_encoder\r\nnum_bi_residual_layers=num_bi_residual_layers))\r\nFile \"C:\\Users\\marhl\\nmt\\tfDeepNLP\\models\\seq2seq\\hierarchical_model.py\", line 768, in _build_bidirectional_rnn\r\ntime_major=self.time_major)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 412, in bidirectional_dynamic_rnn\r\ntime_major=time_major, scope=fw_scope)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 627, in dynamic_rnn\r\ndtype=dtype)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 824, in _dynamic_rnn_loop\r\nswap_memory=swap_memory)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3224, in while_loop\r\nresult = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2956, in BuildLoop\r\npred, body, original_loop_vars, loop_vars, shape_invariants)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2893, in _BuildLoop\r\nbody_result = body(*packed_vars_for_body)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3194, in \r\nbody = lambda i, lv: (i + 1, orig_body(*lv))\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 793, in _time_step\r\nskip_conditionals=True)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 248, in _rnn_step\r\nnew_output, new_state = call_cell()\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 781, in \r\ncall_cell = lambda: cell(input_t, state)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 232, in call\r\nreturn super(RNNCell, self).call(inputs, state)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 717, in call\r\noutputs = self.call(inputs, *args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1292, in call\r\ncur_inp, new_state = cell(cur_inp, cur_state)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1099, in call\r\noutput, new_state = self._cell(inputs, state, scope=scope)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 339, in call\r\n*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 699, in call\r\nself.build(input_shapes)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 767, in build\r\npartitioner=maybe_partitioner)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 546, in add_variable\r\npartitioner=partitioner)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable.py\", line 436, in _add_variable_with_custom_getter\r\n**kwargs_for_getter)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1317, in get_variable\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1079, in get_variable\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 417, in get_variable\r\nreturn custom_getter(**custom_getter_kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1720, in wrapped_custom_getter\r\n*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 235, in _rnn_get_variable\r\nvariable = getter(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 575, in disable_partitioned_variables\r\nreturn getter(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\r\nuse_resource=use_resource, constraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 786, in _get_single_variable\r\nuse_resource=use_resource)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2220, in variable\r\nuse_resource=use_resource)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \r\nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\shared_variable_creator.py\", line 69, in create_new_variable\r\nv = next_creator(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2198, in \r\nreturn lambda **kwargs: captured_getter(captured_previous, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 568, in creator_with_resource_vars\r\nreturn self._create_variable(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 118, in _create_variable\r\nv = next_creator(*args, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2210, in \r\nprevious_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2182, in default_variable_creator\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 282, in init\r\nconstraint=constraint)\r\nFile \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 421, in _init_from_args\r\n\"initializer.\" % name)\r\nValueError: Initializer for variable encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel/replica_1/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\r\n```", "comments": ["/CC @josh11b", "@josh11b Simply delaying the initialization in MirroredStrategy seems to work for the above given code snippet:\r\n\r\n```\r\nwith tape.stop_recording():\r\n  index = {}\r\n  for i, d in enumerate(devices):\r\n    with ops.device(d):\r\n      if i > 0:\r\n        # Give replicas meaningful distinct names:\r\n        var0name = index[devices[0]].name.split(\":\")[0]\r\n        kwargs[\"name\"] = \"%s/replica_%d\" % (var0name, i)\r\n        # Initialize replicas with the same value:\r\n        if context.executing_eagerly():\r\n          initial_value = array_ops.identity(index[devices[0]].value())\r\n        else:\r\n          def initial_value_fn():\r\n            with ops.device(d):\r\n              initial_value = index[devices[0]].initial_value\r\n              return array_ops.identity(initial_value)\r\n\r\n          initial_value = initial_value_fn\r\n\r\n        kwargs[\"initial_value\"] = initial_value\r\n      with context.context().device_policy(context.DEVICE_PLACEMENT_SILENT):\r\n        v = next_creator(*args, **kwargs)\r\n      assert not isinstance(v, values.DistributedVariable)\r\n      index[d] = v\r\n```\r\nIt did however expose another problem. The default accumulation method (tf.add_n) does not seem to support sparse gradients (i.e. IndexedSlices), which I guess is the product of me using tf.nn.sparse_softmax_cross_entropy_with_logits as my error function or my tf.gather operations. Will investigate further.", "Seems like there should be a type check for IndexedSlices somewhere and that the default assumption that add_n can be used for all types of gradient tensors is wrong.", "It appears that IndexedSlices has not been considered throughout the AllReduce code of MirroredStrategy.\r\nWhat would be the best possible way to accumulate IndexedSlices gradients? Seems there are other \"hacks\" regarding IndexedSlices in the code base: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py)", "@guptapriya @isaprykin @yuefengz", "Ok, I managed to cook up a hack/workaround, along with the previously mentioned changes to MirroredStrategy, for the IndexedSlices issue and now my code is running/training. It will use up a lot of memory for IndexedSlices with large dense shapes and it uses non-public API (I feel dirty), but it works and it is kinda similar to what happens in /tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py.\r\n\r\nThe hack consists of the following changes to tensorflow/tensorflow/contrib/distribute/python/cross_tower_ops.py:\r\n\r\n```\r\nfrom tensorflow.python.ops import gradients_impl\r\n\r\ndef choose_the_best(devices, session_config=None):\r\n  \"\"\"Find the best subclass of CrossTowerOps given a tensorflow session.\r\n\r\n  Args:\r\n    devices: a list of devices passed for distribute strategy.\r\n    session_config: a tensorflow session config or None. If None, it will make\r\n      deciesion based on all local devices.\r\n\r\n  Returns:\r\n    a subclass of CrossTowerOps.\r\n  \"\"\"\r\n\r\n  def default_accumulation_fn(inputs):\r\n\r\n    first_element = inputs[0]\r\n    if isinstance(first_element, ops.IndexedSlices):\r\n      return math_ops.add_n(list(map(gradients_impl._IndexedSlicesToTensor, inputs)))\r\n    else:\r\n      return math_ops.add_n(inputs)\r\n\r\n  requested_devices = set([device_util.canonicalize(d) for d in devices])\r\n  machine_devices = device_lib.list_local_devices(session_config=session_config)\r\n  using_devices = []\r\n  for d in machine_devices:\r\n    if device_util.canonicalize(d.name) in requested_devices:\r\n      using_devices.append(d)\r\n    else:\r\n      logging.info(\r\n        \"Device is available but not used by distribute strategy: %s\", d.name)\r\n\r\n  if len(using_devices) != len(requested_devices):\r\n    logging.warning(\"Not all devices in distribute strategy are visible by \"\r\n                    \"TensorFlow sessions.\")\r\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\r\n\r\n  if any([d.device_type.lower() != \"gpu\" for d in using_devices]):\r\n    logging.warning(\"Not all devices in DistributionStrategy are visible to \"\r\n                    \"TensorFlow session.\")\r\n    return ReductionToOneDeviceCrossTowerOps(accumulation_fn=default_accumulation_fn)\r\n\r\n  device_links = [[] for _ in range(len(using_devices))]\r\n  for i, device in enumerate(using_devices):\r\n    for link in device.locality.links.link:\r\n      device_links[i].append(link.device_id)\r\n\r\n  return _choose_all_reduce_algorithm(device_links)\r\n\r\n```\r\n\r\nI hope you will find it useful!\r\n\r\nEDIT: I think this might only work because I am testing on a machine with only one GPU. I will need to make changes to AllReduceCrossTowerOps as well I guess. Not sure if I would have the same problems with IndexedSlices if I ran my code on an actual multi GPU machine.", "Thanks for reporting the 2 issues Martin, and also the suggested solution - super helpful!\r\nWe will look into both the issues (initializer and IndexedSlices support). \r\n\r\n", "I believe we've addressed both the issues, I will go ahead and close the issue. @marhlder thanks again, and please let us know if this addresses your concerns. ", "Thx for the update, it looks great! Looking forward to train on multiple GPUs using the Estimator API."]}, {"number": 19068, "title": " Could not find a version that satisfies the requirement tensorflow (from versions: ) No matching distribution found for tensorflow", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["What is the issue?", "I got the same error message when installing tensorflow with the newest version of python (3.6.5) and the newest version of pip (10.0.1).", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19067, "title": "Fix libpng compilation on PowerPC", "body": "", "comments": ["ping @protoget ", "Looks like a fixed version of this was merged. Please  let me know if we still need this PR. We can reopen it if you can make the requested changes."]}, {"number": 19066, "title": "Segfault Custom Op using KenLM", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: TitanX\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I use KenLM in my C++ test suite it runs just fine but when I try to use it from inside a custom op I get a Segmentation Fault. Any help would be greatly appreciated. Thx!\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```c\r\n#include \"lm/model.hh\"\r\n\r\n#include \"ctc_beam.hh\"\r\n\r\n#ifndef CTC_SCORER_HH\r\n#define CTC_SCORER_HH\r\n\r\nclass CTCBeamScorer {\r\n    public:\r\n    CTCBeamScorer(std::string lm_path, float alpha, float beta) : alpha_(alpha), beta_(beta) {\r\n        model_ = lm::ngram::LoadVirtual(lm_path.c_str());\r\n        vocabulary_ = &model_->BaseVocabulary();\r\n    }\r\n    ~CTCBeamScorer(void) {\r\n        delete model_;\r\n    }\r\n\r\n    void expand_beam(const CTCBeam* beam, int from_label, int to_label) { }\r\n\r\n    void expand_beam_end(CTCBeam* beam) const { }\r\n\r\n    float get_beam_score(const CTCBeam* beam) const { return 0.0f; }\r\n\r\n    void initialize_beam(CTCBeam* beam) const { }\r\n\r\n    private:\r\n    const float alpha_ = 0.0;\r\n    const float beta_ = 0.0;\r\n    lm::base::Model* model_ = nullptr;\r\n    const lm::base::Vocabulary* vocabulary_ = nullptr;\r\n};\r\n\r\n#endif // CTC_SCORER_HH\r\n```\r\n*THIS WORKS*\r\n```c\r\n#include <gtest/gtest.h>\r\n\r\n#include \"../src/ctc_scorer.hh\"\r\n\r\nTEST(ctc_scorer_tests, score) {\r\n    CTCBeamScorer* scorer = nullptr;\r\n    try {\r\n        scorer = new CTCBeamScorer(\"/tmp/test.mmap\", 0.0f, 0.0f);\r\n        ASSERT_FALSE(scorer == nullptr);\r\n    } catch(const std::exception &e) {\r\n        std::cerr << e.what() << std::endl;\r\n    }\r\n\r\n    delete scorer;\r\n}\r\n```\r\n\r\n```\r\ng++ -Wall -O3 -std=c++11 -g -I/home/thomas/projects/thomas/ops/thirdparty/kenlm -DKENLM_MAX_ORDER=6 -DHAVE_ZLIB=1 -DHAVE_BZLIB=1 -DHAVE_XZLIB=1 -o tests/test_runner tests/test_runner.cc tests/ctc_scorer_tests.cc src/ctc_scorer.cc thirdparty/kenlm/util/string_piece.o thirdparty/kenlm/util/exception.o thirdparty/kenlm/util/spaces.o thirdparty/kenlm/util/pool.o thirdparty/kenlm/util/file_piece.o thirdparty/kenlm/util/usage.o thirdparty/kenlm/util/parallel_read.o thirdparty/kenlm/util/bit_packing.o thirdparty/kenlm/util/float_to_string.o thirdparty/kenlm/util/ersatz_progress.o thirdparty/kenlm/util/integer_to_string.o thirdparty/kenlm/util/mmap.o thirdparty/kenlm/util/murmur_hash.o thirdparty/kenlm/util/read_compressed.o thirdparty/kenlm/util/scoped.o thirdparty/kenlm/util/file.o thirdparty/kenlm/lm/value_build.o thirdparty/kenlm/lm/config.o thirdparty/kenlm/lm/model.o thirdparty/kenlm/lm/trie.o thirdparty/kenlm/lm/sizes.o thirdparty/kenlm/lm/search_hashed.o thirdparty/kenlm/lm/lm_exception.o thirdparty/kenlm/lm/virtual_interface.o thirdparty/kenlm/lm/binary_format.o thirdparty/kenlm/lm/trie_sort.o thirdparty/kenlm/lm/vocab.o thirdparty/kenlm/lm/search_trie.o thirdparty/kenlm/lm/read_arpa.o thirdparty/kenlm/lm/quantize.o thirdparty/kenlm/lm/bhiksha.o thirdparty/kenlm/util/double-conversion/strtod.o thirdparty/kenlm/util/double-conversion/fast-dtoa.o thirdparty/kenlm/util/double-conversion/double-conversion.o thirdparty/kenlm/util/double-conversion/cached-powers.o thirdparty/kenlm/util/double-conversion/bignum.o thirdparty/kenlm/util/double-conversion/fixed-dtoa.o thirdparty/kenlm/util/double-conversion/bignum-dtoa.o thirdparty/kenlm/util/double-conversion/diy-fp.o -lgtest -lpthread -lboost_program_options -lboost_system -lboost_thread -lz -lbz2 -llzma\r\n```\r\n\r\n*THIS DOES NOT WORK*\r\n```c\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\n#include \"ctc_scorer.hh\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"CTCKenLMBeamSearchDecoder\")\r\n    .Input(\"scores: float32\")\r\n    .Output(\"output: float32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {\r\n      context->set_output(0, context->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass CTCKenLMBeamSearchDecoderOp : public OpKernel {\r\n    public:\r\n    explicit CTCKenLMBeamSearchDecoderOp(OpKernelConstruction* context) : OpKernel(context) {\r\n        CTCBeamScorer* scorer = nullptr;\r\n        try {\r\n            scorer = new CTCBeamScorer(\"/tmp/test.mmap\", 0.0f, 0.0f);\r\n        } catch(const std::exception &e) {\r\n            std::cerr << e.what() << std::endl;\r\n        }\r\n\r\n        delete scorer;\r\n    }\r\n\r\n    ~CTCKenLMBeamSearchDecoderOp(void) {\r\n        \r\n    }\r\n\r\n    void Compute(OpKernelContext* context) override {\r\n        // Grab the input tensor\r\n        const Tensor& scores_tensor = context->input(0);\r\n        auto scores = scores_tensor.tensor<float, 3>();\r\n\r\n        // Create an output tensor\r\n        Tensor* output_tensor = NULL;\r\n        OP_REQUIRES_OK(context, context->allocate_output(0, scores_tensor.shape(), &output_tensor));\r\n        auto output = output_tensor->tensor<float, 3>();\r\n\r\n        // Copy the scores to the output.\r\n        const auto scores_shape = scores_tensor.shape();\r\n        for (unsigned int x_idx = 0; x_idx < scores_shape.dim_size(0); x_idx++) {\r\n            for (unsigned int y_idx = 0; y_idx < scores_shape.dim_size(1); y_idx++) {\r\n                for (unsigned int z_idx = 0; z_idx < scores_shape.dim_size(2); z_idx++) {\r\n                    output(x_idx, y_idx, z_idx) = scores(x_idx, y_idx, z_idx);\r\n                }\r\n            }\r\n        }\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"CTCKenLMBeamSearchDecoder\").Device(DEVICE_CPU), CTCKenLMBeamSearchDecoderOp);\r\n```\r\n\r\n```\r\ng++ -Wall -O3 -std=c++11 -g -I/home/thomas/projects/thomas/ops/thirdparty/kenlm -DKENLM_MAX_ORDER=6 -DHAVE_ZLIB=1 -DHAVE_BZLIB=1 -DHAVE_XZLIB=1 -I/usr/local/lib/python3.5/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -shared -o libs/ctc_decoder.so src/ctc_decoder_op.cc src/ctc_scorer.cc thirdparty/kenlm/util/string_piece.o thirdparty/kenlm/util/exception.o thirdparty/kenlm/util/spaces.o thirdparty/kenlm/util/pool.o thirdparty/kenlm/util/file_piece.o thirdparty/kenlm/util/usage.o thirdparty/kenlm/util/parallel_read.o thirdparty/kenlm/util/bit_packing.o thirdparty/kenlm/util/float_to_string.o thirdparty/kenlm/util/ersatz_progress.o thirdparty/kenlm/util/integer_to_string.o thirdparty/kenlm/util/mmap.o thirdparty/kenlm/util/murmur_hash.o thirdparty/kenlm/util/read_compressed.o thirdparty/kenlm/util/scoped.o thirdparty/kenlm/util/file.o thirdparty/kenlm/lm/value_build.o thirdparty/kenlm/lm/config.o thirdparty/kenlm/lm/model.o thirdparty/kenlm/lm/trie.o thirdparty/kenlm/lm/sizes.o thirdparty/kenlm/lm/search_hashed.o thirdparty/kenlm/lm/lm_exception.o thirdparty/kenlm/lm/virtual_interface.o thirdparty/kenlm/lm/binary_format.o thirdparty/kenlm/lm/trie_sort.o thirdparty/kenlm/lm/vocab.o thirdparty/kenlm/lm/search_trie.o thirdparty/kenlm/lm/read_arpa.o thirdparty/kenlm/lm/quantize.o thirdparty/kenlm/lm/bhiksha.o thirdparty/kenlm/util/double-conversion/strtod.o thirdparty/kenlm/util/double-conversion/fast-dtoa.o thirdparty/kenlm/util/double-conversion/double-conversion.o thirdparty/kenlm/util/double-conversion/cached-powers.o thirdparty/kenlm/util/double-conversion/bignum.o thirdparty/kenlm/util/double-conversion/fixed-dtoa.o thirdparty/kenlm/util/double-conversion/bignum-dtoa.o thirdparty/kenlm/util/double-conversion/diy-fp.o -lboost_program_options -lboost_system -lboost_thread -lz -lbz2 -llzma -L/usr/local/lib/python3.5/dist-packages/tensorflow -ltensorflow_framework\r\n```\r\n", "comments": ["https://github.com/kpu/kenlm/issues/154", "Please refer to https://github.com/kpu/kenlm/issues/154 for the solution."]}, {"number": 19065, "title": "INTEL MKL: Fix concat related issues", "body": "Fix the following concat related issue (Mkl_Concat Op)\r\n \u2022\tInputs with both TF and MKL layouts  (ref public escalation tensorflow/tensorflow#17494)\r\n\u2022\tInputs with one or more EMPTY tensor(s), that is tensors with ndims==0\r\n\u2022\tIn case of ALL tensors are in MKL layout but with different data formats, , \u201creorder\u201d to \r\n        the most common format for better performance.\r\n", "comments": ["I have done refactoring based on PR code review suggestions, and\r\ncommitted changes to Intel public TF GitHub  \r\n         https://Intel-tensorflow/tensorflow.git   (branch: concat_fix)\r\nThanks!\r\n", "Hi Rasmus and Tatiana, \r\nThank you very much for the code review and valuable suggestions for code refinement. \r\nI am preparing for 4 new PRs related to MKL primitive reuse (conv2d, relu, batchnorm and pooling),\r\nfor which I have done refactoring  based on Rasmus's suggestions on first primitive reuse PR (conv fwd). I look forward to working with you soon.  Regards!   Guozhong (gzmkl)."]}, {"number": 19064, "title": "Update learning.py", "body": "Changed description in learning.py . ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Fixes #19043", "@caisq  Hi , changes are approved but still I am getting a required check for Ubuntu CC. Not sure what does it mean ?\r\n", "@yogiadi PR merged. Thanks.", "@caisq  thx :-) "]}, {"number": 19063, "title": "slim.conv2d_transpose  has no output_shape", "body": "slim.conv2d_transpose  has no output_shape param, So I can't control the output_shape", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19062, "title": "train_and_evaluate does not preserve the Dataset iterator state across train/eval", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nWhen the input function is based on the one-shot dataset iterator, the training phase always starts from the beginning of the iterator. That is, the iterator state gets reset in between the train/eval phases. Therefore, if the dataset is big enough, then the training would only see a subset of the data, which can be processed in `eval_spec.throttle_secs`. \r\n\r\nI think the issue is caused by the fact that the graph is persisted before transitioning to the next phase, and restored upon reentering training. However, I find the behaviour a bit counterintuitive, so if it is not a bug, it should be mentioned in the `train_and_evaluate` docs.\r\n\r\n### Source code / logs\r\n\r\nHere is a small example demonstrating the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef input_fn(data):\r\n    dataset = tf.data.Dataset.from_tensor_slices(data)\r\n    dataset = dataset.batch(batch_size=1)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return {\"x\": tf.Print(x, [x])}, x\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = tf.estimator.LinearRegressor(feature_columns=[\r\n        tf.feature_column.numeric_column(\"x\")\r\n    ])\r\n\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn=lambda: input_fn(list(range(2**20))),\r\n        max_steps=2)\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=lambda: input_fn([42]),\r\n        steps=1,\r\n        start_delay_secs=1,\r\n        throttle_secs=1)\r\n\r\n    tf.logging.set_verbosity(\"INFO\")\r\n    tf.train.create_global_step()\r\n    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n```\r\n\r\nThe code produces the following log output (I've omitted irrelevant lines):\r\n\r\n```\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 1 secs (eval_spec.throttle_secs) or training is finished.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n[0]\r\nINFO:tensorflow:Saving checkpoints for 1 into /var/folders/wr/s7brqkzj74v4pmdwkwn19321l34xg2/T/tmpgtq1ap9_/model.ckpt.\r\nINFO:tensorflow:loss = 0.0, step = 1\r\nINFO:tensorflow:Loss for final step: 0.0.\r\n...\r\nINFO:tensorflow:Starting evaluation at 2018-05-03-16:21:51\r\n...\r\nINFO:tensorflow:Finished evaluation at 2018-05-03-16:21:51\r\n...\r\nINFO:tensorflow:Restoring parameters from /var/folders/wr/s7brqkzj74v4pmdwkwn19321l34xg2/T/tmpgtq1ap9_/model.ckpt-1\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n[0]\r\nINFO:tensorflow:Saving checkpoints for 2 into /var/folders/wr/s7brqkzj74v4pmdwkwn19321l34xg2/T/tmpgtq1ap9_/model.ckpt.\r\nINFO:tensorflow:loss = 0.0, step = 2\r\nINFO:tensorflow:Loss for final step: 0.0.\r\n...\r\n```\r\n", "comments": ["Relates to #15448.", "Just today I came across this problem, and I can't think of a workaround. Also, I think this is also related to #13895.", "@JuanjoAlegria the workaround is to run the TF cluster on a single machine with just two tasks: chief doing the training, and evaluator doing solely the evaluation. Each task should be started in a separate process, e.g. via the [`ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor). \r\n\r\n**Update**: I suspect that multiprocessing is not required here, `ThreadPoolExecutor` will probably do just fine as well.", "Is there any news about this issue? I agree with @superbobry that, if this is not a bug, at least is an issue that should be highlighted in the docs.\r\n\r\nThanks!", "This caused also problems to the cache. Check https://github.com/tensorflow/tensorflow/issues/18266", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @superbobry \r\nWe have updated the behavior of train_and_evaluate. Now train is called only once. So you should not have cache problem in train input-fn.\r\nHaving said that eval input-fn will be called for every evaluate.", "@ispirmustafa So also  https://github.com/tensorflow/tensorflow/issues/18266 is it solved?", "It solves the train input fn part, but not evaluate.\n\nOn Mon, Jun 25, 2018 at 5:01 PM bhack <notifications@github.com> wrote:\n\n> @ispirmustafa <https://github.com/ispirmustafa> So also #18266\n> <https://github.com/tensorflow/tensorflow/issues/18266> is it solved?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-400131932>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASZl7a7YTWpKYL_FpGLVV3Oa9Tfo5Erpks5uAXnYgaJpZM4TxZax>\n> .\n>\n", "@ispirmustafa is there a plan also for evaluate?", "Thanks @ispirmustafa, that is a welcome change. Looks like this documentation should be updated then:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3edb609926f2521c726737fc1efeae1572dc6581/tensorflow/python/estimator/training.py#L280-L284", "Thanks for looking into this, @ispirmustafa! Am I right that the training graph is no longer persisted in local mode? If not, how did you solve iterator persistence issue? \r\n\r\nAlso, for future reference, here's the commit fixing the train part: 3edb609926f2521c726737fc1efeae1572dc6581.", "Hi @guillaumekln , Yes that should be updated. we'll do that", "Hi @bhack ,\r\nFor evaluate we don't have any plan to change graph re-creation.\r\n", "Hi @superbobry, \r\nI guess you mean whether we keep same graph with multiple train graph by 'persisting'.\r\nNo we're not doing that. We call train only once. We do evaluation via a hook (listener)", "@ispirmustafa yes, thanks for clarifying this. I think we can close the issue once the documentation clearly reflects the evaluation graph re-creation.\r\n\r\nOne final question: is there a reason for not caching the evaluation graph in the hook? i.e. do we really need to recreate it?", "Keeping evaluation graph needs to hijack existing estimator API.\n\nOn Wed, Jun 27, 2018 at 3:02 AM Sergei Lebedev <notifications@github.com>\nwrote:\n\n> @ispirmustafa <https://github.com/ispirmustafa> yes, thanks for\n> clarifying this. I think we can close the issue once the documentation\n> clearly reflects the evaluation graph re-creation.\n>\n> One final question: is there a reason for not caching the evaluation graph\n> in the hook? i.e. do we really need to recreate it?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-400614504>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASZl7ejW6wjWGIaeFKlSN21V-UUL6gGdks5uA1grgaJpZM4TxZax>\n> .\n>\n", "I'm sorry, I think I miss context to understand what needs to be hijacked. ", "The way we keep training graph is that we call estimator.train only once.\nWe need to call evaluation repeatedly (for each checkpoint). So if we want\nto keep the same graph we need to access the graph created within\nestimator.evaluate. Estimator does recreate graph for every call of\nevaluate/train by design.\n\nOn Thu, Jun 28, 2018 at 1:23 PM Sergei Lebedev <notifications@github.com>\nwrote:\n\n> I'm sorry, I think I miss context to understand what needs to be hijacked.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19062#issuecomment-401160323>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASZl7RVHU86pawtRr7KuYmCjIlJB3F1gks5uBTtEgaJpZM4TxZax>\n> .\n>\n", "Nagging Assignee @ispirmustafa: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ispirmustafa: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Am I  understanding this right? Suppose my input_fn returns a `tf.data.TextLineDataset(filename).map (lamda line: tf.decode_csv(line))` that references a csv with, say, 1,000,000 lines, and it shuffles with `dataset.shuffle(buffer_size=256)`. If the evaluation runs after a few hundred steps of training, when training resumes, will the input_fn used in training start reading thr cvs from where it left off before doing the evaluation, or will it start over from the first line?\r\n\r\nFrom the explanation above, it sounds like the latest version of train_and_eval doesn't interupt the input_fn used in training (or somehow preserves it) during eval, and thus the input_fn keeps reading the long file line by line independently. Is this the case?", "@formigone your statement is correct. train input_fn will not be impacted by eval. this is the case for tf.version>=1.9"]}, {"number": 19061, "title": "Tensorflow serving pages missing on deploy page", "body": "From the [TensorFlow Deploy page](https://www.tensorflow.org/deploy/), the links for TensorFlow serving all give 404 page not found errors. \r\n\r\nFor example, try the [Installation page](https://www.tensorflow.org/setup)\r\n\r\nFor example, try the [Serving a TensorFlow Model](https://www.tensorflow.org/serving_basic)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 21 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Looks like the links have been updated.  They work now.  \r\n\r\nThanks."]}, {"number": 19060, "title": "data.prefetch_to_device does not work with tf.data.Iterator.from_structure()", "body": "it works with make_one_shot_iterator(), I can see that the training time is 2x lower, when using the make_initializable_iterator() the time is the same as without prefetching to device.\r\nhow to use the new feature when dataset switch (train/val/test) is needed for evaluation purpose?\r\ncode parts\r\n//not working\r\ntrainDataset = tf.data.Dataset.from_tensor_slices((trainFeatures,trainLabels,trainLengths,trainMasks))\r\ntrainDataset = trainDataset.batch(batchSize)\r\ntrainDataset = trainDataset.apply(tf.contrib.data.shuffle_and_repeat(100,nEpochs))\r\niterator = tf.data.Iterator.from_structure(trainDataset.output_types, trainDataset.output_shapes)\r\ntrain_init_op = iterator.make_initializer(trainDataset)\r\ntrainDataset = trainDataset.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))\r\n\r\n//working\r\ntrainDataset = tf.data.Dataset.from_tensor_slices((trainFeatures,trainLabels,trainLengths,trainMasks))\r\ntrainDataset = trainDataset.batch(batchSize)\r\ntrainDataset = trainDataset.apply(tf.contrib.data.shuffle_and_repeat(100,nEpochs))\r\ntrainDataset = trainDataset.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))\r\niterator = trainDataset.make_one_shot_iterator()", "comments": ["Rohan: Can you please take a look at whether it'd be possible to support this?", "Actually, let's close this as a duplicate of #18947 and follow up there."]}, {"number": 19059, "title": "wrong", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 19058, "title": "CNN features feed to LSTM Tensorflow", "body": "Hello Guys\r\nSo recently i am working on a project which i am supposed to take images as input to a CNN and extract the features and feed them to LSTM for training. I am using 2 Layer CNN for feature extraction and im taking the features form fully connected layer and trying to feed them to LSTM. Problem is when i want to feed the FC layer to LSTM as input i get error regarding to wrong dimension. my FC layer is a Tensor with (128,1024) dimension. I tried to reshape it like this tf.reshape(fc,[-1]) which gives me a tensor ok (131072, )\r\ndimension and still wont work. Could anyone give me any ideas of how im suppose to feed the FC to LSTM?here i just write part of my code and teh error i get.\r\n# Convolution Layer with 32 filters and a kernel size of 5\r\n        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\r\n        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\r\n        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\r\n\r\n        # Convolution Layer with 32 filters and a kernel size of 5\r\n        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\r\n        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\r\n        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\r\n\r\n        # Flatten the data to a 1-D vector for the fully connected layer\r\n        fc1 = tf.contrib.layers.flatten(conv2)\r\n\r\n        # Fully connected layer (in contrib folder for now)\r\n        fc1 = tf.layers.dense(fc1, 1024)\r\n        # Apply Dropout (if is_training is False, dropout is not applied)\r\n        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\r\n        s = tf.reshape(fc1, [1])\r\n\trnn_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\r\n\toutputs, states = rnn.static_rnn(rnn_cell, s, dtype=tf.float32)\r\n\treturn tf.matmul(outputs[-1], rnn_weights['out']) + rnn_biases['out']\r\n\there is the error:\r\nValueError: Cannot reshape a tensor with 131072 elements to shape [1] (1 elements) for 'ConvNet/Reshape' (op: 'Reshape') with input shapes: [128,1024], [1] and with input tensors computed as partial shapes: input[1] = [1].\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19057, "title": "do not use caching device at eager mode in tf.contrib.seq2seq.dynamic_decode", "body": "`tf.contrib.seq2seq.dynamic_decode` uses caching device, but caching device is not supported at eager execution mode.\r\n\r\nThis PR checks if eager execution mode is enabled and avoids using caching device if so.\r\n\r\nThere is another issue in `tf.contrib.seq2seq.dynamic_decode` with eager execution mode. `tf.contrib.seq2seq.dynamic_decode` cannot be run yet with this fix alone. I will report the issue when ready.", "comments": ["@TanUkkii007 thanks for the PR! Do you mind resolving the conflicts? ", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@yifeif Sorry for a late reply. I found this problem is already fixed at master while resolving the conflict. I will try if eager mode works for dynamic_decode.\r\nhttps://github.com/tensorflow/tensorflow/blob/371a8dd2e1e18fd35f07ac230c52a471d90d3538/tensorflow/contrib/seq2seq/python/ops/decoder.py#L197-L199\r\nMy fix is not needed anymore so closing this PR."]}, {"number": 19055, "title": "Feature: Add reduce_average (weighted reduce_mean)", "body": "Add `tf.reduce_average` according to issue #7422\r\n\r\nIn this issue, some people doubt why this function is needed. I think broadcasting `weights` and corresponding `axis` is complex enough and worth a built-in function.\r\n\r\n`np.average` is also contributed by me. Related [Issue](https://github.com/numpy/numpy/issues/10989) and [PR](https://github.com/numpy/numpy/pull/10994).  \r\nI add support for any case where `len(axis) == rank(weights)`,  and it's the same in `tf.reduce_average` to keep compatibility with numpy.\r\n\r\nA unit test is attached.", "comments": ["Thanks for the PR @yaox12! Could you resolve the conflict?\r\n\r\n@josh11b do you mind taking a look? Thanks!", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 49 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 19054, "title": "Regularization loss will duplicated when reusing PartitionedVariable in tf.layers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 2.7.11\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: CUDA 8.0/ cuDNN 7\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nWhen reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers. I have found that there is no special treatment for reusing regularization loss of PartitionedVariables in tf.layers .\r\n### Source code / logs\r\nHere is the small script can reproduce the result.\r\n\r\n```\r\nimport tensorflow as tf\r\npartitioner = tf.fixed_size_partitioner(3)\r\nl2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\r\nfor i in xrange(2):\r\n  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):\r\n    inputs_tensor = tf.constant(1.0, shape=[100, 100])\r\n    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=\"fc\", kernel_regularizer=l2_regularizer)\r\nprint (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\r\n```\r\n\r\nThis short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6. \r\n\r\nA pull request has been submitted here to fix this bug: https://github.com/tensorflow/tensorflow/pull/19053\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19053, "title": "Fix bug of declaring regularization loss multiple times when reusing PartitionedVariables in tf layers", "body": "When reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers.  For example:\r\n```\r\nimport tensorflow as tf\r\npartitioner = tf.fixed_size_partitioner(3)\r\nl2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\r\nfor i in xrange(2):\r\n  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):\r\n    inputs_tensor = tf.constant(1.0, shape=[100, 100])\r\n    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=\"fc\", kernel_regularizer=l2_regularizer)\r\nprint (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\r\n```\r\nThis short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6.  To debug the source code, we found that there is no judgement on PartitionedVariables when declaring the regularization loss in /tensorflow/python/layers/base.py", "comments": ["@martinwicke Thanks for your comments! I have made the change. ", "@martinwicke @protoget Could you please take a look at Siyu's latest fix? I has been pending for a while.\r\n\r\nThanks", "@martinwicke @fchollet Thank you for your review and I have changed coding style. It looks good.", "@martinwicke `result` has been eliminated.", "@martinwicke Sorry, I have introduced a bug when I eliminate `result` variable in `_should_add_regularizer`,  which leads to CI build test failed. I missed `return True` when all checks have passed after `for` loop.  The bug has been fixed.  I also refine the code to limit to 80 columes. \r\n\r\nThanks for your review and another CI build test should be triggered.", "Thank you!"]}, {"number": 19052, "title": "document fix: correct code snippets to python3 style", "body": "python code snippets in this doc [Tensors](https://www.tensorflow.org/programmers_guide/tensors) are still using python2 style print function:\r\n\r\n```python\r\nprint tensor.eval()\r\n```\r\n\r\nwhile python3 style is used in other docs, such as [Introduction](https://www.tensorflow.org/programmers_guide/low_level_intro) and [Importing Data](https://www.tensorflow.org/programmers_guide/datasets):\r\n\r\n```python\r\nprint(a)\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 19051, "title": "After tensorflow-lite-0.1.7 update Image classification using inception-v3 model stops working", "body": "Image classification using inceptionv3_slim_2016.tflite is not working after the recent update of 0.1.7 version. Cannot allocate memory for the interpreter.\r\nIt gives the below exception :\r\n\r\njava.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code : NA\r\nOS Platform and Distribution : Android\r\nTensorFlow installed from : 'org.tensorflow:tensorflow-lite:+'\r\nTensorFlow version : tensorflow-lite-0.1.7\r\nBazel version : NA\r\nCUDA/cuDNN version : NA\r\nGPU model and memory : NA\r\nExact command to reproduce : NA(using TfLite InceptionV3 in Android 8.1)", "I also met the same problem, and the float model is so slow. ", "Same here", "Same here", "Thanks princekumar15@ for reporting this issue! I'll start looking into what caused this breakage.", "@princekumar15 : I tried but was unable to reproduce your issue. Everyone, is there something I did differently?\r\n\r\nI suspect you were following the [tflite Android demo instructions](https://www.tensorflow.org/mobile/tflite/demo_android).\r\n\r\n1. I followed the instructions: \"Or, download the floating point Inception-v3 model and unzip and copy inceptionv3_non_slim_2015.tflite to the assets directory. Change the chosen classifier in Camera2BasicFragment.java from: classifier = new ImageClassifierQuantizedMobileNet(getActivity()); to: classifier = new ImageClassifierFloatInception(getActivity());\". I also modified the BUILD file to include the new asset.\r\n\r\n2. I ran into the [setNumThreads issue](https://github.com/tensorflow/tensorflow/issues/18821) and commented the lines out as Bob suggested.\r\nIn the build.gradle file, I tried compile 'org.tensorflow:tensorflow-lite:0.1.7' and compile 'org.tensorflow:+'. I didn't see the error when running the demo in either case.\r\n\r\n3. I also just tried building from HEAD with bazel without using Android Studio and didn't see the error.\r\n\r\nThis was on a Pixel 2 running on Android 8.1.0. It may be useful to know what devices everyone is running their demos on.\r\n\r\n\r\n", "The way I get this error is when following the tutorial TensorFlow for poets 2.\r\n\r\nI first tried the TensorFlow for poets (https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html?index=..%2F..%2Findex#0) to retrain a model with my data (this part works just fine even when using the retrained .pb graph on android but the detection time is still slow and the battery usage high).\r\n\r\nI wanted to try and implement TFLite to help fix these problems, so I followed the tutorial TensorFlow for poets 2 (https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#0) to convert my .pb model into a .tflite model. The output model is the one that causes the java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter when imported in Android, but the inceptionv3_slim_2016.tflite works fine, it just does not work with my retrained model.\r\n\r\nI'm running this on Samsung Galaxy S6 running Android 7.0 by the way, I hope this gives you more information on how to reproduce the problem.", "@jbuisson1: for the pre-made Android app, are you using the one linked to in Tensorflow for Poets 2 (from the codebase cloned in Tensorflow for Poets 1, as opposed to this [one](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo))? I followed both Tensorflow for Poets 1 and 2, and it's worked for me on a Nexus 6 emulator running Android 7.0 and my Pixel 2. I tried hacking the model/labels into the tensorflow/tensorflow repo also and it worked fine also.\r\n\r\nLet's try to see what differences there might be.\r\n\r\n1. Your data is special somehow (e.g. different image dimensions). Did you have the same problem when using the tutorial retraining data (daisy, sunflower, ... categories)?\r\n2. There have been tflite updates to tensorflow-lite:0.1.7 since you last tried. Try [making sure that Android Studio/Gradle has picked up on the changes](https://stackoverflow.com/questions/28683327/how-to-check-if-gradle-dependency-has-new-version).\r\n3. If there aren't other differences that you can think of, I may need to find a Samsung Galaxy S6 to reproduce.\r\n\r\n**Dismissed Differences**\r\n1. I'm running from HEAD. The codebase hasn't really been [updated recently](https://github.com/googlecodelabs/tensorflow-for-poets-2/commits/master/android/tflite).", "Closing this issue.\r\n\r\nI suspect that if you make sure that Android Studio/Gradle has picked up on [tensorflow-lite:0.1.7 changes](https://stackoverflow.com/questions/28683327/how-to-check-if-gradle-dependency-has-new-version), the problem will go away. \r\n\r\nReasoning:\r\nI confirmed that the tensorflow-lite:0.1.7 AAR that Gradle uses was re-released once. This is not something we should be doing in the future, but would explain why I couldn't reproduce the issue (I'm using the newer version of the tensorflow-lite:0.1.7 AAR and some people picked up the older version around the release time).\r\n\r\n"]}, {"number": 19050, "title": "Fix bug of declaring regularization loss multiple times when reusing PartitionedVariables in tf layers", "body": "When reusing a variable in current variable scope, we should always reuse its regularization loss computation. But it will declare regularization loss multiple times when reusing PartitionedVariables in tf.layers.  For example:\r\n```\r\nimport tensorflow as tf\r\npartitioner = tf.fixed_size_partitioner(3)\r\nl2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\r\nfor i in xrange(2):\r\n  with tf.variable_scope(tf.get_variable_scope(), partitioner=partitioner, reuse=False if i == 0 else True):\r\n    inputs_tensor = tf.constant(1.0, shape=[100, 100])\r\n    logits = tf.layers.dense(inputs_tensor, 256, use_bias=False, name=\"fc\", kernel_regularizer=l2_regularizer)\r\nprint (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\r\n```\r\nThis short program should get result 3 because the PartitionedVariable has 3 shards. However, it got 6.  To debug the source code, we found that there is no judgement on PartitionedVariables when declaring the regularization loss in /tensorflow/python/layers/base.py", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 19048, "title": "Android camera demo", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi, I have to create Tensorflow android camera demo using my retrained model. In classifierActivity.java I did like it says set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128, INPUT_NAME = \"Mul\", and OUTPUT_NAME = \"final_result\". You'll also need to update the MODEL_FILE and LABEL_FILE paths to point to the ones you produced. \r\n But I don't understand second part:  To use v3 Inception model, strip the DecodeJpeg Op from your retrained\r\n// model first:\r\n//\r\n// python strip_unused.py \\\r\n// --input_graph=<retrained-pb-file> \\\r\n// --output_graph=<your-stripped-pb-file> \\\r\n// --input_node_names=\"Mul\" \\\r\n// --output_node_names=\"final_result\" \\\r\n// --input_binary=true\r\nI don't understand what is my stripped-pb file,is that original v1 inception model (tensorflow_inception_graph.py)?\r\nI tried run it like that but i got for output : https://imgur.com/a/FbkjKor\r\n\r\nI know these are maybe silly questions ,I don't know a lot about tensorflow,I just follow instructions to make it work because I need it for college project but it doesn't work when I use my retrained model.\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19047, "title": "keras call convert_model from theano error.", "body": "    convert_model(model)\r\n  File \"../utils.py\", line 31, in convert_model\r\n    ops.append (tf.assign (layer.kernel, converted_w).op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 273, in assign\r\n    if ref.dtype._is_ref_dtype:\r\nAttributeError: 'str' object has no attribute '_is_ref_dtype'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19046, "title": "Building from source: No such file or directory", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch pull yesterday (077ae6d)\r\n- **Python version**: Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)  [GCC 7.2.0] on linux\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: only CPU\r\n- **GPU model and memory**: only CPU\r\n- **Exact command to reproduce**: according to tutorial \"bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package\"\r\n\r\n### Describe the problem\r\nWhen I build from source according to the tutorial, I get the following error (see Logs below).\r\n\r\n### Source code / logs\r\n\r\n**My configuration:**\r\n\r\nPlease specify the location of python. [Default is /home/rodin_maxim93/programs/miniconda3/bin/python]: \r\nFound possible Python library paths:\r\n  /home/rodin_maxim93/programs/miniconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is \r\n\r\n[/home/rodin_maxim93/programs/miniconda3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: \r\nNo CUDA support will be enabled for TensorFlow.\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: \r\nClang will not be downloaded.\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n\r\n\r\n**The ERROR:**\r\n\r\n```\r\nERROR: /home/rodin_maxim93/programs/distr/tensorflow/tensorflow/core/BUILD:2015:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 127): bash failed: error executing command \r\n  (cd /home/rodin_maxim93/.cache/bazel/_bazel_rodin_maxim93/7b13afc3c7dd373845b8c107b3b3738b/execroot/org_tensorflow && \\exec env - \\\r\n PATH=/home/rodin_maxim93/programs/miniconda3/bin:/home/rodin_maxim93/programs/miniconda3/bin:/usr/local/sbin:/\r\n\r\nusr/local/bin:/usr/s\r\nbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --\r\n\r\ngenerate external\r\n/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref \"bazel-\r\n\r\nout/host/genfiles/t\r\nensorflow/core/util/version_info.cc\" --git_tag_override=${GIT_TAG_OVERRIDE:-}')\r\n: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 139.686s, Critical Path: 18.66s\r\nINFO: 955 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n```\r\n", "comments": ["Run this command and try to build it again \r\n`bazel clean --expunge`\r\nand run `./configure`\r\n\r\nif you encounter same error run following command and post result\r\n`bazel build -s //tensorflow/core:version_info_gen`", "@smitshilu Thanks for helping!", "@smitshilu Thanks for the answer!\r\nSame error came out. Result of `bazel build -s //tensorflow/core:version_info_gen`: \r\n\r\n```\r\nWARNING: /home/rodin_maxim93/.cache/bazel/_bazel_rodin_maxim93/7b13afc3c7dd373845b8c107\r\nb3b3738b/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/rodin_maxim93/.\r\ncache/bazel/_bazel_rodin_maxim93/7b13afc3c7dd373845b8c107b3b3738b/external/protobuf_arc\r\nhive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's\r\n definition (@protobuf_archive); this will cause a build error in future versions\r\nINFO: Analysed target //tensorflow/core:version_info_gen (0 packages loaded).\r\nINFO: Found 1 target...\r\nSUBCOMMAND: # //tensorflow/core:version_info_gen [action 'Executing genrule //tensorflo\r\nw/core:version_info_gen']\r\n(cd /home/rodin_maxim93/.cache/bazel/_bazel_rodin_maxim93/7b13afc3c7dd373845b8c107b3b37\r\n38b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/rodin_maxim93/programs/miniconda3/bin:/home/rodin_maxim93/programs/minic\r\nonda3/bin:/home/rodin_maxim93/programs/miniconda3/bin:/home/rodin_maxim93/programs/mini\r\nconda3/bin:/home/rodin_maxim93/programs/miniconda3/bin:/home/rodin_maxim93/programs/min\r\niconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/us\r\nr/local/games \\\r\n    PYTHON_BIN_PATH=/home/rodin_maxim93/programs/miniconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/home/rodin_maxim93/programs/miniconda3/lib/python3.6/site-packages\r\n \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/\r\ntools/git/gen_git_source.py --generate external/local_config_git/gen/spec.json external\r\n/local_config_git/gen/head external/local_config_git/gen/branch_ref \"bazel-out/k8-opt/g\r\nenfiles/tensorflow/core/util/version_info.cc\" --git_tag_override=${GIT_TAG_OVERRIDE:-}'\r\n)\r\nERROR: /home/rodin_maxim93/programs/distr/tensorflow/tensorflow/core/BUILD:2015:1: Exec\r\nuting genrule //tensorflow/core:version_info_gen failed (Exit 127)\r\n: No such file or directory\r\nTarget //tensorflow/core:version_info_gen failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.950s, Critical Path: 0.01s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```", "Can you give this https://anaconda.org/anaconda/tensorflow-mkl a try? \r\nYou don't have to build from source. ", "@Bi0max If this is still an issue please take a look at #5096 and try suggestions mentioned there. If it does not help please post the results of running with --verbose_failures.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "> Run this command and try to build it again\r\n> `bazel clean --expunge`\r\n> and run `./configure`\r\n> \r\n> if you encounter same error run following command and post result\r\n> `bazel build -s //tensorflow/core:version_info_gen`\r\n\r\n` bazel --batch build -s //tensorflow/core:version_info_gen\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/C:/Users/Telo%20Anderson/_bazel_Telo%20Anderson/install/3a7287a15200b68eb8146fc168635be1/_embedded_binaries/A-server.jar) to field java.lang.String.value\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/core\r\nAnalyzing: target //tensorflow/core:version_info_gen (7 packages loaded)\r\nINFO: Analysed target //tensorflow/core:version_info_gen (9 packages loaded).\r\nINFO: Found 1 target...\r\n[0 / 2] no action\r\nTarget //tensorflow/core:version_info_gen up-to-date:\r\n  C:/users/telo anderson/_bazel_telo anderson/wvk7snnt/execroot/org_tensorflow/bazel-out/x64_windows-opt/genfiles/tensorflow/core/util/version_info.cc\r\nINFO: Elapsed time: 33.363s, Critical Path: 0.02s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\n`\r\n\r\nthat builds succesfully but i still get on normal build operation `consumes this library\r\nERROR: C:/tensorflow/tensorflow/tensorflow/python/BUILD:5651:1: Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed (Exit 2): bash.exe failed: error executing command`", "ERROR: /root/tf_code/r1.8/tensorflow/python/BUILD:3315:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -shared -o bazel-out/host/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' ... (remaining 90 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\ngcc: error: pywrap_tensorflow_internal_versionscript.lds: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1311.223s, Critical Path: 80.88s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 3422 processes: 3419 linux-sandbox, 3 local.\r\nFAILED: Build did NOT complete successfully\r\n", "root@shap000105362:~/tf_code/r1.8# bazel build -s //tensorflow/core:version_info_gen\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/root/tf_code/r1.8/tools/bazel.rc\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/core:version_info_gen (0 packages loaded, 26 targets configured).\r\nINFO: Found 1 target...\r\nSUBCOMMAND: # //tensorflow/core:version_info_gen [action 'Executing genrule //tensorflow/core:version_info_gen']\r\n(cd /root/.cache/bazel/_bazel_root/4efdd725f5bfd8348a1cc6edabdc588a/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref \"bazel-out/k8-py3-fastbuild/genfiles/tensorflow/core/util/version_info.cc\" --git_tag_override=${GIT_TAG_OVERRIDE:-}')\r\nTarget //tensorflow/core:version_info_gen up-to-date:\r\n  bazel-genfiles/tensorflow/core/util/version_info.cc\r\nINFO: Elapsed time: 2.081s, Critical Path: 0.04s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed successfully, 2 total actions\r\n", "OS: CentOS7\r\nPython: python2\r\n\r\nThe python scripts may not be able to run directly.  A python script as follow:\r\n```python\r\n#!/usr/bin/env python\r\n# test.py\r\nprint \"Hello\"\r\n```\r\nThis script can run with command 'python test.py', but failed with command 'chmod u+x test.py&&./test.py'\r\n\r\nif this happend, modify tf_version_info_genrule in tensorflow/tensorflow.bzl line 1707 from\r\n\"$(location //tensorflow/tools/git:gen_git_source.py) --generate $(SRCS) \\\"$@\\\" --git_tag_override=$${GIT_TAG_OVERRIDE:-}\",\r\nto\r\n\"python $(location //tensorflow/tools/git:gen_git_source.py) --generate $(SRCS) \\\"$@\\\" --git_tag_override=$${GIT_TAG_OVERRIDE:-}\","]}, {"number": 19045, "title": "tf.boolean_mask always return empty results while the mask has true value", "body": "### System information\r\n-  CentOS Linux release 7.4.1708 (Core)\r\n-  TensorFlow installed from source \r\n-  TensorFlow version: 1.3.0\r\n-  Python version: 3.6.3\r\n-  gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC) \r\n-  Cuda compilation tools, release 9.1, V9.1.85\r\n-  GPU GeForce GTX 1080Ti *2\r\n\r\n### Describe the problem\r\nI am trying to use the function of tf.boolean_mask to select value from a 5D tensor. To make things easier, I just make my mask as same shape of target tensor, that is (1,12,125,179,1), but it will return zero dimension tensor as result.\r\n\r\nThen I tried with the example tensorflow provided in official documentation. I found there's some problem with the function.\r\nThis is the document and example in tensorflow document (Since I cannot use axis argument in my environment, I think I might close to this case): https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/boolean_mask\r\nIn my system, the example tensorflow documentation provided would return zero dimension tensor instead of one dimension as the example case.\r\nHere's the example code and result:\r\n![1](https://user-images.githubusercontent.com/18610064/39559423-aca5170e-4ec8-11e8-94dc-d4f29e9876ad.png)\r\n\r\n\r\n### Code\r\nAs to my case, it not so different from the example, the case just from 2D to 5D.\r\n1 make the mask same shape with target tensor, result (none) shape tensor:\r\n![2](https://user-images.githubusercontent.com/18610064/39559462-f9d241e6-4ec8-11e8-8e4b-ae1eb57a7bc6.png)\r\n2 make the mask 1D less than target tensor, result (none, 1) shape tensor:\r\n![3](https://user-images.githubusercontent.com/18610064/39559495-3a5864a2-4ec9-11e8-907b-fec0627792a4.png)\r\n\r\nIt's easy to reproduce with random np.array and tf.tensor.\r\n\r\nIt would be appreciated that you could spend some time to take a look at it. Thanks.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "`TensorShape([Dimension(None)])` is a one-dimension shape.", "@ppwwyyxx Thanks for your reply. You're right. It's interesting. I just tried to run a session to test the output and found it's one dimension tensor. I thought dimension none means a scalar as the official document said, so did not test it. Thanks, problem solved.\r\n![1](https://user-images.githubusercontent.com/18610064/39700634-78566c4e-5230-11e8-86c2-475dcc426dc0.png)\r\n", "Nagging Assignee @asimshankar: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@memelan : A shape of `None` means a scalar, a shape of `[None]` means a one-dimensional tensor with an unknown size. \r\n\r\nFrom your previous comment, it appears that this confusion has been resolved, so I'm closing the issue.\r\nFeel free to reopen and provide details if I'm misunderstood.\r\nThanks!"]}]