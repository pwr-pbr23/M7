[{"number": 55024, "title": "[XLIR] Support bf16 type for gemm.", "body": "[XLIR] Support bf16 type for gemm.\n", "comments": []}, {"number": 55023, "title": "the update ordering in ", "body": "**System information**\r\n-  tf_v5    tf.image.NonMaxSupressionWithScore\r\n\r\n**Describe the current behavior**\r\nI think there is a little bug in the implementation of *non_max_suppression_op.cc* in the  function *DoNonMaxSuppressionOp*\r\n![image](https://user-images.githubusercontent.com/31850863/156863445-27c3d631-d426-4e9d-89f6-d281ce597503.png)\r\nI has tried to change it as \r\n![image](https://user-images.githubusercontent.com/31850863/156863575-a06b6771-90ee-47de-9f91-d90b2c1b2458.png)\r\nIn some extreme cases, I find they have different results because the value of a\\*b\\*c\\*d != d\\*c\\*b\\*a in numerical meaning and I think my change of the order is the normal order of the NMS algorithm proposed.\r\n\r\n**Describe the expected behavior**\r\nJust change the sequence as I showed above.\r\n\r\n\r\n\r\n\r\n", "comments": ["@tomandjake \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "Sorry I has already wirite things according to that template and I do not think this \"little and understandable bug\" need other information such as My environment and etc. I am not convenient to check those information :(", "@tomandjake \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55023\">No</a>\n"]}, {"number": 55022, "title": "[XLA] Speed up WhileLoopSimplifier for large shapes.", "body": "[XLA] Speed up WhileLoopSimplifier for large shapes.\n", "comments": []}, {"number": 55021, "title": "Add map<string, FulltypeDef> for function-dependent ForwardTypeInferenceFn", "body": "Add map<string, FulltypeDef> for function-dependent ForwardTypeInferenceFn\n\nUse\n\n  std::function<StatusOr<FullTypeDef>(\n      const std::vector<std::reference_wrapper<const FullTypeDef>>&,\n      const std::map<std::string, std::reference_wrapper<const FullTypeDef>>&\n  )>\n\nas the function signature for ForwardTypeInferenceFn to support function-dependent types.\n", "comments": []}, {"number": 55020, "title": "Internal changes only.", "body": "Internal changes only.\n", "comments": []}, {"number": 55019, "title": "Rename convenience methods for blocking and non-blocking calls.", "body": "Rename convenience methods for blocking and non-blocking calls.\n\nChange method_name to method_name_async for async calls that return a future.\nChange method_name_blocking to method_name for synchronous calls that return value.\n", "comments": []}, {"number": 55018, "title": "Update naming for TensorFlow Lite in Google Play services", "body": "Update naming for TensorFlow Lite in Google Play services\n", "comments": []}, {"number": 55017, "title": "Reorganizing TF Lite website navigation", "body": "Reorganizing TF Lite website navigation\n", "comments": []}, {"number": 55016, "title": "Turns off pfor warning about legacy RNGs.", "body": "Turns off pfor warning about legacy RNGs.\n", "comments": []}, {"number": 55015, "title": "Roll back broken change.", "body": "Roll back broken change.\n", "comments": []}, {"number": 55014, "title": "Issue with XLA devices or MLIR optimization?", "body": "Hi All, \r\n\r\nHaving a little trouble using tensorflow.  I thought it might be an issue related to the following warnings:\r\n\r\n` Not creating XLA devices, tf_xla_enable_xla_devices not set`\r\nor\r\n`None of the MLIR optimization passes are enabled (registered 2)`\r\n\r\nbut upon reading other posts, I've found that people say these warnings can be ignored?\r\n\r\nI get latter when creating a model:\r\n\r\n`model = N2V(config, model_name, basedir=basedir)`\r\n\r\nwhich takes about 5 minutes to execute and I get the following output:\r\n\r\n /home/sam/miniconda3/envs/n2v/lib/python3.7/site-packages/n2v/models/n2v_standard.py:416: UserWarning: output path for model already exists, files may be overwritten: /home/sam/models/BSD68_reproducability_5x5\r\n  'output path for model already exists, files may be overwritten: %s' % str(self.logdir.resolve()))\r\n2022-03-04 10:33:12.171788: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2022-03-04 10:33:12.172307: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2022-03-04 10:33:12.205486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:33:12.205618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:61:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.68GiB deviceMemoryBandwidth: 715.34GiB/s\r\n2022-03-04 10:33:12.205630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2022-03-04 10:33:12.206557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2022-03-04 10:33:12.206579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2022-03-04 10:33:12.207523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2022-03-04 10:33:12.207667: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2022-03-04 10:33:12.208436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2022-03-04 10:33:12.208839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2022-03-04 10:33:12.210569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2022-03-04 10:33:12.210663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:33:12.210860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:33:12.210941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2022-03-04 10:33:12.211272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-03-04 10:33:12.212089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:33:12.212184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:61:00.0 name: NVIDIA RTX A5000 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 64 deviceMemorySize: 23.68GiB deviceMemoryBandwidth: 715.34GiB/s\r\n2022-03-04 10:33:12.212194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2022-03-04 10:33:12.212205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2022-03-04 10:33:12.212212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2022-03-04 10:33:12.212218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2022-03-04 10:33:12.212224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2022-03-04 10:33:12.212230: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2022-03-04 10:33:12.212236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2022-03-04 10:33:12.212243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2022-03-04 10:33:12.212276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:33:12.212384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:33:12.212461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2022-03-04 10:33:12.212481: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2022-03-04 10:38:35.925719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2022-03-04 10:38:35.925741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2022-03-04 10:38:35.925746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2022-03-04 10:38:35.925939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:38:35.926079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:38:35.926191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-03-04 10:38:35.926283: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2022-03-04 10:38:35.926306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21899 MB memory) -> physical GPU (device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:61:00.0, compute capability: 8.6)\r\n2022-03-04 10:38:35.926510: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n\r\n\r\n\r\nIf I then continue with:\r\n\r\n`history = model.train(X, X_val)`\r\n\r\nI get the following output, after which, it just stops:\r\n\r\n/home/sam/miniconda3/envs/n2v/lib/python3.7/site-packages/n2v/models/n2v_standard.py:194: UserWarning: small number of validation images (only 0.1% of all images)\r\n  warnings.warn(\"small number of validation images (only %.1f%% of all images)\" % (100 * frac_val))\r\n\r\n8 blind-spots will be generated per training patch of size (64, 64).\r\n\r\nPreparing validation data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 533.12it/s]\r\n\r\nEpoch 1/200\r\n\r\n\r\n2022-03-04 10:40:59.811781: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2022-03-04 10:40:59.830334: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3892985000 Hz\r\n2022-03-04 10:41:00.455851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n\r\n\r\n\r\nAny thoughts??\r\n\r\nThanks!\r\nSam\r\n", "comments": ["OK, so I think I've fixed some of it.  I changed the environment installation procedure to use:\r\n\r\n```\r\npip install tensorflow-gpu==2.4.1\r\n\r\n```\r\nthe key difference being that I used `tensorflow-gpu` instead of just `tensorflow` which makes sense. Maybe that can be added to the documentation?\r\n\r\nHowever, when I run the training I get a repeating message of:\r\n\r\n`\r\n`2022-03-04 19:57:19.446524: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'`\r\n\r\nWhich eventually finishes but the messages cause the training to move very slowly.  I know that because if I do a low number of epochs and steps, then execute the training cell again once finished, the error messages go away and everything goes according to plan.\r\n\r\nI have tried inserting:\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\n```\r\nbut to no avail.\r\n\r\nHope this helps anyone else in need! and if anyone has an idea what to do with the \"Your CUDA software stack is old. \" message, that would be of great help!\r\n\r\nSam\r\n`", "@snoreis\r\n\r\nYes, you can ignore all those messages. They are just the information message as they are prefixed with `I`, if it is the error message they would be prefixed with `E` or `W` for warnings.\r\n\r\n>2022-03-04 19:57:19.446524: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance.\r\n\r\nHere above warning clearly states the issue, TF is using older version of `CUDA`. For `TF 2.4` the compatible `CUDA` starts with `11`. Please upgrade drivers accordingly for better performance. For more details please refer [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nThis problem is caused by the old \"ptxas\", which doesn't support RTX3090, 3080 GPUs\r\n\r\n`NVIDIA RTX A5000`  cards are based on the Ampere architecture for which compatible CUDA version start with `11.x`. \r\n\r\nPlease remove/rename old `ptxas` and make sure your  TF can only find and use the `ptxas` in the `CUDA 11.x`. Thanks!\r\n", "@chunduriv \r\n\r\nThan you for the information.  I should have added the full conda environment:\r\n```\r\nconda create --name n2v python=3.7\r\nconda activate n2v\r\nconda install -c anaconda cudatoolkit\r\nconda install cudatoolkit=11.0.*\r\nconda install cudnn=8.0.*\r\nconda install jupyter\r\npip install tensorflow-gpu==2.4.1\r\npip install n2v\r\n```\r\n\r\nSo I did get `CUDA 11` in there.\r\n\r\nDo you have a suggestion? or link to another issue? that was raised about how to:\r\n\r\n> remove/rename old `ptxas` and make sure your TF can only find and use the `ptxas` in the `CUDA 11.x`\r\n\r\nThanks again!\r\n", "@snoreis, \r\n\r\n>2022-03-04 10:33:12.205630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n\r\nFrom Stack trace,  `CUDA 10.1` was also installed in your system. \r\n\r\n>Do you have a suggestion? or link to another issue? that was raised about how to:\r\n\r\nPlease can you refer this [comment](https://github.com/tensorflow/tensorflow/issues/33375#issuecomment-543634440).\r\n\r\nSimple workaround, first uninstall TensorFlow from anaconda environment and try to do a fresh install and make sure you have the tested build configurations as per this [guide](https://www.tensorflow.org/install/source#linux). Thanks!", "> From Stack trace, `CUDA 10.1` was also installed in your system.\r\n\r\n@chunduriv  Very interesting! I'll investigate.\r\n\r\nThanks for the help, and I will update here if I can solve.", "@snoreis,\r\n\r\nCould you please confirm if this issue is resolved for you? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55014\">No</a>\n"]}, {"number": 55013, "title": "[XLA/GPU] Add an option to enable constant sharing between GPU executables.", "body": "[XLA/GPU] Add an option to enable constant sharing between GPU executables.\n- The option is enabled by default, so this should not change existing behavior.\n", "comments": []}, {"number": 55012, "title": "Split tpu_embedding_v2_test and tpu_embedding_v2_correctness_test into multiple files.", "body": "Split tpu_embedding_v2_test and tpu_embedding_v2_correctness_test into multiple files.\n", "comments": []}, {"number": 55011, "title": "[XLA:GPU] Cache large constants in the executor and share them between modules", "body": "[XLA:GPU] Cache large constants in the executor and share them between modules\n\nThis removes some (large) global constants from the LLVM/PTX entirely so they can be allocated and initialized entirely within XLA. Only globals without any other references in the generated code can be removed in this way.\n", "comments": []}, {"number": 55010, "title": "Internal only changes", "body": "Internal only changes\n", "comments": []}, {"number": 55009, "title": "Always do safe parsing", "body": "Always do safe parsing\n", "comments": []}, {"number": 55008, "title": "[XLA] Handles a special case in HloReshapeInstruction dynamic dimension inference.", "body": "[XLA] Handles a special case in HloReshapeInstruction dynamic dimension inference.\n\nIf the input tensor is a 1D tensor and has a dynamic dimension as index 0 with size 1, then sets the output dynamic dimension as index 0.\n\nIn this case, the input dynamic dimension can both be considered as the first dimension (most-major) or the last dimension (most-minor). We prefer to set the output dynamic dimension as the first dimension.\n", "comments": []}, {"number": 55007, "title": "Don't allow multi-output fusion when the \"producer\" op is in-place", "body": "Don't allow multi-output fusion when the \"producer\" op is in-place\n\nFusion in this case is dangerous. Without in-depth analysis of the consumer we can't know whether or not it will treat the producer's in-place operands safely.\n", "comments": []}, {"number": 55006, "title": "Factor out function for getting xla::ExecutableBuildOptions.", "body": "Factor out function for getting xla::ExecutableBuildOptions.\n", "comments": []}, {"number": 55005, "title": "Factor out function for getting vector<xla::Shape*> from vector<xla::Shape>.", "body": "Factor out function for getting vector<xla::Shape*> from vector<xla::Shape>.\n", "comments": []}, {"number": 55004, "title": "Generalize einsum to dot_general lowering to allow transposed results.", "body": "Generalize einsum to dot_general lowering to allow transposed results.\n\nThis allows ops like this to be lowered properly (was generated\nillegal dot_general ops):\n\n```\n\"mhlo.einsum\"(%424, %298) {einsum_config = \"abd,abc->cd\"} :\n  (tensor<1x512x768xf32>, tensor<1x512x3072xf32>) ->\n     tensor<3072x768xf32>\n```\n\nI found the existing lowering quite buggy, and I believe that two of the tests are incorrect, generating dot_general ops that are actually illegal. MHLO is so under-specified and under-verified, though, so that it is next to impossible to say, so I am just applying best judgment.\n", "comments": []}, {"number": 55003, "title": "Enhance shape inference for ReduceDatasetOp.", "body": "Enhance shape inference for ReduceDatasetOp.\n\nSpecifically, trace upwards the IdentityOp chains to get to the op that defines the dataset.\n", "comments": []}, {"number": 55002, "title": "Replace (deprecated) StrEnumAttr with EnumAttr.", "body": "Replace (deprecated) StrEnumAttr with EnumAttr.\n\nAlso, move dialect definition into the td file, to allow EnumAttr to generate\nthe necessary attribute parser/printer.\n", "comments": []}, {"number": 55001, "title": "Extract a function for parsing operator BROADCAST_ARGS and BROADCAST_TO", "body": "Extract a function for parsing operator BROADCAST_ARGS and BROADCAST_TO\n\nExtract the parsing out of a switch statement case to create a standalone function which can be called by the micro op resolver, following steps on https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/docs/porting_reference_ops.md#3-extract-lites-code-for-parsing-op-parameters-to-a-function-pr1\n", "comments": []}, {"number": 55000, "title": "Add Rank as a synonym for NumDimensions", "body": "Add Rank as a synonym for NumDimensions\n\nRank is a common terminology, there are operators (tf.rank) that do the same thing, it is shorter than NumDimensions too.\n\nThis does not yet change any callers to use it. But I plan to use it in the XNNPACK delegate.\n", "comments": []}, {"number": 54999, "title": "Disable tensorflow.asan for saved_model_cli_test.", "body": "Disable tensorflow.asan for saved_model_cli_test.\n", "comments": []}, {"number": 54998, "title": "Use kernel utils helper to get number of dimensions and size", "body": "Use kernel utils helper to get number of dimensions and size\n", "comments": []}, {"number": 54997, "title": "...text exposed to open source public git repo...", "body": "...text exposed to open source public git repo...\n", "comments": []}, {"number": 54996, "title": "[mhlo] Import tuple-return from mhlo::mapOp's reducer-block to flattened return-val.", "body": "[mhlo] Import tuple-return from mhlo::mapOp's reducer-block to flattened return-val.\n\nDuring import (from HLO to MHLO) we flatten the tuple return-type of\nregion-blocks. MHLO mapOp::verifier ensures that the flattened return-type is\ncomaptible with the op-specification.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/54954 from tensorflow:create-pull-request/patch 21bd86102af243cfe92ee6af538ac9070bd35fa0\n", "comments": []}, {"number": 54995, "title": "Add EagerContext option to rewrite jit_compile functions.", "body": "Add EagerContext option to rewrite jit_compile functions.\n\nJust adding an option to enable this.  Not used yet.\n", "comments": []}]