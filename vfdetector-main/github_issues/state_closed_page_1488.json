[{"number": 8300, "title": "print", "body": "I want to print the `height` and `width` as `int` like `123` of the `new_shape`  which is a tensor\r\n``` if input_size is not None:\r\n    h, w = input_size\r\n    if random_scale:\r\n        scale = tf.random_uniform([1], minval=0.75, maxval=1.25, dtype=tf.float32, seed=None)\r\n        h_new = tf.to_int32(tf.mul(tf.to_float(tf.shape(img)[1]), scale))\r\n        w_new = tf.to_int32(tf.mul(tf.to_float(tf.shape(img)[1]), scale))\r\n        new_shape = tf.squeeze(tf.pack([h_new, w_new]), squeeze_dims=[1])\r\n\r\n        img = tf.image.resize_images(img, new_shape)\r\n```\r\n\r\nI`m new with tf, what should i do?\r\nthanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8299, "title": "tf.contrib.seq2seq.prepare_attention doesn't allow decoder states and attention states to be different lengths", "body": "I'm using the new `tf.contrib.seq2seq.prepare_attention` with `tf.contrib.seq2seq.attention_decoder_fn_train` and `tf.contrib.seq2seq.dynamic_rnn_decoder` to do dynamic decoding with attention. \r\n\r\nIf we are using e.g. `attention_option=\"bahdanau\"`, then [this line](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py#L408) implements the standard attention equation:\r\n\r\nscore<sub>i</sub> = v<sup>T</sup> tanh(W<sub>h</sub> h<sub>i</sub> + W<sub>q</sub> q)\r\n\r\nwhere \r\n- h<sub>i</sub> is the ith attention state, a vector length `num_units`\r\n- W<sub>h</sub> is a weight matrix shape [`num_units`, `num_units`]\r\n- q is the query (i.e. current decoder hidden state), a vector length `num_units`\r\n- W<sub>q</sub> is a weight matrix shape [`num_units`, `num_units`]\r\n- v is a weight vector length `num_units`\r\n\r\nIn particular, the code assumes that:\r\n1. Decoder hidden states q and attention states h<sub>i</sub> are the same size (which isn't true if e.g. you want different size hidden vectors for your encoder and decoder, or you want to use bidirectional RNN for encoder but not decoder)\r\n2. v, W<sub>h</sub> h<sub>i</sub> and W<sub>q</sub> q must also be same length `num_units`\r\n\r\nIn particular assumption 1 is very limiting. I think it would be better to allow:\r\n- h<sub>i</sub> is the ith attention state, a vector length `attn_size`\r\n- W<sub>h</sub> is a weight matrix shape [`num_units`, `attn_size`]\r\n- q is the query (i.e. current decoder hidden state), a vector length `query_size`\r\n- W<sub>q</sub> is a weight matrix shape [`num_units`, `query_size`]\r\n- v is a weight vector length `num_units`\r\n\r\nwhere the user can define `num_units`, `attn_size` and `query_size`. From what I can see this would be fairly uncomplicated.", "comments": ["@ebrevdo Do you know who is the owner of this code?  ", "Probably thang luong.  He and I are working on a new dynamic attention\nRNNCell wrapper  where I do separate out these dimensions into separate\nparameters!  Will try to push that code on monday.\n\nOn Sat, Mar 11, 2017 at 11:33 AM, Paul Barham <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Do you know who is the owner of\n> this code?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8299#issuecomment-285893940>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzPxcsvHBudvzJ6esD4RTrQV1-yHks5rkvcOgaJpZM4MaGiT>\n> .\n>\n", "(this code has been deleted in github master, in favor of the new object\noriented style)\n\nOn Sat, Mar 11, 2017 at 12:11 PM, Eugene Brevdo <ebrevdo@gmail.com> wrote:\n\n> Probably thang luong.  He and I are working on a new dynamic attention\n> RNNCell wrapper  where I do separate out these dimensions into separate\n> parameters!  Will try to push that code on monday.\n>\n> On Sat, Mar 11, 2017 at 11:33 AM, Paul Barham <notifications@github.com>\n> wrote:\n>\n>> @ebrevdo <https://github.com/ebrevdo> Do you know who is the owner of\n>> this code?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/8299#issuecomment-285893940>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtimzPxcsvHBudvzJ6esD4RTrQV1-yHks5rkvcOgaJpZM4MaGiT>\n>> .\n>>\n>\n>\n", "@ebrevdo Thanks for the information.  I'm going to close this as a \"bug\" since it looks like the existing code is going away and a replacement is imminent which doesn't have this problem."]}, {"number": 8298, "title": "Broken initialization when graph has data dependencies. ", "body": "As per conversation at OpenAI (cc @alextp @yaroslavvb). \r\n\r\nHere's an example of broken variable initialization and the super-slow fix:\r\nhttps://gist.github.com/nivwusquorum/551b502e1cf36a09c9c05385ccea5eb5\r\n\r\nLet me know if you need more details", "comments": ["This problem is described in https://github.com/tensorflow/tensorflow/issues/4920\r\nThe solution proposed is to do \"while exception: try initialize\", which is guaranteed to eventually succeed because at each iteration at least one more variable gets initialized. Because of Moore's law, the performance will eventually become acceptable, therefore please integrate this into TensorFlow, kthanksbye", "Please take a look at the new behavior of Variable.initialized_value (introduced in https://github.com/tensorflow/tensorflow/commit/b25d1c7d3e9f30925aa132ba62e79d281191a3dc ). Now it doesn't force initialization, so it can be safely used when initializing variables from other variables.\r\n\r\nThe only overhead of using this instead of the raw Variable is that it will trigger recomputing the initializer in the beginning of the step even if the variable has already been initialized (this is mildly annoying to fix but it's doable).", "Hi Alex, \r\n\r\nThanks for letting me know!\r\n\r\nI tested this on `1.1.0rc0` (that should include this commit, right?) and the sample I posted in this issue still fails with `FailedPreconditionError`.\r\n\r\nSzymon", "@nivwusquorum the releases are non-linear -- things are cherrypicked from Master into release using criteria I don't understand. On other hand, nightlies should be linear", "I just tried it on the nightly:\r\n\r\n`https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc0-cp35-cp35m-linux_x86_64.whl`\r\n\r\nAnd it still breaks in the same way.\r\n\r\n@alextp does it work for you for this specific example?", "I meant doing something like\nhttps://gist.github.com/alextp/ab074004cd7be65d762ac2d0ba123266 to it (i.e.\nusing V.initialized_value instead of V). I didn't try yet (a little swamped)\n\nOn Thu, Mar 30, 2017 at 12:29 PM, Szymon Sidor <notifications@github.com>\nwrote:\n\n> I just tried it on the nightly:\n>\n> https://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/\n> lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.\n> 0rc0-cp35-cp35m-linux_x86_64.whl\n>\n> And it still breaks in the same way.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8298#issuecomment-290518881>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVyh9jYUAWlBB88AW2eNhKGK8yVnks5rrAKSgaJpZM4MaGU0>\n> .\n>\n\n\n\n-- \n - Alex\n", "ah, I see, yeah it seems to work beautifully! Thanks very much!"]}, {"number": 8297, "title": "zhangshuo_R1.0", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Looks like misoperation. Closing PR."]}, {"number": 8296, "title": "MKL support for max/avg pooling and relu", "body": "This has been branched off the MKL pull request for convolution so please review the last 2 commits only. Once the MKL convolution pull request is complete, we will merge the tensorflow master here and that should make things clearer.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I'm the wrong person to review this.  Cc @rmlarsen or @benoitsteiner in case they're interested.", "Please restart tests.", "@tensorflow-jenkins test this please", "There's a failure with tensorflow/python:input_test that I can't reproduce on my setup. How do I access \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/bazel_pip/tensorflow/python/input_test/test.log\"?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Fixed some of the code review comments and rebased to the latest master to pull code from one of the other merged pull requests.", "Looks good, but please address my comments before submitting.", "@tensorflow-jenkins test this please", "I think this is ready for merge - please let me know if we missed anything.", "@tensorflow-jenkins test this please", "@andydavis1 Are we sure we want these ops in core immediately? Could we add them to contrib, or at least expose them in the python api in contrib first?", "I added [wip] to the title, remove it once the convolution change is it and this is unblocked. Thanks!", "Convolution changes have been added to the mkl-kernels pull request (https://github.com/tensorflow/tensorflow/pull/8184)", "@martinwicke thanks. We are almost through all of their kernels at this point, so it would be difficult to move things to contrib first.", "Ok. Maybe it's not a big problem here... how would these kernels be exposed?\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@martinwicke These kernels are applied in the graph by replacing their Eigen counterparts when MKL is enabled during configure. This option (in configure) is not enabled yet as we do not have all the pieces for MKL in the mainline yet.", "jenkins, test this please."]}, {"number": 8295, "title": "Fixed a bug where inception layers had 2 3x3 conv kernels instead of \u2026", "body": "\u2026one 3x3 and 5x5 kernel.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Jenkins, test this please (flakes)", "Please, Can I know what needs to be done next?\n\n- Manju\n\nOn Mar 23, 2017 4:29 AM, \"Sergio Guadarrama\" <notifications@github.com>\nwrote:\n\n> *@sguada* requested changes on this pull request.\n>\n> This would break the current checkpoint.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8295#pullrequestreview-28561832>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANNNQnDvAdDL6Ko3a1oaT59DDwPf6HAks5roecCgaJpZM4MaFQS>\n> .\n>\n", "Maybe make the change including an extra parameter that changes the behavior, but maintains the previous behavior by default.", "@manjunaths feel free to address.", "@manjunaths you would need to add an extra parameter to the network that keeps the old behavior by default, and that can be changed when needed.", "Hello @sguada ,\r\n\r\nThank you. I added a switch for this and left the original behavior as default. Can you review the code and see if it matches to the standards  or if anything more needs to be done ? I found one more issue addressing the same change issue #443. I think that can also be closed.\r\n\r\nThanks,\r\n- Manju", "@sguada can you take a look at the changes?", "Jenkins, test this please.", "@tensorflow-jenkins test this please", "The changes have not been done.", "@manjunaths any updates?", "Sorry, I have a personal emergency and am  unable to look at this for now.\n\n\n\nOn May 5, 2017 3:15 AM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n@manjunaths <https://github.com/manjunaths> any updates?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/8295#issuecomment-299317701>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AANNNZ3pYyz2tvkM6HpyPSOGtsiKQEPPks5r2kbigaJpZM4MaFQS>\n.\n", "I'm very sorry to hear that :(.  I hope things get better, and feel free to reopen a new PR if you are still able and interested."]}, {"number": 8294, "title": "Avoid using parallel_gpu_execute.sh on macos.", "body": "", "comments": ["Test failure is a known flaky test\r\nbazel_pip/tensorflow/python:input_test\r\n\r\nJenkins, test this please."]}, {"number": 8293, "title": "Could not use tensorboard in Win10?", "body": "When I attempt to use tensorboard to show the tensor scalar and image, however, it can not show the detail in the browser.\r\n![qq 20170311094208](https://cloud.githubusercontent.com/assets/16380725/23819278/0c0a960c-063f-11e7-87f0-e49730661e0d.png)\r\n\r\nBut when I used it in Ubuntu, it works. What's wrong with Win10?\r\n", "comments": ["Could you please provide details of how your are running TensorBoard and some output from the log files.\r\n\r\nAlso, please can you verify that you are not having the same problem as #7856\r\n\r\n", "YES.\u00a0It's\u00a0the\u00a0same\u00a0question\u00a0as\u00a0#7856.\u00a0When\u00a0transoform\u00a0c:/\u00a0to\u00a0f:/(where\u00a0work\u00a0directory\u00a0in).\u00a0It\u00a0works\u00a0perfectly.\r\n\r\nTHANKS\u00a0A\u00a0LOT.\r\n"]}, {"number": 8292, "title": "Update documentation for BN", "body": "The [documentation code](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm) to update `moving_mean` and `moving_variance` for BN raise an error with TF 1.0:\r\n \r\n```\r\nAttributeError: module 'tensorflow' has no attribute 'control_flow_ops'\r\n``` \r\n\r\nIt seems the new way to add dependencies between ops is to use the `tf.control_dependencies`.\r\n\r\nI also attached the `updates_op` to the `train_op` instead of `total_loss` to be more coherent with the associated warning: `[...] they need to be added as a dependency to the train_op, example:`. It also make more sense as we don't want to update `moving_mean` when computing the loss on the testing set.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 8291, "title": "No allowing GPU memory growth option in contrib.learn. Estimator", "body": "When creating estimator using contrib.learn, only GPU option that I can find is \"gpu_memory_fraction\" which can be found at \"tf.contrib.learn.RunConfig\"\r\n\r\n`classifier = learn.Estimator(\r\n    model_fn=cnn_model_fn,\r\n    config=learn.RunConfig(gpu_memory_fraction=0.9)\r\n    )`\r\n\r\nIs there a way to allow GPU memory growth option when using contrib.learn ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@taejoon89 you can use monkey-patching trick from https://github.com/tensorflow/tensorflow/issues/7794", "Thanks ~"]}, {"number": 8290, "title": "Branch 149810103", "body": "", "comments": ["Discussed with Gunan, merging despite the XLA failure."]}, {"number": 8289, "title": "Branch 149796694", "body": "", "comments": []}, {"number": 8288, "title": "Dynamic RNN, Initialization, Control Dependencies", "body": "I am trying to write a custom RNN Cell with regularized weights and am encountering a problem caused by the treatment of control flow dependencies.\r\n\r\nI initialize variables in the RNNCell as follows:\r\n\r\nwith tf.variable_scope(scope):\r\n  w = tf.get_variable(..., reg=tf.layers.l2_reg())\r\n\r\nand use the cell in a dynamic_rnn call.\r\n\r\nAt the end of graph construction, I retrieve all the regularized losses (using tf.GraphKeys.REGULARIZATION_LOSSES).\r\n\r\nWhen I add the regularization losses to my cross entropy loss, I run into the following error: \"InvalidArgumentError: The node 'Sum/input' has inputs from different frames.\" It seems that, because the variables are only created conditionally, they can't be used together with other tensors.\r\n\r\nCould you please suggest a workaround or provide a mechanism for adding regularization to an RNN?\r\n", "comments": ["Is this in the __call__?  I believe there's a workaround for this. Sergio?", "The variables and regularization should be created in advance."]}, {"number": 8287, "title": "filter_format for Native Layouts of Convolution Filter Weights", "body": "Repost from: https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-285217279.\r\n\r\nI suggest adding a `filter_format` to the conv2d op to indicate how the filter weights are stored (similar to `data_format` for the input / output format). \r\n\r\nThe [docs for TF](https://www.tensorflow.org/extend/tool_developers/#weight_formats) state: \r\n>The ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.\r\n\r\nThis will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).\r\n\r\nIn addition to cuDNN, BNNS (https://github.com/tensorflow/tensorflow/issues/3001) also uses that same format.\r\n\r\nThis needs to be benchmarked, but I assume there is some cost to having to transpose the filter weights during both fprop and bprop (twice). This could even explain some of the performance differences between TF and other frameworks (eg pytorch).\r\n\r\nIt looks like TF already [has some concept of these difference](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/dnn.h#L340-L346). Although as of now all weights are converted from / back to one canonical format.\r\n\r\nWhile [most users won't tune this](https://github.com/tensorflow/tensorflow/issues/8227), it could be used by xla / optimization passes to choose better layouts.", "comments": ["@zheng-xq Not sure if you have looked into this?\r\n\r\nI don't understand the comment about XLA and rewriting though - if the format of the data and filter are canonical, and consistently transformed under the hood to improve performance, it shouldn't make any difference whether the user program has tried to prematurely optimize the layout in a platform-specific manner ;-) ", "I was thinking right now if an optimization pass wanted to change the filter format, the current ops don't expose the parameter for rewriting. ", "There's no reason an optimization pass needs to keep the same ops...  In my personal opinion, the cleanest design is for the program to be written using generic ops with well defined semantics, and then any 3rd party wanting to add support for their super-fast library or accelerator supplies two things:\r\n1) A set of additional (internal) ops if necessary, with whatever flags/attrs extra inputs/outputs they feel like, and\r\n2) A GraphOptimizationPass which transparently, at execution time, rewrites the clean, portable graphdef to use their platform specific ops (the rewritten graph is cached so this is a one-off overhead)\r\n\r\nThis way we avoid the need to add extra vendor- and platform-specific options to every op in TensorFlow ;-) \r\n\r\nWhen we translate parts of a graph into XLA, entire parts of the graph are replaced by a single 'XlaLaunch' op and and conv ops in the fused computation get turned into XLA convolution operators which support arbitrary layout permutations.  The XLA optimizer is free to do what it likes with physical layout (though on GPU we still currently call out to cuDNN).", "Your point about optimization passes makes sense. I guess a better case would be, does it make sense to allow users to _explicitly_ choose a different format for the filter weights, and if so, should that be done by adding a new parameter to the existing conv2d op? The idea would be in the vein of `data_format`. It actually might be more important to make this explicit since when the user dumps the weights out, they need to know what format they are in.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I think this is still relevant as an open issue.", "@cancan101 It looks like no one is working on it, or planning to work on it.  Is that right?"]}, {"number": 8286, "title": "Pass NHWC Tensors to cuDNN Directly", "body": "Currently when running `conv2d` on cuDNN with image data in `NHWC` format, TF [converts the input tensor from NHWC to NCHW](https://github.com/tensorflow/tensorflow/blob/0be81439c91e297b078152dd0c266471b24bde7f/tensorflow/core/kernels/conv_ops.cc#L558-L575), runs the conv and then [converts the output tensor back from NHWC to NCHW](https://github.com/tensorflow/tensorflow/blob/0be81439c91e297b078152dd0c266471b24bde7f/tensorflow/core/kernels/conv_ops.cc#L711-L720). However, cuDNN natively supports taking in data in `NHWC` format. \r\n\r\nAssuming this has not been tested and concluded to be optimal, my prior would be to allow Nvidia to handle the optimal execution of a conv2d on `NHWC` data rather trying to transpose within TF.", "comments": ["Does CuDNN `NHWC` run faster than TF transpose + `NCHW`? Is anyone else using CuDNN `NHWC`? I see that PyTorch only supports `NCWH`\r\n\r\ncc @zheng-xq ", "My prior would be that if TF transpose + NCHW + unTranspose were faster, then Nvidia would do that under the surface (which may in fact be what they do). Given that the default layout in TF is `NCHW`, seems reasonable to see if this path can be made faster.", "Our previous experiments on earlier Cudnn versions show that conversion + NCHW is faster. If there are evidence showing this has changed, we can look into this again. \r\n", "Looks like cuddn 4 was the first to support NHWC, so perhaps they improved in 5/5.1?", "Since people are converging on NCWH, there's less reason for NVidia to improve NHWC, so it could be even slower in the latest version. Will reopen if there's evidence this feature is desirable", "@yaroslavvb Can you clarify what you mean? I thought movement was toward NCHW and away from NHWC, if anything. PyTorch doesn't even support NHWC.", "@yaroslavvb Was that a typo in your latest post? (Edit: Was fixed.)\r\nBtw., why is that that NCWH is faster than NHWC? ([Related StackOverflow question.](https://stackoverflow.com/questions/44280335/how-much-faster-is-nchw-compared-to-nhwc-in-tensorflow-cudnn)) Is that also the official Nvidia statement, or have people just seen that in benchmarks? Do you have any references for such benchmarks?\r\n", "Is it right that the CPU implementation currently only supports NHWC and fails otherwise? Looked like that in the code, no automatic transposing. But maybe I'm missing something.", "I think @yaroslavvb's post was a typo \u2013 NCHW is cuDNN's native format, and is what most other DL libraries use. As such the justification is perhaps that the built-in NHWC operations in cuDNN are slower than transpose-run-transpose.\r\n\r\nThe CPU implementations in TF largely do only support NHWC, and will fail otherwise.", "@albertz The official TensorFlow documentation states that NCHW is (typically, not always!) faster on GPUs: https://www.tensorflow.org/performance/performance_guide#use_nchw_imag", "ah yes, I got h's and c's backwards, edited", "Please Noted, NCHW is also prefer data format from CPU side !!!\r\n\"Set to NCHW format to get maximum performance.\" quote from https://ai.intel.com/tensorflow-optimizations-intel-xeon-scalable-processor/", "Having built a CNN using multiple go-routines in go. I can see that NCHW is at least easier to stack the outputs of each of the neurons in NCHW.  Since, it naturally wants to work out that way (Even if you do a convolution in NHWC the outputs want to stack into NCHW). That being said. In NHWC the channels of the weights and inputs line up and the cpu doesn't need to skip around in memory when doing the convolution, and NHWC beats NCHW in single threaded performance.  I would say depending on the resources at your disposal it would be best to test both to see which one works better.", "> Please Noted, NCHW is also prefer data format from CPU side !!!\r\n> \"Set to NCHW format to get maximum performance.\" quote from https://ai.intel.com/tensorflow-optimizations-intel-xeon-scalable-processor/\r\n\r\n@melodylail Maybe they are benefiting from `NCHW[x]c` layout (check *Blocked layout* section of this [mkl-dnn doc](https://intel.github.io/mkl-dnn/understanding_memory_formats.html) rather than NCHW."]}, {"number": 8285, "title": "[Windows] Tensorflow GPU fails to find CUDA", "body": "I have downloaded cuDNN v5.1 and CUDA 8.0 and also moved the folders lib. include, and bin  to the Cuda directory where i installed it.\r\n\r\nC:\\Users\\Owner\\Desktop\\tensorflow>python try.py\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll\r\nlocally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll lo\r\ncally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll l\r\nocally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll local\r\nly\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll\r\nlocally\r\nExtracting /tmp/data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\r\n\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information\r\n for host: Owner-PC\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stre\r\nam_executor\\cuda\\cuda_diagnostics.cc:165] hostname: Owner-PC\r\n\r\nIs there any solution for that ??\r\n", "comments": ["Please see the following instructions: https://www.tensorflow.org/install/install_windows\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8284, "title": "TF_ImportGraphDef crashes for the following graph", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04 \r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n#### CUDA\r\n-rw-r--r-- 1 root root 558720 Nov 30 13:53 libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Nov 30 13:53 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Nov 30 13:53 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root 415432 Nov 30 13:53 libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 775162 Nov 30 13:53 libcudart_static.a\r\n\r\n\r\n#### CUDNN\r\nlrwxrwxrwx 1 ajayaram ajayaram       13 Nov 30 14:02 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 ajayaram ajayaram       17 Nov 30 14:02 libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxrwxr-x 1 ajayaram ajayaram 79337624 Jul 27  2016 libcudnn.so.5.1.5\r\n-rw-rw-r-- 1 ajayaram ajayaram 69756172 Jul 27  2016 libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n29a6b4661258ef99842904d7c54993c963a8c2c0\r\n\r\n2. The output of `bazel version`\r\n.............................\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n[lstm_issue.zip](https://github.com/tensorflow/tensorflow/files/834801/lstm_issue.zip)\r\n\r\n\r\n```\r\n#lstmkt.py : Write a keras model with LSTM layer to tensorflow graph\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Input, Dense, Reshape\r\nfrom keras.layers.recurrent import LSTM\r\nfrom keras import backend as K\r\n\r\nimport tensorflow as tf\r\n\r\ndef build_model():\r\n  model = Sequential()\t\r\n  model.add(LSTM(4, input_shape= (1,1) ))  \r\n  model.add(Dense(1))\r\n  model.compile(optimizer='adam', loss='mse')\r\n  return model\r\n\r\nmodel = build_model()\r\nsess = K.get_session()\r\ngraph = sess.graph\r\nsess.run('init')\r\n\r\ntf.train.write_graph(sess.graph, './', 'lstmkt.pb', as_text=False)\r\n```\r\n\r\n```\r\nvoid free_buffer(void* data, size_t length);\r\nTF_Buffer* read_file(const char* file);\r\n\r\nint main()\r\n{\r\n\r\n    const char filename[] = \"lstmkt.pb\";\r\n    \r\n    TF_Status* s = TF_NewStatus();\r\n    TF_Graph* graph = TF_NewGraph();\r\n    TF_Buffer* graph_def = read_file(filename);\r\n    \r\n    // Import graph\r\n    TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\r\n \r\n    // This line of code crashes if the keras model contains an LSTM layer\r\n    TF_GraphImportGraphDef(graph, graph_def, opts, s);\r\n\r\n    return 0;\r\n}\r\n```\r\n## What other attempted solutions have you tried?\r\nNone. Stuck here. Works in python but I need to load this graph in C.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nStack Trace (from fault):\r\n[  0] 0x00007f91b7a87338 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+63492920 _ZN10tensorflow15shape_inference16InferenceContext8WithRankENS0_11ShapeHandleEiPS2_+00000024\r\n[  1] 0x00007f91b66b8c88 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+42724488\r\n[  2] 0x00007f91b7a65a44 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+63355460 _ZNSt17_Function_handlerIFN10tensorflow6StatusEPNS0_15shape_inference16InferenceContextEEPS5_E9_M_invokeERKSt9_Any_dataOS4_+00000020\r\n[  3] 0x00007f91b79e2094 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+62816404 _ZN10tensorflow12ShapeRefiner7AddNodeEPKNS_4NodeE+00002996\r\n[  4] 0x00007f91b5bd42e5 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+31302373\r\n[  5] 0x00007f91b7a0dc2c /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+62995500\r\n[  6] 0x00007f91b7a0e6d3 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+62998227 _ZN10tensorflow14ImportGraphDefERKNS_21ImportGraphDefOptionsERKNS_8GraphDefEPNS_5GraphEPNS_12ShapeRefinerE+00000499\r\n[  7] 0x00007f91b5cb2434 /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+32212020\r\n[  8] 0x00007f91b5cb52fc /home/ajayaram/tensorflow/bazel-bin/tensorflow/libtensorflow.so+32223996 TF_GraphImportGraphDef+00000204\r\n```", "comments": ["@ajayaraman : Thanks for the very detailed report and the instructions to reproduce it. This is very helpful. There seems to be some issue triggered in the shape inference code, I'm looking into it."]}, {"number": 8283, "title": "tensorflow/compiler/xla/service/allocation_tracker.cc:178:54: error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]", "body": "\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/8238\r\n\r\n### Environment info\r\nOS X 10.11.6\r\n$ clang --version\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n\r\nInstalled version of CUDA and cuDNN: \r\ndrwxrwxrwx  15 root  wheel  510 May  3  2015 CUDA-7.0\r\nDavid-Laxers-MacBook-Pro:tensorflow davidlaxer$ ls -l /Developer/NVIDIA/CUDA-7.0/lib/libcud*\r\n-rw-r--r--  1 davidlaxer  staff  292184 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudadevrt.a\r\n-rwxr-xr-x  1 davidlaxer  staff  274176 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudart.7.0.dylib\r\nlrwxr-xr-x  1 davidlaxer  staff      19 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudart.dylib -> libcudart.7.0.dylib\r\n-rw-r--r--  1 davidlaxer  staff  562856 Mar  6  2015 /Developer/NVIDIA/CUDA-7.0/lib/libcudart_static.a\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nprint(tensorflow.__version__)\r\n1.0.1\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n\r\n99e30bc6b22b259ddc6a2cfc6aec1d9ebc635da4\r\n\r\n2. The output of `bazel version`\r\n\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 15:54:21 2016 (1481126061)\r\nBuild timestamp: 1481126061\r\nBuild timestamp as int: 1481126061\r\n", "comments": ["Closing as duplicate of #8238 \r\n@dbl001 Not sure why you opened a new issue given you reference the identical problem?"]}, {"number": 8282, "title": "fixed a bug in tensordot", "body": "This should address #8281", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this please", "Discussed this with Moritz Hardt offline and he LGTM'd, going to let him connect his github account and officially approve it.\r\nPending test passage, of course", "I'll just merge given @dandelionmane's latest comment :)\r\n\r\nJenkins, test this please"]}, {"number": 8281, "title": "Tensordot sometimes fails one of the operands has a single dimension", "body": "This error occurs when one operand has a single dimension, and the other operand has a dimension of None.  To reproduce:\r\n\r\nA = tf.placeholder(tf.float32, [None, 20])\r\nB = tf.placeholder(tf.float32, [20])\r\nC = tf.tensordot(A, B, [[1], [0]])\r\n\r\nthrows `TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.` when trying to calculate the shape of the resulting tensor.\r\n\r\nI've actually fixed this, I'll submit a pull request momentarily.  I just figured I would submit an issue first for clarity.\r\nThis is probably related to #6682; I'll might look into that as well", "comments": ["Thanks.\r\n\r\nOK - leaving open as a tracking bug.  I'll assign the 'awaiting response' tag to record the fact that we're expecting to see a PR", "By the way, I've submitted the pull request at #8282", "Resolved with #8282", "TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types"]}, {"number": 8280, "title": "Reduction Functionality for Java API", "body": "Hey! It seems like the reduction functionality available in Python (https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/reduction) has not made it to the Java API. Are there plans to port it over yet? There are some useful functions such as `reduce_mean` which I need.", "comments": ["@asimshankar Could you please comment?", "This question is probably better suited for [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) since the primitives for building such operations are available in the Java API, so there isn't a missing \"feature\" as such  :)\r\n\r\nThe Python reduction functions like [`tf.reduce_mean`](https://github.com/tensorflow/tensorflow/blob/169a096/tensorflow/python/ops/math_ops.py#L1329) are simple wrappers over the TensorFlow operations like [`Mean`](https://github.com/tensorflow/tensorflow/blob/169a096a2d9f4c129363d3f56b8b25c9ca4d8aa6/tensorflow/core/ops/math_ops.cc#L1102), where it fills in the `ReductionIndices` argument based on the shape of the input.\r\n\r\nFor example, to reduce a vector of floats, the following works:\r\n```java\r\nimport org.tensorflow.DataType;\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Output;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.Tensor;\r\n\r\npublic class ReduceMean {\r\n  public static void main(String[] args) {\r\n    try (Graph g = new Graph();\r\n        Session s = new Session(g)) {\r\n      // Build the graph\r\n      Output placeholder =\r\n          g.opBuilder(\"Placeholder\", \"input\").setAttr(\"dtype\", DataType.FLOAT).build().output(0);\r\n      Output mean = reduce(g, \"Mean\", \"output\", placeholder);\r\n\r\n      // Execute it\r\n      try (Tensor in = Tensor.create(new float[]{1,2,3,4,5});\r\n          Tensor out = s.runner().feed(\"input\", in).fetch(\"output\").run().get(0)) {\r\n        System.out.println(\"Result: \" + out.floatValue());\r\n      }\r\n    }\r\n  }\r\n\r\n  public static Output reduce(Graph g, String type, String name, Output input) {\r\n    try (Tensor t = Tensor.create(new int[] {0})) {\r\n      Output reductionIndices =\r\n          g.opBuilder(\"Const\", \"ReductionIndices\")\r\n              .setAttr(\"dtype\", t.dataType())\r\n              .setAttr(\"value\", t)\r\n              .build()\r\n              .output(0);\r\n      return g.opBuilder(type, name)\r\n          .setAttr(\"T\", DataType.FLOAT)\r\n          .setAttr(\"Tidx\", DataType.INT32)\r\n          .addInput(input)\r\n          .addInput(reductionIndices)\r\n          .build()\r\n          .output(0);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAdmittedly, this is not easy to discover, primarily because we don't yet generate Java code for all the TensorFlow ops (and thus they aren't discoverable via javadoc).  #7149  is about that.\r\n\r\nI'll close this issue for now, given that I think it is helped by #7149 and similar questions are probably best suited for stackoverflow at this time."]}, {"number": 8279, "title": "TF Learn: updating links for tutorials", "body": "I can also just link to the docs in gh [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src/tutorials) and [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src/get_started) if that's preferred. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please\r\neven though as a docs only change it really doesn't need testing :)"]}, {"number": 8278, "title": "Using 3-dimensional Tensor in Java", "body": "Hi! After feeding an input image through my session I receive a `FLOAT tensor with shape [299, 299, 3]`.\r\n\r\nIt doesn't seem like `reduce_mean` has made it into the Java API yet. Is there anything else I can do to use my output tensor?", "comments": []}, {"number": 8277, "title": "Different Code Path Taken in conv2d for Constant vs Variable Filter", "body": "It appears that a different code path is taken in `conv2d` for Constant vs Variable filters.\r\n\r\nOn a box with GPU, this works:\r\n```python\r\nimages = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))\r\nfilters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    output = nn_ops.conv2d(\r\n      images,\r\n      filters,\r\n      strides=(1,1,1,1),\r\n      padding='VALID',\r\n      data_format='NCHW',\r\n  ).eval()\r\n```\r\nhowever this fails with `Check failed: data_format == FORMAT_NHWC Generic conv implementation only supports NHWC tensor format for now.`:\r\n```python\r\nimages = tf.constant(np.arange(2*15. * 15*2).reshape((2,2,15,15)).astype(np.float32))\r\nfilters = tf.constant(1 * np.ones((1,1,2,1), np.float32))\r\nwith tf.Session() as sess:\r\n    output = nn_ops.conv2d(\r\n      images,\r\n      filters,\r\n      strides=(1,1,1,1),\r\n      padding='VALID',\r\n      data_format='NCHW',\r\n  ).eval()\r\n```\r\n\r\nBoth of these work (the data_format is supported):\r\n```python\r\nimages = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))\r\nfilters = tf.Variable(initial_value=1 * np.ones((1,1,2,1), np.float32))\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    output = nn_ops.conv2d(\r\n      images,\r\n      filters,\r\n      strides=(1,1,1,1),\r\n      padding='VALID',\r\n  ).eval()\r\n```\r\nand\r\n```python\r\nimages = tf.constant(np.arange(2*15. * 15*2).reshape((2,15,15,2)).astype(np.float32))\r\nfilters = tf.constant(1 * np.ones((1,1,2,1), np.float32))\r\nwith tf.Session() as sess:\r\n    output = nn_ops.conv2d(\r\n      images,\r\n      filters,\r\n      strides=(1,1,1,1),\r\n      padding='VALID',\r\n  ).eval()\r\n```\r\nI don't see a reason for the Constant filter to take what I am guessing is a less efficient code path (not using GPU op) than the Variable Filter.\r\n\r\nTo make things even weirder, it seems like even though the GPU is not being used for the Constant, there is still cuda `memcpy`. Run on 20 calls:\r\n```\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 52.45%  90.108us        20  4.5050us  4.0950us  6.0800us  [CUDA memcpy HtoD]\r\n 42.91%  73.726us        20  3.6860us  3.5520us  4.5120us  [CUDA memcpy DtoH]\r\n  4.64%  7.9680us         1  7.9680us  7.9680us  7.9680us  [CUDA memset]\r\n```", "comments": ["Is there any way to convert a pre-trained NCHW weight into NHWC and load it into the same graph?", "What do you mean? Weights are always stored the same way. ", "@tfboyd Any ideas?", "Hi @cancan101 ,\r\n\r\nI've a net (graph) with some conv2D and maxPool operations declared using NCHW format. I've trained the net on GPUs and created some checkpoints. The graph and the model works well on GPUs when you test by restoring the checkpoints.\r\n\r\nNow I want to test the same code/graph on CPU.  The same net/graph is not working because the conv2D and maxPool operations do not run since they were declared using NCHW.\r\nI'm getting the following error:\r\n\r\n```\r\n2017-09-18 17:57:48.890761: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC.\r\n         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"SAME\", strides=[1, 1, 2, 2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](text_box_300/conv1/conv1_2/Relu)]]\r\n2017-09-18 17:57:48.892768: E main_textd_test.cc:457] Running model failed: Invalid argument: Default MaxPoolingOp only supports NHWC.\r\n         [[Node: text_box_300/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 2, 2], padding=\"SAME\", strides=[1, 1, 2, 2], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](text_box_300/conv1/conv1_2/Relu)]]\r\n```\r\n\r\nSo I've modified the net so that the operations are declared using NHWC format. Is it possible to restore the same checkpoint weights with this new net/graph without retraining the new graph (that has  NHWC ops) ?\r\n\r\nfinally I want to freeze the graph and have a standalone graph with weights to run/test forward pass.  I was not able to create a standalone graph using checkpoints I've got when I trained me NCHW graph and use the standalone graph on CPU for testing (running forward pass) ? Is there a way to do it ?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I tried with tensorflow-gpu v1.4.0, and wasn't able to reproduce this issue. Close for now. Please feel free to reopen it if you think the issue still exists."]}, {"number": 8276, "title": "Fix XLA build breakage on Mac OS X.", "body": "Fixes two compilation problems on Mac OS X:\r\n* call_graph.*: `error: incomplete type 'xla::CallGraph' used in type trait expression`\r\n* allocation_tracker.cc: `error: non-constant-expression cannot be narrowed from type 'std::vector<se::DeviceMemoryBase>::size_type' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]`", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 8275, "title": "make sparse_tensor (SparseTensor) serializable in meta graph", "body": "`tf.sparse_placeholder` Sparse tensors aren't serialized in meta graphs, one has to serialize tf.placeholder tensors for the indices and values individually.\r\n\r\nAt present `add_collection_def` (called by `create_meta_graph_def`, which is typically called by `export_scoped_meta_graph`, found in [/tensorflow/python/framework/meta_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py), ) **fails** when a `key` resolves to a `SparseTensor` as `SparseTensor`s do not have `name` attribute: here is the specific [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py#L290).\r\n\r\n## Suggested solutions:\r\n- give `SparseTensor` a `name` attribute and any other methods and properties necessary\r\n\r\nor\r\n\r\n- Enable `SparseTensor` or `sparse_placeholder` to be added to `_proto_function_registry` so that [ops.get_to_proto_function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py#L275) works in `add_collection_def`, perhaps using the `SAVEABLE_OBJECTS` key, something like the following being found in the SparseTensor file:\r\n\r\n```\r\nops.register_proto_function(\r\n    ops.GraphKeys.SAVERS,\r\n    proto_type=saver_pb2.SaverDef,\r\n    to_proto=Saver.to_proto,\r\n    from_proto=Saver.from_proto)\r\n```\r\n\r\nspeculative, I don't know the best implementation:\r\n\r\nCould do something similar to the [`_as_graph_def`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L2114) method of `Graph` and loop through `graph._nodes_by_id`, compare the `SparseTensor`'s `_indices` to an `op` (node) `_outputs` from `_nodes_by_id`.\r\n\r\n## Small Test\r\n\r\n```python\r\n\r\ng = tf.Graph()\r\nwith g.as_default(), tf.Session(graph=g).as_default() as sess:\r\n  st = tf.sparse_placeholder(tf.string)\r\n  ops.add_to_collection('_sparse_test', st)\r\n  ops.add_to_collection('_test', tf.placeholder(tf.string))\r\n  # FORMERLY GAVE A WARNING WHEN EXCEPTION THROWN\r\n  tf.train.export_meta_graph('tmp_model/test-model.meta', as_text=True, graph=g,\r\n                             collection_list=['_sparse_test', '_test']) \r\n  # WORKS\r\n  print(sess.run(st, feed_dict={ st: ([[i,0] for i in range(10)],\r\n                                      list(map(str,range(10))),\r\n                                      [10,1]) }))\r\n    \r\ng2 = tf.Graph()\r\nwith g2.as_default(), tf.Session(graph=g2).as_default() as sess2:\r\n  tf.train.import_meta_graph('tmp_model/test-model.meta')\r\n  t = tf.get_collection('_test')[0]\r\n  # FAILS\r\n  st = tf.get_collection('_sparse_test')[0]\r\n  print(sess2.run(st, feed_dict={ st: ([[i,0] for i in range(10)],\r\n                                      list(map(str,range(20,30))),\r\n                                      [10,1]) }))\r\n\r\n\r\n```\r\n\r\nSometimes I get a segfault with this", "comments": ["@concretevitamin Could you please comment?", "Adding @ebrevdo who knows more about SparseTensor and @sherrym who is the owner of MetaGraph.", "Seems reasonable (we can always add a 4th argument with default=None);\nthough I wonder if we can just pull a name from the name scope of the\nindices tensor.  sherry, think it's reasonable to add ser/de for\nSparseTensors?\n\nOn Fri, Mar 10, 2017 at 2:23 PM, Zongheng Yang <notifications@github.com>\nwrote:\n\n> Adding @ebrevdo <https://github.com/ebrevdo> who knows more about\n> SparseTensor and @sherrym <https://github.com/sherrym> who is the owner\n> of MetaGraph.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8275#issuecomment-285801485>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwngtULJae_6oa03t_E2CYo75sHBks5rkc1vgaJpZM4MZlx2>\n> .\n>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am facing the same issue while creating SparseTensor() with indices dtype int32.\r\nSparseTensor with int64 data graph is not loading in Android.\r\nError:\r\nCaused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32\r\n; NodeDef: Less_1 = Less[T=DT_INT64](ToInt64_2, ToInt64_3); Op<name=Less; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_INT32]>\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:439)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.(TensorFlowInferenceInterface.java:98)\r\n\r\n\r\nIf we change dtype int64 to int32 SparseTensor give the error.\r\nFile \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 119, in init\r\nindices, name=\"indices\", dtype=dtypes.int64)\r\nFile \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 676, in convert_to_tensor\r\nas_ref=False)\r\nFile \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 741, in internal_convert_to_tensor\r\nret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\nFile \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 614, in _TensorTensorConversionFunction\r\n% (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"x_indices:0\", shape=(?, 3), dtype=int32)'", "Yours is an unrelated problem. Can you create a new issue?", "@manishvatsa your problem are solved?"]}, {"number": 8274, "title": "LSTM Network", "body": "Hi, I have the error below when i execute the line \"\"lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(config.n_hidden, forget_bias=1.0)\"\"\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'\r\nI was installed Tensorflow on windows using \"Installing with Anaconda\"\r\nAny one have the same error and solve it Please!", "comments": ["they were all moved to tf.contrib.rnn.core_rnn_cell, if you read any tutorial about rnn like language model on the official site,  you will found that", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8273, "title": "Error:", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Empty issue report.  Closing."]}, {"number": 8272, "title": "Fix the initializer and make global variable specified in word2vector example", "body": "- Fix the initializer\r\n- Make global variable explicitly specified\r\nOriginal modification in #7872 is not for master.\r\n```\r\nWARNING:tensorflow:From word2vec_basic.py: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nOriginal version works for me. The referenced issue is not for current version.\r\n```\r\nThe modification will cause the example arise this warning.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "Looks like the cmake issue is a tool failure. Since it's a py-only change I feel comfortable merging it. Thanks for the fix!"]}, {"number": 8271, "title": "ImportError: No module named 'tensorflow.contrib.ffmpeg.ops'", "body": "### Environment info\r\nOperating System: Windows 10\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0, cuDNN 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: `tensorflow_gpu-1.0.1-cp35-cp35m-win_amd64.whl`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: 1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n`python -c \"from tensorflow.contrib import ffmpeg\"`\r\n\r\nOutput:\r\n\r\n`Traceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\Guillaume COTER\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\ffmpeg\\__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.ffmpeg.ffmpeg_ops import decode_audio\r\n  File \"C:\\Users\\Guillaume COTER\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\ffmpeg\\ffmpeg_ops.py\", line 23, in <module>\r\n    from tensorflow.contrib.ffmpeg.ops import gen_decode_audio_op_py\r\nImportError: No module named 'tensorflow.contrib.ffmpeg.ops'`", "comments": ["The `tf.contrib.ffmpeg` ops aren't currently supported on Windows. As far as I remember, they rely on some POSIX-specific code for running a subprocess that would need to be ported to using `CreateProcess()`. I've assigned @fredbertsch, who's the owner of that code, to comment on the chances of this being supported on Windows in the future.", "Same question, any news on support?", "@fredbertsch Any comments on windows support?", "Having the same issue here on Win10. Everything else is working fine.", "Same problem here, is there a workaround? I don't think @fredbertsch will fix this any time soon as he seems quite busy (https://groups.google.com/a/tensorflow.org/forum/#!topic/magenta-discuss/gVdc5345qYQ)\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@mrry irrespectively of support for the module itself, it breaks inspection module, as `ismodule` fails to inspect `tensorflow.contrib.ffmpeg` as a member of `tensorflow.contrib`"]}]