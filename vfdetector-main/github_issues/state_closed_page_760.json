[{"number": 30759, "title": "Add the XLA_FLAGS xla_gpu_ptx_code to allow specifing the PTX code to use in XLA", "body": "This add an XLA_FLAGS xla_gpu_ptx_code to allow specifing the PTX code to use in XLA.\r\nThe goal is to allow quick testing of new PTX code to compare the speed.\r\n\r\n@thomasjoerg ", "comments": ["I pushed fixes for all the comment except one.\r\n\r\nYou suggested having only 1 VLOG per module, which is good (if we loaded PTX from a file or not).\r\nBut I understood that you also wanted to raise the VLOG to 2 for the VLOG left after the above change. Is that right?\r\n\r\nI think we should keep them at 0. By default, when the option isn't used, there is nothing more printed.\r\nIt is easy to have a typo in the file name while using this feature. I had that problem when using it. Having no output by default makes it hard to catch that.\r\nAlso, there is already performance-related information by default. When this feature is used, this change the behavior of XLA. So it would be great to have a minimal log about it.\r\nI expect this debug/code development feature to be used in small enough graph with replay_computation. So there shouldn't be too many lines printed.\r\nBut as I wrote, when this feature isn't used, nothing is printed.\r\n\r\nIf you still prefer me to change the VLOG level, tell me.\r\n", "I rebased this PR to fix a merge conflict."]}, {"number": 30758, "title": "MAXIMUM and RESIZE_NEAREST_NEIGHBOR not available for tensorflow-lite-gpu", "body": "**System information**\r\n- OS Platform and Distribution: Android Oreo\r\n- TensorFlow installed from: source\r\n- TensorFlow version (or github SHA if from source):\r\nUsing both tensorflow-lite and tensorflow-lite-gpu libraries\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n\r\n**Provide the text output from tflite-gpu**\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MAXIMUM: Operation is not supported.\r\n    RESIZE_NEAREST_NEIGHBOR: Operation is not supported.\r\n    First 3 operations will run on the GPU, and the remaining 74 on the CPU.TfLiteGpuDelegate Prepare: Output tensor is not found in the graph.Node number 77 (TfLiteGpuDelegate) failed to prepare.\r\n```\r\n\r\nDo you plan on implementing the MAXIMUM and RESIZE_NEAREST_NEIGHBOR ops to run on the gpu build of tensorflow-lite-gpu? I'm using a neural network, which up-samples from feature vectors.\r\n", "comments": ["@hvalev \r\n\r\nMy sincere apologies for the super late reply; for some reason, this completely dropped off the radar.\r\n\r\nWe have received multiple requests for these two operations and will be prioritized.  Not so sure about `MAXIMUM` but `RESIZE_NEAREST_NEIGHBOR` should come for sure.  Don't have an ETA yet; we're quite short staffed at the moment.", "@impjdi \r\nExpecting your answers.\r\n\r\nIf GPU doesn't support such kinds of operations then will it affect the detection results?\r\n\r\nI use yolov3 model and the following is my code. \r\nI have run the following code successfully without GPU module.\r\nMy model is float model not quantized model.\r\n\r\n\r\n`/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\npackage org.tensorflow.lite.examples.detection.tflite;\r\n\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.RectF;\r\nimport android.os.SystemClock;\r\nimport android.os.Trace;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.examples.detection.env.Logger;\r\n\r\nimport java.io.BufferedReader;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.io.InputStream;\r\nimport java.io.InputStreamReader;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\nimport java.util.ArrayList;\r\nimport java.util.HashMap;\r\nimport java.util.Iterator;\r\nimport java.util.List;\r\nimport java.util.Map;\r\nimport java.util.TreeMap;\r\nimport java.util.Vector;\r\nimport org.tensorflow.lite.gpu.GpuDelegate;\r\nimport org.tensorflow.lite.nnapi.NnApiDelegate;\r\n/**\r\n * Wrapper for frozen detection models trained using the Tensorflow Object Detection API:\r\n * github.com/tensorflow/models/tree/master/research/object_detection\r\n */\r\npublic class TFLitePanoObjectDetectionAPIModel implements Classifier {\r\n  private static final Logger LOGGER = new Logger();\r\n\r\n  // Only return this many results.\r\n  private static final int NUM_of_classes = 19;\r\n  // Float model\r\n  // Number of threads in the java app\r\n  private static final int NUM_THREADS = 4;\r\n  private boolean isModelQuantized;\r\n  // Config values.\r\n  private int inputSize;\r\n  // Pre-allocated buffers.\r\n  private Vector<String> labels = new Vector<String>();\r\n  private int[] intValues;\r\n  // outputLocations: array of shape [Batchsize, NUM_DETECTIONS,4]\r\n  // contains the location of detected boxes\r\n  private float[][][][] output_1;\r\n  // outputClasses: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the classes of detected boxes\r\n  private float[][][][] output_2;\r\n  // outputScores: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the scores of detected boxes\r\n  private float[][][][] output_3;\r\n  // numDetections: array of shape [Batchsize]\r\n  // contains the number of detected boxes\r\n  //private float[] numDetections;\r\n\r\n  private ByteBuffer imgData;\r\n\r\n  private Interpreter tflite;\r\n  private MappedByteBuffer tfliteModel;\r\n  private final Interpreter.Options tfliteOptions = new Interpreter.Options();\r\n  private GpuDelegate gpuDelegate = null;\r\n  private NnApiDelegate nnapiDelegate = null;\r\n  private int gridNum;\r\n\r\n  private int BoxNum_each_gird=3;\r\n  private float[][][][] floatValues;\r\n\r\n  private static final float[] ANCHORS = {\r\n          21.0f, 18.0f,\r\n          30.0f, 26.0f,\r\n          38.0f, 44.0f,\r\n          53.0f, 31.0f,\r\n          73.0f, 64.0f,\r\n          94.0f, 37.0f,\r\n          118.0f, 107.0f,\r\n          163.0f, 56.0f,\r\n          177.0f, 175.0f\r\n  };\r\n\r\n  private static final String[] LABELS = {\r\n          \"handle\",\r\n          \"dashboard\",\r\n          \"control_panel\",\r\n          \"air_conditioner\",\r\n          \"navigation_display\",\r\n          \"lever\",\r\n          \"drink_holder\",\r\n          \"seat_belt_buckles\",\r\n          \"side_break_lever\",\r\n          \"gear_stick\",\r\n          \"tripod\",\r\n          \"door_mirror\",\r\n          \"door_handle\",\r\n          \"window_power_switch\",\r\n          \"seat_belt_tongue\",\r\n          \"room_light\",\r\n          \"room_mirror\",\r\n          \"sun_visor\",\r\n          \"seat_head\"\r\n  };\r\n\r\n\r\n  private TFLitePanoObjectDetectionAPIModel() {}\r\n  private float scoreThreshold = 0.3f;\r\n  private int blockSize=32;\r\n  /** Memory-map the model file in Assets. */\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)\r\n      throws IOException {\r\n    AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n\r\n\r\n  public static Classifier create(\r\n      final AssetManager assetManager,\r\n      final String modelFilename,\r\n      final String labelFilename,\r\n      final int inputSize,\r\n      final boolean isQuantized)\r\n      throws IOException {\r\n    final TFLitePanoObjectDetectionAPIModel d = new TFLitePanoObjectDetectionAPIModel();\r\n\r\n    InputStream labelsInput = null;\r\n    String actualFilename = labelFilename.split(\"file:///android_asset/\")[1];\r\n    labelsInput = assetManager.open(actualFilename);\r\n    BufferedReader br = null;\r\n    br = new BufferedReader(new InputStreamReader(labelsInput));\r\n    String line;\r\n    while ((line = br.readLine()) != null) {\r\n      LOGGER.w(line);\r\n      d.labels.add(line);\r\n    }\r\n    br.close();\r\n\r\n    d.inputSize = inputSize;\r\n\r\n\r\n    try {\r\n\r\n      d.tflite = new Interpreter(loadModelFile(assetManager, modelFilename));//--cpu\r\n      d.tfliteModel=loadModelFile(assetManager, modelFilename);\r\n      if (d.gpuDelegate == null)\r\n      {\r\n        d.gpuDelegate = new GpuDelegate();\r\n        d.tfliteOptions.addDelegate(d.gpuDelegate);\r\n      }\r\n        if(d.tflite!=null)\r\n        {\r\n            d.tflite.close();\r\n            d.tflite = new Interpreter(d.tfliteModel,d.tfliteOptions);\r\n        }\r\n\r\n//      d.tflite.setNumThreads(6);//cpu \u8d77\u591a\u7ebf\u7a0b\u7684\u65f6\u5019\u53ef\u4ee5\u52a0\u901f\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n    d.isModelQuantized = isQuantized;\r\n    // Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 4; // Floating point\r\n    }\r\n    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * 2*d.inputSize * 3 * numBytesPerChannel);//\r\n    d.imgData.order(ByteOrder.nativeOrder());\r\n    d.intValues = new int[d.inputSize * 2*d.inputSize];\r\n\r\n//    d.tflite.setNumThreads(NUM_THREADS);\r\n\r\n    d.gridNum=d.inputSize/32;\r\n    d.floatValues=new float[1][d.inputSize][2 * d.inputSize ][3];\r\n    d.output_1 = new float[1][][][];\r\n    d.output_2 = new float[1][][][];\r\n    d.output_3 = new float[1][][][];\r\n\r\n\r\n    return d;\r\n  }\r\n\r\n  private void recreateInterpreter() {\r\n    if (tflite != null) {\r\n      tflite.close();\r\n      tflite = new Interpreter(tfliteModel, tfliteOptions);\r\n    }\r\n  }\r\n\r\n  @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n    imgData.rewind();\r\n\r\n\r\n    for (int i = 0; i < inputSize; ++i)\r\n    {\r\n      for (int j = 0; j < 2*inputSize; ++j)\r\n      {\r\n        int pixelValue = intValues[i * 2*inputSize + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else {\r\n          // Float model\r\n          floatValues[0][i][j][0]=((pixelValue >> 16) & 0xFF)/255.0f ;\r\n          floatValues[0][i][j][1]=((pixelValue >> 8) & 0xFF)/255.0f ;\r\n          floatValues[0][i][j][2]=(pixelValue& 0xFF) /255.0f;\r\n        }\r\n      }\r\n    }\r\n\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n\r\n    int channelNum=BoxNum_each_gird*(NUM_of_classes+5);\r\n\r\n    output_1 = new float[1][gridNum][2*gridNum][channelNum];\r\n    output_2 = new float[1][2*gridNum][4*gridNum][channelNum];\r\n    output_3 = new float[1][4*gridNum][8*gridNum][channelNum];\r\n\r\n    Object[] inputArray = {floatValues};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, output_1);\r\n    outputMap.put(1, output_2);\r\n    outputMap.put(2, output_3);\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    long startTime = SystemClock.uptimeMillis();\r\n//    tflite.modifyGraphWithDelegate(gpuDelegate);\r\n    tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    long lastingTime=SystemClock.uptimeMillis()-startTime;\r\n\r\n    LOGGER.i(\"runForMultipleInputsOutputs time of each image: \" + lastingTime + \"ms\");\r\n    Trace.endSection();\r\n\r\n    // Show the best detections.\r\n    // after scaling them back to the input size.\r\n    final ArrayList<Recognition> recognitions = new ArrayList<>();\r\n    //processing of output_1,output_2,output_3\r\n    int step=NUM_of_classes+5;\r\n\r\n    //13x26 output results\r\n    for (int i=0;i<gridNum;++i)\r\n    {\r\n      for(int j=0;j<2*gridNum;++j)\r\n      {\r\n\r\n        for(int b=0;b<3;++b){\r\n\r\n          final int offset =\r\n                  (2*gridNum * (3 * (NUM_of_classes + 5))) * i\r\n                          + (3 * (NUM_of_classes + 5)) * j\r\n                          + (NUM_of_classes + 5) * b;\r\n\r\n          float xPos=(j + sigmoid(output_1[0][i][j][b*step+0]))*blockSize;\r\n          float yPos=(i + sigmoid(output_1[0][i][j][b*step+1]))*blockSize;\r\n\r\n          float w= (float) (Math.exp(output_1[0][i][j][b*step+2])*(ANCHORS[2 * (b + 6) + 0] / 32.0) * blockSize);\r\n          float h= (float) (Math.exp(output_1[0][i][j][b*step+3])*(ANCHORS[2 * (b + 6) + 1] / 32.0) * blockSize);\r\n\r\n          final RectF rect =\r\n                  new RectF(\r\n                          Math.max(0, xPos - w / 2),\r\n                          Math.max(0, yPos - h / 2),\r\n                          Math.min(bitmap.getWidth() - 1, xPos + w / 2),\r\n                          Math.min(bitmap.getHeight() - 1, yPos + h / 2));\r\n\r\n          final float confidence = sigmoid(output_1[0][i][j][b*step + 4]);\r\n\r\n          int detectedClass = -1;\r\n\r\n          final float[] classes = new float[NUM_of_classes];\r\n          float[] scores = new float[NUM_of_classes];\r\n\r\n          for (int c = 0; c < NUM_of_classes; ++c)\r\n          {\r\n            classes[c] = sigmoid(output_1[0][i][j][b*step + 5 + c]);\r\n            scores[c] = confidence * classes[c];\r\n\r\n            if (scores[c] > scoreThreshold)\r\n            {\r\n              detectedClass = c;\r\n              LOGGER.i(\r\n                      \"%s (%d) %f %s\", LABELS[detectedClass], detectedClass, scores[c], rect);\r\n              recognitions.add(new Recognition(\"\" + offset, LABELS[detectedClass], scores[c], rect,detectedClass));\r\n            }\r\n\r\n          }\r\n\r\n        }\r\n      }\r\n    }\r\n\r\n    //26x52 output results\r\n    for (int i=0;i<2*gridNum;++i)\r\n    {\r\n      for(int j=0;j<4*gridNum;++j)\r\n      {\r\n\r\n        for(int b=0;b<3;++b)\r\n        {\r\n\r\n          final int offset =\r\n                  (4*gridNum * (3 * (NUM_of_classes + 5))) * i\r\n                          + (3 * (NUM_of_classes + 5)) * j\r\n                          + (NUM_of_classes + 5) * b;\r\n\r\n          float xPos=(j + sigmoid(output_2[0][i][j][b*step+0]))*(blockSize/2);\r\n          float yPos=(i + sigmoid(output_2[0][i][j][b*step+1]))*(blockSize/2);\r\n\r\n          float w=(float)Math.exp(output_2[0][i][j][b*step+2])*(ANCHORS[2 * (b + 3) + 0] / 16) *(blockSize/2) ;\r\n          float h=(float)Math.exp(output_2[0][i][j][b*step+3])*(ANCHORS[2 * (b + 3) + 1] / 16) * (blockSize/2);\r\n\r\n          final RectF rect =\r\n                  new RectF(\r\n                          Math.max(0, xPos - w / 2),\r\n                          Math.max(0, yPos - h / 2),\r\n                          Math.min(bitmap.getWidth() - 1, xPos + w / 2),\r\n                          Math.min(bitmap.getHeight() - 1, yPos + h / 2));\r\n\r\n          final float confidence = sigmoid(output_2[0][i][j][b*step + 4]);\r\n\r\n          int detectedClass = -1;\r\n\r\n          final float[] classes = new float[NUM_of_classes];\r\n          float[] scores = new float[NUM_of_classes];\r\n          for (int c = 0; c < NUM_of_classes; ++c)\r\n          {\r\n            classes[c] = sigmoid(output_2[0][i][j][b*step + 5 + c]);\r\n            scores[c] = confidence * classes[c];\r\n            if (scores[c] > scoreThreshold)\r\n            {\r\n              detectedClass = c;\r\n              LOGGER.i(\r\n                      \"%s (%d) %f %s\", LABELS[detectedClass], detectedClass, scores[c], rect);\r\n              recognitions.add(new Recognition(\"\" + offset, LABELS[detectedClass], scores[c], rect,detectedClass));\r\n            }\r\n          }\r\n\r\n        }\r\n      }\r\n    }\r\n\r\n    //52*104 output results\r\n    for (int i=0;i<4*gridNum;++i)\r\n    {\r\n      for(int j=0;j<8*gridNum;++j)\r\n      {\r\n\r\n        for(int b=0;b<3;++b)\r\n        {\r\n\r\n          final int offset =\r\n                  (8*gridNum * (3 * (NUM_of_classes + 5))) * i\r\n                          + (3 * (NUM_of_classes + 5)) * j\r\n                          + (NUM_of_classes + 5) * b;\r\n\r\n          float xPos=(j + sigmoid(output_3[0][i][j][b*step+0]))*(blockSize/4);\r\n          float yPos=(i + sigmoid(output_3[0][i][j][b*step+1]))*(blockSize/4);\r\n\r\n          float w=(float)Math.exp(output_3[0][i][j][b*step+2])*(ANCHORS[2 * (b + 0) + 0] / 8) *(blockSize/4) ;\r\n          float h=(float)Math.exp(output_3[0][i][j][b*step+3])*(ANCHORS[2 * (b + 0) + 1] / 8) * (blockSize/4);\r\n\r\n          final RectF rect =\r\n                  new RectF(\r\n                          Math.max(0, xPos - w / 2),\r\n                          Math.max(0, yPos - h / 2),\r\n                          Math.min(bitmap.getWidth() - 1, xPos + w / 2),\r\n                          Math.min(bitmap.getHeight() - 1, yPos + h / 2));\r\n\r\n          final float confidence = sigmoid(output_3[0][i][j][b*step + 4]);\r\n\r\n          int detectedClass = -1;\r\n\r\n          final float[] classes = new float[NUM_of_classes];\r\n          float[] scores = new float[NUM_of_classes];\r\n\r\n          for (int c = 0; c < NUM_of_classes; ++c)\r\n          {\r\n            classes[c] = sigmoid(output_3[0][i][j][b*step + 5 + c]);\r\n\r\n            scores[c] = confidence * classes[c];\r\n            if (scores[c] > scoreThreshold)\r\n            {\r\n              detectedClass = c;\r\n              LOGGER.i(\r\n                      \"%s (%d) %f %s\", LABELS[detectedClass], detectedClass, scores[c], rect);\r\n              recognitions.add(new Recognition(\"\" + offset, LABELS[detectedClass], scores[c], rect,detectedClass));\r\n            }\r\n          }\r\n\r\n        }\r\n      }\r\n    }\r\n\r\n    ArrayList<Recognition> recognitions_nms = new ArrayList<Recognition>();\r\n    recognitions_nms=nms(recognitions);\r\n    Trace.endSection(); // \"recognizeImage\"\r\n    return recognitions_nms;\r\n  }\r\n\r\n  @Override\r\n  public void enableStatLogging(final boolean logStats) {}\r\n\r\n  @Override\r\n  public String getStatString() {\r\n    return \"\";\r\n  }\r\n\r\n  @Override\r\n  public void close() {}\r\n\r\n  public void setNumThreads(int num_threads) {\r\n    if (tflite != null) tflite.setNumThreads(num_threads);\r\n  }\r\n\r\n  @Override\r\n  public void setUseNNAPI(boolean isChecked) {\r\n    if (tflite != null) tflite.setUseNNAPI(isChecked);\r\n  }\r\n\r\n  private float sigmoid(final float x) {\r\n    return (float) (1. / (1. + Math.exp(-x)));\r\n  }\r\n}`", "@impjdi  Any update on RESIZE_NEAREST_NEIGHBOR? I just benchmarked the newly released MNasFPN https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#mobile-models and am getting the same issue:\r\n`ERROR: Next operations are not supported by GPU delegate:\r\nRESIZE_NEAREST_NEIGHBOR: Operation is not supported.`", "in `//tensorflow/lite/delegates/gpu/common/model_builder.cc`, we do have `Resize2DOperationParser` with `SamplingType::NEAREST`.  And I do think we have working style transfer demos which employ `RESIZE_NEAREST_NEIGHBOR`.  Are you synced to head?", "@hvalev Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30758\">No</a>\n"]}, {"number": 30757, "title": "Multiply complex number with a scalar", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen I multiply a `tf.complex` with any other like `tf.float` it would be helpful to take the float as a complex number with imag part =0 (treat it as a scalar).\r\n\r\n**Will this change the current api? How?**\r\n When using `tf.matmult()` between a complex and a real number it will automatically cast the real one to complex and perform the operation instead of trhowing:\r\n`TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type complex64 of argument 'a'.`\r\n \r\n**Who will benefit with this feature?**\r\n\r\nI am currently trying to do a Complex-Valued Neural Network and I am having some headache with that. So I think it will helpful when doing a CVNN\r\n\r\n**Any Other info.**\r\n", "comments": ["@NEGU93 The error is correct as you are trying to multiply a complex number (of dtype tf.complex64) with another dtype. Hence, there is a mismatch in dtype. But, if you define `b` as of dtype=tf.complex64, then it works without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/dad77e1df747c75d394b284597c18259/tf_30757.ipynb). \r\n\r\nIf you are looking for different feature and you have a use-case for that then you could raise a PR. Please let me know what you think? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30756, "title": "Fix types for tflite micro builds", "body": "When using `-Wall` then some warnings regarding types make the build fail. An example of these warnings is the next one:\r\n\r\n`tensorflow/lite/experimental/micro/micro_interpreter.cc: In member function 'TfLiteStatus tflite::MicroInterpreter::Invoke()':\r\ntensorflow/lite/experimental/micro/micro_interpreter.cc:104:21: warning: comparison of integer expressions of different signedness: 'int' and 'flatbuffers::uoffset_t' {aka 'long unsigned int'} [-Wsign-compare]\r\n   for (int i = 0; i < operators_->size(); ++i) {`\r\n\r\nWith this PR I was able to build without errors using CMAKE on the stm32f746 and the listed flags:\r\n\r\n`set (CMAKE_CXX_FLAGS \"-g -mthumb -fno-builtin -mcpu=cortex-m7 -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -Wall -std=c++11 -ffunction-sections -fdata-sections -fomit-frame-pointer -mabi=aapcs -fno-unroll-loops -ffast-math -ftree-vectorize\" CACHE INTERNAL \"cxx compiler flags\")\r\nset (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -lm -lc -lnosys --specs=nosys.specs\")`", "comments": ["Hi people. I don't know why this PR is blocked. I followed all the suggested guidelines, but I'm getting the above bot errors.\r\n\r\nThanks!", "@dimtass Could you please resolve the conflicts? Thanks!", "Hi @gbaned, I'm on holidays so I don't have access to my stuff, but I've used the online tool to resolve this. That means that I wasn't able to build my self. I'll be able to do this in a few weeks, though.", "Can one of the admins verify this patch?", "@dimtass wondering if you still need this PR, if yes can you please resolve conflicts. Thanks!", "@gbaned, yes I do. If you don't mind, I'll update the PR in a couple of\nweeks when I'm back, because currently I don't have access to any computer\nto build and test.\n\ngbaned <notifications@github.com> schrieb am Do., 26. Sep. 2019, 14:36:\n\n> @dimtass <https://github.com/dimtass> wondering if you still need this\n> PR, if yes can you please resolve conflicts. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30756?email_source=notifications&email_token=AD4VQVBYWVSYBEVGMT5VRVDQLSNFHA5CNFSM4IEAYIM2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7VH6UI#issuecomment-535461713>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AD4VQVC44C26SN4BF5BTI3TQLSNFHANCNFSM4IEAYIMQ>\n> .\n>\n", "gentle ping @petewarden for review ? Thanks!", "@dimtass Could you please resolve the conflicts? Thanks!", "Hi @gbaned since I'm not using TF-Lite anymore, I don't have what is needed installed on my workstation to build and check if everything is ok.\r\nTherefore, I'll close this ticket and in the future maybe someone else that has the same problem can re-open this.\r\nRegards."]}, {"number": 30755, "title": "Android: Failed to resolve: tensorflow-lite", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.2 HighSierra\r\n\r\n- TensorFlow installed from (source or binary): gradle dependency 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n- TensorFlow version: 0.0.0-nightly\r\n- Android Studio : 3.2.1\r\n- Android compile / targetSdkVersion  = 28\r\n- Gradle Version: 4.6\r\n\r\n\r\n**Describe the problem**\r\n\r\nTensorflow Lite dependency is not resolved \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nRun assembleDebug command -> \r\n_Could not find tensorflow-lite.aar (org.tensorflow:tensorflow-lite:0.0.0-nightly).\r\n  Searched in the following locations:\r\n      https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/0.0.0-nightly/tensorflow-lite-0.0.0-nightly.aar_\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["I am receiving this error as well. Assuming this is a nightly build error, but will wait for feedback.", "You can try using the explicit version 1.14.0\r\n\r\nhttps://bintray.com/google/tensorflow/tensorflow-lite#", "@joelromanpr Thanks it worked!", "@joelromanpr I can easily replace the dependency, **but there are some classes missing** in this version\r\n\r\nerror: cannot find symbol public class TFLiteObjectDetectionAPIModel implements Classifier {\r\nerror: cannot find symbol private Interpreter tfLite;\r\nerror: cannot find symbo public List<Recognition> ", "@peterszaszlateral that looks like a different issue. TFLiteObjectDetectionAPIModel is a part of the sample, not of tensorflow-lite.aar.\r\n\r\nI'm not able to reproduce the issue with the nightly build, though. Are you able to successfully the build the classification example? https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android", "@jdduke It all started working again last week at some point + the sample app you specified is also building successfully. Thanks for the help!", "Glad it's working!"]}, {"number": 30754, "title": "What's the state of RNNs and tf.lite for Tensorflow 2.0?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0-beta1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_rnn_tflite.py\", line 12, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 392, in convert                                                                                                  \r\n    **converter_kwargs)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 404, in toco_convert_impl                                                                                     \r\n    input_data.SerializeToString())\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos                                                                                   \r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-07-16 14:25:07.645148: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20                                                                                                       \r\n2019-07-16 14:25:07.645184: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20                                                                                                       \r\n2019-07-16 14:25:07.645191: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20                                                                                                       \r\n2019-07-16 14:25:07.645280: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: StatefulPartitionedCall                                                                                        \r\n2019-07-16 14:25:07.653916: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)                                                                                     \r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00000001104045c0 (most recent call first):\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute                                                                                   \r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run                                                                                                     \r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main                                                                                      \r\n  File \"/Users/ben/miniconda3/envs/wakeword/bin/toco_from_protos\", line 10 in <module>\r\n```\r\n\r\nCode to reproduce:\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential(\r\n    [tf.keras.layers.LSTM(10, input_shape=(16000, 40))]\r\n)\r\nmodel.build()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = set(\r\n    [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n)\r\n\r\ntflite_model = converter.convert()\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nThere are several issues about support of RNNs in tf.lite, most of them are rather old and about TF 1.x. What's the current state of supporting RNNs in tf.lite for TF 2.x?", "comments": ["There is currently limited support for LSTMs in TFLite. The documented path is available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/g3doc/README.md). We are working on improving our support of control flow based operations and models. We will update documentation and the GitHub issues as we make progress on this issue.", "Hi,\r\n\r\nYou can try to convert the keras RNN model using the new TF Lite converter now:\r\n\r\nTry set converter.experimental_new_converter = True and then convert.\r\n\r\nA known issue is that if the time step is very large, like 16000 in your example, the converted lstm could be slow. But it should generally be fine for small time steps. We are currently working on improving its performance.\r\n\r\nThanks.", "@BeWe11 Is this still an issue? As mentioned by @haozha111, I tried `converter.experimental_new_converter = True`  and your code runs without any error. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/fd8f60999881add469b1713d2b715ac9/untitled638.ipynb).\r\n\r\nPlease close this issue if it was resolved for you. Thanks!", "@haozha111 \r\nAre complex dtypes planned to be supported in the experimental converter? using nightly 1108 build and it is throwing an error. This is super useful for MFCC calculations.\r\n\r\n```python\r\nsample_rate = 16000.0\r\n# A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\r\npcm = tf.compat.v1.placeholder(tf.float32, [None, None])\r\n\r\n# A 1024-point STFT with frames of 64 ms and 75% overlap.\r\nstfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,\r\n                       fft_length=1024)\r\nspectrograms = tf.abs(stfts)\r\n\r\n# Warp the linear scale spectrograms into the mel-scale.\r\nnum_spectrogram_bins = stfts.shape[-1].value\r\nlower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\nlinear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n  num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\r\n  upper_edge_hertz)\r\nmel_spectrograms = tf.tensordot(\r\n  spectrograms, linear_to_mel_weight_matrix, 1)\r\nmel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\r\n  linear_to_mel_weight_matrix.shape[-1:]))\r\n\r\n# Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\nlog_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n# Compute MFCCs from log_mel_spectrograms and take the first 13.\r\nmfccs = tf.signal.mfccs_from_log_mel_spectrograms(\r\n  log_mel_spectrograms)[..., :13]\r\n```", "@mattc-eostar Can you please create a new issue and provide a simple standalone code to reproduce the issue? Thanks!", "#34324 Created here. Thanks!", "I am closing this issue as it was resolved with the new converter `converter.experimental_new_converter = True`. Please feel free to reopen the issue if it persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30754\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30754\">No</a>\n"]}, {"number": 30753, "title": "TF >=1.13 doesn't support CUDA 9.0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): build from source\r\n- TensorFlow version: `1.13.1, 1.14.0, 2.0.0a, 2.0.0b1`\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: ...\r\n- Bazel version (if compiling from source): `0.25.2`\r\n- GCC/Compiler version (if compiling from source): `5.4.0`\r\n- CUDA/cuDNN version: `CUDA 9.0, cudnn 7.6.1`\r\n- GPU model and memory: Tesla V100\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to compile newer versions of TF from source to support CUDA 9.0 . I tried building all versions after 1.12.0 and always get the same error. Bazel build sucessfully finishes but after I install the resulting pip file and try to import TF it doesn't use GPU's\r\n`Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda-9.0/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64`\r\n```\r\n2019-07-16 14:37:23.856339: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.9.0\r\n2019-07-16 14:37:23.857504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.9.0\r\n2019-07-16 14:37:23.858807: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.9.0\r\n2019-07-16 14:37:23.859066: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.9.0\r\n2019-07-16 14:37:23.859437: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda-9.0/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64\r\n2019-07-16 14:37:23.860627: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.9.0\r\n2019-07-16 14:37:23.864452: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-16 14:37:23.864478: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI use `configure` script with all parameters at default except specifying CUDA version. Then I build with:\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nThis problem was already reported before: #30389  \r\nI also found discussion, where people had the same issue with cusolver library:\r\nhttps://groups.google.com/forum/#!topic/kaldi-help/8OThZ1lwE9I\r\n", "comments": ["Try updating your Cuda version to 10.0 exact (also not 10.1). It should support training on GPUs.", "@rishabhsahrawat updating CUDA on the server is not an option for multiple reasons. I want to compile TF specifically for CUDA 9.0", "@bonlime ,\r\nPlease take a look at [software requirements](https://www.tensorflow.org/install/gpu#software_requirements) for TF installation.Thanks!", "If TensorFlow only support CUDA 10.0, I think `configure` script should prevent from using other CUDA versions.", "@anush-o  it says \"TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0)\". I supposed it meant TF should support old versions of CUDA 9.0 as well as new 10.0", "@bonlime ,\r\nThis is Tested and existing compatibility of CUDA and TF versions. In order to use CUDA 9.0 you can downgrade the TF version to 1.12.Thanks!", "The reason why opened this issue is because I expected TF 1.13 to support lower versions of CUDA as well. Looks like it doesn't. \r\nI'll close the issue, but if anyone finds a way to compile from sources for CUDA 9.0 feel free to comment here", "@oanush how do you downgrade to 1.12? I got the error message\r\n\r\n ERROR: Could not find a version that satisfies the requirement tensorflow==1.12 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)", "You need to downgrade Python to 3.6. TF 1.12 does not support Python 3.7.\r\n\r\n> On Aug 17, 2019, at 02:41, kk325ic <notifications@github.com> wrote:\r\n> \r\n> \ufeff\r\n> @oanush how do you downgrade to 1.12? I got the error message\r\n> \r\n> ERROR: Could not find a version that satisfies the requirement tensorflow==1.12 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\n> \r\n> \u2014\r\n> You are receiving this because you are subscribed to this thread.\r\n> Reply to this email directly, view it on GitHub, or mute the thread.\r\n", "@bonlime \r\n```\r\nimport ctypes\r\nctypes.CDLL(\"libgomp.so.1\", mode=ctypes.RTLD_GLOBAL)\r\n```\r\nThis two lines might allow you to run CNN networks with TF 1.14 + CUDA 9.\r\nBut it might cause strange behavior with LSTM models, where TF tries to allocate extensive VRAM and fails).\r\n", "@004307ec Can you please describe where to add this? Also, do I have to build this from source in order to run TF 1.14 with CUDA 9.0?", "@smr97 Yes you have to build from source in order to use TF 1.14 with CUDA 9.0. You need to add the lines above to the beginning of your scripts which use TF", "@bonlime , can you confirm that those two lines really affect LSTM behaviour?", "@sudonto No. I was only using TF for computer vision tasks, didn't check the LSTM behavior "]}, {"number": 30752, "title": "Display exported concrete and polymorphic functions in SavedModel CLI", "body": "Adding features to `saved_model_cli` to show SavedModel2.0 Functions.\r\nSavedModelCLI doesn't show details of Polymorphic Functions or multiple Concrete Functions. This Pull request addresses this problem,\r\nThe Sample output is provided here: https://github.com/tensorflow/tensorflow/pull/30752#issuecomment-514622204\r\nThis PR is made for Google Summer of Code 2019\r\n\r\nCC: @vbardiovskyg @srjoglekar246 ", "comments": ["As Requested, Output of SavedModel2.0\r\n\r\nModel\r\n----------------\r\n\r\n```python3\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behaviour()\r\nclass A(tf.train.Checkpoint):\r\n  @tf.function\r\n  def func1(self, a, b, c):\r\n    if c:\r\n      return a + b \r\n    else:\r\n      return a * b\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=(2, 2), dtype=tf.float32)])\r\n  def func2(self, x):\r\n    return x + 2\r\n  @tf.function\r\n  def __call__(self, y, c=7):\r\n    return y + 2 * c\r\n\r\na = A()\r\na.func1(tf.constant(5), tf.constant(9), {\"key1\": [{\"k1\":tf.constant(5), \"k2\":1}, 2], \"key2\": 2})\r\na.func1(tf.constant(5), tf.constant(9), False)\r\na(tf.constant(5))\r\ntf.saved_model.save(a, \"/tmp/model\")\r\n```\r\nRun\r\n---------------\r\n```bash\r\n$ saved_model_cli show --dir /tmp/model --all\r\n```\r\n\r\nOutput\r\n------------------\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['x'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (2, 2)\r\n        name: serving_default_x:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output_0'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (2, 2)\r\n        name: PartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\n\r\nDefined Functions:\r\n  Function Name: 'func2'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          x: TensorSpec(shape=(2, 2), dtype=tf.float32, name='x')\r\n\r\n  Function Name: 'func1'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          a: TensorSpec(shape=(), dtype=tf.int32, name='a')\r\n        Argument #2\r\n          b: TensorSpec(shape=(), dtype=tf.int32, name='b')\r\n        Argument #3\r\n          DType: bool\r\n          Value: True\r\n    Option #2\r\n      Callable with:\r\n        Argument #1\r\n          a: TensorSpec(shape=(), dtype=tf.int32, name='a')\r\n        Argument #2\r\n          b: TensorSpec(shape=(), dtype=tf.int32, name='b')\r\n        Argument #3\r\n          DType: dict\r\n          Value: {'key1': [{'k1': TensorSpec(shape=(), dtype=tf.int32, name='c/key1/0/k1'), 'k2': 1}, 2], 'key2': 2} \r\n\r\n  Function Name: '__call__'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          y: TensorSpec(shape=(), dtype=tf.int32, name='y')\r\n        Argument #2\r\n          DType: int\r\n          Value: 7\r\n```", "Added the test, @vbardiovskyg please take a look :)", "@vbardiovskyg ", "@vbardiovskyg ", "@vbardiovskyg , This should remove the linter issues."]}, {"number": 30751, "title": "Fix types for tflite micro", "body": "When build the tflite-micro for the stm32f746 with `-Wall` the build fails in several points because there are warnings about types and different signedness.\r\n\r\nFor example:\r\n`code-stm32f476-tflite/source/libs/tensorflow/lite/experimental/micro/micro_interpreter.cc: In member function 'TfLiteStatus tflite::MicroInterpreter::Invoke()':\r\ncode-stm32f476-tflite/source/libs/tensorflow/lite/experimental/micro/micro_interpreter.cc:104:21: warning: comparison of integer expressions of different signedness: 'int' and 'flatbuffers::uoffset_t' {aka 'long unsigned int'} [-Wsign-compare]\r\n   for (int i = 0; i < operators_->size(); ++i) {`\r\n\r\nWith this commit my cmake project that uses `-Wall` compiles without errors.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30751) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30751) for more info**.\n\n<!-- need_author_cla -->", "recheck?"]}, {"number": 30750, "title": "tf.sparse.to_dense does not work on sparse tensor with string values. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS (Bionic Beaver)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nTrying to convert a SparseTensor of type string into the corresponding dense tensor using tf.sparse.to_dense throughs an exception: TypeError: Expected string passed to parameter 'default_value' of op 'SparseToDense', got 0 of type 'int' instead. Error: Expected string, got 0 of type 'int' instead.\r\n\r\n**Describe the expected behavior**\r\nLike with using an integer valued SparseTensor I expect tf.sparse.to_dense to return a dense tensor with string values. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nsample_int    = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\r\nsample_string = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=['a', 'b'], dense_shape=[3, 4])\r\ntf.sparse.to_dense( sample_int )\r\ntf.sparse.to_dense( sample_string )\r\n```\r\nThere is also a colab notebook where you can execute the code directly: [https://colab.research.google.com/drive/1OxZCVWdnEYAkmGDZ7hAvP2mkFbxYX4Zh](https://colab.research.google.com/drive/1OxZCVWdnEYAkmGDZ7hAvP2mkFbxYX4Zh)\r\n\r\n**Other info / logs**\r\nI came across this issue while reading in some TFRecords that included VarLenFeatures with string-typed lists. Everything worked fine with the VarLenFeatures with integers. Right now I have not even found a way to convert the SparseTensor's string values into a new Tensor as a workaround. \r\n", "comments": ["Add a PR #30781 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30750\">No</a>\n"]}, {"number": 30749, "title": "Pass Input to tensorflow lite model in Android", "body": "I have created a neural network that take numerical data as input and saved it as tensorflow lite model using python. I am trying to pass input to the model in Android. Shape of ndarray is 1*3\r\n\r\nSample of the input in python is as follows\r\n\r\n`np.array([[-0.276786765 ,8.41897583008  ,-0.0222015380859]])`\r\n\r\nBut i do not know to create the same input in java to pass it to model.\r\n\r\nI tried using nd4j library. But still not able to write the proper code to create the input which is required by the model.", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Automatically closing due to lack of recent activity. Thanks!"]}, {"number": 30748, "title": "Requested GPU:0, but only XLA_GPU:0 exsits, tf-gpu1.14.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.8 / 3.7.3\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n- CUDA/cuDNN version: 10/7.4\r\n- GPU model and memory:\r\n![image](https://user-images.githubusercontent.com/5757359/61288486-e3987880-a7cf-11e9-8f1d-efb1a7130879.png)\r\n\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n1. Installed `tensorflow-gpu` latest version by performing: `pip install --upgrade --force-reinstall tensorflow-gpu`\r\n2. Run `tf.device('/gpu:0')`\r\n\r\nThe error seems to say I have 4 gpus, but they don't match GPU, only XLA_GPU. I have no idea why, earlier versions of tensorflow do say I have GPU, but claim other bugs.\r\n\r\nError:\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation pooled_center_map/center_map/AvgPool: {{node pooled_center_map/center_map/AvgPool}}was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3 ]. Make sure the device specification refers to a valid device.\r\n\r\n\r\nEven if I try:\r\n`tf.device('/job:localhost/replica:0/task:0/device:XLA_GPU:0')`\r\n\r\nI get:\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation sub_stages/sub_conv1/weights/Initializer/random_uniform/RandomUniform: Could not satisfy explicit device specification '' because the node {{colocation_node sub_stages/sub_conv1/weights/Initializer/random_uniform/RandomUniform}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:XLA_GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3, /job:localhost/replica:0/task:0/device:XLA_CPU:0]. \r\n", "comments": ["@AmitMY You could use `allow_soft_placement=True` as an argument to ConfigProto so that it uses any other existing and supported devices for that operation. Check the following [example](https://www.tensorflow.org/guide/using_gpu) from TF website.\r\n\r\n```\r\n# Creates a graph.\r\nwith tf.device('/device:GPU:2'):\r\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n  c = tf.matmul(a, b)\r\n# Creates a session with allow_soft_placement and log_device_placement set\r\n# to True.\r\nsess = tf.Session(config=tf.ConfigProto(\r\n      allow_soft_placement=True, log_device_placement=True))\r\n# Runs the op.\r\nprint(sess.run(c))\r\n```\r\nPlease let me know how it progresses. Thanks!", "Thanks @jvishnuvardhan \r\nUsing this config, it now does not error.\r\nHowever, with the `log_device_placement` I see:\r\n> Ignoring device specification /device:GPU:0 for node 'Assign_4' because the input edge from 'sub_stages/sub_conv3/weights' is a reference connection and already has a device field set to /job:localhost/replica:0/task:0/device:CPU:0\r\n> Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [/job:localhost/replica:0/task:0/device:CPU:0].\r\n\r\nAlthough I only create the computation graph under `tf.device('gpu:0')`\r\n\r\nI also see:\r\n\r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n\r\n(To make sure I didn't change anything except for this config, when I remove the config I am getting that error again, still seeing my GPUs as XLA_GPU)", "@sanjoy would you please help to take a look?", "Can you please attach the full log?\r\n\r\nXLA creates an XLA_GPU device for every present on the system whereas TF creates a GPU device only for GPUs suitable for compute (i.e. ignores slower GPUs) so that could explain what you're seeing.  TF logs out \"Ignoring visible gpu device\" when it does this enhanced filtering so you should see it in your logs.", "I wouldn't say 1080Ti is slow...\r\nI also tried on a server with 4 Titan X Pascals.\r\n\r\nI can't attach the full log including `log_device_placement`, as it is at least 100 times the size of my console buffer.\r\nWithout that flag:\r\n\r\nTitan X log output:\r\n\r\n> WARNING: Logging before flag parsing goes to stderr.\r\n> W0719 10:34:50.315408 139938591782720 deprecation_wrapper.py:119] From /home/nlp/amit/project-fingering-extraction/components/hand_pose/detect_hand_pose.py:45: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n> \r\n> W0719 10:34:50.318552 139938591782720 deprecation_wrapper.py:119] From /home/nlp/amit/project-fingering-extraction/components/hand_pose/cpm_hand_slim.py:24: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n> \r\n> W0719 10:34:51.952562 139938591782720 deprecation_wrapper.py:119] From /home/nlp/amit/project-fingering-extraction/components/hand_pose/detect_hand_pose.py:57: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n> \r\n> W0719 10:34:52.075365 139938591782720 deprecation_wrapper.py:119] From /home/nlp/amit/project-fingering-extraction/components/hand_pose/detect_hand_pose.py:61: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n> \r\n> W0719 10:34:52.075703 139938591782720 deprecation_wrapper.py:119] From /home/nlp/amit/project-fingering-extraction/components/hand_pose/detect_hand_pose.py:61: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n> \r\n> 2019-07-19 10:34:52.076576: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-07-19 10:34:52.098554: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n> 2019-07-19 10:34:52.788527: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5636a27cda20 executing computations on platform CUDA. Devices:\r\n> 2019-07-19 10:34:52.788595: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2\r\n> 2019-07-19 10:34:52.788614: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2\r\n> 2019-07-19 10:34:52.788629: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): GeForce GTX TITAN X, Compute Capability 5.2\r\n> 2019-07-19 10:34:52.788644: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): GeForce GTX TITAN X, Compute Capability 5.2\r\n> 2019-07-19 10:34:52.795911: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199900000 Hz\r\n> 2019-07-19 10:34:52.801752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5636a29364d0 executing computations on platform Host. Devices:\r\n> 2019-07-19 10:34:52.801799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n> 2019-07-19 10:34:52.803912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\n> pciBusID: 0000:02:00.0\r\n> 2019-07-19 10:34:52.805028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \r\n> name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\n> pciBusID: 0000:03:00.0\r\n> 2019-07-19 10:34:52.806183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \r\n> name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\n> pciBusID: 0000:81:00.0\r\n> 2019-07-19 10:34:52.807128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \r\n> name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\n> pciBusID: 0000:82:00.0\r\n> 2019-07-19 10:34:52.807368: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n> 2019-07-19 10:34:52.807494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n> 2019-07-19 10:34:52.807617: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n> 2019-07-19 10:34:52.807745: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n> 2019-07-19 10:34:52.807868: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n> 2019-07-19 10:34:52.807989: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n> 2019-07-19 10:34:52.815678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-07-19 10:34:52.815715: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n> 2019-07-19 10:34:52.815868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-07-19 10:34:52.815894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 \r\n> 2019-07-19 10:34:52.815916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y N N \r\n> 2019-07-19 10:34:52.815933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N N N \r\n> 2019-07-19 10:34:52.815948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   N N N Y \r\n> 2019-07-19 10:34:52.815963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   N N Y N \r\n> W0719 10:34:52.821785 139938591782720 deprecation_wrapper.py:119] From /home/nlp/amit/project-fingering-extraction/components/hand_pose/detect_hand_pose.py:63: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n> \r\n> 2019-07-19 10:34:52.867261: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv1/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv1/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv1/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_75 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.867633: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv1/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv1/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv1/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_74 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.867914: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv2/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv2/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv2/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv2/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_87 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.868169: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv2/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv2/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv2/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv2/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_86 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.868447: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv3/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv3/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv3/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv3/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_89 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.868734: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv3/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv3/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv3/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv3/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_88 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.869076: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv4/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv4/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv4/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv4/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_91 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.869361: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv4/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv4/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv4/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv4/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_90 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.869657: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv5/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv5/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv5/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv5/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_93 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.869937: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv5/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv5/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv5/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv5/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_92 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.870231: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv6/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv6/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv6/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv6/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_95 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.870506: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv6/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv6/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv6/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv6/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_94 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.870804: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv7/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv7/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv7/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv7/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_97 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.871075: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv7/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv7/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv7/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv7/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_96 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.871368: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv8/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv8/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv8/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv8/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_99 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.871645: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv8/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv8/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv8/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv8/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_98 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.871980: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv9/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv9/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv9/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv9/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_101 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.872245: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv9/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv9/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv9/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv9/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_100 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.872517: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv10/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv10/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv10/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv10/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_77 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.872785: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv10/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv10/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv10/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv10/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_76 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.873052: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv11/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv11/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv11/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv11/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_79 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.873305: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv11/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv11/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv11/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv11/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_78 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.873576: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv12/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv12/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv12/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv12/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_81 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.873833: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv12/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv12/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv12/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv12/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_80 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.874103: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv13/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv13/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv13/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv13/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_83 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.874355: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv13/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv13/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv13/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv13/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_82 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.874618: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_conv14/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_conv14/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv14/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv14/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_85 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.874873: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_conv14/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_conv14/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_conv14/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_conv14/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_84 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.875141: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform/shape (Const) \r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform/min (Const) \r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform/max (Const) \r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform/sub (Sub) \r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform/mul (Mul) \r\n>   sub_stages/sub_stage_img_feature/weights/Initializer/random_uniform (Add) \r\n>   sub_stages/sub_stage_img_feature/weights (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_stage_img_feature/weights/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_stage_img_feature/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_103 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.875392: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   sub_stages/sub_stage_img_feature/biases/Initializer/zeros (Const) \r\n>   sub_stages/sub_stage_img_feature/biases (VariableV2) /device:GPU:0\r\n>   sub_stages/sub_stage_img_feature/biases/Assign (Assign) /device:GPU:0\r\n>   sub_stages/sub_stage_img_feature/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_102 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.875655: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_1/conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_1/conv1/weights/Initializer/random_uniform/min (Const) \r\n>   stage_1/conv1/weights/Initializer/random_uniform/max (Const) \r\n>   stage_1/conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_1/conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_1/conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_1/conv1/weights/Initializer/random_uniform (Add) \r\n>   stage_1/conv1/weights (VariableV2) /device:GPU:0\r\n>   stage_1/conv1/weights/Assign (Assign) /device:GPU:0\r\n>   stage_1/conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_1 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.875909: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_1/conv1/biases/Initializer/zeros (Const) \r\n>   stage_1/conv1/biases (VariableV2) /device:GPU:0\r\n>   stage_1/conv1/biases/Assign (Assign) /device:GPU:0\r\n>   stage_1/conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.876172: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform/min (Const) \r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform/max (Const) \r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_1/stage_heatmap/weights/Initializer/random_uniform (Add) \r\n>   stage_1/stage_heatmap/weights (VariableV2) /device:GPU:0\r\n>   stage_1/stage_heatmap/weights/Assign (Assign) /device:GPU:0\r\n>   stage_1/stage_heatmap/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_3 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.876422: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_1/stage_heatmap/biases/Initializer/zeros (Const) \r\n>   stage_1/stage_heatmap/biases (VariableV2) /device:GPU:0\r\n>   stage_1/stage_heatmap/biases/Assign (Assign) /device:GPU:0\r\n>   stage_1/stage_heatmap/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_2 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.876703: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv1/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv1/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv1/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_5 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.876955: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv1/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv1/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv1/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_4 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.877215: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv2/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv2/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv2/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv2/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_7 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.877468: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv2/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv2/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv2/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv2/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_6 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.877737: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv3/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv3/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv3/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv3/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_9 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.877992: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv3/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv3/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv3/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv3/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_8 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.878253: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv4/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv4/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv4/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv4/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_11 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.878501: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv4/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv4/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv4/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv4/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_10 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.878769: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv5/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv5/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv5/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv5/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_13 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.879019: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv5/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv5/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv5/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv5/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_12 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.879283: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv6/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv6/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv6/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv6/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_15 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.879535: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv6/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv6/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv6/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv6/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_14 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.879803: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform/min (Const) \r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform/max (Const) \r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_2/mid_conv7/weights/Initializer/random_uniform (Add) \r\n>   stage_2/mid_conv7/weights (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv7/weights/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv7/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_17 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.880057: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_2/mid_conv7/biases/Initializer/zeros (Const) \r\n>   stage_2/mid_conv7/biases (VariableV2) /device:GPU:0\r\n>   stage_2/mid_conv7/biases/Assign (Assign) /device:GPU:0\r\n>   stage_2/mid_conv7/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_16 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.880329: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv1/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv1/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv1/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_19 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.880584: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv1/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv1/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv1/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_18 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.880855: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv2/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv2/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv2/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv2/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_21 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.881106: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv2/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv2/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv2/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv2/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_20 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.881368: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv3/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv3/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv3/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv3/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_23 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.881616: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv3/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv3/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv3/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv3/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_22 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.881887: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv4/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv4/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv4/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv4/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_25 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.882138: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv4/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv4/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv4/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv4/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_24 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.882405: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv5/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv5/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv5/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv5/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_27 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.882654: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv5/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv5/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv5/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv5/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_26 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.882925: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv6/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv6/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv6/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv6/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_29 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.883174: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv6/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv6/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv6/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv6/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_28 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.883436: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform/min (Const) \r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform/max (Const) \r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_3/mid_conv7/weights/Initializer/random_uniform (Add) \r\n>   stage_3/mid_conv7/weights (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv7/weights/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv7/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_31 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.883699: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_3/mid_conv7/biases/Initializer/zeros (Const) \r\n>   stage_3/mid_conv7/biases (VariableV2) /device:GPU:0\r\n>   stage_3/mid_conv7/biases/Assign (Assign) /device:GPU:0\r\n>   stage_3/mid_conv7/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_30 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.883977: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv1/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv1/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv1/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_33 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.884228: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv1/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv1/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv1/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_32 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.884498: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv2/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv2/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv2/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv2/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_35 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.884804: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv2/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv2/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv2/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv2/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_34 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.885094: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv3/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv3/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv3/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv3/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_37 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.885347: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv3/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv3/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv3/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv3/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_36 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.885614: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv4/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv4/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv4/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv4/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_39 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.885877: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv4/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv4/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv4/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv4/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_38 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.886139: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv5/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv5/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv5/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv5/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_41 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.886390: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv5/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv5/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv5/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv5/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_40 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.886654: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv6/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv6/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv6/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv6/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_43 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.886911: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv6/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv6/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv6/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv6/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_42 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.887173: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform/min (Const) \r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform/max (Const) \r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_4/mid_conv7/weights/Initializer/random_uniform (Add) \r\n>   stage_4/mid_conv7/weights (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv7/weights/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv7/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_45 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.887426: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_4/mid_conv7/biases/Initializer/zeros (Const) \r\n>   stage_4/mid_conv7/biases (VariableV2) /device:GPU:0\r\n>   stage_4/mid_conv7/biases/Assign (Assign) /device:GPU:0\r\n>   stage_4/mid_conv7/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_44 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.887708: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv1/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv1/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv1/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_47 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.887958: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv1/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv1/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv1/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_46 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.888220: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv2/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv2/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv2/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv2/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_49 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.888468: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv2/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv2/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv2/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv2/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_48 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.888775: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv3/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv3/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv3/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv3/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_51 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.889057: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv3/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv3/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv3/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv3/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_50 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.889327: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv4/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv4/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv4/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv4/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_53 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.889576: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv4/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv4/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv4/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv4/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_52 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.889851: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv5/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv5/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv5/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv5/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_55 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.890099: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv5/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv5/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv5/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv5/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_54 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.890361: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv6/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv6/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv6/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv6/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_57 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.890609: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv6/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv6/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv6/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv6/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_56 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.890875: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform/min (Const) \r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform/max (Const) \r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_5/mid_conv7/weights/Initializer/random_uniform (Add) \r\n>   stage_5/mid_conv7/weights (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv7/weights/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv7/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_59 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.891125: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_5/mid_conv7/biases/Initializer/zeros (Const) \r\n>   stage_5/mid_conv7/biases (VariableV2) /device:GPU:0\r\n>   stage_5/mid_conv7/biases/Assign (Assign) /device:GPU:0\r\n>   stage_5/mid_conv7/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_58 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.891398: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv1/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv1/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv1/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv1/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_61 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.891648: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv1/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv1/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv1/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv1/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_60 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.891913: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv2/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv2/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv2/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv2/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_63 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.892160: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv2/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv2/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv2/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv2/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_62 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.892418: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv3/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv3/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv3/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv3/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_65 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.892673: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv3/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv3/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv3/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv3/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_64 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.892935: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv4/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv4/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv4/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv4/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_67 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.893182: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv4/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv4/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv4/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv4/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_66 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.893445: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv5/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv5/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv5/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv5/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_69 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.893702: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv5/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv5/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv5/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv5/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_68 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.893966: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv6/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv6/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv6/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv6/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_71 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.894212: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv6/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv6/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv6/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv6/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_70 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.894468: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Mul: CPU XLA_CPU XLA_GPU \r\n> Add: CPU XLA_CPU XLA_GPU \r\n> Sub: CPU XLA_CPU XLA_GPU \r\n> RandomUniform: CPU XLA_CPU XLA_GPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform/shape (Const) \r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform/min (Const) \r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform/max (Const) \r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform/RandomUniform (RandomUniform) \r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform/sub (Sub) \r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform/mul (Mul) \r\n>   stage_6/mid_conv7/weights/Initializer/random_uniform (Add) \r\n>   stage_6/mid_conv7/weights (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv7/weights/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv7/weights/read (Identity) /device:GPU:0\r\n>   save/Assign_73 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:52.894722: W tensorflow/core/common_runtime/colocation_graph.cc:1016] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n>   /job:localhost/replica:0/task:0/device:CPU:0].\r\n> See below for details of this colocation group:\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\n> Assign: CPU \r\n> Identity: CPU XLA_CPU XLA_GPU \r\n> VariableV2: CPU \r\n> Const: CPU XLA_CPU XLA_GPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   stage_6/mid_conv7/biases/Initializer/zeros (Const) \r\n>   stage_6/mid_conv7/biases (VariableV2) /device:GPU:0\r\n>   stage_6/mid_conv7/biases/Assign (Assign) /device:GPU:0\r\n>   stage_6/mid_conv7/biases/read (Identity) /device:GPU:0\r\n>   save/Assign_72 (Assign) /device:GPU:0\r\n> \r\n> 2019-07-19 10:34:53.181507: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n> Load model 0:00:10.898345\r\n> 1st\r\n> [(179.0, 299.0), (107.0, 228.0), (164.0, 140.0), (228.0, 131.0), (244.0, 155.0), (268.0, 188.0)]\r\n> \r\n> Process finished with exit code 0\r\n", "I got the same problem too.", "Actually, works fine on the same machine hardware with CUDA Version 10.0.130\r\nIt doesn't work on CUDA Version 8.0.61\r\n\r\nMy bad, I checked CUDA version on the wrong server when reporting the issue.\r\n\r\nIs this version of Tensorflow even supposed to work for v8? if not, I'll close this issue.\r\n\r\n\r\n", "The failure is almost certainly because of the cuda version, going by the log:\r\n\r\n```\r\n2019-07-19 10:34:52.807368: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 10:34:52.807494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 10:34:52.807617: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 10:34:52.807745: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 10:34:52.807868: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 10:34:52.807989: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2019-07-19 10:34:52.815678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-19 10:34:52.815715: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n2\r\n```\r\n\r\nThe build probably bakes in `TF_CUDA_VERSION` as `10`.  I'm not sure if we have a build that supports older cuda version, but you could try building one for yourself to check (`configure.py` should ask you for the cuda version).", "mine combination is tf 1.13 and cuda 10.. even cuda 9 is not working.\n\n\nOn Fri, Jul 19, 2019 at 10:44 AM Amit Moryossef <notifications@github.com>\nwrote:\n\n> Actually, works fine on the same machine hardware with CUDA Version\n> 10.0.130\n> It doesn't work on CUDA Version 8.0.61\n>\n> My bad, I checked CUDA version on the wrong server when reporting the\n> issue.\n>\n> Is this version of Tensorflow even supposed to work for v8? if not, I'll\n> close this issue.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30748?email_source=notifications&email_token=AJKLC6GLBGBDXX4DO27SP4LQAH4PNA5CNFSM4ID7TGW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2MIYOA#issuecomment-513313848>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJKLC6D4A7UN7WRQCI7ZAGTQAH4PNANCNFSM4ID7TGWQ>\n> .\n>\n", "Same problem here , not detecting any GPU device after upgrading the tensorflow version \r\ntensorflow only detect GPU XLA\r\nBut now , I cannot go back to tensorflow1.10.0 as it was been deleted from pip \r\nAny help ? cuda version V9.0.176", "I downgraded the tensorflow to an older version and the problem is\nresolved..\ncommand is like\nconda install tensorflow-gpu==1.9.0\n\n\nOn Wed, Jul 24, 2019 at 2:20 PM alaayadi <notifications@github.com> wrote:\n\n> Same problem here , not detecting any GPU device after upgrading the\n> tensorflow version\n> tensorflow only detect GPU XLA\n> But now , I cannot go back to tensorflow1.10.0 as it was been deleted from\n> pip\n> Any help ?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30748?email_source=notifications&email_token=AJKLC6C56BDJDTRSDWU762LQBDBQFA5CNFSM4ID7TGW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2XUDJA#issuecomment-514802084>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJKLC6H37AS6XXGYE6MMM63QBDBQFANCNFSM4ID7TGWQ>\n> .\n>\n", "> I downgraded the tensorflow to an older version and the problem is resolved.. command is like conda install tensorflow-gpu==1.9.0\r\n> [\u2026](#)\r\n> On Wed, Jul 24, 2019 at 2:20 PM alaayadi ***@***.***> wrote: Same problem here , not detecting any GPU device after upgrading the tensorflow version tensorflow only detect GPU XLA But now , I cannot go back to tensorflow1.10.0 as it was been deleted from pip Any help ? \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub <#30748?email_source=notifications&email_token=AJKLC6C56BDJDTRSDWU762LQBDBQFA5CNFSM4ID7TGW2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2XUDJA#issuecomment-514802084>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AJKLC6H37AS6XXGYE6MMM63QBDBQFANCNFSM4ID7TGWQ> .\r\n\r\nThat's just working ! thanks ", "If you have cuda 10.1 you need to update cuDNN to 7.6.2\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html\r\n\r\nThat's what got it working for me without downgrading Tensorflow", "For me, it was the placement of ArgMax ops on GPUs that broke it; relocating them to CPU did the trick.", "> For me, it was the placement of ArgMax ops on GPUs that broke it; relocating them to CPU did the trick.\r\n\r\nI'm having the same issue after accidentally upgrading CUDA and then downgrading again, how did you manage to relocate the ArgMax ops to CPU?", "@albertNod something along these lines: https://github.com/aymericdamien/TensorFlow-Examples/blob/042c25ce2c3d91bf5a2e0a308fea578b1e290f82/examples/6_MultiGPU/multigpu_cnn.py#L110\r\njust add 'ArgMax' to the list", "I met the same problem on ubuntu 18.04, cuda 10.1 and Tensorflow 1.14.0. However, I uninstalled the pip version tensorflow using `pip uninstall tensorflow-gpu` and then use `conda install -c anaconda tensorflow-gpu` to install conda version,  and it works for me. You can have a try. @AmitMY ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30748\">No</a>\n", "> I met the same problem on ubuntu 18.04, cuda 10.1 and Tensorflow 1.14.0. However, I uninstalled the pip version tensorflow using `pip uninstall tensorflow-gpu` and then use `conda install -c anaconda tensorflow-gpu` to install conda version, and it works for me. You can have a try. @AmitMY\r\n\r\nworks great! \r\nAlso, if you run into out of memory error\r\n`CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory`\r\nreducing the batch size will fix the problem", "> I met the same problem on ubuntu 18.04, cuda 10.1 and Tensorflow 1.14.0. However, I uninstalled the pip version tensorflow using `pip uninstall tensorflow-gpu` and then use `conda install -c anaconda tensorflow-gpu` to install conda version, and it works for me. You can have a try. @AmitMY\r\n\r\nThank you! Saved my day. I'm on ubuntu 16.04, cuda 10.1. And this works:\r\npip uninstall tensorflow\r\npip uninstall tensorflow-gpu\r\nconda install tensorflow-gpu==1.13.1\r\nThen check GPU availability by [example](https://www.tensorflow.org/guide/gpu)", "By installing CUDA 10.0, this problem may be solved.\r\n```\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-0 \\\r\n    libcudnn7=7.6.5.32-1+cuda10.0  \\\r\n    libcudnn7-dev=7.6.5.32-1+cuda10.0\r\n```", "I have similar issue with Tensorflow and Keras. And, don't have any issue with pytorch:\r\n- Tensorflow 2.2\r\n- CUDA: 10.2\r\n- CUDNN: 7.6.5\r\n\r\nNotice that I got feedback `Could not load dynamic library 'libcudart.so.10.1` when checking GPU with `test.is_gpu_available()`. \r\n\r\nSince I got libcudart.so.10.2 installed, I solved this issue by: make link` libcudart.so.10.1` from `libcudart.so.10.2` in `/usr/local/cuda-10.2/targets/x86_64-linux/lib` and make sure that `/usr/local/cuda-10.2/targets/x86_64-linux/lib` is in `LD_LIBRARY_PATH`.\r\n\r\nUpdate: got similar issue again in jupyter, need to close, then start jupyter again, but doesn't solve CUDNN_STATUS_INTERNAL_ERROR\r\n issue 100%. Switch to train with .py >> it works\r\n\r\nSee more in [this repository](https://github.com/t-thanh/gpu-check)", "```\r\nimport ctypes\r\n\r\ncuda_path = \"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.2\\\\bin\\\\\"\r\nto_load = [\"cudart64_102.dll\",\r\n           \"cublas64_10.dll\",\r\n           \"cufft64_10.dll\",\r\n           \"curand64_10.dll\",\r\n           \"cusolver64_10.dll\",\r\n           \"cusparse64_10.dll\",\r\n           \"cudnn64_7.dll\"]\r\n\r\nfor dll in to_load:\r\n    ctypes.WinDLL(cuda_path+dll)\r\n```\r\nDoesn't matter CUDA 10.0 , 10.1 10.2, for all dll not found, manually load them into ram. I have done all the setting PATH environment variable and nothing worked for me except this. \r\nMy setup is Windows 10 RTX2070  tf2.2 cuda 10.2 cudnn 7.6.5 avx2 from https://github.com/fo40225/tensorflow-windows-wheel. \r\n\r\nI usually use wheels built by others because official tensorflow software builds lag the hardware (tf2.2 is only tested up to cudnn 7.4 from https://www.tensorflow.org/install/source_windows). Last year June when i was trying to install tf1.4, the official tf only went up to 10.0 but my GPU required 10.1 \r\n\r\n**EDIT**\r\nI stopped doing this hack. The issue was python installed from Windows Store could not read PATH. Solution is install python from https://www.python.org/"]}, {"number": 30747, "title": "Penalty in the contractive auto-encoder", "body": "How can I compute a contraction penalty with Tensorflow? The contraction penalty is the squared of Frobenius norm \u2016 \u00b7 \u2016F of the Jacobian matrix of the encoder.", "comments": ["@Ramzi09 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "OS: Ubuntu 16.04 (64-bit)\r\nCPU: core i5-3230M CPU 2.60GHZ\r\nRAM: 8Go\r\nTensorFlow 1.10.1 \r\nI did install a binary \r\nI found the code in Keras. The link is : \r\nhttps://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\r\nHow can I change in TensorFlow?", "@Ramzi09 ,\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "Thanks, I asked this question on StackOverflow, but no answer until now ", "@Ramzi09 ,\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Closing since this is not a bug or feature request.Thanks!"]}, {"number": 30746, "title": "Couldn't find and import the 'graph_transforms' module in tensorflow1.14", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 24.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow.tools.graph_transforms as graph_transforms\r\n```\r\nError message:\r\n```\r\nAttributeError: 'module' object has no attribute 'graph_transforms'\r\n```\r\n\r\n**Describe the expected behavior**\r\nSuccessfully import the graph_transforms module.\r\n This code succesfully run with tf 1.13 but failed on tf 1.14.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow.tools.graph_transforms as graph_transforms\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe installed 'graph_transforms' module seems has been moved to tensorflow_core folder in site-packages.\r\nIs this as expected? \r\n\r\n", "comments": ["It seems when running latest the build_pip_package.sh script,  it would move the code from tensorflow folder into the tensorflow_core folder.\r\n```\r\n  # In order to break the circular dependency between tensorflow and\r\n  # tensorflow_estimator which forces us to do a multi-step release, we are\r\n  # creating a virtual python package called tensorflow and moving all the tf\r\n  # code into another python package called tensorflow_core:\r\n  #\r\n  #   * move code from tensorflow to tensorflow_core\r\n  #   * create the virtual pip package: create folder and __init__.py file with\r\n  #     needed code for transparent forwarding\r\n```", "@Leslie-Fang \r\nI tried reproducing issue with the provided code  using both TF1.13 and 1.14 and i am able to import \r\ngraph _transforms.Thanks!\r\n", "@ravikyram \r\nReally thanks  for your reply.\r\nI aslo try \r\n```\r\npip install tensorflow\r\n```\r\nIt would install tf1.14 and would **not** reproduce my issue.\r\n\r\nHowever if I build the TF from src with the commit-id: 81acfa851ecf413df02c6bdf4795630524f2f859\r\nIt do reproduce the issue.\r\n\r\n@perfinion Is my understanding right?", "I use this command to print tensorflow version\r\n```\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n1. Build from source code: https://github.com/tensorflow/tensorflow/commit/81acfa851ecf413df02c6bdf4795630524f2f859\r\n('v1.12.1-5541-g81acfa851e', '1.14.0')\r\n\r\n2. pip install tensorflow\r\n('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n\r\nIs this due to some version difference?\r\nWhich one would be the long-term supported?", "@ravikyram @ymodak  Hi, sorry to interrupt, but any update of this question? ", "I was able to reproduce your issue with TF-Nightly version '1.15.0-dev20190723' too.", "Anu update/solution on this issue?", "try `import tensorflow_core.tools.graph_transforms as graph_transforms` for tf1.x", "> try `import tensorflow_core.tools.graph_transforms as graph_transforms` for tf1.x\r\n\r\nupdate, use:\r\n`from tensorflow.tools.graph_transforms import xxx as yyy` for tf1.x", "For 2.0 it seems the issue has cropped up again:\r\n\r\n```bash\r\nIn [8]: tf.__version__                                                                                                                                                                        \r\nOut[8]: '2.1.0-dev20191114'\r\n\r\nIn [9]: from tensorflow.tools import graph_transforms                                                                                                                                         \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-9-81608bad8d7f> in <module>\r\n----> 1 from tensorflow.tools import graph_transforms\r\n\r\nImportError: cannot import name 'graph_transforms' from 'tensorflow_core.tools' (/usr/lib/python3.7/site-packages/tensorflow_core/tools/__init__.py)\r\n\r\nIn [10]: from tensorflow import tools                                                                                                                                                         \r\n\r\nIn [11]: dir(tools)                                                                                                                                                                           \r\nOut[11]: \r\n['__builtins__',\r\n '__cached__',\r\n '__doc__',\r\n '__file__',\r\n '__loader__',\r\n '__name__',\r\n '__package__',\r\n '__path__',\r\n '__spec__']\r\n```\r\nThe issue is reproducible both with TF2.0 release version and TF nightly.\r\n", "I think there's no entry to python wrapper of \"transform_graph\" when building tf2.0, which was in contrib module on tf1.x", "Was there any resolution or suggestion on how to proceed on this?\r\n", "Is there any solution to this?. Thanks.", "if i try to import from core, i get the following error:\r\n\r\n(Pdb) tf.__version__\r\n'2.2.0'\r\n(Pdb) from tensorflow_core.tools.graph_transforms import TransformGraph\r\n\r\n*** ImportError: cannot import name 'TransformGraphWithStringInputs' from 'tensorflow.python.pywrap_tensorflow' (/home/anurag/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py)\r\n\r\n(Pdb) from tensorflow.tools import graph_transforms\r\n\r\n*** ImportError: cannot import name 'graph_transforms' from 'tensorflow.tools' (/home/anurag/anaconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/tools/__init__.py)\r\n", "I am having this issue now with 2.2 I will go back to 1.13.1", "Is there any solution for TF 2.2?", "TransformGraphWithStringInputs is available in tf2.0 however I am planning to jump to tf2.2 or tf2.3 but I couldn't find \"TransformGraphWithStringInputs\" in these 2 versions. Anyone found any solution regarding that? ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30746\">No</a>\n", "Tensorflow: \r\n```\r\ntensorboard             2.5.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.0\r\ntensorflow              2.5.0\r\ntensorflow-estimator    2.5.0\r\ntensorflow-serving-api  2.5.1\r\n\r\n```\r\nError:\r\n\r\n```\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\n```\r\n\r\n> \r\n> --------------------------------------------------------------------------\r\n> ModuleNotFoundError                      Traceback (most recent call last)\r\n> <ipython-input-77-1fd86d9792e0> in <module>\r\n> ----> 1 from tensorflow.tools.graph_transforms import TransformGraph\r\n> \r\n> ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n\r\n\r\nI do not want to downgrade as 1.x has gone EOL. \r\nLast option would be to go [bazel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#transform-reference) it.\r\n", "Is there a solution for this?\r\nI am trying to use tf2onnx tool but I get an error:\r\n\r\n`python3.7 -m tf2onnx.convert --saved-model . --output model.onnx`\r\n\r\n```\r\n File \"/home/jupyter/.local/lib/python3.7/site-packages/tf2onnx/tfonnx.py\", line 20, in <module>\r\n    from tensorflow.tools.graph_transforms import TransformGraph\r\nModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n```\r\n\r\nUsing tensorflow 2.8.0\r\n\r\n", "\r\n\r\n\r\n> Is there a solution for this? I am trying to use tf2onnx tool but I get an error:\r\n> \r\n> `python3.7 -m tf2onnx.convert --saved-model . --output model.onnx`\r\n> \r\n> ```\r\n>  File \"/home/jupyter/.local/lib/python3.7/site-packages/tf2onnx/tfonnx.py\", line 20, in <module>\r\n>     from tensorflow.tools.graph_transforms import TransformGraph\r\n> ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n> ```\r\n> \r\n> Using tensorflow 2.8.0\r\n\r\n\r\nMake sure you are using recent versions of `onnx` and `tf2onnx`. \r\n\r\nYou can paste you issue on [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx) as well.\r\n\r\n### references\r\n- #### tfonnx\r\n    - https://github.com/onnx/tensorflow-onnx/blob/v1.9.3/tf2onnx/tfonnx.py  \r\n    - https://github.com/onnx/tensorflow-onnx/releases/tag/v1.9.3\r\n- ##### graph_transforms\r\n    - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#using-the-graph-transform-tool\r\n\r\n\r\n\r\n\r\n\r\n"]}, {"number": 30745, "title": "How to pad input sequences for implementing CuDNN LSTM in TF2.0?", "body": "For implementing CuDNN LSTM, according to [docs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM) there are 6 requirements as follows- \r\n\r\n> activation == 'tanh'\r\nrecurrent_activation == 'sigmoid'\r\nrecurrent_dropout == 0\r\nunroll is False\r\nuse_bias is True\r\nInputs are not masked or strictly right padded.\r\n\r\nAccording to my understanding, the last one say to have input sequences not right padded. However, in text classification I want to pad sequences and I am using [the function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#padded_batch), now I do not know how to left pad using this function, as I could check it always right pad the input sequences, which means one can never use CuDNN LSTM for training on GPU(s) if they are padding the inputs. However, it can be done easily using `tf.keras..pad_sequences` function, defined [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences). ", "comments": ["I think you are misreading the requirement. Inputs should be either [unmasked] or [strictly right-padded]. For reference, see the following function used to determine right-paddedness for CuDNN compatibility: https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/keras/layers/recurrent_v2.py#L1182\r\n\r\nNote: the LSTM with CuDNN is experiencing a lot of bug reports atm (e.g. in loops, in distributed strategies, etc.), all of which are open issues as of right now. However, concerning your question, I think you can close this issue.", "@jkamalu thank you for your answer and clarification. I still have a question about how to left pad in TF2.0, without using `tf.keras..pad_sequences` function. Is it possible?\r\n\r\n-Rishabh Sahrawat", "Use tf.pad. Choose the constant mode and parameterize the padding on the left-hand side with the max desired length and current length of the sequence and use no padding on the right side. Play around with eager tensors to get this right if it doesn't seem obvious from the documentation. \r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/pad\r\n\r\n If you want your input data padded, use tf.pad with the Dataset map function during data feeding. \r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map\r\n\r\nHowever, in my experience, this adds unnecessary overhead and can bottleneck training... So, what you should do if you can is preprocess all sequences to be padded however you like, save them to disk, and then load them as such during Dataset creation/iteration.", "@jkamalu thank you for your answer. I appreciate your help. "]}, {"number": 30744, "title": "why GetLocalGPUInfo() using cuda runtime api, I guess it should use cudart_stub instead.", "body": "Base on below commit, cuda Runtime API should wrapped in dynamic load\uff0c but I still see  tensorflow/core/grappler/clusters/utils.cc using cuda Runtime API.\r\n\r\ncommit: 7c5aed39f0148f93cebef850d939558a938b41e7\r\nRemove tensorflow core runtime's explicit dependency on cuda runtime.\r\n- Add stub implementation which is wrapped in dynamic loader\r\n- Replace the runtime dependency with stub in tf_cuda_library\r\n\r\nAdopted csigg@'s method in cl/230858143 to work around the issue where the cuda runtime calls in Eigen::initializeDeviceProp() cannot be wrapped with the macro method.\r\n\r\nPiperOrigin-RevId: 232063778", "comments": ["@chsigg any ideas?", "@yifeif might have missed them or they might have been introduced afterwards.", "@yifeif \uff0ccould you please tell me how to fix this issue? I tried use cc_library instead of tf_cuda_library, code path in GOOGLE_CUDA will be ignore as no DGOOGLE_CUDA = 1 passed to copts. \r\nThis is strange as you already use tf_cuda_library to build utils, but it still link to CUDA Runtime Library. \r\nDo you think alwayslink = 1 should be added in tf_gpu_library ? ", "Any suggestion? @yifeif ", "@chengleiwang I believe tensorflow/core/grappler/clusters/utils.cc is using cudart_stub.\r\n[`utils`](https://github.com/tensorflow/tensorflow/blob/4eb56665431534476801486257087f16d8ddee4c/tensorflow/core/grappler/clusters/BUILD#L22) is a tf_cuda_library, which depends on [`cudart_stub`](https://github.com/tensorflow/tensorflow/blob/4eb56665431534476801486257087f16d8ddee4c/tensorflow/tensorflow.bzl#L1359) for its implementation. ", "@yifeif, yes, utils depends on cudart_stub, but actually it doesn't call dynamic cuda runtime api. I have debug this for  several days. You can do a quick check by set a breakpoints at GetLocalGPUInfo() in utils.cc. You can see it still execute cuda runtime's cudaGetDeviceProperties Api without using dynamic one.\r\n\r\nI see that TensorFlow still using static library during the building step. So I guess this is the root cause for this issue. utils.cc will link to static library's symbol. So may I know why TensorFlow still depends on static library? I tried to remove static link part but I got build errors~ would you pls give me some suggestion? I worked on this issue for long days but no progress ~~", "Which platform are you on? ", "Ubuntu18.04\r\ncuda 10.0\r\nbuild command:  bazel build --config=opt --jobs=6  //tensorflow/tools/pip_package:build_pip_package --cxxopt='-g'", "@yifeif , Are you able to reproduce this issue? Any suggestion?", "@chengleiwang Could you please let us know if it is still an issue in TF v2.6.0 ,Please refer to [build from source](https://www.tensorflow.org/install/gpu), similar [issue](https://stackoverflow.com/questions/41409842/ubuntu-16-04-cuda-8-cuda-driver-version-is-insufficient-for-cuda-runtime-vers) ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30744\">No</a>\n"]}, {"number": 30743, "title": "How to build Tensorflow lib files with bazel from source code on Windows?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 pro Visual studio 2015\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): trying to build from source\r\n- TensorFlow version: gpu-1.13.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0/ cudnn 7.4.2\r\n- GPU model and memory: Geforce 1080Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to build Tensorflow-gpu lib files(tensorflow.dll and .lib files) with bazel from source code on Win10, but I have encountered serveral problems.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAfter build configure\r\n```cmd\r\nC:\\tensorflow>python configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 412c8d20-03eb-46e5-bf73-fd54952f1e01\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]:\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\nI ran \r\n```cmd\r\nC:\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\n**Any other info / logs**\r\nAfter a long time of compiling, the build did indeed succeed but the lib file it generates is \r\n    <code>libtensorflow.so</code>\r\n\r\nI expect the output lib file for windows to be <code>tensorflow.dll</code> and <code>tensorflow.lib</code> but I think this <code>.so</code> file is not useful on Windows, so I'd like to know:\r\n\r\n1. if I can build a <code>tensorflow.dll</code> and <code>tensorflow.lib</code> file on Win10 or not?\r\n\r\n2. and if not, how do I suppose to use this <code>libtensorflow.so</code> on Windows and deal with all the <code>include</code> file?\r\n", "comments": ["reference on [this thread](https://github.com/guikarist/tensorflow-windows-build-script/issues/21)"]}, {"number": 30742, "title": "AttributeError: module 'tensorflow' has no attribute 'get_default_graph'", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 30741, "title": "Mask R-CNN not learning on pet dataset even without any changes to the code", "body": "I am attempting to retrain pretrained model on COCO to detect images from Pet Dataset. I followed tutorials and managed to start the training. But according to results, the model is not trained properly.\r\n\r\nSteps to recreate:\r\n\r\n 1. `git clone https://github.com/cocodataset/cocoapi.git`\r\n 2. `conda create --name tf_test python=3.6`\r\n 3. `conda activate tf_test`\r\n 4. `pip install tensorflow`\r\n 5. `pip install pillow, lxml, jupyter, matplotlib, opencv, contextlib2`\r\n 6. download protoc-3.9.0-win32.zip extract to Program Files\\Google Protobuf\\\r\n 7. add Program Files\\Google Protobuf\\bin to path\r\n 8. `for /f %i in ('dir /b object_detection\\protos\\*.proto') do protoc object_detection\\protos\\%i --python_out=.`\r\n 9. created environment variable PYTHONPATH with 3 entries: TF\\models, TF\\models\\research\\slim, TF\\models\\research\\object_detection\r\n 10. `conda install -c anaconda cython`\r\n 11. `pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI`\r\n 12. restart\r\n 13. `cd TF\\models\\research\\object_detection`\r\n 14. `jupyter notebook`\r\n 15. open object_detection_tutorial.ipynb\r\n 16. added imports:\r\n\tfrom object_detection.utils import label_map_util\r\n\tfrom object_detection.utils import visualization_utils as vis_util\r\n 17. went through demo in object_detection without any problems (I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2)\r\n\r\nDirectory structure:\r\n\r\n - TF\r\n     + models\r\n     + training \r\n         * pet_dataset\r\n         * pet_tf_records\r\n         * model\r\n         * checkpoints\r\n\r\nThen:\r\n\r\n 18. downloaded pet dataset https://www.robots.ox.ac.uk/~vgg/data/pets/\r\n 19. paste contents into pet_dataset\r\n 20. `cd TF`\r\n 21. `conda activate tf_test`\r\n 22. manually set faces_only to False in create_pet_tf_record.py\r\n 23. \r\n```\r\npython models\\research\\object_detection\\dataset_tools\\create_pet_tf_record.py\r\n\t--data_dir=training\\pet_dataset \r\n\t--output_dir=training\\pet_tf_records \r\n\t--mask_type png \r\n\t--label_map_path=models\\research\\object_detection\\data\\pet_label_map.pbtxt\r\n```\r\n\r\n 24. downloaded mask_rcnn_inception_v2_coco from github to TF\\training\\model\r\n 25. moved config from models/research/object_detection/sample/configs/ to TF\\training\\model\r\n 26. changed class number to 37\r\n 27. changed paths accordingly in config file\r\n\r\n 28. \r\n```\r\npython models\\research\\object_detection\\model_main.py \r\n\t--model_dir=training\\checkpoints \r\n\t--num_train_steps 1000 \r\n\t--pipeline_config_path=training\\model\\mask_rcnn_inception_v2_coco.config\r\n```\r\n 29. let train for 20 hours\r\n\r\nI got following results, when examining in tensorboard, mAP gradually drops from the beginning of the training:\r\nI got following results:\r\n\r\n```\r\nAverage Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.003\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.019\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.030\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.030\r\nI0716 07:35:44.988143  3956 evaluation.py:275] Finished evaluation at 2019-07-16-07:35:44\r\nI0716 07:35:44.988143  3956 estimator.py:2039] Saving dict for global step 196: DetectionBoxes_Precision/mAP = 0.00063327036, DetectionBoxes_Precision/mAP (large) = 0.00067928224, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.0025016854, DetectionBoxes_Precision/mAP@.75IOU = 0.0002254508, DetectionBoxes_Recall/AR@1 = 0.018629802, DetectionBoxes_Recall/AR@10 = 0.030065296, DetectionBoxes_Recall/AR@100 = 0.030065296, DetectionBoxes_Recall/AR@100 (large) = 0.030065296, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.11927358, Loss/BoxClassifierLoss/localization_loss = 0.10073859, Loss/BoxClassifierLoss/mask_loss = 2.1088743, Loss/RPNLoss/localization_loss = 0.11095625, Loss/RPNLoss/objectness_loss = 0.54912615, Loss/total_loss = 2.988967, global_step = 196, learning_rate = 0.0002, loss = 2.988967\r\n```\r\n\r\nPlease, do you have any suggestions where did i go wrong?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 30740, "title": "If padding='same', which side is padded with 0?", "body": "If set `padding='same'` for `Conv1D`, which side (the start or the end) is padded with 0 if the `kernel_size=2` and `stride=1`?\r\nI checked the code, but didn't find any clue. Hope this simple question can be clarified here. Thanks.\r\n", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "@ravikyram Thanks for the suggestion. Before asking it here, I put the question on StackOverflow, but no reply after many days. Thus, I came here and thought this place would be more likely to have some responses.", "@JaneShenYY \r\n\r\nNormally padding with zeros will be distributed equally from both left and right  and also up and down.Normally It will not take from one direction.\r\n\r\nI am closing this issue since it is not a bug or feature request.Thanks!"]}, {"number": 30739, "title": "Refactor OptimizeDatasetOp and MapDefunOp", "body": "This PR refactors `OptimizeDatasetOp` and `MapDefunOp` and adds tests for `MapDefunOp`.\r\n\r\ncc: @jsimsa ", "comments": ["@gbaned An error happened while migrating the change for the internal checks. Could you please help re-trigger the tests?"]}, {"number": 30738, "title": "saved_model.save bug in 2.0.0b0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): maybe binary (pip)\r\n- TensorFlow version (use command below): 2.0.0b0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: P40/24451MB\r\n\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI save saved model.pb using tf.saved_model.save() in 2.0.0b0.\r\nand it gives this error.\r\n```\r\nW0716 04:34:59.213974 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49adb5da0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:34:59.375897 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49add7940>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:34:59.670762 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ade14a8>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:34:59.837181 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ade1fd0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:00.001157 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ade7b00>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:00.162233 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad6f668>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:00.321973 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad761d0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:00.503687 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad80198>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:00.669191 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad80cc0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:00.855475 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad87cc0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:01.017992 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad92828>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:01.204613 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad99828>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:01.524981 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ada1390>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:01.706990 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad28390>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:01.868694 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad28eb8>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:02.059983 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad30eb8>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:02.229396 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad35a20>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:02.413010 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad3da20>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:02.573688 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad45588>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:02.756634 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad4f588>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:02.916903 140620342499136 saved_model.py:722] Skipping full serialization of object <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7fe49ad550f0>, because an error occurred while tracing layer functions. Error message: Expected Operation, Variable, or Tensor, got None\r\nW0716 04:35:03.265970 140620342499136 saved_model.py:722] Skipping full serialization of object <deepnormal.Model object at 0x7fe4b82cd240>, because an error occurred while tracing layer functions. Error message: in converted code:\r\n\r\n\r\n    TypeError: tf__wrapped_call() takes 1 positional argument but 2 were given\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 106, in <module>\r\n    train(dataset, FLAGS.epochs)\r\n  File \"train.py\", line 78, in train\r\n    tf.saved_model.save(model, path)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 835, in save\r\n    meta_graph_def, saveable_view, signatures)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 531, in _fill_meta_graph_def\r\n    object_map, resource_map, asset_info = saveable_view.map_resources()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/save.py\", line 266, in map_resources\r\n    tensor_util.constant_value(capture))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 246, in constant\r\n    allow_broadcast=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 284, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\", line 455, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\n**Describe the expected behavior**\r\nWhen I reinstall tensorflow with 2.0.0a0 it works and available for tf serving server.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nMaybe the model should have `batchnorm` layer\r\ntf.saved_model.save(model, path)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jusonn,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@anush-o \r\nThanks for reply.\r\nSince this is my first issue report, I'm confusing how much code should I share.\r\nWould you let me know how much code snippet is needed?\r\nI think it's not the entire code.", "@jusonn ,\r\nThanks for response. Please provide us the code with which we can replicate the issue.", "Closing due to lack of recent activity.Thanks!", "Hi, I have the same problem in tensorflow 1.14. Any help is more than welcomed.", "Any update on it? This issue is not solved yet \ud83d\ude15", "For me, It was issued with tensorflow==2.0.0b0.\r\nNow, with tensorflow==2.0.0,  `tf.saved_model.save(model, dir)` works with model above.", "I get the same error:\r\n```\r\n--------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nc:\\Users\\Mislav\\Documents\\GitHub\\trademl\\trademl\\modeling\\random_forest\\train_nn.py in \r\n----> 293 tf.saved_model.save(model, model_path)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py in save(obj, export_dir, signatures)\r\n    820   object_saver = util.TrackableSaver(checkpoint_graph_view)\r\n    821   asset_info, exported_graph = _fill_meta_graph_def(\r\n--> 822       meta_graph_def, saveable_view, signatures)\r\n    823   saved_model.saved_model_schema_version = (\r\n    824       constants.SAVED_MODEL_SCHEMA_VERSION)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions)\r\n    508   resource_initializer_ops = []\r\n    509   with exported_graph.as_default():\r\n--> 510     object_map, resource_map, asset_info = saveable_view.map_resources()\r\n    511     for resource_initializer_function in resource_initializer_functions:\r\n    512       asset_dependencies = []\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py in map_resources(self)\r\n    243             and capture not in self.captured_tensor_node_ids):\r\n    244           copied_tensor = constant_op.constant(\r\n--> 245               tensor_util.constant_value(capture))\r\n    246           node_id = len(self.nodes)\r\n    247           node = _CapturedConstant(\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    244   \"\"\"\r\n    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 246                         allow_broadcast=True)\r\n    247 \r\n    248 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    282       tensor_util.make_tensor_proto(\r\n    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 284           allow_broadcast=allow_broadcast))\r\n    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    286   const_tensor = g.create_op(\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    452   else:\r\n    453     if values is None:\r\n--> 454       raise ValueError(\"None values not supported.\")\r\n    455     # if dtype is provided, forces numpy array to be the type\r\n    456     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n```\r\nafter I tried to save a model: `tf.saved_model.save(model, model_path)`\r\n\r\nIS there solution to this error?\r\n", "I have changed tf versoin, now I get different error, I will post in another issue."]}, {"number": 30737, "title": "Implement GetStats function for cuda malloc allocator", "body": "Addresses issue #30736 by overriding Allocator's virtual GetStats function in the cuda malloc allocator.\r\n\r\nThis rudimentary implementation calls GetStats with the base_allocator_ private member, as the [GPUDebugAllocator does](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc#L135-L137).", "comments": ["@MattConley Could you please check failed build errors? Thanks!"]}, {"number": 30736, "title": "GPU device creation fails when using the CUDA Malloc Allocator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14/master\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: RTX 2080\r\n\r\n**Describe the current behavior**\r\nGPU device creation fails when the cuda malloc allocator is selected, with the error \"No allocator statistics\".  This is because of an allocator stats check introduced between releases 1.13.1 and 1.14 in the gpu common runtime  [BaseGPUDeviceFactory::CreateGPUDevice function](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1310-L1312).  This check fails when using the cuda malloc allocator because the virtual Allocator method GetStats was never overridden.\r\n\r\n**Describe the expected behavior**\r\nGPU device creation should succeed if the user specifies use of the cuda malloc allocator.\r\n\r\n**Code to reproduce the issue**\r\nUse the TF_GPU_ALLOCATOR environment variable to select the cuda malloc allocator:\r\n`$ export TF_GPU_ALLOCATOR=\"cuda_malloc\"`\r\n\r\nThen, in a python shell, try to use the gpu:\r\n```\r\n$ python\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.test.is_gpu_available()\r\n```", "comments": ["Submitted PR #30737 as a proposed solution to implement the GetStats function, thereby not failing the `if(!stats)` check in gpu_device.cc", "Looks like my PR was successfully merged; closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30736\">No</a>\n", "Im encountering the same issue \r\n#48571 "]}, {"number": 30735, "title": "New Updated curl to 7.65.1 #30650 ", "body": "Updated curl to latest version", "comments": ["> The curl change looks good, but there are quite a bunch of unrelated changes in this file. Mind rebasing to master?\r\n\r\n@d0k Done with the rebasing , kindly check it again please.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only \"I consent.\" in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30735) for more info**.\n\n<!-- need_author_consent -->", "> I consent\r\n\r\n"]}, {"number": 30734, "title": "timeline export has bug for multiple gpu case", "body": "Timeline export has bug when using multiple tower on multiple gpu: the exported ops are almost all placed on gpu:0, while the other device(gpu:1, gpu:2) has almost no ops. I dumped the device placement, and find out that the placement is not the same as those in timeline.json. \r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github ](https://github.com/tensorflow/tensorflow/issues/new/choose)new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30733, "title": "[Intel MKL] Fixing static scan issue in mkl_layout_pass.cc", "body": "", "comments": ["In the original logic, if Tinput and TFilter are OK, then T1 and T2 are never initialized, but can then be subsquently passed to GetMklOpName()\r\n\r\nConversely, if Tinput and Tfilter are not OK, and T1 and T2 are OK, and the first call to IsMklLayoutDependentOp() returns false, then the 2nd call to IsMklLayoutDependentOp() will use uninitialized Tinput and Tfilter values. ", "@claynerobison Thank you for the explanation!", "@gbaned The `Windows Bazel` and `Windows Bazel GPU` test failures seem unrelated to this PR. Could you please help pull the PR in? Thank you very much!", "> @gbaned The `Windows Bazel` and `Windows Bazel GPU` test failures seem unrelated to this PR. Could you please help pull the PR in? Thank you very much!\r\n\r\n@penpornk  Sure. Thank you for the confirmation."]}, {"number": 30732, "title": "How do you export this model into a tensorflow model", "body": "Im following this code to train a CNN image classifier to classify Philippines currency. How would you export this model? to get the file to be interpreted by tensorflow", "comments": ["@thekingrenz23 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Closing due to lack of recent activity.Thanks!"]}, {"number": 30731, "title": "Contributor Addition?", "body": "", "comments": ["Grammar and contributor request", "This list is auto-generated from the commits. If you have submitted code, your alias or name should end up in the next release notes automatically."]}, {"number": 30729, "title": "Best practice of using tensor-core on TensorFlow r1.12", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: cuda 9.2\r\n- GPU model and memory: Volta 100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI'm wondering what is the best practice of leveraging V100 GPU on TensorFlow r1.12. I understand that Nvidia support it nicely with their TensorFlow container and TF-nightly build may have good support as well. For older version of TensorFlow, (r1.12 or r1.13), is \"--use_fp16=True\" and \"--xla_compile=True\" the best practice of leveraging tensor-cores?\r\n\r\nRight now my benchmark results indicates that fp16/xla can give 844 images/sec (resnet50, batch size = 128) while fp32/no xla gives 412.5. It is no where near the reported 3x improvement. \r\n\r\nThank you,", "comments": ["@jw447 can you share which exact script you are using to run the benchmark?\r\n\r\nThe 3x speed-up reported on [this page](https://devblogs.nvidia.com/nvidia-automatic-mixed-precision-tensorflow/) includes **doubling the batch size to 256**. This could explain the bulk of the difference.\r\n\r\nIn addition, I noticed that you are using CUDA 9.2 (and most probably a older version of cuDNN as well). Simply upgrading to CUDA 10.0 and the latest cuDNN (7.6 at time or writing) along with TF1.14 will increase performance due to performance enhancements in TF/cuDNN etc. \r\n\r\nThe `--use_fp16=True` option should be enabling mixed precision, which will enable the use of Tensor Cores, while the `--xla_compile=True` enables the XLA compiler which allows TF to fuse GPU kernels to increase performance by reducing the overall overhead of kernel launches, unnecessary transposes etc.\r\n\r\nGoing forward, the suggested way of tapping into Tensor Cores is to use the new **Automatic Mixed Precision (AMP)** API available in TF 1.14 and newer. You can find out more about AMP on this [page](https://developer.nvidia.com/automatic-mixed-precision) or on some improved documentation currently stuck at PR #29249 .", "Hi @tlkh,\r\n\r\nThanks for getting back to me. The script I'm using is as follows:\r\n\r\nargs='--data_format=NCHW --batch_size=512 --num_batches=100 --model=resnet50 --optimizer=sgd --variable_update=replicated --all_reduce_spec=nccl --use_fp16=True --xla_compile=False --nodistortions --gradient_repacking=2 --datasets_use_prefetch=True --per_gpu_thread_count=2 --loss_type_to_report=base_loss --compute_lr_on_cpu=True --single_l2_loss_op=True --local_parameter_device=gpu --num_gpus=6 --display_every=10'\r\n\r\nscript='~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py'\r\n\r\njsrun --nrs 1 --tasks_per_rs 1 --rs_per_host 1 --gpu_per_rs 6  python $script $args\r\n\r\nI will try TensorFlow 1.14 with CUDA 10.0 and cudnn 7.6. Is the env variable TF_ENABLE_AUTO_MIXED_PRECISION available in TensorFlow 1.14 already?\r\n\r\nMany thanks!", "In TensorFlow 1.14, automatic mixed precision is available via the `tf.train.experimental.enable_mixed_precision_graph_rewrite` function call. \r\nHere's a simple example how to use it with tf.keras. The usage in TensorFlow is very similar, just wrap a `tf.train.Optimizer` instead. \r\n\r\n```python\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    ...\r\n])\r\n\r\nopt = tf.keras.optimizers.SGD()\r\nopt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\n\r\nmodel.compile(loss=\"categorical_crossentropy\",\r\n              optimizer=opt,\r\n              metrics=[\"accuracy\"])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs)\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]