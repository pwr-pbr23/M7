[{"number": 45145, "title": "TFLu: Pass the correct size of buffers to the driver", "body": "Calculate the size of the buffers correctly before passing them to the Ethos-U\r\ndriver.\r\n\r\nFix for: https://issuetracker.google.com/u/1/issues/174060224", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45144, "title": "TFLu: Port squeeze op from TFLite", "body": "This is a fix for: https://github.com/tensorflow/tensorflow/issues/44789", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Again, no changes needed until we hear from @petewarden but in the case there might be value in refactor to add a reference implementation of squeeze in [lite/kernels/internal/reference](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/kernels/internal/reference) and then have the Lite and Micro kernels make use of that shared code.\r\n\r\nI haven't looked at this very carefully so very real chance of this comment being inaccurate, but it looks like the logic in the Prepare function might be shareable.", "As discussed, let's continue with this PR.", "@mansnils  Can you please resolve conflicts? Thanks!", "@petewarden @advaitjain ping for review", "@petewarden ping for re-review after resolving conflicts", "@mansnils  Can you please resolve conflicts? Thanks!", "@petewarden @advaitjain ping for review", "@petewarden @advaitjain ping for review", "@gbaned ping for merge", "@gbaned ready for merge", "@advaitjain is there problem merging?", "@mansnils can you please sync your branch to master ? thank you", "@rthadur branch is synced to master\r\n@petewarden can you please re-approve? thanks"]}, {"number": 45143, "title": "RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 51 (FlexResizeNearestNeighbor) failed to prepare.", "body": "Hello.,sorry to bother you. I am learning to build Android programs using tflite. The environment I use is\r\ntensorflow-2.2,SDK 29.0.3,NDK 21.\r\nWhen I use the model.tflite to build app,I get an error:\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'  Registration failed.\r\n``` \r\nI don't know how to solve this problem,maybe the conversion failed\uff1f\r\nThe program  of yolo-tiny I used  is https://github.com/bubbliiiing/yolov4-tiny-tf2.\r\nThe model of yolo-tiny can be used for detecting correctly.\r\nI have build tensorflow-lite-select-tf-ops.aar in Android project.\r\nMy English is poor,please excuse me.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (or github SHA if from source):tensorflow2.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nfrom nets.yolo4_tiny import yolo_body,yolo_eval\r\n\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom train import get_anchors\r\nfrom train import get_classes\r\n\r\nmodel=load_model('yolo_voc.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_types = [tf.float16]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \r\n                                      tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nwith tf.io.gfile.GFile('fmodel.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n \r\nfrom tflite_support import metadata as _metadata\r\npopulator = _metadata.MetadataPopulator.with_model_file(\"fmodel.tflite\")\r\npopulator.load_associated_files([\"model_data/voc_classes.txt\"])\r\npopulator.populate() \r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\nWhen I run the code above,the terminal prints:\r\n```\r\nUserWarning: File, 'voc_classes.txt', does not exsit in the metadata. But packing it to tflite model is still allowed.\r\n  \"tflite model is still allowed.\".format(f)\r\n```\r\nIf I use tf2.2 to get the details of tflite_model,I would get the error:\r\n\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 51 (FlexResizeNearestNeighbor) failed to prepare.\r\n\r\n```\r\n\r\nif I use tf2.3,I would get:\r\n\r\n```\r\nTfLiteFlexDelegate delegate: 1 nodes delegated out of 56 nodes with 1 partitions.\r\n[  1 416 416  3]\r\n<class 'numpy.float32'>\r\n[ 1 13 13 75]\r\n<class 'numpy.float32'>\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://drive.google.com/drive/folders/1sXTFLk5xkFSao7edJXFIsFIcojtR8wjK?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["Sorry,the mobile is HUAWEI mate30", "@Ben-Lee-2019 Can you please share a gist or a jupyter notebook. I am getting different error because of some `nets` dependencies. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/c3bf509b0905bdf0e4fe530b95eb4790/untitled.ipynb). Thanks!", "> @Ben-Lee-2019 Can you please share a gist or a jupyter notebook. I am getting different error because of some `nets` dependencies. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/c3bf509b0905bdf0e4fe530b95eb4790/untitled.ipynb). Thanks!\r\n\r\nHI\uff0cI am so sorry that I didn't describe the problem clearly,this conversion should be ran under this.(https://github.com/bubbliiiing/yolov4-tiny-tf2)", "@Ben-Lee-2019 It looks you're trying to use model converted using newer version than the runtime.\r\n\r\nCan you please convert and run your model from the same TF version - it will be better if you used a newer version 2.4 may be\r\n\r\nThanks", "> @Ben-Lee-2019 It looks you're trying to use model converted using newer version than the runtime.\r\n> \r\n> Can you please convert and run your model from the same TF version - it will be better if you used a newer version 2.4 may be\r\n> \r\n> Thanks\r\n\r\nThank you for your advice,I will try as soon as possible.", "@Ben-Lee-2019 Was this resolved with recent `tf-version` or provide a simple standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45143\">No</a>\n"]}, {"number": 45142, "title": "tflite android gpu delegate init error, what does it mean? MEAN: Index for axis out of range", "body": "error log:\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: MEAN: Index for axis out of range\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 58 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nwhen i run my own model producing by tensorflow 2.3, all keras layers.\r\n\r\ndevice: xiaomi 8 se\r\ndependence: \r\n implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly')\r\n implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly')\r\n implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly')", "comments": ["@csjiyw,\r\nCould you please provide the exact sequence of commands and complete code that you executed before running into the problem, so that we can look into this? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "we identified and fixed the bug.  please sync past commit 8206491e82f6614fa2daa314e1ec8b8ca0ed946d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45142\">No</a>\n"]}, {"number": 45141, "title": "TFLITE Flower Classification codelab not working on own model it's always crashed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (Ubuntu 20.04 ):\r\n- Mobile device (OPPO A37fw) \r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (2.2.0):\r\n- Python version:3.7\r\n\r\n\r\n**Describe the current behavior**\r\nI changed the model.tflite and label.txt on my own model and labels but it's not working. Using the default tflite model and label.txt work though.\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n", "comments": ["@gabbygab1233 \r\nPlease share a simple stand alone code to replicate the issue or share a colab gist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45141\">No</a>\n"]}, {"number": 45140, "title": "Some QUESTION about tflite\uff08Node number 51 (FlexResizeNearestNeighbor) failed to prepare.\uff09", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["\r\nsorry for operation error."]}, {"number": 45139, "title": "label_image build fail on x86", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: source (Branch: Master, Commit Id: b0d40302ec4ce301e841eb58fc7e6937e4624ab7)\r\n- TensorFlow version: 3.1.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nAfter cloning tensorflow sources:\r\n$ cd tensorflow/tensorflow/lite/tools/make\r\n$ ./download_dependencies.sh\r\n$ ./build_lib.sh label_image\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\nar: creating /home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a\r\ng++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_HAVE_CPUINFO -DTFLITE_WITHOUT_XNNPACK -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \\\r\n-o /home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/label_image /home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/examples/label_image/bitmap_helpers.o /home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/examples/label_image/label_image.o /home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/tools/evaluation/utils.o \\\r\n /home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a  -lstdc++ -lpthread -lm -lz -ldl -ldl\r\n/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/obj/tensorflow/lite/examples/label_image/label_image.o: In function `tflite::label_image::Main(int, char**)':\r\nlabel_image.cc:(.text+0x907d): undefined reference to `tflite::tools::ToolParams::Merge(tflite::tools::ToolParams const&, bool)'\r\nlabel_image.cc:(.text+0x921d): undefined reference to `tflite::Flags::Parse(int*, char const**, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)'\r\nlabel_image.cc:(.text+0x92a9): undefined reference to `tflite::Flags::Usage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tflite::Flag, std::allocator<tflite::Flag> > const&)'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:367: recipe for target '/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/label_image' failed\r\nmake: *** [/home/khan/vaibhav/review/tensorflow/tensorflow/lite/tools/make/gen/linux_x86_64/bin/label_image] Error 1\r\nmake: Leaving directory '/home/khan/vaibhav/review/tensorflow'\r\n\r\n", "comments": ["@vaibhavexleapsemi Can you please describe the issue with little more details. Thanks!", "I've confirmed the issue. Let me handle it.", "Hi @terryheo \r\nWhile building label_image following files are missing for linking i.e tensorflow/lite/tools/command_line_flags.cc tensorflow/lite/tools/tool_params.cc\r\nSo I had added the above files in ./tensorflow/lite/tools/make/Makefile as follows:\r\nLABEL_IMAGE_SRCS := \\\r\n        tensorflow/lite/examples/label_image/bitmap_helpers.cc \\\r\n        tensorflow/lite/examples/label_image/label_image.cc \\\r\n        tensorflow/lite/tools/evaluation/utils.cc \\\r\n        tensorflow/lite/tools/command_line_flags.cc \\\r\n        tensorflow/lite/tools/tool_params.cc\r\nand label_image got build.", "Also If we check BUILD file at ./tensorflow/lite/examples/label_image, in target label_image the dependency for command_line_flags and tool_params are present. \r\nSo by adding tensorflow/lite/tools/command_line_flags.cc and tensorflow/lite/tools/tool_params.cc to LABEL_IMAGE_SRCS in ./tensorflow/lite/tools/make/Makefile, will create same label_image as built by bazel.", "@terryheo thanks for fixing the issue in Commit Id: 684cec69dbbd0ede4eaa59397750be265fac17b3", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45139\">No</a>\n"]}, {"number": 45138, "title": "keras doesn't pickle the correct layer in model Sequential", "body": "**System information**\r\n- OS Platform: Windows, Linux\r\n- TensorFlow version ('v2.3.0-54-gfcc4b966f1', '2.3.1')\r\n- Python version: 3.8.6\r\n\r\n**Current behavior**\r\nI need to test two versions of `BatchNormalization` for making sure that moving to from `tf1` to `tf2` doesn't cause any dramatic change in the model performance. \r\n\r\nI have two alternatives in `tf2`:\r\n```\r\nfrom tensorflow.python.keras.layers.normalization import BatchNormalization as BatchNormalization1\r\nfrom tensorflow.python.keras.layers.normalization_v2 import BatchNormalization as BatchNormalization2\r\n```\r\nThe first option replicates `tf1` behavior whereas the second one replicates `tf2` behavior (the second import is also equivalent to `from tensorflow.keras.layers import BatchNormalization as BatchNormalization2`)\r\nNow I create two simple sequential models containing only one layer each\r\n\r\n```\r\nfrom tensorflow.keras.models import Sequential, load_model\r\n\r\nbatch_norm1 = BatchNormalization1(input_shape=[28, 28])\r\nbatch_norm2 = BatchNormalization2(input_shape=[28, 28])\r\n\r\nmodel1 = Sequential([batch_norm1])\r\nmodel2 = Sequential([batch_norm2])\r\n```\r\nIf I print the models layers I get:\r\n\r\n```\r\nprint(model1.layers)\r\n[<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x0000022446319160>]\r\n\r\nprint(model2.layers)\r\n[<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x0000022446319460>]\r\n\r\n```\r\nSo far so good, nothing unexpected. \r\nHowever, if I now save the two models and then load them in a different session (or also in the same one) the first model mysteriously changes its class meaning that is loaded as `tensorflow.python.keras.layers.normalization_v2.BatchNormalization object`\r\n instead of `tensorflow.python.keras.layers.normalization.BatchNormalization object`\r\n\r\n```\r\nmodel1.save('m1.h5')\r\nmodel2.save('m2.h5')\r\n\r\nmodel1 = load_model('m1.h5')\r\nmodel2 = load_model('m2.h5')\r\n\r\nprint(model1.layers)\r\n[<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x00000224463C5DC0>]\r\n\r\nprint(model2.layers)\r\n[<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x0000022446360C40>]\r\n```\r\n\r\nI believe this is a bug. \r\n\r\n**Useful info for isolating the problem**\r\n- The same behavior is present if I test `tensorflow.python.keras.layers.recurrent.GRU` and `tensorflow.python.keras.layers.recurrent_v2.GRU`\r\n- This problem is only present if I use the layer inside a `keras Model`, but if try to pickle the layer directly everything works as expected.\r\n\r\nMany thanks\r\nGio", "comments": ["any news on this?\r\nMany thanks ", "@ymodak \r\nI am able to replicate this issue,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f35f56ba80a00b33449ebb8ce0ffbc53/untitled474.ipynb\r\n).", "I see that if you load the model in TF 1.X session (1.15.2) it gives,\r\n```python\r\n[<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a4561d0>]\r\n[<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a3ef8d0>]\r\n```", "> I see that if you load the model in TF 1.X session (1.15.2) it gives,\r\n> \r\n> ```python\r\n> [<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a4561d0>]\r\n> [<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7fa89a3ef8d0>]\r\n> ```\r\n\r\nthat's because in TF1 you don't have the _v2 modules ", "This is actually intended behavior. The difference between v1 batch normalization and v2 batch normalization is runtime-specific: they're not two different objects with different behaviors, as far as the saved model is concerned. Saving a BN layer in v1 and loading it in v2 is supposed to create the v2 class.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45138\">No</a>\n"]}, {"number": 45137, "title": "TextLineDataset stops after first segment of multiple gzip'ed files concatenated together", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian sid\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):v1.12.1-38511-ge95a955af8 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source):  10.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nCreating a TextLineDataset with the 'GZIP' compression type stops returning lines if the datafile consists of multiple gzipped files concatenated together.  This method of concatenating compressed files is explicitly called out in the gzip manpage as supported behavior.  And indeed, zcat, gunzip, and others do return all the uncompressed data.  Even Python's gzip.decompress() method does so, so it is confusing that Tensorflow does not.  The alternative is to decompress the files, concatenate them, then recompress, but this is undesirable due to the time required.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport tensorflow as tf\r\nimport gzip\r\n\r\nf = open('data.gz', 'wb')\r\nfor i in range(10):\r\n        line = str(i) + '\\n' + str(i + 100) + '\\n'\r\n        f.write(gzip.compress(line.encode('utf-8')))\r\nf.close()\r\n\r\nds = tf.data.TextLineDataset('data.gz', compression_type='GZIP')\r\nfor i in ds:\r\n        print(i)\r\n```\r\n\r\ntf.Tensor(b'0', shape=(), dtype=string)\r\ntf.Tensor(b'100', shape=(), dtype=string)\r\n\r\nThis only returns 2 of the 20 expected items.", "comments": ["I have tried in colab with TF 2.4, nightly version (`2.5.0-dev20201123`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1f64705325efcbd92eb88f177bea626d/untitled534.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45137\">No</a>\n"]}, {"number": 45136, "title": "[TF2.4rc2] linalg.svd missing kernel for DT_HALF", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab (Ubuntu)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.0rc2\r\n- Python version: 3.6 (probably all)\r\n\r\n**Describe the current behavior**\r\ntf.linalg.svd fails on tf.half dtypes in TF2.4\r\nhttps://colab.research.google.com/drive/1Y3FRV5qmQnnGUVHBX2wsFa0hC2PVDUON?usp=sharing\r\n```\r\nNotFoundError: Could not find device for node: {{node Svd}} = Svd[T=DT_HALF, compute_uv=true, full_matrices=false]\r\nAll kernels registered for op Svd:\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n [Op:Svd]\r\n```\r\n\r\n**Describe the expected behavior**\r\ntf.linalg.svd works on tf.half dtypes in TF2.3\r\nhttps://colab.research.google.com/drive/1Gf_C6SAQgUHzEQfkZPXD8PITL8lW17-I?usp=sharing\r\n", "comments": ["Was able to reproduce the issue. Code runs without any issues on [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f88cb994811e3f3b1c0cbedf90831e84/45136-2-3.ipynb). \r\n\r\nHowever, on running with [TF v2.4.0rc2](https://colab.research.google.com/gist/amahendrakar/db9d1b97d70ec1a2b51d1055f02bb739/45136-2-4.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/35974fde861cb25d739bc80c91e03911/45136-2-5.ipynb#scrollTo=3PSlWW6GcMFu) it throws an error stating `NotFoundError: Could not find device for node`. Please find the attached gist. Thanks!", "@seanpmorgan We can either fix it by decorating SVD with `tf.function(experimental_compile=True)` in addons or specialize float16 kernel for SVD. It makes no sense to me when ops allow float16, but kernel does not specialize it. Therefore, I prefer option 2, but I'm afraid that it cannot meet 2.4.0 stable version release unless some owners can cherrypickup it.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/linalg/svd_op_gpu.cu.cc#L431-L436\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/linalg_ops.cc#L548\r\n\r\n**TL;DR**\r\n\r\nIn TF2.4, XLA_{CPU,GPU} are disabled by default. See the following lines or verify with `tf.config.list_logical_devices`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/compiler/jit/flags.cc#L162\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/compiler/jit/flags.cc#L162\r\nhttps://github.com/tensorflow/tensorflow/commit/68016e2697820763096254bfb139478f40c79442\r\n\r\nBecause soft device placement is enabled by default and float16 SVD kernel is registered for XLA, SVD thus can work in TF2.3 without float16 registeration on CPU/GPU.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/xla_op_registry.h#L48-L49\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/xla_svd_op.cc#L96-L97", "@WindQAQ,\r\nDo you mean to say the behavior is as expected? Can you please confirm if we can close this issue? Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45136\">No</a>\n"]}, {"number": 45135, "title": "Save model to json on TF 2.3 and TF 2.2", "body": "Hi, I'm Jerry Kim in Korea of Republic.\r\n\r\nI faced difference between TF 2.3 and TF 2.2.\r\n\r\nAfter training model using tf 2.3, I saved model using `.to_json()`.\r\n\r\nAt loading model time, I got error about `Value error:unknown layer : functional`.\r\n\r\nIf you want to correct this, Edit \r\n\r\n`{\"class_name\": \"Functional\", \"config\": {\"name\": \"functional_1\", \"layers\": [{\"class_name\": \"InputLayer\" ...`\r\n\r\nto \r\n\r\n`{\"class_name\": \"Model\", \"config\": {\"name\": \"model_1\", \"layers\": [{\"class_name\": \"InputLayer\" ...`\r\n\r\nAfter If you get error like `('Keyword argument not understood:', 'groups')`, \r\n\r\n![image](https://user-images.githubusercontent.com/25097024/100039069-e38a9d00-2e47-11eb-8c8a-ed76840b7072.png)\r\n\r\nErase all `\"groups\":1, `\r\n\r\n\r\nAfter then, loading model will work.\r\n\r\nThank you.", "comments": ["@jjerry-k \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease paste the error message/or any info (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue."]}, {"number": 45134, "title": "How to create multi-timeseries dataset input ? (a list of tf.tensor)", "body": "## URL(s) with the issue:\r\n\r\nExamples doesn't contain my case : \r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\nFor example, I want to\r\n\r\n1. feed 3 timeseries to 3 individual lstm t1,t2,t3\r\n2. concat t1,t2,t3 , then pass to dense or conv layers\r\n\r\nTo achieve that , I have to use functional api , which need :\r\n- input :  [ tf.Tensor(..), tf.Tensor(..), tf.Tensor(..)]   # must be list or tuple\r\n- target: tf.Tensor(..)\r\n\r\nThe problem is : official api `tf.keras.preprocessing.timeseries_dataset_from_array` would return tensor , not list of tensor . \r\n\r\n\r\n# Dummy code:\r\n\r\n### # Model Stucture\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ndata = pd.DataFrame(np.random.uniform(size=(1000,3)), columns=['Sales', 'SalesDiff7', 'SalesAggMean7'])\r\n\r\nmulti_inputs = []\r\nmulti_outputs = []\r\nwindow_size = 1\r\n\r\nfor i in range(data.shape[1]):\r\n    ti = keras.Input(shape=(window_size, 1), name=f't{i}')\r\n    tlstm = layers.LSTM(32)(ti)\r\n    tp = keras.layers.Dense(units=1)(tlstm)\r\n    multi_inputs.append(ti)\r\n    multi_outputs.append(tp)\r\n    \r\nr = tf.concat(multi_outputs, -1)\r\nc = keras.layers.Flatten()(r)\r\nresult = keras.layers.Dense(units=1)(c)\r\n```\r\n### # WindowGenerator\r\n\r\nMainly copy from https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets\r\n```\r\nn = len(data)\r\ntrain_df = data[0:int(n*0.7)]\r\nval_df = data[int(n*0.7):int(n*0.9)]\r\ntest_df = data[int(n*0.9):]\r\n\r\ndefault_batch_size = 32\r\n\r\nclass WindowGenerator():\r\n  def __init__(self, input_width, label_width, shift,\r\n               train_df=train_df, val_df=val_df, test_df=test_df,\r\n               label_columns=None):\r\n    # Store the raw data.\r\n    self.train_df = train_df\r\n    self.val_df = val_df\r\n    self.test_df = test_df\r\n\r\n    # Work out the label column indices.\r\n    self.label_columns = label_columns\r\n    if label_columns is not None:\r\n      self.label_columns_indices = {name: i for i, name in\r\n                                    enumerate(label_columns)}\r\n    self.column_indices = {name: i for i, name in\r\n                           enumerate(train_df.columns)}\r\n\r\n    # Work out the window parameters.\r\n    self.input_width = input_width\r\n    self.label_width = label_width\r\n    self.shift = shift\r\n\r\n    self.total_window_size = input_width + shift\r\n\r\n    self.input_slice = slice(0, input_width)\r\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\r\n\r\n    self.label_start = self.total_window_size - self.label_width\r\n    self.labels_slice = slice(self.label_start, None)\r\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\r\n\r\n  def __repr__(self):\r\n    return '\\n'.join([\r\n        f'Total window size: {self.total_window_size}',\r\n        f'Input indices: {self.input_indices}',\r\n        f'Label indices: {self.label_indices}',\r\n        f'Label column name(s): {self.label_columns}'])\r\n\r\n\r\n@property\r\ndef train(self):\r\n  return self.make_dataset(self.train_df)\r\n\r\n@property\r\ndef val(self):\r\n  return self.make_dataset(self.val_df)\r\n\r\n@property\r\ndef test(self):\r\n  return self.make_dataset(self.test_df)\r\n\r\nWindowGenerator.train = train\r\nWindowGenerator.val = val\r\nWindowGenerator.test = test\r\n\r\n```\r\n\r\n### # Dataset generator\r\n\r\n```\r\n\r\ndef split_multi_window(self, features):\r\n  inputs = features[:, self.input_slice, :]\r\n  labels = features[:, self.labels_slice, :]\r\n  if self.label_columns is not None:\r\n    labels = tf.stack(\r\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\r\n        axis=-1)\r\n\r\n  # Slicing doesn't preserve static shape information, so set the shapes\r\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\r\n  inputs.set_shape([None, self.input_width, None])\r\n\r\n  # split to multi-timeseries\r\n  inputs = tf.split(inputs, num_or_size_splits=features.shape[-1], axis=len(features.shape)-1)\r\n  labels.set_shape([None, self.label_width, None])\r\n\r\n  return inputs, labels\r\n\r\nWindowGenerator.split_multi_window = split_multi_window\r\n\r\ndef make_dataset(self, data):\r\n  data = np.array(data, dtype=np.float32)\r\n  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n      data=data,\r\n      targets=None,\r\n      sequence_length=self.total_window_size,\r\n      sequence_stride=1,\r\n      shuffle=True,\r\n      batch_size=default_batch_size,)\r\n\r\n  ds = ds.map(self.split_multi_window)  # here is the problem\r\n\r\n  return ds\r\n\r\nWindowGenerator.make_dataset = make_dataset\r\n\r\n```\r\n\r\n### Trainning\r\n\r\n\r\n```\r\nsingle_step_window = WindowGenerator(\r\n    input_width=1, label_width=1, shift=1,\r\n    label_columns=['Sales'])\r\n\r\nmodel = keras.Model(\r\n    inputs=multi_inputs,\r\n    outputs=result,\r\n)\r\nmodel.compile(loss=tf.losses.MeanSquaredError(),\r\n            optimizer=tf.optimizers.Adam(),\r\n            metrics=[tf.metrics.MeanAbsoluteError()])\r\n\r\nhistory = model.fit( single_step_window.train ,  epochs=MAX_EPOCHS,\r\n                  validation_data=single_step_window.val,\r\n                  callbacks=[early_stopping])\r\n\r\n\r\n```\r\n\r\n\r\n### got error \r\n\r\n```\r\n\r\nAssertionError: in user code:\r\n\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\r\n        y_pred = self(x, training=True)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:385 call\r\n        return self._run_internal_graph(\r\n    /home/ufo/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:517 _run_internal_graph\r\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\r\n\r\n    AssertionError: Could not compute output Tensor(\"dense_37/BiasAdd:0\", shape=(None, 1), dtype=float32)\r\n\r\n```\r\n\r\n\r\nAfter some debug , I found `single_step_window.train` became `[3, 32, 1, 1]` tensor instead of a list of 3 `[32, 1, 1]` tensor . \r\n\r\nI tried :\r\n```\r\ndef make_dataset(self, data):\r\n  data = np.array(data, dtype=np.float32)\r\n  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n      data=data,\r\n      targets=None,\r\n      sequence_length=self.total_window_size,\r\n      sequence_stride=1,\r\n      shuffle=True,\r\n      batch_size=default_batch_size,)\r\n\r\n  def gen_ds():\r\n    for features in ds:\r\n      yield self.split_3d_window(ds)\r\n\r\n  ds = tf.data.Dataset.from_generator(gen_ds, (list, tf.float32))\r\n    \r\n  return ds\r\n```\r\n\r\nBut it complain `from_generator` can not return list . What should I do ??? It better provides an example for this case. \r\n\r\n", "comments": ["@eromoe \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using?\r\nThanks!", "It is 2.3.1 .\r\nI choosed Documentation Issue ,  it didn't ask for TF version .", "@eromoe \r\n\r\nI have tried in colab with TF version 2.3 and i am seeing the error `NameError: name 'MAX_EPOCHS' is not defined`.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/613a265ee2f7de62cf16d363c455cc10/untitled533.ipynb#scrollTo=b_Llfb0h_BdV).Please, help us with colab link or reproducible code.It helps us in localizing the issue faster. Thanks!", "Just a missing variable , added it \r\n\r\nhttps://colab.research.google.com/gist/eromoe/2264ab2cc83142c26862afea9f650b6c/untitled533.ipynb#scrollTo=ZOrJWX7R_GQ2", "Hi @eromoe, does [this notebook ](https://www.kaggle.com/nicapotato/keras-timeseries-multi-step-multi-output)cover your use case?\r\n\r\nGithub is for bugs and performance issues. For support requests like this one please post on Stack Overflow. There's a larger community there to answer questions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45134\">No</a>\n"]}, {"number": 45133, "title": "[Intel-MKL] Adding support to use Icelake compilation options.", "body": "These changes enable icelake option during compilation", "comments": ["@penpornk \r\nplease take a look ", "@penpornk please review this.\r\nthank you\r\n", "@penpornk \r\nhave updated with all the changes requested.", "@penpornk \r\nThank you good catch\r\nupdated"]}, {"number": 45132, "title": "Update r2.4 cmake", "body": "This is for testing presumit.", "comments": []}, {"number": 45131, "title": "r2.4: Update CMake build rules of TFLite", "body": "These are cherry-picks of master branch to make CMake build more stable on 2.4 branch.\r\n\r\nChanges are mostly on tensorflow/lite/CMakeLists.txt.", "comments": ["Please don't merge before @goldiegadde has a look too", "@goldiegadde could you approve this? This is needed for our public announcement on TFLite CMake support."]}, {"number": 45130, "title": "r2.4: Update TFLite CMake build rules", "body": "", "comments": []}, {"number": 45129, "title": "[Intel-MKL] ICX changes.", "body": "Enabling ICX changes for running MKL on ICX", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45129) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45129) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45129) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45129) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45129) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45129) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 45128, "title": "[CherryPick:r2.4] Rename exec_tools to tools", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45128) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45127, "title": "Add build badge for community supported continuous build for Xtensa.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45126, "title": "Update version numbers for TensorFlow 2.4.0-rc3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.4.0-rc2\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.4.0rc2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 45125, "title": "[CherryPick:r2.4]Change NCCL_PATCH to 6", "body": "We're building against NCCL 2.7.6, but were incorrectly reporting the version as\n2.7.3.\n\nPiperOrigin-RevId: 343586102\nChange-Id: I4cf2c4f1b7fdd97ff94662baf9c586b1055681ee", "comments": []}, {"number": 45124, "title": "[CherryPick:r2.4]Rename exec_tools to tools", "body": "Follow up to #43156\nBased on\nhttps://github.com/bazelbuild/bazel/issues/12059#issuecomment-725641997\nexec_tools might no longer be needed and hence can be replaced by tools.\nThis fixes various build failures caused by missing environment\nvariables in environments where they are required, e.g. using custom\ncompilers.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45124) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45123, "title": "Metadata writer for custom TFLite object detection model", "body": "I would like to see more detail on how to add metadata to **custom** models, e.g. for object detection and segmentation models.\r\n\r\nThe new TFLite file format includes metadata, like input/ouput node info, normalization, and labels in a zip file that is appended after the .tflite binary.   This was **very** confusing, and makes it hard for those of us making custom models.   Documentation on adding metadata is available here:\r\n\r\nhttps://www.tensorflow.org/lite/convert/metadata\r\n\r\nThe web page includes a script to add metadata to a classifier:\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/metadata/metadata_writer_for_image_classifier.py\r\n\r\nHowever I haven't found any docs or scripts on how to do this for custom object detection or segmentation models.   I've modified the script above for object detection, trying to match relevant metadata in the TFHub SSD model below:\r\n\r\nhttps://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2\r\n\r\nI believe the metadata, combined with TFHub models, make it easier for beginners to get started.   However, it complicates things for those of us creating custom models from scratch, and not using TFHub models.\r\n\r\n", "comments": ["Sample hacked script for SSD-style object detector, based on fumbling through metadata of a TFHub SSD model.   Trial and error.  Seems to work for a custom model, but not sure if it's 100% correct.\r\n\r\nIt would be nice if more scripts for object detection and segmentation are made available in the official documentation.\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\n# Origin:\r\n#     https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/metadata/metadata_writer_for_image_classifier.py\r\n#\r\n# References:\r\n#     https://www.tensorflow.org/lite/convert/metadata\r\n#     https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2\r\n\r\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"Writes metadata and label file to object detection models.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\n\r\nfrom absl import app\r\nfrom absl import flags\r\nimport tensorflow as tf\r\n\r\nimport flatbuffers\r\n# pylint: disable=g-direct-tensorflow-import\r\nfrom tflite_support import metadata_schema_py_generated as _metadata_fb\r\nfrom tflite_support import metadata as _metadata\r\n# pylint: enable=g-direct-tensorflow-import\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef define_flags():\r\n  flags.DEFINE_string(\"model_file\", None,\r\n                      \"Path and file name to the TFLite model file.\")\r\n  flags.DEFINE_string(\"label_file\", None, \"Path to the label file.\")\r\n  flags.DEFINE_string(\"export_directory\", None,\r\n                      \"Path to save the TFLite model files with metadata.\")\r\n  flags.mark_flag_as_required(\"model_file\")\r\n  flags.mark_flag_as_required(\"label_file\")\r\n  flags.mark_flag_as_required(\"export_directory\")\r\n\r\n\r\nclass ModelSpecificInfo(object):\r\n  \"\"\"Holds information that is specificly tied to an image classifier.\"\"\"\r\n\r\n  def __init__(self, name, version, image_width, image_height, image_min,\r\n               image_max, mean, std, num_classes, description, author, license):\r\n    self.name = name\r\n    self.version = version\r\n    self.image_width = image_width\r\n    self.image_height = image_height\r\n    self.image_min = image_min\r\n    self.image_max = image_max\r\n    self.mean = mean\r\n    self.std = std\r\n    self.num_classes = num_classes\r\n    self.description = description\r\n    self.author = author\r\n    self.license = license\r\n\r\n\r\n_MODEL_INFO = {\r\n    \"detect.tflite\":\r\n        ModelSpecificInfo(\r\n            name=\"SSD detector\",\r\n            version=\"v1\",\r\n            description = \"Sample SSD detector\",\r\n            author = \"author here\",\r\n            license = \"license here\",\r\n            image_width=300,\r\n            image_height=300,\r\n            image_min=0,\r\n            image_max=255,\r\n            mean=[127.5],\r\n            std=[127.5],\r\n            num_classes=3)\r\n}\r\n\r\n\r\nclass MetadataPopulatorForObjectDetector(object):\r\n  \"\"\"Populates the metadata for an object detector.  \"\"\"\r\n\r\n  def __init__(self, model_file, model_info, label_file_path):\r\n    self.model_file = model_file\r\n    self.model_info = model_info\r\n    self.label_file_path = label_file_path\r\n    self.metadata_buf = None\r\n\r\n  def populate(self):\r\n    \"\"\"Creates metadata and then populates it for an image classifier.\"\"\"\r\n    self._create_metadata()\r\n    self._populate_metadata()\r\n\r\n  def _create_metadata(self):\r\n    \"\"\"Creates the metadata for an image classifier.\"\"\"\r\n\r\n    # Creates model info.\r\n    model_meta = _metadata_fb.ModelMetadataT()\r\n    model_meta.name = self.model_info.name\r\n    model_meta.description = self.model_info.description\r\n    model_meta.version = self.model_info.version\r\n    model_meta.author = self.model_info.author\r\n    model_meta.license = self.model_info.license\r\n\r\n    # Creates input info.\r\n    input_meta = _metadata_fb.TensorMetadataT()\r\n    input_meta.name = \"normalized_input_image_tensor\" #\"input_1\"\r\n    input_meta.description = (\r\n        \"Input image to be classified. The expected image is {0} x {1}, with \"\r\n        \"three channels (red, blue, and green) per pixel. Each value in the \"\r\n        \"tensor is a single byte between {2} and {3}.\".format(\r\n            self.model_info.image_width, self.model_info.image_height,\r\n            self.model_info.image_min, self.model_info.image_max))\r\n    input_meta.content = _metadata_fb.ContentT()\r\n    input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\r\n    input_meta.content.contentProperties.colorSpace = (\r\n        _metadata_fb.ColorSpaceType.RGB)\r\n    input_meta.content.contentPropertiesType = (\r\n        _metadata_fb.ContentProperties.ImageProperties)\r\n    input_normalization = _metadata_fb.ProcessUnitT()\r\n    input_normalization.optionsType = (\r\n        _metadata_fb.ProcessUnitOptions.NormalizationOptions)\r\n    input_normalization.options = _metadata_fb.NormalizationOptionsT()\r\n    input_normalization.options.mean = self.model_info.mean\r\n    input_normalization.options.std = self.model_info.std\r\n    input_meta.processUnits = [input_normalization]\r\n    input_stats = _metadata_fb.StatsT()\r\n    input_stats.max = [self.model_info.image_max]\r\n    input_stats.min = [self.model_info.image_min]\r\n    input_meta.stats = input_stats\r\n\r\n    output_tensor_groups = _metadata_fb.TensorMetadataT()\r\n    output_tensor_groups.name = \"detection result\"\r\n    output_tensor_groups.output_tensor_names = ['location', 'category', 'score']\r\n\r\n    location_meta = _metadata_fb.TensorMetadataT()\r\n    location_meta.name = \"location\"\r\n    location_meta.description = \"bounding boxes\"\r\n    location_meta.content = _metadata_fb.ContentT()\r\n    location_meta.content.contentPropertiesType = (_metadata_fb.ContentProperties.BoundingBoxProperties)\r\n    location_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\r\n    location_meta.content.contentPropertiesType = (\r\n        _metadata_fb.ContentProperties.FeatureProperties)\r\n    location_meta.content.content_properties.type = \"BOUNDARIES\"\r\n    location_meta.content.content_properties.index = [1, 0, 3, 2]\r\n\r\n    location_range = _metadata_fb.ValueRangeT()\r\n    location_range.max = 2\r\n    location_range.min = 2\r\n    location_meta.content.range = location_range\r\n\r\n    category_meta = _metadata_fb.TensorMetadataT()\r\n    category_meta.name = \"category\"\r\n    category_meta.description = \"category of boxes\"\r\n    label_file = _metadata_fb.AssociatedFileT()\r\n    label_file.name = os.path.basename(self.label_file_path)\r\n    label_file.description = \"Labels for objects that the model can recognize.\"\r\n    label_file.type = _metadata_fb.AssociatedFileType.TENSOR_VALUE_LABELS\r\n    #label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\r\n    category_meta.associatedFiles = [label_file]\r\n    category_meta.content = _metadata_fb.ContentT()\r\n    category_meta.content.contentPropertiesType = (_metadata_fb.ContentProperties.FeatureProperties)\r\n    category_range = _metadata_fb.ValueRangeT()\r\n    category_range.min = 2\r\n    category_range.max = 2\r\n    category_meta.content.range = location_range\r\n\r\n    score_meta = _metadata_fb.TensorMetadataT()\r\n    score_meta.name = \"score\"\r\n    score_meta.description = \"scores of detected boxes\"\r\n    score_meta.content = _metadata_fb.ContentT()\r\n    score_meta.content.contentPropertiesType = (_metadata_fb.ContentProperties.FeatureProperties)\r\n    score_range = _metadata_fb.ValueRangeT()\r\n    score_range.min = 2\r\n    score_range.max = 2\r\n    score_meta.content.range = location_range\r\n\r\n    ndetections_meta = _metadata_fb.TensorMetadataT()\r\n    ndetections_meta.name = \"numdets\"\r\n    ndetections_meta.description = \"number of detected boxes\"\r\n    ndetections_meta.content = _metadata_fb.ContentT()\r\n    ndetections_meta.content.contentPropertiesType = (_metadata_fb.ContentProperties.FeatureProperties)\r\n\r\n\r\n    # Creates subgraph info.\r\n    subgraph = _metadata_fb.SubGraphMetadataT()\r\n    subgraph.inputTensorMetadata = [input_meta]\r\n    subgraph.outputTensorGroups = [output_tensor_groups]\r\n    subgraph.outputTensorMetadata = [location_meta, category_meta, score_meta, ndetections_meta]\r\n    model_meta.subgraphMetadata = [subgraph]\r\n\r\n    b = flatbuffers.Builder(0)\r\n    b.Finish(\r\n        model_meta.Pack(b),\r\n        _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\r\n    self.metadata_buf = b.Output()\r\n\r\n  def _populate_metadata(self):\r\n    \"\"\"Populates metadata and label file to the model file.\"\"\"\r\n    populator = _metadata.MetadataPopulator.with_model_file(self.model_file)\r\n    populator.load_metadata_buffer(self.metadata_buf)\r\n    populator.load_associated_files([self.label_file_path])\r\n    populator.populate()\r\n\r\n\r\ndef main(_):\r\n  model_file = FLAGS.model_file\r\n  model_basename = os.path.basename(model_file)\r\n  if model_basename not in _MODEL_INFO:\r\n    raise ValueError(\r\n        \"The model info for, {0}, is not defined yet.\".format(model_basename))\r\n\r\n  export_model_path = os.path.join(FLAGS.export_directory, model_basename)\r\n\r\n  # Copies model_file to export_path.\r\n  tf.io.gfile.copy(model_file, export_model_path, overwrite=True)\r\n\r\n  # Generate the metadata objects and put them in the model file\r\n  populator = MetadataPopulatorForObjectDetector(\r\n      export_model_path, _MODEL_INFO.get(model_basename), FLAGS.label_file)\r\n  populator.populate()\r\n\r\n  # Validate the output model file by reading the metadata and produce\r\n  # a json file with the metadata under the export path\r\n  displayer = _metadata.MetadataDisplayer.with_model_file(export_model_path)\r\n  export_json_file = os.path.join(FLAGS.export_directory,\r\n                                  os.path.splitext(model_basename)[0] + \".json\")\r\n  json_file = displayer.get_metadata_json()\r\n  with open(export_json_file, \"w\") as f:\r\n    f.write(json_file)\r\n\r\n  print(\"Finished populating metadata and associated file to the model:\")\r\n  print(model_file)\r\n  print(\"The metadata json file has been saved to:\")\r\n  print(export_json_file)\r\n  print(\"The associated file that has been been packed to the model is:\")\r\n  print(displayer.get_packed_associated_file_list())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  define_flags()\r\n  app.run(main)\r\n```", "Hey @lu-wang-g , could you take a look at this? Not sure if you have thought of something like this.", "@holokai-ai Thanks for the feedback! We are actively working on publishing a metadata writer library that will help to create the metadata for popular ML tasks. Please stay tuned.\r\n\r\nFor object detection, please refer to this [stack overflow question](https://stackoverflow.com/questions/64097085/issue-in-creating-tflite-model-populated-with-metadata-for-object-detection/64493506?noredirect=1#comment115038303_64493506) for sample script.", "Update:\r\n\r\nThe [Metadata Writer library](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata/python/metadata_writers) has been released. It currently supports image classifier and object detector, and more supported tasks are on the way.\r\n\r\nHere is an example to write metadata for an object detector model:\r\n1. Install the TFLite Support nightly Pypi package:\r\n```\r\npip install tflite_support_nightly\r\n```\r\n\r\n2. Write metadata using the following script:\r\n```\r\nfrom tflite_support.metadata_writers import object_detector\r\nfrom tflite_support.metadata_writers import writer_utils\r\nfrom tflite_support import metadata\r\n\r\nObjectDetectorWriter = object_detector.MetadataWriter\r\n_MODEL_PATH = \"ssd_mobilenet_v1_1_default_1.tflite\"\r\n_LABEL_FILE = \"labelmap.txt\"\r\n_SAVE_TO_PATH = \"ssd_mobilenet_v1_1_default_1_metadata.tflite\"\r\n\r\nwriter = ObjectDetectorWriter.create_for_inference(\r\n    writer_utils.load_file(_MODEL_PATH), [127.5], [127.5], [_LABEL_FILE])\r\nwriter_utils.save_file(writer.populate(), _SAVE_TO_PATH)\r\n\r\n# Verify the populated metadata and associated files.\r\ndisplayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\r\nprint(\"Metadata populated:\")\r\nprint(displayer.get_metadata_json())\r\nprint(\"Associated file(s) populated:\")\r\nprint(displayer.get_packed_associated_file_list())\r\n```", "@holokai-ai,\r\nCan you please confirm if we can close this issue as [Meta Data Writer Library](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/metadata/python/metadata_writers) is released as stated in the [above comment](https://github.com/tensorflow/tensorflow/issues/45123#issuecomment-754856953)? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45122, "title": "[TF-TRT] Added an environment variable to user specified TRT Algorithm ID", "body": "This PR allows users to use environment variable `TF_TRT_FIXED_ALGORITHM_ID` to specify the ID for a TRT algorithm selection. The valid values for the environment variable are any non-negative integer. This is useful to debug TensorRT or workaround TensorRT bugs.", "comments": ["@bixia1 I finally validated everything with TRT team:\r\n\r\nIt turns out that `Cast` is completely broken in TRT5. It's safer to deactivate completely for TRT < 7.0.0.11\r\nI updated the code in consequence ;) \r\n\r\nMy commits are squashed and you can now launch the unittests and merge if good ;) .", "@bixia1 if fine with you I will keep \"TACTIC\" instead of \"ALGORITHM\" because this really refers to TensorRT Tactic.\r\nI prefer keeping the accurate name for TensorRT.\r\n\r\nI will keep the env var name as follow: `TF_DEBUG_TRT_DETERMINISTIC_TACTIC_ID` to match the naming format already existing in the codebase: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L2347 \r\n\r\nAs a general idea, I would prefer to keep things \"uniform\". So following the format `TF_DEBUG_TRT_<SOMETHING>`", "the class is name AlgorithmSelector and the routine is call selectAlgorithm. The word tactic doesn't appear anywhere in the code. Also can we remove the word \"deterministic\" from the env variable and use \"static\" or \"fixed\" instead?  The user is specifying a fix ID for the algorithm.", "Regarding the prefix for the env, I think we should fix things to make them all use prefix TF_TRT instead. See the newer env variable TF_TRT_OP_DENYLIST.", "It doesn't look like that you saw my comments, can you check them and response:\r\nhttps://github.com/tensorflow/tensorflow/pull/45122#issuecomment-733220833\r\nhttps://github.com/tensorflow/tensorflow/pull/45122#issuecomment-733221636\r\n"]}, {"number": 45121, "title": "Implementations 2/3 of LocallyConnected1D layer don't work with saved_model.save()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.3.0\r\n- Python version:\r\n3.6 \r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.\r\n\r\n**Describe the expected behavior**\r\nSuccessfully saving the model (as with implementation=1)\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ninput = tf.keras.Input(\r\n            shape=(10,1),\r\n)\r\noutput = tf.keras.layers.LocallyConnected1D(\r\n            1,\r\n            5,\r\n            5,\r\n            implementation=3,  # implementation=1 does not cause error\r\n)(input)\r\nmodel = tf.keras.Model(\r\n            inputs=input,\r\n            outputs=output,\r\n)\r\ntf.saved_model.save(model, \"tmp\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sgdre217 \r\nI ran the code shared on tf nightly and [tf 2.3](https://colab.research.google.com/gist/Saduf2019/8ab958a6790d5df7559b3ac0329f384c/untitled469.ipynb), i do not see any error on nightly please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5c2a17183155bdd13381e01282456350/untitled468.ipynb).", "While I don't have access to your gist, I have independently confirmed that the error does not occur on tf nightly. Thanks!", "@sgdre217 \r\nAccess has been granted to the gist, please move the issue to closed status as it is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45121\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45121\">No</a>\n", "I am reopening this issue as the bug has persisted through TF version 2.5.0. Additionally, it seems like the bug is now present in tf-nightly. Here is a collab (note that it seems like the one @Saduf2019 linked has since been edited): https://colab.research.google.com/drive/1YUrOJautQiBZASDNZ97jIHlxCIsd9NHV#scrollTo=NhL5OdF3tx4z", "I am able to replicate this issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bb7cb39157ff14d9cbc33a51480c8ed2/untitled605.ipynb). Thanks!", "We have already two tickets and a PR https://github.com/tensorflow/tensorflow/pull/49230", "@Saduf2019 , you can close the issue because we have exact same 2 tickets as @bhack mentioned. You can subscribe to these 2 tickets although so that you can get updates.\r\n\r\nAttaching the 2 tickets: #48584 and #47689 .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45121\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45121\">No</a>\n"]}, {"number": 45120, "title": "Missing wheels tagged for macosx_11_x", "body": "Current wheels are built for `macosx_10_9_*` .e.g `tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl`. Pip attempts to build a list of compat tags that includes previous minor releases of macOS, however macOS Big Sur is a major release, which prevents those wheels from being installed:\r\n\r\n```\r\n$ pip install ~/Downloads/tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl\r\nERROR: tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl is not a supported wheel on this platform.\r\n```\r\n\r\nSolution: since the package itself is compatible for both macOS 10 and 11, the tagging needs to include both 10_x and 11_x major versions. Following installs without issues on Big Sur.\r\n```\r\n$ pip install ~/Downloads/tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.macosx_11_0_x86_64.whl\r\n```\r\n\r\nList of wheels for TensorFlow 2.3.1: https://pypi.org/project/tensorflow/2.3.1/#files", "comments": ["So this is just a wheel rename issue?", "Ultimately yes, but we want to change how `bdist` is generated so we don't have to rename manually.", "@dgladkov ,\r\nIs this issue still persists?Can you please check and confirm the same or try the latest version.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45120\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45120\">No</a>\n"]}, {"number": 45119, "title": "Tflite Model gives same output for every input", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook. [Link for my Google Colab Notebook](https://colab.research.google.com/drive/1f0394NZJ-Jjt1bJ6eILTS1qH6hamvkEr?usp=sharing)\r\n\r\n```\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('/content/drive/MyDrive/model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\nMy goal is to develop an app that does image classification. Keras model seems to work good but after i try to convert to Tensorflow Lite, output is always the same.\r\n\r\n**Code that I use to use to feed the tflite model random data**\r\n```\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"/content/drive/MyDrive/my_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\nThe output of the model is always something like this\r\n```\r\n[[ 1.5353261   0.5791437  -0.00934554 -1.5525526 ]]\r\n```\r\nKeras model seems to classify correct the images but I can't understand what is wrong after the conversion.\r\n\r\nMany thanks in advance!\r\n\r\n", "comments": ["@p17bail,\r\nOn running the Colab notebook you have linked I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/final_dataset'`. \r\n\r\nIn order to reproduce the issue, could you please share the contents of the `final_dataset` folder with us. Thanks!", "@amahendrakar  \r\nThis is the [Link](https://drive.google.com/drive/folders/1eW63OhZARj0I4YUAdmgUwfBdc67HgWR4?usp=sharing) for my dataset in google drive. Find the file in shared with me and then choose add shortcut to my drive so you can have access from Google Colab.\r\n\r\nThanks for your time.\r\n\r\n", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/8e8eb6ad6c5c038b377bff044853d31d/45119.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/7994b4a31ae2ca59051888ad36d673d7/45119-tf-nightly.ipynb). Pleas find the attached gist. Thanks!", " You need to resize your input image size to be compatible with the model image sizes prior to inference.\r\nTo know more see https://stackoverflow.com/a/63312051/11127923", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45119\">No</a>\n"]}, {"number": 45118, "title": "[INTEL MKL]  Added mkl_einsum op", "body": "Added mkl_einsum op Implementation, In addition to   adding mkl_einsum, It also refactors mkl_batch_matmul op.", "comments": ["@vishakha-nervana  Can you please check @kkimdev's comments and keep us posted ? Thanks!", "Also, please wait for @penpornk 's review.", "@penpornk Please review my PR.I have acknowledged changes.", "@penpornk I have made changes as per your request, please let me know if there's any AR for me regarding refactoring.", "@vishakha-nervana Can you please resolve conflicts? Thanks!", "@vishakha-nervana Any update on this PR? Please. Thanks!", "> @vishakha-nervana Any update on this PR? Please. Thanks!\r\n\r\nI have resolved all the conflicts long ago, @penpornk Is there any change I need to do in this branch?", "@vishakha-nervana Can you please address Ubuntu Sanity errors? Thanks!", "> @vishakha-nervana Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\n@gbaned Done."]}, {"number": 45117, "title": "INTEL MKL Added mkl_einsum_op implementation", "body": "", "comments": []}, {"number": 45116, "title": "[CherryPick:r2.4][tf.data] Making sure setting tf.data threading options does not prevent auto-sharding from using file-level sharding.", "body": "PiperOrigin-RevId: 343746154\nChange-Id: I241e0c951c46086e9f043588ddb3aa0eff23e593", "comments": []}]