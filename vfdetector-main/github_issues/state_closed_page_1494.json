[{"number": 8118, "title": "Setup instructions link broken", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md\r\n\r\nThe above link is broken. Loading leads to a 404 error.", "comments": ["This is being addressed by #8111 and has been already [reported](https://github.com/tensorflow/tensorflow/issues/8071). ", "Closing as duplicate of #8111"]}, {"number": 8117, "title": "could not find cuDevicePrimaryCtxSetFlags", "body": "when i execute sess = tf.Session() or sess = tf.InteractiveSession(), it shows problems below: \r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nF tensorflow/stream_executor/cuda/cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: /usr/lib/x86_64-linux-gnu/libcuda.so.1: undefined symbol: cuDevicePrimaryCtxSetFlags\r\nAborted (core dumped)\r\nhow can i fix this , thank you !", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Similar crash when creating session.\r\n\r\nOS: windows 8.1 x64\r\nCUDA: 8.0\r\ncuDNN: 5.1\r\nGPU: EN8600GT (8600 GTS)\r\n\r\nOutput:\r\n\r\nC:\\...>python\r\nPython 3.5.1 (v3.5.1:37a07cee5969, Dec  6 2015, 01:54:25) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll\r\nlocally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll lo\r\ncally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll l\r\nocally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll local\r\nly\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll\r\nlocally\r\n>>> sess = tf.Session()\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:94] Check failed: s.ok() could not find cuDevicePrimaryCtxSetFlags in libcuda DSO; dlerror: cuDevicePrimaryCtxSetFlags not found\r\n"]}, {"number": 8116, "title": "Errno 110 occured when running all python files in /examples/tutorials/mnist/ directory.", "body": "davidtest@CaffeVM:~/tensorflow/tensorflow/tensorflow/examples/tutorials/mnist$ python fully_connected_feed.py \r\nTraceback (most recent call last):\r\n  File \"fully_connected_feed.py\", line 277, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"fully_connected_feed.py\", line 222, in main\r\n    run_training()\r\n  File \"fully_connected_feed.py\", line 120, in run_training\r\n    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 211, in read_data_sets\r\n    SOURCE_URL + TRAIN_IMAGES)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 208, in maybe_download\r\n    temp_file_name, _ = urlretrieve_with_retry(source_url)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 165, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 190, in urlretrieve_with_retry\r\n    return urllib.request.urlretrieve(url, filename)\r\n  File \"/usr/lib/python2.7/urllib.py\", line 94, in urlretrieve\r\n    return _urlopener.retrieve(url, filename, reporthook, data)\r\n  File \"/usr/lib/python2.7/urllib.py\", line 240, in retrieve\r\n    fp = self.open(url, data)\r\n  File \"/usr/lib/python2.7/urllib.py\", line 208, in open\r\n    return getattr(self, name)(url)\r\n  File \"/usr/lib/python2.7/urllib.py\", line 345, in open_http\r\n    h.endheaders(data)\r\n  File \"/usr/lib/python2.7/httplib.py\", line 1013, in endheaders\r\n    self._send_output(message_body)\r\n  File \"/usr/lib/python2.7/httplib.py\", line 864, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python2.7/httplib.py\", line 826, in send\r\n    self.connect()\r\n  File \"/usr/lib/python2.7/httplib.py\", line 807, in connect\r\n    self.timeout, self.source_address)\r\n  File \"/usr/lib/python2.7/socket.py\", line 571, in create_connection\r\n    raise err\r\n", "comments": ["Source website http://yann.lecun.com is down that why you get the issue.\r\n\r\nYou can download dataset via _archive machine_ https://web.archive.org/web/20160828233817/http://yann.lecun.com/exdb/mnist/index.html and put it on corresponding log folder", "Closing as duplicate of #8126"]}, {"number": 8115, "title": "Use vector constructor to initialize Array2D.", "body": "  Current constructor of Array2D calls resize to initialize the inner vector of\r\n  Array2D and follows up with a Fill call. This can be replaced with a simple\r\n  call to vector constructor. Vector constructor might also be more efficient.", "comments": ["Can one of the admins verify this patch?", "@hawkinsp please review.", "@hawkinsp Thanks for the review!", "@hawkinsp will create a PR for `Array3d` to address the same issue.", "@tensorflow-jenkins Test this please"]}, {"number": 8114, "title": "image_retrainning can not run", "body": "my tensorflow  version is 1.0.0\r\nI follow the image_retraining.md\r\n1.    curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\r\ntar xzf flower_photos.tgz\r\n2.   bazel build --config opt tensorflow/examples/image_retraining:retrain\r\n\r\nbut when I run\r\n bazel-bin/tensorflow/examples/image_retraining/retrain\r\nthere is a errorLast login: Mon Mar  6 14:46:11 on ttys000\r\nwangjingsideMacBook-Pro:~ ginthva$ cd tensorflow\r\nwangjingsideMacBook-Pro:tensorflow ginthva$ bazel build --config opt tensorflow/examples/image_retraining:retrain\r\nINFO: Found 1 target...\r\nTarget //tensorflow/examples/image_retraining:retrain up-to-date:\r\n  bazel-bin/tensorflow/examples/image_retraining/retrain\r\nINFO: Elapsed time: 0.374s, Critical Path: 0.00s\r\nwangjingsideMacBook-Pro:tensorflow ginthva$ bazel-bin/tensorflow/examples/image_retraining/retrain \r\nTraceback (most recent call last):\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 79, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 124, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/Users/ginthva/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 50, in <module>\r\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\r\nImportError: No module named mock\r\n\r\n\r\nI also uninstall mock and reinstall, but it doesn't work. what should I  do?", "comments": ["Please provide details about what platform you are using  (operating system, architecure). Also please include details of how you installed TensorFlow. We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "It's so happy to receive your response, thank you very     @prb12 \r\nMy system is macOS Sierra 10.12.\r\n3,2.9 GHz Intel Core i5,\r\n8 GB 2133 MHz LPDDR3\r\nIntel Iris Graphics 550 1536 MB\r\nI install tensorflow according  to install_sources.md ", "My system is macOS Sierra 10.12.\r\nPython 2.7.13\r\nGCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)\r\nbazel release 0.4.4-homebrew\r\n\r\nwangjingsideMacBook-Pro:tensorflow ginthva$ pip install mock\r\nRequirement already satisfied: mock in /usr/local/lib/python2.7/site-packages\r\nRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/site-packages (from mock)\r\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/site-packages (from mock)\r\nRequirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from mock)", "The symptoms seem related to the problems in #7980.  Could you please see if the solution in that thread works for you?\r\n", "Having the same issues, have reinstalled tensorflow and pip, still getting same error?", "Same issue here.\r\n```\r\nbleach (1.5.0)\r\nfuncsigs (1.0.2)\r\nhtml5lib (0.9999999)\r\nMarkdown (2.2.0)\r\nmock (2.0.0)\r\nnumpy (1.12.1)\r\npbr (3.0.1)\r\npip (9.0.1)\r\nprotobuf (3.3.0)\r\nsetuptools (36.0.1)\r\nsix (1.10.0)\r\ntensorflow (1.2.0rc0)\r\nWerkzeug (0.12.2)\r\nwheel (0.29.0)\r\n```\r\nStill tell me ``` ImportError: No module named mock ```\r\n\r\nI also tried the methods in #7980 , none is working for me.\r\nMy system info : \r\n```\r\nmacOS Sierra 10.12.4\r\nPython 2.7.13\r\npip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7)\r\n```\r\n\r\nAnd if I import ```mock``` in some other python file, it's OK."]}, {"number": 8113, "title": "I can not run my code Tensorflow", "body": "I working on `Ubuntu 14.04` ,i wrote a code for Recognition of letters whith `Tensorflow V 0.11` ,\r\ni'm creat a code source for uses the model `LeNet5` \r\nmy code source : \r\n\r\n`\r\n\r\n> \r\n>     import PIL\r\n>     \r\n>     import numpy\r\n>     import tensorflow as tf\r\n>     # from tensorflow.examples.tutorials.mnist import input_data\r\n>     import Input as input_data\r\n>     from tensorflow.python.framework.importer import import_graph_def\r\n>     \r\n>     from Resize import Resize_img\r\n>     \r\n>     # these functions to optimize the accurancy of the mnist training\r\n>     #from imp_image import imp_img\r\n>     import scipy.misc\r\n>     \r\n>     \r\n>     def weight_variable(shape):\r\n>         initial = tf.truncated_normal(shape, stddev=0.1)\r\n>         return tf.Variable(initial)\r\n>     \r\n>     \r\n>     def bias_variable(shape):\r\n>         initial = tf.constant(0.1, shape=shape)\r\n>         return tf.Variable(initial)\r\n>     \r\n>     \r\n>     def conv2d(x, W):\r\n>         return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n>     \r\n>     \r\n>     def max_pool_2x2(x):\r\n>         return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n>     \r\n>     \r\n>     # ============================================================ End Functions part\r\n>     \r\n>     # mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n>     \r\n>     class MNIST:\r\n>     \r\n>         def __init__(self):\r\n>     \r\n>             # Open the compuation session\r\n>             self.sess = tf.InteractiveSession()\r\n>             # Load the network\r\n>             self.Deep_Network()\r\n>     \r\n>         def Deep_Network(self):\r\n>     \r\n>             # nodes for the input images and target output classes.\r\n>             # supervised classifier\r\n>             self.x = tf.placeholder(tf.float32, shape=[None, 784])\r\n>             self.y_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n>     \r\n>             # First convolutionanal Layer =====================================\r\n>             # It will consist of convolution, followed by max pooling\r\n>             # The convolutional will compute 32 features for each 5x5 patch.\r\n>             self.W_conv1 = weight_variable([5, 5, 1, 32])\r\n>             self.b_conv1 = bias_variable([32])\r\n>     \r\n>             # To apply the layer, we first reshape x to a 4d tensor,\r\n>             #  with the second and third dimensions corresponding to image width and height,\r\n>             #  and the final dimension corresponding to the number of color channels.\r\n>             self.x_image = tf.reshape(self.x, [-1, 28, 28, 1])\r\n>     \r\n>             # We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\r\n>             self.h_conv1 = tf.nn.relu(conv2d(self.x_image, self.W_conv1) + self.b_conv1)\r\n>             self.h_pool1 = max_pool_2x2(self.h_conv1)\r\n>     \r\n>             # Second Convolutional Layer =====================================\r\n>     \r\n>             # In order to build a deep network, we stack several layers of this type.\r\n>             # The second layer will have 64 features for each 5x5 patch.\r\n>     \r\n>             self.W_conv2 = weight_variable([5, 5, 32, 64])\r\n>             self.b_conv2 = bias_variable([64])\r\n>     \r\n>             self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)\r\n>             self.h_pool2 = max_pool_2x2(self.h_conv2)\r\n>     \r\n>             # Densely Connected Layer\r\n>     \r\n>             # Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons\r\n>             # to allow processing on the entire image. We reshape the tensor from the pooling layer into\r\n>             # a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU.\r\n>     \r\n>             self.W_fc1 = weight_variable([7 * 7 * 64, 1024])\r\n>             self.b_fc1 = bias_variable([1024])\r\n>     \r\n>             self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, 7 * 7 * 64])\r\n>             self.h_fc1 = tf.nn.relu(\r\n>                 tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)  # ReLu Computes rectified linear: max(features, 0).\r\n>     \r\n>             # Dropout\r\n>     \r\n>             self.keep_prob = tf.placeholder(tf.float32)\r\n>             self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)\r\n>     \r\n>             # Readout Layer ========================================\r\n>             # Finally, we add a softmax layer, just like for the one layer softmax regression above.\r\n>     \r\n>             self.W_fc2 = weight_variable([1024, 10])\r\n>             self.b_fc2 = bias_variable([10])\r\n>     \r\n>             self.y_conv = tf.nn.softmax(tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2)\r\n>             self.cross_entropy = -tf.reduce_sum(self.y_ * tf.log(self.y_conv))\r\n>             self.correct_prediction = tf.equal(tf.argmax(self.y_conv, 1), tf.argmax(self.y_, 1))\r\n>             self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\r\n>     \r\n>         def Prediction(self, imageName):\r\n>     \r\n>             # Load the trained model\r\n>             ' Restore the model '\r\n>             'here i should create the model saver'\r\n>             Saved_model_dir = '/home/brm17/Desktop/PFE/'\r\n>             saver = tf.train.Saver()\r\n>             ckpt = tf.train.get_checkpoint_state(Saved_model_dir)\r\n>     \r\n>             'verifie if the saved model exists or not!'\r\n>             if ckpt and ckpt.model_checkpoint_path:\r\n>                 saver.restore(self.sess, ckpt.model_checkpoint_path)\r\n>             else:\r\n>                 print '# No saved model found!'\r\n>                 exit()  # exit the prgm\r\n>     \r\n>             # image_test = 'number-3.jpg'\r\n>             ResizedImage = Resize_img(imageName)\r\n>     \r\n>             ImageInput = ResizedImage.mnist_image_input.reshape(1, -1)\r\n>     \r\n>             print 'Predection > ', tf.argmax(self.y_conv, 1).eval(feed_dict={self.x: ImageInput, self.keep_prob: 1.0})\r\n>     \r\n>         # print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: myTestImg, y_: myLabel, keep_prob: 1.0}))\r\n>     \r\n>     \r\n>     def main():\r\n>         image = '/home/brm17/Desktop/PFE/n2.jpeg'\r\n>         model = MNIST()\r\n>         model.Prediction(image)\r\n>     \r\n>     if __name__ == \"__main__\":\r\n>         main()\r\n>     \r\n\r\n    `\r\n    \r\nif i run this code , he print the error :\r\n\r\n>     brm17@Brahim:~/Desktop/PFE$ python LeNet5.py \r\n>     Traceback (most recent call last):\r\n>       File \"LeNet5.py\", line 137, in <module>\r\n>         model.Prediction(image)\r\n>       File \"LeNet5.py\", line 120, in Prediction\r\n>         saver.restore(self.sess, ckpt.model_checkpoint_path)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1129, in restore\r\n>         {self.saver_def.filename_tensor_name: save_path})\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\r\n>         run_metadata_ptr)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\r\n>         feed_dict_string, options, run_metadata)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\r\n>         target_list, options, run_metadata)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\r\n>         raise type(e)(node_def, op, message)\r\n>     tensorflow.python.framework.errors.NotFoundError: Tensor name \"Variable_1\" not found in checkpoint files /home/brm17/Desktop/PFE/MNISTmodel-20000\r\n>     \t [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]\r\n>     Caused by op u'save/restore_slice_1', defined at:\r\n>       File \"LeNet5.py\", line 137, in <module>\r\n>         model.Prediction(image)\r\n>       File \"LeNet5.py\", line 115, in Prediction\r\n>         saver = tf.train.Saver()\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 861, in __init__\r\n>         restore_sequentially=restore_sequentially)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 519, in build\r\n>         filename_tensor, vars_to_save, restore_sequentially, reshape)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\r\n>         values = self.restore_op(filename_tensor, vs, preferred_shard)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\r\n>         preferred_shard=preferred_shard)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\r\n>         preferred_shard, name=name)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\r\n>         preferred_shard=preferred_shard, name=name)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\r\n>         op_def=op_def)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\r\n>         original_op=self._default_original_op, op_def=op_def)\r\n>       File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\r\n>         self._traceback = _extract_stack()\r\n>     \r\n>      \r\n> \r\n\r\nwhat is the problem and how resolved this ? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nWe don't have time to debug everybody's TensorFlow program, but from a superficial glance at your error it looks like you are trying to restore a checkpoint from a different version of your program and the names of variables in the graph are now different.  Perhaps worth deleting the old checkpoint and re-running the code?"]}, {"number": 8112, "title": "tf_upgrade doesn't update RNN cells", "body": "Migration script `tf_upgrade.py` does not update RNN cell locations from `tf.nn.rnn_cell` to `tf.contrib.rnn` leading to error: `AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'`\r\n\r\nUpgrading\r\n```python\r\nimport tensorflow as tf\r\n\r\nrnn = tf.nn.rnn_cell.GRUCell(128)\r\ntf.initialize_all_variables()  # Checking if tf_upgrade works at all\r\n```\r\nbecomes\r\n```python\r\nimport tensorflow as tf\r\n\r\nrnn = tf.nn.rnn_cell.GRUCell(128)\r\ntf.global_variables_initializer()  # Checking if tf_upgrade works at all\r\n```\r\nwhen it should be\r\n```python\r\nimport tensorflow as tf\r\n\r\nrnn = tf.contrib.rnn.GRUCell(128)\r\ntf.global_variables_initializer()  # Checking if tf_upgrade works at all\r\n```", "comments": ["@aselle just a gentle reminder. Unless we can close this @jaakkopasanen?", "Preparing a cl internally that will resolve this.", "@aselle  Hi, any update on this ?\r\n", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}, {"number": 8111, "title": "Fix broken links to g3doc/get_started/os_setup.md", "body": "", "comments": ["Can one of the admins verify this patch?", "Partial duplicate of https://github.com/tensorflow/tensorflow/pull/8169. Please merge and fix conflicts. ", "Fixed.", "Jenkins, test this please."]}, {"number": 8110, "title": "How to Implement a Secure, Multi-Tenant tensorflow?", "body": "I am using tensorflor as miner/ML platform, and now customers are required to support multi-tenant capabilities;\r\nBetween the tenants to achieve resource isolation (CPU, MEM, disk, etc.), security isolation.\r\nHow do i achieve it?\r\nHas the new version already planned for these features?", "comments": ["TensorFlow is basically a math library, multi-tenant capabilities should be enabled by whichever system you are using to run it (Docker/Kubernetes is a good combination IMHO)"]}, {"number": 8109, "title": "libxsmm build errors with Python 3", "body": "It seems like some dependencies are not Python 3 compatible.\r\n\r\nBuilding with `--define tensorflow_xsmm=1 --define tensorflow_xsmm_backward=1` gives following \r\n\r\ncc @benoitsteiner \r\n\r\n```\r\nERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:112:1: Co\r\nuldn't build file python3/external/libxsmm_archive/scripts/libxsmm_utilities.py:\r\n Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_utilities.py f\r\nailed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host\r\n/genfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (\r\nremaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitSta\r\ntusException: Process exited with status 1.                                     ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:130:1: Co\r\nuldn't build file python3/external/libxsmm_archive/scripts/libxsmm_dispatch.py:\r\nConverting to Python 3: external/libxsmm_archive/scripts/libxsmm_dispatch.py fai\r\nled: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/g\r\nenfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (re\r\nmaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatu\r\nsException: Process exited with status 1.                                       ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:112:1: Co\r\nuldn't build file python3/external/libxsmm_archive/scripts/libxsmm_specialized.p\r\ny: Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_specialized.\r\npy failed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/\r\nhost/genfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files .\r\n.. (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExi\r\ntStatusException: Process exited with status 1.                                 ERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:124:1: Co\r\nuldn't build file python3/external/libxsmm_archive/scripts/libxsmm_config.py: Co\r\nnverting to Python 3: external/libxsmm_archive/scripts/libxsmm_config.py failed:\r\n 2to3 failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfi\r\nles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nERROR: /local_home/yaroslav/bazel/external/libxsmm_archive/BUILD.bazel:118:1: Couldn't build file python3/external/libxsmm_archive/scripts/libxsmm_interface.py:\r\n Converting to Python 3: external/libxsmm_archive/scripts/libxsmm_interface.py f\r\nailed: 2to3 failed: error executing command bazel-out/host/bin/external/bazel_to\r\nols/tools/python/2to3 --no-diffs --nobackups --write --output-dir bazel-out/host/genfiles/python3/external/libxsmm_archive/scripts --write-unchanged-files ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitSta\r\ntusException: Process exited with status 1.\r\n```", "comments": ["Hi! Do you solve this problem?", "@tfboyd Could you comment on this?", "Some python targets are missing the srcs_version=\"PY2AND3\" annotation in the BUILD file.  In the meantime, any python target in python/BUILD without this annotation will fail when compiled with python 3. #791", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8108, "title": "unable to install tensor flow using pip.", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@jaideepsiddula Sorry you're having issues. In order to help you solve the problem it would be useful more information. For instance, operating system, what commands did you use and the log. Thanks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8107, "title": "import tf fails in \"imp.find_module('_pywrap_tensorflow_internal',\"", "body": "I get error below trying to import TensorFlow built at commit 7e9f2dc ([head](https://github.com/yaroslavvb/tensorflow/tree/dev) as of Sat 4).\r\n\r\nI haven't made any changes to my [build procedure](https://github.com/yaroslavvb/tensorflow-community-wheels) in last month, and installing similarly built wheel from Mar 1 works.\r\n\r\nLooking at relevant differences between Mar 1 and Mar 4 I see that @jhseu 's  718812c refactored Python module loading in https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/python/pywrap_tensorflow.py\r\n\r\nStack trace\r\n```\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/yaroslav/.conda/envs/tf-mar4/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["Same problem using today's head.\r\n\r\nLooking at the failure, it happens in `pywrap_tensorflow_internal.py` trying to load `__pywrap_tensorflow_internal` module. I did a search and there's no file called `_pywrap_tensorflow_internal` that was included in wheel installation\r\n\r\nThe problem is in this block of `pywrap_tensorflow_internal.py`\r\n```\r\n      try:\r\n            fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n\r\n```\r\nThis is the wheel that got built (Ubuntu 14.04, Python 3)\r\nhttps://storage.cloud.google.com/tensorflow-community-wheels/mar5.xla.tensorflow-1.0.0-cp35-cp35m-linux_x86_64.whl\r\n", "I'm having trouble reproducing this issue. Your .whl is indeed missing _pywrap_tensorflow_internal.so, but it exists when I create it in a fresh Docker container:\r\n```bash\r\n$ unzip tensorflow-1.0.0-cp35-cp35m-linux_x86_64.whl\r\n$ find . -name _pywrap_tensorflow_internal.so\r\n./tensorflow-1.0.0.data/purelib/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\nHave you tried on a freshly cloned tensorflow repo? If you're reusing an existing tree, it's possible it's picking up older artifacts.", "Do you get this file after the build?\r\n```bash\r\n$ ls -l bazel-out/local-py3-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so\r\n-r-xr-xr-x 1 user user 76380704 Mar  6 20:09 bazel-out/local-py3-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```", "Note: it gets copied for packaging here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/build_pip_package.sh#L118\r\n\r\nSo a symlink should exist at `bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so` after build.\r\n\r\nI verified that deleting that file still allows us to build the .whl package successfully since we don't do any explicit checks.", "OK, looking closer at logs, it looks like compilation failed because of a syntax error. So the wheel was getting built with an incomplete file set, probably left over from previous build. \r\n\r\nThe error is that one of the files has\r\n`size_typ i = 0;`  which should be `size_type i = 0;`\r\n\ud83d\ude2e \r\n\r\nIsn't this kind of thing supposed to get caught by continuous tests?\r\n\r\nSomeone else is trying to fix it in: https://github.com/tensorflow/tensorflow/pull/8039 \r\n", "I'll close this issue since there's already a PR in flight to fix it", "Yeah, I think the issue is that we disable XLA in our OSS tests since it's experimental.\r\n\r\nNot sure people would want to add a separate run just for XLA.", "Hi,\r\nI want to develop a sequence to sequence attention model using tensorflow. I am trying to install tensorflow from source. Hence following this tutorial https://www.tensorflow.org/tutorials/seq2seq which asks me to clone tensorflow from a git repo.\r\n\r\nI am trying to build through bazel and then installing the wheel using the pip. But I get the same error as mentioned above. Please help me by pointing to a stable, I am working on Ubuntu 16.04", "I have the same problem ad @345ishaan", "Note: if you're getting this error, then you're ignoring an error that occurred during the build step. Scroll up and find the underlying error :)", "This 1.0.0 build was such a pain! After way more time that I wanted to on this, I got something that works on MacOS.\r\n\r\n1.  First I had to deal with a Mac-specific issue where a protection scheme, SIP, prevents you from defining DYLD_ anything in env, so it could not find libcudart.8.0.dylib. This dude blogged a workaround - [https://srikanthpagadala.github.io/notes/2016/11/07/enable-gpu-support-for-tensorflow-on-macos](https://srikanthpagadala.github.io/notes/2016/11/07/enable-gpu-support-for-tensorflow-on-macos)\r\n\r\n2. Then I ran into an issue where it could not find another library, libcublas. Since I used Homebrew (as per all guidance everywhere) to get bazel, it got 0.4.4 which doesn't work with this source. Someone posted on this issue how to roll back to 0.4.3 - [https://github.com/tensorflow/tensorflow/issues/7227](https://github.com/tensorflow/tensorflow/issues/7227)\r\n\r\n3. Finally, the problem you are all having here. After finally getting the build and installing the wheel in my conda env, I ran into an error trying to find pywrap_tensorflow_internal.so when I ran the interpreter and tried to import tensorflow. Turns out the wheel had one error while being assembled. I looked at the conda env site packages to see if the .so is actually there. It was, but it was named _pywrap_tensorflow_internal.so (notice the initial underscore). I removed the underscore by hand and it finally worked. :/ This last bit might help you guys on Linux maybe?", "I have a fix for 1 where we the path to CUDA to the rpath of the shared library on macOS. I'll ping on the change. No clue how moving the file is fixing it for 3. It's intentionally named with the underscore.", "make sure you aren't getting this error as a result of running python from the actual git directory where you downloaded tensorflow. \r\n\r\nif you're following the step-by-step install from the documentation page, and get to the final step where you verify the install, you start python from the terminal and run 'import tensorflow', and you would get this error. go to any other directory and you're fine.", "@robmsylvester Thank you very much for point this out...would you mind sharing what might be the probable cause for this", "@eliethesaiyan In my case it tried to load the tensorflow library from the local tensorflow folder in the build directory instead of the global one. If that happens it should be obvious from the stack trace though.", "@robmsylvester @AimainaHito I tried to moving to other directory and even tried to load from idle but it's still not working.", "I'm outside of the tensorflow repo, and I'm still getting this issue when doing ```import tensorflow``` in the python interpreter:\r\n\r\n```ImportError: /usr/local/google/home/njonas/tensorflow/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuDevicePrimaryCtxRetain```\r\n\r\nMy CUDA paths are exported, and here are my system specs:\r\n\r\nUbuntu 14.04 x86-64\r\nIntel Xeon CPU E5-2690 @ 2.60 GHz x 12\r\nNVIDIA GM107GL Quadro K2200\r\nNVIDIA GM200 GTX TITAN X\r\n\r\nPython version 3.5.3\r\ntensorflow_gpu-1.2.1-cp35-cp35m-manylinux1_x86_64.whl\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\n", "Perhaps wrong CUDA version?\r\nThe errors means dynamic linker found a reference to `cuDevicePrimaryCtxRetain` in the `.so` library, but this symbol was not present in any other library it loaded, which could happen if your cuda .so's are too old"]}, {"number": 8106, "title": "Fix a bug of initializing", "body": "Prevent the exceptional illegal call of __initializer.\\_\\_init\\_\\_(shape, dtype, partition_info)__ appears when the param \"initializer\" is an init_ops class instead of its instance.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!\r\n\r\nThis bug is exposed when using tf/models/slim/train_image_classifier.py to run vgg-16.", "CLAs look good, thanks!\n\n<!-- ok -->", "> And the line actually means\r\n> isinstance(initializer, type(tensorflow.python.ops.init_ops.Initializer))\r\n\r\nIf that's what it means, then please code it like that. And add a clarifying comment in code so nobody has to trace it back to this discussion (in case someone wants to read or change this code even again).", "@tensorflow-jenkins test this please.", "Guys, can we get this in?", "@tensorflow-jenkins Test this please"]}, {"number": 8105, "title": "ValueError: '-_Loss/Const' is not a valid node name", "body": "I am trying to build a network using tflearn library and tensorflow v.1, and I got this error \r\n\r\n**error**\r\nScipy not supported!\r\n[+] Building CNN\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 491, in apply_op\r\n    preferred_dtype=default_dtype)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 169, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1220, in __init__\r\n    raise ValueError(\"'%s' is not a valid node name\" % node_def.name)\r\nValueError: '-_Loss/tags' is not a valid node name\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"emotion_recognition.py\", line 101, in <module>\r\n    import poc\r\n  File \"C:\\Users\\Mohamed\\Desktop\\Capstone Project\\emotion-recognition-neural-networks-master\\poc.py\", line 46, in <module>\r\n    network.build_network()\r\n  File \"C:\\Users\\Mohamed\\Desktop\\Capstone Project\\emotion-recognition-neural-networks-master\\emotion_recognition.py\", line 42, in build_network\r\n    tensorboard_verbose = 2\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tflearn\\models\\dnn.py\", line 57, in __init__\r\n    session=session)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 111, in __init__\r\n    clip_gradients)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tflearn\\helpers\\trainer.py\", line 561, in initialize_training_ops\r\n    ema_num_updates=self.training_steps)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tflearn\\summaries.py\", line 243, in add_loss_summaries\r\n    summaries_collection_key)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tflearn\\summaries.py\", line 46, in get_summary\r\n    summ = tf.summary.scalar(tag, value)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\summary\\summary.py\", line 120, in scalar\r\n    tags=scope.rstrip('/'), values=tensor, name=scope)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 281, in _scalar_summary\r\n    name=name)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 504, in apply_op\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 169, in constant\r\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1220, in __init__\r\n    raise ValueError(\"'%s' is not a valid node name\" % node_def.name)\r\nValueError: '-_Loss/Const' is not a valid node name", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include details of how you installed TensorFlow and the command lines necessary to reproduce your problem. We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "OS: windows 10 64 bit\r\ninstalled tensorflow using pip\r\nI think the code I am trying to run is incomplete.", "> I think the code I am trying to run is incomplete.\r\n\r\nCould you please explain this comment?  ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8104, "title": "Fix broken links in documentation adding_an_op.md `g3doc/how_tos` -> `examples`", "body": "This fix fixes broken links in documentation adding_an_op.md with\r\n```\r\ns/g3doc\\/how_tos/examples/g\r\n```\r\n\r\nThis fix fixes #8044.\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Test failures are unrelated. Merging PR. Thanks for the fixes, @yongtang !"]}, {"number": 8103, "title": "bmp format", "body": "If I use opencv to read a bmp image into a mat. Can I feed it to a tensor? Or I have to comvert it to jpg or png?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8102, "title": "Scatter_nd doc not clear about concurrent updates", "body": "The doc on the scatter_nd does not specify the consequence of multiple updates that reference the same location. \r\n\r\nI've tested this using the following code:\r\n\r\n```python\r\nindices = tf.constant([[4], [3], [1], [1]])\r\nupdates = tf.constant([9, 10, 11, 12])\r\nshape = tf.constant([8])\r\nscatter = tf.scatter_nd(indices, updates, shape)\r\nwith tf.Session() as sess:\r\n  print sess.run(scatter)\r\n```\r\nThe resulting tensor is:\r\n```python\r\n[0, 23, 0, 10, 9, 0, 0, 0]\r\n```\r\nSo it seems the updates are added. Is this the intended usage? If so, it would be great to clarify this in the docs. \r\n\r\nThanks!", "comments": ["The CPU/GPU implementation initializes to zero and uses [add](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L189) rather than assign.\r\n\r\nIt seems like a somewhat stronger promise than assign, where the output would be\r\n```\r\n[0, 12, 0, 10, 9, 0, 0, 0] or [0, 11, 0, 10, 9, 0, 0, 0]\r\n```\r\nnon-deterministically, or in an order that we define.\r\n\r\n@rmlarsen for opinion\r\n", "@vrv the document is still confusing for this function.\r\n\r\n> WARNING: The order in which updates are applied is nondeterministic, so the output will be nondeterministic if indices contains duplicates.\r\n\r\nEven if order is nondeterministic, the summation is deterministic. Why not just promise that duplicates will be summed?", " I didn't write the code or the doc, so I'm not the best person to clarify this, but maybe because order of operations for floating point values matter that it's still technically a non-deterministic result.  See \"catastrophic cancellation\" for cases where addition order matters.", "Also, I think the language implies all updates will be attempted in some order, but feel free to send a PR to update the doc to clarify that point, if you think it's useful.", "OK. Thanks.", "Is there a way to change add to update?", "If you are in the unpooling business:\r\n\r\n@teramototoya what I did as a hack: with https://www.tensorflow.org/api_docs/python/tf/unique_with_counts i counted the multiplication in the indices and i divided the tensor which was holding the values (aka tensor named 'updates' in the first comment)  with this counter\r\nso when add() comes it will undo what the division made\r\n\r\nyou can check this simple script: https://github.com/csnemes2/conv-net-viz/blob/master/ut_unpool.py\r\n\r\nIf not, so your problem is general, then you have to somehow flatten your indices, and then tf.unique, see this post:\r\nhttps://stackoverflow.com/questions/44117430/how-to-use-tf-scatter-nd-without-accumulation", "@csnemes2 I also tried the same implementation as your hack, but I could not do it well. It worked well with your implementation! Thank you!"]}, {"number": 8101, "title": "tf.nn.moments produces NaNs with axes=[-1]", "body": "Hi,\r\n\r\ntf.nn.moments does not work with negative axes. Is this intentional? Will this be possible in future versions? Thanks.\r\n\r\n```\r\n>>> x = tf.placeholder(dtype=tf.float32)\r\n>>> mean, var = tf.nn.moments(x, axes=[1])\r\n>>> sess.run([mean, var], {x:np.arange(12).reshape((2,6)).astype(np.float32)})\r\n[array([ 2.5,  8.5], dtype=float32), array([ 2.91666675,  2.91666675], dtype=float32)]\r\n>>> mean, var = tf.nn.moments(x, axes=[-1])\r\n>>> sess.run([mean, var], {x:np.arange(12).reshape((2,6)).astype(np.float32)})\r\n[array([ nan,  nan], dtype=float32), array([ nan,  nan], dtype=float32)]\r\n```\r\n", "comments": ["I don't see anything in the documentation which suggests that tf.nn.moments should accept axes <0.  To be clear, you would like axis -N to refer to dimension rank-N?", "Yes, I was hoping -1 would refer to the last axis (and rank-N in general). I agree the documentation does not mention this. However for functions like reduce_sum, it is not mentioned explicitly either (except for the \"Equivalent to np.sum\" note) but they do support negative axes. \r\n\r\nIt is quite tricky to remember which functions support negative axes and which don't and if you get it wrong you get NaNs. Maybe a simple 'assert all axes are non-negative' might prevent a lot of time debugging NaNs in the future.  \r\n\r\n", "@jozef-mokry Yes, it does seem reasonable that the axes argument should be validated in this case. Are you running on GPU or CPU?", "I ran this on a GPU.", "@drpngx Any update on this?", "I tried with the latest master and it seems the issue has been fixed:\r\n```\r\nubuntu@ip-172-31-28-246:~$ python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n2017-09-01 19:55:56.968431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-01 19:55:56.968903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-09-01 19:55:56.968928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n>>> \r\n>>> x = tf.placeholder(dtype=tf.float32)\r\n>>> mean, var = tf.nn.moments(x, axes=[1])\r\n>>> sess.run([mean, var], {x:np.arange(12).reshape((2,6)).astype(np.float32)})\r\n[array([ 2.5,  8.5], dtype=float32), array([ 2.91666675,  2.91666675], dtype=float32)]\r\n>>> mean, var = tf.nn.moments(x, axes=[-1])\r\n>>> sess.run([mean, var], {x:np.arange(12).reshape((2,6)).astype(np.float32)})\r\n[array([ 2.5,  8.5], dtype=float32), array([ 2.91666675,  2.91666675], dtype=float32)]\r\n>>> \r\n```", "Thank you for reporting back!"]}, {"number": 8100, "title": "XLA Design :: Incorporate Polyhedral Compilation (through LLVM Polly).", "body": "Polyhedral Compilation is a method of modeling iterations of loop nests into points on a multidimensional space (Determined by the loop nest depth). A detailed description can be found on the website as given [here](http://polyhedral.info/).\r\n\r\nIs it possible to pass the deeply nested kernels (for-loops) modeled in TensorFlow via XLA-JIT (or independently) through [Polly](http://polly.llvm.org/) that will extract dependence information and perform scheduling using Polyhedral compilation? Tensorflow Ops can be reproduced and modeled to basic math operators and can be applied to points on the Integer Polyhedra. \r\n\r\nThis can lead to significant speedup in program execution. However, since Polyhedral compilation can be expensive, it can lead to increase in compile time. (Trade-off in compile time)\r\n\r\nI am a student working on Polyhedral Compilation and would love to contribute to this if it spans out!", "comments": ["We have open-sourced the XLA compiler precisely so that it is possible for external contributors and researchers to experiment with this sort of idea.   \r\n\r\n@eliben @learyg May be able to comment further.\r\n", "@annanay25 XLA is exactly the kind of compiler that could greatly benefit form polyhedral optimizations. I would say certainly try it! You could fork the project and try applying Polly in your fork and see the results you get. @jpienaar for more insights", "@eliben @prb12 Thanks for the replies! \r\n\r\nI was trying to put together a compilation workflow to incorporate Polly in XLA. The glitch being that we need the Graph composed by XLA (the entire computation graph of the program and not a subgraph) and pass this to Polly - in a way that it can extract the SCoPs properly. The reason for this is because Polly can then be used to schedule the entire program at one go, which could otherwise be expensive if performed on each subgraph.\r\n\r\nIt was not very obvious to me as to how I could get this done. Any help on this would be great!\r\n\r\nCouldn't be happier to see XLA Open Source! ", "Definitely would be good to try XLA with Polly. Forking and modifying the CPU/GPU compiler backend might be the easiest. The [CG]PuCompiler's Compile function takes as input the HloModule with all the HloComputations. From there you'd have access to all the graphs translated from TF to XLA and can perform the optimizations you wish. You could look at entire graphs or just operations within the graph (for example, look at IrEmitter). The requirement of the entire graph is more of an optimization (large input graph, better return on optimization) requirement than a correctness one (correct me if wrong).\r\n\r\nI think there are multiple parts/steps here: 1) being able to use Polly in a backend, 2) quantifying performance gains possible from using Polly vs the regular backend, 3) improving the compile time performance. As a note the CPU backend is not the most optimized, so for performance evaluations the GPU backend is a better target.", "I'm not in the Polyhedral field but I'm interested to follow this experiment. Do you know if there is any  different impact here between Polly and Apollo approach? See also the integration plan in [Polly Gsoc 2017](https://www.pollylabs.org/gsoc2017.html).", "@jpienaar Thanks for those amazing pointers! \r\n\r\nYes, I believe that passing the entire computation graph through XLA will give Polly entire program to optimize which is a favorable optimization scenario.\r\n\r\nWill begin experimenting with the same.\r\n\r\n", "For starters, I inserted ```LOG(INFO) << llvm_ir::DumpModuleToString(*llvm_module)``` in ```CpuCompiler::Compile()``` and obtained the LLVM IR. But this seems to be just a small fraction of the complete program. \r\n\r\nI need some help regarding the same. IMO, the bigger picture here is to implement Polly as a module within the existing XLA backend. \r\n\r\nAnother thing that concerns me is the compilation overhead of using Polyhedral compilation in JIT (compilation at runtime) which is not very reassuring. ( I may be wrong about this ).\r\n\r\n@jpienaar @eliben @learyg , any help would be great!", "IIUC, currently the XLA backends are not implemented in a way that using Polly would provide much value. -- convolution and gemm are implemented though LLVM calls into pre-compiled Eigen code, which is to say LLVM at this level has no visibility into the low level loops (e.g., https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/ir_emitter.cc#L846). Moving beyond this would require integrating clang into XLA or implementing a new XLA backend that emits convolutions in LLVM IR.", "@b33pr Yes, I understand, I am willing to tweak XLA's IrEmitter to make the visibility clear - to help Polly optimize the loop nests further. ", "I will point out there are LLVM IR emitters for Dot and Conv, e.g.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/dot_op_emitter.h\r\nand IrEmitter::HandleConvolution in ir_emitter.h:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/ir_emitter.cc\r\nThey are reference implementations and not tuned for performance, which is why Eigen is used in practice. However, they may make a good starting point for something like Polly.", "@hawkinsp Can I ask you what is the roadmap of IrEmitters effort and if this will substitute eigen template code? ", "We [started a disscussion](https://github.com/soumith/convnet-benchmarks/issues/93#issuecomment-198745652) also one year ago on how much progress could be achived targetting IR /cc @scott-gray", "I don't think anyone is actively working on the IR emitter versions of dot and conv. Contributions welcome! I think it would be a fun project!\r\n\r\nIf the dot and conv IR emitters were more competitive, it would open new optimization opportunities (e.g., fusions into the dot kernels). But it is not easy to match the performance of hand-tuned dot and conv kernels via LLVM. It seems more tractable to do so on CPU rather than GPU, due to the limitations like the lack of control over register allocation in NVidia's PTX.\r\n\r\n(In the short term, we are probably going to look into replacing or augmenting Eigen with libxsmm or MKL for dot and conv on CPU.)", "@hawkinsp It could be really useful if HLO will become a standard rappresentstion also for other frameworks. But then the compiler need to be a standalone project.", "@bhack The XLA compiler is open-source, and everyone is welcome to use it and improve it. We mean it! Even though it lives in the Tensorflow repository, XLA is quite modular and does not depend on more than a few low level C++ utility libraries from Tensorflow. You can build and use XLA without the rest of Tensorflow right now if you like. It's still early days, and we are open to ideas on how to evolve XLA.\r\n\r\nWe haven't publicized this widely yet, but there is a new mailing list for XLA development:\r\nhttps://groups.google.com/forum/#!forum/xla-dev\r\nThis sort of open-ended discussion might be better on that list rather than in a Github issue.", "Thank you.. I was not aware of this group.", "Greetings to the tensorflow community,\r\n\r\nAnnanay proposed to do this as a GSoC project. From the Polly (Labs) side we think this is a great idea. However, none of our mentors has experience with tensorflow. Is there someone from the tensorflow community who can and would like to co-mentor this project?\r\n\r\nMichael", "While trying out Polly on the generated LLVM IR is useful to explore, the application of Polly there may be at  too low a level to realize the full potential of polyhedral analyses/transformations.  It will instead be more powerful to explore them on HLO IR itself, as part of its target dependent analyses and optimizations (SCoPs can perhaps be extracted from HLO IR there, transformed, and HLO IR written back). This in particular will provide the freedom to perform data layout/memory optimizations in conjunction with execution reordering -- information necessary to enable the former will be hard / nearly infeasible once LLVM IR has been generated (array liveness information in particular). This in turn will limit the effectiveness of the latter (exec reordering).", "@bondhugula I think this is a great idea. We had discussions on this idea in our class but I dismissed it as LLVM IR seemed a safer choice. I recall, in our discussions, we concluded that the operations in the HLO IR are very similar in functionality to the operators in [AlphaZ](www.cs.colostate.edu/AlphaZ/wiki/), where the operations are performed on Polyhedra (equivalent of N-dimensional tensors in TensorFlow). Now that I revisit it, I am more than excited to explore this path. \r\n\r\ncc @abhishek111226\r\n\r\nPS: I am proposing this as my GSoC project, and I haven't yet submitted a proposal. So I would be glad to incorporate this into my proposal if possible.", "We have no infrastructure to handle it. I am not familiar with HLO, but my estimation is that implementing a polyhedral transformation framework for a new language exceeds the workload one can do in a single GSoC.", "@annanay25 After changing cpu_compiler.cc did you build whole tensorflow again? I can't see my changes getting reflected while I use tensorflow library. It links to .so dynamically So I thought I wouldn't need to compile it explicitly. Can you please help me here. Am I missing something?\r\n", "@ankitachandak Hi, what exactly are you trying to do?\r\n\r\nYes you will have to build tensorflow again for the changes to show.", "@annanay25  even I wanted the llvm IR and thus added log to cpu_compiler.cc file. building entire tensorflow takes time and I was thinking if there's a way to just build the single file or subfiles under cpu directory (since we have BUILD file defined for that)", "@ankitachandak When you build with ```-c opt``` or ```--config=opt``` it will only build the modified files and link them.", "@annanay25   I tried.. Even that is taking time. I think linking takes lot of time.", "@annanay25 Well I don't think the changes are getting picked up. you built it using bazel and then installed it again? somehow I don't see the change in my whl file didn't change (the date). \r\nI got Target //tensorflow/tools/pip_package:build_pip_package up-to-date after the build.", "@ankitachandak, could you post the list of commands you are using to build it?\r\n\r\nIf you change just one file, building and linking should not take more than 10s. \r\n\r\nBut if you _only want the LLVM IR_, there is another way to do it. Run:\r\n```\r\n$ export TF_CPP_MIN_VLOG_LEVEL=2\r\n$ python <filename.py>         # With XLA enabled\r\n```\r\n\r\nor:\r\n\r\n```\r\n$ TF_XLA_FLAGS=\"--xla_cpu_llvm_cl_opts=-print-after-all\" python matmul.py --xla=true\r\n```", "@annanay25 I am building it with\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nI want XLA to be enables. No I know the workaround for LLVM IR. we can use llvm python libraries. But I need it to be in built. I have to do something later with the IR in the source code.", "@ankitachandak Hi, lets switch to hangouts to avoid spamming the others. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "> For starters, I inserted `LOG(INFO) << llvm_ir::DumpModuleToString(*llvm_module)` in `CpuCompiler::Compile()` and obtained the LLVM IR. But this seems to be just a small fraction of the complete program.\r\n> $ export TF_CPP_MIN_VLOG_LEVEL=2\r\n> $ python <filename.py>         # With XLA enabled\r\n> or:\r\n> $ TF_XLA_FLAGS=\"--xla_cpu_llvm_cl_opts=-print-after-all\" python matmul.py --xla=true\r\n\r\n\r\n@annanay25 Would it be possible for you to chat for a bit, I wanted to generate LLVM IR from TF code, but the commands you mentioned are probably not working for me ? I am v.sahil1@gmail.com on Hangouts.", "@ankitachandak Can you please specify which versions of Tensorflow were you using, also were you able to get LLVM IR ?", "Is Polly integrated into Tensorflow now by default? Or does polly not present better optimizations for targets such as CPU? I cannot find any new information on this.", "It's not no. I'd say for ML we are starting from a more structured set of computation that can be targetted more directly to such optimizations rather than recovering structure from LLVM IR and that has been the focus more generally. Whether or not Polly presents more optimizations/optimizations beyond that (for the long tail?) I'm not aware of any results.", "Result have been summarized here: https://www.pollylabs.org/gsoc2017/Enable-Polyhedral-Optimizations-in-XLA-through-LLVM-Polly.html"]}, {"number": 8099, "title": "Fix link in documentation", "body": "Fix a link in the documentation. The anchor of the URL was not spelled correctly.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins Test this please"]}, {"number": 8098, "title": "ValueError: setting an array element with a sequence. (tf 1.0.0's invalid use of np)", "body": "Using `tf 1.0.0` I get the following error.\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-153-fcd2318ab5dc> in <module>()\r\n     56         batch_ys = labels[randidx, :]\r\n     57         # Fit training using batch data\r\n---> 58         sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\r\n     59         # Compute average loss\r\n     60         avg_cost += sess.run(cost, \r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    936                 ' to a larger type (e.g. int64).')\r\n    937 \r\n--> 938           np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n    939 \r\n    940           if not subfeed_t.get_shape().is_compatible_with(np_val.shape):\r\n\r\n/opt/conda/lib/python3.5/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)\r\n    529 \r\n    530     \"\"\"\r\n--> 531     return array(a, dtype, copy=False, order=order)\r\n    532 \r\n    533 \r\n\r\nValueError: setting an array element with a sequence.\r\n```\r\n\r\nThis is a minimal example that should replicate this. I came across this problem also using several higher level abstractions to tf but not until I got to barebones `tf` I believed that this is a bug and not just an error on my part.\r\n\r\n```\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom keras.utils import np_utils\r\nlabels = ['Male', 'Female'] * 5 \r\nencoder = LabelEncoder()\r\nencoder.fit(labels)\r\nlabels = encoder.transform(labels)\r\nlabels = np_utils.to_categorical(labels)\r\nlabels.shape\r\n\r\nresults = np.ndarray(shape=(10,2623))\r\n\r\ntf.set_random_seed(0)\r\n# Parameters\r\nlearning_rate   = 0.001\r\ntraining_epochs = 200\r\nbatch_size      = 5\r\ndisplay_step    = 5\r\nntrain =  10\r\n\r\n# Network Parameters\r\nn_hidden_1 = 50 # 1st layer num features\r\nn_input    = 2623 # data input \r\nn_classes  = 2 # total classes (0-9 digits)\r\n\r\n# tf Graph input\r\nx = tf.placeholder(\"float\", [None, n_input])\r\ny = tf.placeholder(\"float\", [None, n_classes])\r\n\r\n# Create model\r\ndef multilayer_perceptron(_X, _weights, _biases):\r\n    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \r\n    return tf.matmul(layer_1, _weights['out']) + _biases['out']\r\n    \r\n# Store layers weight & bias\r\nstddev = 0.1 # <== This greatly affects accuracy!! \r\nweights = {\r\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\r\n    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=stddev))\r\n}\r\nbiases = {\r\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\r\n    'out': tf.Variable(tf.random_normal([n_classes]))\r\n}\r\nprint (\"Network Ready to Go!\")\r\n\r\npred = multilayer_perceptron(x, weights, biases)\r\n\r\n# Define loss and optimizer\r\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) \r\noptm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\ncorr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \r\naccr = tf.reduce_mean(tf.cast(corr, \"float\"))\r\n\r\n# Initializing the variables\r\ninit = tf.initialize_all_variables()\r\nprint (\"Functions ready\")\r\nsess = tf.Session()\r\nsess.run(init)\r\n# Training cycle\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0.\r\n    total_batch = int(ntrain/batch_size)\r\n    # Loop over all batches\r\n    for i in range(total_batch):\r\n        randidx = np.random.randint(ntrain, size=batch_size)\r\n        batch_xs = results[randidx, :]\r\n        batch_ys = labels[randidx, :]   \r\n        # Fit training using batch data\r\n        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\r\n        # Compute average loss\r\n        avg_cost += sess.run(cost, \r\n                feed_dict={x: batch_xs, y: batch_ys})/total_batch\r\n        # Display logs per epoch step\r\n    if epoch % display_step == 0:\r\n        print (\"Epoch: %03d/%03d cost: %.9f\" % \r\n               (epoch, training_epochs, avg_cost))\r\n        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\r\n        print (\" Training accuracy: %.3f\" % (train_acc))\r\n\r\n        print (\"Optimization Finished!\")\r\n```", "comments": ["@mrry Could you please take a quick look at this?", "This usually occurs when you try to feed a NumPy array that's built from an irregular list of lists, and cannot be converted to a regular dense tensor. \r\n\r\nIt's not clear whether `batch_xs` or `batch_ys` is the problem. Can you print out their `.dtype` and `.shape` properties and verify that they match what you expected?", "Closing this due to lack of activity... hopefully my suggestion worked for you, but feel free to reopen if there still seems to be a bug in TensorFlow!", "I got the same error.\r\n\r\n- This code produces `ValueError: setting an array element with a sequence.`\r\n```{python}\r\nw = tf.placeholder(dtype=tf.float32, name='w')\r\ncost = tf.nn.l2_loss(w)\r\nw1 = tf.get_variable('w1', shape=[3, 4], initializer=tf.contrib.layers.xavier_initializer(seed=1188))\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print(sess.run(cost, feed_dict={w: w1}))\r\n```\r\n- This code (without using a placeholder) works\r\n```{python}\r\nw1 = tf.get_variable('w1', shape=[3, 4], initializer=tf.contrib.layers.xavier_initializer(seed=1188))\r\ncost = tf.nn.l2_loss(w1)\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print(sess.run(cost))\r\n```\r\n\r\nSince I'm new to tensorflow, I'm leaning towards it's something I don't understand. I'm trying to keep the placeholder for easy feeding, but not sure how to do it.", "@tuanh118 I solved my issue by looking at my input sequences. Turns out, the size of sequences was varying in my input list due to a misplaced variable in my data building function.", "> This usually occurs when you try to feed a NumPy array that's built from an irregular list of lists, and cannot be converted to a regular dense tensor.\r\n> \r\n> It's not clear whether `batch_xs` or `batch_ys` is the problem. Can you print out their `.dtype` and `.shape` properties and verify that they match what you expected?\r\n\r\nI have the same problem and the shape of the train_dict is:\r\n\r\n(3 items) {Tensor with shape : True, Tensor with shape (?, 12, 120, 120, 1): [ndarray with shape (100, 120, 120, 1), ndarray with shape (80, 120, 120, 1)], Tensor with shape (?, 12, 120, 120, 1): [ndarray with shape (100, 120, 120, 1), ndarray with shape (80, 120, 120, 1)]}"]}, {"number": 8097, "title": "r1.0 python 3.5 fixes", "body": "", "comments": ["Aww man, I just triggered release builds.\r\nI'll kill them to quickly run these PR builds instead.", "How to set up GPU driver on Ubuntu ?\r\n", "This is definitely not the correct location for any questions.\r\nPlease visit the GPU manufacturer website for its driver installation instructions.", "Jenkins, test this please"]}, {"number": 8096, "title": "Error while install tensorflow via recommended method on Getting Started guide", "body": "I'm having trouble install the Tensorflow engine via  `conda` and `pip3`. I'm pasting the information regarding the python toolchain and the system I'm trying to install on.\r\n\r\n\r\n```sh\r\n\u276f conda install -c conda-forge tensorflow\r\nFetching package metadata .........\r\nSolving package specifications: ..........\r\n\r\nPackage plan for installation in environment /Users/eklavya/anaconda:\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    tensorflow-1.0.0           |           py35_0        32.7 MB  conda-forge\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    mock:       2.0.0-py35_0  conda-forge\r\n    pbr:        1.10.0-py35_0 conda-forge\r\n    protobuf:   3.2.0-py35_0  conda-forge\r\n    tensorflow: 1.0.0-py35_0  conda-forge\r\n\r\nThe following packages will be SUPERCEDED by a higher-priority channel:\r\n\r\n    conda:      4.2.13-py35_0             --> 4.2.13-py35_0 conda-forge\r\n    conda-env:  2.6.0-0                   --> 2.6.0-0       conda-forge\r\n\r\nProceed ([y]/n)? y\r\n\r\nFetching packages ...\r\nAn unexpected error has occurred.#####################################################                 | ETA:  0:02:21  50.21 kB/s\r\nPlease consider posting the following information to the\r\nconda GitHub issue tracker at:\r\n\r\n    https://github.com/conda/conda/issues\r\n\r\n\r\n\r\nCurrent conda install:\r\n\r\n               platform : osx-64\r\n          conda version : 4.2.13\r\n       conda is private : False\r\n      conda-env version : 4.2.13\r\n    conda-build version : 1.21.3\r\n         python version : 3.5.2.final.0\r\n       requests version : 2.10.0\r\n       root environment : /Users/eklavya/anaconda  (writable)\r\n    default environment : /Users/eklavya/anaconda\r\n       envs directories : /Users/eklavya/anaconda/envs\r\n          package cache : /Users/eklavya/anaconda/pkgs\r\n           channel URLs : https://repo.continuum.io/pkgs/free/osx-64\r\n                          https://repo.continuum.io/pkgs/free/noarch\r\n                          https://repo.continuum.io/pkgs/pro/osx-64\r\n                          https://repo.continuum.io/pkgs/pro/noarch\r\n            config file : None\r\n           offline mode : False\r\n\r\n\r\n\r\n`$ /Users/eklavya/anaconda/bin/conda install -c conda-forge tensorflow`\r\n\r\n\r\n\r\n\r\n    Traceback (most recent call last):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 228, in _error_catcher\r\n        yield\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 310, in read\r\n        data = self._fp.read(amt)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/http/client.py\", line 448, in read\r\n        n = self.readinto(b)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/http/client.py\", line 488, in readinto\r\n        n = self.fp.readinto(b)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/socket.py\", line 575, in readinto\r\n        return self._sock.recv_into(b)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/ssl.py\", line 929, in recv_into\r\n        return self.read(nbytes, buffer)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/ssl.py\", line 791, in read\r\n        return self._sslobj.read(len, buffer)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/ssl.py\", line 575, in read\r\n        v = self._sslobj.read(len, buffer)\r\n    socket.timeout: The read operation timed out\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/models.py\", line 664, in generate\r\n        for chunk in self.raw.stream(chunk_size, decode_content=True):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 353, in stream\r\n        data = self.read(amt=amt, decode_content=decode_content)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 320, in read\r\n        flush_decoder = True\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n        self.gen.throw(type, value, traceback)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/packages/urllib3/response.py\", line 233, in _error_catcher\r\n        raise ReadTimeoutError(self._pool, None, 'Read timed out.')\r\n    requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/fetch.py\", line 421, in download\r\n        for chunk in resp.iter_content(2**14):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/requests/models.py\", line 671, in generate\r\n        raise ConnectionError(e)\r\n    requests.exceptions.ConnectionError: HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/install.py\", line 405, in install\r\n        execute_actions(actions, index, verbose=not context.quiet)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/plan.py\", line 643, in execute_actions\r\n        inst.execute_instructions(plan, index, verbose)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/instructions.py\", line 135, in execute_instructions\r\n        cmd(state, arg)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/instructions.py\", line 47, in FETCH_CMD\r\n        fetch_pkg(state['index'][arg + '.tar.bz2'])\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/fetch.py\", line 353, in fetch_pkg\r\n        download(url, path, session=session, md5=info['md5'], urlstxt=True)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/fetch.py\", line 440, in download\r\n        raise CondaRuntimeError(\"Could not open %r for writing (%s).\" % (pp, e))\r\n    conda.exceptions.CondaRuntimeError: Runtime error: Could not open '/Users/eklavya/anaconda/pkgs/tensorflow-1.0.0-py35_0.tar.bz2.part' for writing (HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.).\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/exceptions.py\", line 479, in conda_exception_handler\r\n        return_value = func(*args, **kwargs)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/main.py\", line 145, in _main\r\n        exit_code = args.func(args, p)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/main_install.py\", line 80, in execute\r\n        install(args, parser, 'install')\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/install.py\", line 422, in install\r\n        raise CondaSystemExit('Exiting', e)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n        self.gen.throw(type, value, traceback)\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/common.py\", line 573, in json_progress_bars\r\n        yield\r\n      File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/conda/cli/install.py\", line 420, in install\r\n        raise CondaRuntimeError('RuntimeError: %s' % e)\r\n    conda.exceptions.CondaRuntimeError: Runtime error: RuntimeError: Runtime error: Could not open '/Users/eklavya/anaconda/pkgs/tensorflow-1.0.0-py35_0.tar.bz2.part' for writing (HTTPSConnectionPool(host='binstar-cio-packages-prod.s3.amazonaws.com', port=443): Read timed out.).\r\n\r\n```\r\n### Environment info\r\nOperating System:\r\n\r\n```sh\r\n\u276f uname -a\r\nDarwin abhinavs-MBP.Home 14.5.0 Darwin Kernel Version 14.5.0: Wed Jul 29 02:26:53 PDT 2015; root:xnu-2782.40.9~1/RELEASE_X86_64 x86_64\r\n                                                                                                                                  \r\n```\r\n### Other attempts\r\n\r\nI've also tried installing via the pip3 package\r\n```sh\r\n\u276f pip3 install tensorflow    \r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n                                                                                                                                  \r\n~   \r\n\u276f pip3 install tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Could not find a version that satisfies the requirement tensorflow-gpu (from versions: )\r\nNo matching distribution found for tensorflow-gpu\r\n                                                                                                                                  \r\n```\r\n", "comments": ["Tried the alternative suggested by the site\r\n\r\nThe first part is completed without issues\r\n```sh\r\n~   \r\n\u276f export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl\r\n                                                                                                                                  \r\n~   \r\n\u276f pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\nCollecting tensorflow-gpu==0.12.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl (83.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 83.6MB 6.9kB/s \r\nCollecting protobuf>=3.1.0 (from tensorflow-gpu==0.12.1)\r\n  Downloading protobuf-3.2.0-py2.py3-none-any.whl (360kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 368kB 71kB/s \r\nCollecting six>=1.10.0 (from tensorflow-gpu==0.12.1)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting wheel>=0.26 (from tensorflow-gpu==0.12.1)\r\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 78kB/s \r\nCollecting numpy>=1.11.0 (from tensorflow-gpu==0.12.1)\r\n  Downloading numpy-1.12.0-cp35-cp35m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4MB 50kB/s \r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow-gpu==0.12.1)\r\n  Downloading setuptools-34.3.1-py2.py3-none-any.whl (389kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 78kB/s \r\nCollecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==0.12.1)\r\n  Downloading appdirs-1.4.2-py2.py3-none-any.whl\r\nCollecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==0.12.1)\r\n  Downloading packaging-16.8-py2.py3-none-any.whl\r\nCollecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu==0.12.1)\r\n  Downloading pyparsing-2.1.10-py2.py3-none-any.whl (56kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 63kB/s \r\nInstalling collected packages: six, appdirs, pyparsing, packaging, setuptools, protobuf, wheel, numpy, tensorflow-gpu\r\nSuccessfully installed appdirs-1.4.2 numpy-1.12.0 packaging-16.8 protobuf-3.2.0 pyparsing-2.1.10 setuptools-34.3.1 six-1.10.0 tensorflow-gpu-0.12.1 wheel-0.29.0\r\n```\r\n\r\nBut when I actually try to import tensorflow, the session raises the following error\r\n\r\n```python3\r\n\r\n~\r\n\u276f python3\r\nPython 3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12)\r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/eklavya/anaconda/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n>>>\r\n```", "Could you please clarify exactly which instructions you are following?\r\n\r\nI infer from the error messages above that you are running on Mac.  Are you following the instructions here: https://www.tensorflow.org/install/install_mac ?\r\n\r\nYour output suggests that you are alternating between a CPU-only installation and a GPU enabled installation which is unable to find the Nvidia CUDA8 libraries.  Could you double-check the section entitled 'Requirements to run TensorFlow with GPU support' on that page?\r\n\r\n", "@prb12 \r\n\r\nYes, you are right. I tried installing the the gpu version once the first install failed.\r\n\r\nMoreover when I tried uninstalling the `tensorflow` it raised `not installed` error\r\n\r\n```sh\r\n\u276f pip3 uninstall tensorflow\r\nCannot uninstall requirement tensorflow, not installed\r\n```\r\n\r\nNor can I install tensorflow again.\r\n```sh\r\n\u276f pip3 install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```", "So, I ran the command again.\r\n\r\n```sh \r\n\r\n> sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl\r\n```\r\nAnd it install without issues.\r\n\r\nBut raises the same error while importing the tensorflow package in a session.", "@abhi18av I suspect that your original problem was due to trying to `import tensorflow` while running python in a directory with a subdir named tensorflow?  Could you please try this in a different directory?\r\n\r\n```\r\n\u276f pip3 uninstall tensorflow\r\nCannot uninstall requirement tensorflow, not installed\r\n```\r\nThis is mentioned in the instructions on https://www.tensorflow.org/install/install_mac:\r\n> Optional. If Step 5 failed (typically because you invoked a pip version lower than 8.1), install TensorFlow in the active virtualenv environment by issuing a command of the following format...\r\n\r\n", "@prb12 not really. I know that it mentions \"using from diff directory\" but I was trying to invoke it from the default shell log in - `~/ `", "However it mentions that my `pip` is out of date.\r\n\r\n```sh\r\n\r\n   99% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39.3MB 50kB/s eta 0:0    99% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39.3MB 53kB/s eta 0:0    99% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39.3MB 50kB/s eta 0:0    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 39.4MB 20kB/s \r\nRequirement already up-to-date: six>=1.10.0 in /usr/local/lib/python3.5/site-packages (from tensorflow==1.0.0)\r\nRequirement already up-to-date: wheel>=0.26 in /usr/local/lib/python3.5/site-packages (from tensorflow==1.0.0)\r\nRequirement already up-to-date: numpy>=1.11.0 in /usr/local/lib/python3.5/site-packages (from tensorflow==1.0.0)\r\nRequirement already up-to-date: protobuf>=3.1.0 in /usr/local/lib/python3.5/site-packages (from tensorflow==1.0.0)\r\nRequirement already up-to-date: setuptools in /usr/local/lib/python3.5/site-packages (from protobuf>=3.1.0->tensorflow==1.0.0)\r\nRequirement already up-to-date: packaging>=16.8 in /usr/local/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorflow==1.0.0)\r\nRequirement already up-to-date: appdirs>=1.4.0 in /usr/local/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorflow==1.0.0)\r\nRequirement already up-to-date: pyparsing in /usr/local/lib/python3.5/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow==1.0.0)\r\nInstalling collected packages: tensorflow\r\n  Found existing installation: tensorflow 1.0.0\r\n    Uninstalling tensorflow-1.0.0:\r\n      Successfully uninstalled tensorflow-1.0.0\r\nSuccessfully installed tensorflow-1.0.0\r\nYou are using pip version 8.1.2, however version 9.0.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n                                                                \r\n~   725s\r\n\r\n```\r\nBut the pip versions for both python2 and python3 are up to date. Can't figure out the issue.\r\n\r\n```sh\r\n\u276f pip --version\r\npip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7)\r\n                                                                \r\n~   \r\n\u276f pip3 --version\r\npip 9.0.1 from /Users/eklavya/anaconda/lib/python3.5/site-packages (python 3.5)\r\n                                                                \r\n~   \r\n\u276f \r\n```\r\n\r\n@prb12 I appreciate you time  and prompt responses\ud83d\udc4d ", "This thread is getting very confused by lots of different unrelated symptoms.\r\n\r\nLet's focus on your comment from 5 days ago, where you successfully installed tensorflow and then when you tried to `import tensorflow` but got an error.\r\n\r\nWhen I scroll your console output to the right, I notice that the long line with the error message is talking about trying to load the CUDA8.0 runtime:\r\n\r\n```\r\nImportError: dlopen(/Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/eklavya/anaconda/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n```\r\n\r\nAre you sure that a) you installed the CUDA8.0 runtime correctly, and b) are the cuda libraries on your `LD_LIBRARY_PATH` environment variable?", "@prb12 , I am happy to report that the installation finally got working though I have no idea how.\r\nThe only difference there was between running the basic `conda install` again was different - updated - versions of pip3.\r\n\r\n```sh\r\n\r\n\u276f conda install -c conda-forge tensorflow\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for installation in environment /Users/eklavya/anaconda:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    mock:       2.0.0-py35_0  conda-forge\r\n    pbr:        1.10.0-py35_0 conda-forge\r\n    protobuf:   3.2.0-py35_0  conda-forge\r\n    tensorflow: 1.0.0-py35_0  conda-forge\r\n\r\nThe following packages will be SUPERCEDED by a higher-priority channel:\r\n\r\n    conda:      4.3.14-py35_0             --> 4.2.13-py35_0 conda-forge\r\n    conda-env:  2.6.0-0                   --> 2.6.0-0       conda-forge\r\n\r\nProceed ([y]/n)? y\r\n\r\ntensorflow-1.0 100% |##################################################################################| Time: 0:00:26   1.31 MB/s\r\n                                                                                                                                  \r\n~   104s\r\n\u276f refresh\r\n```\r\n\r\nAble to import the library now.\r\n\r\n```python3                                                                                                                                  \r\n~   \r\n\u276f python3\r\nPython 3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> \r\n```\r\n\r\n\r\n\r\nAs far as the error regarding the lack of CUDA-8 installation, unfortunately I'm unable to update to macOS Sierra ( req. for CUDA-8 ) for a couple months more at least so I'll have to make do with the CPU version. \r\n\r\n\r\nI'll close this issue as the issue has been sorted out, but please do comment your views on what might've caused the trouble in the first place.\r\n\r\n\r\nThe fact that it throws up some \"Tensorflow library wasn't compiled with...\" might be relevant for a future reader.\r\n\r\n\r\n```python\r\n\r\n>>> import tensorflow as tf\r\n>>> \r\n>>> # Simple hello world using TensorFlow\r\n... \r\n>>> # Create a Constant op\r\n... # The op is added as a node to the default graph.\r\n... #\r\n... # The value returned by the constructor represents the output\r\n... # of the Constant op.\r\n... hello = tf.constant('Hello, TensorFlow!')\r\n>>> \r\n>>> # Start tf session\r\n... sess = tf.Session()\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n>>> \r\n>>> # Run the op\r\n... print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n>>> \r\n```", "Thank you @prb12 @aselle  for you time and patience. I appreciate it \ud83d\udc4d ", "Relevant issues\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/7778\r\nhttps://github.com/tensorflow/tensorflow/issues/7257", "I am having same issues issues am i am not able to install  tensorflow on anaconda .Error my mac throws is  The directory '/Users/muktamani/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nThe directory '/Users/muktamani/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\n\r\nBut I could sort out with conda install -c conda-forge tensorflow\r\nThanks @abhi18av \r\n\r\n", "I've trying so many tools to install TensorFlow on my mac. But I finally installed with this conda environment. Thanks bro!", "I had same problem when using virtulenv, but I installed tensorflow successfully after try \"**sudo pip install --default-timeout=100 future**\"."]}, {"number": 8095, "title": "when using convert_to_tensor the variable must have same value format", "body": "such as:\r\na=[np.array]\r\nwill get error\r\na=np.array([list])\r\nwill work fine.\r\nhope to metion in api document.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8094, "title": "Remove extra exported symbols.", "body": "Change: 149211889", "comments": []}, {"number": 8093, "title": "CentOS - failed call to cuInit: CUresult(-1)", "body": "So, again me - I'm on centOS on a cluster. Have used [this](http://stackoverflow.com/questions/33655731/error-while-importing-tensorflow-in-python2-7-in-ubuntu-12-04-glibc-2-17-not-f) hack for bypassing and using a different GLIBC. Now, however, there seems to be some weird issue with Tensorflow not wanting to run my GPU by failing something on the CUDA side. \r\nSome general info:\r\n```\r\nLSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\r\nDistributor ID: CentOS\r\nDescription:    CentOS release 6.5 (Final)\r\nRelease:        6.5\r\nCodename:       Final\r\nCUDA version: 8.0\r\nCUDNN version: 5.1\r\nPython version: 3.6.0\r\nLD_LIBRARY_PATH=/share/apps/barber/system/lib/:/share/apps/barber/system/lib64/:/opt/gridengine/lib/linux-x64:/opt/openmpi/lib/:/share/apps/barber/cuda/lib64/:/share/apps/barber/cuda/nvvm/lib64/:/share/apps/barber/cuda/extras/CUPTI/lib64:/share/apps/barber/cudnn/lib64/:/share/apps/barber/arrayfire-3/lib/:/share/apps/python-3.6.0-shared/lib/\r\n```\r\nThe actual error I get from my code is (first 2 lines are printed by me):\r\n```\r\nRunning tool: tensorflow\r\n/gpu:0\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could spe\r\ned up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could s\r\npeed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could s\r\npeed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could spee\r\nd up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could spe\r\ned up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could spee\r\nd up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUresult(-1)\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: tesla2.local\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: tesla2.local\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PD\r\nT 2016\r\nGCC version:  gcc version 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC)\r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nTraceback (most recent call last):\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1000, in _run_fn\r\n    self._extend_graph()\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1049, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/share/apps/python-3.6.0-shared/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'gradients/ff_1/add_grad/BroadcastGradientArgs': Could not satisfy explicit d\r\nevice specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:\r\n0\r\n         [[Node: gradients/ff_1/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/device:GPU:0\"](gradients/ff_1/add_grad/Shape, gradients/ff_1/\r\nadd_grad/Shape_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bench.py\", line 23, in <module>\r\n    execute_all(**vars(parser.parse_args()))\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/executor.py\", line 40, in execute_all\r\n    wide=w, depth=d, batch=b)\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/executor.py\", line 123, in single_experiment\r\n    f(model, device, temp_folder, **kwargs)\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/executor.py\", line 80, in execute_experiment\r\n    session.run(init)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'gradients/ff_1/add_grad/BroadcastGradientArgs': Could not satisfy explicit d\r\nevice specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:\r\n0\r\n         [[Node: gradients/ff_1/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/device:GPU:0\"](gradients/ff_1/add_grad/Shape, gradients/ff_1/\r\nadd_grad/Shape_1)]]\r\n\r\nCaused by op 'gradients/ff_1/add_grad/BroadcastGradientArgs', defined at:\r\n  File \"bench.py\", line 23, in <module>\r\n    execute_all(**vars(parser.parse_args()))\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/executor.py\", line 40, in execute_all\r\n    wide=w, depth=d, batch=b)\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/executor.py\", line 123, in single_experiment\r\n    f(model, device, temp_folder, **kwargs)\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/executor.py\", line 75, in execute_experiment\r\n    x_in, y_in, inference, train, cost = build_model(batch)\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/ff.py\", line 21, in build_ff_net\r\n    train = optimizer.minimize(cost)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 288, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 354, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 586, in _AddGrad\r\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 411, in _broadcast_gradient_args\r\n    name=name)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op 'ff_1/add', defined at:\r\n  File \"bench.py\", line 23, in <module>\r\n    execute_all(**vars(parser.parse_args()))\r\n[elided 2 identical lines from previous traceback]\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/executor.py\", line 75, in execute_experiment\r\n    x_in, y_in, inference, train, cost = build_model(batch)\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/ff.py\", line 14, in build_ff_net\r\n    h = net.dense_layer(h, shape=(arch_specs[i-1], arch_specs[i]), nonlinearity=\"tanh\")\r\n  File \"/home/abotev/work/python/deep-bench/deep_bench/frameworks/tensorflow/net.py\", line 40, in dense_layer\r\n    return f(tf.matmul(x_in, w) + tf.reshape(b, [1, -1]))\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 884, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 73, in add\r\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/share/apps/barber/system/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'gradients/ff_1/add_grad/BroadcastGradientArgs': Could not satisfy explicit device specific\r\nation '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n         [[Node: gradients/ff_1/add_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/device:GPU:0\"](gradients/ff_1/add_grad/Shape, gradients/ff_1/\r\nadd_grad/Shape_1)]]\r\n```\r\nOne thing slightly suspicious is that it is picking up the default GCC (4.4) rather than the local user one (4.9), however I don't see how this relates to CUDA. \r\n\r\nHowever, the second error about `libcuda` version not found has been reported before in #4267, while a similar issue seems to be #2882. At this stage, there is not much I can really say why this is happening.\r\n\r\nAlso for reference, both Theano and Pytorch worked out of the box, no errors, no complaints directly linking to both CUDA and CuDNN, so this is not a CUDA installation issue. Potentially the fact I use a separate GLIBC might be an issue, but again I don't see why myself. \r\n\r\nPS: This potentially could be some problem of the alternative GLIBC not detecting the GPUs, which to be more general than tensorflow, but I will need to talk with the system admin.\r\n", "comments": ["@zheng-xq  Any ideas what might be going wrong here?  StreamExecutor claims to load libcuda, but then cuInit fails... Maybe the dynamic loader state being a mess?  (There seems to be a lot of dubious library hackery going on, so it may be pretty much impossible for us to reproduce.  )", "This can be closed, @prb12 thanks for the fast response, it turned out there was some issue with my GLIBC installation, which however I did not manage to nail down and understand. We had a new one reinstalled a new and now it works!."]}, {"number": 8092, "title": " fail to build tensorflow from source::unexpected pipe read status", "body": "Hello,everyone!\r\nI am new about tensorflow,and I am just trying to build tensorflow from source according to the steps described step by step, but I get the following error, Is there anyone who can help on it? That would be very kind of you!\r\nThank you very much!\r\n\r\ncynthia@cynthia-pc:~/tensorflow$ ./configure\r\n\r\n..........continue........\r\nthen the result came with the following......\r\nI guess that maybe something wrong with my bazel ,but I'm new about this,and I'm really no idea how to solve it.\r\n\r\nunexpected pipe read status: No such file or directory\r\nServer presumed dead. Now printing '/home/cynthia/.cache/bazel/_bazel_cynthia/581b5660c7c191b39a76e8a607014a30/server/jvm.out':\r\nJava HotSpot(TM) Server VM warning: You have loaded library /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so which might have disabled stack guard. The VM will try to fix the stack guard now.\r\nIt's highly recommended that you fix the library with 'execstack -c ', or link it with '-z noexecstack'.\r\nJNI initialization failed: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: wrong ELF class: ELFCLASS64 (Possible cause: architecture word width mismatch). Possibly your installation has been corrupted; if this problem persists, try 'rm -fr /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132'.\r\njava.lang.UnsatisfiedLinkError: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: wrong ELF class: ELFCLASS64 (Possible cause: architecture word width mismatch)\r\nat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\nat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\nat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1857)\r\nat java.lang.Runtime.loadLibrary0(Runtime.java:870)\r\nat java.lang.System.loadLibrary(System.java:1122)\r\nat com.google.devtools.build.lib.UnixJniLoader.loadJni(UnixJniLoader.java:28)\r\nat com.google.devtools.build.lib.unix.ProcessUtils.(ProcessUtils.java:28)\r\nat com.google.devtools.build.lib.util.ProcessUtils.getpid(ProcessUtils.java:47)\r\nat com.google.devtools.build.lib.util.OsUtils.forceJNI(OsUtils.java:55)\r\nat com.google.devtools.build.lib.util.OsUtils.maybeForceJNI(OsUtils.java:43)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.newRuntime(BlazeRuntime.java:934)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.createBlazeRPCServer(BlazeRuntime.java:838)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:777)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:565)\r\nat com.google.devtools.build.lib.bazel.BazelMain.main(BazelMain.java:57)", "comments": ["Superficially, this looks like a 32/64 bit problem with bazel or java JDK installation.  Could you please double check that you followed the instructions here https://bazel.build/versions/master/docs/install.html\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Similar here on ubuntu 16.04 with output\r\n\r\n```\r\n(deep) user@user:~/Downloads/tensorflow-1.1.0$ ./configure\r\nPlease specify the location of python. [Default is /home/user/.virtualenvs/deep/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /home/user/.virtualenvs/deep/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/user/.virtualenvs/deep/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /home/user/.virtualenvs/deep/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\n...........\r\nunexpected pipe read status: (error: 2): No such file or directory\r\nServer presumed dead. Now printing '/home/user/.cache/bazel/_bazel_user/143f576e9f98f3e1c355aa517023abba/server/jvm.out':\r\njava.lang.ExceptionInInitializerError\r\n\tat java.lang.J9VMInternals.ensureError(J9VMInternals.java:141)\r\n\tat java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:130)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeExecutor.skyFunctions(SkyframeExecutor.java:351)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeExecutor.init(SkyframeExecutor.java:578)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutor.init(SequencedSkyframeExecutor.java:249)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutor.create(SequencedSkyframeExecutor.java:208)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutor.create(SequencedSkyframeExecutor.java:156)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutorFactory.create(SequencedSkyframeExecutorFactory.java:50)\r\n\tat com.google.devtools.build.lib.runtime.WorkspaceBuilder.build(WorkspaceBuilder.java:86)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.initWorkspace(BlazeRuntime.java:204)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.newRuntime(BlazeRuntime.java:1011)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.createBlazeRPCServer(BlazeRuntime.java:838)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:777)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:565)\r\n\tat com.google.devtools.build.lib.bazel.BazelMain.main(BazelMain.java:57)\r\nCaused by: java.lang.ClassCastException: com.ibm.lang.management.UnixExtendedOperatingSystem incompatible with com.sun.management.OperatingSystemMXBean\r\n\tat com.google.devtools.build.lib.util.ResourceUsage.<clinit>(ResourceUsage.java:45)\r\n\t... 13 more\r\n```\r\n\r\nI have no idea about which file is not found according to `unexpected pipe read status: (error: 2): No such file or directory`.", "Had the same error as @Yevgnen. Using the 'testing' version of bazel instead of the 'stable' version solved it for me (see [this section in the install instructions](https://bazel.build/versions/master/docs/install-ubuntu.html#1-add-bazel-distribution-uri-as-a-package-source-one-time-setup)).\r\n\r\nIf it helps, I'm on\r\n```\r\nDistributor ID: LinuxMint\r\nDescription:    Linux Mint 18.1 Serena\r\nRelease:        18.1\r\nCodename:       serena\r\n```\r\nwith\r\n```\r\nLinux version 4.4.0-53-generic (buildd@lcy01-28) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) ) #74-Ubuntu SMP Fri Dec 2 15:59:10 UTC 2016\r\n```\r\n", "@prb12 same issue here. I installed Bazel using custom APT repository. Do I have to install Java JDK then? I feel like there's some other 'Java thing'  not being installed, which I have no idea what it is.\r\n\r\nUpdate: Why does installing Bazel using custom APT repository not install the JDK mentioned in the second installing option? I Tried to install JDK, and the issue is solved. Phew", "Hello I had the same issue, I uninstall jdk 1.9, checked that was installed JDK 1.8 after it installed open jdk 1.8 and finally it works.  Check this https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/\r\n", "I had this issue:  \r\n  \r\n`Extracting Bazel installation...\r\n..........\r\nunexpected pipe read status: (error: 2): No such file or directory\r\nServer presumed dead. Now printing '/home/kyle/.cache/bazel/_bazel_kyle/c7d12c37ed7a4d74cf8d7e2aae5a727d/server/jvm.out':\r\njava.lang.ExceptionInInitializerError\r\n\tat java.lang.J9VMInternals.ensureError(J9VMInternals.java:141)\r\n\tat java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:130)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeExecutor.skyFunctions(SkyframeExecutor.java:348)\r\n\tat com.google.devtools.build.lib.skyframe.SkyframeExecutor.init(SkyframeExecutor.java:586)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutor.init(SequencedSkyframeExecutor.java:252)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutor.create(SequencedSkyframeExecutor.java:211)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutor.create(SequencedSkyframeExecutor.java:162)\r\n\tat com.google.devtools.build.lib.skyframe.SequencedSkyframeExecutorFactory.create(SequencedSkyframeExecutorFactory.java:48)\r\n\tat com.google.devtools.build.lib.runtime.WorkspaceBuilder.build(WorkspaceBuilder.java:81)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.initWorkspace(BlazeRuntime.java:204)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.newRuntime(BlazeRuntime.java:1023)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.createBlazeRPCServer(BlazeRuntime.java:850)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:789)\r\n\tat com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:570)\r\n\tat com.google.devtools.build.lib.bazel.BazelMain.main(BazelMain.java:56)\r\nCaused by: java.lang.ClassCastException: com.ibm.lang.management.UnixExtendedOperatingSystem incompatible with com.sun.management.OperatingSystemMXBean\r\n\tat com.google.devtools.build.lib.util.ResourceUsage.<clinit>(ResourceUsage.java:45)\r\n\t... 13 more`\r\n\r\nTurns out I simply did not have jdk1.8 installed. Looks like the custom APT installation does not include this by default. \r\n  Solved by `sudo apt-get install openjdk-8-jdk`.", "I had the same issue. Installing jdk manually solved the issue. "]}, {"number": 8091, "title": "unexpected pipe read status", "body": "Hello,everyone!\r\nI am new about tensorflow,and I am just trying to build tensorflow from source according to the steps described step by step, but I get the following error, Is there anyone who can help on it? That would be very kind of you!\r\nThank you very much!\r\n\r\ncynthia@cynthia-pc:~/tensorflow$ ./configure\r\n\r\n..........continue........\r\nthen the result came with the following......\r\nI guess that maybe something wrong with my bazel ,but I'm new about this,and I'm really no idea how to solve it.\r\n\r\nunexpected pipe read status: No such file or directory\r\nServer presumed dead. Now printing '/home/cynthia/.cache/bazel/_bazel_cynthia/581b5660c7c191b39a76e8a607014a30/server/jvm.out':\r\nJava HotSpot(TM) Server VM warning: You have loaded library /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so which might have disabled stack guard. The VM will try to fix the stack guard now.\r\nIt's highly recommended that you fix the library with 'execstack -c ', or link it with '-z noexecstack'.\r\nJNI initialization failed: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: wrong ELF class: ELFCLASS64 (Possible cause: architecture word width mismatch). Possibly your installation has been corrupted; if this problem persists, try 'rm -fr /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132'.\r\njava.lang.UnsatisfiedLinkError: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: /home/cynthia/.cache/bazel/_bazel_cynthia/install/6a615f09339464d6ea7e8fe00f96c132/_embedded_binaries/libunix.so: wrong ELF class: ELFCLASS64 (Possible cause: architecture word width mismatch)\r\nat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\nat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\nat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1857)\r\nat java.lang.Runtime.loadLibrary0(Runtime.java:870)\r\nat java.lang.System.loadLibrary(System.java:1122)\r\nat com.google.devtools.build.lib.UnixJniLoader.loadJni(UnixJniLoader.java:28)\r\nat com.google.devtools.build.lib.unix.ProcessUtils.(ProcessUtils.java:28)\r\nat com.google.devtools.build.lib.util.ProcessUtils.getpid(ProcessUtils.java:47)\r\nat com.google.devtools.build.lib.util.OsUtils.forceJNI(OsUtils.java:55)\r\nat com.google.devtools.build.lib.util.OsUtils.maybeForceJNI(OsUtils.java:43)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.newRuntime(BlazeRuntime.java:934)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.createBlazeRPCServer(BlazeRuntime.java:838)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.serverMain(BlazeRuntime.java:777)\r\nat com.google.devtools.build.lib.runtime.BlazeRuntime.main(BlazeRuntime.java:565)\r\nat com.google.devtools.build.lib.bazel.BazelMain.main(BazelMain.java:57)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@Cynthia0629 , can you please resolve conflicts first? Also, please sign the CLA."]}, {"number": 8090, "title": "fused_batch_norm doesn't work with epsilon < 1e-5", "body": "[This line in nn_impl.py](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/python/ops/nn_impl.py#L806) has the following comment: \"Add 1e-12 to epsilon when epsilon <= 1e-5 to prevent CUDNN exception.\" However, I still get the cuDNN exception when using values of epsilon less than 1e-5.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNothing related.\r\n\r\n### Environment info\r\n\r\n- Operating System: CentOS release 6.3 (Final)\r\n- Installed version of CUDA: 8.0.44\r\n- Installed version of cuDNN: 8.0v5.1\r\n- Installed `tensorflow-gpu` from pip-3.5\r\n\r\n```\r\n$ python3.5 -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom itertools import chain\r\n\r\nwith tf.device('/gpu:0'):\r\n        x = tf.zeros([4, 4, 4, 4])\r\n        scale, offset = tf.ones(4), tf.zeros(4)\r\n\r\n        y, _, _ = tf.nn.fused_batch_norm(x, scale, offset, mean=None, variance=None, epsilon=1e-8,\r\n                data_format='NCHW', is_training=True)\r\n\r\n        init_op = tf.group(*(v.initializer for v in chain(tf.global_variables(),\r\n                tf.local_variables())))\r\n\r\n        sess = tf.Session()\r\n        sess.run(init_op)\r\n        sess.run(y)\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n\r\n```\r\n$ python3.5 tests/fused_batch_norm_crash.py\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:84:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:84:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:2177] failed to enqueue forward batch normalization on stream: CUDNN_STATUS_BAD_PARAM\r\nTraceback (most recent call last):\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/share/apps/python3/3.5.3/intel/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape ([4,4,4,4])\r\n\t [[Node: FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.0001e-08, is_training=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_normal, ones, zeros, Const, Const_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tests/fused_batch_norm_crash.py\", line 16, in <module>\r\n    sess.run(y)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape ([4,4,4,4])\r\n\t [[Node: FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.0001e-08, is_training=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_normal, ones, zeros, Const, Const_1)]]\r\n\r\nCaused by op 'FusedBatchNorm', defined at:\r\n  File \"tests/fused_batch_norm_crash.py\", line 9, in <module>\r\n    data_format='NCHW', is_training=True)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py\", line 818, in fused_batch_norm\r\n    name=name)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1257, in _fused_batch_norm\r\n    is_training=is_training, name=name)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ar2922/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): cuDNN launch failure : input shape ([4,4,4,4])\r\n\t [[Node: FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.0001e-08, is_training=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_normal, ones, zeros, Const, Const_1)]]\r\n```\r\n", "comments": ["@zhangyaobit Could you take a look?", "The minimum value of epsilon 1e-12 in this line of code needs to be updated: \r\nepsilon = epsilon if epsilon > 1e-5 else epsilon + 1e-12\r\n\r\nBefore I get time to look at this, anyone is welcome to experiment on what a proper value should be. Marked as contribution welcome for now, so I'm not blocking on this issue.", "Hi,\r\nI am having the same issue on ubuntu 17.04, tensorflow 1.2.1, python 2.7 , cuda 8.0.61 and cudnn v5.1. \r\nCould you please share any possible solution?", "Are you using an epsilon less than 1e-5? Could you set it equal or larger than 1e-5?", "Hello @zhangyaobit ,\r\nThanks for your reply.\r\n\r\nI am using an epsilon value of 0.001. But, I tried it with epsilon values of equal or less than 1e-5 which didn't work. I could finally avoid this error by compiling tensorflow from the source for pyhton3. Though, I didn't try compiling tensorflow from the source for python 2. ", "I got the same problem when I set the epsilon value less than 1e-5 on Ubuntu 16.04+CUDA 8.0.61+cuDNN 5.1.10. I used tensorflow 1.3.0 installed from source also.\r\n\r\nAccording to the error, I read the cuDNN library user guide, and find the description about `epsilon` (in Section 4.95.\u00a0cudnnBatchNormalizationForwardInference):\r\n```\r\nInput. Epsilon value used in the batch normalization formula. Minimum\r\nallowed value is CUDNN_BN_MIN_EPSILON defined in cudnn.h.\r\n```\r\n\r\n\r\nSo I check my cudnn.h, there is one line defined:\r\n```\r\n#define CUDNN_BN_MIN_EPSILON 1e-5 // Minimum epsilon allowed to be used in the Batch Normalization formula\r\n```\r\nI think that is why the smaller epsilon caused cuDNN launch failure.\r\n", "Closing this issue, as the issue has been fixed with a min epsilon set to 1e-5: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L811", "> Closing this issue, as the issue has been fixed with a min epsilon set to 1e-5: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L811\r\n\r\nI find that file, in nn_impi.py min epsilon originally is 1e-5,but i still have this error,\r\nInternalError (see above for traceback): cuDNN launch failure : input shape ([4,4,4,4])\r\nso, how to fix"]}, {"number": 8089, "title": "Feature requests: erfcinv", "body": "I find there is no erfcinv op in TensorFlow. Since erfcinv is provided by CUDA, it should be straightforward to add such an op. \r\nThe gradient is also very simple:\r\n-0.5*exp(erfcinv(x)**2)*sqrt(pi)\r\n\r\nIs it possible to add it in the near future?\r\n\r\nThanks!", "comments": ["Has the `erfinv` op been implemented in Tensorflow anywhere? I can't seem to find it, so I'm planning on implementing my own version (following this Stack Overflow [answer](https://stackoverflow.com/questions/42381244/pure-python-inverse-error-function)).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]