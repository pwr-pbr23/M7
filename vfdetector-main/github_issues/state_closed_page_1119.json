[{"number": 19668, "title": "tf.layers.conv2d_transpose does not accept output_shape", "body": "I believe there is no reason why using layer interface user should not be able to specify this. Output shape is not obvious for some cases (as stated here: https://github.com/tensorflow/tensorflow/issues/2118) and without accepting output_shape argument one cannot match desired output. Potential use case is trying to build autoencoder for 2d images in form of (conv1->conv2->conv3->deconv1->deconv2->deconv3). With stride >1 there is no way for doing this with layers interface right now.", "comments": ["duplication of #19236?", "So, I am going to close this since we are now recommending using the keras layers. Thank you for your understanding."]}, {"number": 19667, "title": "Messed up dimentions expanding within transform calling", "body": "https://github.com/tensorflow/tensorflow/blob/25b2f01d5fc9ae500bc5969291ba90e48b25cceb/tensorflow/contrib/image/python/ops/image_ops.py#L252\r\n\r\nIf image has no channels, we have to add one in the last dim\r\nSo the code have to be:\r\nimages = image_or_images[ :, :, :, None]", "comments": ["But code might be useful to process one picture.\r\nI've got errors when I tried to process multiple images k X 32 X 32 with multiple transforms k X 8 with no good info", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi, @tatianashp!\r\nWhat do you mean? All info is provided.", "@mikhailkin My apologies for closing this issue by mistake. \r\nWe don't have anybody working on this code at the moment. It would be great if you submitted the correction that you propose as PR.\r\n\r\n", "@mikhailkin In the documentation:\r\n```\r\n    images: A tensor of shape (num_images, num_rows, num_columns, num_channels)\r\n       (NHWC), (num_rows, num_columns, num_channels) (HWC), or\r\n       (num_rows, num_columns) (HW). The rank must be statically known (the\r\n       shape is not `TensorShape(None)`.\r\n```\r\n\r\nIt looks like if the rank is 3, then the expected input is `HWC`. So the `N` (batch) will be added instead, which will be dim 0. \r\n\r\nI think this might be expected behavior?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@mikhailkin I think this is the expected behavior. Will close the issue for now but feel free to reopen if issue still exists."]}, {"number": 19666, "title": "R1.6", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 19665, "title": "Update LICENSE 2017 to 2018", "body": "I found a 2017 in line 191 and update to 2018.", "comments": ["I was told by our lawyers to leave the license years alone."]}, {"number": 19664, "title": "Speech command pretrained model in tensorflow lite is slow", "body": "\r\nSystem information\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from (source): tensorflow v1.8.0\r\nPython version: 2.7\r\nBazel version (if compiling from source): 0.11.1\r\nGCC/Compiler version (if compiling from source): 5.4\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: see code below\r\n\r\n\r\n\r\nI am trying to use pretrained model (conv_actions_frozen.tflite from \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/conv_actions_tflite.zip\") \r\nto inference on my ubuntu desktop , it took around 100ms for invoking. \r\nBut in tensorflow, using the same pretrained model(conv_action_frozen.pb) label_wav.py to inference just around 80ms. \r\nI would expect the inference time on tflite should be faster than tensorflow and i found it spend most time on Depthwise_conv2 operator. \r\nIs there any reason or clue can check this?\r\nThanks.\r\n\r\n\r\n\r\n", "comments": []}, {"number": 19663, "title": "Add gradient for operation 'SparseSlice' ", "body": "Fix #19373.", "comments": ["@rachellim Thank you for your review. I think all your comments have been resolved, could you take a look again?", "Nagging Assignee @martinwicke: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hey @facaiy -- apologies about the delay pushing this PR through! Looks like you have some lint failures  [(see the log)](https://source.cloud.google.com/results/invocations/06ee8145-56ad-4acb-9eea-4ac830829874/log), primarily to do with indentation. Can you fix them?", "@rachellim Thank you. I have fixed them."]}, {"number": 19662, "title": "Add C++ no gradient for Floor operation.", "body": "Fixes #13982.", "comments": []}, {"number": 19661, "title": "Fix multiple values for keyword argument error in ModelAverageOptimizer and ElasticAverageOptimizer", "body": "Fix multiple values for keyword argument error in ModelAverageOptimizer and ElasticAverageOptimz. \r\nFor detailed error, please look at [here](https://github.com/tensorflow/tensorflow/issues/19617)", "comments": ["Nagging Assignee @martinwicke: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19660, "title": "Correct \"No package nasm\" issue", "body": "Corrected as per suggestion in https://github.com/tensorflow/tensorflow/issues/16862", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn Thu, May 31, 2018 at 10:55 AM, googlebot <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19660#issuecomment-393393151>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAHtbivtj4S_a579AvXAftw7atKGdy_mks5t32nOgaJpZM4UUYGw>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@rnunziata has trouble downloading this file -- are we sure this this link is correct and produces the same binary with the same hash? \r\n\r\nIt'll only be used if the other two fail (unlikely), but I'd hate to set up a mystery bug down the road.", "Also, please reopen this against master if you want to pursue this."]}, {"number": 19659, "title": "aceholders", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing since this seems to have been filed in error? (No description, the title just says \"aceholder\")?"]}, {"number": 19658, "title": "feature request:TFLite seem don't support conv3d yet?", "body": "### System information\r\nirrelevant\r\n\r\n### Describe the problem\r\nI try to convert a model that contains conv3d op into tflite model using TOCO.But it seems that TFLite limits input shape dim to less than 4.That is to say,it can only support at most conv2d?I hope that TFLite should support conv3d someday~\r\n\r\n### Source code / logs\r\nF tensorflow/contrib/lite/toco/import_tensorflow.cc:182] Check failed: input_shape.dim_size() <= 4 (5 vs. 4)\r\n\r\n", "comments": ["@aselle Does TF Lite have conv3d support on the roadmap?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It is not on our nearterm roadmap. We are not opposed to adding it, and we will eventually do so. What is your use-case (what particular model?)", "just a gesture recognition model using 3D videos for trainning.it uses conv3d op.", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We will track missing operations in the above aggregation issue, to avoid having an issue for every single operation.", "As we work toward fleshing out the builtin op library for TensorFlow Lite, we've been working on an experimental feature that allows using select TensorFlow ops (including conv3d) from within the TensorFlow Lite runtime. The goal is to help reduce some of the friction for using models that rely on ops not yet natively supported by TensorFlow Lite (at the cost of increased binary size). This feature requires opting in during model conversion, as well as adding an additional dependency. More details can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md).\r\n\r\nFeedback is very much appreciated (either via GitHub or directly via tflite@tensorflow.org), and we'll be adding and refining functionality over the coming weeks. Cheers."]}, {"number": 19657, "title": "Crash when saving model: \"tensorflow.GraphDef was modified concurrently during serialization\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.4 (also Linux Ubuntu)\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A \r\n- **GPU model and memory**: N/A (problem on CPU and GPU)\r\n- **Exact command to reproduce**: saver.save(sess,fname)\r\n\r\n### Describe the problem\r\nTF consistently crashes when saving a model after loading datasets of a certain size into memory and creating iterators for them.  This occurs both on macOS and on Linux.\r\n\r\nSome additional information:\r\n- If I make the iterators initializable instead of one-shot, there is no crash.\r\n- If I only make one iterator (not two), there is no crash\r\n- If I reduce the size of the dataset, there is no crash\r\n\r\n### Source code / logs\r\nThe most minimal example I could come up with:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n### The following two lines are necsesary to cause the crash\r\n### Also, reduce the size to eliminate the crash\r\ndata1 = tf.data.Dataset.from_tensor_slices(np.ones([20000000,2,2]))\r\niter1 = data1.make_one_shot_iterator()\r\n\r\ndata2 = tf.data.Dataset.from_tensor_slices(np.ones([20000000,2,2]))\r\niter2 = data2.make_one_shot_iterator()\r\nfeat2 = iter2.get_next()\r\nout = tf.layers.dense(inputs=feat2, units=10)\r\n\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    # Crash here\r\n    saver.save(sess=sess,save_path='/tmp/crash-example')\r\n```\r\n\r\nThis causes the following crash:\r\n```\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\r\nAborted (core dumped)\r\n```", "comments": ["If the size matters, it's possible this is related to the `2**32` size limit for protobuf.", "When I used feed for input data, no problem occured, now when using tf.data, the input is part of the graph resulting in this problem. Can I somehow save all variables except the input data? Respectively, can we save all trainable variables only?\r\n", "tf.saved_model.simple_save seems to work, I tried this example with a sufficiently large dataset, so large, I got the memory error\r\nhttp://vict0rsch.github.io/2018/05/17/restore-tf-model-dataset/", "the same error rises when using SavedModelBuilder and this tutorial http://stackabuse.com/tensorflow-save-and-restore-models/", "I now understand that this bug was caused because tf.data.Dataset.from_tensor_slices stores the entire dataset in the TF graph.  There is a comment on [the datasets guide](https://www.tensorflow.org/programmers_guide/datasets) about this.  This also causes my TensorBoard and checkpoint files to be enormous.", "Apologies for the delayed response. Yup, `tf.data.Dataset.from_tensor_slices(x)` will embed `x` as a constant tensor inside the graph. With the dataset size you're describing, using an initializable iterator (as you suggested) with a placeholder would be more appropriate. Something like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = tf.placeholder(tf.float32)\r\nds = tf.data.Dataset.from_tensor_slices(data)\r\nitr = ds.make_initializable_iterator()\r\nnext = itr.get_next()\r\n\r\nwith tf.Session() as sess:\r\n  # Feed the data when initializing the iterator\r\n  sess.run(itr.initializer, feed_dict={data: np.random.rand(100, 100)})\r\n  for _ in range(10):\r\n    print(sess.run(next))\r\n```\r\n\r\nWithout the placeholder, the tensor content ends up in the graph def. With `one_shot_iterator`, it ends up in the graph def twice.\r\n\r\n@mrry @jsimsa : Perhaps the documentation could be enhanced to make this more obvious?\r\n\r\n@samsamoa @fogelton : Does this sound reasonable?", "Yes, this works well.  It is described well on the datasets guide, though not in the tf.Dataset documentation.", "ok, thanks", "Hi,\r\n\r\nI have used `tf.data.Dataset.from_generator` in my code, and I get the same error. \r\n\r\nI was wondering if even by using generators the graph becomes too big to save.\r\n\r\nThanks", "@amirmazaheri1990 : No, that shouldn't be the case, so you might want to file a separate issue, including more details on what you're doing. Perhaps some other aspect of your program results in large constants embedded in the graph?", "@asimshankar  I'm running into the same issue while trying to train a dataset with 80,000 rows and 3800 columns ( i.e. 3800 covariates) using a LinearRegressor estimator( `tf.estimator.LinearRegressor`)  Your solution regarding using an initializable iterator doesn't work with the Estimator api easily (https://www.tensorflow.org/guide/datasets) Do you have an example where an estimator api for prediction works with an initializable iterator ? ", "@freshforlife : could you file a new issue with all the details (version etc) and ideally a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve). The original issue here seemed to be about reaching the 2GB limit on protobufs, so (without knowing all the details), your problem sounds different", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 19656, "title": "Fix setuptools version to avoid a bad release.", "body": "", "comments": []}, {"number": 19655, "title": "Fix setuptools version to avoid a bad release.", "body": "", "comments": []}, {"number": 19654, "title": "Branch 198629366", "body": "", "comments": []}, {"number": 19653, "title": "Add C++ SegmentSum gradient operation.", "body": "This pull request adds the C++ counter part of the gradient of the SegmentSum python operation.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Reviewer @suharshs: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "It's been more than a month now...", "Nagging Reviewer @suharshs: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "Truly sorry for the delay. This slipped under my radar :( . Looks great!", "What is the next step?\r\nCan someone do a \"please test\"?"]}, {"number": 19652, "title": "Fix TRT Node rewiring", "body": "This PR fixes the wiring issues present when shared inputs or outputs present in multi-input multi-output TRTEngineOps and TRTCalibOps. It also try-imports tensorrt ops to import_pb_to_tensorboard.py so that networks containing TensorRT ops can be visualized ", "comments": ["@samikama thanks for the fix. As discussed, please add some explanation/examples as code comments to describe the changes, to remind us of how it works in the future."]}, {"number": 19651, "title": "Fix incorrect documentation for `tf.reduce_any`/`tf.reduce_all`", "body": "This fix fixes the incorrect documentation for `tf.reduce_any`/`tf.reduce_all`. The previous description:\r\n```\r\n   If `axis` has no entries, all dimensions are reduced, and a\r\n   tensor with a single element is returned.\r\n```\r\n\r\nis not correct. See below:\r\n```\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> x = tf.constant([[True,  True], [False, False]])\r\n>>> v1 = tf.reduce_any(x, [])\r\n>>> tf.Session().run(v1)\r\narray([[ True,  True],\r\n       [False, False]])\r\n>>> v2 = tf.reduce_any(x, None)\r\n>>> tf.Session().run(v2)\r\nTrue\r\n>>>\r\n```\r\n\r\nInstead, the correct description should be:\r\n```\r\n   If `axis` is None, all dimensions are reduced, and a\r\n   tensor with a single element is returned.\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19650, "title": "Add support of string split behavior compatible with python's `str.split`", "body": "This fix tries to address the issue raised in #18271 where the existing `tf.string_split` does not match python's `str.split`. Specifically, the `tf.string_split` does not handle the case where separator might be multi-char.\r\n\r\nThis fix adds the implementation of string split compatible with `str.split`. In order to maintain backward-compatible, this fix exposes the new implementation of `array_ops.string_split_v2` into `tf.strings.split` namespace.\r\n\r\nThis fix fixes #18271.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Does this new array_ops.string_split_v2 op support utf8 in contrast to the old tf.string_split?", "I don't know that it does, but that would be nice to add."]}, {"number": 19649, "title": "Add normalizer_fn support for sequence_numeric_column", "body": "This fix tries to address the issue raised in #19628 where there were no normalizer_fn support for sequence_numeric_column (unlike numeric_column). This fix adds the normalizer_fn support.\r\n\r\nThis fix fixes #19628.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@ispirmustafa I modifier this to add a `dense_normalizer_fn` that runs after the sparse tensor is converted to a dense tensor. This is because it is easier for us to normalize with dense tensor ops.\r\n\r\n`sequence_numeric_column(key, dense_normalizer_fn=lambda dense_tensor:  (dense_tensor - mean) / std`)\r\n\r\nShould I work on a PR with tests to TF core -- IE something you think it worth adding to TF? Or should we be approaching this differently?\r\n\r\n```\r\n  def _get_sequence_dense_tensor(self, inputs, weight_collections=None, trainable=None):\r\n......\r\n    dense_shape = array_ops.concat(\r\n        [array_ops.shape(dense_tensor)[:1], [-1], self._variable_shape], axis=0)\r\n    dense_tensor = array_ops.reshape(dense_tensor, shape=dense_shape)\r\n\r\n    # OUR CHANGE IS HERE\r\n    if self.dense_normalizer_fn is not None:\r\n      dense_tensor = self.dense_normalizer_fn(dense_tensor)\r\n```", "I think dense_normalizer_fn is a good idea. Will be happy to review PR", "Ok so it turns out we actually have fixed length sequences. I created a `sequence_dense_numeric_column` that parses a `FixedLenSequenceFeature` and builds a sequence length from a dense tensor. Should I add tests + a PR for this?\r\n\r\nhttps://gist.github.com/jperl/245c414793a5271da72183bada93c55c", "Hi @jperl,\r\nI couldn't understand your use case. Could you please elaborate on it more? Also discussing it as a feature-request, instead of this pr make more sense.", "@ispirmustafa I apologize, this is the wrong forum. Added the FR here https://github.com/tensorflow/tensorflow/issues/20478", "It seems not working properly. Here is a minimal example\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\na = np.reshape(np.arange(12), (3, 4, 1))\r\nwith tf.Session() as sess:\r\n    a_t = tf.constant(a)\r\n    idx = tf.where(tf.not_equal(a_t, 0))\r\n    sparse = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), a_t.get_shape())\r\n    seq_cols = tf.contrib.feature_column.sequence_numeric_column('test', default_value=.0, normalizer_fn=lambda x: (x - 2.) / .3)\r\n    seq_features, sequence_length = tf.contrib.feature_column.sequence_input_layer({'test': sparse}, [seq_cols])\r\n\r\n    b = sess.run(seq_features)\r\n```\r\nThis rises\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-12-137e79726dfc>\", line 9, in <module>\r\n    seq_features, sequence_length = tf.contrib.feature_column.sequence_input_layer({'test': sparse}, [seq_cols])\r\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py\", line 120, in sequence_input_layer\r\n    trainable=trainable)\r\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py\", line 448, in _get_sequence_dense_tensor\r\n    sp_tensor = inputs.get(self)\r\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column.py\", line 2263, in get\r\n    transformed = column._transform_feature(self)  # pylint: disable=protected-access\r\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py\", line 435, in _transform_feature\r\n    input_tensor = self.normalizer_fn(input_tensor)\r\n  File \"<ipython-input-12-137e79726dfc>\", line 8, in <lambda>\r\n    seq_cols = tf.contrib.feature_column.sequence_numeric_column('test', default_value=.0, normalizer_fn=lambda x: (x - 2.) / .3)\r\nTypeError: unsupported operand type(s) for -: 'SparseTensor' and 'float'\r\n```\r\n\r\nI am running tf 1.11.0 on python2.7"]}, {"number": 19648, "title": "kernel_constraint=maxnorm(3) raises error with eager execution", "body": "I posted this already on Stackoverflow and it was suggested this is indeed a bug:\r\nhttps://stackoverflow.com/questions/50594025/how-to-include-kernel-constraints-in-tensorflow-eager-conv2d\r\n\r\nI'm having trouble using `kernel_constraint=maxnorm(3)` within keras when using Tensorflow eager execution. This works fine when not using the standard `Sequential` method outside of eager execution, but seems to fail with an error here (it seems to be because of a multiplication step `*=` which I don't know if there is a substitute for in this context). \r\n\r\n**Question:** Is there a workaround to incorporate the maximum $L^2$ norm functionality within the Eager Tensorflow execution framework? Below are more details.\r\n\r\nHere is how I activate `tensorflow` eager. \r\n\r\n\r\n\r\n    from __future__ import absolute_import, division, print_function\r\n    import tensorflow as tf\r\n    import tensorflow.contrib.eager as tfe\r\n    from keras.datasets import cifar10\r\n    tf.enable_eager_execution()\r\n\r\n\r\n\r\nThe following code works fine\r\n\r\n\r\n**Works:**\r\n\r\n\r\n    class ObjectDet(tf.keras.Model):\r\n        def __init__(self):\r\n            super(ObjectDet,self).__init__()\r\n            self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\r\n            self.layer2=tf.keras.layers.Dropout(0.2)\r\n            self.layer3=tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')\r\n            self.layer4=tf.keras.layers.MaxPooling2D(pool_size=(2,2))\r\n            self.layer5=tf.keras.layers.Flatten()\r\n            self.layer6=tf.keras.layers.Dense(512, activation='relu')\r\n            self.layer7=tf.keras.layers.Dropout(0.1)\r\n            self.layer8=tf.keras.layers.Dense(10, activation='softmax')\r\n\r\n        def call(self, input):\r\n            \"\"\"Run the model.\"\"\"\r\n            result = self.layer1(input)\r\n            result = self.layer2(result)\r\n            result = self.layer3(result)\r\n            result = self.layer4(result)\r\n            result = self.layer5(result)\r\n            result = self.layer6(result)\r\n            result = self.layer7(result)\r\n            result = self.layer8(result)\r\n        \r\n            return result\r\n\r\n\r\n\r\n    def loss(model, x, y):\r\n      prediction = model(x)\r\n      return cross_entropy(prediction,y)\r\n    \r\n    def grad(model, inputs, targets):\r\n      with tf.GradientTape() as tape:\r\n        loss_value = loss(model, inputs, targets)\r\n      return tape.gradient(loss_value, model.variables)\r\n    \r\n    \r\n    x, y = iter(train_ds).next()\r\n    print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\r\n    \r\n    # Training loop\r\n    for (i, (x, y)) in enumerate(train_ds):\r\n      # Calculate derivatives of the input function with respect to its parameters.\r\n      grads = grad(model, x, y)\r\n      # Apply the gradient to the model\r\n      \r\n      optimizer.apply_gradients(zip(grads, model.variables),\r\n                                global_step=tf.train.get_or_create_global_step())\r\n      if i % 200 == 0:\r\n        pass\r\n        print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\r\n\r\n\r\n\r\n**Does not work:**\r\n\r\nIf I replace \r\n\r\n    self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\r\n\r\nwith\r\n\r\n    self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu',kernel_constraint=maxnorm(3))\r\n\r\nI obtain the error:\r\n\r\n\r\n    RuntimeErrorTraceback (most recent call last)\r\n    <ipython-input-74-629273c4a534> in <module>()\r\n         19 \r\n         20   optimizer.apply_gradients(zip(grads, model.variables),\r\n    ---> 21                             global_step=tf.train.get_or_create_global_step())\r\n         22   if i % 200 == 0:\r\n         23     pass\r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\r\n        615           scope_name = var.op.name\r\n        616         with ops.name_scope(\"update_\" + scope_name), ops.colocate_with(var):\r\n    --> 617           update_ops.append(processor.update_op(self, grad))\r\n        618       if global_step is None:\r\n        619         apply_updates = self._finish(update_ops, name)\r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in update_op(self, optimizer, g)\r\n        166     if self._v.constraint is not None:\r\n        167       with ops.control_dependencies([update_op]):\r\n    --> 168         return self._v.assign(self._v.constraint(self._v))\r\n        169     else:\r\n        170       return update_op\r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/keras/constraints.pyc in __call__(self, w)\r\n         51         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\r\n         52         desired = K.clip(norms, 0, self.max_value)\r\n    ---> 53         w *= (desired / (K.epsilon() + norms))\r\n         54         return w\r\n         55 \r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.pyc in __imul__(self, unused_other)\r\n        931 \r\n        932   def __imul__(self, unused_other):\r\n    --> 933     raise RuntimeError(\"Variable *= value not supported. Use \"\r\n        934                        \"variable.assign_mul(value) to modify the variable \"\r\n        935                        \"value and variable = variable * value to get a new \"\r\n    \r\n    RuntimeError: Variable *= value not supported. Use variable.assign_mul(value) to modify the variable value and variable = variable * value to get a new Tensor object.\r\n\r\n\r\nThanks!\r\n\r\n", "comments": ["Thanks for the report!\r\n\r\nWhich version of TensorFlow are you using? I suspect 1.7.\r\nThis should have been fixed in 1.8 via [this commit](https://github.com/tensorflow/tensorflow/commit/255712a4544a404177d75d492713c0267f874f60#diff-4a7c925d693473a6751c940335523e14).\r\n\r\nLet us know if that is not the case. Thanks!", "Oh wait, from the stack trace I see you're using `keras.max_norm` instead of `tf.keras.max_norm`. Is that accurate? Could you switch to `tf.keras.max_norm` instead?\r\n\r\nNote that the Keras implementation in `tf.keras` includes features (such as compatibility with eager execution) that are not part of the `keras` package.", "Thanks! I changed my code to\r\n\r\n```\r\nself.layer3=tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same',kernel_constraint=tf.keras.constraints.max_norm(3))\r\n```\r\n\r\nAnd now this works. I'm closing the issue. PS: I am using TensorFlow 1.8."]}, {"number": 19647, "title": "TensorFlowInferenceInterface(\u00a0)\u00a0 in TensorFlowInferenceInterface\u00a0cannot be applied to (android.content.res.AssetManager, java", "body": "package com.technicalshow.siraj.models;\r\n\r\n\r\n//Provides access to an application's raw asset files;\r\nimport android.content.res.AssetManager;\r\n//Reads text from a character-input stream, buffering characters so as to provide for the efficient reading of characters, arrays, and lines.\r\nimport java.io.BufferedReader;\r\n//for erros\r\nimport java.io.IOException;\r\n//An InputStreamReader is a bridge from byte streams to character streams:\r\n// //It reads bytes and decodes them into characters using a specified charset.\r\n// //The charset that it uses may be specified by name or may be given explicitly, or the platform's default charset may be accepted.\r\nimport java.io.InputStreamReader;\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\nimport java.lang.String;\r\n//made by google, used as the window between android and tensorflow native C++\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n\r\n/**\r\n * Changed from https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/blob/master\r\n * /app/src/main/java/com/mindorks/tensorflowexample/TensorFlowImageClassifier.java\r\n * Created by marianne-linhares on 20/04/17.\r\n */\r\n\r\n//lets create this classifer\r\npublic class TensorFlowClassifier implements Classifier {\r\n\r\n    // Only returns if at least this confidence\r\n    //must be a classification percetnage greater than this\r\n    private static final float THRESHOLD = 0.1f;\r\n\r\n    private TensorFlowInferenceInterface tfHelper;\r\n\r\n    private String name;\r\n    private String inputName;\r\n    private String outputName;\r\n    private int inputSize;\r\n    private boolean feedKeepProb;\r\n\r\n    private List<String> labels;\r\n    private float[] output;\r\n    private String[] outputNames;\r\n\r\n    //given a saved drawn model, lets read all the classification labels that are\r\n    //stored and write them to our in memory labels list\r\n    private static List<String> readLabels(AssetManager am, String fileName) throws IOException {\r\n        BufferedReader br = new BufferedReader(new InputStreamReader(am.open(fileName)));\r\n\r\n        String line;\r\n        List<String> labels = new ArrayList<>();\r\n        while ((line = br.readLine()) != null) {\r\n            labels.add(line);\r\n        }\r\n\r\n        br.close();\r\n        return labels;\r\n    }\r\n\r\n   //given a model, its label file, and its metadata\r\n    //fill out a classifier object with all the necessary\r\n    //metadata including output prediction\r\n    public static TensorFlowClassifier create(AssetManager assetManager, String name,\r\n            String modelPath, String labelFile, int inputSize, String inputName, String outputName,\r\n            boolean feedKeepProb) throws IOException {\r\n        //intialize a classifier\r\n        TensorFlowClassifier c = new TensorFlowClassifier();\r\n\r\n        //store its name, input and output labels\r\n        c.name = name;\r\n\r\n        c.inputName = inputName;\r\n        c.outputName = outputName;\r\n\r\n        //read labels for label file\r\n        c.labels = readLabels(assetManager, labelFile);\r\n\r\n        //set its model path and where the raw asset files are\r\n        c.tfHelper = new TensorFlowInferenceInterface( assetManager, modelPath);\r\n        int numClasses = 10;\r\n\r\n        //how big is the input?\r\n        c.inputSize = inputSize;\r\n\r\n        // Pre-allocate buffer.\r\n        c.outputNames = new String[] { outputName };\r\n\r\n        c.outputName = outputName;\r\n        c.output = new float[numClasses];\r\n\r\n        c.feedKeepProb = feedKeepProb;\r\n\r\n        return c;\r\n    }\r\n\r\n    @Override\r\n    public String name() {\r\n        return name;\r\n    }\r\n\r\n    @Override\r\n    public Classification recognize(final float[] pixels) {\r\n\r\n        //using the interface\r\n        //give it the input name, raw pixels from the drawing,\r\n        //input size\r\n        tfHelper.addFeed(inputName, pixels, 1, inputSize, inputSize, 1);\r\n\r\n        //probabilities\r\n        if (feedKeepProb) {\r\n            tfHelper.addFeed(\"keep_prob\", new float[] { 1 });\r\n        }\r\n        //get the possible outputs\r\n        tfHelper.run(outputNames);\r\n\r\n        //get the output\r\n        tfHelper.fetch(outputName, output);\r\n\r\n        // Find the best classification\r\n        //for each output prediction\r\n        //if its above the threshold for accuracy we predefined\r\n        //write it out to the view\r\n        Classification ans = new Classification();\r\n        for (int i = 0; i < output.length; ++i) {\r\n            System.out.println(output[i]);\r\n            System.out.println(labels.get(i));\r\n            if (output[i] > THRESHOLD && output[i] > ans.getConf()) {\r\n                ans.update(output[i], labels.get(i));\r\n            }\r\n        }\r\n\r\n        return ans;\r\n    }\r\n}\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 17 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 32 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19646, "title": "macOS build fail on __shared__ variable", "body": "- **Code changes**:\r\nFixes from https://github.com/tensorflow/tensorflow/issues/17067#issuecomment-366544970 .\r\nFix from https://github.com/tensorflow/tensorflow/issues/18564#issue-314731167\r\nFix from https://github.com/tensorflow/tensorflow/issues/14174#issuecomment-342163130\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS High Sierra 10.13.4 (17E202)\r\nXCode 7.2.1, XCode 8.2\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nDuring compilation from sources\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.6.0, 1.7.1\r\n\r\n- **Python version**: \r\n3.6.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.13.1-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed May 23 16:57:59 2018 (1527094679)\r\nBuild timestamp: 1527094679\r\nBuild timestamp as int: 1527094679\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nXCode 7.2.1\r\nApple LLVM version 7.0.2 (clang-700.1.81)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.2\r\ncudNN 7.1.4\r\n\r\n- **GPU model and memory**:\r\nGeForce 1080Ti 11Gb\r\n\r\n- **Exact command to reproduce**:\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nDuring compilation from sources, an errorr occurs about __shared__ variable expression used. \r\n\r\n### Source code / logs\r\n`INFO: From Compiling tensorflow/core/kernels/reduction_ops_gpu_complex64.cu.cc:\r\n./tensorflow/core/lib/core/status.h(32): warning: attribute \"warn_unused_result\" does not apply here\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h(187): warning: missing return statement at end of non-void function \"Eigen::internal::device::numeric_limits<T>::quiet_NaN [with T=std::__1::complex<float>]\"\r\n          detected during:\r\n            instantiation of \"T Eigen::internal::device::numeric_limits<T>::quiet_NaN() [with T=std::__1::complex<float>]\" \r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h(180): here\r\n            instantiation of \"T Eigen::GenericNumTraits<T>::quiet_NaN() [with T=std::__1::complex<float>]\" \r\n./tensorflow/core/kernels/reduction_ops.h(59): here\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h(180): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n          detected during instantiation of \"T Eigen::GenericNumTraits<T>::quiet_NaN() [with T=std::__1::complex<float>]\" \r\n./tensorflow/core/kernels/reduction_ops.h(59): here\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h(187): warning: missing return statement at end of non-void function \"Eigen::internal::device::numeric_limits<T>::quiet_NaN [with T=std::__1::complex<double>]\"\r\n          detected during:\r\n            instantiation of \"T Eigen::internal::device::numeric_limits<T>::quiet_NaN() [with T=std::__1::complex<double>]\" \r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h(180): here\r\n            instantiation of \"T Eigen::GenericNumTraits<T>::quiet_NaN() [with T=std::__1::complex<double>]\" \r\n./tensorflow/core/kernels/reduction_ops.h(60): here\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h(180): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n          detected during instantiation of \"T Eigen::GenericNumTraits<T>::quiet_NaN() [with T=std::__1::complex<double>]\" \r\n./tensorflow/core/kernels/reduction_ops.h(60): here\r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: initializer not allowed for __shared__ variable \r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(319): error: initializer not allowed for __shared__ variable \r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: initializer not allowed for __shared__ variable \r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(319): error: initializer not allowed for __shared__ variable \r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(271): error: initializer not allowed for __shared__ variable \r\n\r\n./tensorflow/core/kernels/reduction_gpu_kernels.cu.h(319): error: initializer not allowed for __shared__ variable \r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h(180): warning: calling a __host__ function(\"Eigen::internal::device::numeric_limits< ::std::__1::complex<float> > ::quiet_NaN\") from a __host__ __device__ function(\"Eigen::GenericNumTraits< ::std::__1::complex<float> > ::quiet_NaN\") is not allowed\r\n\r\n6 errors detected in the compilation of \"/var/folders/by/bnpbnnbj2m180m_vjq9l97n40000gn/T//tmpxft_00009e05_00000000-6_reduction_ops_gpu_complex64.cu.cpp1.ii\".\r\nERROR: tensorflow/tensorflow/core/kernels/BUILD:2796:1: output 'tensorflow/core/kernels/_objs/reduction_ops_gpu/tensorflow/core/kernels/reduction_ops_gpu_complex64.cu.pic.o' was not created\r\nERROR: tensorflow/tensorflow/core/kernels/BUILD:2796:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 486.544s, Critical Path: 212.55s\r\nINFO: 595 processes, local.\r\nFAILED: Build did NOT complete successfully`\r\n", "comments": ["I also have this exact problem, but I'm trying to run on ubuntu 16.04.\r\nPlease, if you fix it, let me know!", "@luizzanlorensi I managed to compile v1.7.1 by downgrading to CUDA9.1+cudnn7, so I think this is CUDA-wide issue so far.", "@yarrseni Thanks!\r\nI downgrading to cuda 9.0, then installed tensorflow 1.8 via pip, now everything is working!", "@yarrseni How did you downgrade to CUDA9.1? I thought only CUDA 9.2 supports macOS High Sierra 10.13.4 (17E202). What is your macOS / nVidia display driver / CUDA driver combination?", "@fqx I just installed the cuda from here [https://developer.nvidia.com/cuda-91-download-archive?target_os=MacOSX&target_arch=x86_64&target_version=1013](CUDA9.1 for mac). I think the support for CUDAs is for second (10.**13**) part of version, so 10.13.3 or 10.13.4 does not matter (and it works).\r\n\r\nTo downgrade, follow the instructions to uninstall CUDA9.2 (it should be somewhere in the docs, it involves running a perl script that is created during installation basically).\r\nThen, install CUDA9.1. Install cudnn7.0.5 for CUDA9.1 (somewhere in archived versions).\r\nI had to update CUDA driver using system dialogue because CUDA installation installed some old version (current version is 396.64).\r\nGPU driver is 387.10.10.10.30.107 .\r\nSystem is 10.13.4 (17E202).\r\n\r\nI managed to compile TF with XCode_9.2 after the downgrade, but supported version is XCode_7.2, so for clearer run you might use the latter one.", "@yarrseni Thank you for your reply! I could finally use wheels from this site https://j74th.com/tensorflow/\r\nmacOS tensorflow with eGPU users were usually overlooked by official communities, solutions are hard to find. I encourage you share your compiled wheels on GitHub.", "Closing as this issue seems resolved. Please reopen if there is still a problem.", "I have the same problem but I have not the permission to downgrade CUDA because I'm working on the Cluster from the university and they have their own version of cuda\r\nPython 3.6.8 Anaconda\r\nCUDA/cuDNN version:\r\nCUDA 10\r\ncudNN 7\r\nAny solution please ?", "> I have the same problem but I have not the permission to downgrade CUDA because I'm working on the Cluster from the university and they have their own version of cuda\r\n> Python 3.6.8 Anaconda\r\n> CUDA/cuDNN version:\r\n> CUDA 10\r\n> cudNN 7\r\n> Any solution please ?\r\n\r\nIn reduction_gpu_kernels.cu.h(271) and reduction_gpu_kernels.cu.h(319), change \r\n```c++\r\n__shared__ value_type partial_sums[32 * 33];\r\n```\r\nto\r\n```c++\r\n  __shared__ __align__(alignof(value_type)) char\r\n      partial_sums_raw[32 * (32 + 1) *\r\n                       sizeof(value_type)];\r\n  value_type* partial_sums = reinterpret_cast<value_type*>(partial_sums_raw);\r\n```"]}, {"number": 19644, "title": "Feature Request: 1 bit quantization in FakeQuantWithMinMaxVars ", "body": "Feature Request: 1 bit quantization in FakeQuantWithMinMaxVars \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: Does not apply\r\n- **GCC/Compiler version (if compiling from source)**: Does not apply\r\n- **CUDA/cuDNN version**: Does not apply\r\n- **GPU model and memory**: Does not apply\r\n- **Exact command to reproduce**: Does not apply\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nBinary NN are raising the attention of researchers/developers.\r\nRight now `fake_quant `operations do support quantization in the range 2-16 bits. It would be really usefull extending this (relying on FakeQuantWithMinMaxVars) to 1b quantization.\r\nIf so, `Quantize` methods (such as  `tf.contrib.quantize.create_eval_graph`) should be updated to include this new quantization limit (now  `tf.contrib.quantize.experimental_create_eval_graph` supports bit selection for quantization).\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Dear @jart, tell me if I may be of help.\r\nRegards", "@raghuraman-k , could you please respond to this issue.", "Nagging Assignee @raghuraman-k: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@fgr1986 Thanks for bringing this to our attention. We will take this into consideration. Currently `tf.contrib.quantize.experimental_create_eval_graph` include options that may have undefined behavior and is still being researched. `tf.contrib.quantize.create_eval_graph` have better hardware support and will be more suitable for users aiming to work with graphs on backends, e.g. tflite.\r\n\r\n"]}, {"number": 19643, "title": "keras.layers.BatchNormalization update_ops not added", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-1660-ga543d94\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.3\r\n- **CUDA/cuDNN version**: 9.1 / 7.1\r\n- **GPU model and memory**: GeForce GTX 1070 , 8119MiB\r\n- **Exact command to reproduce**: See source code below\r\n\r\n### Describe the problem\r\n`tf.keras.layers.BatchNormalization()` does not add any operations to `UPDATE_OPS` under certain version constraints.\r\n\r\n### Source code / logs\r\n```python\r\n#!/usr/bin/python\r\nimport tensorflow as tf\r\n\r\ngraph = tf.get_default_graph()\r\ntf.keras.backend.set_learning_phase(True)\r\nfeatures = tf.zeros(shape=(3, 10), dtype=tf.float32)\r\nnormed = tf.keras.layers.BatchNormalization()(features)\r\n\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nprint('n_ops:        %d' % len(graph.get_operations()))\r\nprint('n_update_ops: %d' % len(update_ops))\r\n```\r\nOutput using v1.8.0-1660-ga543d94, cuda 9.1, cudnn 7.1, installed from source (other details as above):\r\n```\r\nn_ops:        51\r\nn_update_ops: 0\r\n```\r\nOutput using v1.8.0-0-g93bc2e2072, cuda 9.0, cudnn 7.0, pip installed\r\n```\r\nn_ops:        41\r\nn_update_ops: 2\r\n```", "comments": ["Same behaviour with freshly pulled master build,  `tf.GIT_VERSION == v1.8.0-1660-ga543d94` (51 ops, 0 update ops).", "Have you seen https://github.com/tensorflow/tensorflow/issues/16102?", "Sounds similar, but I'm unsure if it's related. That's based on tf 1.4.1, but given that things work as expected in some versions of 1.8 (and from what I remember, 1.7) I'd be confused if they were the same. Are the keras versions of tensorflow shipped differently somehow? i.e. could we be using the same keras implementations under-the-hood despite different tf versions? Or has there been a recent rollback on certain aspects of the code base which could undo a bug fix?\r\n\r\nJust confirmed that it's unlikely anything to do with CUDA/cudnn (don't see how it could have, but... you never know). Same behaviour (51 ops, 0 control ops) with 9.0 / 7.0 / v1.8.0-2594-g25b2f01", "That is by design. Global collections (and global states in general) are prone to various issues (like most global variables in software engineering) and should be avoided.\r\n\r\nYou can retrieve the updates created by your BatchNormalization layer via:\r\n\r\n```python\r\nupdates = layer.updates\r\n```\r\n\r\nAdditionally you can retrieve the aggregated updates of all stateful layers in a Keras model via:\r\n\r\n```python\r\nupdates = model.updates\r\n```\r\n\r\nIf you are writing your own custom training loops, you will have to run these updates as part of the call to `sess.run()`. But do note that tf.keras provides a built-in training loop in the form of `model.fit()`, as well as primitive methods for building your own custom training loops (in particular `model.train_on_batch()`  and `model.predict_on_batch()`). If you are using Keras you should generally never have to manually manage your model's updates.", "... ... ... you're obviously entitled to make design choices such as this, but for what it's worth: I think this is a terrible decision. I'm no fan of the `collections` system used by tensorflow, but changing it in this subpackage is:\r\n* inconsistent with the rest of tensorflow;\r\n* inconsistent with previous `tf.keras` versions - even within 1.8.0; and\r\n* inconsistent with other usages of collections by `tf.keras.layers` (see code below)\r\n```python\r\nimport tensorflow as tf\r\nx = tf.zeros(shape=(2, 3), dtype=tf.float32)\r\ny = tf.keras.layers.Dense(4)(x)\r\nprint('variables: %d' % len(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)))\r\n```\r\n\r\nI can appreciate as a separate package it may be convenient to force people to do things only 'the keras way', and I'd be less opposed if this issue was in the main keras repository. However, keras exists within tensorflow as well as external to it, so I'd expect it to follow its conventions. This change is breaking for anyone who uses non-keras session management (e.g. estimators) - myself included. I'd love to be able to mix and match tensorflow and keras - mostly to use other people's code, and take advantage of the excellent `tf.keras.applications` - but decisions like this make me more and more inclined to purge any reference to it, since there seems to be no guarantee about forward compatibility.", "@jackd You have mentioned estimators. Do you think it is involved also in https://github.com/tensorflow/tensorflow/issues/17950?", "If this is a conscious design choice as mentioned above, I highly doubt the `estimator` framework will cause the update ops to run, so this should probably be dealt with by `keras`'s own mechanism during conversion. As far as I can tell, it isn't.\r\n\r\nI'm not overly familiar with the keras code base but I can't find any explicit reference to `model.updates`, and the place where I suspect they're meant to be collected in they just aren't.\r\n\r\nfrom `keras/engine/training.py`, `Model._make_train_function`:\r\n```python\r\n        with K.name_scope(self.optimizer.__class__.__name__):\r\n          # Training updates\r\n          updates = self.optimizer.get_updates(\r\n              params=self._collected_trainable_weights, loss=self.total_loss)\r\n        # Unconditional updates\r\n        updates += self.get_updates_for(None)\r\n        # Conditional updates relevant to this model\r\n        updates += self.get_updates_for(self._feed_inputs)\r\n        # Stateful metrics updates\r\n        updates += self.metrics_updates\r\n        # Gets loss and metrics. Updates weights at each call.\r\n````\r\n\r\nIt's not an unconditional update (see below), and I doubt its a metric update or an update required for feed inputs. They're just not added to update_ops.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.backend.set_learning_phase(True)\r\ninput_shape = (3,)\r\ninp = tf.keras.Input(shape=input_shape, dtype=tf.float32)\r\nx = tf.keras.layers.Dense(4, input_shape=input_shape)(inp)\r\nx = tf.keras.layers.BatchNormalization()(x, training=True)\r\n\r\nmodel = tf.keras.Model(inp, x)\r\nmodel.compile(tf.train.AdamOptimizer(1e-3), 'mean_squared_error')\r\nprint('model_updates: %d' % len(model.updates))\r\nfor update in model.updates:\r\n    print(update.name, update._unconditional_update)  # False\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(model)\r\n\r\nz = tf.zeros((2, 3), dtype=tf.float32)\r\nlabels = tf.zeros((2, 4), dtype=tf.float32)\r\n\r\nspec = estimator.model_fn(z, labels, mode='train', config=None)\r\n\r\nprint(len(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))  # 0\r\n```", "@jackd Yes I have the same suspect that it was related to the converter API not handling update_ops correctly. This now it is closed but https://github.com/tensorflow/tensorflow/issues/17950 is still open. I've also added there a reference to the upstream https://github.com/keras-team/keras/issues/9214. If you want subscribe to the https://github.com/tensorflow/tensorflow/issues/17950 thread.", "@jackd the update ops should be created through conditional updates from _feed_inputs (correct me if I'm wrong)", "> You can retrieve the updates created by your BatchNormalization layer via:\r\n> \r\n> ```python\r\n> updates = layer.updates\r\n> ```\r\n\r\n@fchollet Design philosophy aside, I'm not sure how this works. Under 1.12, the following code prints the empty list for the first `print` and raises an `AttributeError` on the second print, because a `Tensor` does not have an `updates` attribute. According to your prescribed approach, I should be getting a non-empty list of update ops from `bn.updates`. How's it supposed to work? How should the code be modified?\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as kl\r\n\r\ndef main():\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        layer = tf.placeholder(name='x', shape=(4, 4), dtype=tf.float32)\r\n        bn = kl.BatchNormalization()\r\n        print(bn.updates)\r\n        layer = bn(layer, training=True)\r\n        print(layer.updates)\r\n\r\nmain()\r\n```", "@jchia change `print(layer.updates)` to `print(bn.updates)` :)", "> @jchia change `print(layer.updates)` to `print(bn.updates)` :)\r\n\r\nThanks, that works."]}, {"number": 19642, "title": "Linking issues with tensorflow-lite.a with android-studio and CMake.", "body": "### System information\r\n- **Have I written custom code:**\r\n- **OS Platform and Distribution ( )**:\r\nLinux Ubuntu 16.04 / Android\r\n- **TensorFlow installed from:**\r\n source\r\n\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.13\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNDK16b \r\n\r\n### Describe the problem\r\nI am having problems linking libtensorflow-lite.a with CMake and Android Studio. \r\nIt seems that the tf-lite library is not compatible with my architecture, but I do not know why this is happening. \r\nI have succesfully compiled the libtensorflow-lite.a library, following these steps: \r\n\r\n```\r\nexport NDK_ROOT=<your_ndk>\r\ntensorflow/contrib/lite/download_dependencies.sh\r\nmake -f tensorflow/contrib/lite/Makefile TARGET=ANDROID ANDROID_ARCH=armeabi-v7a\r\n```\r\nThe problem comes when I try to link the library with CMake. it seems that even I compiled the armeabi-v7a the linker, I have these errors: \r\n\r\n>  incompatible target\r\n\r\n>  the vtable symbol may be undefined because the class is missing its key function\r\n\r\n### Source code / logs\r\nThis my error log: \r\n```\r\nBuild command failed.\r\nError while executing process /home/VICOMTECH/uelordi/Android/Sdk/cmake/3.6.4111459/bin/cmake with arguments {--build /home/VICOMTECH/uelordi/projects/SwitchTrackingEngine/kanvas_tracker/kt/tools/Deep-learning-tools/android/tfliteTest/app/.externalNativeBuild/cmake/debug/armeabi-v7a --target native-lib}\r\n[1/1] Linking CXX shared library ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so\r\nFAILED: : && /home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++  --target=armv7-none-linux-androideabi --gcc-toolchain=/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 --sysroot=/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/sysroot -fPIC -isystem /home/VICOMTECH/uelordi/SDK/android-ndk-r16b/sysroot/usr/include/arm-linux-androideabi -D__ANDROID_API__=21 -g -DANDROID -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -fno-integrated-as -mthumb -Wa,--noexecstack -Wformat -Werror=format-security  -std=c++11 -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a --sysroot /home/VICOMTECH/uelordi/SDK/android-ndk-r16b/platforms/android-21/arch-arm -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--fix-cortex-a8 -Wl,--no-undefined -Wl,-z,noexecstack -Qunused-arguments -Wl,-z,relro -Wl,-z,now -shared -Wl,-soname,libnative-lib.so -o ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so CMakeFiles/native-lib.dir/src/main/cpp/native-lib.cpp.o  /home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a /home/VICOMTECH/uelordi/SDK/android-ndk-r16b/platforms/android-21/arch-arm/usr/lib/liblog.so -latomic -lm \"/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a\" && :\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: /home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a(error_reporter.o): incompatible target\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: /home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a(interpreter.o): incompatible target\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: /home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a(register.o): incompatible target\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: /home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a(model.o): incompatible target\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: /home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/gen/lib/libtensorflow-lite.a(op_resolver.o): incompatible target\r\n/home/VICOMTECH/uelordi/projects/SwitchTrackingEngine/kanvas_tracker/kt/tools/Deep-learning-tools/android/tfliteTest/app/src/main/cpp/native-lib.cpp:23: error: undefined reference to 'tflite::DefaultErrorReporter()'\r\n/home/VICOMTECH/uelordi/projects/SwitchTrackingEngine/kanvas_tracker/kt/tools/Deep-learning-tools/android/tfliteTest/app/src/main/cpp/native-lib.cpp:23: error: undefined reference to 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n/home/VICOMTECH/uelordi/projects/SwitchTrackingEngine/kanvas_tracker/kt/tools/Deep-learning-tools/android/tfliteTest/app/src/main/cpp/native-lib.cpp:27: error: undefined reference to 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/home/VICOMTECH/uelordi/projects/SwitchTrackingEngine/kanvas_tracker/kt/tools/Deep-learning-tools/android/tfliteTest/app/src/main/cpp/native-lib.cpp:28: error: undefined reference to 'tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\n/home/VICOMTECH/uelordi/projects/SwitchTrackingEngine/kanvas_tracker/kt/tools/Deep-learning-tools/android/tfliteTest/app/src/main/cpp/native-lib.cpp:30: error: undefined reference to 'tflite::InterpreterBuilder::operator()(std::unique_ptr<tflite::Interpreter, std::default_delete<tflite::Interpreter> >*)'\r\n/home/VICOMTECH/uelordi/SDK/tensorflow/tensorflow/contrib/lite/op_resolver.h:70: error: undefined reference to 'vtable for tflite::MutableOpResolver'\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/unique_ptr.h:76: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'\r\n/home/VICOMTECH/uelordi/SDK/android-ndk-r16b/sources/cxx-stl/gnu-libstdc++/4.9/include/bits/unique_ptr.h:76: error: undefined reference to 'tflite::Interpreter::~Interpreter()'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\r\n```\r\n\r\n\r\nMy CmakeScript: \r\n```\r\n# For more information about using CMake with Android Studio, read the\r\n# documentation: https://d.android.com/studio/projects/add-native-code.html\r\n\r\n# Sets the minimum version of CMake required to build the native library.\r\n`\r\nSET(PROJECT_NAME  native-lib)\r\ncmake_minimum_required(VERSION 3.4.1)\r\nset(PREBUILT_DIR ${TENSORFLOW_ROOT_DIR}/tensorflow/contrib/lite)\r\n\r\nadd_library(lib_tflite STATIC IMPORTED )\r\nset_target_properties(lib_tflite PROPERTIES IMPORTED_LOCATION\r\n${PREBUILT_DIR}/gen/lib/libtensorflow-lite.a)\r\n\r\nINCLUDE_DIRECTORIES(${TENSORFLOW_ROOT_DIR}\r\n    ${PREBUILT_DIR}/downloads/flatbuffers/include)\r\n\r\nadd_library( # Sets the name of the library.\r\n            ${PROJECT_NAME}\r\n             SHARED\r\n             src/main/cpp/native-lib.cpp )\r\n\r\nfind_library( log-lib\r\n              log )\r\n\r\n\r\ntarget_link_libraries( # Specifies the target library.\r\n                       ${PROJECT_NAME}\r\n                       lib_tflite\r\n                       ${log-lib} )\r\n```\r\n\r\n\r\n\r\nMy build_gradle (I have only copy the relevant part)\r\n```\r\nandroid {\r\n    compileSdkVersion 27\r\n    defaultConfig {\r\n        applicationId \"com.androidapp.kt.kanvas.tflitetest\"\r\n        minSdkVersion 21\r\n        targetSdkVersion 27\r\n        versionCode 1\r\n        versionName \"1.0\"\r\n        testInstrumentationRunner \"android.support.test.runner.AndroidJUnitRunner\"\r\n        externalNativeBuild {\r\n            cmake {\r\n                cppFlags \"-std=c++11\"\r\n                arguments \"-DTENSORFLOW_ROOT_DIR=\" + getTensorflowDir().toString()\r\n                abiFilters  'armeabi-v7a' \r\n            }\r\n        }\r\n    }\r\n```\r\n\r\n", "comments": ["/CC @aselle", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", " @uelordi01 Have you resolved this issue? I also meet the similar problem.", "Unfortunately not, but I think my error is related to the architecture mismatch, actually when I tried to make objdump in the tf-lite static library it says that is x86-64 architecture.\r\n\r\nI think this `make -f tensorflow/contrib/lite/Makefile TARGET=ANDROID ANDROID_ARCH=armeabi-v7a` is it does not really take into account the android architecture.\r\n\r\nFurthermore I tried to use this issue explanations:\r\nhttps://github.com/tensorflow/tensorflow/issues/14688\r\n to build armv7 target abi with no success.\r\n\r\nSo it seems that we need an android toolchain to say to gcc or clang compiler the correct compilation flgs.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@uelordi01 any reason you can use the bazel build?", "@uelordi01 : Let me know if you are still having issues, reopen the issue.", "hi, have you solved the \"incompatible target\" problem ?", "Currently I am able to compile the tensorflow-lite.a through bazel, and also I can build shared lib adding in tese lines to build_file:\r\n```\r\ncc_binary(\r\nname = \"libtensorflowLite.so\",\r\n\r\nlinkopts=[\"-shared\", \"-Wl\"],\r\nlinkshared=1,\r\n\r\ncopts = tflite_copts(),\r\ndeps = [\r\n    \":framework\",\r\n    \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n],\r\n```", "@uelordi01   hi, follow your instruction, I get a libtensorflowLite.so and run it on mobile phone.  But the size is nearly 37M, is it normal ? ", "My tensorflow-lite library size is 34.7 MB, so I guess depending on the version of the tensorflow-lite is the reason that you have 37 MB size library.\r\nProbably if you link a static library, you will reduce your final library size. But I couln't compile for android ndk, and now I am not on it.", "@uelordi01   yes, now I have integrate th dynamic library in my android project. ", "@zhyj3038  Hi, how can you use the libtensorflowLite.so in an  android project? which head files do you include \uff1f", "@holyhao   the whole tflite folder", "@zhyj3038 Thans for your reply, i have some trouble about compiling to the dynamic library. If any possible, would you please share it ?", "just add\r\n\r\ncc_binary(\r\nname = \"libtensorflowLite.so\",\r\n\r\nlinkopts=[\"-shared\", \"-Wl\"],\r\nlinkshared=1,\r\n\r\ncopts = tflite_copts(),\r\ndeps = [\r\n    \":framework\",\r\n    \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n],\r\n\r\nto any positon of tensorflow's BUILD file. the BUILD file is in tensorflow's root folder", "@zhyj3038 Because of some proxy problem , i failed compiling with bazel .\r\nThe following are the error message, do you have some idea about this?:\r\n  Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': Error downloading [https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz, https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz] to /home/haolei/.cache/bazel/_bazel_haolei/3a2d9699e4c676169595171b9b4d83da/external/io_bazel_rules_closure/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz: All mirrors are down: []\r\n", "@uelordi01 hi, I get a libtensorflowLite.so sucessfully, But the size is almost 40MB. I know that the offical libtensorflowlite_jni.so only 2.04MB, how could it be so different,  do you have some idea?", "I cannot assure, but this can be due to the lite jni library could work as a wrapper to make calls to tensorflow-lite core functions, so in concluision, probably in this library are placed only wrapper functions. \r\n\r\nAnother idea could be because  the jni lit library linking could be done through static library where this library would have had only the operations that the jni library. \r\nHowever, I think is the first option.", "@uelordi01, hi, I have the same problem when using `make` the resulting library appear to be for `x86-64`. \r\nOn 19 Sep You said :\r\n> Currently I am able to compile the tensorflow-lite.a through bazel \r\n\r\nWould you mind sharing this ? :) (I'm able to build a shared lib but due to external limitations I need a static lib).\r\n\r\n"]}, {"number": 19641, "title": "tflite::Interpreter::Invoke() changes input data if call multiple times", "body": "OS Platform and Distribution: Android Native C, Android API 26\r\nTensorFlow installed from: Compiled from the official github master branch\r\nTensorFlow version: Master\r\nBazel versio: 0.13.0-homebrew\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nCode below:\r\n\r\n```\r\nint input = interpreter->inputs()[0];\r\nfloat* input_to_model = interpreter->typed_tensor<float>(input);\r\n//put data into input_to_model here\r\n\r\nfor (int i = 0; i < loop_count; i++) {\r\n        if (interpreter->Invoke() != kTfLiteOk) {\r\n            __android_log_print(ANDROID_LOG_INFO, \"Invoke\", \"ailed to invoke tflite!\");\r\n        }\r\n}\r\n```\r\ninput_to_model content will be changed when Invoke() is called. Thus only the first time the output is correct, the second to tenth times results are wrong.\r\n", "comments": ["Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe this is resolved here: \r\nhttps://github.com/tensorflow/tensorflow/commit/85c518b8d306204cd7111f321a4b7b204fc554f4\r\nThis is not yet available in the main release, but is available on nightly builds and from master builds.", "Are you sure?\r\n\r\nI tested it with `examples/label_image`, passing `-c 2`, to make it call `Invoke()` twice, and I still observe the issue."]}, {"number": 19639, "title": "Core ML in android", "body": "is there any way to use Ios coreML file in android for object detaction?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 33 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19638, "title": "Update WORKSPACE", "body": "there was a typo in the comment, also made a sentence out of it.", "comments": []}, {"number": 19637, "title": "Fix for Raspberry Pi wheel architecture tags", "body": "", "comments": []}]