[{"number": 7968, "title": "TypeError: __init__() got an unexpected keyword argument 'state_is_tuple' on GRU or RNN", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nCompile error on RNN or GRU models.\r\n\r\n### Environment info\r\nOperating System:\r\n> # lsb_release -a\r\n> No LSB modules are available.\r\n> Distributor ID:\tUbuntu\r\n> Description:\tUbuntu 16.10\r\n> Release:\t16.10\r\n> Codename:\tyakkety\r\n\r\nInstalled version of CUDA and cuDNN: \r\n> # ls -l /opt/cuda/lib64/libcud*\r\n> lrwxrwxrwx 1 ubuntu users       13 Jul 27  2016 /opt/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\n> lrwxrwxrwx 1 ubuntu users       17 Jul 27  2016 /opt/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n> -rwxrwxr-x 1 ubuntu users 79337624 Jul 27  2016 /opt/cuda/lib64/libcudnn.so.5.1.5\r\n> -rw-rw-r-- 1 ubuntu users 69756172 Jul 27  2016 /opt/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n> # python3.5 -c \"import tensorflow; print(tensorflow.__version__)\"\r\n> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\n> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\n> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\n> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\n> I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n> 1.0.0\r\n> \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\n        if args.model == 'rnn':\r\n            cell_fn = rnn.BasicRNNCell\r\n        elif args.model == 'gru':\r\n            cell_fn = rnn.GRUCell\r\n        elif args.model == 'lstm':\r\n            cell_fn = rnn.BasicLSTMCell\r\n        else:\r\n            raise Exception(\"model type not supported: {}\".format(args.model))\r\n\r\n        cell = cell_fn(args.rnn_size, state_is_tuple=True)\r\n```\r\nusing lstm is fine but any other than that 'rnn' or 'gru' causes the failure.", "comments": ["@mcr-ksh Thanks for the info.  But your minimal example is too minimal.  :)\r\n\r\nCan you provide a **self-contained** minimal example that demonstrates the problem, along with the full log / error trace?  Thanks!", "sure.\r\n\r\n```\r\n# git clone https://github.com/sherjilozair/char-rnn-tensorflow.git\r\n# python3.5 train.py --model gru\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nloading preprocessed files\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 114, in <module>\r\n    main()\r\n  File \"train.py\", line 48, in main\r\n    train(args)\r\n  File \"train.py\", line 82, in train\r\n    model = Model(args)\r\n  File \"/devel/char-rnn-tensorflow/model.py\", line 23, in __init__\r\n    cell = cell_fn(args.rnn_size, state_is_tuple=True)\r\nTypeError: __init__() got an unexpected keyword argument 'state_is_tuple'\r\n```", "@mcr-ksh \r\n\r\nHere's the API documentation:\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GRUCell\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicRNNCell\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\r\n\r\nNote that only `BasicLSTMCell` has a `state_is_tuple` argument, the others don't.  And also note that the state_is_tuple=False behavior will soon be deprecated, and the default is True.  So you should change this line:\r\nhttps://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py#L23\r\n\r\nFrom \r\n`cell = cell_fn(args.rnn_size, state_is_tuple=True)`\r\nto\r\n`cell = cell_fn(args.rnn_size)`\r\n\r\nFor future reference, please limit github issues to bugs or feature requests.  We don't have time to debug bugs in all user code, and there is a larger community on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) to read and answer such questions.  Thanks!"]}, {"number": 7967, "title": "\"data_format == FORMAT_NHWC\" error discrepancy", "body": "`tf.nn.conv2d()`'s behaviour on throwing the \"data_format == FORMAT_NHWC\" assertion seems odd. I cooked up a minimal example:\r\n\r\n```py\r\ninput_np = np.zeros([1,1,4,4], dtype=np.float32)\r\nx = tf.constant(input_np, dtype=tf.float32)\r\nfilter = tf.ones([1,1,1,1], dtype=tf.float32)\r\nout = tf.nn.conv2d(x, filter, [1,1,1,1], padding='SAME', data_format='NCHW')\r\nsess= tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(out)\r\n```\r\nThrows:\r\n```\r\nF tensorflow/core/kernels/conv_ops.cc:62] Check failed: data_format == FORMAT_NHWC\r\nGeneric conv implementation only supports NHWC tensor format for now.\r\nAborted (core dumped)\r\n```\r\n\r\nWhile this works fine, and it uses exactly the same variables, but now I am feeding a placeholder.\r\n```py\r\ninput_np = np.zeros([1,1,4,4], dtype=np.float32)\r\nx = tf.placeholder(\"float\", [None, 1,4,4])\r\nfilter = tf.ones([1,1,1,1], dtype=tf.float32)\r\nout = tf.nn.conv2d(x, filter, [1,1,1,1], padding='SAME', data_format='NCHW')\r\nsess= tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(out, feed_dict={x:input_np})\r\n```\r\n\r\nUsing 0.12.1, error is consistent across: OS-X as Ubuntu 14.04, regardless of CPU or GPU device placement.", "comments": ["I think what's happening here is:\r\n* the GPU implementation of Convolution support NCHW format, but the CPU implementation does not\r\n* the CPU implementation crashes with a CHECK() failure if passed an NCHW format input rather than failing gracefully.\r\n* in your first example, the convolution is a constant (does not depend on any variables or placeholders), and the constant-folder attempts to use the CPU implementation to evaluate the convolution. This causes a crash.\r\n\r\nThe fix is probably just to make the CPU convolution fail more gracefully for NCHW inputs (via an OP_REQUIRES).", "@TimZaman Perhaps you can try this on TensorFlow 1.0 to see if this is fixed?", "#8151 is a duplicate and has more information.\r\nClosing this one.", "I have encountered the same error. Im using a CPU not GPU. So I changed the data format of the tensors to overcome in the code.Above Suggested solutions will not work for CPU.\r\nMore can be found in tensorflow documentation. Refer to data formats\r\nhttps://www.tensorflow.org/guide/performance/overview"]}, {"number": 7966, "title": "Compile TensorFlow 1.0 on Windows 10 for Python 3.6 (Anaconda) cause compile issues", "body": "As per agreement in #6999 [specific comment](https://github.com/tensorflow/tensorflow/issues/6999#issuecomment-280749568), I'm creating a new issue for compilation errors for TF 1.0 under Windows for Python 3.6\r\n\r\n**Pre-requisites:**\r\nWindows 10 v.14986\r\nPython 3.6 64 bit (Anaconda 4.3)\r\ngit version 2.11.0.windows.1\r\nVisual Studio Build Tools (called \"Build Tools for Visual Studio 2017 RC\" in the [download](https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017-rc) section)\r\ncmake-3.8.0-rc1 and swigwin-3.0.12 (just downloaded latest version at that moment)\r\nTensorFlow 1.0.0 from source (master branch from github)\r\n\r\n**Reproduction**. I followed [instruction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) for CMake just running two commands - CMake first and MSBuild second.\r\n\r\nTo compile, I made two changes in sources. I found them based on compilation errors.\r\nOne was adding `#include <intrin.h>` to `tensorflow\\core\\platform\\windows\\cpu_info.h` inside of `#ifndef` block because of the following error:\r\n> error C3861: '__cpuidex': identifier not found \r\n\r\nI found [commit](https://github.com/tensorflow/tensorflow/commit/a672cb166dae93ae955c1d38f3de8903dd242373) introduced usage of `__cpuidex` function. Based on [this](https://msdn.microsoft.com/en-us/library/hskdteyh.aspx) MSDN link, I discovered that Header file `<intrin.h>` has to be included.  That's why I put it inside `cpu_info.h` (not sure if it's exactly perfect place).\r\n\r\n**Second issue** was commenting out procedures `_mm256_extract_epi32` and `_mm256_insert_epi32` in `tensorflow\\core\\platform\\windows\\intrinsics_port.h` due to error\r\n\r\n> error C2169: '_mm256_extract_epi32': intrinsic function, cannot be defined \r\n\r\nThe second issue with `_mm256_extract_epi32` may be connected with the same reason, I don't know. \r\nIn the meantime, `platform/windows/intrinsics_port.h` [references](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/intrinsics_port.h#L22) `immintrin.h`, but I don't see it's actually included anywhere. I assume it may correlate with `instrih.h` inclusion (based on [this](https://msdn.microsoft.com/en-us/library/26td21ds.aspx) link). Meanwhile, `_mm256_extract_epi32 `can be found in `avxintrin.h` which may be is included by `immintrin.h`.\r\n\r\n> // the following avx intrinsics are not defined on windows\r\n> // in immintrin.h so we define them here.\r\n\r\nAfter fixing those two issues, that, I was able to successfully run\r\n> MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\nWhich gave me `tensorflow-1.0.0-cp36-cp36m-win_amd64.whl`, that I successfully installed into Python using `pip`. It seems to be working - I was able to run couple of jupyter notebooks, including udacity assignments.\r\n\r\nBTW, compilation took about 2-3 hours on my week laptop (i5-4200U).", "comments": ["GPU compilation on windows does take a long time, due to Visual Studio not properly parallelizing the compilation when using an external compiler (nvcc).\r\nLooks like you already have a solution to the reported issue. We would be happy to review and accept your solution to the repository. Would you like to create a Pull Request?", "@gunan I created PR #8008, but I'm unsure about the fix. \r\nIt allows me to successfully compile under my environment, but it seems it won't be generic.\r\n\r\nThe thing is that two functions from `intrinsics_port.h` (`_mm256_extract_epi32` and `_mm256_insert_epi32`) are declared in `immintrin.h` for VS2017 build tools, but I didn't found them in the same header file for VS2015 (VS14). There is no `#define` before these new declarations so no easy way to put a check in `intrinsics_port.h` in TF to avoid double declaration. Only base on `_MSC_VER >= 1910`?\r\n\r\nBTW, I only do compilation of TensorFlow CPU version, which involves only `Microsoft C/C++ Optimizing Compiler Version 19.10.24930 for x64` from VS2017 build tools.", "CC @mrry @guschmue any ideas?\r\n\r\nAFAIK immintrin.h is a header file for intel SIMD intrinsics.\r\nMaybe the compilation issues are related to VS2017 as we are not seeing them in our builds that use vs2015?\r\nThe files you edited should be specific to windows. We should be able to test if your changes are OK on older versions of VS (2015 update 3 is the only one we actually test on).", "intrinsics_port.h is there because vs2015 does not implement all intrinsics.\r\nbut vs2017 does have them. So maybe something like\r\n#if _MSC_VER < 1910\r\nin intrinsics_port.h will make both vs2015 and vs2017 work.", "btw, #8008 will pass CI for windows because CI does not build for /arch:avx2. \r\nIf you'd turn on avx2 it would fail on vs2015.\r\nBut avx2 fails anyway since https://github.com/tensorflow/tensorflow/commit/ed6357cbd4f6e47ab87b219a0e0840739c92c970 which I was planning to fix (I only noticed because we build for avx2).\r\n ", "@guschmue I changed precompiler check to `#if _MSC_VER < 1910` and it expectedly works for VS2017 build tools.\r\n\r\nMeanwhile, is it OK that inclusion of `immintrin.h` is implicit? It's only mentioned in `intrinsics_port.h`, but not included directly anywhere.\r\nIt seems inclusion of immintrin is passed by usage of external `highwayhash`. But these operations (`_mm256_extract_epi32 `and `_mm256_insert_epi32`) are used in `core/kernels/sparse_matmul_op.h`", "Might be better to use intrin.h, by don't think its critical.\r\nThe calls in sparse_matmul.h are not used in the default windows build: EIGEN_VECTORIZE_AVX is not set. I think if you'd compile with /arch:avx2 or so those become hot. But I know /arch:avx2 has some issue since a week ... will look at it soon. \r\n", "I think this is resolved? Or are we still seeing this build failure with avx2?", "yes, avx2 build are still failing - looking at it.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7965, "title": "Fixing broken link to DebugDumpDir.find()", "body": "", "comments": ["Can one of the admins verify this patch?", "@rodrigob Thanks for sending the fix. However, the file you are editing has moved. See:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc\r\n\r\nSo I am going to close this PR now. Feel free to comment or reopen it if you have any questions."]}, {"number": 7964, "title": "How to specify the original name of an image using summary.image? ", "body": "I want to visualize detection results on images using `tf.summary.image` and record the original name of this image at the same time.\r\nIn the old version `tf.image_summary()`, I can pass a placeholder for an image name and feed the name in `sess.run`.  Like this,\r\n```\r\nlog_image_data = tf.placeholder(tf.uint8, [None, None, 3])\r\nlog_image_name = tf.placeholder(tf.string)\r\nlog_image = tf.image_summary(log_image_name, tf.expand_dims(log_image_data, 0), max_images=1)\r\n```\r\nBut the new version of api only allows to speficy a fixed `name` in string type instead of a placeholder when building the summry graph. \r\n ```\r\ntf.summary.image(name, tensor, max_outputs=3, collections=None) \r\n# Args: \r\n# name: A name for the generated node, which cannot be a placeholder\r\n```\r\n\r\nHow can I name the visualized image on the fly? ", "comments": ["A substitute is bypassing `summary.image` using a low level api `gen_logging_ops._image_summary` \r\n```\r\nlog_image_data = tf.placeholder(tf.uint8, [None, None, 3])\r\nlog_image_name = tf.placeholder(tf.string)\r\nfrom tensorflow.python.ops import gen_logging_ops\r\nfrom tensorflow.python.framework import ops as _ops\r\nlog_image = gen_logging_ops._image_summary(log_image_name, tf.expand_dims(log_image_data, 0), max_images=1)\r\n_ops.add_to_collection(_ops.GraphKeys.SUMMARIES, log_image)\r\n```", "duplicate of #5558 ", "Thanks for the note @ppwwyyxx .\r\n\r\nI'm closing this out.  @CharlesShang please follow up on #5558 ", "Thank you "]}, {"number": 7963, "title": "Fix TensorFlow compilation errors with KNL optimization flags (-mavx512f)", "body": "This fixes the compilation errors (listed below) encountered when building TensorFlow with -mavx512f flag. -mavx512f enables generation of AVX-512 Foundational instructions in TF/Eigen and are needed for optimal performance on Knights Landing (Intel Xeon Phi x200) architecture.\r\n\r\n./tensorflow/core/kernels/sparse_matmul_op.h:258:46: error: cannot convert 'const Packet8d {aka const __vector(8) double}' to '__m512 {aka __vector(16) float}' for argument '1' to '__m128 _mm512_extractf32x4_ps(__m512, int)'\r\n./tensorflow/core/kernels/sparse_matmul_op.h:263:61: error: cannot convert 'const Packet8d {aka const __vector(8) double}' to '__m512 {aka __vector(16) float}' for argument '1' to '__m128 _mm512_extractf32x4_ps(__m512, int)'\r\n./tensorflow/core/kernels/sparse_matmul_op.h:420:77: error: cannot convert 'const Packet16f {aka const __vector(16) float}' to '__m512i {aka __vector(8) long long int}' for argument '1' to '__m256i _mm512_castsi512_si256(__m512i)'\r\n./tensorflow/core/kernels/sparse_matmul_op.h:427:59: error: cannot convert 'const Packet16f {aka const __vector(16) float}' to '__m512d {aka __vector(8) double}' for argument '1' to '__m256d _mm512_extractf64x4_pd(__m512d, int)'\r\n./tensorflow/core/kernels/eigen_pooling.h:334:67: error: cannot convert '__m512i {aka __vector(8) long long int}' to '__vector(16) float' in initialization\r\n./tensorflow/core/kernels/eigen_pooling.h:335:57: error: cannot convert '__vector(16) float' to '__m512i {aka __vector(8) long long int}' for argument '1' to '__m512i _mm512_ternarylogic_epi64(__m512i, __m512i, __m512i, int)'\r\n./tensorflow/core/kernels/eigen_pooling.h:335:57: error: cannot convert '__vector(16) float' to '__m512i {aka __vector(8) long long int}' for argument '1' to '__m512i _mm512_ternarylogic_epi64(__m512i, __m512i, __m512i, int)'\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 7962, "title": "Clarify Migration From session_bundle.exporter to saved_model.builder.SavedModelBuilder", "body": "In looking at the docs for [Serving a TensorFlow Model](https://tensorflow.github.io/serving/serving_basic.html), and [comparing to wayback machine](https://web.archive.org/web/20160415224950/https://tensorflow.github.io/serving/serving_basic.html), it seems the serving guide went from using `session_bundle.exporter` to `saved_model.builder.SavedModelBuilder` with no documentation as to this change or the migration path.\r\n\r\nBoth tools wrap `Savers`. What is the difference between these? I believe `session_bundle` is deprecated and `SavedModelBuilder` is supposed to be a replacement however I have not seen this point made explicitly in the docs. All I could find is a comment in the `loader`:\r\n```\r\nLoad legacy TF Exporter/SessionBundle checkpoint.\r\n```\r\n\r\n[Report from SO](http://stackoverflow.com/questions/42499228/tensorflow-session-bundle-vs-saved-model).", "comments": ["@concretevitamin might have some advice here.", "@sukritiramesh: can you take a look since you own SavedModel?", "So it seems the exporter is now marked deprecated: https://github.com/tensorflow/tensorflow/commit/02762f0db00c45ac2cb2cf9c6f629ad96c38fe9e, but are there any notes on how to make the migration?", "Seems like with the TF 1.1.0 release there are now warnings about this deprecation but little in the way of instructions:\r\n```\r\nexport (from tensorflow.contrib.session_bundle.exporter) is deprecated and\r\nwill be removed after 2017-06-30.\r\nInstructions for updating:\r\nPlease use SavedModel instead.\r\n```", "Look at https://tensorflow.github.io/serving/serving_basic for docs. We'll have more soon.", "Yes I linked to that initially though there is no migration docs. ", "@davidsoergel @sukritiramesh Do we have any migration docs?"]}, {"number": 7961, "title": "PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py", "body": "I am trying to install tensorflow within virtual environment. After creating virtual env , I run following command : \r\n\r\n`pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl`\r\n\r\nAnd the output is : \r\n\r\nCollecting tensorflow==0.9.0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl\r\n  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl\r\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow==0.9.0)\r\nCollecting protobuf==3.0.0b2 (from tensorflow==0.9.0)\r\n  Using cached protobuf-3.0.0b2-py2.py3-none-any.whl\r\nRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.4/dist-packages (from tensorflow==0.9.0)\r\nRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.4/dist-packages (from tensorflow==0.9.0)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.4/dist-packages (from protobuf==3.0.0b2->tensorflow==0.9.0)\r\nRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.4/dist-packages (from setuptools->protobuf==3.0.0b2->tensorflow==0.9.0)\r\nRequirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.4/dist-packages (from setuptools->protobuf==3.0.0b2->tensorflow==0.9.0)\r\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.4/dist-packages (from packaging>=16.8->setuptools->protobuf==3.0.0b2->tensorflow==0.9.0)\r\nInstalling collected packages: protobuf, tensorflow\r\n  Found existing installation: protobuf 3.0.0b1.post2\r\n    Uninstalling protobuf-3.0.0b1.post2:\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.4/shutil.py\", line 523, in move\r\n    os.rename(src, real_dst)\r\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py' -> '/tmp/pip-frvrcwtb-uninstall/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.4/dist-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python3.4/dist-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/usr/local/lib/python3.4/dist-packages/pip/req/req_set.py\", line 778, in install\r\n    requirement.uninstall(auto_confirm=True)\r\n  File \"/usr/local/lib/python3.4/dist-packages/pip/req/req_install.py\", line 754, in uninstall\r\n    paths_to_remove.remove(auto_confirm)\r\n  File \"/usr/local/lib/python3.4/dist-packages/pip/req/req_uninstall.py\", line 115, in remove\r\n    renames(path, new_path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/pip/utils/__init__.py\", line 267, in renames\r\n    shutil.move(old, new)\r\n  File \"/usr/lib/python3.4/shutil.py\", line 536, in move\r\n    os.unlink(src)\r\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-packages/google/protobuf/__init__.py'", "comments": ["`/usr/local/lib/python3.4/dist-packages/google/protobuf/init.py` means it is trying to write to the central python package directory. please mage sure you activate the virtualenv before running the `pip install` command.", "@sijanonly I believe that should resolve your problem, so I'm closing this out.\r\n\r\nIf you have other problems feel free to file new issues.  Thanks!", "I use pip instead of pip3 and solve this problem:\r\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp34-cp34m-linux_x86_64.whl"]}, {"number": 7960, "title": "double free or corruption (!prev) in Docker image tensorflow/tensorflow:1.0.0", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/6968\r\nhttps://github.com/tensorflow/tensorflow/issues/7839\r\n\r\n### Environment info\r\n\r\nI run tensorflow in Docker image [tensorflow/tensorflow:1.0.0](https://hub.docker.com/r/tensorflow/tensorflow/).\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI used tensorflow/tensorflow:1.0.0 to run [examples/tutorials/mnist/mnist_softmax.py](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/tutorials/mnist/mnist_softmax.py). My docker run command is:\r\n\r\n```shell\r\n$ docker run -ti \\\r\n    -v /Users/lienhua34/Programs/python/tensorflow/tensorflow/examples/tutorials/mnist:/mnist \\\r\n   tensorflow/tensorflow:1.0.0 /bin/bash\r\n```\r\n\r\nThen cd to /mnist, and run mnist_softmax.py:\r\n\r\n```shell\r\nroot@5a17d6873afc:/notebooks# cd /mnist/\r\nroot@5a17d6873afc:/mnist# ls -l\r\ntotal 52\r\n-rw-r--r-- 1 root root 2581 Feb 16 15:42 BUILD\r\n-rw-r--r-- 1 root root  979 Feb 16 15:42 __init__.py\r\ndrwxr-xr-x 6 root root  204 Mar  1 03:41 data\r\n-rw-r--r-- 1 root root 9515 Feb 16 15:42 fully_connected_feed.py\r\n-rw-r--r-- 1 root root 1107 Feb 16 15:42 input_data.py\r\n-rw-r--r-- 1 root root 5292 Feb 16 15:42 mnist.py\r\n-rw-r--r-- 1 root root 2772 Feb 17 09:01 mnist_softmax.py\r\n-rw-r--r-- 1 root root 3684 Feb 16 15:42 mnist_softmax_xla.py\r\n-rw-r--r-- 1 root root 8391 Feb 16 15:42 mnist_with_summaries.py\r\nroot@5a17d6873afc:/mnist# python mnist_softmax.py --data_dir=data\r\nLoading data....\r\nExtracting data/train-images-idx3-ubyte.gz\r\nExtracting data/train-labels-idx1-ubyte.gz\r\nExtracting data/t10k-images-idx3-ubyte.gz\r\nExtracting data/t10k-labels-idx1-ubyte.gz\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n0.919\r\n*** Error in `python': double free or corruption (!prev): 0x000000000179b6c0 ***\r\nAborted\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI followed the issue https://github.com/tensorflow/tensorflow/issues/6968 to install libtcmalloc-minimal4,\r\n\r\n```shell\r\nsudo apt-get install libtcmalloc-minimal4\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\r\n```\r\n\r\nAfter that, it will run fine.\r\n\r\n```\r\nroot@5b7be5bcf590:/mnist# python mnist_softmax.py --data_dir=data\r\nLoading data....\r\nExtracting data/train-images-idx3-ubyte.gz\r\nExtracting data/train-labels-idx1-ubyte.gz\r\nExtracting data/t10k-images-idx3-ubyte.gz\r\nExtracting data/t10k-labels-idx1-ubyte.gz\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n0.9212\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Upgrade to the one of the docker images based off 16.04.", "@cancan101 Which one? https://hub.docker.com/r/tensorflow/tensorflow/tags/ ", "I use `1.0.0.2-devel-gpu` and it is based on 16.04.", "But I don't want to use GPU. It's too large.", "Then grab another image with the same version number", "@cancan101 Sorry, I don't understand.", "If you do not need gpu, you can simply use the `1.0.0.2-devel` docker image. Same version as the one @cancan101 suggested, but no gpu.", "Closing this issue, as it sounds like it was resolved post 1.0."]}, {"number": 7959, "title": "ImportError: cannot import name model_fn", "body": "I tried run cnn_mnist.py and I got the following error.\r\n\r\nTraceback (most recent call last):\r\n  File \" cnn_mnist.py\", line 13, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\r\nImportError: cannot import name model_fn\r\n\r\ncuda veriosn\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Sun_Sep__4_22:14:01_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.44\r\n\r\ntensorflow version\r\ntensorflow (0.10.0)\r\n\r\npython version\r\nPython 2.7.12\r\n", "comments": ["@blueevangelion Can you try this on TensorFlow 1.0 instead?", "When i use 1.0 instead, i got the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"cnn_mnist.py\", line 162, in <module>\r\n    tf.app.run()\r\n  File \"/home/joe/tensorflow1/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"cnn_mnist.py\", line 151, in main\r\n    learn.metric_spec.MetricSpec(\r\nAttributeError: 'module' object has no attribute 'metric_spec'\r\n", "Anyone knows  what is going on?\r\n", "@blueevangelion \r\n\r\nWhere are you getting `cnn_mnist.py`?\r\n\r\nThe version of this file at master hasn't changed in 2 months, and doesn't correspond to either of your stack traces:\r\nhttps://github.com/tensorflow/tensorflow/blame/master/tensorflow/examples/tutorials/layers/cnn_mnist.py", "Use learn.MetricSpec instead. The examples need to be updated. ", "Closing due to lack of activity.", "Just hit the same problem. Here is the issue:\r\nI followed the installation instructions from Sherry Moore's tf-tutorial github page. Her instructions rely on using --upgrade $TF_BINARY_URL when using pip to install tensor flow, and she provides the old 0.2.0 URL.\r\nI uninstalled tensorflow, and reinstalled it using: pip install tensorflow, and it grabbed version 1.0.1 automatically... after that, no more problems importing this model_fn", "I'm getting this error as well. Not exactly clear how to solve the issue tho.", "Check your Tensorflow version\r\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n\r\nIf the version is not 1.* upgrade Tensorflow.\r\n$ pip3 install --upgrade tensorflow-gpu\r\n"]}, {"number": 7957, "title": "tf.contrib.layers.sparse_column_with_hash_bucket() lead to 'out of memory error' with large hash_bucket_size?", "body": "  I'm using the wide_n_deep model on my own data, which has feature dimension about _100million_. When I use the _sparse_column_with_hash_bucket()_ and set the _hash_bucket_size_ to _10million_ (because some categorical features have that much different values), the program always failed with the out of memory error.\r\n\r\n  So I wonder if tensorflow's wide_n_deep model could handles data with high dimension like the case above? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7955, "title": "Add --pull to docker build to automatically pull the latest image in \u2026", "body": "\u2026(#7597)\r\n\r\nFROM.\r\n(cherry picked from commit c56c873fbaf976d26d487ad57c8efbc87f05331c)", "comments": []}, {"number": 7954, "title": "Feature Request - Session Cancellation Function", "body": "It'd be great to have a between node cancellation function for the tensorflow python session. In particular, if you call `sess.cancel_graph()` on a separate thread, then the tensorflow session on the main thread would be cancelled (raising an exception like the timeout exception). It would also be great for these cancellation signals to be caught between every tf operation, rather than only after particular operations like dequeue, etc.\r\n\r\n\r\n\r\n", "comments": ["Does `tf.Session.close()` not already satisfy this requirement? It cancels all concurrently running steps in a particular session, and these calls raise `tf.errors.CancelledError`.", "Ah I wasn't aware of this feature. Thanks for pointing it out! I'll try it out.", "Is there a way to keep the session alive but still cancel all the current steps asynchronously? I'm doing this in a distributed context, so keeping the session alive is necessary to continue training.", "Is there any update on this? ", "Note that recreating session is normal part of distributed training. For instance, if you have a network delay and session.run times out, the session becomes invalid and needs to be recreated.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 7953, "title": "Replace broken junit download link with maven.", "body": "", "comments": ["Don't worry about it for this patch, but in the long run we should switch\nover to hashes which aren't broken (sha256, for instance).\nOn Tue, Feb 28, 2017 at 3:41 PM Yifei Feng <notifications@github.com> wrote:\n\n> *@yifeif* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/workspace.bzl\n> <https://github.com/tensorflow/tensorflow/pull/7953#discussion_r103579170>\n> :\n>\n> > @@ -418,15 +418,15 @@ def tf_workspace(path_prefix = \"\", tf_repo_name = \"\"):\n>    )\n>\n>    # Make junit-4.12 available as //external:junit\n> -  native.http_jar(\n> -      name = \"junit_jar\",\n> -      url = \"https://github.com/junit-team/junit4/releases/download/r4.12/junit-4.12.jar\",\n> -      sha256 = \"59721f0805e223d84b90677887d9ff567dc534d7c502ca903c0c2b17f05c116a\",\n> +  native.maven_jar(\n> +      name = \"junit_junit\",\n> +      artifact = \"junit:junit:4.12\",\n> +      sha1 = \"2973d150c0dc1fefe998f834810d68f278ea58ec\",\n>\n> I followed the example on bazel doc\n> https://bazel.build/versions/master/docs/be/workspace.html#maven_jar\n> Do we not need the sha1 here?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7953#discussion_r103579170>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAETb5c_WM93F1Sf6wTf859phR284gVkks5rhLCWgaJpZM4MPBHD>\n> .\n>\n", "ah totally behind on news :). Good to know. Don't think this change should affect input_test in cpu build. @tensorflow-jenkins test this please.", "cpu build passed, merging."]}, {"number": 7952, "title": "[Java] Loading frozen graph causes a segfault", "body": "I froze then saved a tensorflow graph in python with [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) producing a single model.pb file.\r\n\r\nHowever, loading the graph in Java (where modelStream is an InputStream from reading the .pb file):\r\n```\r\nGraph g = new Graph();\r\nInputStream modelStream = new FileInputStream(\"/path/to/model.pb\");\r\ng.importGraphDef(IOUtils.toByteArray(modelStream);\r\n```\r\ncauses a segfault:\r\n```\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x0000000125526b1c, pid=6482, tid=7171\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_91-b14) (build 1.8.0_91-b14)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.91-b14 mixed mode bsd-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [libtensorflow_jni.dylib+0x1bfdb1c]  tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, int, tensorflow::shape_inference::ShapeHandle*)+0x1c\r\n#\r\n```\r\nThe same model.pb file loads and runs properly in Python.\r\n\r\n### Environment info\r\nOperating System: MacOS Sierra 10.12.3\r\nNative library for OSX downloaded from [here](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-darwin-x86_64-1.0.0-PREVIEW1.tar.gz) by following the [readme on the Java Tensorflow repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md)", "comments": ["Thanks for the report. Can you share instructions to reproduce (if possible, a toy model and the `freeze_graph.py` invocation used)?\r\n\r\nIt seems this might have something to do with the model or with `freeze_graph`. For example, this doesn't happen with the frozen inception graph used by the [LabelImage example in the README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md), so any more information on reproducing the problem will be helpful.", "The model I used is quite bulky and was originally developed in Keras (2 inputs -> GRU -> concat -> 2 feed forward steps -> sigmoid) so not sure if that will end up being helpful (although I will try and work on a toy model that reproduces the bug). \r\n\r\nHowever, this is the freeze_graph invocation I used:\r\n\r\n```\r\n    checkpoint_name = \"saved_checkpoint\"\r\n    checkpoint_prefix = os.path.join(temp_dir, checkpoint_name)\r\n    checkpoint_state_name = \"checkpoint_state\"\r\n    input_graph_name = \"input_graph.pb\"\r\n    output_graph_name = \"model.pb\"\r\n    input_graph_path = os.path.join(temp_dir, input_graph_name)\r\n    input_checkpoint_path = os.path.join(temp_dir, checkpoint_name)\r\n    output_graph_path = os.path.join(export_dir, output_graph_name)\r\n\r\n    #### Save the model to a checkpoint\r\n    sess = K.get_session()\r\n    saver = saver_lib.Saver()\r\n    checkpoint_path = saver.save(\r\n            sess,\r\n            checkpoint_prefix,\r\n            global_step=0,\r\n            latest_filename=checkpoint_state_name)\r\n\r\n    ### Save graph definition\r\n    tf.train.write_graph(sess.graph.as_graph_def(), temp_dir, input_graph_name)\r\n\r\n    ### Freeze graph\r\n    output_node_names = \"Sigmoid\"\r\n    restore_op_name = \"save/restore_all\"\r\n    filename_tensor_name = \"save/Const:0\"\r\n    input_saver_def_path = \"\"\r\n    input_binary = False\r\n    clear_devices = True\r\n\r\n    freeze_graph.freeze_graph(input_graph_path,\r\n                              input_saver_def_path,\r\n                              input_binary,\r\n                              checkpoint_path,\r\n                              output_node_names,\r\n                              restore_op_name,\r\n                              filename_tensor_name,\r\n                              output_graph_path,\r\n                              clear_devices,\r\n                              \"\")\r\n```\r\nI am exporting the graph from Keras, so it is possible that its an issue on their side, although the model does load fine afterwards with:\r\n```\r\n#### Test to make sure it has worked\r\n    graph = load_graph(output_graph_path)\r\n    q1_input = graph.get_tensor_by_name('import/q1_input:0')\r\n    q2_input = graph.get_tensor_by_name('import/q2_input:0')\r\n    y_tensor = graph.get_tensor_by_name('import/Sigmoid:0')\r\n\r\n    with tf.Session(graph = graph) as session:\r\n        acc = 0.0;\r\n        for point in data.dev_generator():\r\n            x1 = point[0][0]\r\n            x2 = point[0][1]\r\n            y = point[1]\r\n            y_out = session.run(y_tensor, feed_dict = {\r\n                    q1_input: x1,\r\n                    q2_input: x2,\r\n                })\r\n            acc += np.sum(y == np.round(y_out)) / 64.0\r\n        print(\"Final acc: {}\".format(acc / num))\r\n```", "@tianwang95 an example model that causes the crash would be great, thanks!\r\n\r\n@asimshankar Before we have that, do you have any other ideas?", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available. Thanks!", "I am running into the same issue and have reduced it to a minimal failing example.\r\n```shell\r\ndbSpotify:~ dadi$ sw_vers\r\nProductName:\tMac OS X\r\nProductVersion:\t10.12\r\nBuildVersion:\t16A323\r\n```\r\n\r\nThe problem seems to be in dynamic_rnn. When we unroll the network manually rather than use dynamic_rnn it works fine.\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef define_graph():\r\n    features = tf.placeholder(tf.float32, [1, None, 3], name='input')\r\n    cell = tf.contrib.rnn.BasicLSTMCell(20)\r\n    output, _ = tf.nn.dynamic_rnn(cell, features, dtype=tf.float32)\r\n    tf.identity(output, name='output')\r\n\r\nif __name__ == '__main__':\r\n    print(\"Tensorflow version: \" + tf.__version__)\r\n    define_graph()\r\n    with tf.Session() as sess:\r\n        sess.run([tf.global_variables_initializer()])\r\n        in_graph_def = tf.get_default_graph().as_graph_def()\r\n        out_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, in_graph_def, ['output'])\r\n\r\n    with tf.gfile.GFile('frozen_model.pb', 'wb') as f:\r\n        f.write(out_graph_def.SerializeToString())\r\n\r\n```\r\n\r\n```shell\r\ndbSpotify:models dadi$ python example.py\r\nTensorflow version: 1.0.1\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nConverted 2 variables to const ops.\r\n```\r\nDowloaded jar / CPU-only native libs as per [instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java)\r\nLoading the model in java:\r\n```java\r\npackage com.spotify;\r\n\r\nimport java.io.File;\r\nimport java.io.IOException;\r\nimport java.nio.file.Files;\r\nimport org.junit.Test;\r\nimport org.tensorflow.Graph;\r\n\r\npublic class TfModelLoadTest {\r\n\r\n  @Test\r\n  public void testImportTfModel() throws IOException {\r\n    final File modelFile = new File(\"./frozen_model.pb\");\r\n    byte[] graphDef = Files.readAllBytes(modelFile.toPath());\r\n    Graph graph = new Graph();\r\n    graph.importGraphDef(graphDef);\r\n  }\r\n\r\n}\r\n\r\n```\r\n\r\nLeads to this [error report](https://gist.github.com/dadib/6270b70797997b0b0d9163cf4b8877ce)\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@dadib: Thanks for providing a minimal example. \r\n\r\nDigging a bit, I believe the same issues as in #8284 that should be fixed at head (and will be fixed in the next release - 1.1). Could you try with the nightly builds:\r\n\r\n- Java source: https://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow/TYPE=mac-slave/lastSuccessfulBuild/artifact/lib_package/libtensorflow.jar\r\n- JNI: https://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow/TYPE=mac-slave/lastSuccessfulBuild/artifact/lib_package/libtensorflow_jni-cpu-darwin-x86_64.tar.gz\r\n\r\nAnd then switch to 1.1 when that is released?", "@asimshankar works with those nightly builds. Thanks!"]}, {"number": 7951, "title": "[Enhancement] Redesigning TensorFlow's input pipelines", "body": "[**TL;DR:** We're designing a new input pipeline API for TensorFlow, and we'd like to collect your feature requests on this issue.]\r\n\r\nWe've noticed that one of the biggest challenges in getting started with TensorFlow is how to load your own data into your programs. While TensorFlow has several methods that can be used to build complex input pipelines (such as [`tf.train.string_input_producer()`](https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer), [`tf.train.batch()`](https://www.tensorflow.org/api_docs/python/tf/train/batch), etc.), they were designed for a particular use case (processing a static set of files repeatedly), and the average user experience with these methods is not great. For example:\r\n\r\n* Once you reach the end of a pipeline, it becomes closed and you can never use it again in the same session. This requires users to use unnatural workarounds&mdash;with control flow or multiple sessions&mdash;to get a signal after processing an entire epoch, or switch between processing two datasets (e.g. training and validation data) in the same program.\r\n  * See #2514 and #4535 for feature requests about handling multiple epochs.\r\n  * See #7902 and numerous Stack Overflow questions for examples of processing different datasets in the same program.\r\n* The current pipelines use TensorFlow queues and multiple Python threads, which can lead to poor performance (lock contention in the queues and the Python GIL) and hard-to-understand exceptions (`tf.errors.OutOfRangeError`).\r\n  * See #6845 for a discussion of input pipeline performance.\r\n  * See #7525 and [many more Stack Overflow questions](http://stackoverflow.com/search?q=%5Btensorflow%5D+outofrangeerror) for an example of the confusing error.\r\n* The pipelines behave poorly if you forget to call `tf.train.start_queue_runners(sess)`: in fact, they hang indefinitely and deadlock the user program.\r\n  * See #7945 and [many Stack Overflow questions](http://stackoverflow.com/search?q=%5Btensorflow%5D+hang) for some examples of users who have been bitten by this problem.\r\n\r\nWe're decided to start from a clean slate and redesign the input pipeline API. The existing methods will remain until TF 2.0 (at least), but we are planning to add a new set of methods for loading and manipulating datasets. We're still preparing a detailed design, which we plan to share soon, but we anticipate that there will be two new APIs:\r\n\r\n* A `Dataset` represents a collection of data elements. Each element  can be a tuple of one or more tensors (e.g. an image and its label). We will provide methods for creating datasets from tensors, and deriving them from another dataset (e.g. by slicing its elements, repeating its elements, shuffling its elements, batching its elements, mapping a function over its elements, etc.). \r\n* An `Iterator` can be created from a `Dataset`. An iterator represents the current position within a dataset, and exposes an operation (like `tf.QueueBase.dequeue()`) that can be run to get the next element. There will be explicit operations for initializing an iterator, so that it can be reused after you have processed all of the elements in a dataset.\r\n\r\nA similar pattern turns up in many different settings, including [Java's Stream API](https://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html), [Scala's collections](http://docs.scala-lang.org/overviews/collections/overview.html) (and hence Spark's RDDs), and [.NET's Language Integrated Query](https://msdn.microsoft.com/en-us/library/bb308959.aspx).\r\n\r\nWe're announcing this plan early because we want to collect feedback on what features you&mdash;as TensorFlow users&mdash;would like to see in an input pipeline API. What other pain points have we missed? What features do you miss from other systems? What other suggestions do you have?\r\n\r\nWe look forward to hearing from you!", "comments": ["A must-have for one of our use cases is ad-hoc creation of data elements via a callback function (which creates tensors on the fly, e.g. using py_func() or through some other means).\r\n\r\nMore specifically, we currently have a use case where we employ two queues; an outer one, using a string_input_producer (with shuffling), where each string denotes/points to a \"data set\", and the inner queue is then produced by generating a variable amount of samples from each \"data set\". Which and how many samples are generated differs per epoch (potentially conditional on past training behavior). Actually, we don't even use the nomenclature of an epoch anymore, since the same data is never seen twice, and above mentioned generation/sampling goes beyond the usual data augmentation.\r\n\r\nLong story short: With a slightly out-of-the-ordinary use case, we've been hit by pretty much all of the problems you have mentioned above, and our workarounds have not been pretty. We'd be extremely happy to see a very flexible mechanism, where such cases are supported, and data generation doesn't have to be shoehorned into forced-upon concepts like epochs, finitely repeating queues, etc. (although they can be modeled by its primitives).\r\nI am not sure how well the planned Dataset/Iterator API would support this.\r\n\r\nEdit: Things we still need of course, include multi-threaded data generation, and multi-threaded random shuffle producer-consumer queues. But without the bane of GIL -- maybe via easy C++/Eigen hooks and thread control on the native side? Back and forth, via pybind?\r\n\r\nEdit2: The new input pipeline should also take support for variable-sized tensors (i.e. different per example) into account, for both training and inference, e.g. in a fully-convolutional setting.", "@kmhofmann We'll certainly support `tf.py_func()` inside a new-style input pipeline (as well as, in general, compositions of any other TensorFlow ops). I'd like to understand more about your use case, though. How frequently do you move from one outer \"data set\" to the next? Are there any specific operations that you perform at the end of a \"data set\" or can your training loop handle the concatenation of records from different \"data sets\"?\r\n\r\nWe're planning to have a few nested iteration primitives, so that you can write a function mapping an element (e.g. a string representing your outer \"data set\") to a `Dataset` (representing the records in that \"data set\") and then flattening them down to a single `Dataset`. (Think `SelectMany()` in C#, `flatMap()` in Java and Scala.) So I think you could implement your logic for sampling from a \"data set\" in one of these `flatMap()` functions.\r\n\r\nLet me know if any of this is unclear!", "Oh good timing! Now I can stop writing my own (horrible) dataset class. Many of the things said already resonate with my experience.\r\n\r\nTo the extent possible, I would like to code dataset-independent tensorflow computations. I don't want to have 3 different gan classes: each with their own create graph and fit methods, simply because one dataset doesn't fit in memory, the other is an np.array, and the other is generated on the fly.\r\n\r\nThe use case that affects me the most is [do n times: train for k iter/epoch, validate model, repeat]. There are clear problems with queues like you said. A minor issue for which I offer no solution is that while the scheduling(how long to train for before validate) is done perhaps by some method of a model class that I would like to be dataset-independent, whether it makes sense to talk in terms of iter or epoch is determined by the dataset--ruining some of the independence.\r\n\r\nSome other ideas I jotted down while brainstorming my own class:\r\n- The dataset class (and not the model) should probably the one to have batch_size passed to it. It's awkward to ask for batch_size as a parameter during fitting and during dataset/queue creation, and ideally the compute graph doesn't have batch_size baked in.\r\n- A \"verbose\" dataset class should keep track of it's own statistics. It should maintain its own counters(tensors) that keep track of iterations, samples, and epochs. In most use cases I imagine these being restored the same time model parameters are restored.", "- Most importantly we need to address the dequeueing overhead. I've seen dozens and dozens of cases where (in the profiler; iirc MEMCpyWhatever) was really slow. This was mostly an issue where the GPU would get the data from the CPU.\r\n- It would be great if there is still a way to have an **input feed that comes from multi-threaded or multi-processing python**. The following is a great and reliable hack to do currently do that:\r\n`session.run(enqueue_op, feed_dict=$some_numpy_batch_input)` Where you can asynchronously feed the queue from python.\r\n- It would also be wow to have **GPU resident queues**.", "Good point. One thing is that currently the queue operations are \"baked in\" the computation graph, so it's hard to modify anything on the go. A higher abstraction can make it much easier without considering using control flows or other hacks.", "For a lot of my use cases, my input data is either 1. not on the file system, or 2. require complex preprocessing unavailable in TensorFlow. For both cases the existing input pipeline cannot help, so I use an input thread with enqueue/feed_dict + a training thread with dequeue a lot.\r\n\r\nLet's **assume** that in most cases, you don't need to use the model itself to produce data (though sometimes it's not true). Then a solution I really like to see, is to be able to receive(similar to dequeue) tensors from a different process. (Like #4836)\r\nThe benefits are:\r\n1. Can use whatever tools/languages to produce data from any sources, as long as they're finally sent with certain message protocol.\r\n2. (theoretically) doesn't require an extra python thread in the training process.\r\n3. If the message protocol supports pub/sub, then (1) multiple training sessions can subcribe and reuse the same input data, which is very useful when trying new models. (2) data can be generated from different machines if the pre-processing is too heavy for a single CPU.\r\n\r\nThese are the features I really missed from a private system I've been using.\r\nOne disadvantage is that IPC/socket has smaller bandwidth than RAM but usually it's not a bottleneck.\r\nI know this feature may be too far away, but I hope the new design could allow such possible future feature.", "@mrry One \"data set\" can be composed of anything between ~500-30,000 dynamically generated samples. At the moment, we don't perform specific operations at the end of each data set, i.e. everything gets put into the same (large) random shuffle queue, to mix samples between data sets. But I could also imagine cases where separation of sets might be helpful.", "Please support reading hdf5 file directly.", "Personally, I'm a very big fan of the `feed_dict` method of feeding data into the graph. It is by far the most flexible, makes debugging way easier and makes for much simpler code. Thus my biggest wish would be to make that method more performant. Right now, this method starves my GPU all the time, which is a shame because most other DL frameworks (even those based on computational graphs) manage to make this much more performantly. I assume there is more copying/handling going on in the background than would be necessary.", "I am glad to see this initiative. The input pipeline is definitely the\r\nsteepest part of the learning curve.\r\n\r\nI'd like:\r\n* A unified API to manage both in-memory (`feed_dict`) dataset and large one so that the same code scale and your model only have to talk to one API. Although, I have not use it yet, I liked what I read in the input pipeline documentation.\r\n* More iterators! They are great. Asynchronous iterators would be even better (see [PEP492](https://www.python.org/dev/peps/pep-0492/)). Iterators implementing `__len__` are great for progress report.\r\n* multiprocessing rather than threading\r\n* Please no `Dataset` class because, IMHO, the \"dataset\" concept is ill-defined. The `Dataset` class described in the original post already exists in Python: it is a list of tuples. And what is a \"dataset\", anyway? A collection of train/valid/test data or simply a collection of data? Is it just a file? directory? generator? Are each data item (input/target) couple? Is that always true? Is the dictionary part of the text dataset?\r\nThe choice of the data container is driven by a lot of constrains depending on its size and the execution environment. Instead of a `Dataset` container, I would prefer to have a rich set of containers offering different trade-off with respect to memory/time complexity. In addition, I would like to have a rich set of iterators, splitters, loaders, dumpers, slicers, repeaters, servers, generators to actually work with data coming from various source.\r\n* The epoch concept does not have a clear semantic either. In my experience, it is best defined by `epoch = global_step / steps_per_epoch` and `steps_per_epoch = dataset_size / batch_size`.\r\n\r\nHere my attempt to translate to small in-memory dataset some of the routines available in the TF's input pipeline for large dataset. Here some examples of what I would like to see available in TensorFlow:\r\n* [cycle_range](https://gist.github.com/nicolasdespres/fed915518cfcdb5c4dad73409eec77ea)\r\n* [shuffle_iter](https://gist.github.com/nicolasdespres/e07aa951ba62aeaea60e740238571664)\r\n* [batch_iter](https://gist.github.com/nicolasdespres/bbc62cb43f1ffe9b81d971cb957650c2)\r\n* [iter_shuffle_batch_range](https://gist.github.com/nicolasdespres/d380cc43ebb1eb2d780b816fde294869)\r\n* [iter_tensors_slice](https://gist.github.com/nicolasdespres/418279e152dca606a78584fc99630738)\r\n* [iter_shuffle_batch_tensors](https://gist.github.com/nicolasdespres/04dd05d0a8ebf66205a687f1706748dc)\r\n* [iter_shuffle_batch_window](https://gist.github.com/nicolasdespres/81689421f56b86a315a81f19d301508a)\r\n\r\nThese routines demonstrate how far you can go with just simple iterators over list of indices.\r\n", "+1 to something like feed_dict. That's the only way to learn by interacting with external world (training robot arms, Atari games, [Universe](https://github.com/openai/universe) ).\r\n\r\nIt could be made more efficient by avoiding copies. Like PyTorch, whose Tensors share memory buffers with underlying numpy arrays", "I don't know TF as well as others here, so please take my comments with some skepticism:\r\n\r\n- With `tf.py_func` I was able to solve most of my input-related problems, like loading .mat files in a symbolic-ish manner. The one I'm currently struggling with is the integration of `tf.train.batch` with the ability of picking the source from which the input should come, for having train/val data in the same symbolic variable. #8168 \r\n\r\n  I understand these functions were initially thought for simple use cases, but it would be nice to have more control of the pipeline without the burden of managing _everything_ (e.g. using `tf.QueueBase.from_list` but being forced to feed queues and manage threads kind of manually).\r\n\r\n- I'm not sure if TensorFlow optimizes the dequeue operation under the hood but, if not, I think we could greatly benefit from a parallel dequeue operation that charges data (i.e. next batch) into the GPU memory while it processes the previous data (i.e. current batch).\r\n\r\n- I think `feed_dict`-like solutions are not optimal for passing big chunks of data to the train function, like a batch of images, since they're basically a pause in the execution graph to force TF to interact with _python's runtime_. An in-graph solution sounds better, with pointers to guide the graph execution, like `feed_dict={is_training = True}` to indicate the input should come from the training pipeline, the model should set batchnorm and dropout (et al) to train mode etc. This way, TF could better optimize/parallelize the execution, and all solutions would scale.\r\n\r\n- The standard functions for creating batches apparently do not provide an index to indicate which batch we are processing. For example, a `slice_input_producer` receives the number of epochs to be generated but there seems to be no way of knowing the epoch of one sample without counting how many we have already evaluated.", "right now there are two very divergent paths to getting data into Tensorflow: feed_dict and queues. queues are wonderful until you don't have a way to manipulate your data natively -- for example, if you want to load a .wav file, chop it into parts, and convert it to a spectrogram. at that point, you have to write a C++ op (doable, but a context switch + it makes a very inflexible pipeline) or pop back into Python land (slower, but very easy and flexible).\r\n\r\nit seems like the best compromise between speed and flexibility is to create a TF queue and then make a bunch of Python threads that feed it with data. this allows you to do flexible data processing in Python (roughly parallelized on the CPU, apart from GIL issues) while maintaining some amount of speed benefit.\r\n\r\nwhat if you just formalized that? the interface would be: push_data, end_of_data (for signaling the end of an epoch), and a dequeue_batch function that feeds the model. then your code could just load data in Python and stuff it onto the queue in parallel, while the model sits totally separate from all of that.", "We should make feed_dict faster (likely by not copying the numpy.arrays like @yaroslavvb mentioned), but that's orthogonal to this change. No matter how much we optimize it, feed_dict will never be the fastest way to feed data into a training job.", "`feed_dict` specifically may not be essential. To be more precise, we need support for pipelines where learning is done in an online fashion, and training data is generated by a system responding to actions of a TensorFlow network (learning Atari simulator, robotics simulator, robot interacting with real world, etc). This is necessary for most of the applications at OpenAI, here's one example -- https://github.com/openai/universe-starter-agent", "The fastest option would be to create a TensorFlow op that maintains state, takes actions as input, and generates the training data. Then add a placeholder to specify the action.\r\n\r\nMy guess is that you're looking for something that can be done completely in Python, though. There may be some mid-point between the two.", "I am not sure it this concept has been brought up yet, but I will at least put the problem in my own terms.\r\n\r\nIn dealing with RL problems and the training replay buffer, I couldn't find an easy way to use the Queues to speed up this feeding of samples through the feed_dict.  Also, when randomly creating a sample set, it seemed like the samples were consumed when I wanted them left in the buffer.\r\n\r\nWhat I was hoping to do is feed (possibly through feed_dict, or file) a Queue with a new sample and once the size of the buffer is exceeded, the oldest sample is removed from the buffer.  So some concept of \"sample age\" would be nice.  I am sure using a circular buffer will work to fix to a number of samples, but \"age\" might be of interest as well, maybe passed as part of the tuple, but in the RL case, simply the sequence of the sample being added might cover the age (FIFO).\r\n\r\nAgain, it may have just not been clear to me how to use the queues, but being able to randomly pull a mini-batch from this sample buffer and not remove the samples so a new set of samples can be collected (possibly with prior sampled examples) would be nice.", "I may not understand the distributed settings that TF data input pipeline API is targeting to solve. Is it possible to have a simple API design as Pytorch does: only three simple classes. I can pick up pytorch's dataset API in 5 minutes and it's good enough for all the popular academic datasets. http://pytorch.org/docs/data.html\r\n\r\nIt's great to see new efforts to solve the pain points in TF dataset API. Looking forward to a simple/beautiful/flexible API with minimum number of classes/concepts introduced. Thanks.", "@lming Yeah, the first two comments here cover that: by making a Dataset implementation that uses [py_func](https://www.tensorflow.org/api_docs/python/tf/py_func), it'd be equivalent to the PyTorch implementation.", "I second @lming's sentiment above.\r\n\r\nOur biggest issue with the current data loading scheme is just that it's very complicated and involves a lot of new concepts.\r\n\r\nWe don't find it spectacularly difficult to write a multithreaded data loader ourselves in Python, and generally we don't find it overly difficult to ensure that our data loading and preprocessing runs sufficiently quickly that it doesn't actually bottleneck training.\r\n\r\nWhere we're stuck is that to optimally follow recommendations, we end up in an awkward situation, one of:\r\n- Using `feed_dict` and suffering any relevant performance hits\r\n- Feeding from a separate thread and dealing with some one-off queue boilerplate (except this didn't speed things up at all when we tried it)\r\n- Reimplementing our data loading and transformation pipeline with TF primitives, perhaps with `py_func`, but still using the TF API for managing queue runners\r\n\r\nThe Python threading API isn't perfect, but in general when we're doing mostly non-GIL-taking tasks in NumPy or whatever, the TF queue API seems more of a burden than a help.", "A couple of concrete use cases that come up for us:\r\n\r\n- One of our models is a localization model. We use scikit-image `AffineTransform` objects to apply cropping and resizing operations for this model, because those objects let us easily translate our model output back to the original input coordinate space. It seems tricky to square this with a fully tensor-based API.\r\n- We have data that comes from a large number of imbalanced segments, and in training we use some custom stratified sampling logic to ensure we present examples from each segment evenly. It's relatively straightforward for us to, in Python, generate a new draw from our original data set for every epoch, but it seems like in the API proposed above, we'd have to figure out how to implement this behavior as an `Iterator`, which seems less straightforward.", "We primarily deal with time series data, and prefer to not have to batch preprocess the whole dataset prior to training every unique model input architecture.  In fact, the preprocessed dataset size for one input architecture variant can be easily an order of magnitude larger than the unprocessed file set size.  \r\n\r\nWe've been able to deal with this using the current system of queues,TF ops, batch_join/etc to enable multiple on-the-fly preprocessing threads with across-file example mixing.  I have to say it's really nice and flexible for hyperparameter tuning the input architecture, and I like that the entire pipeline lives in the graph, feeds data in response to sess.run(train_op) calls, and can be restored from a common checkpoint with the model.  \r\n\r\nIf you're planning to deprecate the current queues paradigm, I would like to know that the `Dataset` and `Iterator` would enable the same flexibility.  For my use case it seems like `Dataset` could represent a collection of time series, and the `Iterator` would behave like a python iterator/generator and could handle any preprocessing to form batches of examples?\r\n     ", "feature request: control mechanism for queues, especially in combination with TFRecordReader's/TextFileReader's read() method, which automatically dequeues. Reason: the automatic preevaluation of pending enqueues.", "MXNet [IIterator](https://github.com/dmlc/mxnet/search?utf8=%E2%9C%93&q=IIterator&type=Code) is a more relevant example than Java's Stream API, Scala's collections (and hence Spark's RDDs), and .NET's Language Integrated Query.  The design enables flexible composition of various components of the input pipeline such as  [ImageRecordIter](https://github.com/dmlc/mxnet/blob/d8e64a67f1b5b8eff287a5305e7a86714d3b1aeb/src/io/iter_image_recordio.cc), [ImageNormalizeIter](https://github.com/dmlc/mxnet/search?utf8=%E2%9C%93&q=ImageNormalizeIter), [BatchLoader](https://github.com/dmlc/mxnet/search?utf8=%E2%9C%93&q=BatchLoader&type=Code), [PrefetcherIter](https://github.com/dmlc/mxnet/search?utf8=%E2%9C%93&q=PrefetcherIter&type=Code) and [ImageAugmenter](https://github.com/dmlc/mxnet/search?utf8=%E2%9C%93&q=aug_default&type=Code).\r\n```cpp\r\nMXNET_REGISTER_IO_ITER(ImageRecordIter)\r\n.describe(\"Create iterator for dataset packed in recordio.\")\r\n.add_arguments(ImageRecParserParam::__FIELDS__())\r\n.add_arguments(ImageRecordParam::__FIELDS__())\r\n.add_arguments(BatchParam::__FIELDS__())\r\n.add_arguments(PrefetcherParam::__FIELDS__())\r\n.add_arguments(ListDefaultAugParams())\r\n.add_arguments(ImageNormalizeParam::__FIELDS__())\r\n.set_body([]() {\r\n    return new PrefetcherIter(\r\n        new BatchLoader(\r\n            new ImageNormalizeIter(\r\n                new ImageRecordIter<real_t>())));\r\n  });\r\n```\r\nCaffe [DB](https://github.com/BVLC/caffe/search?l=C%2B%2B&p=1&q=db&utf8=%E2%9C%93) is simpler but still usable.\r\n\r\nIdeally, the newly designed API should be able to load existing datasets of Caffe & MXNet with easy to implement [plugins](https://github.com/dmlc/mxnet/search?utf8=%E2%9C%93&q=Caffe+Iter&type=Code).", "Just my 2 cents. Happy for this decision. I think that a *huge* effort should be placed in tutorials: the hugest difficulty I am having -- and some colleagues with me -- is that the documentation that you can find is quite lousy and not very self-contained. I would be happy to help, of course.", "> A Dataset represents a collection of data elements.\r\n\r\nI'm not sure if this is implied but a requirement would be to stream large datasets from disk. A `Dataset` explicitly containing all data elements doesn't seem to support that.", "In the last release notes I see that you have added a new RecordInput class, which seems to be the new class intended to use as input provider? Unfortunately the documentation is still lacking further explanations. I can only find some basic infos in the C++ API docs. \r\nWould be really interested to read something for the Python API + some example code. If you need any help, feel free to contact me or e.g. @petrux also offered help. I think he is right, that extending the documentation and providing better tutorials is highly important. Because otherwise the people will stick with feed_dict inputs until TensorFlow 3.0 and moan about bad performance of TF", "I ended up doing some benchmarking for other reasons, and observed comparable performance between `feed_dict` and using queues: https://github.com/tensorflow/tensorflow/issues/9322#issuecomment-295775991", "Feed dict overhead is essentially the cost of doing an extra memcpy (Python->TensorFlow CPU->TensorFlow GPU) vs. using native ops like queues which do (TensorFlow CPU->TensorFlow GPU). So if this memcpy is small, there should be negligible. ", "That makes sense, and it's what I was assuming.\r\n\r\nI think that makes the advice against `feed_dict` a bit overblown, though \u2013 really the issue seems more like inefficient data feeding that starves the GPUs, rather than the use of `feed_dict` itself.", "@yaroslavvb correct me if I'm wrong but this isn't entirely right. Unless you haven't implemented some input pipeline using Pythons Queue library or something similar it will be additionally the time of loading data from disk into memory and eventually preprocess them.\r\n\r\nFor Images and especially larger batch size, this might take quite a while. Here is where you can really speed things up using TFs input queues because they will load e.g. images into disk ( + preprocess ) on CPU while you are training/evaluating your network on GPU. When computations are done, you can directly grab the next batch on copy the data on the GPU, without waiting for native Python to load new data into memory.", "@kratzert, that's precisely what @taion means by\r\n\r\n> the issue seems more like inefficient data feeding that starves the GPUs\r\n\r\nIt's hard to do asynchronous preprocessing well, so most users benefit from `tf.train` doing it for them.", "Is it? By no means I want to defend TFs input queues, but as I read the post of @yaroslavvb he states that the additional time comes (only) from passing memory between native Python and TF ( + GPU). The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle. I can't find any of this statement in the post of @taion, but could be a misunderstanding of my side as I'm not a English native speaker.\r\n\r\nAnd I know, that for many cases asynchronous preprocessing is not possible, but for the cases it is (simple training of image classification CNN) TFs input queues help quite a lot. ", "> The only thing I wanted to add is, that if you can't store all your trainings data in memory, also loading memory from disk into memory adds up time to one training cycle.\r\n\r\nThat also applies to `tf.train`. Data doesn't just appear in the graph without reading it from disk, regardless of whether you do it via Python or TensorFlow's execution engine.\r\n\r\nAnyway, I find memapped NumPy arrays via [joblib.cache.Memory](https://pythonhosted.org/joblib/memory.html) and feed_dict to be quite performant (GPU load over 90% throughout a training session), despite the extra copy.", "Ah okay so maybe a misunderstanding. Thought I made myself clearer.\r\n\r\nI know that data does not appear magically in memory if I use TF. But TF makes it quite easy to place e.g. image loading + preprocessing explicitly on CPU and graph computation on GPU and both is done in parallel. So while GPU calculates ops on some data, CPU is already loading the next data into memory. And since this is done in parallel, we are effectively reducing the computational time by the amount of loading data from disk (since this takes usually less time then one forward + backward pass through the networks graph). But yes this only applies for working with CPU + GPU and has no effect if you use CPU only.\r\n\r\nedit: The maybe only thing I like of TFs input queues is that I the state of the queues (and batch producers) can be observed in Tensorboard. For the rest I fought quite some time to get them running with all the preprocessing I wanted and with queues for testing and validation in the same run.", "I would say that aside from the steep learning curve of input pipeline which can be overcome with documentation too, the key missing points are:\r\n\r\n1. A decent way to make custom preprocessing of the data, whether it be based on queue's or not, the idea of being able to foresee all possible data input needs is doomed.\r\n2. An easy and established way to change the input pipeline of a graph, after it has been created, because it is the most typical usage pattern. Current `input_map` provides such functionality, but it is rather hacky. And documented too (when it comes to `import_meta_graph`).\r\n3. A way to control and monitor the epochs - currently they are rather deeply hidden and unaccessible even simple checks.\r\n\r\nI'd support @nicolasdespres for \"no Dataset\" pledge, mainly because all-in-one bundle is not flexible, not future proof and also - not consistent with the TF's paradigm of providing small, stable, well-defined and assemble-able blocks for building custom models. Having some bundles, predefined \"easy-starter\" wrappers should be welcomed.", "I think TF does not really require another attempt to unify the data pre-processing to put it directly *into* the graph. Things get worse if one need custom stuff and on-the-fly generation/modification of data. Typically these modifications are not part of the forward model for a good reason: these operations do not require any backpropagation. Hence, they should be only loosely coupled.\r\n\r\nSo the ideal input pipeline (everything without backprop) should be quite simple and slim: It should consist of a queue operation which receives data (list of tensors) from **some** source (sockets). #8728 is a good step in this direction with the pros:\r\n- you can use any library for preprocessing (opencv, nltk, ...)\r\n- prefetching is totally parallel and can be done at any/multiple machine\r\n- the sender-code can be put into any place -- even directly in game-engines or render-engines\r\n- data generation can be done in any programming language (without custom ops)\r\n\r\nI am not sure, if you really need something else and I do not understand why you really need `tf.image.random_*` *in* the graph.\r\n", "I don't think the proposal here is to get rid of queues entirely, is it?\r\n\r\nDataset-style abstractions are pretty common in this space, and they're quite useful. The existence of a higher-level abstraction doesn't preclude the lower-level API from also existing.\r\n\r\nIn fact for these kinds of higher-level abstractions, sooner seems better than later \u2013 one of the greatest frustrations of reading published TF research code is that the vast majority of codebases use their own idiosyncratic layers library, as opposed to e.g. the ones in `tf.layers` or `tf.contrib.layers`, and these libraries are all different, which makes it more difficult than it should be to share work.", "I often try to use TensorFlow on very large inputs (potentially >1GB minibatch) with relatively light computation on each minibatch. These inputs are in a HDF5 file or a Numpy array either on disk or in memory, so I typically feed with ``feed_dict``, potentially asynchronously into a queue. When running with multiple GPUs, TensorFlow is not able to even saturate the PCI-e bandwidth to the GPUs because of the memcpy from the feed_dict to the CPU tensor.\r\n\r\nAs @yaroslavvb mentioned, the ``feed_dict`` memcpy (on a single CPU core?) can be a huge performance bottleneck, and I'd like to see this addressed in any refactor of TensorFlow's input handling.\r\n\r\n@jhseu You mentioned that you consider removing the ``feed_dict`` copy as orthogonal to this issue. Do you know if there's any issue or work being done on removing the copy (at least in some cases, like row-major Numpy arrays with nice strides)?", "@eamartin there have been a number of changes since the beginning of March by @alextp to speed up feed_dict; when the memory is aligned with 16 bytes, I think we share buffers with numpy, so nightly releases may be faster for you.\r\n\r\nThe 16 byte alignment issue comes from Eigen, unfortunately, which requires the beginning of the memory addresses to be aligned with 16 bytes.  I'm not sure why Eigen was not written to handle unaligned first and last \"packets\" so it wouldn't matter. :/\r\n\r\n", "Would it be possible for numpy to share buffers with tensorflow variables\nwhen they are returned from a session run?\n\nI realise this will probably raise all sorts of mutability and state\nissues, but these should be avoidable by setting the WRITABLE flag on the\nreturned numpy arrays to false.\n\n\n\n\nOn 29 Apr 2017 1:37 am, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n@eamartin <https://github.com/eamartin> there have been a number of changes\nsince the beginning of March by @alextp <https://github.com/alextp> to\nspeed up feed_dict; when the memory is aligned with 16 bytes, I think we\nshare buffers with numpy, so nightly releases may be faster for you.\n\nThe 16 byte alignment issue comes from Eigen, unfortunately, which requires\nthe beginning of the memory addresses to be aligned with 16 bytes. I'm not\nsure why Eigen was not written to pad the first and last \"packets\" so it\nwouldn't matter. :/\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-298129805>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ADXd5AD4yuTBPXo8pp-uvVbr9QyYnJhDks5r0nhMgaJpZM4MO9tN>\n.\n", "Thanks for the info @vrv (and for the features @alextp ). I did a little looking around, and it looks like https://github.com/tensorflow/tensorflow/commits?author=alextp&since=2017-03-01T06:00:00Z&until=2017-04-01T05:00:00Z are the relevant commits. From my checking, these didn't make it into TF 1.1 but hopefully will be in 1.2.", "Another thing that would be cool would be the ability for Session's to return Futures that could then be used as input to other Session runs. Said future could then be passed through the graph until the Tensor it represents requires evaluation.\r\n\r\n```python \r\nwith tf.Session() as S:\r\n   # Note executor semantics\r\n   future = S.submit(op_1, feed_dict={'input_1': 1.0, 'input_2':2.0})\r\n   result = S.run(op_2, feed_dict={'data': future})\r\n```\r\n\r\nThis concept is inspired by [dask distributed](https://github.com/dask/distributed) and other executor frameworks -- I think the flexibility offered by this abstraction is great!", "@sjperkins sadly the way our current unit tests are written makes them break if variables are returned without extra copies (because many tests do a = session.run(variable); session.run(update_variable); b = session.run(variable); assertDifferent(a, b) (which fails if they share buffers).\r\n\r\nI considered making a ConfigProto option to share buffers even when they are not exclusively owned by the C-python bridge but didn't. Should be easy to do from the commits listed above, if you're interested.", "@sjperkins for the futures, stay tuned, as we're prototyping something in this direction.", "My primary request is that however you build the new input pipeline system that it should be completely separate from the rest of the graph.  I'm giving a shot at migrating to tensorflow for our deep learning models for devices; but the input pipeline is so tightly bound to the rest of the compute graph that its like performing surgery to run inference against it.\r\n\r\nExample: I trained using TFRecords and input queues; I got my weights/model.  I want to perform inference by running my prediction operation; but because the input queue runners etc are part of the graph before that; I am stuck with that mechanism for performing inference.\r\n\r\nSee issue here: http://stackoverflow.com/questions/43708616/tensorflow-inference \r\n\r\nI like the tf record and queue runner thing now that I'm used to it; the issue is the tight binding to the graph....", "This is why tools like tf.estimator.Estimator were developed, to allow for\neasier separation of concerns between training and inference and to allow\nfor swapping of input pipelines. Can you use Estimator to write your model?\n\nOn Mon, May 1, 2017 at 2:54 PM, David Crook <notifications@github.com>\nwrote:\n\n> My primary request is that however you build the new input pipeline system\n> that it should be completely separate from the rest of the graph. I'm\n> giving a shot at migrating to tensorflow for our deep learning models for\n> devices; but the input pipeline is so tightly bound to the rest of the\n> compute graph that its like performing surgery to run inference against it.\n>\n> Example: I trained using TFRecords and input queues; I got my\n> weights/model. I want to perform inference by running my prediction\n> operation; but because the input queue runners etc are part of the graph\n> before that; I am stuck with that mechanism for performing inference.\n>\n> See issue here: http://stackoverflow.com/questions/43708616/tensorflow-\n> inference\n>\n> I like the tf record and queue runner thing now that I'm used to it; the\n> issue is the tight binding to the graph....\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-298442742>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXYK7comPG84UqyKek2mbzhnifD4ks5r1lSWgaJpZM4MO9tN>\n> .\n>\n\n\n\n-- \n - Alex\n", "Currently I am worried that my training/prediction preprocessing will diverge over time.\r\n\r\nI would be interested in a pipeline where:\r\n\r\n* Preprocessing and post processing can be serialized with the inference in a single model and then used from another language (no `tf.py_func`, but able to provide implementation at runtime)\r\n* There is a clearer distinction between input shapes, for training you usually want batches, but for prediction you often care only about single examples\r\n\r\nThe preprocessing and post processing do not require backprop, but they sill need to carry some values with them (normalization divisors or one hot mappings). It would be ideal to have some `Op` that can `train` some values like this during training, carry them over to production within the serialized graph and then I could provide an implementation for this pre/post processing `Op` in the target production language.", "@mirosval, it seems to me that this might be what `tf.Estimator` is intended for eventually.", "Not sure if this was mentioned above and I missed it, but I would appreciate a much easier way to switch between train and validation data sets. When using `feed_dict`, this is very simple, and this is what I'm used to. I've recently been trying to make the switch from `feed_dict`, but this has been (at least in my limited experience) a major difficulty. The tutorial page [here](https://www.tensorflow.org/programmers_guide/reading_data) mainly just suggests using separate processes, but this can be a pain, especially if I want to do early stopping based on the validation data. If I could create some sort of input method (Queue, Dataset, whatever) where I can cleanly swap between training and validation inputs, that would be much nicer (again, `feed_dict` is great for this, but if it will always be slower, it'd be nice to have a more performant alternative).", "@neighthan: yes this is something that @mrry has planned as well :)", "@neighthan Using current constructs, we've constructed a [`SwitchableDataSet`](https://github.com/mozilla/DeepSpeech/blob/master/util/data_set_helpers.py#L116) that allows you to at run time switch between train, validation, and test.", "@kdavis-mozilla that sounds interesting, but the link appears to be dead. Is there another reference?", "@neighthan Sorry, fixed.", "Very promising! That is one of my biggest issues with the current API.\r\nThat said, looking at SwitchableDataSet, it seems like implementing new kinds of data feeding use cases will be mostly done by implementing use-case specific classes. Will the new programming model also feature an API to, say, implement what the SwitchableDataSet offers and beyond from more generic, lower-level primitives? I'm just wondering about what things users will come up with (w.r.t. data generation and usage) that would otherwise require (specific) additions to the API...", "Am I correct to assume that https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn/dataframe is part of the new input pipeline initiative? Is there any code out there that uses this, even if that code is undocumented? I'd like to see an example of it in use. \r\n\r\nI also assume that the dataframes and transforms are intended to be closely integrated with estimators? At the moment, I don't see how estimators fit (nicely) with the dataframes and transforms. I see where you have ways to generate feed_fns from dataframes for estimators, but it seems more like an adapter to another approach, instead of part of the pipeline design. \r\n\r\nI understand this is all very new and under development. I really like what I see!  Keep up the good work.", "@vonclites That's for interacting with Pandas DataFrame/Series, not the new Dataset construct.", "@jimfleming It seems to be more general than that. There are methods to create TensorFlowDataFrames from csv files, dicts, numpy arrays, TFRecords, as well as from pandas. It follows the nomenclature of pandas, but it also resembles Spark's pipeline, as @mrry mentioned, and has many of the features he described in the original post.", "I agree that it's fairly generalized and it may be the basis for future\nwork but this has been available for quite a while and most of the files\nhaven't been updated in months.\nOn Sat, May 13, 2017 at 9:46 AM vonclites <notifications@github.com> wrote:\n\n> @jimfleming <https://github.com/jimfleming> It seems to be more general\n> than that. There are methods to create TensorFlowDataFrames from csv files,\n> dicts, numpy arrays, TFRecords, as well as from pandas. It follows the\n> nomenclature of pandas, but it also resembles Spark's pipeline, as @mrry\n> <https://github.com/mrry> mentioned, and has many of the features he\n> described in the original post.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-301259803>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAN0-Uztf2Y5vav67n_-YYSpISvvTEOBks5r5d5ngaJpZM4MO9tN>\n> .\n>\n", "@jimfleming Good point", "First documentation on master under `tf.contrib.data`: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\r\n\r\nAll this seems pretty amazing !", "Very nice indeed. The following iterators would be great:\r\n\r\n- [chain](https://docs.python.org/2/library/itertools.html#itertools.chain)\r\n- [product](https://docs.python.org/2/library/itertools.html#itertools.product)\r\n", "Any planing for supporting a seek-able file format, which can be started to read from an arbitrary offset?\r\n\r\nBelow is why we need it:\r\nAssume there is a large training data set which is in text format, and we need to convert it into tfrecord format.  Then we started a map-reduce job, converted it into 10 tfrecord files, started 10 workers to read them, Perfect! Then we want to run it faster, we would change the worker count to 20, 30, 40, ...  it would be great if we could do this without re-generate the training data.\r\n\r\nSolution:\r\nFirst, we need to pass an offset and length together with the filename into Dataset 's constructor. Only filename is not enough.\r\n\r\nSecond, the file format it self must be seekable(aka. splitable).  it should be one of the following:\r\n 1. Text format\r\n 2. Blocked binary format with paddings. \r\n 3. Has an index file. \r\n\r\n\r\n\r\n\r\n", "This new API is faster than the old one. I've integrated this new dataset API into my factoration machine trainer, and it saved me 20% training time. Thanks @mrry ", "I feel like the default code for looping over a dataset is a bit ugly with an exception breaking out of a `while True` loop:\r\n```python\r\nsess.run(iterator.initializer)\r\nwhile True:\r\n  try:\r\n    sess.run(train_op)\r\n  except tf.errors.OutOfRangeError:\r\n    break\r\n```\r\n\r\nWouldn't there be a way to have an `is_empty` tensor indicating when the iterator is empty? Like:\r\n```python\r\nsess.run(iterator.initializer)\r\nis_empty = False\r\nwhile not is_empty:\r\n  _, is_empty = sess.run([train_op, iterator.is_empty])\r\n```", "If you're using a MonitoredSession and its variants you should still be able to do this:\r\n\r\n```\r\nwith tf.train.SingularMonitoredSession() as sess:\r\n    while not sess.should_stop():\r\n        sess.run(train_op)\r\n```", "Edit: Sorry, it totally works. I just realized it exits the session context entirely, but the program keeps running.  The .end() calls on hooks work as well.  @jimfleming you're right again ;)\r\n\r\n~~I'm not having success with the monitored session suggestion. The program simply exits without even reporting the exception.~~\r\n```\r\nwith tf.train.MonitoredTrainingSession() as sess:\r\n    while not sess.should_stop():\r\n        data = sess.run(next_batch)\r\n    print('stopped')\r\n```\r\nthe above code prints nothing, just exits after attempting to run past the last batch. However, the following will print \"hit exception\" forever, unless `break` is used.\r\n\r\n```\r\nwith tf.train.MonitoredTrainingSession() as sess:\r\n    while not sess.should_stop():\r\n        try:\r\n            data = sess.run(next_batch)\r\n        except tf.errors.OutOfRangeError:\r\n            print('hit exception')\r\n    print('stopped')\r\n```\r\n\r\n~~Not sure if I should make a feature request or bug report?~~ ", "Do the [High Performance Benchmarks example](https://www.tensorflow.org/performance/performance_models) recommendations still apply now that the new [Dataset API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data) exists? \r\n\r\nFor instance in the benchmark [RecordInput](https://github.com/tensorflow/tensorflow/blob/e4296aefff97e6edd3d7cee9a09b9dd77da4c034/tensorflow/python/ops/data_flow_ops.py#L2074) is [split into minibatches](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/preprocessing.py#L356), which I tried to incorporate via #10143 just before I found this. If those recommendations are all good ones, the datasets API might benefit from the addition of those very same recommendations.\r\n\r\nMight I suggest updating the benchmarks in accordance with this API and vice versa?", "I've had my head down for a while, so there's lots to respond to here:\r\n\r\n* @sjperkins: Adding `Dataset.chain()` and `Dataset.product()` iterators shouldn't be too hard. I'd like to understand your use case a little better. Do you imagine that most uses will combine exactly two datasets (and hence we might use method chaining to combine them, e.g. `ds1.chain(ds2)`, `ds1.product(ds2)`) or will it be more common to combine more datasets (and hence we'd take a similar approach to `Dataset.zip()`, e.g. `Dataset.chain([ds1, ds2])`, `Dataset.product([ds1, ds2])`)? Also note that, if you need `product()` in the short term, I think you can write `ds1.flat_map(lambda x: tf.contrib.data.Dataset.zip((tf.contrib.data.Dataset.from_tensors(x).repeat(), ds2)))`. You could also fake out `chain()` with `Dataset.flat_map()` and `tf.cond()` but that would be quite ugly :).\r\n\r\n* @snnn: Thanks for kicking the tires! It's great to hear that the new API brought a speedup for your code... we've definitely favored flexibility over performance with the initial version of the API, but look out for improvements over the coming versions.\r\n\r\n   The idea of supporting seekable file formats is very appealing, and we're figuring out a good API for that. Right now you can use `Dataset.skip()` and `Dataset.take()` to select a sub-dataset from the files, but it is not very efficient, because they materialize the skipped-over inputs before discarding them. I could imagine adding an `Iterator::Seek(size_t n)` method internally, which would allow iterators to specialize their behavior in this case (or fall back to using `GetNext()` in a loop). This also seems like it would be important for checkpointing iterators, which might be useful for fault tolerance.\r\n\r\n* @omoindrot: I agree that the `try-except` construction is pretty ugly, and we should try to find ways to improve it. The current version is designed to be a drop-in replacement for the queues, which use `tf.errors.OutOfRangeError` to signal completion, and various other classes are designed to catch that exception. Exposing an `iterator.is_empty` property would be possible, but it would be tricky to make it work in the way you suggest, because (to avoid an exception being raised) you'd need to guard the training subgraph with a `tf.cond(iterator.is_empty, make_train_op, lambda: tf.no_op())`. Another possibility would be to change the Python API for iterators to use two ops: `Iterator.move_next()` and `Iterator.get_current()` (e.g. like the C++ `IEnumerator` protocol), but that would introduce an additional `sess.run()` call, and make it harder to share the iterator between threads.\r\n\r\n   One possibility I've considered is to create a wrapper that turns an `Iterator`-consuming step into a Python iterator. e.g. Some straw-man code:\r\n\r\n   ```python\r\n   def iterate_step(sess, fetches):\r\n     cached_step = sess.make_callable(fetches)\r\n     try:\r\n       while True:\r\n         yield cached_step.run()\r\n     except tf.errors.OutOfRangeError:\r\n       pass\r\n\r\n   # ...\r\n   for _, step, loss in iterate_step(sess, [train_op, global_step, loss]):\r\n     if step % 100 == 0:\r\n       # Run periodic eval, e.g.\r\n   ```\r\n\r\n* @jimfleming and @vonclites: Thanks for looking into the `MonitoredSession` integration. It's great to hear that it \"just works\"! I think we'll still need to do something better for the more advanced cases where we might want to reinitialize an iterator in the same session.\r\n\r\n* @ahundt: From our initial experiments, the *peak* performance of the benchmark input pipeline is still slightly higher than what you get from using the `tf.contrib.data` API. However, the peak performance is much higher than the throughput of actually using the data to train a model like Inception or ResNet, so you might not notice the difference in regular use. We're investigating how to close the gap, and it's very likely that we'll incorporate some of the ideas from the benchmark code into the `Dataset` implementation.\r\n\r\n   In particular, one current limitation of the `Dataset` and `Iterator` implementation is that the entire pipeline runs on a single device, whereas the more explicit code in the benchmarks is able to split the processing across multiple CPU and GPU devices. For the best performance, it's going to be important to integrate optimizations like the `StagingArea` for prefetching data to the GPU before it is needed, and we're working on a way to do that more transparently. For now, you can manually pipeline the output of an `Iterator.get_next()` op with the `StagingArea.put()` op in a similar manner to the benchmark code.", "This is fantastic! I'm using it now and loving it. @vrv @mrry is the ability to swap between train and validation datasets already there or is that still coming? I see the reinitializable iterators give us the ability to use the same iterator with multiple datasets but each time you run an init op, it essentially starts over on that dataset. I'm currently using the new `Dataset` and `Iterator` apis along with `tf.cond` to accomplish this but is there a more direct/natural way in the works?", "Hi @mrry \r\n\r\nThe Iterator API doesn't have a read_up_to/enqueue_many like interface, Will you add it?\r\n\r\n", "> @sjperkins: Adding Dataset.chain() and Dataset.product() iterators shouldn't be too hard. I'd like to understand your use case a little better. Do you imagine that most uses will combine exactly two datasets (and hence we might use method chaining to combine them, e.g. ds1.chain(ds2), ds1.product(ds2)) or will it be more common to combine more datasets (and hence we'd take a similar approach to Dataset.zip(), e.g. Dataset.chain([ds1, ds2]), Dataset.product([ds1, ds2]))?\r\n\r\n@mrry The approach similar to Dataset.zip() please, for the flexiblity. Thanks for the workarounds. :-)\r\n\r\n\r\n\r\n", "Wanted to let you know I migrated from queues to Dataset and it's been great. Definitely going in the right direction. There's been a few things that are currently missing and I had to work around:\r\n\r\n- Being able to use dictionaries for Tensor structs, in addition to tuples and lists (this would be very natural when parsing Examples, which have named features and give you dictionaries)\r\n- Support for SparseTensors. `tf.batch` supports SparseTensor and automatic batching of SparseTensor would make my life a whole lot easier\r\n\r\nAdditionally, I had this idea where you could maybe implement a random test/train split functionality right into Dataset. This could make things easier too. Something like\r\n\r\n```python\r\ndataset_train, dataset_test = dataset.split_train_test(test_ratio=0.2, seed=1234)\r\n```\r\n\r\nKeep up the good work!\r\n", "@lhlmgr The way i understand that example is that iterator needs to be reinitialized every time you switch between train and validation. This isn't the end of the world but for large datasets where we are shuffling minibatches, we want a reasonable `buffer_size` which means each initialization is quite slow. I find the `tf.cond` approach with two separate datasets/iterators to work better/faster in that case. That way we can periodically run through validation data without losing our place int the training set.", "Maybe here is a good place to refer to my Dataset related questions:\r\nhttps://stackoverflow.com/questions/44132579/feed-data-into-a-tf-contrib-data-dataset-like-a-queue\r\nhttps://stackoverflow.com/questions/44132307/tf-contrib-data-dataset-repeat-with-shuffle-notice-epoch-end-mixed-epochs\r\n", "I really like the new Dataset/Iterator API!  Here is a feature that would help my use-case:\r\n\r\nI would like to be able to create iterators that share part of the data pipeline.  As a simple example, something like this:\r\n``` python\r\na = np.arange(12)\r\n\r\ndata = tf.contrib.data.Dataset.from_tensor_slices(a)\r\ndata = data.shuffle(12)\r\ndata0 = data.map(my_func0)\r\ndata1 = data.map(my_func1)\r\n\r\niter0 = data0.make_one_shot_iterator()\r\niter1 = data1.make_one_shot_iterator()\r\n\r\nop0 = iter0.get_next()\r\nop1 = iter1.get_next()\r\n```\r\nWhat I'd like is for `op0` and `op1` there to output elements in the same order (because they share the `shuffle` step), but with different functions (`my_func0`/`my_func1`) applied.  That is, I would like to create input pipelines that share some processing, and then diverge at some point for additional processing.", "What @drasmuss suggests is very useful for segmentation tasks where both labels and images need to be augmented. For example the images could very reasonably use bilinear interpolation, but interpolating label values is not okay because a label pixel boundary of 0 and 2 should not be interpolated to the completely different label of 1.", "I am only starting to read into the new API, but I want to share two problems that I had with the old Input Queues in concurrence with using MonitoredSession with SessionRunHooks. \r\nWe also used two separate queues, one handling input data_files as string names and the other one the resulting input data with preprocessing being done in between those two. \r\nWe needed to make sure that the enqueue operations fills at least a certain multiple of the batch size into the first queue for our code to run without problems (otherwise the second input queue stalled)\r\nLooking something like this \r\n```\r\nqueue_size_train = sess.run([ipc.train.rsq_pre_size)\r\n            while queue_size_train <= batch_size * 5:\r\n                sess.run([ipc.train.rsq_pre_enq],\r\n                         feed_dict={ipc.train.ph_in: dataset.train.inputs,\r\n                                    ipc.train.ph_tgt: dataset.train.targets})\r\n                queue_size_train = sess.run([ipc.train.rsq_pre_size])[0]\r\n```\r\n\r\nNow when I switched from a normal Session to using MonitoredSession and added a logging hook and told it to log the 'accuracy' tensor, it tried in vain to evaluate the first session run call as the hook had added that tensor to the fetch list, but with the queue being still empty there was no way to evaluate accuracy yet. \r\nProblematic was that the program just stopped and waited for some process to begin filling the queue, but there was non, so it just did nothing, but also didn't throw an exception or give any kind of warning, which made understanding what was happening a bit difficult. \r\nThe problem was eventually easily solved just executing one enqueue operation as the very first sess.run() call within the `with MonitoredSession(...) as sess:` block, but it would be nice if e.g. the input queues could be pre-filled with some initial values upon creation, so that this issue doesn't arise. \r\n\r\nAside from that, we use two different input pipelines for training and validation data that we connect to the network part of our graph alternating through a switch implemented through `tf.QueueBase.from_list`.\r\nNow in the `with ... as sess:` block you can easily implement an if block which pipeline to choose based on `global_step % interval == 0` but that means copy pasting the same code (sess.run() and add_summary() calls) times the number of different input pipelines you use (e.g. train, validation_1, validation_2, ...) \r\nIt would be nice to integrate this directly into MonitoredSession using Hooks somehow (i.e. every 10th step create and save summaries using validation_1 input pipeline, every 25th step ...)", "1. Anything that helps me analyze if and where the bottleneck lies in the input pipeline would be great. For example, a way to monitor the number of examples in the buffers along the input pipeline would be helpful. Or the number of items processed per thread of a .map() operation  Basically, something along the lines of how the queues create summaries for the number of images they are holding.\r\n2. I think others have mentioned something like this, but a way to create a Dataset from a streaming source of data. Something that provides similar functionality as the generator feed function in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/inputs/queues/feeding_functions.py  (but it would be cool if the source of the data could be from an arbitrary source, maybe using a kind of publisher/subscriber model?)\r\n\r\nThanks for the hard work. I'm digging the new API.\r\n", "In the last couple of days I was playing around quite a bit with the new Input API and I think the `Dataset` and `Iterator` classes improve highly the clearness and readability of code (compared to the old input queues). Also switching between e.g. training and validation dataset within one session works quite effortless. \r\n\r\nBut I have also some questions and suggestions.\r\nMaybe first a question: Is the `Dataset` class implemented based on queues? Because from the post here it doesn't get clear to me if or if not. In Tensorboard there is no additional information added with the new API about the status of any queue (how many objects are currently queued). Also observing my CPU/GPU resources/workload I can see, that the GPU workload drops to zero often (I guess in between batches). \r\n\r\nThen a suggestion:\r\nI think the `dataset.shuffle()` could be improved, if shuffling is not done only on the _n_ ( = buffer_size) samples in memory, but somehow on the whole list of inputs. For example: I'm storing path to images and labels in a text file. If shuffling is not done already in the text file you can often have thousands of lines after each other of the same class. If I now only work with `dataset.shuffle()` it can easily happen (depending of the buffer_size) that all elements that get shuffled are anyhow of the same class. The only. Maybe some toy example (ignoring labels and only working with image paths) to make my point clearer. For reasons of readability I work with a very small `buffer_size` and list of file names. But I guess everyone can imagine the same just with thousands of filenames in the list and a buffer_size e.g. of 5000.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimage_paths = tf.constant(['train/class1/img1.png',\r\n                          'train/class1/img2.png',\r\n                          'train/class1/img3.png',\r\n                          'train/class1/img4.png',\r\n                          'train/class1/img5.png',\r\n                          'train/class1/img6.png',\r\n                          'train/class1/img7.png',\r\n                          'train/class2/img1.png', \r\n                          'train/class2/img2.png',\r\n                          'train/class2/img3.png',\r\n                          'train/class2/img4.png',\r\n                          'train/class2/img5.png',\r\n                          'train/class1/img6.png',\r\n                          'train/class1/img7.png'])\r\n\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(image_paths)\r\ndataset = dataset.shuffle(3)\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    while True:\r\n        try:\r\n            path = sess.run(next_element)\r\n            print(path)\r\n        except tf.errors.OutOfRangeError:\r\n            break\r\n```\r\nThis would print something like:\r\n```\r\n'train/class1/img1.png'\r\n'train/class1/img2.png'\r\n'train/class1/img4.png'\r\n'train/class1/img3.png'\r\n'train/class1/img5.png'\r\n'train/class1/img7.png'\r\n'train/class2/img1.png'\r\n'train/class1/img6.png'\r\n'train/class2/img3.png'\r\n'train/class2/img4.png'\r\n'train/class1/img6.png'\r\n'train/class2/img5.png'\r\n'train/class1/img7.png'\r\n'train/class2/img2.png'\r\n```\r\nSo since there is only a shuffling between the 3 examples in the buffer, the first samples (same for batches) will all have samples only of one class. So unless the shuffling isn't done already in the list of filenames you'll have troubles training any network. And if the available dataset of images is huge, increasing the buffer_size is often not a solution.\r\nAnother problem I see, is that like shuffling currently is implemented, there is no true shuffling of the entire dataset possible. The only workaround I found was pre-shuffling the filelist I read from the text file before creating the dataset. But once the dataset is created, it's only possible to shuffle in the range of the buffer_size.", "@mrry  Thanks for the a preview of the new API; I think this is a good starting point!\r\nOne function that still seems to be missing, but would be essential for one of our primary use cases (see comment above: https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-283186552) is a\r\n\r\n`Dataset.map_to_multiple()`\r\nused as in\r\n`dataset2 = dataset1.map_to_multiple(func)`\r\n\r\nfunction, where one element of `dataset1` is mapped to one **or more** elements for `dataset2`; i.e. #`dataset2` >= #`dataset1`.\r\nAs far as I understand it, `Dataset.map()` preserves a 1:1 mapping, which is not sufficient for our use case.\r\n\r\nOne concrete example of why this would be useful:\r\nAssume `dataset1` is a list of large images (e.g. 8192x8192 each) [with corresponding labels]. Then, `dataset2` is created by (randomly) iterating over each element `d1` of `dataset1`, and for each of these elements the function `func` samples a (variable) number of sub-images [and sub-labels] (e.g. 256x256 each) from `d1`, taken from various regions of `d1`. For example, in one instance, `func` might return 142 new {image, label} pairs that will be added to `dataset2`. In another instance, it might return 389 new pairs, etc. The number of elements generated each time is variable and conditional on the properties of element `d1`.", "@kmhofmann I think you can map one example to multiple examples with `Dataset.flat_map()`. Inside your flatmap function, create a new Dataset object with one or multiple examples for every input example.", "@EdeMeijer That could be -- it's really hard to tell, as the documentation is quite sparse, with no examples. (There seem to be two places containing some amount of separate documentation: either on [GitHub](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data) or on [tensorflow.org](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/data/Dataset#flat_map))\r\nOne thing I notice is that the arguments `num_threads` and `output_buffer_size` from `map()` are missing in `flat_map()`; does that mean no parallel processing is possible? Or is this a TODO? Hard to scope both functionality and feature set...", "I based my suggestion on the one flat_map code example on the github page you linked. There, single string tensors come in (file names) and whole Datasets are emitted in the map function, so that seems pretty clear. I guess the parallel processing from `map()` is a TODO, I'm sure they'd love a PR :)", "Ah, thanks, I missed that, as it was in an example about text processing. Still, the function description in the documentation seems a bit sparse, consisting of `Maps map_func across this dataset and flattens the result`.", "input_fn has to return features and labels only. What about extra params which can be used while training progress to customise loss for given input? ", "Another feature request: it'd be great if there was an `iterator.peek()` operator, which would return the current iterator value (like `iterator.get_next()`), but not advance the iterator.  This would make it easier to coordinate multiple elements of a model that all want to read from the iterator before advancing it one step.", "Hi, firstly thanks for this API, Im very keen on using it. Primarily I am interested in using it to switch between training and validation datasets in the same process.\r\n\r\nHowever I'm confused how one does that in this new paradigm. For instance I see no way to \"get an iterator\" in the middle of a dataset. As an example here is a piece of code that demonstrates what I'd like to do. Every few steps in an epoch, I'd like to run a validation op, but the output of this code shows that the iterator never advances ahead of item 0 in either dataset.  How does one do that?\r\n\r\n```\r\ntraining_dataset = tf.contrib.data.Dataset.range(100)\r\nvalidation_dataset = tf.contrib.data.Dataset.range(900,950)\r\niterator = tf.contrib.data.Iterator.from_structure(training_dataset.output_types,\r\n                                   training_dataset.output_shapes)\r\nnext_element = iterator.get_next()\r\n\r\ntraining_init_op = iterator.make_initializer(training_dataset)\r\nvalidation_init_op = iterator.make_initializer(validation_dataset)\r\n\r\n# Run 3 epochs with 10 steps each\r\nfor e in range(3):\r\n  print(\"Epoch: %d\" %e)\r\n  for step in range(11):\r\n    sess.run(training_init_op)\r\n    ne = sess.run(next_element)\r\n    print(\"Step: %d Training set: next_element: %d \" %(step, ne))\r\n    # Run validation every 5 steps only\r\n    if step % 5 == 0:\r\n      sess.run(validation_init_op)\r\n      ne = sess.run(next_element)\r\n      print(\"Step: %d Validation set: next_element: %d \" %(step, ne))\r\n```\r\n\r\nIn the above, since we run an init each time to get an iterator pointing to its required dataset, we end up running a training and validation on the first item of each dataset, always. How does one modify this to get the updated position of the iterator in each dataset?", "@nirmalthacker Every time you run an iterator init op, it restarts the iterator. Take a look at the documentation here [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data#creating-an-iterator). This approach has the limitations i discussed [here](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-303909270).", "@jasonkriss , I see - thanks! This is what I have now, and it handles what I wanted. Is this what you meant?\r\n\r\n```\r\ntraining_dataset = tf.contrib.data.Dataset.range(100)\r\nvalidation_dataset = tf.contrib.data.Dataset.range(900,950)\r\n\r\nt_iterator = tf.contrib.data.Iterator.from_structure(training_dataset.output_types,\r\n                                   training_dataset.output_shapes)\r\nv_iterator = tf.contrib.data.Iterator.from_structure(validation_dataset.output_types,\r\n                                   validation_dataset.output_shapes)\r\nis_validating = tf.placeholder(dtype=bool,shape=())\r\nnext_element = tf.cond(is_validating, lambda:v_iterator.get_next(), lambda:t_iterator.get_next())\r\n\r\ntraining_init_op = t_iterator.make_initializer(training_dataset)\r\nvalidation_init_op = v_iterator.make_initializer(validation_dataset)\r\n\r\nsess.run([training_init_op, validation_init_op])\r\n# Run 3 epochs with 10 steps each\r\nfor e in range(3):\r\n  print(\"Epoch: %d\" %e)\r\n  for step in range(11):\r\n    ne = sess.run(next_element, feed_dict={is_validating: False})\r\n    print(\"Step: %d Training set: next_element: %d \" %(step, ne))\r\n    if step % 5 == 0:\r\n      ne = sess.run(next_element, feed_dict={is_validating: True})\r\n      print(\"Step: %d Validation set: next_element: %d \" %(step, ne))\r\n```", "@nirmalthacker Yep, that's essentially what I meant. Although, at each validation step, I will reinit the validation iterator and run through the full validation dataset. But that's certainly the gist of it.", "I'm trying migrating input pipeline from tf.train.string_input_producer & tf.train.shuffle_batch to Dataset APIs. The parameter \"allow_smaller_final_batch\" in tf.train.shuffle_batch(...) is useful when I'd like to assure all batches are evenly divisible by number of gpus. (I'm doing data parallelization on multiple gpus, and the batch_size is multiple of num_gpus). Is there any setting for Dataset APIs to drop the final smaller batch if any?", "@winston-li I think you could just use `.filter()` for that. If you know your batch size is for example 32, then something like\r\n```python\r\ndataset = dataset.filter(lambda batch: tf.shape(batch)[0] == 32)\r\n```\r\nshould do the trick.", "@ppwwyyxx I believe a custom pipeline could be designed using TF server with distributed runtime, i.e. IPC send/recv ops. It's included in C API so it won't be too hard to port it to other languages. A down side is that you do need package the whole TF runtime wherever you need this pipeline.\r\n\r\nWe are actually working on a out-of-band data plane for TF, but it is still a great deal of ongoing work. The design would be similar to the [ExternalShuffleService](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala) in Spark, using an in-memory storage such as LevelDB or LMDB, and a reader client in TF. If performance is a primary concern then it should be tightly integrated with hardware, i.e. GPU/NVMe/RNIC, etc.", "@EdeMeijer  Thanks. I thought it should work, but after some experiments, I can't make it work as expected. I followed the guidelines of dataset README.md, with pseudo code like following:\r\n\r\n    def _parse_function(example_proto):\r\n        features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n                    \"label\": tf.FixedLenFeature((), tf.int32, default_value=0)}\r\n        parsed_features = tf.parse_single_example(example_proto, features)\r\n        return parsed_features[\"image\"], parsed_features[\"label\"]\r\n\r\n    BATCH_SIZE = 256\r\n    filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\r\n    dataset = tf.contrib.data.TFRecordDataset(filenames)\r\n    dataset = dataset.map(_parse_function)\r\n    dataset = dataset.batch(BATCH_SIZE)\r\n    dataset = dataset.filter(lambda imgs, lbls: tf.shape(imgs)[0] == BATCH_SIZE)\r\n    iterator = dataset.make_initializable_iterator()\r\n    next_element = iterator.get_next()\r\n    images, labels = next_element\r\n\r\n    # Training cycles for 100 epochs.\r\n    for _ in range(100):\r\n        sess.run(iterator.initializer)\r\n        while True:\r\n            try:\r\n                images_r, labels_r = sess.run([images, labels])\r\n                print(images_r.shape)\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n \r\nAfter applied the filter, no data available in training cycles. I found the dataset (after batch, prior to filter) was in this form: \r\n\r\n    (<tf.Tensor 'arg0:0' shape=(?, 43200) dtype=float32>, <tf.Tensor 'arg1:0' shape=(?, 36) dtype=float32>)\r\n\r\nLooks like the batch dimension is \"?\" (None?), so the predicate always fails... or I did something wrong?", "@winston-li seeing \"?\" as shape is because at that point you're looking at the 'static' shape of the tensor, which isn't always defined (the graph doesn't know in advance how many examples there will be). However, `tf.shape()` evaluates the dynamic, real-time shape of a tensor, so I thought this should work.\r\n\r\nAnyway, I tried creating a minimal example, but I'm getting internal kernel errors when my filter excludes any element, so for starters I opened https://github.com/tensorflow/tensorflow/issues/10725. However, what I **did** find was that, obviously, we should use Tensorflow ops instead of standard comparisons since the result of `tf.shape()` is a tensor and not a normal array.\r\n\r\nMy tf version's filter is broken, but if yours works could you try this instead?\r\n\r\n```python\r\ndataset = dataset.filter(lambda imgs, lbls: tf.equal(tf.shape(imgs)[0], BATCH_SIZE))\r\n```", "@EdeMeijer Thank you very much, it works in my case :-)", "For sequence data, it is common to batch sequences with same (or similar) lengths. This is usually achieved by simply sorting the dataset based on sequence length. However, this does not seem possible to achieve with the current API (if yes, please let me know how to do it).\r\n\r\nIt would be nice so see something in this flavor:\r\n\r\n```python\r\ndataset = dataset.sort(lambda a, b: tf.shape(a)[0] < tf.shape(b)[0], buffer_size=10000)\r\n```\r\n\r\nwhich will sort items by chunk of 10000 (similar to `dataset.shuffle`) using the given comparison function.", "I would like to ask again (since there was no reply to my [comment](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-305435143) above: Am I right, that the new input pipeline isn't implemented using Queues? I did some tests using the new input pipeline to load an preprocess images, but it seems everything is done sequentially and their is only a negligible performance improvement over using e.g. OpenCV to load and preprocess images. I was hoping the new input pipeline would be build on top of queues, since they provide major performance boosts but make it quite hard to work with (e.g. having seperate input pipelines with queues to switch between training and validation datasets). This is quite easy with the new API but it seems there is no real performance boost. Anybody observed the same or opposite?", "@kratzert I, too, have experienced issues with getting GPUs to 100% usage and keeping them there. The Dataset API, it seems, is implemented less efficiently and though it is a welcome change regarding code clarity and simplicity as well as a more natural way of doing training and validation, it cannot (yet) substitute queues for high data rate usecases, such as computer vision.", "@kratzert You are right. The new input pipeline do not have queues.  If you aren't satisfied with the shuffling it provides, you can do it outside TF: You can load all the filenames into memory then shuffle them in any way you like.", "@snnn Yes I know and I do exactly this. But by doing this I can't find a way to shuffle the entire data in e.g. my training data every epoch. I can shuffle e.g. the list of filenames before creating a dataset, but once I start a session to my knowledge I can only shuffle the data from the dataset that I have in memory using dataset.shuffle(buffer_size). But with images this can be hardly done for the entire dataset in memory. And I can't shuffle the filenames again and create a new dataset from them, once inside the session or am I wrong?\r\n\r\n@vvekic Thanks for your reply, so I know it's not only me having performance issues. Of course the code clarity and simplicity of working with the new dataset class is a huge step forward and very welcome. But it seems that for training computer vision networks queues are still the way to go (unfortunately) as the performance boost is immense. \r\n\r\n", "@kratzert Using queues increases your performance by overlapping data loading latency, and this is independent of what you use to load your data. You can always insert queues or StagingArea in the input pipeline, regardless of whether the actual data loading is done by dataset API, the old input operators, or Python.\r\n\r\n@byronyi What I meant is to receive tensors from non-TF processes. Because as @PatWie pointed out above, data processing doesn't really need to happen in the graph.", "@ppwwyyxx do you have any example code for combining queues and the new dataset api? Sounds awesome and I will definitely try this out later. ", "@snnn Well I'll look into it, but since my knowledge in C++ isn't so profound we'll see how successfully I'll be. Anyway, I think this could be a feature that more people than I might be interested in and should/could possibly be integrated into master. ", "@kratzert https://www.tensorflow.org/performance/performance_models and the associated code shows how to use StagingArea.", "@kratzert it is certainly possible to re-shuffle the filenames for every epoch, I'm doing exactly that for my own training. You can use an initializable iterator together with a placeholder to achieve this. I'm doing something like this:\r\n```python\r\nfilenames_op = tf.placeholder(tf.string, shape=[None])\r\ndataset = tf.contrib.data.TFRecordDataset(filenames)\r\niterator = dataset.make_initializable_iterator()\r\nnext_elem = iterator.get_next()\r\n\r\nfilenames = ['file1', 'file2', 'file3']\r\n\r\n# For every epoch, shuffle the file names and initialize the iterator\r\nrandom.shuffle(filenames)\r\nsess.run(iterator.initializer, {filenames_op: filenames})\r\n```", "@EdeMeijer That's smart. Thank you very much. I should have come to this on my own! Here is a complete working code snippet for anybody interested:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnum_epochs = 2\r\n\r\nimage_paths = ['train/class1/img1.png',\r\n               'train/class1/img2.png',\r\n               'train/class1/img3.png',\r\n               'train/class1/img4.png',\r\n               'train/class1/img5.png',\r\n               'train/class1/img6.png',\r\n               'train/class1/img7.png',\r\n               'train/class2/img1.png', \r\n               'train/class2/img2.png',\r\n               'train/class2/img3.png',\r\n               'train/class2/img4.png',\r\n               'train/class2/img5.png',\r\n               'train/class1/img6.png',\r\n               'train/class1/img7.png']\r\n\r\nnum_samples = len(image_paths)\r\n\r\nfilenames_op = tf.placeholder(tf.string, shape=[None])\r\n\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices(filenames_op)\r\n\r\niterator = dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n\r\n    for epoch in range(num_epochs):\r\n        print(\"Starting epoch %i\" % (epoch))\r\n        np.random.shuffle(image_paths)\r\n        sess.run(iterator.initializer, {filenames_op: image_paths})\r\n\r\n        for i in range(num_samples):\r\n            path = sess.run(next_element)\r\n            print(path)\r\n```\r\n\r\nThis shuffles as desired on every epoch the entire dataset and gives an output e.g. like this:\r\n\r\n```\r\nStarting epoch 0\r\n'train/class2/img4.png'\r\n'train/class2/img1.png'\r\n'train/class1/img2.png'\r\n'train/class1/img7.png'\r\n'train/class1/img1.png'\r\n'train/class1/img6.png'\r\n'train/class1/img6.png'\r\n'train/class1/img5.png'\r\n'train/class1/img4.png'\r\n'train/class1/img7.png'\r\n'train/class1/img3.png'\r\n'train/class2/img2.png'\r\n'train/class2/img3.png'\r\n'train/class2/img5.png'\r\nStarting epoch 1\r\n'train/class1/img7.png'\r\n'train/class1/img4.png'\r\n'train/class2/img1.png'\r\n'train/class1/img6.png'\r\n'train/class1/img2.png'\r\n'train/class1/img3.png'\r\n'train/class1/img6.png'\r\n'train/class2/img5.png'\r\n'train/class2/img2.png'\r\n'train/class1/img1.png'\r\n'train/class2/img3.png'\r\n'train/class2/img4.png'\r\n'train/class1/img5.png'\r\n'train/class1/img7.png'\r\n```", "Currently the tutorial says that we can use \r\n```python\r\ndataset = dataset.repeat()\r\ndataset = dataset.shuffle(buffer_size=10000)\r\n```\r\nto get shuffled data. The pattern is also used in [`tf.contrib.data.read_batch_features`](https://github.com/tensorflow/tensorflow/blob/1efd7f171ba30421b5d8369a93526395a721c0d9/tensorflow/contrib/data/python/ops/dataset_ops.py#L595)\r\n\r\nHowever calling `repeat` before `shuffle` could lead to the shuffle across multiple epochs.\r\nFor example, the following code\r\n```python\r\nimport tensorflow as tf\r\n\r\ndata = tf.contrib.data\r\n\r\ndataset = data.Dataset.from_tensor_slices(['file_0', 'file_1'])\r\n\r\nrepeat_count = 5\r\nshuffle_buffer_size = 100\r\n\r\nrepeat_then_shuffle = dataset.repeat(count=repeat_count)\r\nrepeat_then_shuffle = repeat_then_shuffle.shuffle(\r\n    buffer_size=shuffle_buffer_size)\r\nrepeat_then_shuffle_iter = repeat_then_shuffle.make_one_shot_iterator()\r\nget_next = repeat_then_shuffle_iter.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    result = []\r\n    try:\r\n        while True:\r\n            result.append(sess.run(get_next))\r\n    except tf.errors.OutOfRangeError:\r\n        pass\r\n    print(result) # [b'file_0', b'file_0', b'file_0', b'file_1', b'file_0', b'file_1', b'file_0', b'file_1', b'file_1', b'file_1']\r\n```\r\ngets 3 `file_0` before getting a `file_1`.\r\n\r\nAre there any concerns about calling `shuffle` before `repeat`?\r\n", "Just want to add something here, I implemented a **multiprocess-based** data feeding pipeline for multi-task learning. It can achieve avg. GPU utilization >90% and quad-core CPU utilization >95%. Less prone to memory leak and particularly good for days-long training. Not saying it's perfect, but at least works much better than current TF queue API in my case. If anyone interested: https://hanxiao.github.io/2017/07/07/Get-10x-Speedup-in-Tensorflow-Multi-Task-Learning-using-Python-Multiprocessing/ ", "That was already done in [TensorPack](https://github.com/ppwwyyxx/tensorpack) for a while now by @ppwwyyxx. There you also get further speedup using ZMQ -- plus it has a nice interface using Python generators. For me, the way tensorpack handles input data, is the most elegant way. I hope to see something like this in a future TF.", "@PatWie thanks for pointing this out! I just quickly checked @ppwwyyxx repo really awesome! Thanks again", "It would be great to have GPU resident queues.", "> It would be great to have GPU resident queues.\r\n\r\n@xieqihuiPG See [StagingArea](https://www.tensorflow.org/api_docs/python/tf/contrib/staging/StagingArea) and [MapStagingArea](https://github.com/tensorflow/tensorflow/pull/9686)", "Would greatly appreciate:\r\n1. Efficient random sampling:\r\n`Dataset.sample_random()`\r\n2. Dynamical changing and resizing methods:\r\n`Dataset.update(), Dataset.pop()` etc., e.g. for creating streaming buffers, replay memory objects...\r\n3. Meta- and descriptive statistic integration into dataset object and supportive methods like `Dataset.describe()`\r\n4. Closer integration with HDF5 anyway", "#11591 We need efficient sampling/shuffling for large datasets", "What about supporting custom ops to create a Dataset? For example, let's say I have a Python function which returns a new batch on each call (a generator). I want to wrap this function using `tf.py_func` and use it to build a `Dataset`. This doesn't seem to be supported?\r\n\r\nI currently use this method with `tf.train.*batch*` ops and it works nicely but I'd like to find a way to do this for evaluation as well (and figured maybe Dataset is a good way to do this with the \"reintializable\" iterator).", "@mrry This is great work and definitely very useful for creating nice learning APIs on top of TensorFlow. However, I have a couple main concerns:\r\n- I cannot see a way currently to \"unzip\" a dataset. Let's say we have a trainable model that has both a `train`/`fit` method and a `infer`/`predict` method. Let's call the type of the (potentially) nested structure of inputs to our model `I` and the type of training inputs, which are only needed when training (e.g., supervision labels), `TI`. In this case, we want the `train` method to accept datasets with elements of type `(I, TI)` (i.e., a tuple of `I` and `TI`) and the `predict` method to accept datasets with elements of type `I` or `(I, TI)` (in which case it would ignore the labels). We also want the model to only have one underlying graph, supporting all these types of input. The way I could see doing that was for the underlying model to construct two iterators (one with elements type `I` and one with type `TI`) and initialize them according to the provided datasets. However, if somebody provides a dataset with elements of type `(I, TI)` to the `train` method, there is no way to unzip this dataset and initialize both iterators. One has to use `Dataset.map` twice, which is not efficient (I think but please correct me if I'm wrong) and which may also not pull matching elements from the datasets (if each pull advances the current index in the original first dataset -- I'm not sure if that happens).\r\n- It would be nice to support iterators over tensors defined in other languages as @sirfz mentioned. I cannot see an efficient way to do that with the current API. Please correct me if I'm wrong but currently one would have to create a new `TensorDataset` per batch and re-initialize an existing iterator.\r\nI think @fchollet may be able to comment on my first point as currently my understanding is that they are thinking or creating an entirely new graph for training only, for such cases (third step described [here](https://github.com/fchollet/keras/issues/7503)).\r\n\r\nAlso, if my description is terribly unclear, please let me know and I'll try to clarify.", "The new input pipelines are great! But unfortunately, we are unable to use them for large-scale training because our data preprocessing is quite costly and needs to be distributed across multiple machines--or we just haven't figured out the right way to do it. \r\n\r\nWe have thus been using the old `FIFOQueue` interface in the following manner (pseudocode):\r\n\r\n```python\r\n# Set up queues\r\nkwargs = {'capacity': ..., 'dtypes': ..., 'names': ..., 'shapes': ...}\r\ntrain_queue = tf.FIFOQueue(**kwargs)\r\nvalid_queue = tf.FIFOQueue(**kwargs)\r\nqueue_index = tf.Variable(0, trainable=False)\r\nqueue = tf.QueueBase.from_list(queue_index, [train_queue, valid_queue])\r\nbatch_size = ...\r\nbatch = queue.dequeue_many(batch_size)\r\n\r\n# Build model\r\noutput = build_model(batch['X'])\r\nloss = evaluate_loss(output, batch['y'])\r\n\r\n# Fill queues\r\ntrain_data_stream = some_iterable_for_training_data()\r\nvalidation_data_stream = some_iterable_for_training_data()\r\nstart_filling_queues_in_background_thread(train_queue, train_data_stream)\r\nstart_filling_queues_in_background_thread(validation_queue, validation_data_stream)\r\n```\r\n\r\nHaving two different queues with `from_list` allows us to switch between the training and validation queue by either setting the `queue_index` or feeding it in the `feed_dict`.\r\n\r\nThe `some_interable_for_xxx_data` are usually generators that get data from a bunch of workers sitting behind a load balancer (e.g. using ZeroMQ, RabbitMQ, or PubSub). This approach works well (because the queues provide a buffer) but we don't have any way of telling when the iterator is exhausted. Some workarounds are\r\n\r\n* closing the queue in the background thread such that a `tf.errors.OutOfRangeError` is raised when the queue is exhausted (but then we can't reopen it again #4535)\r\n* setting a timeout on the `session.run` of the training op and assuming that a timeout is due to the queue being exhausted (but the network connection might be down or our workers might be too slow)\r\n* counting the number of items we've processed and comparing with the expected number of items in the iterator (but that's fiddly and sometimes we don't even know how long the iterator is)\r\n* adding an `exhausted` field to the queue `names` and letting the background thread enqueue an item with `exhausted=True` together with an assertion around the dequeue operation (but using `dequeue_many` will dequeue elements from the next epoch if the number of items per epoch is not an integer multiple of the batch size, see also #2514)\r\n\r\nNone of these are satisfactory and it would be great to see either the ability to construct `Dataset`s from python iterator *with a queue for buffering* or fix #4535 (which will automatically fix #2514).\r\n\r\nLooking forward to hear whether we've just not been using the datasets API right.", "I think the queues are nice enough. I'd like to see two things improved though:\r\n\r\nAn easier way of inputting data from native python other than using placeholders, and managing threads. \r\n\r\nMaybe a class `InputQueue(delegate, fn, n_filler_threads)` that takes a tensorflow queue delegate and a python function `fn`. `fn` returns a (possibly nested) tuple of np.array or lists. The InputQueue starts `n_filler_threads` that calls `fn` and puts these on the `delegate`. The threads are daemons so shuts down when the main process does. \r\n\r\nAnyway, that's just my thoughts. It's probably a lot harder than this due to the static requirements of tensorflow. Maybe you just have to provide the sizes when you create the `delegate`.", "I am using the new api `Dataset` now. But still find the problem that how to dynamically feed data to the Dataset. There are two similar questions in [here](https://stackoverflow.com/questions/45788433/how-to-enable-dataset-pipeline-has-distributed-reading-and-consuming) and [here](https://stackoverflow.com/questions/44132579/feed-data-into-a-tf-contrib-data-dataset-like-a-queue)@albertz.\r\n\r\nAs you can see, the real-world problems are more than just feeding into a series of images or texts. So I would really appreciate if you could let me to feed the data **freely** in terms of **when** and **how**.\r\n\r\nI can image two options. One is efficient distributed reading through `feed_dict`. Although it is slow, but with multi-processing, it is just a matter of machine. The other one is to _wrap_ some mature and widely accepted implementation. ", "use placeholder as input to a queue, and the model reads inputs from the queue, then use a session run thread to feed inputs(maybe produced by hadoop mapreduce) to the queue. use staging area you can even hide all preprocessing and input time.", "I'm trying to test the example in the [doc](https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets#preprocessing_data_with_datasetmap)\r\n\r\nBut seems that this call is passing only 1 argument to the function:\r\n> dataset = dataset.map(_parse_function)\r\n\r\nInstead the function is defined with two parameter\r\n", "@eaplatanios one relevant PR for zip/unzip is https://github.com/tensorflow/tensorflow/issues/10837", "@mrry Have you tested this [section of the documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md#applying-arbitrary-python-logic-with-tfpy_func) with python3? ", "@bhack I haven't been able to make it work with more than one parameter in return from the function given to py_func. I'm using python3 and didn't tried with python2.\r\nIs your problem similar ?", "@AMairesse The first problem was solved with https://github.com/tensorflow/tensorflow/commit/2139e7d8b10764f2245f34548f6fbfc25d29bff8", "@bhack Thanks, will try that soon, I was using a workaround which I'm not proud of :-)\r\nThe fix in the documentation is one month old and prior to v1.3 release, the tensorflow.org website is not updated when there is a new release ? [Official doc does not have the fix](https://www.tensorflow.org/programmers_guide/datasets#applying_arbitrary_python_logic_with_tfpy_func)", "@AMairesse I suggest you to notify this in https://github.com/tensorflow/tensorflow/issues/11786", "need more operator for image process, like [map_coordinates](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.map_coordinates.html), so We can build image augmentation pipe line only use tensorflow \r\n\r\nAnd Dataset do not stably init variable defined in the map function as https://github.com/tensorflow/tensorflow/issues/12648", "I'd like to re-raise an earlier performance-related question by @kratzert that seems to have fallen out of focus. The performance gain of using the new Dataset API is negligible.\r\n\r\n@ppwwyyxx stated that queues and StagingArea can still be used with the Dataset API, but I still haven't seen a working example of this. Do we have one?\r\n\r\nWhat purpose does the new API serve if one must still include queues, data_flow_ops or StagingArea complexities?", "@vvekic, I experimented a bit with queues and the Dataset API after realising in horror that of the 0.8s/step in my inference loop, 0.2s is data fetching (with GPU at 0% utilization), raising to almost 2 seconds if the HDD is being used by something else at the same time.\r\nMy pipeline looks as follows:\r\n\r\n```\r\n  def preprocess_image(fn):\r\n    im_s = tf.read_file(fn)\r\n    im = tf.image.decode_jpeg(im_s, channels=3)\r\n    im = inception_preprocessing.preprocess_for_eval(im, width=299, height=299)\r\n    return fn, im\r\n\r\n  dataset = tf.contrib.data.Dataset.list_files('{}/*/*.jpg'.format(FLAGS.dataset_dir))\r\n  dataset.map(preprocess_image, num_threads=FLAGS.num_threads)\r\n  iterator = dataset.make_one_shot_iterator()\r\n  input_queue = tf.FIFOQueue(capacity=100*FLAGS.batch_size,\r\n                             dtypes = iterator.output_types,\r\n                             shapes=iterator.output_shapes)\r\n  enqueue_sample = input_queue.enqueue(iterator.get_next())\r\n  tf.train.add_queue_runner(tf.train.QueueRunner(input_queue, [enqueue_sample]*FLAGS.num_threads))\r\n  \r\n  filenames, images = input_queue.dequeue_up_to(FLAGS.batch_size)\r\n```\r\nI still have to run this on a big dataset and check if there's any performance improvement, but at least it seems to execute correctly. The catch is, I couldn't find a way to iterate over the data more than once (which luckily enough is not my use-case), because the only iterator that won't raise an error when the `QueueRunner`s spawn the threads is the `one_shot_iterator`.\r\n", "I don't know if I'm right here, but I have a question about the dataset API. My dataset contains one column with sequences and one with sequence length which i want treat different, because i want to pad the sequences. Is it possible to address a single column in the dataset so that it is treated different from the other column? E.g.:\r\n\r\n```python\r\ntwo_column_dataset = ... # This contains the column sequence and sequence length\r\nfirst_column_dataset = two_column_dataset[0].padded_batch(64, ...) # Pad only first column\r\nsecond_column_dataset = two_column_dataset[1].batch(64) # Get corresponding sequence length for sequences\r\ntwo_column_dataset = Dataset.zip((first_column_dataset, second_column_dataset))\r\n```\r\n\r\nEdit: After writing this, i found it out:\r\n\r\n```python\r\ndef flat_map_func(sequence, sequence_length):\r\n    first_column_dataset = Dataset.from_tensors(sequence).padded_batch(64, ...)\r\n    second_column_dataset = Dataset.from_tensors(sequence_length).padded_batch(64)\r\n    zipped_dataset = Dataset.zip((first_column_dataset, second_column_dataset))\r\n    return zipped_dataset\r\n\r\ntwo_column_dataset = two_column_dataset.flat_map(flat_map_func)\r\n```", "This issue thread is becoming a bit unwieldy and it's getting hard to keep track of the individual discussions, so I'm going to lock it after responding to a few of the recent comments. Please feel free to open a new issue about any specific topics of feature requests related to `tf.contrib.data` and we can continue the discussion there.\r\n\r\nIn response to a few recent questions:\r\n\r\n* @GPhilo ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-325698349)) and @kratzert ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308845375)): The Dataset API includes methods for prefetching, so it shouldn't be necessary to add a queue here, and you can retain the other advantages of Datasets (like reinitialization etc.). Passing `output_buffer_size=100 * FLAGS.batch_size` to the `dataset.map()` call, and following that with `dataset.batch(FLAGS.batch_size)` will run your `preprocess_image` function in parallel and should decently increase the performance.\r\n\r\n  ```python\r\n  dataset = tf.contrib.data.Dataset.list_files('{}/*/*.jpg'.format(FLAGS.dataset_dir))\r\n  dataset = dataset.map(preprocess_image, num_threads=FLAGS.num_threads,\r\n                        output_buffer_size=100*FLAGS.batch_size)\r\n  dataset = datsaet.batch(FLAGS.batch_size)\r\n  iterator = dataset.make_one_shot_iterator()\r\n  filenames, images = iterator.get_next()\r\n  ```\r\n\r\n  Note that in TF 1.4 there will be a `Dataset.prefetch()` method that makes it easier to add prefetching at any point in the pipeline, not just after a `map()`. (You can try it by downloading the current nightly build.)\r\n\r\n  In reponse to @kratzert's [specific question](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308845375) about the implementation, the `Dataset` and `Iterator` classes don't use TensorFlow's previous producer/consumer queues (such as `tf.FIFOQueue` or `tf.RandomShuffleQueue`), but they do include simpler (and more efficient) implementations of the core ideas. For example, `Dataset.prefetch()` will start a background thread to populate a ordered buffer that *acts like* a `tf.FIFOQueue`, so that downstream pipeline stages need not block. However, the [`prefetch()` implementation](https://github.com/tensorflow/tensorflow/blob/d236d19f7753ae23f91d794701efa70ace1629da/tensorflow/core/kernels/prefetch_dataset_op.cc) is much simpler, because it doesn't need to support as many different concurrent operations as a `tf.FIFOQueue`.\r\n\r\n* @vvekic ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-325357396)): I'd be curious to see your code before and after trying the Dataset API, and perhaps you could follow up by opening an issue describing the performance bottleneck. Compared to feeding or a (non-`StagingArea`) queue-based pipeline, the new API should be more efficient, and I'd be curious to know which parts aren't! \r\n\r\n  At present, you're correct that the `StagingArea` functionality is not included in the Dataset API, and for peak performance in GPU workloads you will need to add a staging area manually. However, we are actively working on implementing Datasets that can span devices (see 19a55725af8102d72d4e081c5139f0e4bd5a4bb7 for some of the work in progress) and one of the first use cases for that is to support prefetching into GPU memory.\r\n\r\n* @tengerye ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-323665406)): For dynamically feeding data into a Dataset, I'd suggest you try out the `Dataset.from_generator()` method that we're adding to TF 1.4 (and which is available in nightly builds already). I answered @albertz's Stack Overflow question about doing this [here](https://stackoverflow.com/a/45928467/3574081). (Supporting distributed pipelines will depend on the cross-device Dataset support that I mentioned in the last answer, and we'll be implementing that soon.) I think this will also work for @rasmusbergpalm's [request](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-322407451), because you can create concurrent generators, and for @tillahoffmann's [request](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321976369) and @sirfz's [request](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-321091444) as well. This API is very new though, so if you have any feedback, please let us know!\r\n\r\n* @jasonkriss ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-303602263)) We've implemented something called \"feedable\" iterators, which let you switch the input for single graph between multiple iterators (e.g. one for training and one for testing). The programmers' guide has [more details](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator) about how to use this feature.\r\n\r\n* @guillaumekln ([link](https://github.com/tensorflow/tensorflow/issues/7951#issuecomment-308789560)) If you want to batch sequences with different lengths, you can use the `Dataset.group_by_window()` transformation. Have a look at [how this is used in the NMT model code](https://github.com/tensorflow/nmt/blob/04c8c04a8b4e805f3d0a9c42b4d17c85f1324c55/nmt/utils/iterator_utils.py#L194) for an example.\r\n\r\nThanks again to all of you for your continued interest in this part of TensorFlow!"]}, {"number": 7950, "title": "Allow zero samples in RandomGamma", "body": "Fixes #7406.  Instead of an error with zero samples,\r\nwe now return a zero-shape tensor.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 7949, "title": "Can't do \"weights\" quantization on an LSTM RNN", "body": "On TensorFlow 1.0, trying to do quantization on the following graph:\r\n\r\n```python\r\ncell = tf.contrib.rnn.BasicLSTMCell(num_units=64)\r\n\r\noutputs, _ = tf.nn.dynamic_rnn(\r\n    cell=cell,\r\n    dtype=tf.float32,\r\n    sequence_length=tf.constant([3, 2]),\r\n    inputs=tf.constant([[[1.,1.,1.]], [[1.,1.,0.]]]))\r\n\r\noutputs = tf.identity(outputs, name=\"y\")\r\n```\r\n\r\nFails with:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"import.py\", line 18, in <module>\r\n    result = sess.run(outputs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'rnn/while/Select_1' has inputs from different frames. The input 'rnn/while/basic_lstm_cell/add_1' is in frame ''. The input 'rnn/while/GreaterEqual_1' is in frame 'rnn/while/rnn/while/'.\r\n```\r\n\r\nInstructions to reproduce here: https://github.com/reuben/tf-export-test (just run reproduce.sh)\r\n\r\nIt seems like the quantization code is unaware of control flow contexts, so it creates a broken graph when adding dequantize nodes. Interestingly, using a BasicRNNCell instead of a BasicLSTMCell works. Maybe because BasicRNNCell only does a single matmul + bias_add, and those have quantized kernel implementations and don't need dequantize nodes? It's not clear to me.", "comments": ["@cwhipkey or @keveman might have some ideas here.", "@andrehentz and @petewarden as well.", "Have you tried using the new Graph Transform Tool approach:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#shrinking-file-size\r\nIt's a replacement for the quantize_graph Python script, and it may have more success.", "That does seem to work! Thanks :)", "I use bazel-bin/tensorflow/tools/graph_transforms/transform_graph, but still can't work;\r\n\r\ntransform is\r\n\r\n```\r\n        --transforms='fold_constants(ignore_errors=true)\r\n                      fold_batch_norms\r\n                      quantize_nodes\r\n                      quantize_weights \r\n                      fold_old_batch_norms\r\n                      sort_by_execution_order\r\n                      round_weights(num_steps=256)\r\n                      fold_constants(ignore_errors=true)\r\n```\r\n\r\nerror is \uff1a\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'sentence_embedding/birnn_text_embedding/birnn/birnn/fw/fw/while/gru_cell/gates/gates/MatMul_eightbit/sentence_embedding/birnn_text_embedding/birnn/birnn/fw/fw/while/gru_cell/gates/gates/MatMul/Enter/reshape' has inputs from different frames. The input 'sentence_embedding/birnn_text_embedding/birnn/birnn/fw/fw/while/gru_cell/gates/gates/MatMul/Enter' is in frame 'sentence_embedding/birnn_text_embedding/birnn/birnn/fw/fw/while/sentence_embedding/birnn_text_embedding/birnn/birnn/fw/fw/while/'. The input 'sentence_embedding/birnn_text_embedding_1/birnn_text_embedding/EmbeddingLookupUnique/embedding_lookup/Reshape_1_eightbit/sentence_embedding/birnn_text_embedding_1/birnn_text_embedding/EmbeddingLookupUnique/embedding_lookup/reshape_dims' is in frame ''.\r\n```", "I'm facing with the same problem."]}, {"number": 7948, "title": "Second derivative computation in softmax xent op", "body": "PR for issue #7403\r\n\r\n@girving I rewrited second derivative with reshapes and one `matmul`, no more `diag_part` or `reduce_sum`.\r\n\r\n* Also, I writed test for checking that no extra computation performed when only first gradient is requested.\r\n\r\n* I am not sure whether I implemented hessian correctness test properly, but if I did it poorly, please guide me into right way.\r\n\r\n* Is such fix suites trivially for `_SparseSoftmaxCrossEntropyWithLogitsGrad`? If so, I will modify it too.\r\n\r\n* I think that I can fix `SoftmaxGrad` op, because it uses `reduce_sum(grad * softmax)` approach to compute the derivative. I could do it with `matmul` and reshapes.", "comments": ["@girving fixed", "Can one of the admins verify this patch?", "Jenkins, test this please.", "Looks like we're having infrastructure issues, so there will be a delay before the tests can be run.", "Jenkins, test this please.", "@gunan Are Jenkins builds still broken?  I fired off the last build after you said they were good, but it still failed with a timeout.", "They are fixed internally, but externally we do not have the workaround.\r\nWe can separately make the same change to fix the external builds\r\n@yifeif could you send a PR?", "Fix at #7953 ", "Jenkins, test this please.", "Jenkins, test this please.", "@girving tests look okay. What about `SparseSoftmaxCrossEntropyGrad`? Also, maybe `SoftmaxGrad` needs to be rewritten? For now, the implementation of it is this:\r\n```python\r\nsoftmax = op.outputs[0]\r\ngrad_x = ((grad_softmax - array_ops.reshape(\r\n    math_ops.reduce_sum(grad_softmax * softmax, [1]), [-1, 1])) * softmax)\r\nreturn grad_x\r\n```", "@persiyanov Thank you for the contribution!  Looks like `SparseSoftmaxCrossEntropyGrad` has the same issue as you say.  For `SoftmaxGrad`, and you referring to the fact that it doesn't use `MatMul`?", "@girving Yes, I say that I also could fix the sparse xentropy grad (in the same way) and rewrite softmax grad with `matmul`, not reshape followed by `reduce_sum`. What do you think?", "That would be good, but it's worth checking that I'm actually right that it's a performance win.  It's always possible that I've misled you. :)", "@girving Okay, I will measure the performance before and after and if its okay I will make a PR."]}, {"number": 7947, "title": "Allows C++ API to be built on Windows within Visual Studio and Debug configurations.", "body": "Problem: TensorFlow does not build directly from Visual Studio.\r\n\r\n**Cause 1**: The CMake scripts were written to assume building from CMake directly due to it's usage of the CMAKE_BUILD_TYPE variable.  If you generate a Visual Studio solution and build from directly within that solution, the build will fail because Visual Studio has multiple-configurations, rendering this variable blank when generating a solution.\r\n**Solution 1**: Replace CMAKE_BUILD_TYPE with $(Configuration) which is a Visual Studio macro.  During build time, Visual Studio will replace $(Configuration) with the currently active configuration being built.\r\n\r\n**Cause 2**: The CMake build scripts were written to assume 'Release' builds would only be built.  When building in Debug, some external libraries generate 'lib' files with a 'd' fixed to the end of the name.  However, the reference to these 'lib' files do not account for the 'd' at the end of the name.  During debug builds, linker errors occur when the file is not found.\r\n**Solution 2**: use the CMake command 'debug' and 'optimized' to specify references to difference file names upon 'Debug' and 'Release' builds respectively.\r\n\r\n**Cause 3**: The CMake build scripts were written to assume 'Release' builds would only be built.  When building in Debug, linker errors occur because _ITERATOR_DEBUG_LEVEL is defined and set to zero for all configurations.  External libraries (like Eigen) build with it's own CMake scripts do not define this, causing a different code path to be built.\r\n**Solution 3**: Only define _ITERATOR_DEBUG_LEVEL in Debug builds.\r\n\r\n**Cause 4**: Visual Studio compile errors occur.  Functions must always have a return value.\r\n**Solution 4**: For function stubs without proper implementation, we return NULL.\r\n\r\n**Cause 5**: Visual Studio compile errors occur.  We define a \"double\" code path for Eigen that is not available for CPU builds using SSE only.\r\n**Solution 5**: We remove the \"double\" code path if AVX and CUDA are not available on Windows builds.  This error only shows up on Debug builds.  I'm assuming because the compiler doesn't strip out unused symbols and tries to link against it.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@markslwong, can you please sign the CLA before the review starts. Thanks.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Ping! What's the status of this? If it is abandoned, I will close it soon. ", "I've been chipping away at it.  I've had to setup a new environment at home to be able to work at it.  I'm leaving my current job next week... so haven't had time at work.  Sorry it's been a bit slow.  :)  But I have worked been putting in some time this weekend and yesterday night.", "Thanks @markslwong, comment when you're done so @mrry knows to take another look.", "I've disabled the problematic kernel directly as suggested by @mrry \r\n\r\nI've placed the D_ITERATOR_DEBUG_LEVEL in RelWithDebug and MinSizeRel to address @snnn   However, RelWithDebug never worked for me as I discuss later in a bigger issue below.\r\n\r\nI don't know why GitHub diff shows the whole file as different.  My first assumption was it was different line endings... but after a binary diff it showed that it had not changed.\r\n\r\nA new change that I want to get in is to not have to play with the Debug flags for a Debug build.  Visual Studio runs out of memory when building (even when using the 64-bit version of the compiler on a machine with 8gb physical memory and 126gb page file available).  This can be resolved by removing the /Zi /Od and /RTC1 flags... which effectively build a release build with _DEBUG defined.  Since our company statically links to Tensorflow... we cannot create a Debug build because the _DEBUG flag is defined for our builds... and when we build the headers, it runs a different code path which runs into linker errors.  Doing this allows us to get a lib files that we make a Debug build of our application and Debug our own code.  However, we cannot Debug the Tensorflow code as a consequence.  So I'm not sure whether you want this change or not.\r\n\r\nThen there is the bigger question of RELWITHDEBINFO.  I did not make the same changes to this build.  I just let it fail naturally because it should always fail on Windows.  I could apply the same changes as I did with Debug.  However, I feel it's entirely contradictory to it's name... so I think failure is an acceptable outcome in this circumstance.\r\n\r\nLet me know your thoughts.\r\n", "Hi @markslwong could you tell me why it fails? At least it works for me. Have you tried to compile a RELWITHDEBINFO build from master HEAD ?", "I have not.  I going to assume the difference might be command line build single configuration build vs Visual Studio IDE build.", "I'll test my repo for RelWithDebugInfo and report the outcome.  I'm assuming it would work.", "RelWithDebInfo works for sure - I have been using those ever since windows is supported.", "RelWithDebug works for me on both Tensorflow HEAD and my fork with the command line.  It is the good idea to keep things as is.  However this brings the bigger question of what to do with the Debug build.  I can remove those flags.  However, the consequence of doing so is that it can be only be built via MSBuild.  I've setup my previous company to build directly from Visual Studio so I have a personal preference to keep it in there so that they can continue with the existing workflow.  I also think building a solution within Visual Studio seems like a fairly reasonable thing to do.  As most people are likely users of the Tensorflow libraries and not necessarily going to need to be debugging it... I would guess that this would be the most common path.  However, I can only take a guess from my own bias lens.  I don't have any true statistics as to the most typical way to build.", "@markslwong could you pull rebase and push again?", "@drpngx will do.  ", "Maybe it would simplify to split this PR into 2:\r\nThere is the issue that debug builds don't compile - should be a nice, clean PR to fix this.\r\n\r\nThere is the other part in this PR to replace CMAKE_BUILD_TYPE with $(Configuration) which worries me a little because  CMAKE_BUILD_TYPE is defined by cmake where $(Configuration) is only known by Visual Studio and should not be in cmake ... the cmake generator for Visual Studio is supposed to deal with this. I  normally don't switch the configuration in VS but just tried and it seems to be doing the right thing (cmake 3.8). CMAKE_BUILD_TYPE is used consistently in the cmake files, including the external projects (like grpc), CI, ... so if it is changed the review and test are much more involved.\r\n", "@guschmue I have been running CMake 3.7.2, so I was curious to see if something had changed to make it work.  I upgraded to CMake 3.8.0 (official release) and tried to compile with Visual Studio 2017 Community Edition on the tensorflow official repository at commit 97c6203.  Seems like I get the same errors that I tried to fix with this change.\r\n\r\nI think there might be a misunderstanding as to where the errors are introduced.  If you use CMake on the command line and generate a single configuration solution, regardless what version of CMake (3.7.2 or 3.8.0), Tensorflow will successfully build.  The problem is introduced when you generate a multi-configuration solution.  This is done by default if you use the CMake GUI.\r\n\r\nI understand your concerns about not relying on CMAKE_BUILD_TYPE.  I was equally as concerned as I was trying to find a solution.  I thoroughly went through the CMake documentation and could not find an equivalent option for multi-configuration solutions.  \r\n\r\nYou are correct external projects like grpc do use the CMAKE_BUILD_TYPE flag.  But that is okay because Visual Studio will call CMake during the build for the active configuration.  I have not needed to modify the CMake scripts of these projects.  Most of what I have changed is when the build references a build output.  (eg: a pre-build step that calls `proto.exe` will actually look like this: `protobuf\\src\\protobuf\\\\\\\\protoc.exe` in the generated solution for multiple configurations.  This is wrong since the \\\\\\\\ is missing the build configuration in between.)\r\n\r\nThe changes that I have made are in cmake scripts are nested within \"Windows\" only build configurations.  I have tested this change both against the single-configuration solutions with the command line and multi-configuration with the CMake GUI and building within Visual Studio.\r\n\r\nGiven this new information... are you still concerned about the change?  Would you still like me to separate this into two CL's? ", "I guess if $(Configuration) is under if(WIN32) it should not break something and I should not be worried about it.\r\nSplitting the pr:  smaller pr addressing one topic is easier but I with this one already there ...\r\n", "@markslwong is this ready for @mrry to take another look?", "friendly ping, is there progress being made here behind the scenes that we should wait for?", "Not yet.  Sorry... after the new job I moved and purchased a home.\r\n\r\nThings are starting to get back to normal.  I'll rebase this before Monday.", "Can one of the admins verify this patch?", "I was following this PR with interest... @markslwong are you planning to open a new one (or multiple)?", "Hi @adennie.  GitHub was having trouble with the diff... so I reapplied the changes on head revision and force pushed.  I thought that GitHub would treat it as the same pull request but it did not.  A new pull request has been made here https://github.com/tensorflow/tensorflow/pull/9666", "@mrry The changes have been reapplied here https://github.com/tensorflow/tensorflow/pull/9666"]}, {"number": 7946, "title": "(Saver.max_to_keep) should keep every checkpoint by default", "body": "I was wondering why my checkpoints were being deleted and it's only after looking up the behavior of `Saver.max_to_keep` in the docs that I understood what was happening. I think a better default setting for this is to keep all checkpoints, at least that's what I would have expected.", "comments": ["@tgy Thanks for filing the issue!\r\n\r\nDefault values are always tricky; no matter what default we choose, someone will be surprised.  In this case I believe we've chosen the default max_to_keep=5 to avoid filling up your disk space.  It would be unfortunate if you left a job running for a while, only to discover that all of your disk had been used up by checkpoints!\r\n\r\nIt's true that it might be surprising upon first encountering the default max_to_keep=5 behavior, but we hope it's the \"safer\" alternative.\r\n\r\nI'm closing this out.  If you feel I've missed something, feel free to respond to this issue, and I'll re-open.  Or file new issues if you encounter them.  Thanks!"]}, {"number": 7945, "title": "Input producer freezes if queue runner not started", "body": "Hi,\r\n\r\nmy entire program freezes when I pop an image from an input queue before starting the queue runner. The terminal window becomes unresponsive (including CTRL+C or CTRL+D signals) and the only way to kill the process is to close the terminal window. In rare cases zombie processes remain even after closing the terminal. \r\nI understand that I am [not supposed to do that](https://github.com/tensorflow/tensorflow/issues/7034), but I think that Tensorflow absolutely HAS to check whether the queue runner is already initialized and throw an exception otherwise. Several reported issues might be related to that: [7503](https://github.com/tensorflow/tensorflow/issues/7573), [7573](https://github.com/tensorflow/tensorflow/issues/7573) etc.\r\n\r\nFor reproducibility I provide a minimum (non-)working example. It requires a subfolder `images` with a handful of .jpg images:\r\n```\r\nimport tensorflow as tf\r\n\r\n# Read images from queue\r\nfilename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(\"./images/train/*.jpg\"), shuffle=False)\r\nimage_reader = tf.WholeFileReader()\r\n_, image_file = image_reader.read(filename_queue)\r\nimage_query = tf.image.decode_jpeg(image_file)\r\n\r\n# Decode the image as a JPEG file, resize and assign to a variable\r\nimage_query = tf.image.resize_images(image_query, (256, 256))\r\nimage_batch = tf.Variable(tf.zeros((256, 256, 3)), trainable=False, name=\"image_batch\")\r\nnext_image = tf.assign(image_batch, image_query)\r\nloss = tf.reduce_mean(image_batch)\r\n\r\n# Start a new session to show example output.\r\nwith tf.Session() as sess:\r\n    # Fix randomness, init variables and run threads\r\n    tf.set_random_seed(42)\r\n    tf.global_variables_initializer().run()\r\n\r\n    # Go to the next image\r\n    sess.run([next_image])\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n    for i in xrange(0, 5):\r\n\r\n        # Get an image tensor and print its value.\r\n        image_tensor = sess.run([image_batch])\r\n        print(image_tensor[0][0, 0, 0])\r\n\r\n    # Finish off the filename queue coordinator.\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\nand here is the working code (the line `sess.run([next_image])` was moved):\r\n```\r\nimport tensorflow as tf\r\n\r\n# Read images from queue\r\nfilename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(\"./images/train/*.jpg\"), shuffle=False)\r\nimage_reader = tf.WholeFileReader()\r\n_, image_file = image_reader.read(filename_queue)\r\nimage_query = tf.image.decode_jpeg(image_file)\r\n\r\n# Decode the image as a JPEG file, resize and assign to a variable\r\nimage_query = tf.image.resize_images(image_query, (256, 256))\r\nimage_batch = tf.Variable(tf.zeros((256, 256, 3)), trainable=False, name=\"image_batch\")\r\nnext_image = tf.assign(image_batch, image_query)\r\nloss = tf.reduce_mean(image_batch)\r\n\r\n# Start a new session to show example output.\r\nwith tf.Session() as sess:\r\n    # Fix randomness, init variables and run threads\r\n    tf.set_random_seed(42)\r\n    tf.global_variables_initializer().run()\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n    for i in xrange(0, 5):\r\n        # Go to the next image\r\n        sess.run([next_image])\r\n\r\n        # Get an image tensor and print its value.\r\n        image_tensor = sess.run([image_batch])\r\n        print(image_tensor[0][0, 0, 0])\r\n\r\n    # Finish off the filename queue coordinator.\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```", "comments": ["This is the expected behavior, if you look at the doc of `dequeue` function from `tf.QueueBase`:\r\n\r\n> If the queue is empty when this operation executes, it will block until there is an element to dequeue.\r\n", "It makes sense for a thread to wait for more input from a queue. However it does not make sense for the program to wait if the threads haven't even been started yet!", "@nightrome Thanks for filing the issue!  Unfortunately there isn't a simple fix to this problem with the existing input pipeline design.  Currently the best advice is to always make sure you've started the queue runner.\r\n\r\nI know this isn't satisfying; we don't like it either.  That's the purpose of the referenced #7951 , which aims to simplify usage of input pipelines.  We're actively working on this!\r\n\r\nAs for the freezing problem, please follow up on #7573 that you referenced.  Thanks!\r\n\r\nI'm closing this out.  Please don't hesitate to file new issues, or follow up on this issue if I've overlooked something, and I'll re-open this issue."]}, {"number": 7944, "title": "can not save the .ckpt file ", "body": "\r\n\r\n\r\nI run such piece of codes on my lab server (Tensor flow 1.0 + cuda7.5)\r\n`\twith tf.Session() as session:\r\n\t\twith tf.device(\"/cpu:1\"):\r\n\t\t\tsession.run(init)\r\n\t\t\tcoord = tf.train.Coordinator()\r\n\t\t\tthreads = tf.train.start_queue_runners(coord=coord)\r\n\t\t\tmax_iter = 9000\r\n\t\t\titer= 0\r\n\t\t\tif os.path.exists(os.path.join(\"model\", 'model.ckpt')) is True:\r\n\t\t\t\ttf.train.Saver(max_to_keep=None).restore(session, os.path.join(\"model\", 'model.ckpt'))\r\n\t\t\twhile iter < max_iter:\r\n\t\t\t\tloss_np, _, label_np, image_np, inf_np = session.run([loss, opti, batch_image, batch_label, inf])\r\n\r\n\t\t\t\tif iter % 50 == 0:\r\n\t\t\t\t\tprint 'trainloss:', loss_np\r\n\t\t\t\t#if iter % 1000== 0:\r\n\t\t\t\t\t#tf.train.Saver(max_to_keep=None).save(session, os.path.join('model','model.ckpt'))\r\n\t\t\t\titer = iter+1\r\n\r\n\t\t\t# \u00a0if iter%500==0: \u00a0\r\n\t\t\t# \u00a0accuracy_np=session.run([accuracy]) \u00a0\r\n\t\t\t# \u00a0print '***************test accruacy:',accuracy_np,'*******************' \u00a0\r\n\t\t\t# \u00a0tf.train.Saver(max_to_keep=None).save(session, os.path.join('model','model.ckpt')) \u00a0\r\n\t\t\t\t#iter += 1\t\r\n\t\t\tcoord.request_stop()# queue\r\n\t\t\tcoord.join(threads)\r\n\t\t\ttf.train.Saver(max_to_keep=None).save(session, os.path.join('model','model.ckpt'))`\r\n\r\nI get this error:(last line of my codes)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'save/restore_all': Could not satisfy explicit device specification '/device:CPU:1' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/gpu:0, /job:localhost/replica:0/task:0/gpu:1, /job:localhost/replica:0/task:0/gpu:2, /job:localhost/replica:0/task:0/gpu:3, /job:localhost/replica:0/task:0/gpu:4, /job:localhost/replica:0/task:0/gpu:5, /job:localhost/replica:0/task:0/gpu:6, /job:localhost/replica:0/task:0/gpu:7\r\n\r\nI am new to tensorflow, Hope for help!\r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I found the answer \uff1a\r\n**Define the Saver in Your computing Graph \uff08before the Session.run\uff09**\r\nlike this \r\n```\r\nsaver = tf.train.Saver() \r\ninit = tf.initialize_all_variables() \r\nwith tf.Session() as session: with tf.device(\"/cpu:1\"):\r\n\t\t\twhile iter < max_iter:\r\n\t\t\t\tloss_np, _, label_np, image_np, inf_np = session.run([loss, opti, batch_image, batch_label, inf])\r\n\t\t\t\t#print image_np.shape\r\n\t\t\t\t#print label_np.shape\r\n\t\t\t\t#cv2.imshow(str(label_np[0]),image_np[0]) \u00a0\r\n\t\t\t\t#print label_np[0] \u00a0\r\n\t\t\t\t#cv2.waitKey() \u00a0\r\n\t\t\t\t#print label_np\r\n\t\t\t\tif iter % 50 == 0:\r\n\t\t\t\t\tprint 'trainloss:', loss_np\r\n\t\t\t\tif iter % 1000== 0:\r\n\t\t\t\t\tsaver.save(session, os.path.join('model','model.ckpt'))\r\n```"]}, {"number": 7943, "title": "The TensorFlow library wasn't compiled to use ... instructions", "body": "Being flooded with\r\n\"The TensorFlow library wasn't compiled to use AVX(etc) instructions, but these are available on your machine and could speed up CPU computations.\"\r\n\r\nIf they are available on our machine,  is it possible to download a pip archive with these features enabled? If not, it is a strong Feature request.\r\n", "comments": ["Please see #7257. This may be closed as a duplicate."]}, {"number": 7942, "title": "Improved a virtualenv option", "body": "Replaced the \"--system-site-packages\" option of virualenv with \"--no-site-packages\" option, so that the packages required by tensorflow are installed separately with the system packages.\r\n Otherwise, there could be multiple versions of an installed package (e.g. numpy-1.8.2 installed by apt and numpy-1.12.0 installed by pip), and an error may occur when executing \"import tensorflow\".", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@xioryu The change looks good to me. Thank you for your sending the pull request. However, it looks like that you haven't signed the CLA yet. Could you please do that? Thanks.", "I signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "PR merged. Thanks, @xioryu !"]}, {"number": 7941, "title": "XLA standalone", "body": "I don't know all the internals but what do you think to standalone XLA? I think that targetting HLO IR could be of general use also for other opensource projects. Expecially if the streamexecutor CUDA and OpenCL component it is mainlined in LLVM parallel-lib repository.", "comments": ["It's certainly possible to target TensorFlow's XLA HLO IR from other projects, though our team doesn't have the bandwidth to support efforts like this in depth right now.\r\n\r\nI'm closing this issue for now since it isn't a bug or feature request that we'll be addressing in the near term. However, if you **are** planning a major initiative to target XLA from another project, please do let us know sooner rather than later.  XLA is under extremely active development, so staying in touch with us along the way will help you avoid a certain amount of frustration.\r\n\r\nThanks very much!"]}, {"number": 7940, "title": "wrong commands for installing protobuf at tutorial", "body": "The commands for installing protobuf at tutorial is wrong:\r\n\r\n![bug](https://cloud.githubusercontent.com/assets/1631405/23393228/69fcc090-fdbc-11e6-96e3-413608bfee4a.png)\r\n\r\nThey are used for installing tensorflow via pip, but obviously what should be given are ways to install google protobuf via pip.\r\n\r\nPlease fix it.\r\n\r\nHere's the url: https://www.tensorflow.org/install/install_linux#installing_with_native_pip", "comments": ["@wzpan Thanks for filing this issue!\r\n\r\nIt looks like the \"native pip\" section on both linux and mac suffers from this documentation issue.  Also note that the linux instructions incorrectly reference \"your Mac\".\r\nhttps://www.tensorflow.org/install/install_linux#install_tensorflow\r\nhttps://www.tensorflow.org/install/install_mac#install_tensorflow\r\n\r\nAssigning to @martinwicke , feel free to re-assign as appropriate.  Thanks!", "I spoke to a few people, and it looks like this is what we tried to mean there:\r\n\r\nIf you already have a version of protobuf installed, you should run the uninstall command. After that when you run `pip install tensorflow`, this command will install the correct version of protobuf, as it will install all TensorFlow dependencies.\r\n\r\nWe will update the website to reflect the above.", "Looks like this has been addressed, closing. Please reopen if I'm mistaken."]}, {"number": 7939, "title": "tf.losses.mean_squared_error does not support name parameter", "body": "Some times MSE is not the only loss in the model, and there maybe multiple MSE losses and adds up to the total loss, a custom name of mse loss layer without using name_scope can be helpful.", "comments": ["@Hong-Xiang I believe the reason a `name_scope` is used, rather than a single `name` parameter, is because mean_squared_error expands to multiple underlying operations.  Is there a reason why name_scope isn't sufficient for your use case?", "@tatatodd Thank you for your explanation, `name_scope` is sufficient, the only problem is that it may lead a little mess to code than using `name` directly :-).", "@Hong-Xiang OK cool.  And note that although it's a little more code to write, using `name_scope` has the additional benefit that your ops will be nicely grouped in visualization tools like TensorBoard.\r\n\r\nI'm closing this out; feel free to file new issues if you run into them!  :)"]}, {"number": 7938, "title": "Add Python Graph Transform examples", "body": "The docs for the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/blob/d699a66e940b26e991b29b27f4e3ad2e8e3282d2/tensorflow/tools/graph_transforms/README.md#writing-your-own-transforms) talk about writing your own transforms but way to do so is in C++. It would be cool if there was an example of / c++ hook writing transforms in Python as well. For example, here is one Python transform (by @mrry): http://stackoverflow.com/a/40852855/2638485 (https://github.com/tensorflow/tensorflow/issues/5918).", "comments": ["There also seem to be an entire set of python graph transforms: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/tools", "@petewarden might have thoughts on this.", "The rewrite scripts in python/tools are deprecated in favor of the Graph Transform Tool (and I need to update them to reflect that). In general, we're trying to make as much functionality as possible available across multiple languages, which means implementing core operations in C++ and exposing them through APIs. We're still happy to see user transforms written in Python, we've just chosen to take a different approach for these. Unfortunately that means we don't have 'canonical' examples of writing them in Python, as you've seen.", "@cancan101 we'd welcome community contributions in this regard!", "The docs for [quantization](https://github.com/tensorflow/tensorflow/blob/dc6353d12fd41ec869283679dd7766ebff75c520/tensorflow/docs_src/performance/quantization.md) should also probably be updated to point to the new graph transform tool.", "I am interested in working on this. How do I start?", "@petewarden is there still work to do here?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7937, "title": "Adding Intel Conv2D kernel implementation alongwith required Graph pa\u2026", "body": "\u2026sses\r\n\r\nThis commit contains 4 main components:\r\n\r\n1) Intel-optimized kernel implementation for Conv2D op\r\n   Implementation in kernels/mkl_conv_ops.*\r\n\r\n2) Graph passes required to enable Conv2D optimized implementation\r\n   Implementation in graph/mkl_*. We also need a new op, MklToTf op.\r\n   Its implementation is in kernels/mkl_tfconv_op.cc.\r\n\r\n3) Utility functions used in kernel implementation\r\n   Implementation is in common_runtime/mkl_layer_registry* and util/mkl_util.h\r\n\r\n4) BUILD changes for Conv2D, graph passes and utility functions", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Hi,\r\n\r\nThis contribution is under Intel CLA Submissions. I've setup my email on github account, not sure what is wrong here. ", "Hi,\r\n\r\nI've signed new CLA.", "@tensorflow-jenkins test this please", "Failing buildifier. Please run buildifier and then we'll rerun tests.\r\nhttps://github.com/bazelbuild/buildifier", "Fixed the buildifier error but not sure what we need to do for author consent.", "@tensorflow-jenkins test this please", "I asked around and the \"basic_session_run_hooks_test\" failure is occuring on other builds so we are ok to merge here."]}]