[{"number": 16655, "title": "[XLA] Fix subcomputation unification not adjusting conditionals", "body": "When the subcomputation unification finishes, it calls into the module to adjust any instructions which have had their computation parameters invalidated.\r\n\r\nthe conditional instruction was missing from this function.\r\n", "comments": ["Note that the good reviewers for this change are out for the next couple days, so expect a slow response.", "no problem.  i'm not in a hurry.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "ah yes - i noticed that it had been added from elsewhere.  I will abandon the change all-together.  simpler."]}, {"number": 16654, "title": "Bazel version comparison fails with bazel 0.10.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just commented 6 lines in the bzl files out\r\n- **OS Platform and Distribution**: 16.04 on Jetson TX2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: TX2 GPU, 5GB (not sure)\r\n- **Exact command to reproduce**: bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI cannot build tensorflow using bazel 0.10.0. It seems like the version checks in repositories.bzl and wokspace.bzl fail. Commenting them out solves the issue, even though I know that is no persistent solution. I think it is simply that bazel thinks that 0.10.0 is smaller 0.5.4 due to its string comparison, but I am no bazel expert.\r\n\r\n### Source code / logs\r\nERROR: /home/nvidia/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):\r\n\tFile \"/home/nvidia/git/tensorflow/WORKSPACE\", line 15\r\n\t\tclosure_repositories()\r\n\tFile \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories\r\n\t\t_check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n\tFile \"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl\", line 172, in _check_bazel_version\r\n\t\tfail((\"%s requires Bazel >=%s but was...)))\r\nClosure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: /home/nvidia/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):\r\n\tFile \"/home/nvidia/git/tensorflow/WORKSPACE\", line 41\r\n\t\ttf_workspace()\r\n\tFile \"/home/nvidia/git/tensorflow/tensorflow/workspace.bzl\", line 48, in tf_workspace\r\n\t\tcheck_version(\"0.5.4\")\r\n\tFile \"/home/nvidia/git/tensorflow/tensorflow/workspace.bzl\", line 38, in check_version\r\n\t\tfail(\"\\nCurrent Bazel version is {}, ...))\r\n\r\nCurrent Bazel version is 0.10.0- (@non-git), expected at least 0.5.4\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'external': Package 'external' contains errors\r\nINFO: Elapsed time: 3.145s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n", "comments": ["Can you build TensorFlow from HEAD?\r\nOr cherry-pick this commit to fix it: https://github.com/tensorflow/tensorflow/commit/3f57956725b553d196974c9ad31badeb3eabf8bb", "Thanks for the speedy reply! :) I will try it on the other Jetson and will post it when I can confirm anything :)", "Building HEAD works :+1: ", "In r1.5 `check_version` from `tensorflow/workspace.bzl` still fails.", "This is a hilarious bug...so dumb hahaha", "@gunan Is there any chance to cherry-pick 3f57956 into r1.5?", "@meteorcloudy `git cherry-pick 3f57956725b553d196974c9ad31badeb3eabf8bb`\r\n\r\nI also had to add `git cherry-pick 6fcfab770c2672e2250e0f5686b9545d99eb7b2b`, otherwise the check will still fail on r1.5.", "Hope someone can help me out here \u2013 i'm bound to TF 1.4 since only that version works (for now) on macOS (I know macOS isn't officially supported anymore, but it works fine if you follow certain instructions).\r\n\r\nI manually fixed the WORKSPACE file with the mentioned cherry-pick, but i still get this error:\r\n```Current Bazel version is 0.10.0-homebrew, expected at least 0.5.4```\r\n\r\n**Do i also have to change any other files?** Would love to switch to TF 1.5, but compiling still fails. Installed TF 1.4 a month ago (but just for python2, now i need it for python3 too) and everything works fine there.", "I definitely dont know much about bazel but found an easy fix: \r\n\r\nopen the.bzl files generating the error, repositories.bzl and workspace.bzl\r\nchange all the `check_version(\"...\")` to `check_version(\"0.10.0\")` then run\r\n\r\n`bazel build --config=opt --incompatible_load_argument_is_label=false --incompatible_disallow_uncalled_set_constructor=false //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nworked for me, hope it helps\r\n\r\nnote: I'm running cuda 9.1", "You may want to return tuple of integers from _parse_bazel_version(), instead of tuple of strings.\r\n", "@jendrikjoe what does that mean \"build TensorFlow from HEAD\" ?what's the specific procedure?", "@YURIJTJ instead of git checkout r1.5 you do git checkout HEAD and tehn follow the rest of the build instructions :)", "I think it is version representation bug of bazel. (0.10.0-) It should be removed hypen.\r\n```\r\nnvidia@tegra-ubuntu:~$ bazel version\r\n..............................................\r\nBuild label: 0.10.0- (@non-git)\r\nBuild target: bazel-out/arm-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Apr 26 21:36:52 +50060 (1517583937012)\r\nBuild timestamp: 1517583937012\r\nBuild timestamp as int: 1517583937012\r\n```\r\nI changed the version checking code of Tensorflow then compiled it successfully.\r\n\r\nchange this file,\r\n`\"/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories`\r\n\r\n```\r\n_check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n```\r\nto\r\n```\r\n_check_bazel_version(\"Closure Rules\", \"0.10.0-\")\r\n```", "I just skip the 'git checkout' skip completely and that works.\r\nThx~", "Same issue with Bazel 0.11.0\r\n``Current Bazel version is 0.11.0, expected at least 0.5.4``\r\n", "@fgallaire \r\nI am building v1.5 against CUDA8.0, besides changing repositories.bzl file mentioned by @lifefeel    tensorflow/workspace.bzl, I also change tensorflow/workspace.bzl similarly to \"0.11.0-\". It's compiling.", "IntelliJ throws following error:\r\n\r\n```\r\nClosure Rules requires Bazel >=0.4.5 but was 0.11.0-homebrew.\r\n```", "Please keep conversation civil in this issue, per our code of conduct. I've edited out the swearing: help us keep TensorFlow a welcoming environment.\r\n", "So there is still no fix? I am unable to build any tensorflow version.", "@Tvde1 I manged to build TF 1.5 against CUDA8.0 and cuDNN 6.0. ", "@Tvde1 building HEAD should work, at least it did for me.\r\nOtherwise \u2018git cherry-pick 6fcfab770c2672e2250e0f5686b9545d99eb7b2b\u2018 should solve the issue \ud83d\ude0a", "Master gets me the same result and it warns me that the cherry pick commit is empty. \r\nStill after running `configure`, bazel can't load the `external` package.", "after all the updates, could you make sure to run `bazel clean --expunge`\r\nThere may be some leftover files that are breaking your builds.\r\n\r\nWe are working on cherrypicking the fixes to 1.5 and some older releases.", "I cleaned it with the `--expunge` flag and it's building right now!", "For those of us that are stuck using TensorFlow 1.4, the fix is simple. There are two files that need to be edited, both have lines that need to be changed from:\r\n\r\n`version_tuple += (str(number),)` \r\n\r\nto:\r\n\r\n`version_tuple += (int(number),)`\r\n\r\nThe change is needed here:\r\n\r\n- line 184 of /private/.../closure/repositories.bzl\r\n- line 39 of /Users/.../tensorflow/workspace.bzl\r\n\r\nAs others have said, the string comparison used in these lines fails with double digits. An integer comparison here would not fail. For example, `('0', '4', '5') > ('0', '11', '1')` will return `False`. However, `(0, 4, 5) > (0, 11, 1)` will return `True`. \r\n\r\nWorks for me installing TensorFlow v1.4 on my MacBook Pro running macOS High Sierra v10.13.3 when following these instructions: http://paolino.me/tutorial/tensorflow/machine-learning/deep-learning/gpu/2017/11/18/installing-tensorflow-1.4.0-macos-cuda/", "Hi \r\nI changed the `str` to `int` as suggested above. However, it seems to be encountering another problem. \r\n**Tensorflow branch**: `r1.4`\r\n**Bazel Version**: `0.12.0`\r\n\r\nFollowing is the error I am facing: \r\n```\r\nERROR: /home/hadoop/.cache/bazel/_bazel_hadoop/c347cef6a9a3bcd6c02d65a5feb3f818/external/jpeg/BUILD:122:12: Illegal ambiguous match on configurable attribute \"deps\" in @jpeg//:jpeg:\r\n@jpeg//:k8\r\n@jpeg//:armeabi-v7a\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted: \r\n\r\n/home/hadoop/.cache/bazel/_bazel_hadoop/c347cef6a9a3bcd6c02d65a5feb3f818/external/jpeg/BUILD:122:12: Illegal ambiguous match on configurable attribute \"deps\" in @jpeg//:jpeg:\r\n@jpeg//:k8\r\n@jpeg//:armeabi-v7a\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nINFO: Elapsed time: 0.209s\r\nFAILED: Build did NOT complete successfully (7 packages loaded)\r\n    currently loading: @protobuf_archive// ... (2 packages)\r\n```", "I think we did not fix this in older branches. Is there a reason for using 1.4, instead of master or 1.7 or 1.8?", "@gunan: Yes. On my cluster there is no cuda-9 which makes using 1.5+ impossible... also related: \r\nhttps://github.com/tensorflow/tensorflow/issues/17801", "@naji-s you can probably ask the administrators to install a more modern version of cuda. In my experience, they are usually accommodating.", "@Dapid, I might be able to actually... thanks for the suggestion..."]}, {"number": 16653, "title": "Fix docs", "body": "* Fix xcode path error", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16652, "title": "v1.3 batch_norm layer", "body": "I use the batch norm layer like this:\r\n`def batch_norm_layer(x,train_phase,scope_bn):\r\n\r\n\tbn_train = batch_norm(x, decay=0.999, center=True, scale=True,\r\n\tis_training=True,\r\n\treuse=None, # is this right?\r\n\ttrainable=True,\r\n\tscope=scope_bn)\r\n\tbn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\r\n\tis_training=False,\r\n\treuse=True, # is this right?\r\n\ttrainable=True,\r\n\tscope=scope_bn)\r\n\tz = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\r\n\treturn z`\r\nI don't know in v1.3.0 is the code worked?\r\nI saw the [issue1122](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426), someone said it would not work well.\r\n\r\nthank you in advance.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16651, "title": "Temporarily remove three linter checks for now.", "body": "  # C0330 bad-continuation\r\n  # C0301 line-too-long\r\n  # C0326 bad-whitespace\r\nWill fix the following 25 error and add them back:\r\ntensorflow/contrib/session_bundle/bundle_shim.py:85: [C0301(line-too-long), ] Line too long (83/80)\r\n\r\ntensorflow/contrib/session_bundle/bundle_shim.py:94: [C0301(line-too-long), ] Line too long (89/80)\r\n\r\ntensorflow/contrib/session_bundle/bundle_shim.py:135: [C0301(line-too-long), ] Line too long (81/80)\r\n\r\ntensorflow/contrib/session_bundle/bundle_shim.py:136: [C0301(line-too-long), ] Line too long (81/80)\r\n\r\ntensorflow/contrib/kafka/python/ops/kafka_dataset_ops.py:33: [C0301(line-too-long), ] Line too long (85/80)\r\n\r\ntensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:29: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).\r\n\r\ntensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:31: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).\r\n\r\ntensorflow/contrib/tpu/profiler/pip_package/cloud_tpu_profiler/main.py:35: [C0330(bad-continuation), ] Wrong continued indentation (remove 3 spaces).\r\n\r\ntensorflow/contrib/learn/python/learn/datasets/synthetic_test.py:139: [E0102(function-redefined), SyntheticTest.test_spirals] method already defined line 92\r\n\r\ntensorflow/contrib/layers/python/layers/layers.py:63: [C0330(bad-continuation), ] Wrong hanging indentation (remove 7 spaces).\r\n\r\ntensorflow/contrib/layers/python/layers/layers.py:1421: [C0301(line-too-long), ] Line too long (104/80)\r\n\r\ntensorflow/contrib/py2tf/impl/api.py:89: [C0301(line-too-long), ] Line too long (81/80)\r\n\r\ntensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_test.py:160: [C0326(bad-whitespace), ] Exactly one space required after comma\r\n\r\ntensorflow/contrib/ndlstm/python/lstm1d.py:91: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1639: [E0102(function-redefined), WeightNormLSTMCellTest] class already defined line 1548\r\n\r\ntensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [C0301(line-too-long), ] Line too long (98/80)\r\n\r\ntensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:209: [E1124(redundant-keyword-arg), get_graph_def_from_url_tarball] Argument 'filename' passed by position and keyword in function call\r\n\r\ntensorflow/contrib/layers/python/layers/layers_test.py:1311: [C0301(line-too-long), ] Line too long (89/80)\r\n\r\ntensorflow/python/kernel_tests/tensordot_op_test.py:108: [C0326(bad-whitespace), ] Exactly one space required after comma\r\n\r\ntensorflow/python/data/util/nest.py:482: [C0301(line-too-long), ] Line too long (81/80)\r\n\r\ntensorflow/python/data/ops/dataset_ops.py:909: [C0301(line-too-long), ] Line too long (88/80)\r\n\r\ntensorflow/python/ops/image_ops_impl.py:1694: [C0301(line-too-long), ] Line too long (87/80)\r\n\r\ntensorflow/python/ops/image_ops_impl.py:1720: [C0301(line-too-long), ] Line too long (87/80)\r\n\r\ntensorflow/python/ops/image_ops_impl.py:1745: [C0301(line-too-long), ] Line too long (87/80)\r\n\r\ntensorflow/python/ops/image_ops_impl.py:1771: [C0301(line-too-long), ] Line too long (87/80)", "comments": ["tensorflow/contrib/rnn/python/kernel_tests/rnn_cell_test.py:1639: [E0102(function-redefined), WeightNormLSTMCellTest] class already defined line 1548\r\n\r\ntensorflow/contrib/learn/python/learn/datasets/synthetic_test.py:139: [E0102(function-redefined), SyntheticTest.test_spirals] method already defined line 92\r\n\r\nshould be fixed in #16647"]}, {"number": 16650, "title": "Fix do_cmake_python_sanity error.", "body": "Add missing dir to tensorflow/contrib/cmake/python_modules.txt", "comments": []}, {"number": 16649, "title": "fix python 2.7 build break & import error on windows", "body": "fix python 2.7 build break & import error on windows", "comments": []}, {"number": 16648, "title": "Error while building Tensorflow model from bazel ", "body": "INFO: From Linking tensorflow/libtensorflow_framework.so [for host]:\r\nLINK : warning LNK4044: unrecognized option '/Wl,-soname,libtensorflow_framework.so'; ignored\r\nLINK : warning LNK4044: unrecognized option '/pthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\n   Creating library bazel-out/host/bin/tensorflow/libtensorflow_framework.ifso and object bazel-out/host/bin/tensorflow/libtensorflow_framework.exp\r\nERROR: C:/courses/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/array_ops_gen_cc' failed (Exit 1181)\r\nLINK : warning LNK4044: unrecognized option '/pthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : fatal error LNK1181: cannot open input file 'tensorflow_framework.obj'\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3434.951s, Critical Path: 365.14s\r\nFAILED: Build did NOT complete successfully\r\n\r\nIs LNK4044 warning related to c++ path, i've given VisualStudio.exe path while installing", "comments": ["I'm not sure if this is supported just yet.\r\n\r\n@gunan can we build the android contrib directory from Windows in bazel?", "I am guessing probably not. I am sure the options for cross compilation on windows are different.\r\n@meteorcloudy @andrewharp to confirm", "@aditya478492 Can you provide more information?\r\nWhich target did you build? Can you build with `--verbose_failures` so that we can see what command failed exactly.", "ERROR: C:/courses/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/array_ops_gen_cc' failed (Exit 1181): link.exe failed: error executing command\r\n  cd C:/users/aditya/appdata/local/temp/_bazel_aditya/nmddefa1/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.10.25017\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.10.25017\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.15063.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.15063.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.15063.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.15063.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.10.25017\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.10.25017\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.15063.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.15063.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.10.25017\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\;C:\\Program Files (x86)\\Microsoft SDKs\\F#\\4.1\\Framework\\v4.0\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.15063.0\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET TEMP=C:\\Users\\Aditya\\AppData\\Local\\Temp\r\n    SET TMP=C:\\Users\\Aditya\\AppData\\Local\\Temp\r\n    SET USE_LINKER=1\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.10.25017/bin/HostX64/x64/link.exe /nologo /OUT:bazel-out/host/bin/tensorflow/cc/ops/array_ops_gen_cc.exe tensorflow_framework /SUBSYSTEM:CONSOLE -pthread /MACHINE:X64 @bazel-out/host/bin/tensorflow/cc/ops/array_ops_gen_cc.exe-2.params /DEFAULTLIB:msvcrt.lib\r\nLINK : warning LNK4044: unrecognized option '/pthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : fatal error LNK1181: cannot open input file 'tensorflow_framework.obj'\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 398.641s, Critical Path: 150.53s\r\nFAILED: Build did NOT complete successfully\r\n\r\ni was building android:libtensorflow_interface", "Hmm,, I don't think we are able to build android interface on Windows right now. ", "Windows builds of Android TF are not currently supported, but there are workarounds, e.g.: https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600\r\n\r\nOtherwise I'd suggest just using the AAR as shown in the demo's build.gradle file, if possible."]}, {"number": 16647, "title": "Address sanity build issues.", "body": "", "comments": []}, {"number": 16646, "title": "can tf.estimator.Estimator's  parameters be modified by hand? ", "body": "TF's  high level API  is very convenient to defined a new model. \r\nHowever, many DNN Machine Learning task has to reuse some old model's parameter to fill a new model and then  fine-tune it in new tasks. \r\nI have read the tf.estimator.Estimator'API  carefully, but cann't find any API to set It's parameters. Hope  TF developer  add this function to the high level API. \r\nThank very much!", "comments": ["Please describe into more details what you would like to do.", "@drpngx  thanks for you attention!\r\nI have use the tf.estimator.Estimator defined a group of models, and trained a subset of them. \r\nAfter that, I want to use those trained model's Parameter to init the last un-trained model's parameter, and then start to train this last model. \r\nI find way's to get the older trained model's paramter by  tf.estimator.Estimator.get_variable_value(name),  but  don't have a set var 's value's API to  init the last un-trainded model's Parameter  by the get_variable_value()'s Tensors.\r\nCan you give me some advice about the  set var's value's API ? \r\nThanks again!", "You can use `tf.trainable_variables()` to get all the trainable variables in your graph, assuming that what you want. Then you can use `session.run` to retrieve their value if you want get the actual values.\r\nYou probably want to use `tf.variable_scope` to share variables.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "You should use init_from_checkpoint to set directly from the checkpoint. But as @drpngx said, the details are best resolved on StackOverflow.", "@drpngx  When I used the lower level API for training,  tf.trainable_variables() is work.  But High level API's estimator use system defined Graph info and  tf.trainable_variables() doesn't work! \r\nFor example, I have use the tf.estimator.Estimator defined a group of models,  and just test print the  tf.trainable_variables(), It just return empty list. The code is below: \r\n\r\n    clf_ = tf.estimator.Estimator(model_fn=model_fn_wide2deep, params=param, model_dir=ckpt_dir)\r\n    # try init parameters from other model.\r\n    init_ops = [tf.global_variables_initializer(), tf.local_variables_initializer()]\r\n    with tf.Session() as sess:\r\n        sess.run(init_ops)\r\n        print('traininig params: %r' % sess.run(tf.trainable_variables()))\r\n        sys.exit(1)\r\n\r\nThrough  my clf_  model's parameter can't be return, I think Your ways  to change   tf.estimator.Estimator model's parameter doesn't work.  That why I supporse to add a Estimator.set_variable_value(name)  API. \r\nHope you response! Thanks! ", "@martinwicke  Thanks for you advice. My problem is there are many diffent ckpt files to load for initial the last model's parameters, and what's more,   tf.estimator.Estimator trainable variables can get by hand recently! So, I think you advice doesn't work!  Hope you response! Thanks!", "as I did not found full answer in other places (aka [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)) and this issue appears first in search results \r\n\r\nIf you want to update values on Estimator (high level API), you can use this code to do it\r\n\r\n```python\r\n# Restore, Update, Save\r\n# tested only on tesorflow 1.4\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\n\r\nCHECKPOINT_DIR = 'CHECKPOIN_DIR' # for example '/my_checkpoints' as in tf.estimator.LinearClassifier(model_dir='/my_checkpoints'...\r\ncheckpoint = tf.train.get_checkpoint_state(CHECKPOINT_DIR)\r\n\r\nwith tf.Session() as sess:\r\n    saver = tf.train.import_meta_graph(checkpoint.model_checkpoint_path + '.meta')\r\n    saver.restore(sess, checkpoint.model_checkpoint_path)\r\n\r\n    # just to check all variables values\r\n    # sess.run(tf.all_variables())\r\n\r\n    # get your variable\r\n    KEY = 'linear/linear_model/0/weights/part_0:0'# for tf.estimator.LinearClassifier first weight\r\n    var_wights_0 = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name == KEY][0]\r\n    sess.run(var_wights_0)\r\n\r\n    # your update operation\r\n    var_wights_0_updated = var_wights_0.assign(var_wights_0 - 100)\r\n    sess.run(var_wights_0_updated)\r\n\r\n    # you can check that value is updated\r\n    # sess.run(tf.all_variables())\r\n\r\n    # this saves updated values to last checkpoint saved by estimator\r\n    saver.save(sess, checkpoint.model_checkpoint_path)\r\n```", "@kopas that looks promising. Wondering if you could take a look at this.\r\n\r\nI have a model from `tf.estimator.BestExporter` and what I would like to do is read in the graph, update the weights, evaluate with different weights. If I can evaluate with the new weights saving isn't strictly required but would also be nice.\r\n\r\nCurrently I got some stuff like this\r\n\r\n```python\r\n\r\ndef load_estimator_graph(export_dir:str)->None:\r\n    '''Solves import issues when using tf.estimator.(Best)Exporter for saving\r\n    models rather than using the last checkpoint.\r\n\r\n    Arguments:\r\n        export_dir (str): the full path to exported tf.estimator model\r\n    Returns:\r\n        None\r\n    '''\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        meta_graph   = tf.saved_model.loader.load(sess, ['serve'], export_dir)\r\n    with tf.Session() as sess:\r\n        loaded_graph = tf.train.import_meta_graph(meta_graph)\r\n\r\ndef lazy_fetch_variable_values(variable_names:list)->dict:\r\n    '''\r\n    Notes:\r\n        \"lazy\" refers to:\r\n            1. the use of `tf.initialize_all_variables()` to ensure\r\n                variables have values\r\n            2. the use of `tf.trainable_variables()` to search the likely\r\n                releveant values\r\n\r\n    Arguments:\r\n        variable_names (list): list of variable names (str) to retrieve from the\r\n            default tensorflow graph\r\n\r\n    Returns:\r\n        variables (dict): key:value of the variables and the values as pythonic\r\n            data types.\r\n    '''\r\n    init_op = tf.initialize_all_variables()\r\n    variables = {}\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n\r\n        tvars = tf.trainable_variables()\r\n        tvars_vals = sess.run(tvars)\r\n\r\n        for var, val in zip(tvars, tvars_vals):\r\n            if var.name in variable_names:\r\n                variables[var.name] = val\r\n    return variables\r\n\r\n\r\ndef lazy_set_variable_values(variables_to_set:dict):\r\n    '''\r\n    Arguments:\r\n        variables_to_set (dict): variable_name, variable_value pairs for which\r\n            to be updated in the graph\r\n    '''\r\n    init_op = tf.initialize_all_variables()\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        tf_global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\n\r\n        for var_to_find, val_to_set in variables_to_set.items():\r\n            var = [v for v in tf_global_vars if v.name == var_to_find][0]\r\n            sess.run(var)\r\n\r\n            var = var.assign(val_to_set)\r\n            sess.run(var)\r\n```\r\n\r\nand then call something like this:\r\n\r\n```python\r\nload_estimator_graph(best_exported_model_dir)\r\nlayer_name = 'some_layer/kernel:0'\r\nweights = lazy_fetch_variable_values([layer_name])[layer_name]\r\nnew_weights = np.copy(weights)\r\nnew_weights = 0\r\nlazy_set_varriable_values({'some_layer/kernel:0':new_weights})\r\n\r\n\r\nwith tf.Session() as session:\r\n    sess.run(tf.initialize_all_variables())\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, os.path.join(best_exported_model_dir, '..', 'best_updated'))\r\n```\r\n\r\nthen \r\n\r\n```python\r\npredict_fn = predictor.from_saved_model(best_exported_model_dir)\r\n```\r\n\r\nworks but\r\n\r\n```python\r\nest = tf.estimator.Estimator(\r\n    model_fn=model_fn,\r\n    model_dir=os.path.join(best_exported_model_dir, '..', 'best_updated'),\r\n    config=run_config, # same as when ran\r\n    params = params, # same as when ran\r\n)\r\n\r\neval_fn = lambda : input_fn(mode='eval')\r\n\r\nest.evaluate(eval_fn)\r\n```\r\n\r\nyield the error\r\n\r\n```\r\nValueError Input 0 of layer some_layer is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [an_integer]\r\n```\r\n\r\nwhere in the above code\r\n\r\n```python\r\nweights.shape[0] == new_weights.shape[0] == an_integer\r\n```\r\n\r\nReally bewildered and would appreciate help", "@SumNeuron I did not verified your code, but error seems in this place\r\n`\r\n...\r\nnew_weights = 0\r\n...\r\n`\r\nYou assign single number, but your model expects vector (0,0)", "@kopas \r\n\r\nsorry for not clarifying with inline comments.\r\n\r\n`new_weights` was a copy of the old weights (which `tf` interprets as a `np.ndarray`). So rather than copy-pasting a matrix or anything complex, `new_weights = 0`, just sets _element-wise_ the `np.ndarray` (tensor) to 0. \r\n\r\nPerhaps better pseudo code would have been:\r\n\r\n```\r\nweights = lazy_fetch_variable_values(['some_layer/kernel:0'])\r\nnew_weights = modify_weights(weights) # returns a tensor with same shape as weights, but may have different values per element\r\n```", "@kopas for sanity I tried changing how I update to the following:\r\n\r\n```python\r\nexport_dir = # directory from tf.estimator.(Best)Exporter\r\nout_dir = # dir to save to\r\n\r\ntf.reset_default_graph()\r\nwith tf.Session() as sess:\r\n    meta_graph = tf.saved_model.loader.load(sess, ['serve'], export_dir)\r\n    saver = tf.train.import_meta_graph(meta_graph)\r\n#     saver.restore(sess, os.path.join(export_dir, 'saved_model.pb'))\r\n\r\n    # just to check all variables values\r\n#     sess.run(tf.all_variables())\r\n    layer_weight_names = [\r\n       'some_layer/kernel:0'\r\n    ]\r\n    weight_matrices_ref = [\r\n        [v for v in tf.trainable_variables() if v.name == layer_weight_name][0]\r\n        for layer_weight_name in layer_weight_names\r\n    ]\r\n    weight_matrices = [sess.run(wm) for wm in weight_matrices_ref]\r\n    updated_weights  = update_weights(weight_matrices)\r\n        \r\n    # your update operation\r\n    updated_weight_matrices = [weight_matrices_ref[i].assign(updated_weights[i]) for i in range(len(updated_weights))]\r\n    for wm in updated_weight_matrices:\r\n        sess.run(wm)\r\n\r\n#     # you can check that value is updated\r\n#     # sess.run(tf.all_variables())\r\n\r\n    # this saves updated values to last checkpoint saved by estimator\r\n    saver.save(sess, out_dir)\r\n\r\n```\r\n\r\nand this gives the same error.\r\n\r\nNotes:\r\n-  the `update_weights` returns a `np.ndarray` of the same shape as passed matrix. \r\n- this is reading in a model from `BestExporter` (which is different from a checkpoint) and then I would like to save it to the same format if possible, but that seems to be asking too much. So if I can export it as a checkpoint, that also works, as long as I can restore an estimator to use for eval.\r\n", "@SumNeuron \r\nMaybe this issues is with this line:\r\n`#     saver.restore(sess, os.path.join(export_dir, 'saved_model.pb'))`\r\nYou do not restore state from saved checkpoint, maybe this somehow interferes... my experience with high level api is limited. \r\nYou need also check if '(Best)Exporter' supports same checkpoint system as 'LinearClassifier'.\r\n\r\nStill,\r\ncan you provide some debug info?\r\n```\r\n# get your variable\r\nKEY = 'linear/linear_model/0/weights/part_0:0'# for tf.estimator.LinearClassifier first weight\r\nvar_wights_0 = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name == KEY][0]\r\n# this should print result\r\nsess.run(var_wights_0)\r\n```\r\n\r\nPrint for your key 'some_layer/kernel:0' or for key that gives error. Print also variable that you are setting.\r\nCan you do it?\r\n\r\nThe error says clearly, that on some update you try to set number, but vector expected. Maybe it's internal high level api error, then you can try opening new issue about it and I will not be able to help you.", "@kopas  I really appreciate you taking a look. Since estimators are on the way out (although technically around in 2.0) many are not so willing to help those stuck working with them.\r\n\r\n\r\n# Digging in the Source Code\r\nSome context about `BestExporter`. It just calls the estimator's `export_savedmodel`, which can be seen starting at [line 808](https://github.com/tensorflow/estimator/blob/33e13754b0deebe7cc5006c49c53d246b6ff1dec/tensorflow_estimator/python/estimator/estimator.py#L808) in the source code. It seems to use `saved_model_builder` and another function `_add_meta_graph_for_mode(...)` defined on [line 879](https://github.com/tensorflow/estimator/blob/33e13754b0deebe7cc5006c49c53d246b6ff1dec/tensorflow_estimator/python/estimator/estimator.py#L879). \r\n\r\nLooking at this  `_export_all_saved_models(...)` lines 815 to 826, we see that it looks for a `checkpoint`  to use as the model to export. However it is important to note that the exported model had a different structure than `checkpoints`. Namely, you have a directory like:\r\n\r\n```\r\n<timestamp>/\r\n    - variables/\r\n        - variables.data-00000-of-00001\r\n        - variables.index\r\n    - saved_model.pb\r\n```\r\n\r\nThere is a class under `tf.contrib.predict` for loading this model  [`SavedModelPredictor`](https://github.com/tensorflow/tensorflow/blob/6c7c3bb0f3563e8a147935597617cf3d052c7852/tensorflow/contrib/predictor/saved_model_predictor.py). [Lines 149 to 164](https://github.com/tensorflow/tensorflow/blob/6c7c3bb0f3563e8a147935597617cf3d052c7852/tensorflow/contrib/predictor/saved_model_predictor.py#L149) uses something like `loader.load(self._session, tags.split(','), export_dir)` to get the model back.\r\n\r\n# Regarding error\r\nYou state\r\n> The error says clearly, that on some update you try to set number, but vector expected. Maybe it's internal high level api error, then you can try opening new issue about it and I will not be able to help you.\r\n\r\nThe error is\r\n\r\n```\r\nValueError: Input 0 of layer input_layer is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [784]\r\n```\r\nwhich stems from\r\n\r\n```python\r\nest = tf.estimator.Estimator(\r\n    model_fn  = model_fn,\r\n    model_dir = updated_model_dir,\r\n    config    = tf.estimator.RunConfig(**_config['RunConfig']),\r\n    params    = _config,\r\n    \r\n)\r\n\r\neval_fn = lambda : input_fn({**_config, 'mode':'test'})\r\nest.evaluate(eval_fn)\r\n```\r\n\r\n# Requested Debug Info\r\n\r\n```python\r\nexport_dir = # where my model is saved (not checkpoint, but BestExporter version)\r\nout_dir =      # where I want to save my model\r\n\r\ntf.reset_default_graph()\r\nwith tf.Session() as sess:\r\n    meta_graph = tf.saved_model.loader.load(sess, ['serve'], export_dir)\r\n    saver = tf.train.import_meta_graph(meta_graph)\r\n#     saver.restore(sess, os.path.join(export_dir, 'saved_model.pb'))\r\n#     # just to check all variables values\r\n#     sess.run(tf.all_variables())\r\n    layer_weight_names = [\r\n        'input_layer/kernel:0', \r\n    ]\r\n    weight_matrices_ref = [\r\n        [v for v in tf.trainable_variables() if v.name == layer_weight_name][0]\r\n        for layer_weight_name in layer_weight_names\r\n    ]\r\n    weight_matrices = [sess.run(wm) for wm in weight_matrices_ref]\r\n    print(layer_weight_names[0], 'has shape', weight_matrices[0].shape, weight_matrices[0])\r\n    updated_weights  = update_weights(weight_matrices)\r\n    print(layer_weight_names[0], 'updated', 'has shape', updated_weights[0].shape, updated_weights[0])\r\n    \r\n#     # your update operation\r\n    updated_weight_matrices = [weight_matrices_ref[i].assign(updated_weights[i]) for i in range(len(updated_weights))]\r\n    for wm in updated_weight_matrices:\r\n        sess.run(wm)\r\n\r\n#     # you can check that value is updated\r\n#     # sess.run(tf.all_variables())\r\n\r\n#     # this saves updated values to last checkpoint saved by estimator\r\n    saver.save(sess, out_dir)\r\n```\r\n\r\nprints out\r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from <export_dir>/variables/variables\r\ninput_layer/kernel:0 has shape (784, 1000) [[ 0.02498415 -0.03715022  0.00038124 ... -0.01632504  0.01010246\r\n  -0.0130309 ]\r\n [ 0.01805093  0.0504102   0.03946984 ... -0.01649825  0.0529456\r\n   0.0090284 ]\r\n [-0.05145188 -0.00748302  0.02208934 ... -0.01180375 -0.05152242\r\n   0.02814269]\r\n ...\r\n [ 0.02663752 -0.05750908 -0.05161621 ...  0.04808073 -0.03680649\r\n  -0.02869198]\r\n [ 0.03978344  0.03805095  0.02332595 ...  0.04606824  0.01374551\r\n   0.04582741]\r\n [ 0.0301908   0.04157476 -0.05609504 ...  0.00530757 -0.03418655\r\n   0.02162087]]\r\ninput_layer/kernel:0 updated has shape (784, 1000) [[ 0.02498415  0.          0.00038124 ... -0.01632504  0.01010246\r\n  -0.0130309 ]\r\n [ 0.01805093  0.0504102   0.03946984 ... -0.01649825  0.0529456\r\n   0.0090284 ]\r\n [ 0.         -0.00748302  0.02208934 ... -0.01180375  0.\r\n   0.02814269]\r\n ...\r\n [ 0.02663752  0.          0.         ...  0.04808073  0.\r\n  -0.02869198]\r\n [ 0.03978344  0.03805095  0.02332595 ...  0.04606824  0.01374551\r\n   0.04582741]\r\n [ 0.0301908   0.04157476  0.         ...  0.00530757 -0.03418655\r\n   0.02162087]]\r\n\r\n```\r\n", "@kopas  I have got a status update. Error seemed to be system dependent. uninstall / reinstall TF and now the code above lets me read and write models....\r\n\r\n**however**,  what is being written (or possibly read / initialized) is incorrect!\r\n\r\n\r\n```python\r\n# helper function\r\ndef save_session(export_dir:str, model_name:str='estimator')->None:\r\n    '''\r\n    Saves the session to the desired directory\r\n\r\n    Notes:\r\n        Will produce the following directory and files:\r\n            <export_dir>/\r\n                checkpoint\r\n                <model_name>.data-<ckpt-num>-of-<max-ckpt>\r\n                <model_name>.index\r\n                <model_name>.meta\r\n    Arguments:\r\n        export_dir (str): directory where model will be exported\r\n        model_name (str): name of the model\r\n\r\n    Returns:\r\n        None\r\n    '''\r\n    if not os.path.isdir(export_dir): os.makedirs(export_dir)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n        saver = tf.train.Saver()\r\n        saver.save(sess, os.path.join(export_dir, model_name))\r\n```\r\n\r\nSo now I can read an exported model,  save it as a checkpoint, and reload it with an estimator to use `estimator.evaluate(...)`. However,it seems my save function / pipeline is messed up.\r\n\r\nBelow I read in an exported model (using functions posted above) and then just save to a new directory. I then read in the \"clone\" and the weight matrix has changed! \r\n\r\nI want to emphasize that the first loaded weight matrix (from the exported model) seems to be correct. When using `tf.contrib.predict` I get the same results as from the logs (e.g. equivalent accuracy, etc). \r\nBecause the read function seems to work, and it is only reading the clone which has weights all near zero, I am lead to believe that it is the saving which results in this? \r\n\r\n```python\r\nimport copy, os\r\n\r\nfinal_model_dir = # estimator.export_finalmodel(...), acc > 90%\r\nfinal_model_clone_dir = os.path.join(final_model_dir, '..', 'final_model_clone')\r\n\r\nload_estimator_graph(final_model_dir)\r\nweights = lazy_fetch_variable_values(['input_layer/kernel:0',])\r\nweight_matrices = list(weights.values())\r\n\r\noriginal_weight_matrices = copy.deepcopy(weight_matrices) # <--- keep this for later\r\n\r\n# here I would, overwrite with the same value, but I will do you one better\r\n# I will not alter the value at all!\r\n# lazy_set_variable_values(dict(zip(list(weights.keys()), original_weight_matrices)))\r\n\r\n# did nothing but load graph, extract values, but did NOT assign new values\r\nsave_session(final_model_clone_dir)\r\n\r\n# load the clone and see what changed\r\nload_estimator_graph(final_model_clone_dir)\r\nweights = lazy_fetch_variable_values(['input_layer/kernel:0',])\r\nweight_matrices = list(weights.values())\r\n\r\n(weight_matrices[0] == original_weight_matrices[0]) \r\n# returns [[False, ..., False], ..., [False, ..., Flalse]\r\n```", "@kopas  update\r\n\r\nSo I thought maybe all the use of:\r\n\r\n```python\r\ndef foo(...):\r\n    with tf.Session() as sess:\r\n        # do something\r\n```\r\n\r\nhelper functions might be the cause of the issue, so I managed together all those functions from above to this:\r\n\r\n```python\r\nmodel_dir = # <saved_final_model>\r\nexport_dir = # <new_model>\r\nif not os.path.isdir(export_dir): os.makedirs(export_dir)\r\n\r\nweight_matrices_names_to_update = [\"input_layer/kernel:0\",] # same gist as above\r\n\r\n\r\ntf.reset_default_graph()\r\ninit_op = tf.initialize_all_variables()\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n\r\n    # fetch graph\r\n    meta_graph = tf.saved_model.loader.load(sess, ['serve'], model_dir)\r\n\r\n    # load graph\r\n    loaded_graph = tf.train.import_meta_graph(meta_graph)\r\n\r\n    # sess.run(init_op) # <-- throws error\r\n\r\n    # refs to variables to change\r\n    weight_matrices_ref = [\r\n        [v for v in tf.trainable_variables() if v.name == layer_weight_name][0]\r\n        for layer_weight_name in weight_matrices_names_to_update\r\n    ]\r\n\r\n    # actual values of the variables\r\n    weight_matrices = [sess.run(wm) for wm in weight_matrices_ref]\r\n\r\n    # update weights\r\n    updated_weights = update_weights(weight_matrices, percent, method)\r\n\r\n    # update operations\r\n    updated_weight_matrices = [\r\n        weight_matrices_ref[i].assign(updated_weights[i])\r\n        for i in range(len(updated_weights))\r\n    ]\r\n\r\n    # have session assign new values\r\n    for wm in updated_weight_matrices:\r\n        sess.run(wm)\r\n\r\n\r\n    loaded_graph.save(sess, os.path.join(export_dir, 'estimator'))\r\n\r\n```\r\n\r\nNoteworthy, is that this this time:\r\n\r\n- if `update_weights` returns identical matrices to what went in, I get the correct model exported\r\n- if `update_weights` returns anything else, I seem to get just randomly initialized weight matrices", "@drpngx and @martinwicke  perhaps consider reopening the issue? \r\n", "@SumNeuron, I think your code saves the graph, but not the variables. I do not think your graph needs saving, you are not modifying it. But the checkpoint must be saved -- it is what contains the variable values.\r\n\r\nIn fact, you shouldn't have to load the graph at all -- you only need the checkpoint, and you only need to write back the updated checkpoint.", "@martinwicke Foremost, thanks for taking the time to investigate. I really appreciate it ^*^. Nonetheless, I am a bit confused by that response?\r\n\r\nNot savlng the variables would explain why it appears upon making a new `tf.estimator.Estimator` instance (with `warm_start`), the weights appear to be random / just initialized (except in the case where I assign the weights to their current value).\r\n\r\n\r\n\r\nFrom your own [answer](https://stackoverflow.com/a/43531616/5623899) on StackOverflow (`SavedModel` vs `Graph`) you note that `SavedModel`'s are more comprehensive and include a checkpoint. \r\n\r\nIf one is using a `tf.estimator.BestExporter` in their `tf.estimator.Estimator`, then _which_ checkpoint (to my knowledge) is not immediately obvious from the `BestExporter` unless in the exported directory:\r\n\r\n```\r\n<timestamp>/\r\n    - variables/\r\n        - variables.data-<#####>-of-<#####>\r\n        - variables.index\r\n    - saved_model.pb\r\n```\r\n\r\n`<#####>-of-<#####>` corresponds to the `tf.estimator.Estimator`'s `model_dir`'s files `model.ckpt-<#####>-of-<#####>`\r\n\r\nIn addition, I am preferential to using or updating the `SavedModel` as:\r\n1. to reuse the checkpoint in the future, the run-time parameters must be carried along side the `ckpt` so that a new instance of the `tf.estimator.Estimator` can be made (utilizing, `warm_start_from`)\r\n2.  `tf.contrib.predictor` works well with the `SavedModel` and does not require knowing the run-time parameters.\r\n\r\n\r\nSo I guess then, how would I do what you suggest above? or alternatively, how can I just read in `variables.data-<#####>-of-<#####>` update the variables and then I can just clone the `SavedModel` directory with the new `variables.data-<#####>-of-<#####>` file?\r\n\r\n\r\n\r\n", "@kopas it seems your approach using `get_checkpoint_state` does not allow one to specify which checkpoint to use in the checkpoint directory (see [TensorFlow v1.10+: get_checkpoint_state what is the expected value for latest_filename for Estimator API to specify a particular checkpoint](https://stackoverflow.com/questions/56786829/tensorflow-v1-10-get-checkpoint-state-what-is-the-expected-value-for-latest-fi?noredirect=1#comment100128338_56786829) for more details / relevant source code)\r\n", "@martinwicke just for a \"sanity\" check, I copy-pasted @kopas 's solution from [above](https://github.com/tensorflow/tensorflow/issues/16646#issuecomment-384935839) (updating weights according to my previous [comments](https://github.com/tensorflow/tensorflow/issues/16646#issuecomment-504092705)) and it appears that the same issue persists\r\n\r\nnamely:\r\n- if I set a weight matrix to the same value it originally had, things remain the same (same acc, etc)\r\n- if I set a weight matrix to any other value (e.g. all 0, or just plus 0.0000000000001 to all values), then when inspecting the matrices from these exported, modified checkpoints, the values when reloaded are not the same.\r\n\r\nNow to clarify, when I try to evaluate the re-exported checkpoint (in a new directory) I do so via:\r\n\r\n```python\r\nest = tf.estimator.Estimator(\r\n        model_fn  = model_fn,\r\n        model_dir = updated_dir, # <--- where I saved the model (not same location or same directory as originally used)\r\n        config    = tf.estimator.RunConfig(**config['RunConfig']), # same as original run-time\r\n        params    = config, # same as original run-time\r\n        warm_start_from = updated_dir # <--- should start with updated values?\r\n    )\r\n    eval_fn = lambda : input_fn({**config, 'mode':'eval'})\r\n```\r\n\r\nand it could be that this is incorrect... Interestingly, if I then  call\r\n\r\n```python\r\nresults = est.evaluate(eval_fn)\r\nest.export_savedmodel(os.path.join(updated_dir, 'final_model'), serving_input_receiver_fn)\r\n```\r\n\r\nand then inspect the this new `SavedModel`, it appears to be neither the `SavedModel` from the original estimator, or the values that I have updated in the checkpoint \r\n\r\n\r\n\r\nTo clarify, the sanity check using **kopas**'s method with my edits is as follows:\r\n```python\r\nmodel_dir = # <model_dir> as specified when constructing the Estimator, not the exported SavedModel\r\nexport_dir = # <new_model> \r\nif not os.path.isdir(export_dir): os.makedirs(export_dir)\r\n\r\nweight_matrices_names_to_update = [\"input_layer/kernel:0\",] # same gist as above\r\n\r\n\r\ntf.reset_default_graph()\r\ncheckpoint = tf.train.get_checkpoint_state(model_dir)\r\ninit_op = tf.initialize_all_variables()\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    saver = tf.train.import_meta_graph(checkpoint.model_checkpoint_path + '.meta')\r\n    saver.restore(sess, checkpoint.model_checkpoint_path)\r\n    # sess.run(init_op) # <-- throws error\r\n\r\n    # refs to variables to change\r\n    weight_matrices_ref = [\r\n        [v for v in tf.trainable_variables() if v.name == layer_weight_name][0]\r\n        for layer_weight_name in weight_matrices_names_to_update\r\n    ]\r\n\r\n    # actual values of the variables\r\n    weight_matrices = [sess.run(wm) for wm in weight_matrices_ref]\r\n\r\n    # update weights\r\n    updated_weights = update_weights(weight_matrices, percent, method)\r\n\r\n    # update operations\r\n    updated_weight_matrices = [\r\n        weight_matrices_ref[i].assign(updated_weights[i])\r\n        for i in range(len(updated_weights))\r\n    ]\r\n\r\n    # have session assign new values\r\n    for wm in updated_weight_matrices:\r\n        sess.run(wm)\r\n\r\n\r\n    saver.save(sess, os.path.join(export_dir, 'estimator'))\r\n```", "@martinwicke I really don't know how to precede on debugging this myself. I already went through the docs and source code as well as a series of trials / errors. So I would continue to appreciate your guidance, especially as the previously listed approach by @kopas does not seem to work any more, or at least in this context"]}, {"number": 16645, "title": "What's the difference between Univariate prediction and Multivariate prediction?", "body": "My understanding is that paramenters of neurals are shared in Multivariate prediction and they can learn some correlations between series. There is less training time in Multivariate prediction. I wonder if that's right. Could you please explain any basic principles of Multivariate prediction with LSTM or recommend related papers to me? Thank you.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16644, "title": "clear", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16643, "title": "Resolve pylint issues in image ops", "body": "There were 4 line too long errors reported in pylint check of image_ops_impl.py.  Changed the format to not exceed 80 characters based on indentation examples in [style guide](https://google.github.io/styleguide/pyguide.html#Indentation).  Reran pylint against the file and verified errors no longer listed for image_ops_impl.py.", "comments": ["This PR and [PR 16563](https://github.com/tensorflow/tensorflow/pull/16563) a small subset of changes since addressed in [PR 16647](https://github.com/tensorflow/tensorflow/pull/16647).", "Sorry for missing your PRs.\r\nWould you mind if I merge the larger PR, and close the two you sent?\r\nI can still merge your changes and then merge mine, too.", "That sounds great @gunan and @yifeif - thank you both!"]}, {"number": 16642, "title": "Fix typo in attention_wrapper.py", "body": "This is to fix [#14629](https://github.com/tensorflow/tensorflow/issues/14629).\r\n\r\nAs said in the discussion, the description of the output shape should be \"[batch_size, 1, max_time]\" instead of the current \"[batch_time, 1, max_time]\" (which doesn't make sense because batch_time doesn't occur elsewhere).", "comments": []}, {"number": 16641, "title": "ImportError after compiling", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes. I followed this guide in an attempt to gain GPU support under macOS. (https://tweakmind.com/tensorflow-1-5-macos-10-13-2/). The code changes are made by these commands:\r\n\r\nsed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\nsed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/split_lib_gpu.cu.cc\r\nsed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS 10.13.2 High Sierra\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.5\r\n\r\n- **Python version**: \r\n3.6.4\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0-homebrew\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nxcode 8.3.3\r\n\r\n- **CUDA/cuDNN version**:\r\n9.1 / 7\r\n\r\n- **GPU model and memory**:\r\nNvidia GTX 1080 Ti\r\n\r\n- **Exact command to reproduce**:\r\npython\r\nimport tensorflow as tf\r\n\r\n\r\n### Describe the problem\r\nAfter following the guide at the URL above, building w/bazel and creating the wheel, I installed the tensorflow package with pip3 install --upgrade --force-reinstall <package name> and was successful. However, when I try to import tensorflow, I get an ImportError, Symbol not found: _PyCObject_Type\r\n\r\n### Source code / logs\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyCObject_Type\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyCObject_Type\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["/CC @gunan, any ideas? I'm not sure if we support using a GPU on macs. If we don't, this should be tagged with community support.", "Based on what the Tensorflow website states, as of v1.2 you do not support GPU on Mac, however there have been several people post succesful workarounds. My hope in posting this issue was that it might be something common to other builds and someone could help me get it sorted.", "Stackoverflow may be a better place for reaching out for community support. However, your issue does not seem related to cuda itself. This is the line you should be paying attention to:\r\n```\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyCObject_Type\r\n```\r\n", "GPU on mac is not that compatible\r\nbut external GPU cann be set up\r\nread more from here https://egpu.io/", "This is now supported with latest versions , please check https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html. "]}, {"number": 16640, "title": "Support multiple build types in Android build.gradle with the makefile build", "body": "", "comments": []}, {"number": 16639, "title": "Updated adding_an_op.md to reflect newer API and to fix typos", "body": "Changes made to `adding_an_op.md`:\r\n - Typos:\r\n    - changed `#include \"example.h\"` to `\"kernel_example.h\"` in the `.cc` and `.cu.cc` files (under the **GPU kernels** heading)\r\n    - changed `#endif KERNEL_EXAMPLE_H_` to `#endif // KERNEL_EXAMPLE_H_` in `.h` file\r\n - API updates:\r\n    - changed calls from `tf.sysconfig.get_compile_flags()` to `tf.sysconfig.get_include()`, and `get_link_flags()` to `get_lib()` (under **Compile the op using your system compiler** and **Compiling the kernel for GPU device** headings)\r\n    - changed variables `TF_CFLAGS` and `TF_LFLAGS` to `TF_INC` and `TF_LIB` accordingly\r\n - Bug fixes (to files under **GPU kernels** heading):\r\n    - added missing call to `REGISTER_OP` macro in `.cc` file\r\n    - corrected partial specialization of functor template in `.h` file\r\n    - added `#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"` to `.cc` file\r\n\r\nThe last two bullet points are fixes I had to make to my local copies of the files in order for the compilation process to work; if there is a better way to fix the files, please let me know. To that end, the files as described above will compile, but on my machine, the GPU implementations are inaccessible. If anyone would be so kind, I've posted about it on [Stack Overflow](https://stackoverflow.com/questions/48552139/why-cant-tensorflow-find-the-gpu-kernel-of-my-custom-op). Thanks!", "comments": ["Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 16638, "title": "Feature request: use padded_batch with tf.estimator.export.build_parsing_serving_input_receiver_fn", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: I forget\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.8\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nAs far as I can tell, tf.estimator.export.build_parsing_serving_input_receiver_fn() doesn't allow you to control the way examples are batched. So if I have an Estimator that I've trained by using Dataset.padded_batch() in the input_fn, there doesn't seem to be a way for me to use that model with TensorFlow Serving.\r\n\r\n### Source code / logs\r\nN/A?", "comments": ["Whenever there's pre-processing work to be done like embedding variable-length features, is that usually done by pre-processing the data _before_ sending a predict request to TensorFlow Serving / Cloud ML Engine?\r\n\r\nI filed this issue with the expectation that there should be some way to write a graph whose input is a batch of variable-length tf.strings (for example), but maybe that's not how it's meant to be done?\r\n\r\nAlso, I'm not sure what tensorflower is, but is it coming soon? :)", "@sukritiramesh FYI\r\n\r\nServing will accept requests with any batch size and return results according to what the graph returns. Often, you will feed single examples, but there's no restriction on this.\r\n\r\n`build_parsing_serving_input_receiver_fn` however is a utility for the most common case, and it does not give you control over batching. You'll have to write your own function returning a ServingInputReceiver to do that.", "I accidentally came across more information that I'm including here for posterity:\r\n\r\ntensorflow_model_server accepts a [BatchingParameters](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/session_bundle_config.proto#L67) proto with a `bool pad_variable_length_inputs` parameter that might emulate Dataset.padded_batch() well enough. And if not, there's a `Int64Value max_batch_size` parameter that might disable batching if set to 1.\r\n\r\n[Using SavedModel with Estimators](https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators) has more information about when/how to use build_parsing_serving_input_receiver_fn or write your own ServingInputReceiver."]}, {"number": 16637, "title": "Branch 184052073", "body": "", "comments": []}, {"number": 16636, "title": "Add relnote about bug in ptxas in CUDA 9 and 9.1.", "body": null, "comments": ["cc @Artem-B ", "cc @av8ramit ", "Not sure why github-flavored markdown is not linkifying the commit hash in https://github.com/jlebar/tensorflow/blob/28a1dd32666d9062d331fb8d8a5645be43ecb2f4/RELEASE.md.  I double-checked and it does appear to be a valid hash in this repository..."]}, {"number": 16635, "title": "Update ISSUE_TEMPLATE.md", "body": "Fixes 16350", "comments": ["Thanks for this PR. Note: use you need to say Fixes #16350, for this auto closing to work.  Also, it needs to be done to models as well (I'm doing that now)"]}, {"number": 16634, "title": "Fixes issues in tf.contrib.keras.utils.Progbar", "body": "In version 1.5 of Tensorflow, if Progbar's target is set to None, internally it gets set to -1.\r\nChanged code that referenced self.target is not None to self.target != -1, self.target is None to self.target == -1.\r\nAlso, ProgBar is printing twice as reported here https://github.com/tensorflow/tensorflow/issues/16538. Fixed duplicate text", "comments": ["Please just sync the code to https://github.com/keras-team/keras/blob/master/keras/utils/generic_utils.py#L285", "Never mind, we are syncing the progbar internally. It will show up on GitHub shortly.\r\n\r\nThanks for the PR regardless!"]}, {"number": 16633, "title": "Update docs for tf.matching_files to mention non-deterministic", "body": "This fix tries to close the issue of #15374 by updating the\r\ndocs of `tf.matching_files` (as well as `Dataset.list_files`\r\nand `train.match_filenames_once`), to mention that the file names\r\nreturned could be non-deterministic.\r\n\r\nThis fix fixes #15374.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16632, "title": "Re-add missing argument specifier in build_all_android.sh", "body": "The \":\" was erroneously removed in 76f70f5d62f35b5cc95121e6dfffa63a8214b626", "comments": []}, {"number": 16631, "title": "Allow variable_overwrites on scope level", "body": "This is a request for allowing to pass a dict of `variable_overwrites` to variable scopes which to be returned when `tf.get_variable` is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:\r\n```\r\n    import numpy as np\r\n    with tf.variable_scope(\"one\"):\r\n        a = tf.ones((5, 5), tf.float32)\r\n        with tf.variable_scope(\"two\"):\r\n            x1 = tf.get_variable(\"x\", initializer=np.random.randn(5, 5).astype(\"float32\"))\r\n            c1 = tf.sqrt(tf.abs(x1 + a))\r\n\r\n    variables_overwrites = {x1._shared_name: c1}\r\n    with tf.variable_scope(\"one\", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):\r\n        a = tf.ones((5, 5), tf.float32)\r\n        with tf.variable_scope(\"two\"):\r\n           // x2 here is in fact the value of c1\r\n            x2 = tf.get_variable(\"x\", initializer=np.random.randn(5, 5).astype(\"float32\"))\r\n            c2 = tf.sqrt(tf.abs(x2 + a))\r\n```\r\nThis is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly.", "comments": ["/CC @martinwicke, thoughts?", "I believe you can implement this with a custom_getter. I would not want to add additional complexity to the already bloated variable_scope signature.", "With custom getter you can implement it only if you replace all variables in that scope, there is no way to use a custom getter and also allow for get variables. Is it at least possible if the custom getter returns None to fall back to the other code?", "If you implement your custom_getter, you can call the original getter in order to fall back on the existing method to make a variable. So you check your dict, return a special thing if your variable is in there, otherwise return whatever the getter you get in the first arg returns.", "Thanks that indeed solved the issue. I was not aware of the `custom_getter`."]}, {"number": 16630, "title": "Replace 'Dan' with 'Dandelion' in the citations", "body": "", "comments": []}, {"number": 16629, "title": "How to get RunMetadata for tf.data.Dataset ops?", "body": "Tensorflow version: 1.5 (pip)\r\n\r\nI am interested in the runtime of ops when using the `tf.data.Dataset` api.\r\nInformation like how long shuffle, repeat or batch took.\r\nWhen running the code below and visualizing it in tensoboard the intersting ops are marked as \"unused substructure\".\r\n\r\nIs my approach the right one?\r\nI find it a little confusing that they show up, but are not traceable.\r\n\r\nExample code\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.set_random_seed(0)\r\n\r\ndata = np.array([[10] * 10] * 3 + [[4] * 10]).astype(np.float32)\r\n\r\nwith tf.Graph().as_default():\r\n  dataset = tf.data.Dataset.from_tensor_slices(data)\r\n  sess = tf.Session()\r\n\r\n  dataset = dataset.cache()\r\n  dataset = dataset.shuffle(10, seed=0)\r\n  dataset = dataset.repeat(5)\r\n  dataset = dataset.batch(2)\r\n\r\n  iterator = tf.data.Iterator.from_structure(\r\n      output_types=tf.float32)\r\n  batch = iterator.get_next()\r\n\r\n  init1 = iterator.make_initializer(dataset)\r\n\r\n  sess.run(init1)\r\n\r\n  writer = tf.summary.FileWriter('tmp/datasettest/', sess.graph)\r\n  run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n\r\n  i = 0\r\n  while True:\r\n    try:\r\n      run_metadata = tf.RunMetadata()\r\n      sess.run(batch, options=run_options, run_metadata=run_metadata)\r\n      writer.add_run_metadata(run_metadata, 'step{}'.format(i))\r\n      i += 1\r\n    except tf.errors.OutOfRangeError as ex:\r\n      break\r\n    except Exception as ex:\r\n      raise ex\r\n```\r\nThis will result in the following visualization in Tensorboard.\r\n\r\n![image](https://user-images.githubusercontent.com/9438971/35647492-054aa5b0-06d3-11e8-8394-9b821fb79603.png)\r\n", "comments": ["/CC @mrry, can you comment?", "There's no way to do that right now. The `tf.RunMetadata` interface isn't ideal for capturing this kind of information, because a `Dataset` transformation might contribute to 0, 1, or multiple different runs, and it may happen asynchronously with any of them. The transformations produce events for a tracing interface that captures fine-grained execution information, but this interface is [currently unimplemented](https://github.com/tensorflow/tensorflow/blob/995d836e9ba7cbee56948f73bdbd099d419e4511/tensorflow/core/platform/default/tracing_impl.h).", "Assigning to @mrry for implementing the tracing_impl.h interface.", "I doubt I will work on this in the near future. ", "You seem to be using older version(1.x) of Tensorflow which is not supported anymore. Please try the latest [Tensorflow version](https://www.tensorflow.org/install/pip) and let us know if the problem still persists. \r\nAlso, refer the [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) API in Tensorflow 2.x version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 16628, "title": "Tensorflow switches to CPU when using Variable.assign", "body": "### System information\r\n- **OS**:Windows 10\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.4\r\n- **CUDA/cuDNN version**: 8.0 / 64\r\n- **GPU model and memory**: GeForce GTX 1080 8 GB\r\n- **Exact command to reproduce**: run the provided code below\r\n\r\n### Describe the problem\r\nI'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).\r\n\r\n### Source code / logs\r\n\r\nI did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef XOR(x_, y_):\r\n    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=\"Theta1\")\r\n    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=\"Theta2\")\r\n\r\n    Bias1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\r\n    Bias2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\r\n\r\n    with tf.name_scope(\"layer2\"):\r\n        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)\r\n\r\n    with tf.name_scope(\"layer3\"):\r\n        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)\r\n\r\n    with tf.name_scope(\"cost\"):\r\n        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +\r\n                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)\r\n\r\n    return cost\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')\r\n    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')\r\n    xor_cost = XOR(x_, y_)\r\n\r\n    learning_rate = tf.Variable(0.1, dtype=tf.float32)\r\n\r\n    with tf.name_scope(\"train\"):\r\n        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)\r\n\r\n    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]\r\n    XOR_Y = [[0], [1], [1], [0]]\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    for i in range(40001):\r\n        sess.run(learning_rate.assign(0.1))\r\n\r\n        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})\r\n\r\n        if i % 100 == 0:\r\n            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))\r\n\r\n\r\n```", "comments": ["Do not create ops (assign) in the training loop.", "@ppwwyyxx Oh, okay, I think I got how to do this.\r\n\r\n```\r\n update_lr_placeholder = tf.placeholder(learning_rate.dtype, shape=learning_rate.get_shape())\r\n update_lr_op = learning_rate.assign(update_lr_placeholder)\r\n\r\n sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    for i in range(40001):\r\n        sess.run(update_lr_op, {update_lr_placeholder: 0.1})\r\n\r\n        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})\r\n```\r\n\r\nThank you for pointing this out."]}, {"number": 16627, "title": "TF consumes all available RAM with a particular combination of conv2d, batch_norm and LSTM", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0.176 / 7.0.5\r\n- **GPU model and memory**: Tesla V100 (AWS P3), 16GB\r\n- **Exact command to reproduce**: python debug_tf.py\r\n\r\n### Describe the problem\r\nWhen the following code is run on a p3.2xlarge, the python process starts consuming RAM indefinitely, until the entire server RAM is used and the server dies. That's system RAM, not GPU memory. \r\n\r\nIt doesn't look it, but the code below is the smallest I could find that produces the bad behavior.\r\n\r\n1. Replacing residual_conv with a simple convolution makes the code work (i.e not hang).\r\n2. Reducing the repetition number in `layers.repeat` (to e.g 2) makes the code work.\r\n3. Removing the batch normalization from residual_conv makes the code work.\r\n4. Removing the LSTM makes the code work.\r\n5. The weirdest of all, if I set `is_training=True` in batch_norm instead of `is_training=is_training_var` whose value is set to `True` in the feed_dict, then the code works.\r\n\r\nThe same code runs successfully on a AWS P2 server with TensorFlow 1.3.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.layers as layers\r\nfrom tensorflow.contrib.framework import arg_scope\r\n\r\n\r\ndef residual_conv(incoming, num_filters, scope, bn=True):\r\n\twith tf.variable_scope(scope):\r\n\t    input_filters = incoming.get_shape().as_list()[-1]\r\n\t    if input_filters != num_filters:\r\n\t        incoming = layers.conv2d(incoming, num_filters, scope='adjust_conv')\r\n\r\n\t    after_conv1 = layers.conv2d(incoming, num_filters)\r\n\t    after_conv2 = layers.conv2d(after_conv1, num_filters, normalizer_fn=None, activation_fn=None)\r\n\r\n\t    net = incoming + after_conv2\r\n\r\n\t    if bn:\r\n\t        net = layers.batch_norm(net)\r\n\r\n\t    return net\r\n\r\ndef tf_bilstm(incoming, n_units, name):\r\n\tnet = incoming\r\n\r\n\tlstm_f = tf.contrib.rnn.LSTMCell(n_units)\r\n\tlstm_b = tf.contrib.rnn.LSTMCell(n_units)\r\n\r\n\twith tf.variable_scope(name):\r\n\t    results, _ = tf.nn.bidirectional_dynamic_rnn(lstm_f, lstm_b, net, \r\n\t                                                 dtype=tf.float32, time_major=True)\r\n\treturn tf.concat(results, axis=2)\r\n\r\ndef main():\r\n\tx_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))\r\n\tis_training_var = tf.placeholder(dtype=bool)\r\n\r\n\tnet = x_var\r\n\r\n\twith arg_scope([layers.batch_norm], is_training=is_training_var, decay=0.99, scale=True\r\n\t               ), \\\r\n\t     arg_scope([layers.conv2d], padding='SAME', kernel_size=(3, 3)), \\\r\n\t     arg_scope([layers.max_pool2d], padding='SAME', kernel_size=(2, 2), stride=(2, 2)):\r\n\r\n\t    net = layers.repeat(net, 20, residual_conv, 64, scope='block1')\r\n\t    net = layers.max_pool2d(net)\r\n\r\n\t    net = tf.squeeze(net, 2)\r\n\t    net = tf.transpose(net, [1, 0, 2])\r\n\t    net = tf_bilstm(net, 512, 'lstm1')\r\n\r\n\tsess = tf.Session()\r\n\tsess.run(tf.global_variables_initializer())\r\n\t\r\n\tX = np.zeros([10, 100, 2, 512])\r\n\r\n\tprint('Right before TF call!')\r\n\r\n\ta = sess.run(net, {x_var: X, is_training_var: True})\r\n\r\n\tprint(a)\r\n\r\n\r\nif __name__ == '__main__':\r\n\tmain()\r\n```\r\n\r\nEdited: Simplified the code.", "comments": ["Can you give the logs of the errors?\r\n\r\nI believe layers.batch_norm only accept is_training of type python bool, it does not accept tf.placeholder. So if you really want to do `is_training_var`, you may do so,\r\n`\r\ntrain_bn = layers.batch_norm(is_training=True) # put them same name, same scope  \r\n\r\ninference_bn = layers.batch_norm(is_training=False, reuse=True)  \r\n\r\nbn = tf.cond(is_training_var, lambda: train_bn, lambda: inference_bn)\r\n`", "There is no output, it just hangs after the `print` line and starts eating up memory.\r\n\r\nI just tried replacing `batch_norm` with `layer_norm`, which doesn't have an `is_training` parameter at all - no luck, still eats up all the memory.\r\n\r\nIt's also worth mentioning that the exact same code runs successfully on an AWS P2 server which has tensorflow 1.3 installed.", "I can reproduce on 1.5, but not on the master branch. So, this issue has been fixed, but we should probably identity the commit that fixed it and put it in 1.5.\r\n\r\n/CC @ebrevdo, any ideas what commit fixed this?", "I've had the same problem in 1.5.0 with a VQA model (CNN for image inputs, LSTM for text inputs, and merging both, then classifying) and found out the problem happens at the merging layer (tried using concatenate and multiply). RAM goes up -> swap goes up -> machine hangs. Trying to run the sub-models before merging works fine. Works fine on 1.4.1, but I haven't tried using master yet.", "I just encountered a similar bug and a weird workaround when upgrading to 1.5 on a different codebase. While trying to diagnose that problem by injecting various Print ops around the graph I found a bizarre fix: if you wrap the placeholder with a tf.identity() (or Print()) and then feed to that tensor rather than the placeholder, it stops hanging.\r\n\r\nI just tried it and, sure enough, replacing the line in the above code\r\n\r\n`x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))`\r\n\r\nwith\r\n\r\n`x_var = tf.identity(tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512)))`\r\n\r\nfixes the memory hang problem for me on a K80 instance with Tensorflow 1.5. And, since that tensor still gets fed, it does not seem to affect downstream computation.\r\n\r\nNot sure what is common between the codebases, but for what it's worth the one I am working on also involves a combination of conv layers, a tf.while_loop, and placeholders with multiple unknown dimensions.", "Could it be a grappler issue?\n\nOn Thu, Feb 1, 2018, 1:07 PM John Ingraham <notifications@github.com> wrote:\n\n> I just encountered a similar bug and a weird workaround when upgrading to\n> 1.5 on a different codebase. While trying to diagnose that problem by\n> injecting various Print ops around the graph I found a bizarre fix: if you\n> wrap the placeholder with a tf.identity() (or Print()) and then feed to\n> that tensor rather than the placeholder, it stops hanging.\n>\n> I just tried it and, sure enough, replacing the line in the above code\n>\n> x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))\n>\n> with\n>\n> x_var = tf.identity(tf.placeholder(dtype=tf.float32, shape=(None, None,\n> None, 512)))\n>\n> fixes the memory hang problem for me on a K80 instance with Tensorflow\n> 1.5. And, since that tensor still gets fed, it does not seem to affect\n> downstream computation.\n>\n> Not sure what is common between the codebases, but for what it's worth the\n> one I am working on also involves a combination of conv layers, a\n> tf.while_loop, and placeholders with multiple unknown dimensions.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16627#issuecomment-362401867>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzgdJCug1oEyNTbQ39o14OkWeUN_ks5tQieZgaJpZM4R0WeN>\n> .\n>\n", "> Could it be a grappler issue?\r\n\r\n/CC @zhangyaobit", "@ebrevdo nothing obvious comes to mind, but it is possible. Can you try to bisect it internally?", "I can confirm that @jingraham 's workaround fixes the issue. I ended up using this function, since I need to be able to load the placeholders by name:\r\n\r\n```python\r\ndef placeholder(**kwargs):\r\n    name = kwargs['name']\r\n    del kwargs['name']\r\n\r\n    tf_placeholder = tf.placeholder(**kwargs)\r\n    return tf.identity(tf_placeholder, name=name)\r\n```", "I'm gonna close this as it's fixed in master (and presumably 1.6?) and there's a workaround."]}, {"number": 16626, "title": "example script multivariate.py throws \"UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.  This may consume a large amount of memory.\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI am using the example script `tensorflow/contrib/timeseries/examples/multivariate.py`\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS High Sierra (darwin)\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nv1.5.0-rc1-1781-g86c10063c8 1.5.0-rc1\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**:\r\nApple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**:\r\nn/a compiled without CUDA support\r\n- **GPU model and memory**:\r\nn/a GPU not supported on Mac with SIP\r\n- **Exact command to reproduce**:\r\n$ python $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/contrib/timeseries/examples/multivariate.py\r\n\r\n### Describe the problem\r\nThe script throws warning:\r\n`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`\r\n\r\n### Source code / logs\r\nHere is the traceback of the warning on my system (I have tensorflow installed in a virtual environment called \"tf3\".:\r\n```\r\n  File \"multivariate.py\", line 59, in multivariate_train_and_sample\r\n    estimator.train(input_fn=train_input_fn, steps=training_steps)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 352, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 809, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 790, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py\", line 228, in create_estimator_spec\r\n    return self._train_ops(features)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py\", line 85, in _train_ops\r\n    learning_rate=None)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 241, in optimize_loss\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 458, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 610, in gradients\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 376, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 610, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 589, in _PadGrad\r\n    x_grad = array_ops.slice(grad, begin, sizes)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 640, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4591, in _slice\r\n    \"Slice\", input=input, begin=begin, size=size, name=name)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1036, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 97, in _IndexedSlicesToTensor\r\n    \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n  File \"~/.pyenv/versions/3.6.4/lib/python3.6/warnings.py\", line 99, in _showwarnmsg\r\n    msg.file, msg.line)\r\n~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n```\r\nI see discussion about this warning on SO:\r\nhttps://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation/35893467\r\nBut the problem seems to happen in the estimator \"train\" method, not in any custom code I have written.", "comments": ["You probably want to check the max index of your sparse tensor, or set the static shape to be sure.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]