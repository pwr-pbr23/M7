[{"number": 40752, "title": "Fix imports of generated go proto bindings", "body": "### Fixes\r\n[#39744](https://github.com/tensorflow/tensorflow/issues/39744) \r\n\r\n### Summary\r\n\r\nAfter running the following in accordance with the [installation instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md) for go:\r\n```\r\n$ go get -d github.com/tensorflow/tensorflow/tensorflow/go\r\n$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op\r\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\n```\r\n\r\nThe first command generates error message unrelated to this PR, a [fix for which](https://github.com/tensorflow/tensorflow/pull/36523) is ready to pull. That error message is relatively low impact and should not impact the functionality of the library after the `go generate` command is run. However, the third command generates the following error:\r\n\r\n```\r\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\n../../../go/src/github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:25:2: cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t/home/abls/go/src/github.com/tensorflow/tensorflow/tensorflow/go/vendor/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (vendor tree)\r\n\t/usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t/home/abls/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\n```\r\n\r\nThis is caused by a bad import that seemed to be from an accident during a [refactor](https://github.com/tensorflow/tensorflow/commit/fd895bf2b98250929a442e5cf689f6bb272ac52c). The problematic import that occurs in several files is:\r\n```\r\nimport corepb \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\"\r\n```\r\n\r\nThis line is supposed to point to the generated files, however the generated files are actually placed in `github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto` due to the following line in several proto files in `tensorflow/core/protobuf` read by protoc during generation:\r\n```\r\noption go_package = \"github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto\";\r\n```\r\n\r\nThe fix is simply to correct the import lines to point to the actual location of the files.", "comments": ["Needed to change this a bit to match the BUILD rule, which is required for bazel compatibility:\r\nhttps://github.com/tensorflow/tensorflow/commit/e5e495db7bee77cd0fd5dda3b06bd743cbcf1ef8", "> Needed to change this a bit to match the BUILD rule, which is required for bazel compatibility:\r\n> [e5e495d](https://github.com/tensorflow/tensorflow/commit/e5e495db7bee77cd0fd5dda3b06bd743cbcf1ef8)\r\n\r\n@jhseu Any idea when it will be in a release? \r\n"]}, {"number": 40751, "title": "Tensorflow-rocm GPU useage confirmation", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): tensorflow-rocm\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Rocm version: 3.5.1\r\n- GPU model and memory: RX580\r\n\r\n\r\n\r\n**The problem**\r\nwhile importing the tensorflow api module there is no issue but while im trying to confirm that its running on gpu\r\nwith the following code : ```print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))```\r\nthe issue pops up\r\n\r\n\r\n**logs**\r\n```\r\n2020-06-24 14:42:51.220457: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libhip_hcc.so'; dlerror: libhip_hcc.so: cannot open shared object file: No such file or directory\r\n2020-06-24 14:42:51.220491: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: Could not load dynamic library 'libhip_hcc.so'; dlerror: libhip_hcc.so: cannot open shared object file: No such file or directory\r\nAborted (core dumped)\r\n```\r\n**Additional Info**\r\nI have setup the tensorflow-rocm with the following commands :\r\n```\r\nsudo apt install libnuma-dev\r\nwget -q -O - http://repo.radeon.com/rocm/apt/debian/rocm.gpg.key | sudo apt-key add -\r\necho 'deb [arch=amd64] http://repo.radeon.com/rocm/apt/debian/ xenial main' | sudo tee /etc/apt/sources.list.d/rocm.list\r\nsudo apt install rocm-dkms\r\nsudo usermod -a -G video $LOGNAME\r\necho 'export PATH=$PATH:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin' | sudo tee -a /etc/profile.d/rocm.sh\r\nsudo apt install rocm-libs miopen-hip rccl\r\npip3 install tensorflow-rocm\r\n```", "comments": ["I have same error... and exactly same set up too. Any operation relate to tensorflow seem to crash it ( same to your error) . Please tell me if you solved the problem.", "Hi, I did solved it (some other guy tell me) but running into another problem: \r\n\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/1022\r\nbasically you have to run:\r\na) cp /opt/rocm/hip/lib/libamdhip64.so.3.5.30501 /usr/lib/libhip_hcc.so\r\nb) ldconfig (the first letter is L - this is used for updating the links/cache of shared libraries, you can run it from anywhere)\r\n\r\nBut I got another problem: \r\n:\r\n2020-06-24 19:13:58.587688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-24 19:13:58.587693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-24 19:13:58.587837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7399 MB memory) -> physical GPU (device: 0, name: Ellesmere [Radeon RX 470/480/570/570X/580/580X], pci bus id: 0000:01:00.0)\r\nSegmentation fault (core dumped)\r\n\r\nplease try it and tell me you have this same problem.", "and here i got the following error after following the above steps :\r\n```2020-06-24 18:12:21.057717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libhip_hcc.so\r\n2020-06-24 18:12:21.057866: E tensorflow/stream_executor/rocm/rocm_driver.cc:975] could not retrieve ROCM device count: HIP_ERROR_NoDevice\r\n2020-06-24 18:12:21.057886: E tensorflow/stream_executor/rocm/rocm_driver.cc:975] could not retrieve ROCM device count: HIP_ERROR_NoDevice\r\nNum GPUs Available:  0\r\n```\r\nMy rocminfo :\r\n```\r\nAble to open /dev/kfd read-write\r\n=====================    \r\nHSA System Attributes    \r\n=====================    \r\nRuntime Version:         1.1\r\nSystem Timestamp Freq.:  1000.000000MHz\r\nSig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)\r\nMachine Model:           LARGE                              \r\nSystem Endianness:       LITTLE                             \r\n\r\n==========               \r\nHSA Agents               \r\n==========               \r\n*******                  \r\nAgent 1                  \r\n*******                  \r\n  Name:                    AMD Ryzen 5 3500 6-Core Processor  \r\n  Uuid:                    CPU-XX                             \r\n  Marketing Name:          AMD Ryzen 5 3500 6-Core Processor  \r\n  Vendor Name:             CPU                                \r\n  Feature:                 None specified                     \r\n  Profile:                 FULL_PROFILE                       \r\n  Float Round Mode:        NEAR                               \r\n  Max Queue Number:        0(0x0)                             \r\n  Queue Min Size:          0(0x0)                             \r\n  Queue Max Size:          0(0x0)                             \r\n  Queue Type:              MULTI                              \r\n  Node:                    0                                  \r\n  Device Type:             CPU                                \r\n  Cache Info:              \r\n    L1:                      32768(0x8000) KB                   \r\n  Chip ID:                 0(0x0)                             \r\n  Cacheline Size:          64(0x40)                           \r\n  Max Clock Freq. (MHz):   3600                               \r\n  BDFID:                   0                                  \r\n  Internal Node ID:        0                                  \r\n  Compute Unit:            6                                  \r\n  SIMDs per CU:            0                                  \r\n  Shader Engines:          0                                  \r\n  Shader Arrs. per Eng.:   0                                  \r\n  WatchPts on Addr. Ranges:1                                  \r\n  Features:                None\r\n  Pool Info:               \r\n    Pool 1                   \r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    8160184(0x7c83b8) KB               \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       TRUE                               \r\n    Pool 2                   \r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      \r\n      Size:                    8160184(0x7c83b8) KB               \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       TRUE                               \r\n  ISA Info:                \r\n    N/A                      \r\n*******                  \r\nAgent 2                  \r\n*******                  \r\n  Name:                    gfx803                             \r\n  Uuid:                    GPU-XX                             \r\n  Marketing Name:          Ellesmere [Radeon RX 470/480/570/570X/580/580X/590]\r\n  Vendor Name:             AMD                                \r\n  Feature:                 KERNEL_DISPATCH                    \r\n  Profile:                 BASE_PROFILE                       \r\n  Float Round Mode:        NEAR                               \r\n  Max Queue Number:        128(0x80)                          \r\n  Queue Min Size:          4096(0x1000)                       \r\n  Queue Max Size:          131072(0x20000)                    \r\n  Queue Type:              MULTI                              \r\n  Node:                    1                                  \r\n  Device Type:             GPU                                \r\n  Cache Info:              \r\n    L1:                      16(0x10) KB                        \r\n  Chip ID:                 26591(0x67df)                      \r\n  Cacheline Size:          64(0x40)                           \r\n  Max Clock Freq. (MHz):   1360                               \r\n  BDFID:                   1536                               \r\n  Internal Node ID:        1                                  \r\n  Compute Unit:            36                                 \r\n  SIMDs per CU:            4                                  \r\n  Shader Engines:          4                                  \r\n  Shader Arrs. per Eng.:   1                                  \r\n  WatchPts on Addr. Ranges:4                                  \r\n  Features:                KERNEL_DISPATCH \r\n  Fast F16 Operation:      FALSE                              \r\n  Wavefront Size:          64(0x40)                           \r\n  Workgroup Max Size:      1024(0x400)                        \r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)                        \r\n    y                        1024(0x400)                        \r\n    z                        1024(0x400)                        \r\n  Max Waves Per CU:        40(0x28)                           \r\n  Max Work-item Per CU:    2560(0xa00)                        \r\n  Grid Max Size:           4294967295(0xffffffff)             \r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)             \r\n    y                        4294967295(0xffffffff)             \r\n    z                        4294967295(0xffffffff)             \r\n  Max fbarriers/Workgrp:   32                                 \r\n  Pool Info:               \r\n    Pool 1                   \r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      \r\n      Size:                    8388608(0x800000) KB               \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       FALSE                              \r\n    Pool 2                   \r\n      Segment:                 GROUP                              \r\n      Size:                    64(0x40) KB                        \r\n      Allocatable:             FALSE                              \r\n      Alloc Granule:           0KB                                \r\n      Alloc Alignment:         0KB                                \r\n      Accessible by all:       FALSE                              \r\n  ISA Info:                \r\n    ISA 1                    \r\n      Name:                    amdgcn-amd-amdhsa--gfx803          \r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE            \r\n      Profiles:                HSA_PROFILE_BASE                   \r\n      Default Rounding Mode:   NEAR                               \r\n      Default Rounding Mode:   NEAR                               \r\n      Fast f16:                TRUE                               \r\n      Workgroup Max Size:      1024(0x400)                        \r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)                        \r\n        y                        1024(0x400)                        \r\n        z                        1024(0x400)                        \r\n      Grid Max Size:           4294967295(0xffffffff)             \r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)             \r\n        y                        4294967295(0xffffffff)             \r\n        z                        4294967295(0xffffffff)             \r\n      FBarrier Max Size:       32                                 \r\n*** Done ***      \r\n```", "Hmm, for your information I after the steps above could print out number of GPUs device ( which is 1 on my computer ). But I couldn't do any operation GPUS related on tensorflow. Which lead to error I mentioned.\r\n\r\nDid you tried rocm-smi?", "\r\n```========================ROCm System Management Interface========================\r\n================================================================================\r\nGPU  Temp   AvgPwr   SCLK    MCLK    Fan     Perf  PwrCap  VRAM%  GPU%  \r\n0    37.0c  30.047W  600Mhz  300Mhz  42.75%  auto  145.0W    2%   0%    \r\n================================================================================\r\n==============================End of ROCm SMI Log ==============================\r\n```", "It look like your rocm-smi working fine ( 0 is the index of the device ). So indeed rocm detected your GPU. I don't know ... Please keep updating as we have same set up.", "did you try the docker installation tho? worked fine for me tbh \r\nlink : https://hub.docker.com/r/rocm/tensorflow\r\nbut i need the base installation for my work now and the issues are popping up", "> It look like your rocm-smi working fine ( 0 is the index of the device ). So indeed rocm detected your GPU. I don't know ... Please keep updating as we have same set up.\r\n\r\ni fixed the problem by adding my user to the rendergroup thanks alot", "> Hi, I did solved it (some other guy tell me) but running into another problem:\r\n> \r\n> [ROCmSoftwarePlatform#1022](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/1022)\r\n> basically you have to run:\r\n> a) cp /opt/rocm/hip/lib/libamdhip64.so.3.5.30501 /usr/lib/libhip_hcc.so\r\n> b) ldconfig (the first letter is L - this is used for updating the links/cache of shared libraries, you can run it from anywhere)\r\n> \r\n> But I got another problem:\r\n> :\r\n> 2020-06-24 19:13:58.587688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] 0\r\n> 2020-06-24 19:13:58.587693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0: N\r\n> 2020-06-24 19:13:58.587837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7399 MB memory) -> physical GPU (device: 0, name: Ellesmere [Radeon RX 470/480/570/570X/580/580X], pci bus id: 0000:01:00.0)\r\n> Segmentation fault (core dumped)\r\n> \r\n> please try it and tell me you have this same problem.\r\n\r\ni have the same error as well while tryna run the code", "> Hi, I did solved it (some other guy tell me) but running into another problem:\r\n> \r\n> [ROCmSoftwarePlatform#1022](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/1022)\r\n> basically you have to run:\r\n> a) cp /opt/rocm/hip/lib/libamdhip64.so.3.5.30501 /usr/lib/libhip_hcc.so\r\n> b) ldconfig (the first letter is L - this is used for updating the links/cache of shared libraries, you can run it from anywhere)\r\n> \r\n> But I got another problem:\r\n> :\r\n> 2020-06-24 19:13:58.587688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] 0\r\n> 2020-06-24 19:13:58.587693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0: N\r\n> 2020-06-24 19:13:58.587837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7399 MB memory) -> physical GPU (device: 0, name: Ellesmere [Radeon RX 470/480/570/570X/580/580X], pci bus id: 0000:01:00.0)\r\n> Segmentation fault (core dumped)\r\n> \r\n> please try it and tell me you have this same problem.\r\n\r\nThanks, This is working for me (with vega 56 GPU, everything else is same as the first post)", "> > Hi, I did solved it (some other guy tell me) but running into another problem:\r\n> > [ROCmSoftwarePlatform#1022](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/1022)\r\n> > basically you have to run:\r\n> > a) cp /opt/rocm/hip/lib/libamdhip64.so.3.5.30501 /usr/lib/libhip_hcc.so\r\n> > b) ldconfig (the first letter is L - this is used for updating the links/cache of shared libraries, you can run it from anywhere)\r\n> > But I got another problem:\r\n> > :\r\n> > 2020-06-24 19:13:58.587688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108] 0\r\n> > 2020-06-24 19:13:58.587693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0: N\r\n> > 2020-06-24 19:13:58.587837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7399 MB memory) -> physical GPU (device: 0, name: Ellesmere [Radeon RX 470/480/570/570X/580/580X], pci bus id: 0000:01:00.0)\r\n> > Segmentation fault (core dumped)\r\n> > please try it and tell me you have this same problem.\r\n> \r\n> i have the same error as well while tryna run the code\r\n\r\nI fixed it by reinstall every thing AND DID NOT do `cp /opt/rocm/hip/lib/libamdhip64.so.3.5.30501 /usr/lib/libhip_hcc.so` instead, I tried: `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/hip/lib` as in:\r\nhttps://github.com/RadeonOpenCompute/ROCm/issues/1163#issuecomment-648910306\r\n\r\nPlease try it and tell me if that solved yours problem.\r\nSigh ... finally, I can get some sleep. \r\n", "solves the issue for one terminal not fully\r\n", "Ok issue solved fully type these in the terminal\n```\necho 'export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH' >> $HOME/.bashrc \nsource $HOME/.bashrc\nsudo reboot\n```\n\nThanks everyone closing the issue"]}, {"number": 40750, "title": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize", "body": "Tensorflow version 2.2.0\r\nTensorlow nightly installed\r\nTensorflow nighlty-gpu installed\r\n\r\nconda list cudnn gives\r\nversion 7.6.5 build         cuda10.1_0\r\n\r\nnvidia-smi reports \r\ndriver version 446.14     cuda version 11.0\r\n\r\nNvidia 1660TI graphica card.  \r\nWindows 10 Pro 64 bit.\r\n\r\nthe version numbers should be ok as per the Tensorflow webside https://www.tensorflow.org/install/gpu\r\n\r\nCode to train model and use callbacks.  The error is much worse when i use verbose=1, so changed to verbose =10 as per suggestion from previous run error message.\r\n```\r\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \r\n                                                      histogram_freq=1,\r\n                                                      write_images=True,\r\n                                                      update_freq = 'batch')\r\n\r\n\r\nprint(datetime.datetime.now().strftime(\"%Y%m%d-%H:%M\"))\r\nhistory = model.fit(\r\n        train_data_gen,\r\n        batch_size = BATCH_SIZE,\r\n        epochs=EPOCHS,\r\n        verbose = 10,\r\n        validation_data=val_data_gen,\r\n        callbacks = tensorboard_callback)\r\n\r\nprint(datetime.datetime.now().strftime(\"%Y%m%d-%H:%M\"))\r\n```\r\n\r\nERROR Message\r\n20200624-10:18\r\nEpoch 1/25\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-9-127169b78b1e>\", line 15, in <module>\r\n    callbacks = tensorboard_callback)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 611, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node resnet50/conv1_conv/Conv2D (defined at <ipython-input-3-0d7d4ce5ce9a>:8) ]] [Op:__inference_train_function_17457]\r\n\r\nFunction call stack:\r\ntrain_function", "comments": ["@Cillinc,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40646#issuecomment-647224203) from a similar issue and let us know if it works. Another possible duplicate [#24828](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-464910864)  \r\n\r\nThanks!", "thanks - added as suggested.   Got a new error....\r\n\r\n20200624-17:05\r\nEpoch 1/25\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000020E10437D38> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 4, expecting 3\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000020E10437D38> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 4, expecting 3\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-4-b9946ba10810>\", line 8, in <module>\r\n    callbacks = tensorboard_callback)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\n\r\nInvalidArgumentError:  Incompatible shapes: [32,7,7] vs. [32,1]\r\n\t [[node Equal (defined at <ipython-input-4-b9946ba10810>:8) ]] [Op:__inference_train_function_17457]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\n", "changes the VERBOSE value in my call back as i think i had it wrong ...\r\n\r\ntrain_data_gen just for completeness...\r\n\r\n```\r\n> train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\r\n> validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\r\n> \r\n> train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\r\n>                                                            directory=train_path,\r\n>                                                            shuffle=True,\r\n>                                                            target_size=(IMG_HEIGHT, IMG_WIDTH),\r\n>                                                            class_mode='binary')\r\n> \r\n```\r\nas you can see it is based off samples on the Tensorflow website \r\n\r\n```\r\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \r\n                                                      histogram_freq=1,\r\n                                                      write_images=True,\r\n                                                      update_freq = 'batch')\r\n```\r\nhave used that call back before and it worked.....\r\n\r\n```\r\nprint(datetime.datetime.now().strftime(\"%Y%m%d-%H:%M\"))\r\nhistory = model.fit(\r\n        train_data_gen,\r\n        batch_size = BATCH_SIZE,\r\n        epochs=EPOCHS,\r\n        verbose = 1,\r\n        validation_data=val_data_gen,\r\n        callbacks = tensorboard_callback)\r\n```\r\n\r\nprint(datetime.datetime.now().strftime(\"%Y%m%d-%H:%M\"))\r\n20200624-17:08\r\nEpoch 1/25\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-5-0d7d4ce5ce9a>\", line 8, in <module>\r\n    callbacks = tensorboard_callback)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 611, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\cilli\\anaconda3\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\n\r\nInvalidArgumentError:  Incompatible shapes: [32,7,7] vs. [32,1]\r\n\t [[node Equal (defined at <ipython-input-4-b9946ba10810>:8) ]] [Op:__inference_train_function_17457]\r\n\r\nFunction call stack:\r\ntrain_function", "it seems to be working after last nights updates.  However now i can't get Tensorboard to work....fix one break one....", "> it seems to be working after last nights updates. However now i can't get Tensorboard to work....fix one break one....\r\n\r\nCould you please provide the complete code you're running along with the error log, so that we can look into it. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40749, "title": "[EXT-SYSLIB] Add absl_py logging submodule to build flow.", "body": "Enable proper build when ```absl_py``` is external ```LOCAL_LIBS```.\r\n\r\n```\r\nERROR: /home/cbalint/rpmbuild/BUILD/tensorflow/tensorflow/tensorflow/python/distribute/BUILD:1733:11: no such package '@absl_py//absl/logging': BUILD file not found in directory 'absl/logging' of external repository @absl_py. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/python/distribute:multi_process_runner'\r\n```\r\n\r\nCc @perfinion \r\n\r\nThank You !\r\n", "comments": ["@perfinion ping"]}, {"number": 40748, "title": "Gradient calculation of tf.reduce_prod runs in CPU, inducing performance impact", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Ubuntu 19.04_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _No_\r\n- TensorFlow installed from (source or binary): _binary_\r\n- TensorFlow version (use command below): _2.2.0_\r\n- Python version: _3.7.7_\r\n- Bazel version (if compiling from source): _N/A_\r\n- GCC/Compiler version (if compiling from source): _N/A_\r\n- CUDA/cuDNN version: _CUDA Version 10.2.89_\r\n- GPU model and memory: _GeForce RTX 2080 8G_\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nGradient of tf.reduce_prod() runs in CPU instead of GPU, which induces lots of memcopy between GPU & CPU as shown in profiler, and slows down entire training / prediction process as GPU & CPU ops interleaves (Kernel Launch Time would dominate in the profiler report).\r\nPertinent code can be found [here](https://github.com/tensorflow/tensorflow/blob/4f341bb742718721563ce6dccb965c85a1fbdcf5/tensorflow/python/ops/math_grad.py#L270)\r\n\r\n**Describe the expected behavior**\r\nExpect the gradient to be calculated in GPU and streamline the whole process (especially in a typical RNN while loop)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n_To be provided_\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n_To be provided_", "comments": ["@carusyte \r\nI ran the code shared by you and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a255d16383bbaa8db62d3f1458556b47/untitled243.ipynb).", "@Saduf2019 \r\nThanks for your prompt reply. The link to the code I posted is just an excerpt of the tensorflow 2.2.0 codebase, which I meant to highlight the block starting with `with ops.device(\"/cpu:0\")` under `_ProdGrad(op, grad)` function.\r\n```\r\ndef _ProdGrad(op, grad):\r\n  ...omitted code for brevity...\r\n\r\n  # Pack all reduced dimensions into a single one, so we can perform the\r\n  # cumprod ops. If the reduction dims list is empty, it defaults to float32,\r\n  # so we need to cast here.  We put all the shape-related ops on CPU to avoid\r\n  # copying back and forth, and since listdiff is CPU only.\r\n  with ops.device(\"/cpu:0\"):\r\n    rank = array_ops.rank(op.inputs[0])\r\n    reduction_indices = (reduction_indices + rank) % rank\r\n    reduced = math_ops.cast(reduction_indices, dtypes.int32)\r\n    idx = math_ops.range(0, rank)\r\n    other, _ = array_ops.setdiff1d(idx, reduced)\r\n    perm = array_ops.concat([reduced, other], 0)\r\n    reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\r\n    other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\r\n\r\n  ...omitted code for brevity...\r\n```\r\n\r\nI thought this could be the cause of the performance bottleneck, resulting in high percentage of Kernel Launch time and memcpy operations, breaking the streamlined computation in GPU, as shown in the following screenshots from the profile:\r\n\r\n![image](https://user-images.githubusercontent.com/1680881/85830228-d39a5e80-b7be-11ea-86a4-45a633cb20ba.png)\r\n\r\n![image](https://user-images.githubusercontent.com/1680881/85830600-7521b000-b7bf-11ea-80d2-2c85cd8086f5.png)\r\n\r\n_In the trace view above, GPU operation stripes are shaped like a comb, which makes me suspect some kind of op must have been delegated to the CPU, interrupting GPU processing. Going further, I found several `Prod_grad` related ops under the CPU section, amongst other less recognizable ops_ \r\n\r\n![image](https://user-images.githubusercontent.com/1680881/85831574-13fadc00-b7c1-11ea-92a8-d19313746248.png)\r\n\r\nHowever, as I tried to dig a little deeper / get around this issue, by replacing `tf.reduce_prod` with a combination of `tf.math.cumprod`, `tf.slice` etc., the above `Prod_grad` ops went away under the CPU section, but the overall avg. 788 ms step time and its composition didn't make a difference.\r\n\r\nTherefore, I assumed `tf.reduce_prod` might not be the culprit for now... That being said, I'm kind of lost as to where to look for the contributor to the high Kernel Launch time. Here's the [profile](https://github.com/tensorflow/tensorflow/files/4835982/profile.zip) for tensorboard if you would like to investigate further. Note that this is just a simplified model where each step takes 788 ms. In my targeted deeper model it takes more than 2 seconds per step which is greatly hindered by the Kernel Launch time.\r\n\r\n\r\n", "@carusyte Could you please try on the latest stable TensorFlow version 2.7.0 and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40747, "title": "kronecker product", "body": "Doesn't it have a function of kronecker product like np.kron in numpy?", "comments": ["@ZhiwenZong \r\n\r\nCan you please go through this [link](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorKronecker) and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40746, "title": "kronecker product", "body": "Doesn't it have a function of kronecker  product like np.kron in numpy?", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40746\">No</a>\n"]}, {"number": 40745, "title": "[MLIR] Add constant folder for xla_hlo.broadcast_in_dim op", "body": "Fold broadcast_in_dim op if the operand is the result of a tensor splat.", "comments": ["@jpienaar @sherhut for additional visibility.", "Ping @sherhut ", "The test failures don't look like mine or I just can't tell. Any insights here?\r\n```\r\n(/job:localhost/replica:0/task:0/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\r\n*** stack smashing detected ***: /usr/bin/python3 terminated\r\nFatal Python error: Aborted\r\n\r\nThread 0x00007f1317210700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/unary_ops_test_gpu_mlir_bridge_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line \r\n...\r\n```\r\n", "It is failing in the XLA-MLIR path, and it does not fail without this patch: I don't know if this patch is triggering the issue by lack of luck but it does not seem entirely unrelated to me.", "All test cases failures are like the ones above: the trace is opaque and isn't a FileCheck based one. Do these indicate that FileCheck based tests are missing somewhere? Any pointers on where to dig will help. Is it possible that a {0-d const + broadcast} being converted to {n-d const} is triggering failures elsewhere? For IR size and downstream opts, this would appear to only make things easier. @sherhut ", "@bondhugula Can you please resolve conflicts? Thanks!", "@sherhut @jpienaar Any pointers on how to find out what's causing these failures? It looks like this optimization is making something else fail, and the failure from the log is opaque.", "> @bondhugula Can you please resolve conflicts? Thanks!\r\n\r\nResolved conflicts - but this revision is triggering failures at other places. It's not clear if the issue is with this optimization.\r\n", "I'll import it internally and see if I can get more helpful errors with ASAN/MSAN.", "Backtrace with assertion enabled:\r\n\r\n```\r\nunexpected element type\r\nUNREACHABLE executed at third_party/llvm/llvm-project/mlir/lib/IR/Attributes.cpp:645!\r\n*** SIGABRT received by PID 6377 (TID 6542) on cpu 28 from PID 6377; stack trace: ***\r\nPC: @     0x7ff4fd72c602  (unknown)  raise\r\n    @     0x55dd668cf9fa        192  FailureSignalHandler()\r\n    @     0x7ff4fd8e89a0   19457968  (unknown)\r\n    @     0x55dd5f03f4a7         48  llvm::llvm_unreachable_internal()\r\n    @     0x55dd57fe1117        192  mlir::DenseElementsAttr::AttributeElementIterator::operator*()\r\n    @     0x55dd42126f5f        144  mlir::DenseElementsAttr::getSplatValue<>()\r\n    @     0x55dd4211720e         16  mlir::DenseElementsAttr::getSplatValue()\r\n    @     0x55dd54a74be3        192  mlir::xla_hlo::BroadcastInDimOp::fold()\r\n    @     0x55dd54d4d6af        160  mlir::FoldingHook<>::foldHook()\r\n    @     0x55dd58118ca9        160  mlir::Operation::fold()\r\n    @     0x55dd57790ce4        320  mlir::OperationFolder::tryToFold()\r\n    @     0x55dd577906bd        176  mlir::OperationFolder::tryToFold()\r\n    @     0x55dd5779d6f6        336  (anonymous namespace)::GreedyPatternRewriteDriver::simplify()\r\n    @     0x55dd5779cfe7        160  mlir::applyPatternsAndFoldGreedily()\r\n    @     0x55dd574ab251        160  (anonymous namespace)::Canonicalizer::runOnOperation()\r\n\r\n```\r\n", "The offending case is `dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f64>>`", "> The offending case is `dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f64>>`\r\n\r\nThanks very much! The attribute elt iterator then is missing support for complex types. Interesting that no other piece of code tried to obtain the splat value for a complex constant splatted tensor.", "Filed https://bugs.llvm.org/show_bug.cgi?id=46588 against MLIR core. Added a guard to skip splat tensors on complex types.", "> Backtrace with assertion enabled:\r\n\r\nWhat specific options do you use to turn on assertions on the MLIR side while still ensuring a \"Release\" build for it if that's possible? I guess a debug build on the TF side will lead to a debug build for LLVM/MLIR but this would be too resource heavy as you know. Thanks!\r\n\r\n", "I don't think Mehdi used specific options vs just universally turning in assertions. Even if he did the build environments are quite different and what works on one need not translate to another (e.g., it would not be a considered supported/tested). Building more directed tests/tf-opt and using it to explore different tests would be cases of largest reuse.", "`$ bazelisk test -copt=-UNDEBUG //tensorflow/compiler/mlir/...` seems to work for me", "Rebased and resolved conflicts. ", "This is blocked by a failure in IREE, which is being worked on. Sorry for the delays.", "Hi Uday, yes, I'm looking at the failure on IREE side. In the meanwhile, I've a question on this and would like to get your thoughts and others. From what I can see we are materializing \"large\" constants from smaller ones by folding broadcast with constants. Is this always preferable that we want it to be always on? I think this can have consequences over downstream; it may require the downstream to \"redo\" the broadcast to get back the single value and then re-introduce the broadcasts from the single value. Because if following a naive lowering, we might need to back the \"large\" constant with a large buffer and see a large amount of data getting passed through; OTOH, broadcasts are kinda \"free\" via indexing if starting with a single value. So I'm wondering your thoughts on this.", "Seconding Lei; always-on folding/canonicalizations should be universally good. iota is another good example of a bad always-on folder: what would be a simple indexing op that can often be free instead creates giant tensors.", "> Hi Uday, yes, I'm looking at the failure on IREE side. In the meanwhile, I've a question on this and would like to get your thoughts and others. From what I can see we are materializing \"large\" constants from smaller ones by folding broadcast with constants. Is this always preferable that we want it to be always on? I think this can have consequences over downstream; it may require the downstream to \"redo\" the broadcast to get back the single value and then re-introduce the broadcasts from the single value. Because if following a naive lowering, we might need to back the \"large\" constant with a large buffer and see a large amount of data getting passed through; OTOH, broadcasts are kinda \"free\" via indexing if starting with a single value. So I'm wondering your thoughts on this.\r\n\r\nHi Lei, aren't constant splatted tensors dealt in an optimized way? (Are they sent with the constant just replicated?!) In terms of buffer requirements itself, if the broadcast is fused, so can the constant be propagated. If it isn't fused, the buffer requirements are still the same - the result of the broadcast would be that large buffer. How does the constant tensor generating op increase buffer requirements?\r\n", "Hi Uday, thanks for the quick reply! I doubt that we can make the assumption here that all downstream users will handle splat constant tensors in an \"optimized\" way. Different downstream users might have different limitations and cost models regarding broadcasting; so the \"optimized\" way for them might be different at all. It would be nice to avoid introducing always-on patterns at an early stage like HLO to assume a specific downstream use case. \r\n\r\nI don't want to block on your use case though; so I'm wondering whether it's possible to introduce this as a separate pattern (or pass) that allows one to opt-in so that you can pick it into the flow you have. The op's baked-in folder and canonicalizer is always on and there is no way to turn them off. IMO to be a always-on canonicalization/folding pattern it needs to meet with a high bar; it should universally good as Ben said. I see patterns for strength reduction, patterns for removing essentially no-op ops are following into that category. There are others. But IMO materializing large constants is controversial here that we'd think it's better to avoid to be always on. :)", "I'm in favor of having this as a pattern/pass that can be applied, but it seems too far to make it a canonicalization.\r\n\r\nLet's say that we landed this and then, in a future state, I wanted to do an optimization at the tensor-level which detected same-valued constants and demoted them to a scalar constant + a broadcast. I may want to do this in a cost-sensitive way based on program level dataflow based on knowledge about how such forms are materialized across the lower level boundaries.\r\n\r\nAs a canonicalization as done in this PR, it makes it impossible to do this kind of HLO->HLO level transformation because canonicalization will always apply the lowering to a materialized constant value that structurally means something quite different at various levels of the stack.\r\n\r\nFrom this perspective as well, the transformation done here is a lowering as it destroys information encoded in the original program (namely that the tensor originates from a scalar). It seems reasonable to have an optimization that does that transformation but canonicalizations should not be used for lowerings, imo.", "+1 to what Lei said. Instead of adding it as a folding pattern, given the concerns of materializing large constants (and assuming everything downstream handles splat constants effectively) maybe make it opt-in.\r\nFor the failure in IREE, this is hitting a phase ordering issue. There is some optimized handling on splat constants through use of LinalgFusionOfTensorOpsPass in MLIR.  Some pass upstream of this pass introduces some artifact that prevented the fusion and wouldnt optimize the splat constants. There is a simple  fix, but I think it would be good to resolve concerns about materializing large constants before submitting the PR.", "@antiagainst Thanks. How about adding a flag to the pass `fold_const_broadcast` that is false by default?", "> @antiagainst Thanks. How about adding a flag to the pass `fold_const_broadcast` that is false by default?\r\n\r\nLooks like I forgot that this is part of `-canonicalize` itself - so I don't think we can register a flag from the TF/MLIR side. Which test pass can this go into as an optional pattern?", "> +1 to what Lei said. Instead of adding it as a folding pattern, given the concerns of materializing large constants (and assuming everything downstream handles splat constants effectively) maybe make it opt-in.\r\n\r\n@MaheshRavishankar But a constant + broadcast_in_dim also materializes the large constant? How does a constant + broadcast_in_dim save you any buffer space over a constant tensor generating op? Same question for @stellaraccident and @antiagainst The input IR is already materializing the constant due to the `broadcast`  and only a fusion would \"unmaterialize\" it. Just trying to better understand the issue that you are hitting in IREE - is it missing something that exists for `broadcast_in_dim` but not for `constant`?", ">  But a constant + broadcast_in_dim also materializes the large constant? \r\n\r\nDo you mean that it would materialize it at runtime if interpreted in a straightforward manner?", "> > But a constant + broadcast_in_dim also materializes the large constant?\r\n> \r\n> Do you mean that it would materialize it at runtime if interpreted in a straightforward manner?\r\n\r\nThe output of the broadcast would be that large constant tensor - right? - unless the broadcast is fused into another op, in which case the constant generating op could also be fused in just as easily. How does 0-d const op + broadcast lead to a higher buffer requirement than an n-d const op?", "> > > But a constant + broadcast_in_dim also materializes the large constant?\r\n> > \r\n> > \r\n> > Do you mean that it would materialize it at runtime if interpreted in a straightforward manner?\r\n> \r\n> The output of the broadcast would be that large constant tensor - right? - unless the broadcast is fused into another op, in which case the constant generating op could also be fused in just as easily. How does 0-d const op + broadcast lead to a higher buffer requirement than an n-d const op?\r\n\r\nI agree that small constant + broadcast materialized the same large tensor as fused constant. I think the point being made is that eventually constants are materialized and unless care is taken while materializing constants you can end up with large constants. This means all backends that take HLO as input then need to make sure they handle these constants in all cases. Missing a case would blow up the constant usage. So I think the suggestion is to not materialize large constants by default. \r\n\r\nIs there a reason to have this is a folding pattern that is always run instead of adding it as a pattern and pulling it into a separate pass?\r\n\r\nFWIW, the bug in IREE is sort of unrelated. This pattern ended up generating an IR pattern that was not covered in one of IREEs passes which led to the test failure.  This can fixed fairly easily (one-liner), but I think the concern with materializing large constants by default is what is being raised.", "> > > > But a constant + broadcast_in_dim also materializes the large constant?\r\n> > > \r\n> > > \r\n> > > Do you mean that it would materialize it at runtime if interpreted in a straightforward manner?\r\n> > \r\n> > \r\n> > The output of the broadcast would be that large constant tensor - right? - unless the broadcast is fused into another op, in which case the constant generating op could also be fused in just as easily. How does 0-d const op + broadcast lead to a higher buffer requirement than an n-d const op?\r\n> \r\n> I agree that small constant + broadcast materialized the same large tensor as fused constant. I think the point being made is that eventually constants are materialized and unless care is taken while materializing constants you can end up with large constants. This means all backends that take HLO as input then need to make sure they handle these constants in all cases. Missing a case would blow up the constant usage. So I think the suggestion is to not materialize large constants by default.\r\n\r\nPerhaps we are interpreting terminology in different ways. By materializing, you mean storing into a buffer, right? A 0-d const + broadcast will use a storage that is one element more than an n-d constant FWIW (+ 1 op less in the IR and no indirection for the const operand). How do you avoid materializing in the former case. With a simple no-opt scenario, the latter is better. My assumption earlier was that an n-d const op is *always* better than a 0-d const + broadcast - either if you don't optimize or if you optimize (in which case it's easier to optimize/rewrite with n-d const + another op). \r\n\r\n> \r\n> Is there a reason to have this is a folding pattern that is always run instead of adding it as a pattern and pulling it into a separate pass?\r\n\r\nSeparate question: which existing test pass could this fit into? We could of course add it as an optional pattern. Or do we just add this pattern in a dedicate pass registered from the same file as say hlo_legalize_to_lhlo? But I'd like to have clarity on why this folding isn't always the best thing to do.\r\n\r\n> \r\n> FWIW, the bug in IREE is sort of unrelated. This pattern ended up generating an IR pattern that was not covered in one of IREEs passes which led to the test failure. This can fixed fairly easily (one-liner), but I think the concern with materializing large constants by default is what is being raised.\r\n\r\nThis is the part I'm still trying to understand. In both cases, you would be materializing the constant. If you aren't materializing in the `broadcast` case, it would appear you are doing something to the `broadcast` and not doing something simpler with the `constant` op.\r\n\r\n", "> The output of the broadcast would be that large constant tensor - right? \r\n\r\nOnly if you run the IR op by op... A compiler (like XLA could generate code with accessors into the \"small\" buffer).\r\n\r\n(Otherwise I agree that the \"splat\" case seems like safe to fold as a subset of the general constant folding)", "> > The output of the broadcast would be that large constant tensor - right?\r\n> \r\n> Only if you run the IR op by op... A compiler (like XLA could generate code with accessors into the \"small\" buffer).\r\n\r\nBut it won't do the same when the operand is a constant tensor? This goes back to my earlier point - is something simpler missing for operands that are constants that exists for operands that are the result of a broadcast?", "> > > The output of the broadcast would be that large constant tensor - right?\r\n> > \r\n> > \r\n> > Only if you run the IR op by op... A compiler (like XLA could generate code with accessors into the \"small\" buffer).\r\n> \r\n> But it won't do the same when the operand is a constant tensor? This goes back to my earlier point - is something simpler missing for operands that are constants that exists for operands that are the result of a broadcast?\r\n\r\nThe question is how much of HLO a lowering needs to support. From the perspective of just HLO, this is a valid canonicalization, as it replaces an alternative IR encoding of a splat constant with just that. The problem, as I see it, is that the lowering is incomplete and does not support this part of HLO. It would also fail if I just wrote the corresponding input. \r\n\r\nWhat are HLO constants lowered to when lowering HLO to linalg? As there is not linalg constant, I assume to standard. Then linalg should learn how to deal with splat constants.\r\n\r\nOr we disallow splat constants in HLO (if that can be done?). \r\n\r\n@nicolasvasilache what is your take on this?", "> But it won't do the same when the operand is a constant tensor? This goes back to my earlier point - is something simpler missing for operands that are constants that exists for operands that are the result of a broadcast?\r\n\r\nI don't quite understand the question here?", "Hey Uday, sorry for the confusion! Let me try to explain in details so hopefully this helps to resolve your questions. :)\r\n \r\n> Perhaps we are interpreting terminology in different ways. By materializing, you mean storing into a buffer, right? A 0-d const + broadcast will use a storage that is one element more than an n-d constant FWIW (+ 1 op less in the IR and no indirection for the const operand). How do you avoid materializing in the former case. With a simple no-opt scenario, the latter is better. My assumption earlier was that an n-d const op is always better than a 0-d const + broadcast - either if you don't optimize or if you optimize (in which case it's easier to optimize/rewrite with n-d const + another op).\r\n\r\nLet's consider the case where we have a large graph and we want to carve out different components of it to run on different accelerators. (I think this is the general direction we are heading for with the compiler-based holistic approach.) Then each component/subgraph/dispatch-region/whatever will be isolated out from the main graph and all the tensors crossing the boundary need to be materialized and to be backed by some buffers. If we have the original scalar constant and broadcast, I can cut the boundary just before the broadcast so the broadcast is put inside the dispatch-region. Then I just need to pass a single constant in. This is typically very cheap and we have quite some flexibility as to whether to sink the constant itself to the dispatch region or use some special hardware memory type or just use normal buffer or whatever. And we can even analyze different such isolated dispatch regions and may find that actually they are all the same and just starting with a different constant. Then we can deduplicate the dispatch regions and only keep one. Now if we constant fold the broadcast, all the flexibility are lost, unless one does analysis essentially to redo the constant folding and re-discover the original scalar. And constant folding has cascading effect: after this one kicked in, we might see lots of additional constant folding happening and that can create more and more large constants. These large constants, if to be backed by buffers, all need to cross the boundary. Because we need to match ABI (as you cannot just send in one scalar to GPU for example and claiming its size is 512x128x...; the ABI is mismatching there), it can potentially mean large amount of data sending across some bus. So this is why we think it's better to avoid aggressively constant folding broadcasts and it's better to leave the choice to each use cases.", "Related to this, Ben has a very nice summary of performance issues we've hit inside IREE due to broadcasts: https://github.com/google/iree/issues/1583. I think the general idea behind is to defer broadcast as much as possible to avoid the unnecessary cost. ", "> Separate question: which existing test pass could this fit into? We could of course add it as an optional pattern. Or do we just add this pattern in a dedicate pass registered from the same file as say hlo_legalize_to_lhlo?\r\n\r\nAdding a dedicated test pass sounds reasonable to me and we can at the same time expose a `populate*` function for picking the patterns. But I guess others might have better suggestions regarding code placement.", "@antiagainst : it isn't clear to me how your argument applies to splat though (which the current patch is limited to right?)", "> > > > The output of the broadcast would be that large constant tensor - right?\r\n> > > \r\n> > > \r\n> > > Only if you run the IR op by op... A compiler (like XLA could generate code with accessors into the \"small\" buffer).\r\n> > \r\n> > \r\n> > But it won't do the same when the operand is a constant tensor? This goes back to my earlier point - is something simpler missing for operands that are constants that exists for operands that are the result of a broadcast?\r\n> \r\n> The question is how much of HLO a lowering needs to support. From the perspective of just HLO, this is a valid canonicalization, as it replaces an alternative IR encoding of a splat constant with just that. The problem, as I see it, is that the lowering is incomplete and does not support this part of HLO. It would also fail if I just wrote the corresponding input.\r\n> \r\n> What are HLO constants lowered to when lowering HLO to linalg? As there is not linalg constant, I assume to standard. Then linalg should learn how to deal with splat constants.\r\n> \r\n\r\nYou can lower a HLO constant (and std.const)  which are splat constants to a linalg.generic with no inputs and the yield returning the constant value. Thats what we did in IREE for a while, but there was some concern about a 0-input linalg.generic op (I am actually not sure why and it worked in practice). For now we rely on fusion of linalg.generic/linalg.indexed_generic with std.const [here](https://github.com/llvm/llvm-project/blob/4ce56b8122219f7b79d75d33184e3ec890a6e222/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L855) to handle splat constants. We didnt need to do anything more, but adding a lowering from HLO const/std.const for splat constants to linalg.generic would be great (and would also fix the issue at hand).\r\n\r\n> Or we disallow splat constants in HLO (if that can be done?).\r\n> \r\n> @nicolasvasilache what is your take on this?\r\n\r\n", "> > > > > The output of the broadcast would be that large constant tensor - right?\r\n> > > > \r\n> > > > \r\n> > > > Only if you run the IR op by op... A compiler (like XLA could generate code with accessors into the \"small\" buffer).\r\n> > > \r\n> > > \r\n> > > But it won't do the same when the operand is a constant tensor? This goes back to my earlier point - is something simpler missing for operands that are constants that exists for operands that are the result of a broadcast?\r\n> > \r\n> > \r\n> > The question is how much of HLO a lowering needs to support. From the perspective of just HLO, this is a valid canonicalization, as it replaces an alternative IR encoding of a splat constant with just that. The problem, as I see it, is that the lowering is incomplete and does not support this part of HLO. It would also fail if I just wrote the corresponding input.\r\n> > What are HLO constants lowered to when lowering HLO to linalg? As there is not linalg constant, I assume to standard. Then linalg should learn how to deal with splat constants.\r\n> \r\n> You can lower a HLO constant (and std.const) which are splat constants to a linalg.generic with no inputs and the yield returning the constant value. Thats what we did in IREE for a while, but there was some concern about a 0-input linalg.generic op (I am actually not sure why and it worked in practice). For now we rely on fusion of linalg.generic/linalg.indexed_generic with std.const [here](https://github.com/llvm/llvm-project/blob/4ce56b8122219f7b79d75d33184e3ec890a6e222/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L855) to handle splat constants. We didnt need to do anything more, but adding a lowering from HLO const/std.const for splat constants to linalg.generic would be great (and would also fix the issue at hand).\r\n> \r\n\r\nI already have a lowering for this (LHLO -> `linalg.fill` for splat constants) that I was planning to submit. So, either std.const or `hlo.const` would lower to `lhlo.const`, which lowers to linalg.fill. (Any reason you prefer `linalg.generic`?) Recall that the `std.const` to `lhlo.const` lowering PR has been dangling [here](https://github.com/tensorflow/tensorflow/pull/40120 ) for a lack of consensus on where it should live (I had proposed including it in the HLO to LHLO lowering.  )\r\n\r\n", "> @antiagainst : it isn't clear to me how your argument applies to splat though (which the current patch is limited to right?)\r\n\r\nYes, this patch is only for splats. @antiagainst, thanks very much for all the detail. Clearly, there are several lower level issues and interactions, but I think I have the same question as @joker-eph. \r\n\r\nThe whole point of canonicalization is to avoid having the downstream look at and deal with *multiple* things. If you allow both `0-d const + broadcast`  and `n-d const`, other rewrite patterns (like multiplication with an all zero tensor) will no longer see a constant tensor operand but the result of a broadcast, and as such, will either not be as effective in what they are doing or entail additional code to deal with the possibilities. Canonicalization, as you know, is to exactly avoid these possibilities by having a single predictable form. One could of course rewrite it to `0-d const + broadcast` later if it's better for some reason. \r\n\r\n(Separately from the above, why are constants (even the single scalar constant) being passed across boundaries? One already knows they are constants and so they can be pulled in (replicated potentially) - I know one can still choose to not do that, but that's the price to pay for not having done that. )", "> > > > > > The output of the broadcast would be that large constant tensor - right?\r\n> > > > > \r\n> > > > > \r\n> > > > > Only if you run the IR op by op... A compiler (like XLA could generate code with accessors into the \"small\" buffer).\r\n> > > > \r\n> > > > \r\n> > > > But it won't do the same when the operand is a constant tensor? This goes back to my earlier point - is something simpler missing for operands that are constants that exists for operands that are the result of a broadcast?\r\n> > > \r\n> > > \r\n> > > The question is how much of HLO a lowering needs to support. From the perspective of just HLO, this is a valid canonicalization, as it replaces an alternative IR encoding of a splat constant with just that. The problem, as I see it, is that the lowering is incomplete and does not support this part of HLO. It would also fail if I just wrote the corresponding input.\r\n> > > What are HLO constants lowered to when lowering HLO to linalg? As there is not linalg constant, I assume to standard. Then linalg should learn how to deal with splat constants.\r\n> > \r\n> > \r\n> > You can lower a HLO constant (and std.const) which are splat constants to a linalg.generic with no inputs and the yield returning the constant value. Thats what we did in IREE for a while, but there was some concern about a 0-input linalg.generic op (I am actually not sure why and it worked in practice). For now we rely on fusion of linalg.generic/linalg.indexed_generic with std.const [here](https://github.com/llvm/llvm-project/blob/4ce56b8122219f7b79d75d33184e3ec890a6e222/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L855) to handle splat constants. We didnt need to do anything more, but adding a lowering from HLO const/std.const for splat constants to linalg.generic would be great (and would also fix the issue at hand).\r\n> \r\n> I already have a lowering for this (LHLO -> `linalg.fill` for splat constants) that I was planning to submit. So, either std.const or `hlo.const` would lower to `lhlo.const`, which lowers to linalg.fill. (Any reason you prefer `linalg.generic`?) Recall that the `std.const` to `lhlo.const` lowering PR has been dangling [here](https://github.com/tensorflow/tensorflow/pull/40120) for a lack of consensus on where it should live (I had proposed including it in the HLO to LHLO lowering. )\r\n\r\nIn IREE we go from HLO to Linalg on tensors. linalg.fill doesnt operate on tensors. IREE doesnt use LHLO. Lowering from HLO/Std consts to linalg.generic on tensors will allow the fusion on tensors pass to fuse everything seemlessly. ", "I just sync'd with several people offline, and we all made the same mis-read of this PR:\r\n\r\n> dense<[...]> is DenseElementsAttr\r\n> dense<0.0> is SplatElementsAttr\r\n\r\nGiven that a number of people made this mistake, we should probably lobby separately to consider some clearer syntax here that better reveals the structure. Also, since SplatElementsAttr extends DenseElementsAttr, we're somewhat concerned that a lack of care on related folders are easy to write the wrong way (and this is made worse by the spelling subtlety). However, none of that should related to this PR, which is sound. Really sorry for the tangent.", "We're pushing a local fix to the original bug and then will trigger submission of this PR. Stand by.", "> I just sync'd with several people offline, and we all made the same mis-read of this PR:\r\n> \r\n> > dense<[...]> is DenseElementsAttr\r\n> > dense<0.0> is SplatElementsAttr\r\n> \r\n> Given that a number of people made this mistake, we should probably lobby separately to consider some clearer syntax here that better reveals the structure. Also, since SplatElementsAttr extends DenseElementsAttr, we're somewhat concerned that a lack of care on related folders are easy to write the wrong way (and this is made worse by the spelling subtlety). However, none of that should related to this PR, which is sound. Really sorry for the tangent.\r\n\r\nThanks @stellaraccident - no worries; it's good to be aware of the interactions and the impact this has downstream.  I should perhaps update the commit title to reflect 'splat' but the summary already has it. I can add a few more lines to the summary.", "This landed in #40745. "]}, {"number": 40744, "title": "[MLIR] Add tf.VarIsInitializedOp op and its canonicalization pattern", "body": "Add tf.VarIsInitializedOp op to the MLIR TF dialect along with a\r\ncanonicalization to erase it when its result is unused. This op has side\r\neffects on resources but can still be erased if its result is unused.\r\n\r\nSigned-off-by: Uday Bondhugula <uday@polymagelabs.com>", "comments": ["@jpienaar for visibility."]}, {"number": 40743, "title": "keras.Model using EmbeddingColumn with pre-trained ckpt fail to be reconstructed without the original ckpt", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pre-installed in Colab\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n\r\n**Describe the current behavior**\r\n`keras.Model` using `EmbeddingColumn` with pre-trained checkpoint in a `DenseFeatures` layer, after exported to `SavedModel`, cannot be reconstructed without the original embedding checkpoint.\r\n\r\n**Describe the expected behavior**\r\n`SavedModel` exported from `keras.Model` should be fully self-contained and be able to be reconstructed from solely the saved model directory for the sake of portability.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport shutil\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# build and checkpoint mock pre-trained embeddings\r\nEMBD_INPUT_DIM = 1000\r\nEMBD_OUTPUT_DIM = 64\r\n\r\nmock_pretrained_embd = tf.Variable(tf.initializers.GlorotNormal()(shape=(EMBD_INPUT_DIM, EMBD_OUTPUT_DIM)), trainable=True)\r\n\r\nckpt = tf.train.Checkpoint(embeddings=mock_pretrained_embd)\r\nckpt.write('ckpt/mock_embd_ckpt')\r\n\r\n# build keras.Model using EmbeddingColumn in DenseFeatures layer\r\nembd_column = tf.feature_column.embedding_column(\r\n    tf.feature_column.categorical_column_with_identity(key='id', num_buckets=EMBD_INPUT_DIM),\r\n    dimension=EMBD_OUTPUT_DIM,\r\n    ckpt_to_load_from='ckpt/mock_embd_ckpt',\r\n    tensor_name_in_ckpt='embeddings/.ATTRIBUTES/VARIABLE_VALUE',\r\n    trainable=True\r\n)\r\nfeatures_layer = tf.keras.layers.DenseFeatures([embd_column])\r\n\r\nx = {'id': tf.keras.Input(shape=(None,), dtype=tf.int64, name='id')}\r\ny = features_layer(x)\r\ny = tf.keras.layers.Dense(1, name='out')(y)\r\nmodel = tf.keras.Model(inputs=x, outputs=y)\r\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam())\r\n\r\n# fit model on mock data to have embeddings updated\r\nTRAIN_BATCH = 64\r\n\r\nid_len = np.random.randint(low=0, high=15)\r\nx = {'id': tf.convert_to_tensor(np.random.randint(low=0, high=EMBD_OUTPUT_DIM+1, size=(TRAIN_BATCH, id_len)))}\r\ny = tf.convert_to_tensor(np.random.randint(low=0, high=2, size=(TRAIN_BATCH, 1)), tf.float32)\r\n\r\nfor i in range(100):\r\n  model.train_on_batch(x=x, y=y)\r\n\r\n# export model\r\nmodel.save('mock_model')\r\n\r\n# re-construct keras.Model & SavedModel and invoke on mock data\r\nTEST_INPUT_BATCH = 10\r\nTEST_INPUT_ID_LEN = 5\r\n\r\ntest_inputs = {'id': tf.random.uniform((TEST_INPUT_BATCH, TEST_INPUT_ID_LEN), minval=0, maxval=EMBD_INPUT_DIM, dtype=tf.int64)}\r\n\r\nloaded_keras_model = tf.keras.models.load_model('mock_model')\r\nloaded_keras_model(test_inputs)\r\n\r\nloaded_model = tf.saved_model.load('mock_model')\r\nserving_default_fn = loaded_model.signatures['serving_default']\r\nserving_default_fn(**test_inputs)\r\n\r\n# remove original embeddings checkpoint and try invoking on mock data again\r\nshutil.rmtree('ckpt')\r\n\r\nloaded_keras_model(test_inputs)\r\nserving_default_fn(**test_inputs)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-217-d8ce0286d803> in <module>()\r\n     58 shutil.rmtree('ckpt')\r\n     59 \r\n---> 60 loaded_keras_model(test_inputs)\r\n     61 serving_default_fn(**test_inputs)\r\n\r\n15 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    966           with base_layer_utils.autocast_context_manager(\r\n    967               self._compute_dtype):\r\n--> 968             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    969           self._handle_activity_regularization(inputs, outputs)\r\n    970           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    717     return self._run_internal_graph(\r\n    718         inputs, training=training, mask=mask,\r\n--> 719         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    720 \r\n    721   def compute_output_shape(self, input_shape):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\r\n    886 \r\n    887           # Compute outputs.\r\n--> 888           output_tensors = layer(computed_tensors, **kwargs)\r\n    889 \r\n    890           # Update tensor_dict.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    966           with base_layer_utils.autocast_context_manager(\r\n    967               self._compute_dtype):\r\n--> 968             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    969           self._handle_activity_regularization(inputs, outputs)\r\n    970           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/dense_features.py in call(self, features, cols_to_output_tensors)\r\n    143       with ops.name_scope(column.name):\r\n    144         tensor = column.get_dense_tensor(transformation_cache,\r\n--> 145                                          self._state_manager)\r\n    146         processed_tensors = self._process_dense_tensor(column, tensor)\r\n    147         if cols_to_output_tensors is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py in get_dense_tensor(self, transformation_cache, state_manager)\r\n   3275     sparse_tensors = self.categorical_column.get_sparse_tensors(\r\n   3276         transformation_cache, state_manager)\r\n-> 3277     return self._get_dense_tensor_internal(sparse_tensors, state_manager)\r\n   3278 \r\n   3279   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py in _get_dense_tensor_internal(self, sparse_tensors, state_manager)\r\n   3228         self, name='embedding_weights')\r\n   3229     return self._get_dense_tensor_internal_helper(sparse_tensors,\r\n-> 3230                                                   embedding_weights)\r\n   3231 \r\n   3232   def _old_get_dense_tensor_internal(self, sparse_tensors, weight_collections,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py in _get_dense_tensor_internal_helper(self, sparse_tensors, embedding_weights)\r\n   3205         to_restore = to_restore._get_variable_list()  # pylint: disable=protected-access\r\n   3206       checkpoint_utils.init_from_checkpoint(self.ckpt_to_load_from, {\r\n-> 3207           self.tensor_name_in_ckpt: to_restore\r\n   3208       })\r\n   3209 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_utils.py in init_from_checkpoint(ckpt_dir_or_file, assignment_map)\r\n    290   else:\r\n    291     distribution_strategy_context.get_replica_context().merge_call(\r\n--> 292         init_from_checkpoint_fn)\r\n    293 \r\n    294 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in merge_call(self, merge_fn, args, kwargs)\r\n   2418     merge_fn = autograph.tf_convert(\r\n   2419         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\r\n-> 2420     return self._merge_call(merge_fn, args, kwargs)\r\n   2421 \r\n   2422   def _merge_call(self, merge_fn, args, kwargs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)\r\n   2425         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\r\n   2426     try:\r\n-> 2427       return merge_fn(self._strategy, *args, **kwargs)\r\n   2428     finally:\r\n   2429       _pop_per_thread_mode()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    280   def wrapper(*args, **kwargs):\r\n    281     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\r\n--> 282       return func(*args, **kwargs)\r\n    283 \r\n    284   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_utils.py in <lambda>(_)\r\n    285   \"\"\"\r\n    286   init_from_checkpoint_fn = lambda _: _init_from_checkpoint(\r\n--> 287       ckpt_dir_or_file, assignment_map)\r\n    288   if distribution_strategy_context.get_cross_replica_context():\r\n    289     init_from_checkpoint_fn(None)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_utils.py in _init_from_checkpoint(ckpt_dir_or_file, assignment_map)\r\n    296   \"\"\"See `init_from_checkpoint` for documentation.\"\"\"\r\n    297   ckpt_file = _get_checkpoint_filename(ckpt_dir_or_file)\r\n--> 298   reader = load_checkpoint(ckpt_dir_or_file)\r\n    299   variable_map = reader.get_variable_to_shape_map()\r\n    300   for tensor_name_in_ckpt, current_var_or_name in sorted(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_utils.py in load_checkpoint(ckpt_dir_or_file)\r\n     65     raise ValueError(\"Couldn't find 'checkpoint' file or checkpoints in \"\r\n     66                      \"given directory %s\" % ckpt_dir_or_file)\r\n---> 67   return py_checkpoint_reader.NewCheckpointReader(filename)\r\n     68 \r\n     69 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/py_checkpoint_reader.py in NewCheckpointReader(filepattern)\r\n     93   \"\"\"\r\n     94   try:\r\n---> 95     return CheckpointReader(compat.as_bytes(filepattern))\r\n     96   # TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\r\n     97   # issue with throwing python exceptions from C++.\r\n\r\nValueError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ckpt/mock_embd_ckpt: Not found: ckpt; No such file or directory\r\n```", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/51ff32c4eee33a56e8d393ee03b42f1a/40743.ipynb). Thanks!", "Please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/6082#issuecomment-265055615).\r\n\r\nwhen you tried to restore, use the full relative path rather than the absolute path. Thanks!", "@gowthamkpr Thanks for the info! \r\nJust to clarify a bit though, the real issue is: the model can only be re-created with BOTH the pre-trained embedding checkpoint and the SavedModel files(which should have internalized the embedding weights). This is inconvenient for the sake of portability/serving.", "Was able to reproduce the issue using TF 2.7.0. Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1818389a675c47fed66323d4769fab68/untitled85.ipynb).Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40743\">No</a>\n"]}, {"number": 40742, "title": "compilation of rule '//tensorflow/python:bfloat16_lib' failed when building TF2.2 from source", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):2.0.0\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory: 12\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\nI receive an error when build TF2.2 from source. Here's the error message:\r\n\r\n> tensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\n> tensorflow/python/lib/core/bfloat16.cc:655:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n>                        compare_types)) {\r\n>                                     ^\r\n> tensorflow/python/lib/core/bfloat16.cc:610:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n>                              const std::array<int, 3>& types) {\r\n>                                                             ^\r\n> tensorflow/python/lib/core/bfloat16.cc:610:60: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}\u2019\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> ERROR: /home/aptx4869/github/tensorflow/tensorflow/python/tools/BUILD:281:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)\r\n> INFO: Elapsed time: 20.016s, Critical Path: 7.56s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully\r\n\r\nI've taken advice from [this issue](https://github.com/tensorflow/tensorflow/issues/40688?notification_referrer_id=MDE4Ok5vdGlmaWNhdGlvblRocmVhZDk0OTk2NDQwMToxMDE4NDE1Mw%3D%3D#issuecomment-648539630) to downgrade numpy to 1.18.5 but it does not work for me.", "comments": ["@xlnwel Can you please try again. I think the above PR might have resolved your issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks. I finally got it worked", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40742\">No</a>\n", "Commenting to save people my head ache, remember to run `hazel clean` after downgrading numpy", "> \r\n> \r\n> Thanks. I finally got it worked\r\n\r\nCould you please tell the method you solved the problem?", "> after downgrading numpy\r\n\r\nSorry..i don't know how to use 'hazel clean'", "> Commenting to save people my head ache, remember to run `hazel clean` after downgrading numpy\r\n\r\nThank you so much!! Indeed saved me a lot of headache =)! Came here from https://github.com/tensorflow/tensorflow/issues/40688, also had the bfloat16 issue after 20 mins of compiling ... !\r\n", "> > after downgrading numpy\r\n> \r\n> Sorry..i don't know how to use 'hazel clean'\r\n\r\nHe means run `bazel clean` in terminal after downgrading numpy, this restarts the compilation and bfloat16 issue is fixed. I downgraded to numpy==1.18.5"]}, {"number": 40741, "title": "Sign compare warning fixes batch 1 fix2", "body": "Not ready for merge just yet, 35 files changes expected. Please await these changes. ", "comments": ["@joker-eph The mlir related changes are just copies of what is currently at tensorflow/tensorflow:master, i.e. they are not changes at all. ", "@tg-at-google Can you please resolve conflicts? Thanks!", "> Forgot the other 2 items\r\n\r\nHey, yeah, wasn't done yet. \ud83d\ude05", "> > Forgot the other 2 items\r\n> \r\n> Hey, yeah, wasn't done yet. \ud83d\ude05\r\n\r\nAlright @mihaimaruseac  good to go. "]}, {"number": 40740, "title": "First example in keras guide \"customizing_what_happens_in_fit\" does not work", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example\r\n\r\n## Description of issue (what needs changing):\r\nThe First Example doesn't seem to actually call **train_step()**\r\n\r\n### Clear description\r\nI have run this example using a debugger, and it does not actually get called\r\nI have tested this with version 2.2.0\r\n\r\n### Correct links\r\nI have not been able to find anything that explains how this is supposed to work.\r\n\r\n### Parameters defined\r\nN/A\r\n\r\n### Returns defined\r\n\r\nN/A\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nN/A\r\n\r\nIs there a usage example?\r\n\r\nN/A\r\n\r\n### Request visuals, if applicable\r\n\r\nN/A\r\n### Submit a pull request?\r\n\r\nI can't tell if this should work on not.\r\n", "comments": ["@pneumoman \r\n\r\nI have tried executing tutorial in colab with TF 2.2 and i am not seeing any issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/c6436f8bf6d8f20b3ae53b2713a67ff9/untitled62.ipynb).Can you please help me to understand where exactly you are facing the issue.If possible please share colab link or simple standalone code to reproduce the issue.It helps us in localizing the issue faster.Thanks!", "You can't see what I'm talking about with just the standard collab page since it is just calling the normal loss function anyway. so I added some print statements to prove to myself that something was indeed being called.  And indeed it was however not as I expected, since it appears to have been called twice before the actual training and then no more, so I guess you need to be careful what kind of operations you put in there.\r\nI found my problem, even though pip was reporting 2.2.0 for tensorflow, there was a reference to tensorflow-base==2.10, which after several install/uninstall go rounds, I was able to get it to reproduce the expected? output"]}, {"number": 40739, "title": "[CherryPick r2.3] Add the \"--define=no_tensorflow_py_deps=true\" flag for the windows cp\u2026", "body": "\u2026u release\r\n\r\nbuilds.\r\n\r\nPiperOrigin-RevId: 317968971\r\nChange-Id: I7d4db21474d85620928f3a5ffb1e4cfebaa2be9f", "comments": []}, {"number": 40738, "title": "Convergence issue across Keras 2.2.4 with TF 1.14.0 and Keras 2.3.1 with TF 2.2.0", "body": "**System information**\r\n- I am using the Keras functional API directly (not through tf.keras) \r\n- OS Platform and Distribution: Ubuntu 19.10 with Kernel 5.3.0-59-generic\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: It is on PC (HP laptop)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7 (default, May  7 2020, 21:25:33)\r\n- Bazel version (if compiling from source): Not compiling from source\r\n- GCC/Compiler version (if compiling from source): Not compiling from source but GCC 7.3.0\r\n- CUDA/cuDNN version: Not using GPU\r\n- GPU model and memory: Not using GPU\r\n\r\n**Describe the current behavior**\r\nI have a code that converges on an older version of Keras (2.2.4) while it does not converge on a more recent version (2.3.1).\r\n\r\n**Describe the expected behavior**\r\nConvergence is expected to be the same (or at least nearly the same). I expect the validation loss to be approximately the same across different versions of Keras and Tensorflow with the same parameters, model, data, etc.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://drive.google.com/file/d/1P6SrCS8HFw2fPYvwrmEuK9tyf6-e5Biv/view?usp=sharing\r\n\r\n**PS:**\r\n- I downgraded the Tensorflow on Keras 2.3.1 to TF 1.14.0 so that it would be the same as what it is on my older Keras but did not help with the converges but helped with the speed of training (most probably has to do something with the Eager execution).\r\n- I also tried increasing the epoch, despite the fact that it gets slowly close to the desired loss, still does not get to the same expected loss\r\n\r\nIn my opinion, there is either a difference in default values across versions. Looking at the API of the methods/functions that I have used from Keras library I do not notice any difference between the default values so it must be something more serious going on.", "comments": ["Was able to reproduce the issue, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/04e89a2dc4e2d808618d77ca3907f606/40738.ipynb). Thanks!", "Please take a look at this [issue1](https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1) and also this [issue](https://github.com/tensorflow/tensorflow/issues/33487) let me know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!"]}, {"number": 40737, "title": "[CherryPick 2.3] Skip data loading error in multi_worker_tutorial_test (the test does not aim to cover this).", "body": "PiperOrigin-RevId: 317797271\nChange-Id: I8336d7ffeda0836beef0a2d04e633614a44e7fa4", "comments": []}, {"number": 40736, "title": "TF-TRT Slice op converters explicit batch mode", "body": "This PR enables Slice and StridedSlice op conversion in explicit batch mode.\r\n\r\nWe currently enable the conversion only for the cases where the slice parameters (begin, size) can be calculated at conversion time. For other cases, we would need to use shape layers to calculate the slice parameters at runtime. We will implement this in a follow up PR. \r\n\r\nThe StridedSlice converter was improved to accept negative stride, and corresponding tests were added.\r\n\r\nTagging @bixia1 for review. ", "comments": ["@tfeher Can you please resolve conflicts? Thanks!", "@tfeher  Any update on this PR? Please. Thanks!", "This PR depends on #40545, I will resolve the conflicts once #40545 is merged.", "@tfeher Can you please check @DEKHTIARJonathan's comments and keep us posted ? Thanks!", "@tfeher Any update on this PR? Please. Thanks!", "This PR depends on #40545, I will resolve the conflicts once #40545 is merged.", "PR rebased to contain the latest changes fro #40545 and #47215. Marking it as draft, because the dynamic shape input handling needs some improvement."]}, {"number": 40735, "title": "NCCL is not supported when using virtual GPUs, fallingback to reduction to one device", "body": "Hello,\r\n\r\nI tried to run colab notebook present here: https://www.tensorflow.org/guide/gpu.\r\n\r\nWhen I try to use virtual GPU I get NCCL not supported error. can you help on how do I test virtual GPU's in colab?", "comments": ["@asm582 \r\nPlease select \"Runtime tab\" and option \"change runtime\" in this option select \"GPU\".\r\nto confirm, run:\r\n\r\n'''\r\n_lmport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))_ '''\r\n\r\nCan you please refer to [this link](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) and let me know if it helps resolve your issue.\r\n[link](https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc). [link2](https://jovianlin.io/pytorch-with-gpu-in-google-colab/)\r\n\r\n", "Hi @asm582, was that an error message or a warning? From the message you've shared, I would guess that you're trying to run multiworker training on virtual GPUs in colab. When using virtual GPUs in colab, NCCL is not supported, so as the message suggests [tf.distribute.ReductionToOneDevice](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice) will be used instead to aggregate the gradients. Happy to help more if you provide additional information on what you're trying to do and where specifically you saw this message.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@nikitamaia What does it mean for GPU to be virtual and how to make it non-virtual? I have access to the bare metal of a multi-GPU node, operate on \"PhysicalDevices\" allocating one process per GPU with non-adaptive memory and keep seeing the same warning message.", "More on virtual GPUs here https://www.tensorflow.org/guide/gpu#manual_device_placement\r\nFeel free to open a new issue if more support is needed.", "I am still strictly after the warning mentioned in this issue. Docs you pointed to seem to suggest that one needs to explicitly setup Virtual GPUs for them to pop up, whereas I have never done such a thing and still seeing \"NCCL is not supported when using virtual GPUs...\" when using simplest data paralllelism.", "@nikitamaia is tf.distribute.ReductionToOneDevice less efficient than NCCL? Or what exactly are the differences that using these two reduction methods bring on performance? ", "@animesh0794, if you have NCCL on your system then that is the best option for performance. `ReductionToOneDevice` is a good option when you do not have NCCL on your system.", "Hey Nikita,\n\nThanks, for replying!\nI am using AWS Sagemaker and I am not sure how can I check if NCCL is\ninstalled on that virtual machine.\n\nThanks,\nAnimesh\n\nOn Tue, Nov 24, 2020 at 6:04 AM Nikita Namjoshi <notifications@github.com>\nwrote:\n\n> @animesh0794 <https://github.com/animesh0794>, if you have NCCL on your\n> system then that is the best option for performance. ReductionToOneDevice\n> is a good option when you do not have NCCL on your system.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40735#issuecomment-732506612>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQVXDVJRTJKI73CJ4VXHYH3SRL5P7ANCNFSM4OGCMZUQ>\n> .\n>\n"]}, {"number": 40734, "title": "Cherry-pick sqlite version bump", "body": "PiperOrigin-RevId: 317934381\r\nChange-Id: I95cdf789f7f5a89d75d45a1b6d67f1ad993cafab\r\n\r\nHandles [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327), [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655), [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656), [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434), [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435), [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630), [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631) and [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871).", "comments": []}, {"number": 40733, "title": "Cherry-pick sqlite version bump", "body": "PiperOrigin-RevId: 317934381\r\nChange-Id: I95cdf789f7f5a89d75d45a1b6d67f1ad993cafab\r\n\r\nHandles [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327), [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655), [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656), [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434), [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435), [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630), [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631) and [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871).", "comments": ["Last commit handles  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358)."]}, {"number": 40732, "title": "Cherry-pick sqlite version bump", "body": "PiperOrigin-RevId: 317934381\r\nChange-Id: I95cdf789f7f5a89d75d45a1b6d67f1ad993cafab\r\n\r\nHandles [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327), [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655), [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656), [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434), [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435), [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630), [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631) and [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871).", "comments": ["Last commit handles  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358)."]}, {"number": 40731, "title": "Cherry-pick sqlite version bump", "body": "PiperOrigin-RevId: 317934381\r\nChange-Id: I95cdf789f7f5a89d75d45a1b6d67f1ad993cafab\r\n\r\nHandles [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327), [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655), [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656), [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434), [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435), [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630), [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631) and [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871).", "comments": ["Last commit handles  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358)."]}, {"number": 40730, "title": "Cherry-pick sqlite version bump", "body": "PiperOrigin-RevId: 317934381\r\nChange-Id: I95cdf789f7f5a89d75d45a1b6d67f1ad993cafab\r\n\r\nHandles [CVE-2020-9327](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-9327), [CVE-2020-11655](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11655), [CVE-2020-11656](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11656), [CVE-2020-13434](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13434), [CVE-2020-13435](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13435), [CVE-2020-13630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13630), [CVE-2020-13631](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13631) and [CVE-2020-13871](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-13871).", "comments": ["Last commit handles  [CVE-2020-15358](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15358)."]}, {"number": 40729, "title": "Unexpected difference in scatter_nd_update variants", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (subsystem on Windows)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nThis problem (?) was observed for a [unit-test in the RL framework Tensorforce](https://github.com/tensorforce/tensorforce/blob/tf2/test/test_saving.py#L226) when using `tf.saved_model`. When using `variable.scatter_nd_update(indices, updates)`, I get the exception below. When instead using `value = tf.tensor_scatter_nd_update(variable, indices, updates); variable.assign(value)`, everything works well. The assignment happens [here](https://github.com/tensorforce/tensorforce/blob/tf2/tensorforce/core/models/tensorforce.py#L1012).\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect that both functions use the same implementation of the `scatter_nd_update` functionality, so I don't understand why one works and the other doesn't.\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"python3.6/site-packages/tensorflow/python/framework/importer.py\", line 497, in _import_graph_def_internal\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The inner -2 dimensions of input.shape=[] must match the inner 1 dimensions of updates.shape=[?,1]: Shapes must be equal rank, but are 0 and 1 for '{{node agent/ResourceScatterNdUpdate}} = ResourceScatterNdUpdate[T=DT_BOOL, Tindices=DT_INT64, _output_shapes=[], use_locking=true](agent_resourcescatterndupdate_ref:0, agent/stack_1:0, args_0:0)' with input shapes: [], [?,2], [?,1].\r\n \r\nDuring handling of the above exception, another exception occurred:\r\n \r\nTraceback (most recent call last):\r\n  Tensorforce/test/test_saving.py\", line 227, in test_explicit\r\n    agent = tf.saved_model.load(export_dir=os.path.join(self.__class__.directory, 'agent-1'))\r\n  File \"python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 578, in load\r\n    return load_internal(export_dir, tags)\r\n  File \"python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 604, in load_internal\r\n    export_dir)\r\n  File \"python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 116, in __init__\r\n    meta_graph.graph_def.library))\r\n  File \"python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 311, in load_function_def_library\r\n    func_graph = function_def_lib.function_def_to_graph(copy)\r\n  File \"python3.6/site-packages/tensorflow/python/framework/function_def_to_graph.py\", line 63, in function_def_to_graph\r\n    importer.import_graph_def_for_function(graph_def, name=\"\")\r\n  File \"python3.6/site-packages/tensorflow/python/framework/importer.py\", line 412, in import_graph_def_for_function\r\n    graph_def, validate_colocation_constraints=False, name=name)\r\n  File \"python3.6/site-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: The inner -2 dimensions of input.shape=[] must match the inner 1 dimensions of updates.shape=[?,1]: Shapes must be equal rank, but are 0 and 1 for '{{node agent/ResourceScatterNdUpdate}} = ResourceScatterNdUpdate[T=DT_BOOL, Tindices=DT_INT64, _output_shapes=[], use_locking=true](agent_resourcescatterndupdate_ref:0, agent/stack_1:0, args_0:0)' with input shapes: [], [?,2], [?,1].\r\n```\r\n", "comments": ["@AlexKuhnle \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40729\">No</a>\n"]}, {"number": 40728, "title": "Const-qulify PyUFuncGenericFunction's arguments", "body": "Due to recent changes in Numpy [1], PyUFuncGenericFunction's argument should be const-qualified.\r\nThis change is non-breaking (i.e. it will work for both numpy 1.18 and 1.19).\r\n\r\n\r\n[1] https://github.com/numpy/numpy/pull/15355\r\n", "comments": ["I just noticed this change [1]. I believe my PR resolves all the breaking changes in the numpy ABI and the commit can be reversed.\r\n\r\n\r\n[1] https://github.com/tensorflow/tensorflow/commit/79518facb4b857af9d9d5df2da463fdbf7eb0e3e", "@jaingaurav Can you please review this PR ? Thanks!", "I'd suggest to include a revert of https://github.com/tensorflow/tensorflow/commit/79518facb4b857af9d9d5df2da463fdbf7eb0e3e in this PR as this resolves the TODO. I hope the TF guys agree.", "> I'd suggest to include a revert of [79518fa](https://github.com/tensorflow/tensorflow/commit/79518facb4b857af9d9d5df2da463fdbf7eb0e3e) in this PR as this resolves the TODO. I hope the TF guys agree.\r\n\r\nI believe it should be reverted, too.\r\nHowever, before adding it to the PR, I'm awaiting for some feedback.\r\nOne other thing that I do not know is whether the old version of numpy should be supported.", ">  One other thing that I do not know is whether the old version of numpy should be supported.\r\n\r\nWhat do you mean by that exactly? -->\r\n\r\n> This change is non-breaking (i.e. it will work for both numpy 1.18 and 1.19).", "> > One other thing that I do not know is whether the old version of numpy should be supported.\r\n> \r\n> What do you mean by that exactly? -->\r\n> \r\n> > This change is non-breaking (i.e. it will work for both numpy 1.18 and 1.19).\r\n\r\nAh, I totally forgot that it's a non-breaking change. My mistake.", "@edudev, @majnemer Any update on this PR? Please. Thanks!", "> @edudev, @majnemer Any update on this PR? Please. Thanks!\r\n\r\nWell, I'm just waiting for a review.\r\n\r\nSomething that I did notice though is that the relevant function type (`PyUFuncGenericFunction`) is also used in the XLA part of tensorflow. For some reason, when I compiled tensorflow with XLA support, it did not hit the 'broken' code (didn't attempt to compile it, at all).\r\nHowever, when I explicitly compiled it (`//tensorflow/compiler/xla/python:bfloat16`), it was in fact unsuccessful, hence why I just updated my pull request to fix this, as well.\r\nI also reverted the change which limited the numpy version to 1.18.\r\n\r\nAnd something that I sadly just noticed: this change is in fact breaking. Once merged, it will not be possible to compile tensorflow with numpy below 1.19. Moreover, I believe that using tensorflow with lower versions of numpy will also be impossible, due to differences in the mangled name of the relevant functions.\r\nWhich is why I also suggest that the minimum version of numpy also be bumped (from 1.16 to 1.19).", "> this change is in fact breaking. Once merged, it will not be possible to compile tensorflow with numpy below 1.19. Moreover, I believe that using tensorflow with lower versions of numpy will also be impossible, due to differences in the mangled name of the relevant functions.\r\n\r\nCan you elaborate why? I mean: those functions call numpy functions and those numpy function either take non-const pointers (<1.19) or const-pointers (1.19+), so both work now. Only thing which will be impossible is to compile TF with <1.19 and use with 1.19+. Or is that why you meant?", "> > this change is in fact breaking. Once merged, it will not be possible to compile tensorflow with numpy below 1.19. Moreover, I believe that using tensorflow with lower versions of numpy will also be impossible, due to differences in the mangled name of the relevant functions.\r\n> \r\n> Can you elaborate why? I mean: those functions call numpy functions and those numpy function either take non-const pointers (<1.19) or const-pointers (1.19+), so both work now. Only thing which will be impossible is to compile TF with <1.19 and use with 1.19+. Or is that why you meant?\r\n\r\nYes, it will not be possible to compile tensorflow with numpy versions below 1.19 due to the change in the function pointer type (from non-const pointer argument to const pointer argument).\r\nI'm not sure why I did not notice this issue before. I believe it may have been due to some caches. I only noticed the compilation issues after running `bazel clean`.\r\n\r\nAbout the runtime issues: had the relevant code (bfloat16) been compiled as a C source, this would not have been an issue. However, in C++ the argument types actually influence the function names.\r\nA simple example:\r\n```\r\n$ c++filt _Z3fooPc\r\nfoo(char*)\r\n$ c++filt _Z3fooPKc\r\nfoo(char const*)\r\n```\r\nNotice how changing the argument from a normal pointer to a constant pointer changes the name of the function, as well (there's an extra `K`).\r\n\r\nNow, coming to the numpy case, it's... complicated... way more complicated, because the offending argument is actually a pointer to a function.", "Hmm, okay, I took a closer look at this once again. As I said, it's quite complicated.\r\n\r\nThe linking issue is at the boundary of tensorflow and numpy. Including headers and having const/non-const is only a compile-time issue, as already mentioned.\r\nThe only boundary where the offending function pointer type change (`PyUFuncGenericFunction`) is used at a library boundary is when calling `PyUFunc_RegisterLoopForType`. This is because `PyUFunc_RegisterLoopForType` takes an argument, which is `PyUFuncGenericFunction`. These calls are a few lines below my changes in the two `bfloat16.cc` source files of tensorflow.\r\n\r\nNormally, when the tensorflow call to `PyUFunc_RegisterLoopForType` gets executed, the (dynamic) linker will search for the relevant function and, as already mentioned, that one (or two) const qualifiers make a hell of a lot difference in the function names.\r\nJust for illustration purposes, let's assume that `PyUFunc_RegisterLoopForType` had the following declaration (I've removed some parameters to make it simpler, but the most important `PyUFuncGenericFunction` is still there):\r\n```\r\nint PyUFunc_RegisterLoopForType(PyUFuncGenericFunction);\r\n```\r\nHere are how the two names of this function would look like (without and with const qualifiers for the parameters of the `PyUFuncGenericFunction` parameter; told you it was complicated):\r\n```\r\n_Z27PyUFunc_RegisterLoopForTypePFvPPcPlS1_PvE\r\n_Z27PyUFunc_RegisterLoopForTypePFvPPcPKlS2_PvE\r\n```\r\n(they are different)\r\n\r\n\r\nOkay, up until now, this pretty much says that we would not be able to use tensorflow with versions of numpy before 1.19.\r\nHowever, I went to also check the definition of `PyUFunc_RegisterLoopForType`, and it becomes even more obfuscated.\r\nThe declaration is located in `__ufunc_api.h` of the numpy source code; and there are actually two declarations.\r\nI'll ignore the `#ifdef`, which branches between the two, and here they are:\r\n```\r\nNPY_NO_EXPORT  int PyUFunc_RegisterLoopForType \\\r\n       (PyUFuncObject *, int, PyUFuncGenericFunction, const int *, void *);\r\n\r\n#define PyUFunc_RegisterLoopForType \\\r\n        (*(int (*)(PyUFuncObject *, int, PyUFuncGenericFunction, const int *, void *)) \\\r\n         PyUFunc_API[2])\r\n```\r\n(these declarations differ from the one I provided above, as it was intentionally made simpler, with less arguments)\r\n\r\nI have not gone through the numpy source code, so I am not exactly sure when each of these two versions is used. My guess would be that the former is used for internal numpy complication, while the latter is used when compiling against external libraries (such as tensorflow).\r\nNonetheless, the definition of the former is not in the C source code, while the definition of the latter is quite simple (I'm pasting again):\r\n```\r\n#define PyUFunc_RegisterLoopForType \\\r\n        (*(int (*)(PyUFuncObject *, int, PyUFuncGenericFunction, const int *, void *)) \\\r\n         PyUFunc_API[2])\r\n```\r\nAnd this is just a simple cast of `PyUFunc_API[2]`, whatever it may be. This cast does not care about whether a parameter of a function pointer parameter is const-qualified or not.\r\n\r\nTherefore, in conclusion, I believe it may not be an issue to use older versions of numpy with tensorflow.\r\nHowever, I'm not sure how to test this, and I have absolutely no idea how to write an automated test for this (the test itself would pretty much require having multiple versions of numpy).", "I didn't follow the discussion in the most recent comment, but it looks to me like it is overcomplicating this.\r\n\r\nI think all you need to do to fix any source compatibility issues with older versions of NumPy is to call `reinterpret_cast<PyUFuncGenericFunction>(fn)` on the function being passed to `PyUFunc_RegisterLoopForType`. This will be a no-op if the const-qualifiers match, and perform a cast otherwise.", "You can ignore my comments on this failing to link due to the C++ name mangling. I just noticed that there's an `extern \"C\"` around the numpy code for `PyUFunc_RegisterLoopForType`.\r\nI see no issues and I believe the pull request is more or less ready (should it be okay that numpy 1.19 be required to compile tensorflow).\r\n\r\nAbout the reinterpret cast: as I was wrong about the C++ function name mangling occurring, it will not be needed.\r\nHowever, assuming that I was not mistaken about this issue, it would still not work. The compiler decides at compile time which of the overloaded functions to call (the one with the const qualifier or the one without). A reinterpret cast would be able to fix the issue of compiling tensorflow with versions of numpy prior to 1.19.\r\nThe issue that I was describing was a potential one at link time, where a reinterpret cast would not change anything.\r\n\r\nSo... my question is: how bad would it be to require numpy 1.19 for tensorflow compilation (but still be able to work alongside old versions during runtime)?", "It seems to me that you are confusing things. I don't see how this change is not compatible with numpy before and after 1.19. Can you point out why you think this is not the case? Did you try it?\r\n\r\nLet me address what we have up until now:\r\n\r\n- `PyUFunc_RegisterLoopForType` expects a function of type `PyUFuncGenericFunction` and is passed your changed functions\r\n- `PyUFuncGenericFunction` changed in 1.19 to have const params, so the changed functions will no longer match the type ` `PyUFuncGenericFunction`` in pre-1.19 numpy\r\n- `reinterpret_cast<PyUFuncGenericFunction>(fn)` can be used to solve this\r\n- `PyUFunc_RegisterLoopForType` seems to be a macro which casts a `void*` out of an array called `PyUFunc_API` to the (function) type required. Hence ABI for this doesn't matter\r\n- As the changed TF function is passed via a (casted) function pointer to numpy ABI of that doesn't matter either\r\n- Hence with the cast everything is API and ABI compatible with all numpy versions\r\n\r\n> The compiler decides at compile time which of the overloaded functions to call\r\n\r\nWhich overloaded functions? There are no overloaded functions here are they? You either have ones with const-pointers (this change) or without (prior to this changed)\r\n\r\n> The issue that I was describing was a potential one at link time,\r\n\r\nAfter reading the above, do you still see an issue? Which one? Have you tried it?", "Once again, you can ignore my comments on linking errors.\r\n\r\nThe only questions that remains is whether tensorflow should be compiled with numpy < 1.19.\r\nThe pull request, as it is now would allow tensorflow to be compiled only with numpy 1.19.\r\nA reinterpret cast would allow compilation with lower versions of numpy. My personal opinion is that adding the cast is not necessary, unless somebody can point out a good argument for supporting older versions of numpy.\r\n\r\nIn either case (with the reinterpret cast, or without it), tensorflow will continue to work with the older versions of numpy at runtime.", "> A reinterpret cast would allow compilation with lower versions of numpy. \r\n\r\nIMO that is worth it. Maybe with a slightly safer version that static_asserts that the signature is what is expected and does the cast only then. So either:\r\n- check for numpy < 1.19 (if that is possible) and cast to the explicit, known signature\r\n- or, add static_asserts so that it checks that PyUFuncGenericFunction is either the known new or old version and a second check that checks that the function to be cast is either the new or old version\r\n\r\nReason: If they change it again or the function here is changed then it will result in a build error instead of a silent failure\r\n\r\n> somebody can point out a good argument for supporting older versions of numpy.\r\n\r\nOne I can think of: User having numpy already installed. That won't be upgraded unless they do the explicitly (or implicitly if TF requires the new version) but maybe users have other software they use which doesn't work with the older version due to similar problems to this", "Glad we're finally on the same page about the issue of the pull request.\r\nI apologise for all my previous comments which diverted everyone's attention for something irrelevant.\r\n\r\nAbout the cast: I honestly am not sure which is better, which is why I asked for arguments for the cast.\r\nMy reasoning to dislike it is that not having a cast would not affect *users* of tensorflow. The only ones affected are *developers*. If you're a user and have issues with numpy 1.19, you can, of course downgrade to 1.18 (or not upgrade at all).\r\nIf you are a developer of a 3rd party library, then I'd say that there would be issues only if your C/C++ code is not compatible with the new ABI of numpy 1.19 (i.e. you use the numpy ABI). I would argue that it is then the responsibility of this 3rd party to update their code to comply with the breaking changes to numpy 1.19 - the change, as in this pull request is fairly straighforward, and if you really don't want to be compatible with numpy 1.19, you can also delay compatibility with the new tensorflow version (with which this fix would likely get shipped).\r\nIf you're a developer of tensorflow, then the worst thing that would happen is that your build would fail until you do a\r\n```$ pip install numpy>=1.19```\r\nGenerally, I do believe that most developers have separate python virtual environments (right?, at least I do), so several versions of numpy can co-exist without issues.\r\n\r\nAnother reason that I don't like having a cast is that it is to support an old version of numpy. I looked for official information on their release guidelines and end-of-life timeline, but I could find nothing. If a decision is made to support numpy 1.18, I would also add a comment on this (probably also with a link to this issue). Nonetheless, I feel like a cast would be a 'temporary' solution (until numpy 1.18 gets forgotten), which would remain there permanently (I try to avoid such code, as much as possible).\r\n\r\nAnd lastly, the cast is in fact not future-proof, as you've noted. Although extremely unlikely, the ABI may change again to something which is incompatible (unlike the current const qualifiers). I tried to look for some macros to detect for numpy 1.19, but could not find anything (most likely I missed it, though).", "You are missing an important group: people who install packages like system package maintainers and system admins. For them it is not possible to have conflicting versions or \"just upgrade\" a package. So having wider compatibility is a big bonus. \r\n\r\nNumpy 1.18 is not old enough to ignore it. And just adding the cast is less work than all this discussion. \r\n\r\nFor your last point: imo it is irrelevant as long as we detect that at compile time. See my solutions above. They won't completly change it without at least raising the major version and then a decision needs to be made. But that is all fruitless speculation.\r\n\r\nSo: best to get something to work for current versions and don't limit them just to save 4 lines of code or so. Rely on reported versions (is there a macro of the Numpy version?) or add a comment. Then nothing is forgotten", "@alextp does this PR break the C ABI guarantees we have in place?", "@ebrevdo I don't see any change to our ABI-stable headers in tensorflow/c/{eager,} so I don't think it's ABI-breaking.\r\n\r\nI also believe that the numpy abi breaking change is actually not an abi breaking change (i.e. the code links fine) but an api-breaking change (i.e. the const makes C++ compilers not willing to compile old code against the new header file but old code compiled agains the old header file links just fine against existing numpy).\r\n\r\nThat said I didn't review this change at all so don't trust me to approve.", "FWIW: All the functions changed are internal linkage so not exposed. Hence no ABI break is possible. And yes I agree that this is an API break only from numpy\r\n\r\nHowever before this is accepted I'd like to see support for pre and post numpy 1.19 handling. I.e. https://github.com/tensorflow/tensorflow/blob/84cc21ab082a1bb4b3dfd2ee5d32e18598f60282/tensorflow/compiler/xla/python/bfloat16.cc#L798 and https://github.com/tensorflow/tensorflow/blob/84cc21ab082a1bb4b3dfd2ee5d32e18598f60282/tensorflow/python/lib/core/bfloat16.cc#L637 need to be adapted so the new and the old signature (PyUFuncGenericFunction) are accepted (and only those 2)\r\n\r\nTo make the evaluation easier see this godbolt link which demonstrates the effect of compiling against numpy pre 1.19 and post 1.19 via a define: https://godbolt.org/z/5MKMWK", "Thanks for clarifying, @Flamefire ; looks like we may need an #if/#else that checks the version of numpy.", "I think always passing a const should work without the need for an #if. And\nit's also fine if we require a particular version of numpy's header files\nfor TF to build against.\n\nOn Wed, Sep 23, 2020 at 8:58 AM ebrevdo <notifications@github.com> wrote:\n\n> Thanks for clarifying, @Flamefire <https://github.com/Flamefire> ; looks\n> like we may need an #if/#else that checks the version of numpy.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40728#issuecomment-697593899>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPIDPQ4Z2JT6L6G5YTSHILLPANCNFSM4OGBKTEA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Not really. The problem is that the numpy API either expects a function like `foo(const int*)` OR `foo(int*)` and you cannot pass the other one as the types of the functions are different.\r\nSo yes a solution would be to accept this PR and require numpy >= 1.19 which would limit interop with other packages.\r\n\r\nMy proposed alternative: https://godbolt.org/z/5s9xMj\r\n\r\nThe idea is to use a function which accepts the new signature and casts it to what numpy expects. A `static_assert` makes sure we don't cast something wrong (e.g. when numpy adds a parameter):\r\n\r\n```\r\nstatic PyUFuncGenericFunction castToPyUFuncGenericFunction(TF_UFuncGenericFunction_New func) {\r\n    static_assert(\r\n      std::is_same<PyUFuncGenericFunction, TF_UFuncGenericFunction_New>::value || \r\n      std::is_same<PyUFuncGenericFunction, TF_UFuncGenericFunction_Old>::value,\r\n      \"Unknown PyUFuncGenericFunction signature\"\r\n    );\r\n    return reinterpret_cast<PyUFuncGenericFunction>(func);\r\n}\r\n```", "I thought this issue was already resolved by 75ea0b31477d6ba9e990e296bbbd8ca4e7eebadf ?", "@avdv You are right, that commit fixes the first part of the issue and https://github.com/tensorflow/tensorflow/commit/8ed58f3448c6c517f83ead72a9cde9322cb2952c the other. The first is very elegant, the second likely dangerous as it uses an unchecked `reinterpret_cast` which could be improved by my solution or a similar one to https://github.com/tensorflow/tensorflow/commit/75ea0b31477d6ba9e990e296bbbd8ca4e7eebadf by providing an overload", "Question: what do we need to do from this PR, given that it has been partly solved/merged in previous commits (if I understand the previous comments)?\r\n\r\nI am currently working on fixing the dependencies to work with the new pip resolver and this issue popped up.", "It seems I was able to build on Python3.8 with latest numpy.", "With aafe25d numpy is now past the API breakage"]}, {"number": 40726, "title": "Add gcs header to expose symbol", "body": "@mihaimaruseac \r\nThis PR add a header file to expose some functions for testing", "comments": []}, {"number": 40725, "title": "Incorrect documentation for model_from_config ", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/models/model_from_config\r\n\r\n## Description of issue (what needs changing):\r\n\r\ntf.keras.models.model_from_config function can create only layers, not the complete model as it is described in documentation. Correct usage is mentioned at https://www.tensorflow.org/guide/keras/save_and_serialize, but not described in the main documenation for tf.keras.Model class.\r\n\r\n> Calling config = model.get_config() will return a Python dict containing the configuration of the model. The same model can then be reconstructed via Sequential.from_config(config) (for a Sequential model) or Model.from_config(config) (for a Functional API model).\r\n\r\n### Clear description\r\nThe behavior of tf.keras.models.model_from_config does not correspond to the documentation.\r\nMoreover, it is even more confusing when compared with similar methods, like\r\n\r\n`tf.keras.models.model_from_json(model.to_json())`\r\n\r\n> <tensorflow.python.keras.engine.training.Model at 0x7fa2e443aa20>\r\n\r\n`tf.keras.models.model_from_yaml(model.to_yaml())`\r\n\r\n> <tensorflow.python.keras.engine.training.Model at 0x7fa2e443ca40>\r\n\r\nwhile model_from_config \r\n\r\n`tf.keras.models.model_from_config(model.get_config())`\r\n\r\n> > ---------------------------------------------------------------------------\r\n> KeyError                                  Traceback (most recent call last)\r\n> <ipython-input-99-f3b4bb685ac8> in <module>\r\n> ----> 1 tf.keras.models.model_from_config(model.get_config())\r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n>      53                     '`Sequential.from_config(config)`?')\r\n>      54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n> ---> 55   return deserialize(config, custom_objects=custom_objects)\r\n>      56 \r\n>      57 \r\n> \r\n> ~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n>      99   globs['SequenceFeatures'] = sfc.SequenceFeatures\r\n>     100 \r\n> --> 101   layer_class_name = config['class_name']\r\n>     102   if layer_class_name in _DESERIALIZATION_TABLE:\r\n>     103     config['class_name'] = _DESERIALIZATION_TABLE[layer_class_name]\r\n> \r\n> KeyError: 'class_name'\r\n> \r\n\r\n### Correct usage\r\n\r\n`tf.keras.Model().from_config(model.get_config())`\r\n\r\n> <tensorflow.python.keras.engine.training.Model at 0x7fa2e4480080>", "comments": ["Hi, can I work on this issue? Push a PR to solve this?", "@yil532 Feel free to raise one and tag this issue. Thanks!", "Thank you @ymodak ", "Closing this issue since the PR has merged. Thanks!"]}, {"number": 40724, "title": "Invalid test case from test_zero_padding_2d in convolutional_test.py", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n\r\n**Describe the current behavior**\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_test.py#L654 \r\nThey try to test on different data_format configurations, ie. `channels_first`, `channels_last`. But for the code in the abovementioned link, it's meaningless, it always takes the last inputs. We should construct the inputs separately under different conditions. Even though it didn't cause any error but it should be fixed.\r\n\r\n", "comments": ["Would something like:\r\n\r\n```\r\n    data_formats = zip(['channels_first', 'channels_last'], \r\n        [(num_samples, stack_size, input_num_row, input_num_col), \r\n            (num_samples, input_num_row, input_num_col, stack_size)])\r\n            \r\n    for data_format, shape in data_formats:\r\n      inputs = np.ones(shape)\r\n...\r\n```\r\ndo the trick? ", "already fixed. https://github.com/tensorflow/tensorflow/pull/41253", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40724\">No</a>\n"]}, {"number": 40723, "title": "Update version numbers for TensorFlow 2.3.0-rc0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 3\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/micro/examples/magic_wand/train/README.md:106:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:177:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:178:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:184:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:197:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:235:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:269:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:270:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:276:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:289:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:325:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:365:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:366:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:372:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:385:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:421:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:331:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:332:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:338:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:351:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:387:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:34:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:36:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:86:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:141:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:181:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:213:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:216:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:248:2.2.0\ntensorflow/lite/experimental/ios/TensorFlowLiteC.podspec:3:2.2.0\ntensorflow/lite/experimental/ios/TensorFlowLiteSelectTfOps.podspec:3:2.2.0\ntensorflow/lite/experimental/ios/TensorFlowLiteSelectTfOps.podspec:7:2.2.0\ntensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec:3:2.2.0\ntensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec:3:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:69:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:80:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:113:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:138:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:175:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:193:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:201:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:220:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:240:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:257:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:260:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:296:2.2.0\ntensorflow/compiler/mlir/lite/tests/end2end/back2back_fake_quant.pbtxt:1178:2.2.\n0\nBinary file \ntensorflow/python/keras/mixed_precision/experimental/testdata/lso_savedmodel_tf2\n.2/saved_model.pb matches\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:145:2.2.0\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:146:2.2.0\ntensorflow/tools/pip_package/setup.py:66:2.2.0\ntensorflow/tools/pip_package/setup.py:67:2.2.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/micro/examples/magic_wand/train/README.md:106:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:177:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:178:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:184:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:197:2.2.0\ntensorflow/lite/micro/examples/magic_wand/README.md:235:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:269:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:270:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:276:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:289:2.2.0\ntensorflow/lite/micro/examples/micro_speech/README.md:325:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:365:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:366:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:372:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:385:2.2.0\ntensorflow/lite/micro/examples/person_detection/README.md:421:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:331:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:332:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:338:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:351:2.2.0\ntensorflow/lite/micro/examples/hello_world/README.md:387:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:34:2.2.0\ntensorflow/lite/micro/tools/make/third_party_downloads.inc:36:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:86:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:141:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:181:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:213:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:216:2.2.0\ntensorflow/lite/toco/tflite/op_version.cc:248:2.2.0\ntensorflow/lite/experimental/ios/TensorFlowLiteC.podspec:3:2.2.0\ntensorflow/lite/experimental/ios/TensorFlowLiteSelectTfOps.podspec:3:2.2.0\ntensorflow/lite/experimental/ios/TensorFlowLiteSelectTfOps.podspec:7:2.2.0\ntensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec:3:2.2.0\ntensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec:3:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:69:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:80:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:113:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:138:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:175:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:193:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:201:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:220:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:240:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:257:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:260:2.2.0\ntensorflow/lite/tools/versioning/runtime_version.cc:296:2.2.0\ntensorflow/compiler/mlir/lite/tests/end2end/back2back_fake_quant.pbtxt:1178:2.2.\n0\nBinary file \ntensorflow/python/keras/mixed_precision/experimental/testdata/lso_savedmodel_tf2\n.2/saved_model.pb matches\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:145:2.2.0\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:146:2.2.0\ntensorflow/tools/pip_package/setup.py:66:2.2.0\ntensorflow/tools/pip_package/setup.py:67:2.2.0\n\nNo lingering old version strings \"r2.2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 40722, "title": "Segmentation fault interpreter->SetNumThreads CPP", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nHardware : Freescale i.MX6 Quad/DualLite\r\nProcessor: ARMv7 Processor rev 10 (v71)\r\nOS Platform and Distribution: Yocto built Linux distribution (kernel 4.9.4+)\r\nThe tf-lite library was built with common options and using default makefile: https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/tools/make/Makefile\r\nAPI : CPP\r\n\r\n**Describe the problem**\r\nCross compiling the minimal TF Lite interpreter cpp code where I added the line to set the number of threads to use results in a segmentation error when running the program.\r\n\r\nThe relevant code : \r\n```\r\nint main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, \"minimal <tflite model>\\n\");\r\n    return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n  int startt, endd;\r\n  startt = clock();\r\n\r\n  // Load model\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<Interpreter> interpreter;\r\n  std::cout << \"allocating one number of threads\" << std::endl;\r\n  int numthreads=1;\r\n  interpreter->SetNumThreads(numthreads);\r\n  std::cout << \"threads allocated\" << std::endl;\r\n\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n  endd = clock();\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  double time_taken = double(endd-startt)/double(CLOCKS_PER_SEC);\r\n  std::cout << \"4 threads should be selected\" << std::endl;\r\n  std::cout << \"Time taken to load in the model in tflite using CPP API :\" << fixed << time_taken << std::setprecision(5);\r\n  std::cout << \" sec \" << std::endl;\r\n}\r\n```\r\n\r\nThe model I've been using is a custom ENet I trained in TensorFlow and converted to TFLite using both TFLite_builtin_ops and TF_OPS, though I believe the problem has nothing to do with the model itself.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nRunning gdb and checking the stacktrace results in the following:\r\n![image](https://user-images.githubusercontent.com/29673343/85434212-fee63900-b585-11ea-860f-b3af1b4748bc.png)\r\n\r\nI also cross compiled a standard cpp program using multiple threads which runs just fine. So that excludes problems concerning firmware. For the record, the multithreading program I got running is the following : \r\n```\r\n#include <iostream>\r\n#include <cstdlib>\r\n#include <pthread.h>\r\n\r\nusing namespace std;\r\n\r\n#define NUM_THREADS 4\r\n\r\nvoid *PrintHello(void *threadid) {\r\n   long tid;\r\n   tid = (long)threadid;\r\n   cout << \"Hello World! Thread ID, \" << tid << endl;\r\n   pthread_exit(NULL);\r\n}\r\n\r\nint main () {\r\n   pthread_t threads[NUM_THREADS];\r\n   int rc;\r\n   int i;\r\n\r\n   for( i = 0; i < NUM_THREADS; i++ ) {\r\n      cout << \"main() : creating thread, \" << i << endl;\r\n      rc = pthread_create(&threads[i], NULL, PrintHello, (void *)i);\r\n\r\n      if (rc) {\r\n         cout << \"Error:unable to create thread,\" << rc << endl;\r\n         exit(-1);\r\n      }\r\n   }\r\n   pthread_exit(NULL);\r\n}\r\n```\r\n\r\nHow do I go about this problem? What is causing this? ", "comments": ["The problem might be related to that you are not allocating memory for Interpreter object? I would expect to see below;\r\n\r\n```\r\nstd::unique_ptr<Interpreter> interpreter = std::make_unique<Interpreter>();  //Make_unique missing here on your impl.\r\nstd::cout << \"allocating one number of threads\" << std::endl;\r\nint numthreads=1;\r\ninterpreter->SetNumThreads(numthreads);\r\n```\r\n", "@cngzhnp Indeed! Good find. This means that the documentation is not on point. I got my code from the following [link](https://www.tensorflow.org/lite/guide/inference).\r\n\r\nNow that we have that sorted out, I see that running inference on test data results in the use of only one core, even though I am explicitly initlizalizing 4 threads:\r\n\r\n![image](https://user-images.githubusercontent.com/29673343/85520934-dbfa6a00-b603-11ea-8313-5d946e41a3f3.png)\r\n\r\nThe code I am now using (in full) is stated below:\r\n```\r\n/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\n\r\n#include <cstdio>\r\n#include <iostream>\r\n#include <iomanip>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"\r\n#include <time.h>\r\n\r\n// This is an example that is minimal to read a model\r\n// from disk and perform inference. There is no data being loaded\r\n// that is up to you to add as a user.\r\n//\r\n// NOTE: Do not add any dependencies to this that cannot be built with\r\n// the minimal makefile. This example must remain trivial to build with\r\n// the minimal build tool.\r\n//\r\n// Usage: minimal <tflite model>\r\n\r\nusing namespace tflite;\r\nusing namespace std;\r\n\r\n#define TFLITE_MINIMAL_CHECK(x)                              \\\r\n  if (!(x)) {                                                \\\r\n    fprintf(stderr, \"Error at %s:%d\\n\", __FILE__, __LINE__); \\\r\n    exit(1);                                                 \\\r\n  }\r\n\r\nint main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, \"minimal <tflite model>\\n\");\r\n    return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n  int startt, endd;\r\n  startt = clock();\r\n\r\n  // Load model\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<Interpreter> interpreter(new Interpreter());\r\n  std::cout << \"allocating a number of threads\" << std::endl;\r\n  int numthreads=4;\r\n  interpreter->SetNumThreads(numthreads);\r\n  std::cout << \"threads allocated\" << std::endl;\r\n\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n  endd = clock();\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  //tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  double time_taken = double(endd-startt)/double(CLOCKS_PER_SEC);\r\n  std::cout << \"a number of threads should be selected\" << std::endl;\r\n  std::cout << \"Time taken to load in the model in tflite using CPP API :\" << fixed << time_taken << std::setprecision(5);\r\n  std::cout << \" sec \" << std::endl;\r\n\r\n\r\n  // Fill input buffers\r\n  // TODO(user): Insert code to fill input tensors\r\n  int startinput = clock();\r\n  int input = interpreter->inputs()[0];\r\n  float* input_data_ptr = interpreter->typed_tensor<float>(input);\r\n  float valuefloat = 0.5;\r\n  std::cout << \"add dummy image data of 3x480x640\" << std::endl;\r\n  for (int k=0; k<3; k++){\r\n    for (int i=0; i<480; ++i){\r\n        for (int j=0; j<640; j++){\r\n            *(input_data_ptr)=valuefloat;\r\n            input_data_ptr++;\r\n            }\r\n        }\r\n  }\r\n\r\n\r\n  int stopinput = clock();\r\n  double time_taken2 = double(stopinput-startinput)/double(CLOCKS_PER_SEC);\r\n  std::cout << \"Time taken to load data :\" << fixed << time_taken2 << std::setprecision(5);\r\n  std::cout << \" sec \" << std::endl;\r\n\r\n   // Run inference\r\n  std::cout << \"start interpreting\" << std::endl;\r\n  int interpretstart = clock();\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n  int interpretstop = clock();\r\n  double time_taken3 = double(interpretstop-interpretstart)/double(CLOCKS_PER_SEC);\r\n  std::cout << \"Interpreting took :\"<< time_taken3 << std::setprecision(5);\r\n  std::cout << \" sec \" << std::endl;\r\n\r\n  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Read output buffers\r\n  // TODO(user): Insert getting data out code.\r\n  int output_idx = interpreter->outputs()[0];\r\n  float* output = interpreter->typed_tensor<float>(output_idx);\r\n  std::cout << \"OUTPUT: \" << *output << std::endl;\r\n\r\n\r\n  return 0;\r\n}\r\n```\r\nthe screenshot is taken after the line \"start interpreting\" was printed to console, so during model inference.", "@FlorentijnD you're missing here I guess, in this link Interpreter pointer is allocated like below:\r\n\r\n```\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);  //(&interpreter) is missing on your part again.\r\n```\r\nIn your example, you did not give interpreter object as a parameter to InterpreterBuilder class. There is no missing point on documentation.\r\n", "Thanks for your quick reply! Six lines after instantiating the interpreter I am allocating the Interpreter pointer:\r\n `builder(&interpreter);`\r\n\r\nFor reference: I got my code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc", "Yes, I would expect that it should work. ", "I would too :) but setting 4 threads results in only one core being used for me. I'll await the assignee's response", "Only the `InterpreterBuilder` should be used to create a new Interpreter instance. You're right that the documentation for this is pretty poor.\r\n\r\n```\r\nstd::unique_ptr<Interpreter> interpreter(new Interpreter());\r\ninterpreter->SetNumThreads(numthreads);\r\nbuilder(&interpreter);\r\n```\r\nshould be\r\n```\r\nstd::unique_ptr<Interpreter> interpreter;\r\nbuilder(&interpreter);\r\ninterpreter->SetNumThreads(numthreads);\r\n```\r\nor better yet\r\n```\r\nstd::unique_ptr<Interpreter> interpreter;\r\nbuilder(&interpreter, numthreads);\r\n```\r\n\r\nThe fact that we have a public no-arg constructor is a mistake, and I believe is only used for testing. I'll look into restricting its visibility.\r\n", "This solves my problem. Thanks for looking into this", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40722\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40722\">No</a>\n"]}]