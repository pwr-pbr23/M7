[{"number": 34667, "title": "Specify the type of the tf.data.Dataset.range function", "body": "This pull request is related to #33414.", "comments": ["@jsimsa @mihaimaruseac\r\nThank you for your review. I also think that #34685 is much closer to the desired solution.\r\nSo, I close this pull request..."]}, {"number": 34666, "title": "Weight Normalization", "body": "**System information**\r\n- TensorFlow version : 1.11\r\n- Are you willing to contribute it : Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWeight Normalization is described in [Weight Normalization: A Simple Reparameterization\r\nto Accelerate Training of Deep Neural Networks](https://arxiv.org/pdf/1602.07868.pdf).\r\n\r\nCurrent, only in tf2.0 and use tensorflow-addons can achieve this normalization, it's [tfa-2.0](https://www.tensorflow.org/addons/tutorials/layers_weightnormalization)\r\n\r\nIn [Github](https://github.com/zoli333/Weight-Normalization) , someone write it in Conv2D\r\n\r\n**Will this change the current api? How?**\r\nIt should build a serious of new apis.\r\n\r\n**Who will benefit with this feature?**\r\nEspecially who want to train a GAN.\r\n", "comments": ["@MachineJeff ,\r\nHi,Release 1.15.0-rc0 is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.\r\nPlease refer the [link](https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc0).Thanks!", "@MachineJeff ,\r\nany update on the issue?Thanks!"]}, {"number": 34665, "title": "TypeError: Cannot convert value dtype([('resource', 'u1')]) to a TensorFlow DType.", "body": "Hello, I just started learning tf2.0, I tried to implement Chinese word2vec using tf2.0, I trained the model and passed `tf.compat.v1.train.Saver (). Save (session, ckpt_file_path, global_step = 2) The `command saves the trained model. However, when I load tf2.0 for prediction, an unexpected error occurs. I tried many methods but it was not resolved, but I used tf1.x to load the model prediction without any problems. In .0, the unexpected errors are as follows:\r\n```\r\nhome/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/client/session.py:1443: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  run_metadata)\r\nTraceback (most recent call last):\r\n  File \"predict2.py\", line 34, in <module>\r\n    norm = tf.sqrt(tf.reduce_sum(input_tensor=tf.square(embeddings), axis=1, keepdims=True), name=\"norm\")\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 10931, in square\r\n    \"Square\", x=x, name=name)\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 530, in _apply_op_helper\r\n    raise err\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 527, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 472, in make_tensor_proto\r\n    numpy_dtype = dtypes.as_dtype(nparray.dtype)\r\n  File \"/home/zh/sda3/Anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/dtypes.py\", line 721, in as_dtype\r\n    (type_value,))\r\nTypeError: Cannot convert value dtype([('resource', 'u1')]) to a TensorFlow DType.\r\n\r\n```\r\n\r\nmy predict code:\r\n```\r\ndictionary = {\"UNK\": 0, \"\uff0c\": 1, \"\u7684\": 2, \"\u3002\": 3, \"\u4e86\": 4, \"\u662f\": 5, \"\u300e\": 6, \"\u300f\": 7, \"\u8427\u708e\": 8}\r\nreverse_dictionary = {\"0\": \"UNK\", \"1\": \"\uff0c\", \"2\": \"\u7684\", \"3\": \"\u3002\", \"4\": \"\u4e86\", \"5\": \"\u662f\", \"6\": \"\u300e\", \"7\": \"\u300f\", \"8\": \"\u8427\u708e\"}\r\nvalid_word = ['\u8427\u708e']\r\n\r\nvalid_size = 1\r\nvalid_examples =[dictionary[li] for li in valid_word]\r\n# valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\r\nvalid_dataset = np.array(valid_examples)\r\nwith tf.compat.v1.Session() as sess:\r\n    saver = tf.compat.v1.train.import_meta_graph('./model/-2\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/3899434/model.zip)\r\n.meta')  \r\n    saver.restore(sess, tf.train.latest_checkpoint('./model/'))  \r\n    gragh = tf.compat.v1.get_default_graph()\r\n    tensor_name_list = [tensor.name for tensor in gragh.as_graph_def().node]\r\n    input_x = sess.graph.get_tensor_by_name('x:0')\r\n    input_y = sess.graph.get_tensor_by_name('y:0')\r\n    embeddings = sess.graph.get_tensor_by_name('embedding:0')\r\n    feed_dict = {input_x: valid_dataset}\r\n    embeddings = sess.run(embeddings,feed_dict)\r\n    norm = tf.sqrt(tf.reduce_sum(input_tensor=tf.square(embeddings), axis=1, keepdims=True), name=\"norm\")\r\n    normalized_embeddings = embeddings / norm\r\n    valid_embeddings = tf.nn.embedding_lookup(params=normalized_embeddings, ids=valid_dataset)\r\n    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\r\n    sim = similarity.eval()\r\n    for i in xrange(valid_size):\r\n        valid_word = reverse_dictionary[str(valid_examples[i])]\r\n        top_k = 8  # number of nearest neighbors\r\n        nearest = (-sim[i, :]).argsort()[:top_k]\r\n        log_str = \"Nearest to %s:\" % valid_word\r\n        for k in xrange(top_k):\r\n            close_word = reverse_dictionary[str(nearest[k])]\r\n            log_str = \"%s %s,\" % (log_str, close_word)\r\n        print(log_str)\r\n```\r\n`model.zip` is the model used for testing. Could you please help me to see what problems I encountered? I have tried many methods but still not solved, please give me more guidance, thanks!\r\n\r\n\r\n", "comments": ["@ry @jmhodges @eggie5 \r\ntest model\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/3899448/model.zip)\r\n", "I have tried on colab with TF version 2.0 and i am seeing the below error`.UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdf in position 43: invalid continuation byte`.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/632f6fced0f3c6f09ed283ea4dbc0124/untitled421.ipynb).Please, help me in reproducing the issue. It helps in localizing the issue faster. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@zhouhao-learning \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34664, "title": "TensorFlow Lite for Microcontrollers fails to build ", "body": "Attempting to build the test arduino library using:\r\n./tensorflow/lite/experimental/micro/tools/ci_build/test_arduino.sh\r\nThrows a error about not being able to execute Arduino-cli.\r\nI installed Arduino-cli through brew but still get the error.\r\n\r\n**System information**\r\n- Mac OSX 10.15.1\r\n- TensorFlow version: Master\r\n- Python version: 2.7\r\n\r\nIf I had to guess the build script is pulling down the linux binary and failing to run it. But I have not verified this.", "comments": ["Could you share the detailed error to help debugging? Thx!", "@kenthinson,\r\nIs this still an issue? Could you please update TensorFlow to v2.3 and check if you are still facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34664\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34664\">No</a>\n"]}, {"number": 34663, "title": "Switch a bunch of tf.data python tests to use TF combinations", "body": "This PR switches a bunch of tf.data python tests to use TF combinations.", "comments": ["@jsimsa Thanks for your review! The comments are addressed by [this commit](https://github.com/tensorflow/tensorflow/pull/34663/commits/166b4bb3a35f8e8ff5d58e2558918e7258b6d5f6). For `override_threadpool_test`, please find my answers [here](https://github.com/tensorflow/tensorflow/pull/34663#discussion_r352413125).", "Just rebase this PR to solve the conflicts in `tensorflow/python/data/kernel_tests/flat_map_test.py\r\n ` with [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/kernel_tests/flat_map_test.py#L70) ", "@jsimsa Thanks for your suggestion! The test cases for `testNumThreadsDeprecated` are updated [here](https://github.com/tensorflow/tensorflow/pull/34663/commits/c45ea870d61ec7146bea04caea6d318dfec6e227). Could you please take a look?", "@rthadur The failed test in Ubuntu CPU is not related. An error happened when migrating the change to the internal tests. Could you please help retrigger the internal tests?", "@feihugis thank you very much for porting these test to use TF combinations!", "@jsimsa Thanks a lot for your help with my PRs! Please feel free to let me know if there is anything else I can help with."]}, {"number": 34662, "title": "<release 2.1>-<rc1> cherry-pick request: update tflite op versions", "body": "The tflite built-in op version need to be updated to match with TF release.", "comments": ["This looks like it caused some memory corruption. Reverting at the moment, will trigger a test with this PR reverted on the release branch and if it turns out this is not the culprit I will roll it back to be included in the branch.", "We will revert the revert back and this will be part of 2.1 release"]}, {"number": 34661, "title": "<release r2.1>-<rc1> cherry-pick request: <update tflite op version map>", "body": "This version map need to be updated to match the latest TF release, so that user can get the correct runtime version when calling the API.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34661) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 34660, "title": "TF-TRT Backward compatibility test", "body": "- Adds a new test to ensure TF-TRT can execute models saved in previous minor releases of TF2.x.\r\n- The checked in model is generated using tensorflow-gpu==2.0.0\r\n", "comments": ["@pooyadavoodi can you please check build failures ?", "I just fixed a few bugs.\r\n\r\nAny idea about these errors which seem to be related to Eigen?\r\n\r\n```\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found \r\nWARNING: Download from https://bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was d8469c8be9ded39e914a7cd3c1246bae36ab0737d9c173328303766a3c7187e2 but wanted 9edd4860b52813eaf8c023f0de1767ec58e2d67a290b718e6702469208ac5be1\r\nERROR: An error occurred during the fetch of repository 'eigen_archive':\r\n   java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz, https://bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz] to /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/eigen_archive/54bca9936424.tar.gz: Checksum was d8469c8be9ded39e914a7cd3c1246bae36ab0737d9c173328303766a3c7187e2 but wanted 9edd4860b52813eaf8c023f0de1767ec58e2d67a290b718e6702469208ac5be1\r\nAnalyzing: 1471 targets (431 packages loaded, 3071 targets configured)\r\nAnalyzing: 1471 targets (440 packages loaded, 3250 targets configured)\r\nERROR: /tmpfs/src/github/tensorflow/third_party/eigen3/BUILD:33:1: //third_party/eigen3:eigen3 depends on @eigen_archive//:eigen in repository @eigen_archive which failed to fetch. no such package '@eigen_archive//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz, https://bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz] to /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/eigen_archive/54bca9936424.tar.gz: Checksum was d8469c8be9ded39e914a7cd3c1246bae36ab0737d9c173328303766a3c7187e2 but wanted 9edd4860b52813eaf8c023f0de1767ec58e2d67a290b718e6702469208ac5be1\r\n```", "> I just fixed a few bugs.\r\n> \r\n> Any idea about these errors which seem to be related to Eigen?\r\n> \r\n> ```\r\n> WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found \r\n> WARNING: Download from https://bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was d8469c8be9ded39e914a7cd3c1246bae36ab0737d9c173328303766a3c7187e2 but wanted 9edd4860b52813eaf8c023f0de1767ec58e2d67a290b718e6702469208ac5be1\r\n> ERROR: An error occurred during the fetch of repository 'eigen_archive':\r\n>    java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz, https://bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz] to /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/eigen_archive/54bca9936424.tar.gz: Checksum was d8469c8be9ded39e914a7cd3c1246bae36ab0737d9c173328303766a3c7187e2 but wanted 9edd4860b52813eaf8c023f0de1767ec58e2d67a290b718e6702469208ac5be1\r\n> Analyzing: 1471 targets (431 packages loaded, 3071 targets configured)\r\n> Analyzing: 1471 targets (440 packages loaded, 3250 targets configured)\r\n> ERROR: /tmpfs/src/github/tensorflow/third_party/eigen3/BUILD:33:1: //third_party/eigen3:eigen3 depends on @eigen_archive//:eigen in repository @eigen_archive which failed to fetch. no such package '@eigen_archive//': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz, https://bitbucket.org/eigen/eigen/get/54bca9936424.tar.gz] to /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/eigen_archive/54bca9936424.tar.gz: Checksum was d8469c8be9ded39e914a7cd3c1246bae36ab0737d9c173328303766a3c7187e2 but wanted 9edd4860b52813eaf8c023f0de1767ec58e2d67a290b718e6702469208ac5be1\r\n> ```\r\n\r\nThat error is hopefully fixed by this: https://github.com/tensorflow/tensorflow/commit/e3a7bdbebb99352351a19e2e403136166aa52934", "Also @sanjoy ", "Getting this now:\r\n```\r\nMissing the following dependencies from pip_packages:\r\n\r\nMissing dependency: //tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/variables/variables.index\r\nAffected Tests:\r\n//tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/variables/variables.index (null)\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test (70635f4b39bbe1d08d95efe3e379ef53)\r\n\r\nMissing dependency: //tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/variables/variables.data-00000-of-00002\r\nAffected Tests:\r\n//tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/variables/variables.data-00000-of-00002 (null)\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test (70635f4b39bbe1d08d95efe3e379ef53)\r\n\r\nMissing dependency: //tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/variables/variables.data-00001-of-00002\r\nAffected Tests:\r\n//tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/variables/variables.data-00001-of-00002 (null)\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test (70635f4b39bbe1d08d95efe3e379ef53)\r\n\r\nMissing dependency: //tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/saved_model.pb\r\nAffected Tests:\r\n//tensorflow/python/compiler/tensorrt:test/testdata/tftrt_2.0_saved_model/saved_model.pb (null)\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test (70635f4b39bbe1d08d95efe3e379ef53)\r\nTraceback (most recent call last):\r\n  File \"pip_smoke_test.py\", line 184, in <module>\r\n    main()\r\n  File \"pip_smoke_test.py\", line 177, in main\r\n    or add no_pip tag to the test.\"\"\")\r\nRuntimeError:\r\n    One or more added test dependencies are not in the pip package.\r\nIf these test dependencies need to be in TensorFlow pip package, please add them to //tensorflow/tools/pip_package/BUILD.\r\nElse either blacklist the dependencies in //tensorflow/tools/pip_package/pip_smoke_test.py\r\nor add no_pip tag to the test.\r\n```\r\n\r\nWhat do you suggest?"]}, {"number": 34659, "title": "Dataset iterator hanging from within a dataset interleave ", "body": "\r\n> \r\n> == check python ===================================================\r\n> python version: 3.7.4\r\n> python branch: \r\n> python build version: ('default', 'Sep  7 2019 18:27:02')\r\n> python compiler version: Clang 10.0.1 (clang-1001.0.46.4)\r\n> python implementation: CPython\r\n> \r\n> \r\n> == check os platform ===============================================\r\n> os: Darwin\r\n> os kernel version: Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.26~2/RELEASE_X86_64\r\n> os release version: 19.0.0\r\n> os platform: Darwin-19.0.0-x86_64-i386-64bit\r\n> linux distribution: ('', '', '')\r\n> linux os distribution: ('', '', '')\r\n> mac version: ('10.15', ('', '', ''), 'x86_64')\r\n> uname: uname_result(system='Darwin', node='Andrews-MacBook.local', release='19.0.0', version='Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.26~2/RELEASE_X86_64', machine='x86_64', processor='i386')\r\n> architecture: ('64bit', '')\r\n> machine: x86_64\r\n> \r\n> \r\n> == are we in docker =============================================\r\n> No\r\n> \r\n> == compiler =====================================================\r\n> xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\r\n> \r\n> == check pips ===================================================\r\n> numpy                1.17.4   \r\n> protobuf             3.10.0   \r\n> tensorflow           2.0.0    \r\n> tensorflow-estimator 2.0.1    \r\n> \r\n> == check for virtualenv =========================================\r\n> True\r\n> \r\n> == tensorflow import ============================================\r\n> tf.version.VERSION = 2.0.0\r\n> tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d382ca\r\n> tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)\r\n> \r\n> == env ==========================================================\r\n> LD_LIBRARY_PATH is unset\r\n> DYLD_LIBRARY_PATH is unset\r\n> \r\n> == nvidia-smi ===================================================\r\n> ./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n> \r\n> == cuda libs  ===================================================\r\n> \r\n> == tensorflow installed from info ==================\r\n> Name: tensorflow\r\n> Version: 2.0.0\r\n> Summary: TensorFlow is an open source machine learning framework for everyone.\r\n> Home-page: https://www.tensorflow.org/\r\n> Author-email: packages@tensorflow.org\r\n> License: Apache 2.0\r\n> Location: /Users/andrew/.local/share/virtualenvs/lib_andrew_scratch--yvJ8pLH/lib/python3.7/site-packages\r\n> Required-by: \r\n> \r\n> == python version  ==============================================\r\n> (major, minor, micro, releaselevel, serial)\r\n> (3, 7, 4, 'final', 0)\r\n> \r\n> == bazel version  ===============================================\r\n> \r\n\r\n**Describe the current behavior**\r\nIt seems all threads are waiting in a deadlock behaviour\r\n\r\n**Describe the expected behavior**\r\nTo return a dataset iterator\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n\r\n```\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\n\r\ndef gen( ):\r\n\tnoise_tfrecord = \"test.tfrecord\"\r\n\traw_noise_dataset = tf.data.TFRecordDataset(noise_tfrecord)\r\n\tnoise_count = sum(1 for _ in raw_noise_dataset)\r\n\tyield 1.\r\n\r\ndef do_bug():\r\n\tsignal_tfrecord = \"test.tfrecord\"\r\n\traw_signal_dataset = tf.data.TFRecordDataset(signal_tfrecord)\r\n\tdataset = raw_signal_dataset.interleave(lambda x: tf.data.Dataset.from_generator(gen, \r\n\t\t\toutput_types=(tf.float32), \r\n\t\t\targs=( )),\r\n\t\t\tcycle_length=2,\r\n\t\t\tblock_length=1,\r\n\t\t\tnum_parallel_calls = 1 )\r\n\r\n\tfor d in dataset:\r\n\t\tprint(d)\r\n\r\n\treturn \r\n\r\ndef _int_feature(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\ndef make_dataset():\r\n\ttfrecord_filename = \"test.tfrecord\"\r\n\twith tf.io.TFRecordWriter(tfrecord_filename) as writer:\r\n\t\tfor i in range(4):\r\n\t\t\tfeature = {'feat' : _int_feature(42)}\r\n\r\n\t\t\texample_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n\t\t\twriter.write(example_proto.SerializeToString())  \r\n\r\nif __name__== \"__main__\":\r\n\tmake_dataset()\r\n\tdo_bug()\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[bug.py.zip](https://github.com/tensorflow/tensorflow/files/3898706/bug.py.zip)\r\n", "comments": ["Smaller example:\r\n```\r\n\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\n\r\ndef gen( ):\r\n\traw_dataset = tf.data.TFRecordDataset(\"not_a_file\")\r\n\tcount = sum(1 for _ in raw_dataset)\r\n\tyield 1.\r\n\r\ndef do_bug():\r\n\traw_signal_dataset = tf.data.Dataset.from_tensor_slices([1, 1, 1, 1])\r\n\r\n\tdataset = raw_signal_dataset.interleave(lambda x: tf.data.Dataset.from_generator(gen, \r\n\t\t\toutput_types=(tf.float32), args=()), num_parallel_calls = 1 )\r\n\r\n\tfor d in dataset:\r\n\t\tprint(d)\r\n\r\n\treturn \r\n\r\nif __name__== \"__main__\":\r\n\tdo_bug()\r\n\r\n```", "Doesn't hang if:\r\n\r\n- num_parallel_calls = None\r\n- length of the list [1, 1, 1, 1] is shorter than 4\r\n- the dataset within the generator is not built from tf.data.TFRecordDataset\r\n", "I have tried on colab with TF version 2.0,2.1.0-dev20191128 and was able to reproduce the issue.Thanks!", "I can confirm that I can reproduce the issue. This seems to be a general TensorFlow runtime issue related to not properly cancelling TensorFlow computation which executes Python computation (your `gen` function) which in turns executes TensorFlow computation (the `tf.data.TFRecordDataset` call).\r\n\r\nI do not foresee this being prioritized for a fix and would instead recommend not layering TensorFlow and Python computation this way. In this particular case, instead of doing `from_generator`, you could use `tf.data.TFRecordDataset` directly and follow it up with `tf.data.Dataset.map` that uses `py_function`, which performs the Python computation you previously did in `from_generator`.\r\n", "@andrewstanfordjason \r\nPlease let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34659\">No</a>\n"]}, {"number": 34658, "title": "AttributeError: module 'tensorflow' has no attribute 'Session'", "body": "> import tensorflow as tf\r\n> sess = tf.Session()\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-f75057d1d95f> in <module>\r\n----> 1 sess = tf.Session()\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\nEnvironment details : \r\nAnaconda Python : 3.6\r\ntensorflow version : 2.0.0\r\nMachine (OS) : LINUX , Ubuntu 16.04\r\n\r\nNew to tensorflow, I am facing this error help me with this!", "comments": ["@imshreyaa , you are using old TensorFlow 1.X syntax. `tf.Session()`was deprecated in TensorFlow 2.0.0\r\nPlease go through the latest documentation on https://www.tensorflow.org to know more.\r\nIf you still want to use `tf.Session()`, use the syntax `tf.compat.v1.Session()` instead. \r\n*Also because you are new to TensorFlow, I would recommend you practice on Google Colab [notebooks](https://colab.research.google.com/) instead. There is much more flexibility, you won't need to download anything, most example datasets/libraries are already there. You get to try out TensorFlow 1 and TensorFlow 2 side by side and also import other example colab notebooks as it is. Hope this helps. :)", "I'm having the same issue too... Was working before the weekend. Not anymore.", "@shreyasharma98 ,\r\ncan you please check @nikochiko reponse and let us know if it helped.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Thank you So much I have the same problem , and now I can study tensorflow Thx!!!!", "Thank you so much <3", "The \"sess = tf.Session()\" operation is deprecated in tensorflow version 2.0.0\r\nPlease set your tensorflow version less than 2.0.0 .\r\nGood Luck.", "> The \"sess = tf.Session()\" operation is deprecated in tensorflow version 2.0.0\r\n> Please set your tensorflow version less than 2.0.0 .\r\n> Good Luck.\r\n\r\nHi, can you please advise on how to downgrade tensorboard to 2.0? Along with tensorflow?", "i'm using it on google colab but still getting same error", "In google colab also, we have to use tf.compat.v1.Session()"]}, {"number": 34657, "title": "problems when I build tensorflow from source, someone helps please", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source(not sure, i am using ./configure and  then bazel build)\r\n- TensorFlow version: 1.14(for my conda environment)\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: conda create -n xxx python=3.6.8 the enter the env and pip install tensorflow==1.14\r\n- Bazel version (if compiling from source):1.1.0\r\n- GCC/Compiler version (if compiling from source): 6.4.0\r\n- CUDA/cuDNN version: cuda 10/cudnn  CUDNN_MAJOR 7\r\n- GPU model and memory: TESLA P40\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nHI, GUYS, I was trying to build tensorflow transform_graph tools  from source(same way as build tensorflow), I already installed bazel and clone tensorflow from github. When i use command\r\nbazel build tensorflow/tools/graph_transforms:transform_graph, i got this problem\r\n![image](https://user-images.githubusercontent.com/51428350/69752002-2286f500-1105-11ea-9f50-547851b71f59.png)\r\n\r\nafter searching for a while, i think it might because of the CUDA , so i ./configure again, this time, when it needs me to choose do you wish to build tf with cuda support, i chose no\r\nand build again, but this time still not work\r\n![image](https://user-images.githubusercontent.com/51428350/69752146-78f43380-1105-11ea-8407-c0de92760ec9.png)\r\n\r\ncould you please help me out? thanks so much\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34657\">No</a>\n"]}, {"number": 34656, "title": "tensorflow-serving apt-get not working", "body": "**System information**\r\n- OS : Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip\r\n\r\n**Describe the problem**\r\nUpon following instructions in https://www.tensorflow.org/tfx/serving/setup for apt-get method to install tensorflow-model-serving-universal \r\n\r\nget the following error\r\n\r\nErr:3 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease   503  Service Unavailable [IP: 172.217.6.80 80]\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\necho \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\r\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\r\napt-get update && apt-get install tensorflow-model-server\r\n\r\n\r\n", "comments": ["@ajayaraman I am going to close this issue here. Please post this issue in [tensorflow/serving](https://github.com/tensorflow/serving/issues) repository as this issue is more apt to that repository. "]}, {"number": 34655, "title": "[XLA] Add a new XLA mode: XLALite", "body": "Add 2 new XLA flags:\r\n- `TF_XLA_FLAGS=--tf_xla_ops_to_cluster=[FUSIBLE,...]`\r\n- `TF_XLA_FLAGS=--tf_xla_auto_jit=fusible`\r\n    This is a shortcut to `TF_XLA_FLAGS=--tf_xla_ops_to_cluster=FUSIBLE TF_XLA_FLAGS=--tf_xla_auto_jit=1`\r\n\r\n\r\nThis enables XLA but only for a subset of TF operations that XLA know how to fuse together. This allows using XLA operations fusion capabilities while removing some of the current XLA slow down case. In most cases where XLA is slower than TF classic, XLA isn't slower the TF classic. In some cases where XLA give speed up vs TF classic, XLALite give a good part of that speed up.\r\n\r\nThe flag tf_xla_supported_ops can accept TF operation names and/or predefined groups of operation. The group FUSIBLE  includes all the groups defined. Multiple value can be passed by separating them by comma.", "comments": ["@thomasjoerg the issue with the non-trivially destructible global remains, it will break the build.", "I can't reply to the comment, so replying here.\r\nI changed the code (and renamed the function name) to be safer to potential future changes as suggested.", "@rthadur Did CopyBara get stuck somehow? It's been two days since the PR was approved. Can you kick CopyBara?", "@thomasjoerg we can also force run copybara by adding the `kokoro:force-run` tag, let me try to do it.", "> @thomasjoerg we can also force run copybara by adding the `kokoro:force-run` tag, let me try to do it.\r\n\r\n@cheshire Frocing a Kokoro did not do the trick. I imported the PR manually.", "Note, I forgot to update the interface change in my test. So my test currently broke.\r\nWhy the CI didn't failed? Do they run XLA tests?\r\nI just pushed a fix. I saw somewhere that tf nightly build are broken. Could this be related?", "I made a new PR with the last commit that was missing:\r\nhttps://github.com/tensorflow/tensorflow/pull/35263"]}, {"number": 34654, "title": "[r2.1:Cherrypick][Intel MKL] Fixing a bug in Elu Op", "body": "This is bug fix that was merged to master branch recently in #34535.", "comments": ["pinging @penpornk for review."]}, {"number": 34653, "title": "[ROCm] Fix for the broken ROCm CSB - 191127", "body": "The following commit breaks the ROCm CSB\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/4b4bf4223d50cb28da44f081722520889b72583e\r\n\r\nIt introduces new functionality (condition number method) and unit-tests to check the same. The unit-tests need support for the complex number types, which is currently not supported on the ROCm platform, and hence the consequent breakage.\r\n\r\nThe \"fix\" is to skip testing the complex types when running the newly added unit-test on the ROCm platform\r\n\r\n------\r\n\r\n@chsigg @whchung ", "comments": []}, {"number": 34651, "title": "Euclidean distance transform add on to support 3D images?", "body": "Hello as the euclidean distance transform has been implemented with specific input shapes, I was wondering if this would be expanded to more general shapes. I am currently trying to generate a distance matrix based on 3D ground truth data (i.e. W X H X D) which could easily be 4D  (Batch size x W x H x D). Is this something that would be possible?\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!"]}, {"number": 34650, "title": "TFRecordsWriter default write mode?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/io/TFRecordWriter\r\n\r\n## Description of issue:\r\n\r\nSo I've been using TFRecordWriter for a while now and I would be interested in the method it writes to files. If I create two TFRecord files with the exact same name what is the default wrtiing option? `append` or `(re)write`? It would be crucial to know this in my use case. Thanks in advance.", "comments": ["All right, just to make sure I iterated over the datafile and I know from statistics what should be the number of records in the TFRecord file:\r\n\r\n```python3\r\nsum(1 for _ in dataset)\r\n```\r\n\r\nIt seems that the default behaviour is `(re)write` so not appending. Can anyone point me to the code where this is actually defined? ", "The fact that TFRecordWriter (re)writes (as opposed to appends) is not specified in the documentation of `TFRecordWriter`. \r\n\r\nThe Python code that creates the [writer](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/lib/io/tf_record.py#L217) ends up calling into C++ using a generated wrapper, which ends up creating an instance of the following [class](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/lib/io/py_record_writer.cc#L30), which ends up using `NewWritableFile` and its [documentation](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/core/platform/file_system.h#L76) specifies the behavior.\r\n\r\nAppend is not supported out-of-the-box because appending to a compressed file would in general not be possible without first decompressing the file.\r\n", "Thanks!"]}, {"number": 34649, "title": "Tensorflow lite converter error", "body": "I was running a tensorflowlite converter and it gave the following error.\r\n\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-12-501417991c7a> in <module>\r\n----> 1 tflite=converter.convert()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py in convert(self)\r\n    444         input_tensors=input_tensors,\r\n    445         output_tensors=output_tensors,\r\n--> 446         **converter_kwargs)\r\n    447 \r\n    448     if self._is_calibration_quantize():\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n--> 449       enable_mlir_converter=enable_mlir_converter)\r\n    450   return data\r\n    451 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2019-11-27 19:59:16.411699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-11-27 19:59:20.890923: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019-11-27 19:59:20.894284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-27 19:59:20.919992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce 940M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2019-11-27 19:59:20.920714: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-27 19:59:20.921672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-27 19:59:22.502049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-27 19:59:22.502595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-27 19:59:22.502905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-27 19:59:22.504028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2019-11-27 19:59:22.550147: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-11-27 19:59:22.551545: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-27 19:59:22.552833: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-11-27 19:59:22.554116: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-27 19:59:22.555345: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-11-27 19:59:22.556546: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-27 19:59:22.557739: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-27 19:59:22.558973: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-11-27 19:59:22.566435: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 54 arrays (0 quantized)\r\n2019-11-27 19:59:22.568675: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 54 arrays (0 quantized)\r\n2019-11-27 19:59:22.574828: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6 operators, 34 arrays (0 quantized)\r\n2019-11-27 19:59:22.576359: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 6 operators, 34 arrays (0 quantized)\r\n2019-11-27 19:59:22.577888: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 6 operators, 34 arrays (0 quantized)\r\n2019-11-27 19:59:22.579626: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2019-11-27 19:59:22.582696: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.", "comments": ["Hi Sapnil,\r\n\r\nCan you attach the model (or code) you're trying to convert so we can reproduce it?\r\n", "Hi,\r\n\r\nCan you download the latest tf-nightly pip and then try convert this model using our new MLIR converter? You can enable the new converter by setting:\r\n\r\nconverter.experimental_new_converter = True", "> Hi Sapnil,\r\n> \r\n> Can you attach the model (or code) you're trying to convert so we can reproduce it?\r\n\r\n\r\nAttaching my model \r\ntensor=TensorBoard(log_dir=\"logs_final\\{nm}\".format(nm=name),histogram_freq=0,write_graph=True,write_images=True)\r\nmodel = Sequential()\r\nmodel.add(LSTM(20,input_shape=[1,5]))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss=loss, optimizer='adam',metrics=['mae', 'mse',rmse])\r\nmodel.fit(trainX, trainY, epochs=epoch, batch_size=batch, verbose=1,callbacks=[tensor])\r\nmodel.save('model/{nm}.h5'.format(nm=name))\r\n", "> Hi,\r\n> \r\n> Can you download the latest tf-nightly pip and then try convert this model using our new MLIR converter? You can enable the new converter by setting:\r\n> \r\n> converter.experimental_new_converter = True\r\n\r\nThanks man.It worked."]}, {"number": 34648, "title": "Use TPU on Colab shows:Session edefaa660de08d53 is not found.", "body": "I want to use the tpu on colab to do some exercise. i follow the colab tutorial to use the tpu. i can run the example on tpu. but when i run a new  custom model, the console shows \"Session edefaa660de08d53 is not found.\"\r\ni don't know what mistake there is\r\n\r\n**System information**\r\n- env:   colab\r\n- TensorFlow installed from (source or binary):  \r\n- TensorFlow version (use command below): 1.15\r\n- Python version: python3\r\n\r\n**Describe the current behavior**\r\ni download a data from internet on loacl storage. Then i read the data and convert it to the tfrecord,\r\nAnd i bulid a custom model. i  use the tup like the tutorial shows.\r\nBut when i run model.fit(), the console shows this mistake.\r\nhow do i fix it?\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfilename = os.listdir(r'./tfrecord')\r\nfiles = [os.path.join(r'./tfrecord', x) for x in filename]\r\ndataset = tf.data.TFRecordDataset(files)\r\ndataset = dataset.map(_parse_func)\r\ndataset = dataset.shuffle(256)\r\ndataset = dataset.batch(128,drop_remainder=True)\r\n\r\n# define some custom layer, those layer made by Conv2D, Concat, softmax\r\n# define the custom model\r\n\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n    opt = tf.keras.optimizers.Adam()\r\n    def rmse(y_true, y_pred):\r\n        return tf.reduce_mean(tf.math.sqrt(tf.math.squared_difference(y_pred, y_true)))\r\n    generator = generator_() # my custom model\r\n    generator.compile(optimizer=opt, loss=tf.keras.metrics.MSE, metrics=[rmse])\r\nmodel.fit()\r\n```\r\nthe github gist to reproduce the problem is:\r\nhttps://gist.github.com/x7night/45ba9eb08f568909bde0cc5f65050653\r\n**Other info / logs**\r\nSession edefaa660de08d53 is not found.", "comments": ["@x7night Can you please share a google colab gist reproducing this issue. Thanks!", "@gowthamkpr\r\ndoes the gist like blow ? i just find the buttom to save the .ipynb as the github gist. the link is:\r\nhttps://gist.github.com/x7night/45ba9eb08f568909bde0cc5f65050653\r\n", "@x7night This is similar to the issue #32712. Please confirm, if thats the case we can track this in a single issue.", "@gowthamkpr \r\nit seems like that. i get data from the TFRecordDataset, and i also get the mistake ' session not found' when i run model.fit()", "I am gonna close this issue as it has already been tracked here #32712 @x7night. Please follow that issue and it should be resolved soon. Thanks!"]}, {"number": 34647, "title": "How to build static libtensorflowlite.a for Android ?", "body": "bazel build -c opt --config=andrdoi_arm --cxxopt='--std=c++11' --cpu=arm64-v8a \\\r\ntensorflow/lite:libtensorflowlite.so\r\n\r\nI can build libtensorflowlite.so using above command .\r\nIs there any way to build static libtensorflowlite.a for Android ?", "comments": ["bazel does not natively support generating static `.a` libraries.\r\n\r\nIs there a specific reason you need a `.a` for Android? Since Android code gets bundled as an Apk, you typically want to bundle the `libtensorflowlite.so` in the Apk which can then get linked to the binary when the app starts.\r\n"]}, {"number": 34646, "title": "[tflite] Tests for the operator PACK quantization", "body": "There is a bug in the current implementation of the operator PACK.\r\nThe multiple inputs should be requantized.\r\nThe current PR fixes this bug and adds a test for the quantization of the PACK operator.", "comments": ["Hi, I believe the aribitrary_inputs and rescrict_same_input_output_scale params are sufficient, the tests are welcome additions though! Thanks!", "Hi Suharsh,\r\n\r\nYes, it makes sense. We kept this variable to be consistent with our solution to the maximum/minimum bug [PR](https://github.com/tensorflow/tensorflow/pull/32582).\r\nI edited PR and left only C++ test.\r\n\r\nCould you please review it ?\r\nThanks!", "Hi @suharshs, could you please take a look again ? The same problem with this PR - small complaint from the buildifier, that I have fixed.\r\nThanks!", "@suharshs Could you please review again ? I resolved merge conflicts and this dismissed PR approval. Thanks"]}, {"number": 34645, "title": "TypeError: minimize() missing 1 required positional argument: 'var_list'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    features = tf.cast(features, dtype=tf.float32)\r\n    print(features.dtype)\r\n    img = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(features)\r\n    img = tf.keras.layers.MaxPool2D((2, 2), 2)(img)\r\n    img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.MaxPool2D((2, 2), 2)(img)\r\n    img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(img)\r\n    img = tf.keras.layers.Flatten()(img)\r\n    img = tf.keras.layers.Dense(units=256, activation=\"relu\")(img)\r\n    img = tf.keras.layers.Dense(units=128, activation=\"relu\")(img)\r\n    logits = tf.keras.layers.Dense(units=10, activation=None)(img)\r\n\r\n    probs = tf.keras.activations.softmax(logits)\r\n    predicted_classes = tf.argmax(probs, 1)\r\n\r\n    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, probs)\r\n\r\n    acc = tf.compat.v1.metrics.accuracy(labels, predicted_classes)\r\n    metrics = {\"accuracy\": acc}\r\n    tf.summary.scalar(\"accuracy\", acc[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode,\r\n                                          loss=loss, eval_metric_ops=metrics)\r\n\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n\r\n    optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss)\r\n    return tf.estimator.EstimatorSpec(mode,\r\n                                      loss=loss, train_op=train_op)\r\n\r\n\r\ndef load_mnist():\r\n    train, test = mnist.load_data()\r\n    return train, test\r\n\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    # features = tf.convert_to_tensor(features, dtype=tf.float32)\r\n    # labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\n    dataset = dataset.shuffle(500)\r\n    dataset = dataset.repeat().batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef eval_input_fn(imgs, labels, batch_size):\r\n    dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\r\n    dataset = dataset.batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef main(m_dir=None):\r\n    (x_train, y_train), (x_test, y_test) = load_mnist()\r\n    x_train = x_train.reshape(60000, 28, 28, 1)\r\n    x_test = x_test.reshape(10000, 28, 28, 1)\r\n    classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=m_dir)\r\n\r\n    classifier.train(input_fn=lambda: train_input_fn(x_train, y_train, 500), steps=100)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@muhammadfahid51, `optimizer.minimize()` function requires two argument loss and var_list. \r\nIn your code replace `optimizer.minimize(loss)` with `optimizer.minimize(loss, var_list=None)`. For more see this [link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdagradOptimizer#compute_gradients). Thanks!", "In TF2.0, You are using the Keras optimizer, where it requires two arguments including the var_list. \r\nSee the doc here: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#minimize\r\nExample like: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#example\r\nwould explicitly get **var_list** from **trainable_weights**.\r\n\r\nIn TF1.X, instead, you could use the tf.train.AdagradOptimizer, where the **var_list** is not required.\r\nSee the doc here: https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdagradOptimizer#minimize", "In your case, either build a model object and take the **model.trainable_weights**, or define layers one by one and retrieve variables by **layer.weights**.\r\n\r\nCheck the doc here: https://www.tensorflow.org/guide/migrate#2_use_python_objects_to_track_variables_and_losses", "Closing since issue is resolved. Please feel free to reopen if issue still persists. Thanks!", "> @muhammadfahid51, `optimizer.minimize()` function requires two argument loss and var_list.\r\n> In your code replace `optimizer.minimize(loss)` with `optimizer.minimize(loss, var_list=None)`. For more see this [link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdagradOptimizer#compute_gradients). Thanks!\r\n\r\nwhen i  replace optimizer.minimize(loss) with optimizer.minimize(loss, var_list=None), it report a new error:\r\n`ValueError: `tape` is required when a `Tensor` loss is passed.`"]}, {"number": 34644, "title": "Can't save stateful LSTM as savedmodel (TF 2.0)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): UBUNTU\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  DOCKER IMAGE\r\n- TensorFlow version (use command below): DOCKER IMAGE 2.0\r\n- Python version: DOCKER IMAGE 2.0\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: DOCKER IMAGE 2.0\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nAttemping to save a model containing a stateful LSTM to savedmodel format fails with the following error:\r\n\r\n> AssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_1:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n\r\n**Describe the expected behavior**\r\nNo errors, and a correctly saved model :-)\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef get_model(stateful=False, batch_size=None):\r\n    n_channels = 1\r\n    sample_size = 10\r\n    n_units = 5\r\n\r\n    inputs = layers.Input(batch_shape=(batch_size, sample_size, n_channels),\r\n                          name='timeseries_inputs')\r\n    y = layers.LSTM(units=n_units, input_shape=(sample_size, n_channels),\r\n                    activation='tanh', recurrent_activation='sigmoid', return_sequences=False,\r\n                    stateful=stateful)(inputs)\r\n\r\n    y = layers.Dense(1)(y)\r\n\r\n    model = tf.keras.models.Model(inputs=inputs, outputs=y)\r\n\r\n    return model\r\n\r\n\r\nstateless_model = get_model(stateful=False)\r\n\r\nstateful_model = get_model(stateful=True, batch_size=1)\r\n\r\nstateless_model.save('stateless.h5')\r\nstateful_model.save('stateful.h5')\r\n\r\nstateless_model.save('stateless.tf')\r\nstateful_model.save('stateful.tf')\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-dev20191128 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7ff3594c234a8a39408abf72bdaeee8f/untitled417.ipynb). Thanks!", "Error is similar to the issue, #33247. ", "Any news on this?  Thanks!", "@optiluca This was resolved in the recently `tf-nightly`. I am not able to reproduce the issue with recent `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/7bde0bf225d272e883240afe5f7bf62f/34644.ipynb). Thank!\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if the issue didn't resolve for you.  Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34644\">No</a>\n"]}, {"number": 34643, "title": "TypeError while importing tensorflow caused by site.USER_SITE is set to None", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): --\r\n- GCC/Compiler version (if compiling from source): --\r\n- CUDA/cuDNN version: --\r\n- GPU model and memory: --\r\n\r\n**Describe the current behavior**\r\n\r\nWhen importing the tensorflow module with\r\n\r\n`import tensorflow`\r\n\r\nthe following Exception gets thrown\r\n\r\n> Traceback (most recent call last):\r\n>   File \"IMPORT_TENSORFLOW\", line 8, in run\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 98, in <module>\r\n>     from tensorflow_core import *\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py\", line 403, in <module>\r\n>     if _running_from_pip_package():\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py\", line 401, in _running_from_pip_package\r\n>     _current_file_location.startswith(dir_) for dir_ in _site_packages_dirs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/__init__.py\", line 401, in <genexpr>\r\n    _current_file_location.startswith(dir_) for dir_ in _site_packages_dirs)\r\n> TypeError: startswith first arg must be str or a tuple of str, not NoneType\r\n\r\nfrom\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/api_template.__init__.py#L119\r\n\r\nThe cause for this is\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/api_template.__init__.py#L104\r\n\r\nbecause `site.USER_SITE` can be `None` accordingly to the python documentation https://docs.python.org/3.6/library/site.html#site.USER_SITE and in this environment it happens to be the case.\r\n\r\nA workaround for this is to call  site.getusersitepackages() before importing tensorflow. However, in my opinion the `USER_SITE` needs to be checked here before it is used, or the list _site_packages_dirs needs to be filtered for `None` values.\r\n\r\n**Describe the expected behavior**\r\n\r\n`import tensorflow` succeeds without the exception\r\n\r\n**Code to reproduce the issue**\r\n\r\nStart container:\r\n\r\n`docker run --rm -it tensorflow/tensorflow:2.0.0-py3 bash`\r\n\r\nStart python3 in the container without site initialization:\r\n\r\n`python3 -S`\r\n\r\nAdd package locations to the sys.path and import tensorflow:\r\n\r\n```\r\nimport sys\r\nsys.path=['', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages']\r\nimport tensorflow\r\n```\r\n", "comments": ["I can verify the error with TF 2.2.0rc3. Created a PR  #38663 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34643\">No</a>\n"]}, {"number": 34642, "title": "[tflite] Output difference for simple MobileNetV2 model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Arch Linux**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (or github SHA if from source): **Tested on tf_nightly_gpu-2.1.0.dev20191126 from pip and 2.0.0 from the Arch repositories.**\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nOriginal implementation from [here](https://www.tensorflow.org/lite/convert/python_api#end-to-end_mobilenet_conversion_).\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load the MobileNet tf.keras model (and set include_top=False).\r\nmodel = tf.keras.applications.MobileNetV2(\r\n    weights=\"imagenet\", input_shape=(224, 224, 3), include_top=False)\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the TensorFlow Lite model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\ntflite_results = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n# Test the TensorFlow model on random input data.\r\ntf_results = model(tf.constant(input_data))\r\n\r\n# Compare the result.\r\nfor tf_result, tflite_result in zip(tf_results, tflite_results):\r\n  np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nAssertionError: \r\nArrays are not almost equal to 5 decimals\r\n\r\nMismatch: 2.78%\r\nMax absolute difference: 5.4180622e-05\r\nMax relative difference: 0.01288559\r\n x: array([[[0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ],\r\n        [0.     , 0.     , 0.     , ..., 0.     , 0.82339, 0.74743],\r\n        [0.     , 0.88872, 0.     , ..., 0.     , 2.61463, 2.41743],...\r\n y: array([[[0.     , 0.     , 0.     , ..., 0.     , 0.     , 0.     ],\r\n        [0.     , 0.     , 0.     , ..., 0.     , 0.8234 , 0.74745],\r\n        [0.     , 0.88871, 0.     , ..., 0.     , 2.61464, 2.41744],...\r\n\r\n```\r\n\r\n**Failure details**\r\nAs per the [converter documentation](https://www.tensorflow.org/lite/convert/python_api), I am trying to verify that the output of my converted model and the output of my original model are identical. This is the case if I run [this](https://www.tensorflow.org/lite/convert/python_api#end-to-end_mobilenet_conversion_) code from the documentation page, however if I add `include_top=False` to the `MobileNetV2` constructor, then I get different output. I would expect the exact same output, regardless of what part of the model I select.\r\n\r\nAs per the output log, there is a max relative difference of 0.01288559, which in my opinion is pretty significant.\r\n\r\nNote that there is no quantization being performed. It is simply creating a random MobileNetV2 model, converting it and passing through some random data. I find it weird that the output is different, but that the outputs are identical if `include_top=True`.\r\n\r\n**Any other info / logs**\r\nI have tested on other backbones as well, `MobileNet` and `ResNet50`; they have the same issue that the output is different when adding `include_top=False`.\r\n", "comments": ["Small update: I just tested it on tensorflow 1.15.0 and the same issue occurs there.", "Hi,\r\nWe have the same problem, and also with tflite GPU on Android.\r\n1. Difference between \"checkpoint model\" and tflite inferences on a workstation \r\n2. Complete difference between tflite CPU and GPU inferences on Android phone (Google Pixel2, Android 9)\r\n\r\nWe managed to run our model using OpenCL and we use inverse convolution.\r\n\r\nTF version: nightly (OpenCL), 1.15 (OpenGL), 1.13 just CPU \r\nTFLite compiled from source\r\nOS Platform and Distribution: Ubuntu 18.04\r\n\r\nUseNNAPI false\r\nSetAllowFp16PrecisionForFp32: false\r\n", "Many thanks for raising the issue!\r\n\r\nAs for the inference difference between tf vs tflite, it might be caused by the model conversion. @miaout17, do you have any insights into this? Thx!\r\n\r\nAs for the \"complete difference between tflite CPU and GPU inferences on Android phone\", @impjdi, could you shed some light on this? Thx!", "Google Pixel 2 doesn't support OpenCL, so I assume it's OpenGL.\r\n\r\n@multiverse-tf \r\n\r\nDon't we have regression test for MobileNet v2 CPU vs GPU on Pixel 2?", "Below I am sharing output (audio spectrogram) of the app for the same input and parameters.\r\nThe model should denoise the audio and on CPU (upper sample) it is doing its job. And on GPU (lower sample) it is different worse (does not clear upper frequencies).\r\n\r\n![denoised](https://user-images.githubusercontent.com/18346395/70513871-c07db500-1b32-11ea-8f07-14275c4f4cd6.png)\r\n\r\nTFLite version 1.15 compiled from r1.15 branch \r\nDevices:\r\n* Pixel  2, GPU OpenGL\r\n* S10, GPU OpenCL\r\n\r\nFor TF compiled from master, output for both CPU and GPU is the same as 1.15 GPU output ;(\r\n\r\n\r\n", "can you run on some real dataset and check if indeed we loose some accuracy?\r\n\r\n@multiverse-tf don't we have regression test for accuracy?\r\n\r\nThanks,", "We didn't find any accuracy regression w.r.t. this MobileNetV2 model against tflite head version.\r\n\r\n@koodzi as you built the lib against tflite 1.15, could you try the head version and see whether the issue still exists? Also, might instrument your code w/ PrintInterpreterState (i.e. tensorflow/lite/optional_debug_tools.h) to help the debugging? Thx!", "Should a new issue be created for the Android issue, or is this considered the same issue but in a different use case? I'm getting somewhat confused with some of these comments:\r\n\r\n> We didn't find any accuracy regression w.r.t. this MobileNetV2 model against tflite head version.\r\n\r\nAre you referring to the difference in accuracy between tf and tflite **on a Pixel 2**? Or in general? If you run the provided script in the first post, does it show a difference?", "> Should a new issue be created for the Android issue, or is this considered the same issue but in a different use case? I'm getting somewhat confused with some of these comments:\r\nSorry for the confusion as I also missed sth. in previous posts.\r\n> \r\n> > We didn't find any accuracy regression w.r.t. this MobileNetV2 model against tflite head version.\r\n> \r\n> Are you referring to the difference in accuracy between tf and tflite **on a Pixel 2**? Or in general? If you run the provided script in the first post, does it show a difference?\r\nI mean we didn't find accuracy regressions of this model on pixel 2 using either tflite cpu or gpu if using tflite lib compiled from the head.\r\n\r\nI haven't run the provided script yet in the first post. But I guess the difference might be caused by the different numeric computations (i.e. caused by different op kernel implementation, the model graph is different etc.) between the checkpoint model and the converted tflite model.\r\n\r\nAs for \"Complete difference between tflite CPU and GPU inferences on Android phone (Google Pixel2, Android 9)\", I am not sure about the cause. \r\n\r\nBtw, you mentioned \"For TF compiled from master, output for both CPU and GPU is the same as 1.15 GPU output ;(\", do you mean compiling TFLite from the github head?\r\n\r\n", "> I haven't run the provided script yet in the first post. But I guess the difference might be caused by the different numeric computations (i.e. caused by different op kernel implementation, the model graph is different etc.) between the checkpoint model and the converted tflite model.\r\n\r\nShouldn't the outputs be nearly identical? The [converter page](https://www.tensorflow.org/lite/convert/python_api#end-to-end_mobilenet_conversion_) suggests that at least the very last output will be identical with `decimal=5` precision. I would assume, in a network where many computations are performed, that all the preceding computations have to be the same. If there is a small difference in one of the earlier layers, it would propagate throughout the network, causing a large offset at the end, but this doesn't appear to be the case.\r\n\r\nIn other words, how can MobileNetV2 **with** head produce identical results in tf and tflite, while MobileNetV2 **without** head, a subset of the complete network, produces different results? This confuses me.", "@hgaiser @multiverse-tf \r\n\r\nI think the term ``head'' is causing confusion.  When multiverse-tf was talking about tflite head version, he's talking about the latest code snapshot of TFLite repository.  When hgaiser is talking about head, it's about the bottleneck layer in the network.\r\n\r\nNow, having said that, to hgaiser's latest comment, that statement of ``a subset of the complete network produces different results'' is only logically correct.  For GPU, we do graph modifications and tensor reshapes for performance.  If there is a bug somewhere in this process, the subset can produce wrong output, probably at the last layer (right before head) or during output tensor reshaping.  Also I would advise to leave quantization out of the game and do everything in float32 (I just learned that tf.lite.TFLiteConverter by default produces quantized models... =/ )", "My apologies, I wasn't paying full attention.  I see that there are two people asking for different things on this thread: @hgaiser about the discrepancies of models with/without bottleneck layer and @koodzi about CPU/GPU discrepancies.  @koodzi I would advise to open up a separate thread, rather than piggybacking on someone else's thread.  Also, attaching the model can help diagnosing the issue.\r\n\r\nAnyway, in that case, @hgaiser please ignore my comment about GPU.  It looks like either (a) there is a a bug in the converter, or (b) converter APIs are not used in the way it's intended.  I'll leave that out to @miaout17 \r\n\r\n", "Oh I thought that TFLiteConverter uses float32 by default. Where did you see that it uses quantisation? I would suspect that it uses float32 actually, since the differences would be greater otherwise, wouldn't it?", "@hgaiser \r\n\r\nI was referring to #34873 and it was\r\n\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nwhich caused quantization (took me a while to find out).  If you're not using that, you should be in float32 domain.", "Ah I see, I didn't know that, but I'm not using it so it should be float32 then.\r\n\r\nI ran a test where I split up the model first up until the first layer, then up until the second, third, fourth, etc. and compared the output w.r.t. the tflite converted model. For each layer I added I print if the outputs are equal or not equal. Here is the script:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load the MobileNet tf.keras model (and set include_top=False).\r\nmodel = tf.keras.applications.MobileNetV2(\r\n    weights=\"imagenet\", input_shape=(224, 224, 3), include_top=True)\r\n\r\nfor layer in model.layers:\r\n    submodel = tf.keras.Model(inputs=model.inputs, outputs=layer.output)\r\n\r\n    # Convert the model.\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(submodel)\r\n    tflite_model = converter.convert()\r\n\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test the TensorFlow Lite model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n\r\n    # The function `get_tensor()` returns a copy of the tensor data.\r\n    # Use `tensor()` in order to get a pointer to the tensor.\r\n    tflite_results = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n    # Test the TensorFlow model on random input data.\r\n    tf_results = submodel(tf.constant(input_data))\r\n\r\n    # Compare the result.\r\n    for tf_result, tflite_result in zip(tf_results, tflite_results):\r\n      try:\r\n          np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)\r\n          print(f\"EQUAL {layer.__class__.__name__}\")\r\n      except:\r\n          print(f\"NOT EQUAL {layer.__class__.__name__}\")\r\n```\r\n\r\nHere is the output:\r\n```\r\nEQUAL InputLayer\r\nEQUAL ZeroPadding2D\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nNOT EQUAL ZeroPadding2D\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nEQUAL ZeroPadding2D\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nNOT EQUAL ZeroPadding2D\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nNOT EQUAL Add\r\nEQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL ZeroPadding2D\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nEQUAL BatchNormalization\r\nEQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Add\r\nNOT EQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nNOT EQUAL DepthwiseConv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nEQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL Conv2D\r\nNOT EQUAL BatchNormalization\r\nNOT EQUAL ReLU\r\nEQUAL GlobalAveragePooling2D\r\nEQUAL Dense\r\n```\r\nI find it weird that it moves from not equal to equal, I don't really have an explanation for that.", "@miaout17 do you have any idea what is happening here?", "That looks to me a matter of tolerance, try decreasing `decimal` to 4 or 3 \r\n", "Was able to replicate the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5c6bd5f29cc33b76ace6d0e1fb54217a/untitled90.ipynb)..Thanks !", "You can't really compare TF output with TFLite output, because they don't use the same implementation.  For example, floating point operations are *not* commutative, which will cause discrepancies when order of computation is done differently (a + b + c != c + b + a and a = 1e10, b = -1e10, c = 1e-10).  So don't try to compare TF with TFLite, but inspect the values manually and make a more meaningful comparison.\r\n\r\nAlso, one weird thing is that BatchNorm is preserved after conversion.  You might want to specify an option that drops the batch norm layer.", "@hgaiser ,\r\nPlease take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/34642#issuecomment-852346805) and try in latest stable version2.7 and let us know if the issue still persists.Thanks!", "I'm not working on this project anymore so I am unlikely to test this out. I'll close it, assuming the previous comment is correct.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34642\">No</a>\n"]}, {"number": 34641, "title": "Shape_refiner.cc documentation", "body": "I get this as an output from my model, but the model runs and outputs a loss and an accuracy. It always posts the same 49 index numbers (I'm only posting one to avoid flooding), each of them twice. \r\n\r\n`W tensorflow/core/common_runtime/shape_refiner.cc:89] Function instantiation has undefined input shape at index: 380 in the outer inference context.`\r\n\r\nWhat does it mean? I'm using Python and I've posted a question about this [here](https://stackoverflow.com/questions/59057300/what-is-the-outer-inference-context-shape-refiner-cc-in-tensorflow-2?noredirect=1#comment104357261_59057300) and [here](https://stackoverflow.com/questions/59039788/how-to-combine-a-pre-trained-keraslayer-from-tensorflow-v-2-hub-and-tfrecords).\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nCan you please helps us minimal standalone code snippet to reproduce the issue in our environment. It helps us in localizing the issue faster.If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n> \r\n> Can you please helps us minimal standalone code snippet to reproduce the issue in our environment. It helps us in localizing the issue faster.If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\r\nOS: Win 10\r\nTensorFlow: 2.0.0\r\nInstalled with pip install tensorflow-gpu\r\n\r\nNo, I cannot provide a code snippet. More details are in the links provided. I know the warning only pops up when model.fit is being run, just not what it means.", "I am reproducing the same when running one of the tutorials, but only when running it as a Python program, not a Jupyter notebook. \r\n\r\nSteps to reproduce:\r\n- Clone the docs repository https://github.com/tensorflow/docs.git\r\n- Run Jupyter and open transfer_learning_with_hub\r\n- Download the notebook as Python\r\n- Run the Python program\r\n\r\nTensorflow 2.2.0 installed with pip under Ubuntu 18.04, Python 3.6.9, CUDA 10.1.243.\r\n\r\nHere the first bunch of lines I get as output\r\n```/home/fanta/.local/virtualenv/python3/bin/python3.6 /home/fanta/workspace/cats_and_dogs/transfer_learning_with_hub.py\r\n2020-07-23 11:52:06.017072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-23 11:52:06.033250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.033613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.797GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-23 11:52:06.033789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-23 11:52:06.034939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-23 11:52:06.035888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-23 11:52:06.036112: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-23 11:52:06.037404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-23 11:52:06.038121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-23 11:52:06.040719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-23 11:52:06.040844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.041242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.041563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-23 11:52:06.065028: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3605630000 Hz\r\n2020-07-23 11:52:06.065731: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8494000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-23 11:52:06.065770: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-23 11:52:06.177864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.178265: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5d8db90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-23 11:52:06.178277: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2020-07-23 11:52:06.178390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.178721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.797GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-07-23 11:52:06.178756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-23 11:52:06.178770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-23 11:52:06.178784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-23 11:52:06.178798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-23 11:52:06.178811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-23 11:52:06.178825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-23 11:52:06.178839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-23 11:52:06.178881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.179232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.179548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-23 11:52:06.179575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-23 11:52:06.180323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-23 11:52:06.180333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-07-23 11:52:06.180339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-07-23 11:52:06.180412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.180780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-23 11:52:06.181120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6848 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-23 11:52:08.824200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-23 11:52:09.401212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\nFound 3670 images belonging to 5 classes.\r\nImage batch shape:  (32, 224, 224, 3)\r\nLabel batch shape:  (32, 5)\r\n(32, 1280)\r\nModel: \"sequential_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nkeras_layer_1 (KerasLayer)   (None, 1280)              2257984   \r\n_________________________________________________________________\r\ndense (Dense)                (None, 5)                 6405      \r\n=================================================================\r\nTotal params: 2,264,389\r\nTrainable params: 6,405\r\nNon-trainable params: 2,257,984\r\n_________________________________________________________________\r\nWARNING:tensorflow:From /home/fanta/workspace/cats_and_dogs/transfer_learning_with_hub.py:332: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nWARNING:tensorflow:From /home/fanta/workspace/cats_and_dogs/transfer_learning_with_hub.py:332: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nEpoch 1/2\r\n2020-07-23 11:52:14.099525: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 98 in the outer inference context.\r\n2020-07-23 11:52:14.099565: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 68 in the outer inference context.\r\n2020-07-23 11:52:14.099579: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 75 in the outer inference context.\r\n2020-07-23 11:52:14.099588: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 90 in the outer inference context.\r\n2020-07-23 11:52:14.099598: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 97 in the outer inference context.\r\n2020-07-23 11:52:14.099607: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 86 in the outer inference context.\r\n2020-07-23 11:52:14.099617: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 78 in the outer inference context.\r\n2020-07-23 11:52:14.099630: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 43 in the outer inference context.\r\n2020-07-23 11:52:14.099644: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 13 in the outer inference context.\r\n2020-07-23 11:52:14.099656: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 542 in the outer inference context.\r\n2020-07-23 11:52:14.099664: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 565 in the outer inference context.\r\n2020-07-23 11:52:14.099675: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 541 in the outer inference context.\r\n2020-07-23 11:52:14.099689: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 510 in the outer inference context.\r\n2020-07-23 11:52:14.099699: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 564 in the outer inference context.\r\n2020-07-23 11:52:14.099708: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 556 in the outer inference context.\r\n2020-07-23 11:52:14.099721: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 533 in the outer inference context.\r\n2020-07-23 11:52:14.099733: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 509 in the outer inference context.\r\n2020-07-23 11:52:14.099742: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 502 in the outer inference context.\r\n2020-07-23 11:52:14.099750: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 493 in the outer inference context.\r\n2020-07-23 11:52:14.099758: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 563 in the outer inference context.\r\n2020-07-23 11:52:14.099771: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 532 in the outer inference context.\r\n2020-07-23 11:52:14.099780: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 524 in the outer inference context.\r\n2020-07-23 11:52:14.099792: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 501 in the outer inference context.\r\n2020-07-23 11:52:14.099805: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 554 in the outer inference context.\r\n2020-07-23 11:52:14.099818: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 531 in the outer inference context.\r\n2020-07-23 11:52:14.099832: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 500 in the outer inference context.\r\n2020-07-23 11:52:14.099840: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 491 in the outer inference context.\r\n2020-07-23 11:52:14.099849: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 570 in the outer inference context.\r\n2020-07-23 11:52:14.099859: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 553 in the outer inference context.\r\n2020-07-23 11:52:14.099872: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 522 in the outer inference context.\r\n2020-07-23 11:52:14.099886: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 490 in the outer inference context.\r\n2020-07-23 11:52:14.099896: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 479 in the outer inference context.\r\n2020-07-23 11:52:14.099910: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 552 in the outer inference context.\r\n2020-07-23 11:52:14.099919: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 545 in the outer inference context.\r\n2020-07-23 11:52:14.099931: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 521 in the outer inference context.\r\n2020-07-23 11:52:14.099940: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 513 in the outer inference context.\r\n2020-07-23 11:52:14.099956: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 478 in the outer inference context.\r\n2020-07-23 11:52:14.099964: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 470 in the outer inference context.\r\n2020-07-23 11:52:14.099974: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 455 in the outer inference context.\r\n2020-07-23 11:52:14.099989: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 489 in the outer inference context.\r\n2020-07-23 11:52:14.100002: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 568 in the outer inference context.\r\n```\r\n", "Maybe not related to this problem, but to help others coming to this issue by searching this error message:\r\nI had the same warnings with a custom model using a dataset from a generator without specifying the output shape. After adding the `output_shape` argument, the error was gone:\r\n\r\n`tf.data.Dataset.from_generator(generator, output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([3000, 16]), tf.TensorShape([None])))`", "@fantauzzi This is a stale issue. I ran the same code (.py file) as you mentioned and I don't see `shape_refiner.cc` in the output. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/8c0a60ccc5c29ec5a04af8bcd0f5afc0/untitled.ipynb).\r\n\r\nThe following is the trace\r\n\r\n```\r\n2021-09-16 19:22:02.687488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:03.223456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:03.224371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:03.225833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:03.226684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:03.227456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:07.976241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:07.977116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:07.978011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-16 19:22:07.978752: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2021-09-16 19:22:07.978931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\r\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\r\n65536/61306 [================================] - 0s 0us/step\r\n73728/61306 [====================================] - 0s 0us/step\r\n2021-09-16 19:22:10.758315: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-09-16 19:22:13.030183: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\r\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\r\n16384/10484 [==============================================] - 0s 0us/step\r\n24576/10484 [======================================================================] - 0s 0us/step\r\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\r\n228818944/228813984 [==============================] - 2s 0us/step\r\n228827136/228813984 [==============================] - 2s 0us/step\r\nFound 3670 files belonging to 5 classes.\r\nUsing 2936 files for training.\r\nFound 3670 files belonging to 5 classes.\r\nUsing 734 files for validation.\r\n['daisy' 'dandelion' 'roses' 'sunflowers' 'tulips']\r\n(32, 224, 224, 3)\r\n(32,)\r\n2021-09-16 19:22:43.276409: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n(32, 1280)\r\nModel: \"sequential_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nkeras_layer_1 (KerasLayer)   (None, 1280)              2257984   \r\n_________________________________________________________________\r\ndense (Dense)                (None, 5)                 6405      \r\n=================================================================\r\nTotal params: 2,264,389\r\nTrainable params: 6,405\r\nNon-trainable params: 2,257,984\r\n_________________________________________________________________\r\n2021-09-16 19:22:55.251226: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\r\n2021-09-16 19:22:55.251278: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\r\n2021-09-16 19:22:55.251397: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\r\n2021-09-16 19:22:55.489145: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\r\n2021-09-16 19:22:55.489474: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\r\nEpoch 1/10\r\n 1/92 [..............................] - ETA: 5:23 - loss: 2.0944 - acc: 0.09382021-09-16 19:22:59.548043: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\r\n2021-09-16 19:22:59.548100: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\r\n 2/92 [..............................] - ETA: 50s - loss: 1.8873 - acc: 0.2031 2021-09-16 19:22:59.775584: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\r\n2021-09-16 19:22:59.776113: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\r\n2021-09-16 19:22:59.918816: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 262 callback api events and 259 activity events. \r\n2021-09-16 19:22:59.927012: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\r\n2021-09-16 19:22:59.942508: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59\r\n\r\n2021-09-16 19:22:59.949419: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.trace.json.gz\r\n2021-09-16 19:22:59.980905: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59\r\n\r\n2021-09-16 19:22:59.985390: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.memory_profile.json.gz\r\n2021-09-16 19:22:59.986443: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59\r\nDumped tool data for xplane.pb to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.xplane.pb\r\nDumped tool data for overview_page.pb to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.overview_page.pb\r\nDumped tool data for input_pipeline.pb to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.input_pipeline.pb\r\nDumped tool data for tensorflow_stats.pb to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.tensorflow_stats.pb\r\nDumped tool data for kernel_stats.pb to logs/fit/20210916-192255/train/plugins/profile/2021_09_16_19_22_59/66336260c644.kernel_stats.pb\r\n\r\n92/92 [==============================] - 14s 118ms/step - loss: 0.7719 - acc: 0.7173 - val_loss: 0.4277 - val_acc: 0.8610\r\nEpoch 2/10\r\n92/92 [==============================] - 8s 93ms/step - loss: 0.3777 - acc: 0.8743 - val_loss: 0.3406 - val_acc: 0.8883\r\nEpoch 3/10\r\n92/92 [==============================] - 9s 93ms/step - loss: 0.2963 - acc: 0.9057 - val_loss: 0.3075 - val_acc: 0.8951\r\nEpoch 4/10\r\n92/92 [==============================] - 9s 93ms/step - loss: 0.2469 - acc: 0.9223 - val_loss: 0.2911 - val_acc: 0.9005\r\nEpoch 5/10\r\n92/92 [==============================] - 8s 92ms/step - loss: 0.2114 - acc: 0.9353 - val_loss: 0.2819 - val_acc: 0.9087\r\nEpoch 6/10\r\n92/92 [==============================] - 8s 93ms/step - loss: 0.1841 - acc: 0.9472 - val_loss: 0.2764 - val_acc: 0.9087\r\nEpoch 7/10\r\n92/92 [==============================] - 8s 92ms/step - loss: 0.1623 - acc: 0.9574 - val_loss: 0.2730 - val_acc: 0.9087\r\nEpoch 8/10\r\n92/92 [==============================] - 8s 92ms/step - loss: 0.1444 - acc: 0.9632 - val_loss: 0.2708 - val_acc: 0.9142\r\nEpoch 9/10\r\n92/92 [==============================] - 8s 92ms/step - loss: 0.1295 - acc: 0.9710 - val_loss: 0.2695 - val_acc: 0.9142\r\nEpoch 10/10\r\n92/92 [==============================] - 8s 92ms/step - loss: 0.1167 - acc: 0.9751 - val_loss: 0.2688 - val_acc: 0.9142\r\n['roses' 'dandelion' 'tulips' 'sunflowers' 'dandelion' 'roses' 'dandelion'\r\n 'roses' 'tulips' 'dandelion' 'tulips' 'tulips' 'sunflowers' 'tulips'\r\n 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'dandelion'\r\n 'tulips' 'sunflowers' 'roses' 'sunflowers' 'dandelion' 'tulips' 'roses'\r\n 'roses' 'sunflowers' 'tulips' 'sunflowers']\r\n2021-09-16 19:24:53.020450: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n['roses' 'dandelion' 'tulips' 'sunflowers' 'dandelion' 'roses' 'dandelion'\r\n 'roses' 'tulips' 'dandelion' 'tulips' 'tulips' 'sunflowers' 'tulips'\r\n 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'dandelion'\r\n 'tulips' 'sunflowers' 'roses' 'sunflowers' 'dandelion' 'tulips' 'roses'\r\n 'roses' 'sunflowers' 'tulips' 'sunflowers']\r\n```\r\n\r\nI think this was resolved. Please feel free to reopen the issue if it persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34641\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34641\">No</a>\n"]}, {"number": 34640, "title": "Update RELEASE.md", "body": "a format error", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34640) for more info**.\n\n<!-- need_sender_cla -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac "]}, {"number": 34639, "title": "Contribution guideline for Interative Notebook needs to be modified.", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/community/contribute/docs \r\n\r\n## Description of issue (what needs changing):\r\nDocumentation under \"Interactive notebooks\" needs to be modified to accommodate issues created after direct editing of Jupyter Notebook in Colab.\r\n  \r\n\r\n### Clear description. \r\n\r\nDirect editing (on colab or using VSCode) Jupyter Notebook and committing as mentioned [here](https://www.tensorflow.org/community/contribute/docs) adds additional unintended changes like prettifying and escapes unicode symbols. \r\n\r\nFor example, see here: https://github.com/tensorflow/docs/pull/1238 \r\n\r\nRelated code commit : https://github.com/copperwiring/docs/commit/98f35604617d1ecc93b3dc75ac6ec4ab108536eb \r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nN/A\r\n\r\n### Returns defined\r\n\r\nN/A\r\n\r\n### Raises listed and defined\r\n\r\nNone.\r\n\r\n### Usage example\r\n\r\nIt is useful and needed for any PR request for Jupyter Notebook\r\n\r\n### Request visuals, if applicable\r\n\r\nN/A\r\n\r\n### Submit a pull request?\r\n\r\nI am planning to submit a PR to improve the documentation.\r\n", "comments": ["Thanks, @copperwiring \r\n\r\nUnfortunately this is WAI since there are serialization differences between Colab versions. This is being tracked in an internal ticket (b/123414970)\r\n\r\ncc: @csemike @corrieann"]}, {"number": 34638, "title": "when set CollectiveCommunication NCCL, the train is stucking", "body": "System information\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    OS Platform and Distribution: Ubuntu 18.04\r\n    TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n    TensorFlow version (use command below): 2.0\r\n    Python version: 3.6.9\r\n    CUDA/cuDNN version: 10/7.6.4.38\r\n    GPU model and memory: Tesla P4  8G\r\n\r\nDescribe the current behavior\r\nI  run the code described below:\r\n\r\n**TEST 1:   (two machine)**\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:12345\", \"server2:12345\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\nIn the other machine\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:12345\", \"server2:12345\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\n\r\nWhen the script start processing the first epoch it crashes,\r\n\r\n**Describe the expected behavior**\r\n\r\n15s/epoch  is so slow\r\n\r\n<img width=\"1162\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69707374-942a6780-1134-11ea-8dd1-994fd7e41451.png\">\r\n\r\n\r\n**TEST 2:   (one machine)**\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:12345\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\n**Describe the expected behavior**\r\n\r\n5s/epoch      same as use  strategy = tf.distribute.MirroredStrategy()  one GPU card\r\n\r\n<img width=\"1072\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69707387-9b517580-1134-11ea-976a-73c9d36fdc2b.png\">\r\n\r\n\r\n**CODE**\r\n\r\n```\r\nimport ssl\r\n\r\nssl._create_default_https_context = ssl._create_unverified_context\r\nimport os\r\nimport json\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nimport argparse\r\n\r\n\r\ndef configure_cluster(worker_hosts=None, task_index=-1):\r\n    \"\"\"Set multi-worker cluster spec in TF_CONFIG environment variable.\r\n    Args:\r\n      worker_hosts: comma-separated list of worker ip:port pairs.\r\n    Returns:\r\n      Number of workers in the cluster.\r\n    \"\"\"\r\n    tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\r\n    if tf_config:\r\n        num_workers = len(tf_config['cluster'].get('worker', []))\r\n    elif worker_hosts:\r\n        workers = worker_hosts.split(',')\r\n        num_workers = len(workers)\r\n        if num_workers > 1 and task_index < 0:\r\n            raise ValueError('Must specify task_index when number of workers > 1')\r\n        task_index = 0 if num_workers == 1 else task_index\r\n        os.environ['TF_CONFIG'] = json.dumps({\r\n            'cluster': {\r\n                'worker': workers\r\n            },\r\n            'task': {'type': 'worker', 'index': task_index}\r\n        })\r\n    else:\r\n        num_workers = 1\r\n    return num_workers\r\n\r\n\r\nparser = argparse.ArgumentParser(description='TensorFlow Benchmark',\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\nparser.add_argument('--num-epochs', type=int, default=5, help='input batch size')\r\nparser.add_argument('--batch-size-per-replica', type=int, default=64, help='input batch size')\r\nparser.add_argument('--worker-hosts', type=str, default=None)\r\nparser.add_argument('--worker-index', type=int, default=0)\r\n\r\nargs = parser.parse_args()\r\n\r\nworker_num = configure_cluster(args.worker_hosts, args.worker_index)\r\nbatch_size = args.batch_size_per_replica * worker_num\r\nprint('Batch Size: %d' % batch_size)\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nprint(\"Physical GPU Devices Num:\", len(gpus))\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n    communication=tf.distribute.experimental.CollectiveCommunication.RING)\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [128, 128]) / 255.0\r\n    return image, label\r\n\r\n\r\n# if as_supervised is True\uff0creturn image abd label\r\ndataset, info = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, with_info=True, as_supervised=True)\r\ndataset = dataset.map(resize).repeat().shuffle(1024).batch(batch_size)\r\ntotal_num_examples = 3670\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\r\n        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\r\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n        tf.keras.layers.Dropout(0.25),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dropout(0.5),\r\n        tf.keras.layers.Dense(info.features['label'].num_classes, activation='softmax')\r\n    ])\r\n    model.compile(\r\n        optimizer=tf.optimizers.Adam(learning_rate=0.001),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n    )\r\n\r\nmodel.fit(dataset, steps_per_epoch=total_num_examples // batch_size, epochs=args.num_epochs)\r\n\r\n\r\n```\r\n\r\nIn TEST 2:    only 1 worker,   5s/epoch ~ 6s/epoch  \uff08batch_szie = 128\uff09\r\n\r\nIn TEST 1:  2 workers,     15s/epoch  ~ 19s/epoch  \uff08per_batch_szie = 64, batch_size = 64 *2 = 128\uff09\r\n\r\n\r\n**Question1:** \r\nSo, I cannot understand,  with dist MultiWorkerMirroredStrategy  worker nums > 1,  \r\nwhy Training is so slow\r\n\r\n**Question2:**\r\nwhen I  set   strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n    communication=tf.distribute.experimental.CollectiveCommunication.NCCL) \r\n\r\nIn TEST 1:  2 workers,  the training will stuck  in epoch 2,   epoch 1 done.\r\n\r\n<img width=\"1185\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69708066-e750ea00-1135-11ea-9bb4-362340737b76.png\">\r\n\r\n\r\n\r\n", "comments": ["@zwqjoy Try setting your environment with `NCCL_DEBUG=INFO` and please send us the output log.", "@byronyi \r\n\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/12653212/69783404-6ace1200-11ee-11ea-9592-9a6e4ee26b1b.png)\r\n\r\n\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/12653212/69783386-61dd4080-11ee-11ea-9c3f-ba43c4b2069e.png)\r\n\r\n\r\n", "@byronyi \r\n\r\nAnd I see the NCCL Version is:   NCCL version 2.4.7+cudaCUDA_MAJOR.CUDA_MINOR\r\n\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/12653212/69790962-74f80c80-11fe-11ea-909f-dd3844e4dd50.png)\r\n\r\nbut I use : https://github.com/nvIDIA/nccl    \r\nI install the lastest NCCL             version is  NCCL version 2.5.6+cuda10.0\r\n\r\n<img width=\"691\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69790998-850fec00-11fe-11ea-8509-9c94091298ef.png\">\r\n\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/12653212/69791050-a40e7e00-11fe-11ea-9972-12c7766dbf51.png)\r\n\r\n\r\nDoes the tensorflow-gpu has  build-in  NCCL version ?\r\n\r\n\r\nMaybe Tensorflow dis with the NCCL will  hang , is the NCCL verison ? or  I loss some NCCL conf  ?\r\n\r\n\r\n\r\n\r\n", "set NCCL_LL_THRESHOLD=0, It works"]}, {"number": 34637, "title": "multi-device function optimization failure", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 19.10\r\n- TensorFlow installed from (source or binary):github release\r\n- TensorFlow version (use command below):2.0\r\n- Python version:3.7.5\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):9.2.1 20191008\r\n- CUDA/cuDNN version:10.1, 7.6.4\r\n- GPU model and memory: RTX2080Ti x2 NVLink\r\n\r\n**Describe the current behavior**\r\nIt takes to much time and shows W:\"multi-device function optimization failure\"\r\n\r\n**Describe the expected behavior**\r\nFastly start train after compile the model with no W.\r\n\r\n**Code to reproduce the issue**\r\n\r\n> main:\r\n```\r\nimport os\r\nos.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'\r\nimport tensorflow as tf\r\nfrom Models import MCN\r\nfrom DataSets import ImageNet\r\n\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nlogical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n\r\nBATCH_SIZE=20\r\nBATCHS_PER_APLY_GRADIENTS=1000//BATCH_SIZE\r\n\r\nds=ImageNet.ImageNetP()\r\nstarategy=tf.distribute.MirroredStrategy()\r\nwith starategy.scope():\r\n    model=MCN.mcn_520(2,24)\r\n    model.summary()\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        metrics=[tf.keras.metrics.TopKCategoricalAccuracy(1,'top1'),tf.keras.metrics.TopKCategoricalAccuracy(5,'top5')]\r\n        )\r\n    fit_ds,val_ds=ds(BATCH_SIZE)\r\n    model.fit(\r\n        fit_ds,\r\n        epochs=1000000,\r\n        steps_per_epoch=BATCHS_PER_APLY_GRADIENTS*200,\r\n        validation_data=val_ds,\r\n        validation_steps=ds.val_images//BATCH_SIZE,\r\n    )\r\n```\r\n\r\n> MCN.py:\r\n\r\n```\r\nimport math\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass Swish(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Swish, self).__init__()\r\n        self.weight = self.add_weight(initializer='uniform',trainable=True)\r\n\r\n    def __call__(self, inputs):\r\n        return inputs+tf.sigmoid(self.weight*inputs)\r\n\r\n\r\nclass Conv(keras.Model):\r\n    def __init__(self,filters,kernel_size=1,strides=1,padding='valid'):\r\n        super(Conv, self).__init__()\r\n        self.conv = keras.layers.Conv2D(filters,kernel_size,strides,padding)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        return self.ac(self.bn(self.conv(inputs)))\r\n\r\n\r\nclass SEBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(SEBlock, self).__init__()\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,1,1)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(tf.reduce_mean(inputs,[1,2],keepdims=True))))\r\n        return self.ac(self.bn(tf.sigmoid(x)*inputs))\r\n\r\n\r\nclass ResBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(ResBlock, self).__init__()\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,3,1,'same')\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n\r\n    def __call__(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(inputs)))\r\n        return self.ac(self.bn(inputs+x))\r\n\r\ndef mcn_520(width, growth,input_shape=[256,256,3]):\r\n    fs = int(width*growth)\r\n    inputs=keras.layers.Input(input_shape)\r\n    x=keras.layers.Conv2D(fs,8,2)(inputs)\r\n    x=keras.layers.MaxPool2D(2)(x)\r\n    x1=Conv(fs//width)(SEBlock(fs)(x))\r\n    x2=Conv(fs//width)(ResBlock(fs)(x))\r\n    for i, depth in enumerate([2, 3, 5, 4]):\r\n        for _ in range(int(6*depth)):\r\n            fs+=int(math.sqrt(fs*width))\r\n            t=keras.layers.Concatenate()([x,x1,x2])\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            t=Conv(fs//width, 1, 1)(t)\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            x1=SEBlock(fs//width)(t)\r\n            x2=ResBlock(fs//width)(t)\r\n            t=keras.layers.Concatenate()([t,x1,x2])\r\n            t=keras.layers.Dropout(0.25)(t)\r\n            t=Conv(growth,1,1)(t)\r\n            x=keras.layers.Concatenate()([x,t])\r\n        if i != 3:\r\n            fs //= 2\r\n            x=keras.layers.MaxPool2D(2)(Conv(fs)(x))\r\n            x1=keras.layers.MaxPool2D(2)(Conv(fs//width)(x1))\r\n            x2=keras.layers.MaxPool2D(2)(Conv(fs//width)(x2))\r\n    x=keras.layers.GlobalMaxPool2D()(x)\r\n    x=keras.layers.Dropout(0.25)(x)\r\n    outputs=keras.layers.Dense(1000,activation='softmax')(x)\r\n    return keras.Model(inputs=inputs,outputs=outputs,name='MCN520')\r\n```\r\n**Other info / logs**\r\n\r\n> Train for 10000 steps, validate for 2500 steps\r\n> Epoch 1/1000000\r\n> 2019-11-27 07:06:16.642667: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.\r\n> 2019-11-27 07:06:27.657015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n> 2019-11-27 07:06:28.144415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> ", "comments": ["Also met with the same warning with distributed strategy. The model I used is an encoder-decoder UNet++ architecture. The warning only shows up when I connect two UNet++ models together. My guess is the graph compile time is just too long for such sophisticated model that it exceed the predefined time. Is there anyway to bypass this?", "Yeah it looks like grappler is taking too long and timing out for the big graph - and is likely skipping some graph optimizations. \r\n@HLSS-Hen when you say it is slow, can you provide some numbers? is the time to start first step slow or is the actual training slow? Can you provide numbers for 1 GPU (No distribution) vs 2 GPUs with mirrored strategy? \r\n", "@ChunHsinWang  I think so. But I don't known anyways to bypass this. ", "@guptapriya ok, in my test. run without distribution is a little faster than 2GPUs with mirrored startegy. And there is no warning. It direct start train with  fewer time wait.", "I build tf2.1, and it seems repaired.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34637\">No</a>\n", "I use tensorflow 2.0-GPU, nivida-tesla V100, 32G, gru model\r\n\r\nI also get the same warning and the process do not move for two days.\r\nMy code  is :\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\nfrom scipy import sparse\r\n\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\n\r\ntf.random.set_seed(22)\r\nnp.random.seed(22) \r\nassert tf.__version__.startswith('2.')\r\n\r\nbatchsz = 256\r\n\r\n# the most frequest words\r\ntotal_words = 4096\r\nmax_review_len = 4995\r\nembedding_len = 100\r\n\r\nmatrixfile = \"my matrix\"\r\ntargetfile = \"my label\"\r\n\r\nallmatrix = sparse.load_npz(matrixfile).toarray()\r\ntarget = np.loadtxt(targetfile)\r\nprint(\"allmatrix shape: {}\uff1btarget shape: {}\".format(allmatrix.shape, target.shape))\r\n\r\nx = tf.convert_to_tensor(allmatrix, dtype=tf.float32)\r\ny = tf.convert_to_tensor(target, dtype=tf.int32)\r\nidx = tf.range(allmatrix.shape[0])\r\nidx = tf.random.shuffle(idx)\r\nx_train, y_train = tf.gather(x, idx[:int(0.7 * len(idx))]), tf.gather(y,idx[:int(0.7 * len(idx))])\r\nx_val, y_val = tf.gather(x, idx[int(0.7 * len(idx)):int(0.8 * len(idx))]), tf.gather(y, idx[int(0.7 * len(idx)):int(0.8 * len(idx))])\r\nx_test, y_test = tf.gather(x, idx[int(0.8 * len(idx)):]), tf.gather(y, idx[int(0.8 * len(idx)):])\r\n\r\n\r\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ndb_train = db.shuffle(6000).batch(batchsz, drop_remainder=True).repeat()\r\ndb_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\ndb_val = ds_val.batch(batchsz, drop_remainder=True)\r\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\ndb_test = db_test.batch(batchsz, drop_remainder=True)\r\n\r\n\r\nclass MyRNN(keras.Model):\r\n\r\n    def __init__(self, units):\r\n        super(MyRNN, self).__init__()\r\n\r\n\r\n       \r\n        self.embedding = layers.Embedding(total_words, embedding_len,\r\n                                          input_length=max_review_len)\r\n\r\n\r\n        self.rnn = keras.Sequential([\r\n            layers.GRU(units, dropout=0.5, return_sequences=True, unroll=True),\r\n            layers.GRU(units, dropout=0.5, unroll=True)\r\n        ])\r\n\r\n\r\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\r\n        self.outlayer = layers.Dense(1)\r\n\r\n    def call(self, inputs, training=None):\r\n        # [b, 80]\r\n        x = inputs\r\n        # embedding: [b, 80] => [b, 80, 100]\r\n        x = self.embedding(x)\r\n        # rnn cell compute\r\n        # x: [b, 80, 100] => [b, 64]\r\n        x = self.rnn(x, training=training)\r\n\r\n        # out: [b, 64] => [b, 1]\r\n        x = self.outlayer(x)\r\n        # p(y is pos|x)\r\n        prob = tf.sigmoid(x)\r\n\r\n        return prob\r\n\r\ndef main():\r\n    units = 64\r\n    epochs = 100\r\n\r\n    import time\r\n\r\n\r\n    t0 = time.time()\r\n    model = MyRNN(units)\r\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\r\n                  loss = tf.losses.BinaryCrossentropy(),\r\n                  metrics=['accuracy'])\r\n\r\n    model.fit(db_train, epochs=epochs, validation_data=db_val)\r\n\r\n    model.evaluate(db_test)\r\n\r\n\r\n    t1 = time.time()\r\n   \r\n    print('total time cost:', t1-t0)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nthe warning is:\r\n```\r\n2020-01-24 16:19:09.414217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-24 16:19:09.416491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-24 16:19:09.416531: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-24 16:19:09.418195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-24 16:19:09.418215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-01-24 16:19:09.418226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-01-24 16:19:09.421327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30555 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8e:00.0, compute capability: 7.0)\r\nEpoch 1/100\r\n2020-01-24 16:41:16.523636: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.\r\n```\r\n\r\nhas anyone know the solution ?", "@Xiaohui-Z try use tf2.1. Or, add `get_confgi()` function to your `MyRNN` class.\r\nIn my try, custom Model without `get_config()` function in TF2.1 will have the WARNING  -- `WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ` . \r\nI don't know if it matters", "> @Xiaohui-Z try use tf2.1. Or, add `get_confgi()` function to your `MyRNN` class.\r\n> In my try, custom Model without `get_config()` function in TF2.1 will have the WARNING -- `WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ` .\r\n> I don't know if it matters\r\n\r\n@HLSS-Hen \uff0cHi Hen, Thanks. Can you show me how to add `get_config()` to the MyRNN class. I think get_config() just give you a dictionary containing the configuration of the model and only be used when you save a model, How can it affect my current model ?Thanks", "@Xiaohui-Z  `get_confge()` is to tell keras what your parameters you defined in `__init__()`.\r\nHere are my codes:\r\n```\r\nclass Swish(keras.layers.Layer):\r\n    def __init__(self):\r\n        super(Swish, self).__init__() # must have this\r\n\r\n    def build(self,input_shape):\r\n        # if add_weight() function set in __init__(),\r\n        # tensor's name may can't to auto rename,\r\n        # it will cause error when save model or model's weight.\r\n        self.kernel = self.add_weight(name='kernel',trainable=True)\r\n        super(Swish,self).build(input_shape) # must have this\r\n\r\n    def get_config(self):\r\n        # for Layer,get_config() must have base config in 'super(CLASS_NAME,self).get_config()'\r\n        # if your custom layer have other parameters,\r\n        # you need to add then to the dictionary.\r\n        return super(Swish,self).get_config()\r\n\r\n    def call(self, inputs):\r\n        return inputs+tf.sigmoid(self.kernel*inputs)a\r\n```\r\n```\r\nclass SEBlock(keras.Model):\r\n    def __init__(self, filters):\r\n        super(SEBlock, self).__init__() # must have this\r\n        self.conv0 = keras.layers.Conv2D(filters//4,1,1)\r\n        self.drop = keras.layers.Dropout(0.25)\r\n        self.conv1 = keras.layers.Conv2D(filters,1,1)\r\n        self.bn = keras.layers.BatchNormalization()\r\n        self.ac = Swish()\r\n        \r\n    def get_config(self):\r\n        # I tried to use 'supper().get_config()',\r\n        # but it seems not have this function.\r\n        return {'filters':self.conv1.get_config()['filters']}\r\n\r\n    def call(self,inputs):\r\n        x = self.conv1(self.drop(self.conv0(tf.reduce_mean(inputs,[1,2],keepdims=True))))\r\n        return self.ac(self.bn(tf.sigmoid(x)*inputs))\r\n```", "I am having the same issue. If anyone has any ideas on how to go about diagnosing (or fixing) the problem I would be extremely grateful\r\n\r\nI have two gigabyte servers with 8 K20X GPUS. They are identical in pretty much every way. With the same model launched on both one of them executes fine with distribution enabled (~1.5s per epoch) the other has the optimization timeout (~6.5s per epoch).\r\n\r\nSystem Details:\r\n- 2 X Intel E5-2650\r\n- 8 X NVIDIA K20X\r\n- 2 1TB SATA SSD configured in RAID 0\r\n- 128GB DDR3 memory\r\n- 256GB Swap\r\n- Ubuntu 18.04.4\r\n\r\nSoftware Details\r\n- Python 3.6\r\n- TF 2.1.0-2.2.0\r\n\r\nI have attempted the following actions with no success:\r\n- Re-install CUDA 10.1 from scratch\r\n- Re-install TF 2.2.0 from binary\r\n- Re-install TF 2.1.0 from binary\r\n- Build TF 2.2.0 from source", "@pbatk Can you show more about your model? In my experience, This warning may caused by too large model or custom layers don't have `get_config` function.", " @HLSS-Hen Wow! Talk about a quick response! Thanks for the effort!\r\n\r\nUnfortunately I am not allowed to share the model as it is proprietary and the legal blah, blah, blah\r\n\r\nWith that said I can say that is is very similar to a Res-Net50 and it runs successfully on one of the two identical machines. Sorry, I know this doesn't help much or answer you question. I am in a tough spot because I can't disclose too much info but everyone around here is scratching their head on this one", "@pbatk In fact, I have no experience with muti-machine training. I suggest you:\r\nTry use tensorboard to find out which operation use to much time. \r\nTry check your `tf.distribute.xxxx`.\r\nTry check your dataset.\r\nSee these web: [tf.distrivute.experimental](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental), [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training), [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), [Multi-worker training with Estimator](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator)\r\nYou may need to creat a new issue to ask someone have more muti-machine training experiences.", "@HLSS-Hen I'll likely create my own issue. I very much appreciate your effort though! Thank you!", "I have the same issue with tf  2.2.0 (with MirroredStrategy or without MirroredStrategy). If anyone has any ideas on how to fix the problem I would be very grateful (Number of inputs to my model is a few millions).", "@hosseinece You may try use tf2.3.1 or other new version. \r\nIf you use `tf.keras`, try to use `tf.keras.layers` but not comput ops in `tf`,  make sure that your custom layers/blocks have `get_config` method. All these will help to build the GPU graph.\r\nOthers, your inputs seems to be too large. Use small inputs to start training first.\r\n\r\nIn fact, my dual 2080ti with nvlink wasn't normal for a while, until I re install these cards and nvlink. In that time, tf call cudnn will use a long time, make all UI/X-windows error, and `nvidia-smi` will show no cards. I'm not sure if this is a bug to NVIDIA or my mother board(Tachi X570)."]}]