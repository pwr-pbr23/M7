[{"number": 8542, "title": "Modified the file mnist.py to allow for users to set the random seed.", "body": "Modified the file mnist.py to allow for users to set the random seed, either based on the graph-level seed via tf.set_random_seed or an op-level seed via a call like:\r\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, seed=12345).\r\n\r\nThis should allow users to get results that are repeatable during development and testing, especially for those becoming familiar with Tensorflow via the MNIST tutorials.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I am the only committer (who also signed the CLA).  Not sure how to fix the blocking by the googlebot.", "Created an updated pull request with CLA issue fixed."]}, {"number": 8541, "title": "Modified mnist.py to utilize random seed settings.", "body": "Modified the file mnist.py to allow for users to set the random seed, either based on the graph-level seed via tf.set_random_seed or an op-level seed via a call like:\r\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, seed=12345).\r\n\r\nThis should allow users to get results that are repeatable during development and testing, especially for those becoming familiar with Tensorflow via the MNIST tutorials.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Trying to fix issue with submission names and email addresses to match CLA."]}, {"number": 8540, "title": "Added random seed setting to mnist.py (submission email corrected)", "body": "Modified the file mnist.py to allow for users to set the random seed, either based on the graph-level seed via tf.set_random_seed or an op-level seed via a call like:\r\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, seed=12345).\r\n\r\nThis should allow users to get results that are repeatable during development and testing, especially for those becoming familiar with Tensorflow via the MNIST tutorials.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->"]}, {"number": 8539, "title": "Added random seed setting to mnist.py.", "body": "Modified the file mnist.py to allow for users to set the random seed, either based on the graph-level seed via tf.set_random_seed or an op-level seed via a call like: \r\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True, seed=12345).\r\n\r\nThis should allow users to get results that are repeatable during development and testing, especially for those becoming familiar with Tensorflow via the MNIST tutorials.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->"]}, {"number": 8538, "title": "fix runtime link error--gettime", "body": "fix runtime link error--gettime", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "You will need to sign the CLA for us to be able to accept your change.\r\n\r\nAlso, do you have more information on the failure?\r\nmaybe also reproduction instructions, and a unittest to reproduce the problem?\r\nOur CI is happy with the current flags, so I am not sure about accepting this change into our repo.", "May be just rebuild tensorflow with `--linkopt='-lrt' `, more see:\r\nhttps://github.com/wangyum/Anaconda/blob/master/doc/installing-tensorflow-from-sources.md#package-tensorflow", "Closing due to lack of activity.\r\nPlease sign the Google CLA as described above if you would like to send the PR again."]}, {"number": 8537, "title": "Fixed `needs_broadcasting` expression in tf.layers.normalization", "body": "in Python3, `range()` function doesn't create lists, so `[0, 1, 2] != range(4)[:-1]` always returns `True`.", "comments": ["Can one of the admins verify this patch?", "Thanks for this fix. Can you add a test that would have failed? Clearly this code isn't tested well.", "I am sorry for a miscellaneous pull-request with no test code.\r\nI just pointed out a programming mistake.\r\n\r\nI read the code and checked its behavior, but in `normalization.py`,` axis` in initializer is a single integer and `len(reduction_axes)` is always equal to `len(input_shape) - 1`, so the calculation results will be exactly the same regardless of the value of `needs_broadcast`...\r\nI'm wondering if this variable is really necessary.", "Jenkins, test this please.", "Fine. This is a clear improvement. Thanks!"]}, {"number": 8536, "title": "pool allocator spending more than 18h to allocate and still not finished", "body": "Hey guys, I've been having this issue which I don't really understand if it's related to memory issues or not. I've tested it on cpu and the code runs fine without any issues. On gpu I get the following messages:\r\n\r\n```\r\n2017-03-18 14:21:51,379 INFO - I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\n2017-03-18 14:21:51,379 INFO - name: Tesla K80\r\n2017-03-18 14:21:51,379 INFO - major: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\n2017-03-18 14:21:51,380 INFO - pciBusID 0000:00:1e.0\r\n2017-03-18 14:21:51,380 INFO - Total memory: 11.17GiB\r\n2017-03-18 14:21:51,380 INFO - Free memory: 11.11GiB\r\n2017-03-18 14:21:51,381 INFO -  DMA: 0\r\n2017-03-18 14:21:51,381 INFO -  Y\r\n2017-03-18 14:21:51,394 INFO - Creating TensorFlow device (/gpu:0) -> \r\n(device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0) 2017-03-18 14:22:06,005\r\nPoolAllocator: After 2898 get requests, put_count=2334 evicted_count=1000 \r\neviction_rate=0.428449 and unsatisfied allocation rate=0.574189\r\n2017-03-18 14:22:06,025 INFO - Raising pool_size_limit_ from 100 to 110\r\n2017-03-18 14:48:17,797 INFO -  PoolAllocator: After 2898 get requests, put_count=3604\r\n evicted_count=2000 eviction_rate=0.554939 and unsatisfied allocation rate=0.452381\r\n2017-03-18 14:48:17,800 INFO - Raising pool_size_limit_ from 193 to 212\r\n2017-03-18 15:30:38,803 INFO - PoolAllocator: After 2898 get requests, put_count=2881 \r\nevicted_count=1000 eviction_rate=0.347102 and unsatisfied allocation rate=0.364734\r\n2017-03-18 15:30:38,806 INFO - Raising pool_size_limit_ from 449 to 493\r\n2017-03-18 16:46:39,551 INFO - PoolAllocator: After 8694 get requests, put_count=8682 \r\nevicted_count=1000 eviction_rate=0.115181 and unsatisfied allocation rate=0.128479\r\n2017-03-18 16:46:39,554 INFO - Raising pool_size_limit_ from 1158 to 1273\r\n```\r\nIt's been like this for 18h and still no output or training has started, is this a known issue. Why does it take so long for the memory allocator to finish?", "comments": ["Are you sure (1) your training hasn't started, and (2) is it not a memory problem in your model?\r\n\r\nFor (1), check if there's any event/checkpoint files being outputted.  For (2), check your input pipeline, etc.\r\n\r\nA relevant SO question and answer that explains what the pool allocator messages mean:  http://stackoverflow.com/questions/35151207/how-to-interpret-poolallocator-messages-in-tensorflow", "@concretevitamin \r\n(1) yes training hasn't started since I output information regarding epochs and batch_size num out of total training length, accuracy, loss, ... etc.\r\n(2) I am assuming there's not a memory problem since on cpu it runs fine and starts training rather quickly, so I can see the progress plus when running on cpu the allocated RAM doesn't go beyond 3GB.", "@kirk86 can you collect a stack trace from the TensorFlow process while it's hanging? You can use gdb like this:\r\n```\r\n$ gdb\r\n(gdb) attach <tensorflow process id>\r\n```\r\nIf gdb gives you a weird message about ptrace, run the following in a terminal and try attaching again:\r\n`$ echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope`\r\n\r\nOnce you're attached in gdb, collect stack traces with all the threads:\r\n`(gdb) thread apply all bt`", "@skye Sure I'll give it a try, can't give no promises though since I have no control over the environment. It runs inside in a docker container (as far as I know) which is being created and destroyed every time I execute the code. Think of it sth equivalent of google cloud. Let me see what I can collect and I'll get back to you guys. Thanks for the advice.", "@skye Hi, may I ask for a clarification. So I'm running the code inside a multiplexer session and I try to follow your example. When I run the code since it runs on multicore machine it creates pid's for each core. Which one should I choose according to you example?\r\n\r\nWhen I gdb on one of the pid's then I only get information about reading symbols and loaded symbols is that the expected result?\r\n\r\nAn example is the following:\r\n```\r\nLoaded symbols for /home/user/miniconda2/envs/tf-1/lib/python2.7/site-packages/pandas/msgpack/_unpacker.so\r\nReading symbols from /home/user/miniconda2/envs/tf-1/lib/python2.7/site-packages/pandas/util/_move.so...done.\r\nLoaded symbols for /home/user/miniconda2/envs/tf-1/lib/python2.7/site-packages/pandas/util/_move.so\r\nReading symbols from /home/user/miniconda2/envs/tf-1/lib/python2.7/site-packages/pandas/_testing.so...done.\r\nLoaded symbols for /home/user/miniconda2/envs/tf-1/lib/python2.7/site-packages/pandas/_testing.so\r\n0x0000003f67a0d930 in sem_wait () from /lib64/libpthread.so.0\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.12-1.149.el6_6.7.x86_64 libuuid-2.17.2-12.18.el6.x86_64\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n```", "Actually, looking at your log again, it's not clear to me that the problem is in the allocator. It just happens that the last LOG message was from there, but it could be hanging anywhere.\r\n\r\nCould you provide the information asked for in the new issue template, like a repro script for the hang? Thanks.", "@skye hello, so I've created a gist with an example https://gist.github.com/kirk86/99211bd0ea3489915738701285c58c30. The example works fine on cpu but on a gpu with 12GB ram it gives me error exhausted message even when I reduce the batch_size to 1.\r\n\r\nHere's the output message that I get:\r\n\r\n```\r\nResourceExhaustedErrorTraceback (most recent call last)\r\n/output/run.py in <module>()\r\n     40     model.fit(trX, [np.expand_dims(trX_mask[:, :, :, 0], axis=3), trY], batch_size=1, nb_epoch=40,\r\n     41               shuffle=True, validation_data=(valX, [np.expand_dims(valX_mask[:, :, :, 0], axis=3), valY]),\r\n---> 42               verbose=1)\r\n     43     # classes = {0: 'ALB', 1: 'BET', 2: 'DOL', 3: 'LAG', 4: 'NoF',\r\n     44     #            5: 'OTHER', 6: 'SHARK', 7: 'YFT'}\r\n\r\n/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\r\n   1194                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\r\n   1195                               callback_metrics=callback_metrics,\r\n-> 1196                               initial_epoch=initial_epoch)\r\n   1197 \r\n   1198     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\r\n\r\n/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc in _fit_loop(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\r\n    889                 batch_logs['size'] = len(batch_ids)\r\n    890                 callbacks.on_batch_begin(batch_index, batch_logs)\r\n--> 891                 outs = f(ins_batch)\r\n    892                 if not isinstance(outs, list):\r\n    893                     outs = [outs]\r\n\r\n/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc in __call__(self, inputs)\r\n   1939                 value = (indices, sparse_coo.data, sparse_coo.shape)\r\n   1940             feed_dict[tensor] = value\r\n-> 1941         session = get_session()\r\n   1942         updated = session.run(self.outputs + [self.updates_op],\r\n   1943                               feed_dict=feed_dict)\r\n\r\n/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc in get_session()\r\n    123         session = _SESSION\r\n    124     if not _MANUAL_VAR_INIT:\r\n--> 125         _initialize_variables()\r\n    126     return session\r\n    127 \r\n\r\n/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc in _initialize_variables()\r\n    280         sess = get_session()\r\n    281         if hasattr(tf, 'variables_initializer'):\r\n--> 282             sess.run(tf.variables_initializer(uninitialized_variables))\r\n    283         else:\r\n    284             sess.run(tf.initialize_variables(uninitialized_variables))\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1033         except KeyError:\r\n   1034           pass\r\n-> 1035       raise type(e)(node_def, op, message)\r\n   1036 \r\n   1037   def _extend_graph(self):\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[8388608,64]\r\n\t [[Node: random_uniform_28 = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_uniform_28/mul, random_uniform_28/min)]]\r\n\r\nCaused by op u'random_uniform_28', defined at:\r\n  File \"/usr/local/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/local/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/usr/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-285366c3fbbf>\", line 1, in <module>\r\n    get_ipython().magic(u'run -i run.py')\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2158, in magic\r\n    return self.run_line_magic(magic_name, magic_arg_s)\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2079, in run_line_magic\r\n    result = fn(*args,**kwargs)\r\n  File \"<decorator-gen-57>\", line 2, in run\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/magic.py\", line 188, in <lambda>\r\n    call = lambda f, *a, **k: f(*a, **k)\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.py\", line 742, in run\r\n    run()\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.py\", line 728, in run\r\n    exit_ignore=exit_ignore)\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2481, in safe_execfile\r\n    self.compile if kw['shell_futures'] else None)\r\n  File \"/usr/local/lib/python2.7/site-packages/IPython/utils/py3compat.py\", line 289, in execfile\r\n    builtin_mod.execfile(filename, *where)\r\n  File \"/output/run.py\", line 39, in <module>\r\n    model = unet((512, 512, 3))\r\n  File \"/output/unet.py\", line 203, in unet\r\n    dense1 = Dense(64, activation='relu', name='fc1')(flat)\r\n  File \"/usr/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 546, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/usr/local/lib/python2.7/site-packages/keras/layers/core.py\", line 798, in build\r\n    constraint=self.W_constraint)\r\n  File \"/usr/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 418, in add_weight\r\n    weight = initializer(shape, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/keras/initializations.py\", line 66, in glorot_uniform\r\n    return uniform(shape, s, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/keras/initializations.py\", line 33, in uniform\r\n    return K.random_uniform_variable(shape, -scale, scale, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 634, in random_uniform_variable\r\n    low, high, dtype=tf_dtype, seed=seed)(shape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 180, in __call__\r\n    dtype, seed=self.seed)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 246, in random_uniform\r\n    return math_ops.add(rnd * (maxval - minval), minval, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 73, in add\r\n    result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[8388608,64]\r\n\t [[Node: random_uniform_28 = Add[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_uniform_28/mul, random_uniform_28/min)]]\r\n```", "This seems like a different problem than the hang you originally posted, no? Perhaps there's some other application using the GPU's memory, the `nvidia_smi` command will tell you which processes are using how much memory. I can try to repro once I have access to a beefy GPU.", "@skye thanks a lot. Yes you're right this was a bit different in terms of environment. I'll also try to provide more info about the first original problem. Didn't had the time or the resources available. I'll try it over the weekend.", "@kirk86 Hey, I'm facing a similar issue to your first original problem. It takes hours and is not going any further. Any leads on this? Thanks", "@Peri044 Hey, have you checked reducing the batch size. Does it happen the same even with batch size of 1?"]}, {"number": 8535, "title": "Feature request: add support for float16/float64 to tf.contrib.layers.batch_norm()", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n`tf.contrib.layers.batch_norm` does not support `float16` due to defaulting to `dtype=tf.float32` for `get_variable()`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/python/layers/normalization.py#L141\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n- Ubuntu 16.04 Docker container\r\n- Ubuntu 16.10 Docker host\r\n- Dockerfile base `nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04`\r\n- CUDA 8.0.61\r\n- CUDNN 5.1.10\r\n- NVidia driver version 378.13\r\n\r\n```sh\r\n$ uname -a\r\nLinux 97fca57d7bb6 4.8.0-41-generic #44-Ubuntu SMP Fri Mar 3 15:27:17 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root    556000 Jan 26 23:48 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root    415432 Jan 26 23:48 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root    775162 Jan 26 23:48 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 jake users       13 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 jake users       18 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 jake users 84163560 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 jake users 70364814 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n$ conda -V\r\nconda 4.3.14\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n```sh\r\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1\r\n```\r\n\r\nConda `environment.yml`:\r\n\r\n```yaml\r\nname: root\r\n\r\nchannels:\r\n- defaults\r\n- conda-forge\r\n- menpo\r\n\r\ndependencies:\r\n- ipyparallel==5.2.0\r\n- opencv3=3.1.0\r\n- pip:\r\n  - keras==1.2.2\r\n  - pydicom==0.9.9\r\n  - tensorflow-gpu==1.0.1\r\n  - tflearn==0.3\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers import batch_norm\r\n# from normalization import batch_norm\r\nwith tf.Session() as sess, tf.device('/cpu'):\r\n  inputs = tf.convert_to_tensor([1., 2.], dtype=tf.float16)\r\n  norm = batch_norm(inputs)\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(norm)\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-f094f32eca33> in <module>()\r\n      1 a = tf.convert_to_tensor([1., 2.], dtype=tf.float16)\r\n----> 2 tf.contrib.layers.batch_norm(a).eval()\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py in func_with_args(*args, **kwargs)\r\n    175       current_args = current_scope[key_func].copy()\r\n    176       current_args.update(kwargs)\r\n--> 177     return func(*args, **current_args)\r\n    178   _add_op(func)\r\n    179   setattr(func_with_args, '_key_op', _key_op(func))\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py in batch_norm(inputs, decay, center, scale, epsilon, activation_fn, param_initializers, updates_collections, is_training, reuse, variables_collections, outputs_collections, trainable, batch_weights, fused, data_format, zero_debias_moving_mean, scope)\r\n    516           _scope=sc,\r\n    517           _reuse=reuse)\r\n--> 518       outputs = layer.apply(inputs, training=is_training)\r\n    519 \r\n    520       # Add variables to collections.\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, **kwargs)\r\n    301       Output tensor(s).\r\n    302     \"\"\"\r\n--> 303     return self.__call__(inputs, **kwargs)\r\n    304 \r\n    305 \r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, **kwargs)\r\n    271             self.build(input_shapes)\r\n    272           self._built = True\r\n--> 273         outputs = self.call(inputs, **kwargs)\r\n    274 \r\n    275         # Apply activity regularization.\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/normalization.py in call(self, inputs, training)\r\n    191       if not self.updates:\r\n    192         mean_update = moving_averages.assign_moving_average(\r\n--> 193             self.moving_mean, mean, self.momentum, zero_debias=False)\r\n    194         variance_update = moving_averages.assign_moving_average(\r\n    195             self.moving_variance, variance, self.momentum, zero_debias=False)\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)\r\n     70         update_delta = _zero_debias(variable, value, decay)\r\n     71       else:\r\n---> 72         update_delta = (variable - value) * decay\r\n     73       return state_ops.assign_sub(variable, update_delta, name=scope)\r\n     74 \r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in _run_op(a, *args)\r\n    675     def _run_op(a, *args):\r\n    676       # pylint: disable=protected-access\r\n--> 677       return getattr(ops.Tensor, operator)(a._AsTensor(), *args)\r\n    678     # Propagate __doc__ to wrapper\r\n    679     try:\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n    791     with ops.name_scope(None, op_name, [x, y]) as name:\r\n    792       if not isinstance(y, sparse_tensor.SparseTensor):\r\n--> 793         y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n    794       return func(x, y, name=name)\r\n    795 \r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    635       name=name,\r\n    636       preferred_dtype=preferred_dtype,\r\n--> 637       as_ref=False)\r\n    638 \r\n    639 \r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    700 \r\n    701         if ret is None:\r\n--> 702           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    703 \r\n    704         if ret is NotImplemented:\r\n\r\n/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)\r\n    573     raise ValueError(\r\n    574         \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\r\n--> 575         % (dtype.name, t.dtype.name, str(t)))\r\n    576   return t\r\n    577 \r\n\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: 'Tensor(\"BatchNorm_2/Reshape_1:0\", shape=(2,), dtype=float16)'\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nUse a modified `tensorflow/python/layers/normalization.py` which passes `dtype=inputs.dtype.base_dtype`.\r\n", "comments": ["/cc @zhangyaobit - could you comment on this?  Do we have plans to support this?", "I'm having trouble building Tensorflow with a patched `normalization.py` file. After uninstalling Tensorflow and installing the new build, I still get the error above. If I copy the file to the current directory and import `batch_norm` from that module, it runs without the error. Is there another step I'm missing? I'm following the [Installing TensorFlow from Sources](https://www.tensorflow.org/install/install_sources) instructions.\r\n\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers import batch_norm  # error on run\r\n# from normalization import batch_norm  ## no error on run\r\nwith tf.Session() as sess, tf.device('/cpu'):\r\n  inputs = tf.convert_to_tensor([1., 2.], dtype=tf.float16)\r\n  norm = batch_norm(inputs)\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(norm)\r\n```\r\n\r\nUpdate: There were 2 entry points!", "By default, you are using non-fused batch norm, but looks like type conversion from float32 to float16 is not supported for it yet. There is a faster version of batch norm, which could be turned on by setting fused=True. Although its code is templatized by [type](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L47), we currently only have [a float32 instantiation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L621); it would be nice to add support for both float16 and double.\r\n\r\nI marked this as contribution welcome for (1) type conversion for non-fused, default batch norm, and (2) float16 and double support for fused batch norm.\r\n", "@zhangyaobit, I am new to contributing to other projects. Can I work on this issue?", "@narendravardi, yes, that would be great. Please start with: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\r\n\r\nAre you interested in contributing for (1) or (2) or both?", "@narendravardi I have a patch ready for (1), running and updating unit tests now. I would be happy to help with you with (2) if you're interested!", "There seems to be an issue for supporting `float16` concerning moment calculation. Non-fused BatchNorm uses `moments()` internally and `moments()` has this caveat. Except from source code follows.\r\n\r\n```py\r\n    # The dynamic range of fp16 is too limited to support the collection of\r\n    # sufficient statistics. As a workaround we simply perform the operations\r\n    # on 32-bit floats before converting the mean and variance back to fp16\r\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\r\n\r\n\r\nRunning `layers_test.py` with support for `float16` and `float32` BatchNorm resulted in the following numerical errors. The `float16` moving mean values are incorrect.\r\n\r\n@narendravardi What do you think? Does the fused implementation have the same limitations?\r\n\r\n```\r\n======================================================================\r\nFAIL: testTrainMovingVarsNCHW (__main__.BatchNormFloat16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2549, in testTrainMovingVarsNCHW\r\n    self._testTrainMovingVars(False, data_format='NCHW')\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\r\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\r\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.146973,  0.129639,  0.099976,  0.183716,  0.116333,  0.225464,\r\n        0.120422,  0.21936 ,  0.140869,  0.147949,  0.140869,  0.156128,\r\n        0.210205,  0.21936 ,  0.082642,  0.183716,  0.121399,  0.022446,...\r\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\r\n\r\n======================================================================\r\nFAIL: testTrainMovingVarsNHWC (__main__.BatchNormFloat16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2546, in testTrainMovingVarsNHWC\r\n    self._testTrainMovingVars(False, data_format='NHWC')\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\r\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\r\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.175537,  0.158203,  0.179565,  0.151001,  0.17041 ,  0.21936 ,\r\n        0.092834,  0.25415 ,  0.190796,  0.135742,  0.118347,  0.112244,\r\n        0.074463,  0.165283,  0.198975,  0.093872,  0.233643,  0.165283,...\r\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\r\n\r\n----------------------------------------------------------------------\r\nRan 462 tests in 8.531s\r\n\r\nFAILED (failures=2, skipped=39)\r\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\r\nnot close lhs =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\r\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\r\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\r\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\r\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\r\n  0.28051758  0.13879395]\r\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\r\nnot close dif =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\r\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\r\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\r\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\r\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\r\n  0.28051758  0.13879395]\r\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001]\r\ndtype = float16, shape = (32,)\r\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\r\nnot close lhs =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\r\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\r\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\r\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\r\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\r\n  0.16638184  0.24182129]\r\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\r\nnot close dif =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\r\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\r\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\r\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\r\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\r\n  0.16638184  0.24182129]\r\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001]\r\ndtype = float16, shape = (32,)\r\n```", "Full test output log: [layers_test.txt](https://github.com/tensorflow/tensorflow/files/856660/layers_test.txt)\r\n", "Friendly ping. Any updates on this?", "Adding @zheng-xq and @reedwm, who might have an update on the plan or ongoing work on this.", "Float16 support for fused batch norm is being worked on. Then tf.contrib.layers.batch_norm will support tf.float16 if fused=True is passed to it.\r\n\r\nNote the scale and and offset variables will still be stored as float32s, but the inputs and outputs to fused batch norm will have the option of being float16.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "fp16 support has been added. Marking as contribution welcomes for fp64 support.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@reedwm @zheng-xq \r\n`scale` and `offset` variables are still in float32 DT.  I am not very familiar with this code path, looking for a solution to overcome this as an end user. Pls. suggest.\r\n```\r\n> /opt/python3/cerebras-stable/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py(2285)_fused_batch_norm_v2()```\r\n```\r\n```\r\ninputs = tf.layers.batch_normalization( inputs=inputs, axis=3, momentum=_BTN_DECAY, epsilon=_BTN_EPSILON, center=True, scale=True, training=is_training, fused=True)\r\n```\r\n\r\nwhere input tensor is,\r\n```\r\n-> def batch_norm_relu(inputs, is_training, data_format):\r\n(Pdb) inputs\r\n<tf.Tensor 'initial_max_pool:0' shape=(64, 56, 56, 64) dtype=float16>```\r\n```\r\n\r\n(Pdb) \r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py\", line 754, in batch_normalization\r\n    return layer.apply(inputs, training=training)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/base.py\", line 729, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/base.py\", line 619, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py\", line 489, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py\", line 380, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/utils.py\", line 206, in smart_cond\r\n    return fn1()\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/layers/normalization.py\", line 366, in _fused_batch_norm_training\r\n    data_format=self._data_format)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/ops/nn_impl.py\", line 875, in fused_batch_norm\r\n    name=name)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2296, in _fused_batch_norm_v2\r\n    is_training=is_training, name=name)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"/opt/python3/aisuni-stable/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n\r\nTypeError: Value passed to parameter 'scale' has DataType float16 not in list of allowed values: float32```", "@aisuni, it is intended that `scale` and `offset` are still in float32, as recommended [by this guide](http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). You should not create thos variables in fp16 for effective training.", "Thanks, @reedwm  for very useful info. \r\nI have one follow-up, but may not be specific to this issue.  I have following code which works well with single precision (fp32 or fp16), however, the same will not work with mixed precision.  is it a good idea to cast the DT to fp32 at this stage? Can you pls. provide me some pointers here?\r\nI'll step out if I am deviating the flow (considering the scope of this issue)... \r\n```\r\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=net)\r\n            losfn = tf.reduce_mean(cross_entropy, name=\"celoss\")\r\n            celoss = lossfn(net, labels)\r\n            total_loss = None\r\n            if tf.GraphKeys.REGULARIZATION_LOSSES:\r\n                losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n                if losses:\r\n                    loss = tf.reduce_sum([tf.reduce_mean(reg_loss)\r\n                               for loss in losses],\r\n                               name='reg_loss_mean_sum')\r\n                    total_loss = celoss + loss\r\n                else:\r\n                    total_loss = celoss\r\n                if not total_loss:\r\n                    total_loss = celoss\r\n                optimization = tf.contrib.layers.optimize_loss(total_loss, global_step=tf.train.get_global_step(),\r\n                        clip_gradients=FLAGS.clip_grad, increment_global_step=True,**train_params)\r\n```", "@aisuni you should cast the regularization losses to fp32 before calling `tf.reduce_sum`. E.g.:\r\n\r\n```python\r\n                    loss = tf.reduce_sum([tf.reduce_mean(tf.cast(reg_loss, tf.float32))\r\n                               for loss in losses],\r\n                               name='reg_loss_mean_sum')\r\n```\r\n(Also, you should probably use `tf.add_n()` instead of `tf.reduce_sum()` in this case.)\r\n\r\nI recommend reading [this guide](http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) for more info, and asking on StackOverflow if you have more questions.", "@reedwm It that support of fp16 has been added to the main version like 1.6 or 1.5?", "@jackwangshanghai  evidently this support has been added in v1.5 and v1.6.", "The problem will be still present in tf.keras API for https://github.com/keras-team/keras/issues/9582 /cc @fchollet ", "Any news on this?", "The Keras issue only affects Keras, not `tf.keras`, so the TensorFlow repo isn't the best place to discuss this. Still, this should be fixed in Keras, so @fchollet, can you take a look?", "Has float64 been supported? \r\nWhen I used float64, it occurs: `` Value passed to parameter 'x' has DataType float64 not in list of allowed values: float32.``", "Unfortunately, fused batch norm does not support float64.", "This still seems to be broken for me using `tf.layers.batch_normalization` in 1.12.\r\n\r\n```\r\nimport tensorflow as tf\r\n# from normalization import batch_norm\r\nwith tf.Session() as sess, tf.device('/cpu'):\r\n  inputs = tf.convert_to_tensor([1., 2.], dtype=tf.float16)\r\n  norm = tf.layers.batch_normalization(inputs, fused=True)\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(norm)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\projects\\sorter\\dev\\test.py\", line 5, in <module>\r\n    norm = tf.layers.batch_normalization(inputs, fused=True)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 309, in batch_normalization\r\n    return layer.apply(inputs, training=training)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 154, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\", line 631, in call\r\n    self.epsilon)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 835, in batch_normalization\r\n    inv *= scale\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 869, in binary_op_wrapper\r\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1050, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\sqidd\\AppData\\Local\\conda\\conda\\envs\\sorter\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 828, in _TensorConversionFunction\r\n    \"of type '%s'\" % (dtype.name, v.dtype.name))\r\nValueError: Incompatible type conversion requested to type 'float16' for variable of type 'float32_ref'\r\n```\r\n\r\n@reedwm any comments?", "Same here. Problem using FP16 with tf.keras.layers.BatchNormalization. \r\n\r\nUsing TensorFlow 1.13.1 in Eager mode.\r\n\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #0(zero-based) was expected to be a float tensor but is a half tensor [Op:Mul] name: model/u_net_block/convolution3d/batch_normalization_v1/batchnorm/mul/`\r\n\r\n", "@JustASquid and @pldelisle , thank you for the bug reports. This has been fixed in 192b618b9d3f149c98e3b72dd872f5ecc4f79c67. This fix is not in TF 1.13, but will be in the next stable release.", "@reedwm If I compile \u2018master\u2019 will I have the fix? Is it merged?\r\n\r\nThanks !", "@pldelisle yes you will, it is merged.", "Closing out this issue, as `tf.contrib` is no longer supported in TensorFlow 2.x and the `tf.keras.layers.BatchNormalization` [fix](https://github.com/tensorflow/tensorflow/commit/192b618b9d3f149c98e3b72dd872f5ecc4f79c67) referenced above has been merged.\r\n\r\nThanks! \ud83d\ude42 "]}, {"number": 8534, "title": "Performance improvement for io::inputbuffer", "body": "Do not add chars to string one by one.\r\nIn most cases, buffer size >= max line length. \r\nLet's do it in one function call.\r\n", "comments": ["Can one of the admins verify this patch?", "For input like:\r\n\"aaa\\rbbb\\r\\n\"\r\n\r\nThe old code will output: \"aaabbb\"\r\nThis new code will output: \"aaa\\rbbb\"\r\n\r\nI think this behavior change is acceptable. Do you?\r\n", "Have you measured the performance improvement?\r\n\r\nIt certainly *looks* more efficient to me, but I'd like to weigh this against the cost of making a (probably irrelevant, but you never know...) backwards incompatible change.", "Working. I'll post the result later.", "input data: A text file in libsvm format, 10906667862 bytes, 164540416 lines, with CRLF line terminators\r\n\r\ntest code 1:\r\n```cpp\r\n#include <mutex>\r\n#include <Windows.h>\r\n#include <stdio.h>\r\n\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/io/inputbuffer.h\"\r\nusing namespace tensorflow;\r\n\r\nint test1(const char* filename) {\r\n\tLARGE_INTEGER li;\r\n\tQueryPerformanceFrequency(&li);\r\n\tEnv* env = Env::Default();\r\n\tstd::unique_ptr<RandomAccessFile> file_;\r\n\tconst ::tensorflow::Status status = env->NewRandomAccessFile(filename, &file_);\r\n\tif (!status.ok()) {\r\n\t\treturn -1;\r\n\t}\r\n\tLARGE_INTEGER startTime, endTime;\r\n\tint kBufferSize = 2 * 1024 * 1024;\r\n\tio::InputBuffer* input_buffer = new io::InputBuffer(file_.get(), kBufferSize);\r\n\tQueryPerformanceCounter(&startTime);\r\n\tstring line_contents;\r\n\twhile (true) {\r\n\t\tStatus status = input_buffer->ReadLine(&line_contents);\r\n\t\tif (errors::IsOutOfRange(status)) {\r\n\t\t\tbreak;\r\n\t\t}\r\n\t\tif (!status.ok()) {\r\n\t\t\treturn -1;\r\n\t\t}\r\n\t}\r\n\tQueryPerformanceCounter(&endTime);\r\n\tconst double seconds = ((endTime.QuadPart - startTime.QuadPart) / (double)li.QuadPart);\r\n\tprintf(\"dur = %g\\n\", seconds);\r\n\treturn 0;\r\n}\r\n```\r\ntest code 2:\r\nAlmost the same as test code 1, with only one line change:\r\n```\r\nwhile (true) {\r\n    string line_contents;  // this line is moved from the line before \"while(true)\"\r\n    Status status = input_buffer->ReadLine(&line_contents);\r\n}\r\n```\r\nResult:\r\n\r\n<table>\r\n<thead>\r\n<tr>\r\n<td>&nbsp;</td>\r\n<td>Before</td>\r\n<td>After</td>\r\n<td>Changes</td>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr>\r\n<td>test1</td>\r\n<td>67 seconds</td>\r\n<td>26 seconds</td>\r\n<td> - 61%</td>\r\n</tr>\r\n<tr>\r\n<td>test2(cache line_contents)</td>\r\n<td>34 seconds</td>\r\n<td>19 seconds</td>\r\n<td> - 44%</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n\r\n", "Great, thanks for confirming that. I'm satisfied with the change, so let's run the tests and merge if they pass.\r\n\r\n@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please."]}, {"number": 8533, "title": "Branch 150542283", "body": "", "comments": ["cc @ispirmustafa @xiejw @martinwicke for insights into the failures in python/estimator tests.\r\n\r\nSee log at:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/4123/consoleFull\r\n\r\nCould this have to do with the content of https://github.com/caisq/tensorflow/blob/branch_150542283/tensorflow/python/estimator/inputs/__init__.py ?\r\n\r\nFor example:\r\n```\r\nFAIL: //bazel_pip/tensorflow/python/estimator:estimator_test (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/bazel_pip/tensorflow/python/estimator/estimator_test/test.log).\r\nINFO: From Testing //bazel_pip/tensorflow/python/estimator:estimator_test:\r\n==================== Test output for //bazel_pip/tensorflow/python/estimator:estimator_test:\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/bazel_pip/tensorflow/python/estimator/estimator_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/estimator/estimator_test.py\", line 31, in <module>\r\n    from tensorflow.python.estimator.inputs import numpy_io\r\nImportError: cannot import name numpy_io\r\n```", "@caisq `numpy_io` needs to be imported in the init file and added to the allowed symbols. Or you can import `numpy_input_fn` instead of importing `numpy_io` in that test. ", "BTW @martinwicke I haven't dived into how `allowed_symbol` works yet but is there a way for external users to use some internal methods/classes that are not included in `allowed_symbols`?", "@terrytangyuan Thanks for the input. Yes, adding numpy_io, pandas_io and export to allowed_symbols will fix the tests. But I want to check with the tf-learn estimator team that this is the okay, because those symbols might be meant to be hidden from the public API.", "@terrytangyuan Didn't see the second part of your suggestion. Yes you are right - importing numpy_input_fn instead of numpy_io and so on should solve the problem. Will try the fix.", "There is a fix coming from internal. Closing the PR."]}, {"number": 8532, "title": "ImportError: cannot import name pywrap_tensorflow", "body": "After installation of TensorFlow by official tutorial, I occured these import error messages below:\r\n```Shell\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\n/home/yuens/Downloads/code/tensorflow/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/home/yuens/Downloads/code/tensorflow/tensorflow/python/__init__.py in <module>()\r\n     70 for some common reasons and solutions.  Include the entire stack trace\r\n     71 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 72   raise ImportError(msg)\r\n     73 \r\n     74 # Protocol buffers\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name pywrap_tensorflow\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n## Tried\r\nI can't open this page(404 error): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error", "comments": ["Take a look at https://www.tensorflow.org/versions/master/get_started/os_setup#import_error.  Please re-open if that doesn't work for you.  Thanks!", "@concretevitamin Dear, these messages shows after click:\r\n```Shell\r\nError: Not Found\r\n\r\nThe requested URL /versions/master/get_started/os_setup/index.html was not found on this server.\r\n```\r\n:sob:  Besides, my system is Ubuntu16.04 64bit", "I got the same error!", "This [issue](https://github.com/tensorflow/tensorflow/issues/7069#issuecomment-275216149) can help!"]}, {"number": 8531, "title": "Add support for non-scalar string tensors in java", "body": "Currently, string tensors are not supported in java:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L90\r\n\r\nImplementing this  would be very helpful in scenarios of training a model in one language, and serving from a different one, as it allows moving some of the logic to the tensor flow model.", "comments": ["/cc @asimshankar ", "I'm under the impression that someone is working on a contribution for this. I'll check in with them to see if they expect to send a PR anytime soon.", "I completed the implementation of the feature and will contribute it back soon.", "Thanks a lot for making this possible. Any idea when \"soon\" is?", "@erranli could you push your fix, or cut this issue loose?", "@quaeler : I had an earlier attempt at this than I can resurrect. I'll do that and hope to have something in the next few days.", "@asimshankar ok sounds good; if you want to pass the baton, i'm happy to do the resurrection as well.\r\n", "Thanks @ebrevdo - any reason not make this into a PR for tensorflow master?", "Please ignore my commit; this was written by @asimshankar and will show up on master soon.", "@caisq thank you for making the change. Curious when will the change be published to maven central?", "This is part of 1.4, so is available on Maven Central."]}, {"number": 8530, "title": "import error after installing tensorflow from source", "body": "i have build tensorflow from source and got a lot of warning but it was done without errors.But now when I import tensorflow it is giving the error mentioned below:\r\n\r\n`p_rakash@prakash008:~$ python3\r\nPython 3.5.2 |Anaconda 4.3.1 (64-bit)| (default, Jul  2 2016, 17:53:06) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name 'pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/prakash/anaconda3/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name 'pywrap_tensorflow'`_\r\n\r\nany suggestion..?\r\nI have a nvidia 720m gpu and python 3.5.2 from anaconda and I hav installed the gpu version of tensorflow.", "comments": ["Are you running Python from the TensorFlow source directory? See https://www.tensorflow.org/versions/master/get_started/os_setup#import_error", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8529, "title": "Compile Tensorflow 1.0 with GPU on RHEL6 / SL6", "body": "Dear all,\r\ni managed to compile Tensorflow 0.12.1 with GPU option few months ago on our computational grid based on SL6 but moving to version 1.0 is still an issue with trying to activate the GPU support.\r\nI tried to merge all the advise i could find here but still no success.\r\nSo here is a summary:\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThis issue is actually close to the ones as #7118, #2109, #2413, etc.\r\nMy compilation attempts always show similar error but dealing with different packages. Here is the last example:\r\n`ERROR: /gpfs/MUST-SHARE/univ_home/alben/.cache/bazel/_bazel_alben/07a9bf809031fe756469f64b1512dee9/external/grpc/BUILD:71:1: undeclared inclusion(s) in rule '@grpc//:gpr':\r\nthis rule is missing dependency declarations for the following files included by 'external/grpc/src/core/lib/support/thd_windows.c':\r\n  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdint.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 20.467s, Critical Path: 9.90s\r\n`\r\n\r\n**Please note that Tensorflow compiles fine if GPU support is not activated. Then, i suspect a missing include option in third_party/gpus/crosstool/CROOSTOOL.tpl or third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl since such options disappeared from version 0.12 such as `cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include/\" `but i need some detailled information to fix it.**\r\n\r\n \r\n### Environment info\r\nOperating System: \r\n`RedHat Scientific Linux 6 with kernel 2.6.32-642.13.1.el6.x86_64\r\n`\r\nRegarding libc , gcc versions, i rely on the devtoolsets-4.\r\nUsing the following gcc version:\r\n```\r\n gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/var/opt/rh/devtoolset-4/root/usr/bin/../libexec/gcc/x86_64-redhat-linux/5.3.1/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-4/root/usr --mandir=/opt/rh/devtoolset-4/root/usr/share/man --infodir=/opt/rh/devtoolset-4/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --enable-plugin --with-linker-hash-style=gnu --enable-initfini-array --disable-libgcj --with-default-libstdcxx-abi=gcc4-compatible --with-isl=/builddir/build/BUILD/gcc-5.3.1-20160406/obj-x86_64-redhat-linux/isl-install --enable-libmpx --with-mpc=/builddir/build/BUILD/gcc-5.3.1-20160406/obj-x86_64-redhat-linux/mpc-install --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 5.3.1 20160406 (Red Hat 5.3.1-6) (GCC) \r\n```\r\nRegarding system variables, here are the specific tunings:\r\n```\r\nset path=(/univ_home/UNIVSOFT/COMMON/python/2.7.6/bin $path)\r\nset path=(/usr/local/cuda-8.0/bin $path)\r\nset path=(/uds_data/listic/install/bazel/output $path)\r\nset path=(/opt/rh/devtoolset-4/root/usr/bin $path)\r\nsetenv CC /opt/rh/devtoolset-4/root/usr/bin/gcc\r\n\r\nsetenv JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-0.b15.el6_8.x86_64/\r\nsetenv GRPC_JAVA_PLUGIN /gpfs/MUST-DATA/listic/install/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_64.exe\r\n\r\nsetenv CUDA_HOME /usr/local/cuda-8.0\r\nsetenv INCLUDE_PATH /usr/local/cuda-8.0/include\r\nsetenv LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\r\n\r\nsetenv LD_LIBRARY_PATH /gpfs/MUST-DATA/listic/install/caffe_must/cudnn_51/cuda/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/opt/rh/devtoolset-4/root/usr/lib:$LD_LIBRARY_PATH\r\n#set path=(/uds_data/listic/install/gcc/build6.2/bin $path)\r\nsetenv EXTRA_BAZEL_ARGS '-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'\r\n\r\n\r\n```\r\nInstalled version of CUDA and cuDNN: Cuda 8.0 and CuDNN 5.1\r\n```\r\nls /usr/local/cuda-8.0/lib64/libcud*\r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n\r\nls /univ_home/UNIVSOFT/COMMON/cudnn/lib64\r\nlibcudnn.so  libcudnn.so.5  libcudnn.so.5.1.5  libcudnn_static.a  libm.so\r\n\r\n```\r\nTensorflow is being compiled from source \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\ngit rev-parse HEAD\r\ne895d5ca395c2362df4f5c8f08b68501b41f8a98\r\n```\r\n2. The output of `bazel version`\r\nI actually tried with the following bazel versions : 0.4.3, 0.4.4 and 0.4.5 and always got the same errors.\r\n```\r\nbazel version\r\nBuild label: 0.4.3-2017-03-18 (@1d2fb1f)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sat Mar 18 15:58:41 2017 (1489852721)\r\nBuild timestamp: 1489852721\r\nBuild timestamp as int: 1489852721\r\n\r\n```\r\n\r\nHere are my local code changes to target devtoolsets-4 compiling tools:\r\n\r\n```\r\ngit diff\r\ndiff --git a/tensorflow/core/platform/default/build_config.bzl b/tensorflow/core\r\nindex 48ef8df..be831d5 100644\r\n--- a/tensorflow/core/platform/default/build_config.bzl\r\n+++ b/tensorflow/core/platform/default/build_config.bzl\r\n@@ -8,7 +8,7 @@ load(\"//tensorflow:tensorflow.bzl\", \"if_not_mobile\")\r\n WITH_GCP_SUPPORT = False\r\n WITH_HDFS_SUPPORT = False\r\n WITH_XLA_SUPPORT = False\r\n-WITH_JEMALLOC = True\r\n+WITH_JEMALLOC = False\r\n \r\n # Appends a suffix to a list of deps.\r\n def tf_deps(deps, suffix):\r\ndiff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex 3788eed..fa5b670 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -751,7 +751,7 @@ def tf_custom_op_library(name, srcs=[], gpu_srcs=[], deps=[]\r\n   )\r\n \r\n def tf_extension_linkopts():\r\n-  return []  # No extension link opts\r\n+  return [\"-lrt\"]  # No extension link opts\r\n \r\n def tf_extension_copts():\r\n   return []  # No extension c opts\r\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL.tpl b/third_party/gpus/crossto\r\nindex b77a45c..746817f 100644\r\n--- a/third_party/gpus/crosstool/CROSSTOOL.tpl\r\n+++ b/third_party/gpus/crosstool/CROSSTOOL.tpl\r\n@@ -42,10 +42,10 @@ toolchain {\r\n   target_system_name: \"local\"\r\n   toolchain_identifier: \"local_linux\"\r\n \r\n-  tool_path { name: \"ar\" path: \"/usr/bin/ar\" }\r\n-  tool_path { name: \"compat-ld\" path: \"/usr/bin/ld\" }\r\n-  tool_path { name: \"cpp\" path: \"/usr/bin/cpp\" }\r\n-  tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\r\n+  tool_path { name: \"ar\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ar\" }\r\n+  tool_path { name: \"compat-ld\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\r\n+  tool_path { name: \"cpp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/cpp\" }\r\n+  tool_path { name: \"dwp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/dwp\" }\r\n   # As part of the TensorFlow release, we place some cuda-related compilation\r\n   # files in @local_config_cuda//crosstool/clang/bin, and this relative\r\n   # path, combined with the rest of our Bazel configuration causes our\r\n@@ -56,21 +56,23 @@ toolchain {\r\n   cxx_flag: \"-std=c++11\"\r\n   linker_flag: \"-Wl,-no-as-needed\"\r\n   linker_flag: \"-lstdc++\"\r\n-  linker_flag: \"-B/usr/bin/\"\r\n+  linker_flag: \"-lm\"\r\n+  linker_flag: \"-lrt\"\r\n+  linker_flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n \r\n %{gcc_host_compiler_includes}\r\n-  tool_path { name: \"gcov\" path: \"/usr/bin/gcov\" }\r\n+  tool_path { name: \"gcov\" path: \"/opt/rh/devtoolset-4/root/usr/bin/gcov\" }\r\n \r\n   # C(++) compiles invoke the compiler (as that is the one knowing where\r\n   # to find libraries), but we provide LD so other rules can invoke the linker.\r\n-  tool_path { name: \"ld\" path: \"/usr/bin/ld\" }\r\n+  tool_path { name: \"ld\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\r\n \r\n-  tool_path { name: \"nm\" path: \"/usr/bin/nm\" }\r\n-  tool_path { name: \"objcopy\" path: \"/usr/bin/objcopy\" }\r\n+  tool_path { name: \"nm\" path: \"/opt/rh/devtoolset-4/root/usr/bin/nm\" }\r\n+  tool_path { name: \"objcopy\" path: \"/opt/rh/devtoolset-4/root/usr/bin/objcopy\"\r\n   objcopy_embed_flag: \"-I\"\r\n   objcopy_embed_flag: \"binary\"\r\n-  tool_path { name: \"objdump\" path: \"/usr/bin/objdump\" }\r\n-  tool_path { name: \"strip\" path: \"/usr/bin/strip\" }\r\n+  tool_path { name: \"objdump\" path: \"/opt/rh/devtoolset-4/root/usr/bin/objdump\"\r\n+  tool_path { name: \"strip\" path: \"/opt/rh/devtoolset-4/root/usr/bin/strip\" }\r\n \r\n   # Anticipated future default.\r\n   unfiltered_cxx_flag: \"-no-canonical-prefixes\"\r\ndiff --git a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_no\r\nindex b7d6cc6..e78b70c 100755\r\n--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.t\r\n+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.t\r\n@@ -51,7 +51,7 @@ GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\r\n \r\n CURRENT_DIR = os.path.dirname(sys.argv[0])\r\n NVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'\r\n-LLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc')\r\n+LLVM_HOST_COMPILER_PATH = ('/opt/rh/devtoolset-4/root/usr/bin/gcc')\r\n PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)\r\n NVCC_VERSION = '%{cuda_version}'\r\n \r\n(END) \r\n```\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nHere is my configuration step:\r\n```\r\n./configure\r\nPlease specify the location of python. [Default is /univ_home/UNIVSOFT/COMMON/python/2.7.6/bin/python]: \r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] n\r\njemalloc disabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /opt/rh/devtoolset-4/root/usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: /univ_home/UNIVSOFT/COMMON/cudnn/\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\n.\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n...............\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n```\r\n\r\nNext, compiling:\r\n\r\n```\r\nbazel build --linkopt='-lrt' -c opt --config=cuda --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nINFO: Found 1 target...\r\nERROR: /gpfs/MUST-SHARE/univ_home/alben/.cache/bazel/_bazel_alben/07a9bf809031fe756469f64b1512dee9/external/grpc/BUILD:71:1: undeclared inclusion(s) in rule '@grpc//:gpr':\r\nthis rule is missing dependency declarations for the following files included by 'external/grpc/src/core/lib/support/tmpfile_windows.c':\r\n  '/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include/stdint.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 12.842s, Critical Path: 3.24s\r\n\r\n```\r\nThis time the error highlights grpc inclusion errors, it used to be related to @nasm or others. depending on the trials.\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nTried bazel version 0.4.3, 0.4.4, 0.4.5 with the same results.\r\nTried Tensorflow R1.0, v1.0.1, same results\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Well, problem solved, actually, we have to add the following instructions in third_party/gpus/crosstool/CROSSTOOL.tpl :\r\n\r\n```\r\n   # Include directory for cuda headers.\r\n   cxx_builtin_include_directory: \"%{cuda_include_path}\"\r\n+  cxx_builtin_include_directory: \"/opt/rh/devtoolset-4/root/usr/lib\"\r\n+  cxx_builtin_include_directory: \"/opt/rh/devtoolset-4/root/usr/include\"\r\n+  cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include/\"\r\n\r\n```\r\n\r\nBut i guess it would be nice to filter out all the options i reported but are not useful.\r\n\r\nAlso consider this link to check a nice installation tutorial that may complete this \"solved\" issue\r\n[https://www.linkedin.com/pulse/compiling-tensorflow-10-python-27-redhat-6-florian-raudies](url)\r\n\r\nOne can cite how to activate some cpu options support (SSE3, 4, AVX,etc.) by adding `--copt` options\r\n`bazel build -c opt --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mavx2 --copt=-mfma -k//tensorflow/tools/pip_package:build_pip_package\r\n`\r\n\r\nIn my case, looking for standalone package generation, i use the following command:\r\n`bazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mfma --copt=-msse3  --config=cuda --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\r\n`\r\nnext preparing the python package:\r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n`\r\n\r\nPackage compilation worked using 3 configurations:\r\n-with cuda, no jemalloc and all other parameters using defaults\r\n-with cuda, no jemalloc, with XLA,  all above cpu options and all other parameters using defaults\r\n-no cuda, no jemalloc, with XLA, with SSE3 and all other parameters using defaults\r\nNow waiting for installation on various machines. I will extend discussion if something fails\r\n", "TensorFlow does not have official support for RedHat.\r\nI will mark this issue as community support.\r\nYou may be able to get better help through stackoverflow.", "In the end, installation was successful ! I hope this issue can help other users facing the same problem.\r\n\r\nOnly one issue remains and concerns the IoU metric `tf.contrib.metrics.streaming_mean_iou`:\r\n\r\n- the metric was generating segmentation faults when using tensorflow version 0.12.1 using pip precompiled packages on another system (debian testing) and compiled version on this RHEL6\r\n- this metric no more reported problems when tensorflow version has been upgraded to r1.0 using pip precompiled packages on another system (debian testing)\r\n- this metric still generates seg faults using the compiled version of tensorflow r1.0 on RHEL6 (but one can see above that i add several options (cpu options, XLA support) in this configuration.\r\n\r\n\r\n... but i should open a specific issue addressing this.\r\n", "Hi all, still works with Tensorflow 1.1rc0 with bazel 0.4.5 !", "Great to hear that it now works for you!\r\nI will then close this issue.", "Guna, \r\n\r\nI am trying to compile bazel-0.7.0-dist.zip on rel6, the bazel is not compaile:\r\nHere is the step\r\n################\r\n scl enable devtoolset-4 bash\r\n\r\ncat >/tmp/bazelrc <<EOF\r\nstartup --batch\r\nbuild --spawn_strategy=standalone --genrule_strategy=standalone\r\nEOF\r\n\r\nWhich gcc\r\n/opt/rh/devtoolset-4/root/usr/bin/gcc\r\n\r\n\r\n[dsapadmn@vcld002370 bazel-0.7.0]$ BAZELRC=/tmp/bazelrc ./compile.sh\r\n\u00f0\u0178\u008d\u0192  Building Bazel from scratch\r\n################\r\nand the bazel is not compile, could you please provide a tools/cpp/CROSSTOOL to correct gcc (/opt/rh/devtoolset-4/root/usr/bin/gcc) so that I can compile bazel?\r\n\r\nThanks\r\nLucy\r\n", "For bazel installation, please reach out to bazel team, they can help in bazel installation issues much better than I can.", "Hi,\ni am currently trying to compile Tensorflow r1.5 on the same platform.\nI managed to install bazel 0.8.0 as recommended in the install guide and\nuse the devtoolset-4 with gcc 5.3 version.\n\nI try to finish the installation with GPUs but this is quite hazardous\nsince our grid also has issues.\n\nCurrently, here is my setup that worked for TF without GPU:\n\nRegarding bazel, install from the dist package:\n Prepare BAZEL (here, 0.8.0):\n   --> get to https://github.com/bazelbuild/bazel/releases and get a *-dist\nversion\n   353    14:27    wget\nhttps://github.com/bazelbuild/bazel/releases/download/0.8.0/bazel-0.8.0-dist.zip\n   354    14:28    unzip bazel-0.8.0-dist.zip\n   355    14:28    mkdir bazel-0.8.0-dist\n   356    14:28    cd bazel-0.8.0-dist\n   357    14:28    unzip ../bazel-0.8.0-dist.zip\n   361    14:34    ./compile.sh\n\n   --> once compiled, add bazel to path:\nset path=(/path/to/bazel/bazel-0.8.0-dist/output/bazel $path)\n\nAlso, create a .bazelrc in your home that contain:\ncat ~/.bazelrc\nbuild --verbose_failures\n--linkopt=-Wl,-rpath,/opt/rh/devtoolset-4/root/usr/lib64\n--linkopt=-Wl,-rpath,/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-0.b15.el6_8.x86_64/jre/lib/\n--linkopt=-lz --linkopt=-lrt --linkopt=-lm --genrule_strategy=standalone\n--spawn_strategy=standalone --linkopt=-Wl,-rpath,/usr/local/cuda-8.0/lib64/\n\n\n\nNow regarding TF:\n-> checkout r1.5\nand then applied the following changes:\ngit diff\ndiff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\nindex bafb562..5d5d554 100644\n--- a/tensorflow/tensorflow.bzl\n+++ b/tensorflow/tensorflow.bzl\n@@ -1264,7 +1264,7 @@ register_extension_info(\n )\n\n def tf_extension_linkopts():\n-  return []  # No extension link opts\n+  return [\"-lrt\"]  # No extension link opts\n\n def tf_extension_copts():\n   return []  # No extension c opts\ndiff --git a/tensorflow/tools/git/gen/branch_ref\nb/tensorflow/tools/git/gen/branch_ref\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/tensorflow/tools/git/gen/branch_ref\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/tensorflow/tools/git/gen/branch_ref\nb/tensorflow/tools/git/gen/branch_ref\nnew file mode 120000\nindex 0000000..70ed0ba\n--- /dev/null\n+++ b/tensorflow/tools/git/gen/branch_ref\n@@ -0,0 +1 @@\n+/gpfs/MUST-DATA/listic/install/tensorflow.1.5/tensorflow/.git/refs/heads/r1.5\n\\ No newline at end of file\ndiff --git a/tensorflow/tools/git/gen/head b/tensorflow/tools/git/gen/head\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/tensorflow/tools/git/gen/head\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/tensorflow/tools/git/gen/head b/tensorflow/tools/git/gen/head\nnew file mode 120000\nindex 0000000..28074d9\n--- /dev/null\n+++ b/tensorflow/tools/git/gen/head\n@@ -0,0 +1 @@\n+/gpfs/MUST-DATA/listic/install/tensorflow.1.5/tensorflow/.git/HEAD\n\\ No newline at end of file\ndiff --git a/tensorflow/tools/git/gen/spec.json\nb/tensorflow/tools/git/gen/spec.json\nindex 176bbc2..2dbb36c 100644\n--- a/tensorflow/tools/git/gen/spec.json\n+++ b/tensorflow/tools/git/gen/spec.json\n@@ -1,3 +1,5 @@\n {\n-  \"git\": false\n-}\n+  \"path\": \"/gpfs/MUST-DATA/listic/install/tensorflow.1.5/tensorflow\",\n+  \"git\": true,\n+  \"branch\": \"refs/heads/r1.5\"\n+}\n\\ No newline at end of file\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\nb/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\nindex 05290d6..6903779 100644\n--- a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\n+++ b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\n@@ -42,10 +42,10 @@ toolchain {\n   target_system_name: \"local\"\n   toolchain_identifier: \"local_linux\"\n\n-  tool_path { name: \"ar\" path: \"/usr/bin/ar\" }\n-  tool_path { name: \"compat-ld\" path: \"/usr/bin/ld\" }\n-  tool_path { name: \"cpp\" path: \"/usr/bin/cpp\" }\n-  tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n+  tool_path { name: \"ar\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ar\" }\n+  tool_path { name: \"compat-ld\" path:\n\"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\n+  tool_path { name: \"cpp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/cpp\" }\n+  tool_path { name: \"dwp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/dwp\" }\n   # As part of the TensorFlow release, we place some cuda-related\ncompilation\n   # files in @local_config_cuda//crosstool/clang/bin, and this relative\n   # path, combined with the rest of our Bazel configuration causes our\n@@ -56,21 +56,23 @@ toolchain {\n   cxx_flag: \"-std=c++11\"\n   linker_flag: \"-Wl,-no-as-needed\"\n   linker_flag: \"-lstdc++\"\n-  linker_flag: \"-B/usr/bin/\"\n+  linker_flag: \"-lm\"\n+  linker_flag: \"-lrt\"\n+  linker_flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\n\n %{host_compiler_includes}\n-  tool_path { name: \"gcov\" path: \"/usr/bin/gcov\" }\n+  tool_path { name: \"gcov\" path: \"/opt/rh/devtoolset-4/root/usr/bin/gcov\" }\n\n   # C(++) compiles invoke the compiler (as that is the one knowing where\n   # to find libraries), but we provide LD so other rules can invoke the\nlinker.\n-  tool_path { name: \"ld\" path: \"/usr/bin/ld\" }\n+  tool_path { name: \"ld\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\n\n-  tool_path { name: \"nm\" path: \"/usr/bin/nm\" }\n-  tool_path { name: \"objcopy\" path: \"/usr/bin/objcopy\" }\n+  tool_path { name: \"nm\" path: \"/opt/rh/devtoolset-4/root/usr/bin/nm\" }\n+  tool_path { name: \"objcopy\" path:\n\"/opt/rh/devtoolset-4/root/usr/bin/objcopy\" }\n   objcopy_embed_flag: \"-I\"\n   objcopy_embed_flag: \"binary\"\n-  tool_path { name: \"objdump\" path: \"/usr/bin/objdump\" }\n-  tool_path { name: \"strip\" path: \"/usr/bin/strip\" }\n+  tool_path { name: \"objdump\" path:\n\"/opt/rh/devtoolset-4/root/usr/bin/objdump\" }\n+  tool_path { name: \"strip\" path:\n\"/opt/rh/devtoolset-4/root/usr/bin/strip\" }\n\n   # Anticipated future default.\n   unfiltered_cxx_flag: \"-no-canonical-prefixes\"\n@@ -122,6 +124,9 @@ toolchain {\n\n   # Include directory for cuda headers.\n %{cuda_include_path}\n+cxx_builtin_include_directory: \"/opt/rh/devtoolset-4/root/usr/lib\"\n+cxx_builtin_include_directory: \"/opt/rh/devtoolset-4/root/usr/include\"\n+cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include/\"\n\n   compilation_mode_flags {\n     mode: DBG\ndiff --git\na/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\nb/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gc\nindex 2558f46..2d7ff15 100755\n---\na/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\n+++\nb/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\n@@ -47,7 +47,8 @@ import pipes\n\n # Template values set by cuda_autoconf.\n CPU_COMPILER = ('%{cpu_compiler}')\n-GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\n+#GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\n+GCC_HOST_COMPILER_PATH = ('/opt/rh/devtoolset-4/root/usr/bin/gcc')\n\n NVCC_PATH = '%{nvcc_path}'\n PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)\n##########################################\n\nALSO had to explicitely add -lrt option in\nbazel-tensorflow/external/protobuf_archive/BUILD :\nLINK_OPTS = select({\n    \":android\": [],\n    \"//conditions:default\": [\"-lpthread\",\"-lrt\",\"-lm\"],\n})\n:\n\nThen, to compile WITHOUT GPU:\n# clean configuration\nbazel clean --expunge\n\n#run the configure script, all addons set to false, except XLA\n./configure\n\nOnce done, build command NO GPU !!!\nbazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-mavx2\n--copt=-msse4.2 --copt=-msse4.1 --copt=-mfma --copt=-msse3\n--genrule_strategy=standalone --spawn_strategy=standalone\n//tensorflow/tools/pip_package:build_pip_package\n\n\nIt worked.\n\nNow with GPUs, i got issues with nccl but it seems that one can download\nsources, compile and add to path to make it work.... but i wait for grid\naccess to validate.\nHere is the tested workaround:\ngit clone https://github.com/NVIDIA/nccl.git\nInitialized empty Git repository in\n/gpfs/MUST-DATA/listic/install/nccl/.git/\nremote: Counting objects: 651, done.\nremote: Total 651 (delta 0), reused 0 (delta 0), pack-reused 651\nReceiving objects: 100% (651/651), 1.38 MiB | 402 KiB/s, done.\nResolving deltas: 100% (411/411), done.\n[alben@lapp-wngpu002 install]$ cd nccl/\n[alben@lapp-wngpu002 nccl]$ echo $CUDA_HOME/\n/usr/local/cuda-8.0/\n[alben@lapp-wngpu002 nccl]$ make CUDA_HOME=/usr/local/cuda-8.0/\nFinally add to paths\nsetenv LD_LIBRARY_PATH\n/gpfs/MUST-DATA/listic/install/nccl/build/lib/:$LD_LIBRARY_PATH\nsetenv INCLUDE_PATH\n/gpfs/MUST-DATA/listic/install/nccl/build/inlude/:$INCLUDE_PATH\n\nHope it helps.\n\n\n\n2018-02-06 21:15 GMT+01:00 Gunhan Gulsoy <notifications@github.com>:\n\n> For bazel installation, please reach out to bazel team, they can help in\n> bazel installation issues much better than I can.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8529#issuecomment-363550370>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACf6SCB2ZGF54_1q_EiU5nn5b27adLIbks5tSLLrgaJpZM4Mhr7n>\n> .\n>\n\n\n\n-- \nAlexandre BENOIT,\nAssociate Professor / Ma\u00eetre de Conf\u00e9rence\nImage processing and visual scene classification,\nLISTIC Lab / IUT Annecy\nhttps://sites.google.com/site/benoitalexandrevision/\n", "albenoit ,\r\n\r\nThank you for your information\r\n I have devtoolset-4 gcc (GCC) 5.2.1  on rel6\r\n#############\r\ndsapadmn@vcld002370 bazel-0.8.0-dist]$ scl enable devtoolset-4 bash\r\n[dsapadmn@vcld002370 bazel-0.8.0-dist]$ which gcc\r\n/opt/rh/devtoolset-4/root/usr/bin/gcc\r\n[dsapadmn@vcld002370 bazel-0.8.0-dist]$ gcc --version\r\ngcc (GCC) 5.2.1 20150902 (Red Hat 5.2.1-2)\r\n\r\n############\r\nI follow your instruction,\r\n\r\n357    14:28    unzip ../bazel-0.8.0-dist.zip\r\n   361    14:34    ./compile.sh\r\n\r\nit hang in there like this\r\n[dsapadmn@vcld002370 bazel-0.8.0-dist]$ ./compile.sh\r\n\u00f0\u0178\u008d\u0192  Building Bazel from scratch\r\n\r\nno respose, Could you please attach your  10:58 AM \r\n\r\ntools/cpp/CROSSTOOL  file here, so that I can compare?\r\n\r\nThank you for all your helps in advance\r\n", "The surprisin thing is that this time, i did not modify any file and it\nworked (for previous versions i did changes in the crosstool, typically in\nthe local_linux toolchain, similarly to the tensorflow cross tool as i\nshowed you in the previous post.\nHowever, i have several Environment variables set to help :\nset path=(/univ_home/UNIVSOFT/COMMON/python/2.7.6/bin $path)\nset path=(/usr/local/cuda-8.0/bin $path)\nset path=(/uds_data/listic/install/bazel/bazel-0.8.0-dist/output/ $path)\nset path=(/opt/rh/devtoolset-4/root/usr/bin $path)\nsetenv PYTHONPATH\n\"/univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages\"\nsetenv JAVA_HOME\n/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-0.b15.el6_8.x86_64/jre\nsetenv GRPC_JAVA_PLUGIN\n/gpfs/MUST-DATA/listic/install/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_64.exe\nsetenv CC /opt/rh/devtoolset-4/root/usr/bin/gcc\nsetenv CMAKE_C_COMPILER   /opt/rh/devtoolset-4/root/usr/bin/gcc\nsetenv CMAKE_CXX_COMPILER /opt/rh/devtoolset-4/root/usr/bin/g++\nsetenv CUDA_HOME /usr/local/cuda-8.0\nsetenv INCLUDE_PATH\n/usr/local/cuda-8.0/include:/gpfs/MUST-DATA/listic/install/nccl/build/include#:$INCLUDE_PATH\nsetenv LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\n\nsetenv LD_LIBRARY_PATH\n/gpfs/MUST-DATA/listic/install/nccl/build/lib:/uds_data/listic/install/cuda/cudnn-7.0.4_v7/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/opt/rh/devtoolset-4/root/usr/lib:$LD_LIBRARY_PATH\n#set path=(/uds_data/listic/install/gcc/build6.2/bin $path)\nsetenv EXTRA_BAZEL_ARGS '-s --verbose_failures\n--ignore_unsupported_sandboxing --genrule_strategy=standalone\n--spawn_strategy=standalone --jobs 8'\n\nHope it helps.\nAlex\n\n\n\n\n2018-02-07 3:18 GMT+01:00 lucy-itjob <notifications@github.com>:\n\n> albenoit ,\n>\n> Thank you for your information\n> I have devtoolset-4 gcc (GCC) 5.2.1 on rel6\n> #############\n> dsapadmn@vcld002370 bazel-0.8.0-dist]$ scl enable devtoolset-4 bash\n> [dsapadmn@vcld002370 bazel-0.8.0-dist]$ which gcc\n> /opt/rh/devtoolset-4/root/usr/bin/gcc\n> [dsapadmn@vcld002370 bazel-0.8.0-dist]$ gcc --version\n> gcc (GCC) 5.2.1 20150902 (Red Hat 5.2.1-2)\n>\n> ############\n> I follow your instruction,\n>\n> 357 14:28 unzip ../bazel-0.8.0-dist.zip\n> 361 14:34 ./compile.sh\n>\n> it hang in there like this\n> [dsapadmn@vcld002370 bazel-0.8.0-dist]$ ./compile.sh\n> \u00f0\u0178\ufffd\u0192 Building Bazel from scratch\n>\n> no respose, Could you please attach your 10:58 AM\n>\n> tools/cpp/CROSSTOOL file here, so that I can compare?\n>\n> Thank you for all your helps in advance\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8529#issuecomment-363633342>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACf6SAdKSjz94MW2SEXg7K2gV_eFerfHks5tSQgQgaJpZM4Mhr7n>\n> .\n>\n\n\n\n-- \nAlexandre BENOIT,\nAssociate Professor / Ma\u00eetre de Conf\u00e9rence\nImage processing and visual scene classification,\nLISTIC Lab / IUT Annecy\nhttps://sites.google.com/site/benoitalexandrevision/\n", "Alex,\r\n\r\nThank you for all this information.\r\n\r\nI am new in this area. To me,  you are the first one land on the moon in this area,  I am still on the earth, and pulling my hair to get this Tensorflow  working on RHEL6, hoping some day I can land on the moon just like you did.\r\n\r\n\r\nCan you do me a favor? I know you had listed your local customized files which modified by  you in  previously email chain,  could you please sent all the your local customized files (Bazel and Tensorflow) which modified by  you to me directly, my email is  lucy_itjob@comcat.net. could you please include  Crosstool files for  current and previous version (give me the current version name and download url) too ?  So that I can use the code to compare my environment based on your files.\r\n\r\nYou know, to be able to walk on the moon, it is not easy since there are no Tensorflow vendor support on RHEL6.  \r\n\r\nAlex,  I attach an picture for you to enjoy! Thanks a lot\r\n![image](https://user-images.githubusercontent.com/25250352/36062221-4b206a64-0e2d-11e8-87bc-ec48874097a9.png)\r\n\r\n\r\n", "Hi,\r\nnice to know it helps.\r\n\r\nHowever, despite the fact that compilation and installation succeeded, i\r\nhave trouble when running a tensorflow graph because of a conflict between\r\ncudnn versions. The error message appears when Tensorflow 1.5 session starts:\r\n\r\n> 2018-02-08 18:17:27.719349: E\r\n> tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN\r\n> library: 7004 (compatibility version 7000) but source was compiled with\r\n> 5105 (compatibility version 5100).  If using a binary install, upgrade your\r\n> CuDNN library to match.  If building from sources, make sure the library\r\n> loaded at runtime matches a compatible version specified during compile\r\n> configuration.\r\n\r\nBut, when running ldd on the tensorflow so file, it points to cudnn7.0.4\r\nand the trouble seems to come from a link between an old cudnn5.1 and the\r\ncuda8.0 installation. Strange but our grid maintainers are looking at this\r\npoint.\r\n\r\n*** UPDATE, the problem no more exists with Tensorflow 1.6 !!! ***\r\n\r\nThe compilation of Tensorflow 1.6 on REHL SL6 with cuda8.0 and\r\ncudnn7.0.5 was done this way:\r\n\r\nTo make tensorflow build possible  i had to get the devtoolset-4 pakage to\r\nget an appropriate version of gcc and related build tools.\r\nThat done, my variables are the following. Some redundancies exist but are\r\nexplained by requirements of various modules that all need their specific\r\nkeywords...\r\nTake care, since bazel requires a lot of space for caching, i had to set\r\nvariable to a folder with large available space:\r\nmy .cshrc file (some line being of interest for other topics but may also\r\nhelp :\r\n\r\nsetenv LD_LIBRARY_PATH\r\n> /univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/#:$LD_LIBRARY_PATH\r\n> setenv LD_LIBRARY_PATH\r\n> /univ_home/UNIVSOFT/COMMON/Boost/boost_1_57_0/lib:$LD_LIBRARY_PATH\r\n> setenv LD_LIBRARY_PATH\r\n> /univ_home/UNIVSOFT/COMMON/OpenCV/opencv-2.4.10/lib:$LD_LIBRARY_PATH\r\n> setenv LD_LIBRARY_PATH\r\n> /univ_home/UNIVSOFT/COMMON/gdal/2.0.0/lib:$LD_LIBRARY_PATH\r\n> set path=(/univ_home/UNIVSOFT/COMMON/python/2.7.6/bin $path)\r\n> set path=(/usr/local/cuda-8.0/bin $path)\r\n> set path=(/uds_data/listic/install/bazel/bazel-0.8.0-dist/output/ $path)\r\n> set path=(/opt/rh/devtoolset-4/root/usr/bin $path)\r\n> setenv PYTHONPATH\r\n> \"/univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages\"\r\n> #:/uds_data/listic/install/caffe_must/caffe_GPU/python/\"\r\n> setenv JAVA_HOME\r\n> /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-0.b15.el6_8.x86_64/jre\r\n> setenv GRPC_JAVA_PLUGIN\r\n> /gpfs/MUST-DATA/listic/install/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_64.exe\r\n> #/gpfs/MUST-DATA/listic/install/grpc-java/precompiled/p\r\n>\r\nsetenv TEST_TMPDIR /tmp/\r\nsetenv CUDA_HOME /usr/local/cuda-8.0\r\nsetenv INCLUDE_PATH\r\n/usr/local/cuda-8.0/include#:/gpfs/MUST-DATA/listic/install/nccl/build/include#:$INCLUDE_PATH\r\nsetenv LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\r\nsetenv LD_LIBRARY_PATH\r\n/gpfs/MUST-DATA/listic/install/mkl-dnn/build/lib:/uds_data/listic/install/cuda/cudnn-cuda8-v7.0.5/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/opt/rh/devtoolset-4/root/usr/lib:$LD_LIBRARY_PATH\r\nsetenv EXTRA_BAZEL_ARGS '-s --verbose_failures\r\n--ignore_unsupported_sandboxing --genrule_strategy=standalone\r\n--spawn_strategy=standalone --jobs 8'\r\n\r\n\r\n\r\nRegarding Bazel, the only way i got it work was to get the 0.9.0-dist\r\npackage and compile it.\r\n\r\n>    --> get to https://github.com/bazelbuild/bazel/releases and get a\r\n> *-dist version\r\n>    353    14:27    wget\r\n> https://github.com/bazelbuild/bazel/releases/download/0.9.0/bazel-0.9.0-dist.zip\r\n>    354    14:28    unzip bazel-0.9.0-dist.zip\r\n>    355    14:28    mkdir bazel-0.9.0-dist\r\n>    356    14:28    cd bazel-0.9.0-dist\r\n>    357    14:28    unzip ../bazel-0.9.0-dist.zip\r\n>    361    14:34    ./compile.sh\r\n>\r\n>    --> once compiled, add bazel to path:\r\n> set path=(//uds_data/listic/install/bazel/bazel-0.9.0-dist/output/bazel\r\n> $path)\r\n>\r\nIn addition, update/create a .bazelrc file in your home dir with (you\r\ncertainly need to update your java path)\r\nbuild --verbose_failures\r\n--linkopt=-Wl,-rpath,/opt/rh/devtoolset-4/root/usr/lib64\r\n--linkopt=-Wl,-rpath,/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-0.b15.el6_8.x86_64/jre/lib/\r\n--linkopt=-lz --linkopt=-lrt --linkopt=-lm --genrule_strategy=standalone\r\n--spawn_strategy=standalone --linkopt=-Wl,-rpath,/usr/local/cuda-8.0/lib64/\r\n\r\n\r\n\r\n\r\n\r\nRegarding Tensorflow 1.5, here are my local changes on Tensorflow 1.5 :\r\n\r\n> diff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\n> index bafb562..5d5d554 100644\r\n> --- a/tensorflow/tensorflow.bzl\r\n> +++ b/tensorflow/tensorflow.bzl\r\n> @@ -1264,7 +1264,7 @@ register_extension_info(\r\n>  )\r\n>\r\n>  def tf_extension_linkopts():\r\n> -  return []  # No extension link opts\r\n> +  return [\"-lrt\"]  # No extension link opts\r\n>\r\n>  def tf_extension_copts():\r\n>    return []  # No extension c opts\r\n> diff --git a/tensorflow/tools/git/gen/branch_ref\r\n> b/tensorflow/tools/git/gen/branch_ref\r\n> deleted file mode 100644\r\n> index 8b13789..0000000\r\n> --- a/tensorflow/tools/git/gen/branch_ref\r\n> +++ /dev/null\r\n> @@ -1 +0,0 @@\r\n> -\r\n> diff --git a/tensorflow/tools/git/gen/branch_ref\r\n> b/tensorflow/tools/git/gen/branch_ref\r\n> new file mode 120000\r\n> index 0000000..70ed0ba\r\n> --- /dev/null\r\n> +++ b/tensorflow/tools/git/gen/branch_ref\r\n> @@ -0,0 +1 @@\r\n>\r\n> +/gpfs/MUST-DATA/listic/install/tensorflow.1.5/tensorflow/.git/refs/heads/r1.5\r\n> \\ No newline at end of file\r\n> diff --git a/tensorflow/tools/git/gen/head b/tensorflow/tools/git/gen/head\r\n> deleted file mode 100644\r\n> index 8b13789..0000000\r\n> --- a/tensorflow/tools/git/gen/head\r\n> +++ /dev/null\r\n> @@ -1 +0,0 @@\r\n> -\r\n> diff --git a/tensorflow/tools/git/gen/head b/tensorflow/tools/git/gen/head\r\n> new file mode 120000\r\n> index 0000000..28074d9\r\n> --- /dev/null\r\n> +++ b/tensorflow/tools/git/gen/head\r\n> @@ -0,0 +1 @@\r\n> +/gpfs/MUST-DATA/listic/install/tensorflow.1.5/tensorflow/.git/HEAD\r\n> \\ No newline at end of file\r\n> diff --git a/tensorflow/tools/git/gen/spec.json\r\n> b/tensorflow/tools/git/gen/spec.json\r\n> index 176bbc2..2dbb36c 100644\r\n> --- a/tensorflow/tools/git/gen/spec.json\r\n> +++ b/tensorflow/tools/git/gen/spec.json\r\n> @@ -1,3 +1,5 @@\r\n>  {\r\n> -  \"git\": false\r\n> -}\r\n> +  \"path\": \"/gpfs/MUST-DATA/listic/install/tensorflow.1.5/tensorflow\",\r\n> +  \"git\": true,\r\n> +  \"branch\": \"refs/heads/r1.5\"\r\n> +}\r\n> \\ No newline at end of file\r\n> diff --git a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\n> b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\n> index 05290d6..7b2e448 100644\r\n> --- a/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\n> +++ b/third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\n> @@ -42,10 +42,10 @@ toolchain {\r\n>    target_system_name: \"local\"\r\n>    toolchain_identifier: \"local_linux\"\r\n>\r\n> -  tool_path { name: \"ar\" path: \"/usr/bin/ar\" }\r\n> -  tool_path { name: \"compat-ld\" path: \"/usr/bin/ld\" }\r\n> -  tool_path { name: \"cpp\" path: \"/usr/bin/cpp\" }\r\n> -  tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\r\n> +  tool_path { name: \"ar\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ar\" }\r\n> +  tool_path { name: \"compat-ld\" path:\r\n> \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\r\n> +  tool_path { name: \"cpp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/cpp\" }\r\n> +  tool_path { name: \"dwp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/dwp\" }\r\n>    # As part of the TensorFlow release, we place some cuda-related\r\n> compilation\r\n>    # files in @local_config_cuda//crosstool/clang/bin, and this relative\r\n>    # path, combined with the rest of our Bazel configuration causes our\r\n> @@ -56,21 +56,23 @@ toolchain {\r\n>    cxx_flag: \"-std=c++11\"\r\n>    linker_flag: \"-Wl,-no-as-needed\"\r\n>    linker_flag: \"-lstdc++\"\r\n> -  linker_flag: \"-B/usr/bin/\"\r\n> +  linker_flag: \"-lm\"\r\n> +  linker_flag: \"-lrt\"\r\n> +  linker_flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n>\r\n>  %{host_compiler_includes}\r\n> -  tool_path { name: \"gcov\" path: \"/usr/bin/gcov\" }\r\n> +  tool_path { name: \"gcov\" path: \"/opt/rh/devtoolset-4/root/usr/bin/gcov\"\r\n> }\r\n>\r\n>    # C(++) compiles invoke the compiler (as that is the one knowing where\r\n>    # to find libraries), but we provide LD so other rules can invoke the\r\n> linker.\r\n> -  tool_path { name: \"ld\" path: \"/usr/bin/ld\" }\r\n> +  tool_path { name: \"ld\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\r\n>\r\n> -  tool_path { name: \"nm\" path: \"/usr/bin/nm\" }\r\n> -  tool_path { name: \"objcopy\" path: \"/usr/bin/objcopy\" }\r\n> +  tool_path { name: \"nm\" path: \"/opt/rh/devtoolset-4/root/usr/bin/nm\" }\r\n> +  tool_path { name: \"objcopy\" path:\r\n> \"/opt/rh/devtoolset-4/root/usr/bin/objcopy\" }\r\n>    objcopy_embed_flag: \"-I\"\r\n>    objcopy_embed_flag: \"binary\"\r\n> -  tool_path { name: \"objdump\" path: \"/usr/bin/objdump\" }\r\n> -  tool_path { name: \"strip\" path: \"/usr/bin/strip\" }\r\n> +  tool_path { name: \"objdump\" path:\r\n> \"/opt/rh/devtoolset-4/root/usr/bin/objdump\" }\r\n> +  tool_path { name: \"strip\" path:\r\n> \"/opt/rh/devtoolset-4/root/usr/bin/strip\" }\r\n>\r\n>    # Anticipated future default.\r\n>    unfiltered_cxx_flag: \"-no-canonical-prefixes\"\r\n> @@ -122,6 +124,11 @@ toolchain {\r\n>\r\n>    # Include directory for cuda headers.\r\n>  %{cuda_include_path}\r\n> +cxx_builtin_include_directory:\r\n> \"/opt/rh/devtoolset-4/root/usr/lib/gcc/x86_64-redhat-linux/5.3.1/include\"\r\n> +cxx_builtin_include_directory:\r\n> \"/opt/rh/devtoolset-4/root/usr/include/c++/5.3.1/include\"\r\n> +cxx_builtin_include_directory:\r\n> \"/opt/rh/devtoolset-4/root/usr/local/include\"\r\n> +cxx_builtin_include_directory: \"/opt/rh/devtoolset-4/root/usr/include\"\r\n> +cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include/\"\r\n>\r\n>    compilation_mode_flags {\r\n>      mode: DBG\r\n> diff --git\r\n> a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n> b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n> index 2558f46..2d7ff15 100755\r\n> ---\r\n> a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n> +++\r\n> b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n> @@ -47,7 +47,8 @@ import pipes\r\n>\r\n>  # Template values set by cuda_autoconf.\r\n>  CPU_COMPILER = ('%{cpu_compiler}')\r\n> -GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\r\n> +#GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\r\n> +GCC_HOST_COMPILER_PATH = ('/opt/rh/devtoolset-4/root/usr/bin/gcc')\r\n>\r\n>  NVCC_PATH = '%{nvcc_path}'\r\n>  PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)\r\n> diff --git a/third_party/gpus/cuda_configure.bzl\r\n> b/third_party/gpus/cuda_configure.bzl\r\n> index 31a4bfa..2f8dc81 100644\r\n> --- a/third_party/gpus/cuda_configure.bzl\r\n> +++ b/third_party/gpus/cuda_configure.bzl\r\n> @@ -89,8 +89,7 @@ def _get_cxx_inc_directories_impl(repository_ctx, cc,\r\n> lang_is_cpp):\r\n>    # TODO: We pass -no-canonical-prefixes here to match the compiler flags,\r\n>    #       but in cuda_clang CROSSTOOL file that is a `feature` and we\r\n> should\r\n>    #       handle the case when it's disabled and no flag is passed\r\n> -  result = repository_ctx.execute([cc, \"-no-canonical-prefixes\",\r\n> -                                   \"-E\", \"-x\" + lang, \"-\", \"-v\"])\r\n> +  result = repository_ctx.execute([cc,\"-E\", \"-x\" + lang, \"-\", \"-v\"])\r\n>    index1 = result.stderr.find(_INC_DIR_MARKER_BEGIN)\r\n>    if index1 == -1:\r\n>      return []\r\n>\r\n\r\nRegarding configure setup:\r\n\r\n> [alben@lapp-wngpu002 tensorflow]$ ./configure\r\n> INFO: $TEST_TMPDIR defined: output root default is '/tmp/'.\r\n> You have bazel 0.8.0- (@non-git) installed.\r\n> Please specify the location of python. [Default is\r\n> /univ_home/UNIVSOFT/COMMON/python/2.7.6/bin/python]:\r\n>\r\n>\r\n> Found possible Python library paths:\r\n>   /univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages\r\n> Please input the desired Python library path to use.  Default is\r\n> [/univ_home/UNIVSOFT/COMMON/python/2.7.6/lib/python2.7/site-packages]\r\n>\r\n> Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\n> No jemalloc as malloc support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:\r\n> n\r\n> No Google Cloud Platform support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\n> No Hadoop File System support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]:\r\n> n\r\n> No Amazon S3 File System support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\n> XLA JIT support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with GDR support? [y/N]: n\r\n> No GDR support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with VERBS support? [y/N]: n\r\n> No VERBS support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\n> No OpenCL SYCL support will be enabled for TensorFlow.\r\n>\r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n>\r\n> Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave\r\n> empty to default to CUDA 9.0]: 8.0\r\n>\r\n>\r\n> Please specify the location where CUDA 8.0 toolkit is installed. Refer to\r\n> README.md for more details. [Default is /usr/local/cuda]:\r\n> /usr/local/cuda-8.0\r\n>\r\n>\r\n> Please specify the cuDNN version you want to use. [Leave empty to default\r\n> to cuDNN 7.0]: 7\r\n>\r\n>\r\n> Please specify the location where cuDNN 7 library is installed. Refer to\r\n> README.md for more details. [Default is\r\n> /usr/local/cuda-8.0]:/uds_data/listic/install/cuda/cudnn-cuda8-v7.0.5\r\n>\r\n>\r\n> Please specify a list of comma-separated Cuda compute capabilities you\r\n> want to build with.\r\n> You can find the compute capability of your device at:\r\n> https://developer.nvidia.com/cuda-gpus.\r\n> Please note that each additional compute capability significantly\r\n> increases your build time and binary size. [Default is: 3.7,3.7]\r\n>\r\n>\r\n> Do you want to use clang as CUDA compiler? [y/N]:\r\n> nvcc will be used as CUDA compiler.\r\n>\r\n> Please specify which gcc should be used by nvcc as the host compiler.\r\n> [Default is /opt/rh/devtoolset-4/root/usr/bin/gcc]:\r\n>\r\n>\r\n> Do you wish to build TensorFlow with MPI support? [y/N]:\r\n> No MPI support will be enabled for TensorFlow.\r\n>\r\n> Please specify optimization flags to use during compilation when bazel\r\n> option \"--config=opt\" is specified [Default is -march=native]:\r\n>\r\n>\r\n> Add \"--config=mkl\" to your bazel command to build with MKL support.\r\n> Please note that MKL on MacOS or windows is still not supported.\r\n> If you would like to use a local MKL instead of downloading, please set\r\n> the environment variable \"TF_MKL_ROOT\" every time before build.\r\n>\r\n> Would you like to interactively configure ./WORKSPACE for Android builds?\r\n> [y/N]:\r\n> Not configuring the WORKSPACE for Android builds.\r\n>\r\n> Configuration finished\r\n>\r\n\r\nNow the build command:\r\n\r\n> bazel build --config=mkl --linkopt='-lrt' -c opt --copt=-mavx\r\n> --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-mfma --copt=-msse3\r\n> --config=cuda --genrule_strategy=standalone --spawn_strategy=standalone\r\n> //tensorflow/tools/pip_package:build_pip_package\r\n>\r\n\r\nAnd the final package creation:\r\n\r\n\r\n>  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n> /tmp/tensorflow_pkg\r\n>\r\n\r\nUPDATE : adding graph update capabilities:\r\n\r\n> bazel build tensorflow/tools/graph_transforms:transform_graph\r\n\r\n\r\n\r\n> I really hope it helps.\r\n   Alex\r\n\r\n2018-02-10 13:44 GMT+01:00 lucy-itjob <notifications@github.com>:\r\n\r\n> Alex,\r\n>\r\n> Thank you for all this information.\r\n>\r\n> I am new in this area. To me, you are the first one land on the moon in\r\n> this area, I am still on the earth, and pulling my hair to get this\r\n> Tensorflow working on RHEL6, hoping some day I can land on the moon just\r\n> like you did.\r\n>\r\n> Can you do me a favor? I know you had listed your local customized files\r\n> which modified by you in previously email chain, could you please sent all\r\n> the your local customized files (Bazel and Tensorflow) which modified by\r\n> you to me directly, my email is lucy_itjob@comcat.net. could you please\r\n> include Crosstool files for current and previous version (give me the\r\n> current version name and download url) too ? So that I can use the code to\r\n> compare my environment based on your files.\r\n>\r\n> You know, to be able to walk on the moon, it is not easy since there are\r\n> no Tensorflow vendor support on RHEL6.\r\n>\r\n> Alex, I attach an picture for you to enjoy! Thanks a lot\r\n> [image: image]\r\n> <https://user-images.githubusercontent.com/25250352/36062221-4b206a64-0e2d-11e8-87bc-ec48874097a9.png>\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/8529#issuecomment-364649132>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ACf6SPNswb4TaN4ozMc6FfsbQDu_nXFDks5tTY9JgaJpZM4Mhr7n>\r\n> .\r\n>\r\n\r\n\r\n\r\n-- \r\nAlexandre BENOIT,\r\nAssociate Professor / Ma\u00eetre de Conf\u00e9rence\r\nImage processing and visual scene classification,\r\nLISTIC Lab / IUT Annecy\r\nhttps://sites.google.com/site/benoitalexandrevision/\r\n", "Maybe an additionnal hint reported by #14573\r\n\r\n--action_env=LD_LIBRARY_PATH=/path/to/cuda/lib64/stubs:${LD_LIBRARY_PATH}", "Compiling TF 1.12 on CentOS7 with :\r\n\r\n- devtoolset-4 installed\r\n- bazel 0.15\r\n- cuda 9.1 & cudnn 7.1.3\r\n\r\nCurrently works for CPU and GPU modes with the following changes (surely too many changes but it works...):\r\n\r\n```\r\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL.tpl b/third_party/gpus/crosstool/CROSSTOOL.tpl\r\nindex 3189cf8..b2b6885 100644\r\n--- a/third_party/gpus/crosstool/CROSSTOOL.tpl\r\n+++ b/third_party/gpus/crosstool/CROSSTOOL.tpl\r\n@@ -44,6 +44,9 @@ toolchain {\r\n       action: \"c++-compile\"\r\n       flag_group {\r\n         flag: \"-std=c++11\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -56,6 +59,9 @@ toolchain {\r\n       action: \"c++-link-nodeps-dynamic-library\"\r\n       flag_group {\r\n         flag: \"-lstdc++\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -84,6 +90,9 @@ toolchain {\r\n       action: \"c++-link-executable\"\r\n       flag_group {\r\n         flag: \"-Wl,-no-as-needed\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -125,6 +134,9 @@ toolchain {\r\n       action: \"c++-link-nodeps-dynamic-library\"\r\n       flag_group {\r\n         flag: \"-Wl,-z,relro,-z,now\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n     flag_set {\r\n@@ -132,6 +144,9 @@ toolchain {\r\n       flag_group {\r\n         flag: \"-pie\"\r\n         flag: \"-Wl,-z,relro,-z,now\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -144,6 +159,9 @@ toolchain {\r\n       flag_group {\r\n         # All warnings are enabled. Maybe enable -Werror as well?\r\n         flag: \"-Wall\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n         %{host_compiler_warnings}\r\n       }\r\n     }\r\n@@ -157,6 +175,9 @@ toolchain {\r\n       action: \"c++-compile\"\r\n       flag_group {\r\n         flag: \"-fno-omit-frame-pointer\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -171,6 +192,9 @@ toolchain {\r\n         # Stamp the binary with a unique identifier.\r\n         flag: \"-Wl,--build-id=md5\"\r\n         flag: \"-Wl,--hash-style=gnu\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -185,6 +209,9 @@ toolchain {\r\n       action: \"c++-link-nodeps-dynamic-library\"\r\n       flag_group {\r\n         flag:\"-no-canonical-prefixes\"\r\n+        flag:\"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -258,6 +285,9 @@ toolchain {\r\n       action: \"c++-link-executable\"\r\n       flag_group {\r\n         flag: \"-Wl,--gc-sections\"\r\n+        flag: \"-lm\"\r\n+        flag: \"-lrt\"\r\n+        flag: \"-B/opt/rh/devtoolset-4/root/usr/bin/\"\r\n       }\r\n     }\r\n   }\r\n@@ -283,16 +313,16 @@ toolchain {\r\n   tool_path { name: \"gcc\" path: \"%{host_compiler_path}\" }\r\n \r\n   # Use the default system toolchain for everything else.\r\n-  tool_path { name: \"ar\" path: \"/usr/bin/ar\" }\r\n-  tool_path { name: \"compat-ld\" path: \"/usr/bin/ld\" }\r\n-  tool_path { name: \"cpp\" path: \"/usr/bin/cpp\" }\r\n-  tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\r\n-  tool_path { name: \"gcov\" path: \"/usr/bin/gcov\" }\r\n-  tool_path { name: \"ld\" path: \"/usr/bin/ld\" }\r\n-  tool_path { name: \"nm\" path: \"/usr/bin/nm\" }\r\n-  tool_path { name: \"objcopy\" path: \"/usr/bin/objcopy\" }\r\n-  tool_path { name: \"objdump\" path: \"/usr/bin/objdump\" }\r\n-  tool_path { name: \"strip\" path: \"/usr/bin/strip\" }\r\n+  tool_path { name: \"ar\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ar\" }\r\n+  tool_path { name: \"compat-ld\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\r\n+  tool_path { name: \"cpp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/cpp\" }\r\n+  tool_path { name: \"dwp\" path: \"/opt/rh/devtoolset-4/root/usr/bin/dwp\" }\r\n+  tool_path { name: \"gcov\" path: \"/opt/rh/devtoolset-4/root/usr/bin/gcov\" }\r\n+  tool_path { name: \"ld\" path: \"/opt/rh/devtoolset-4/root/usr/bin/ld\" }\r\n+  tool_path { name: \"nm\" path: \"/opt/rh/devtoolset-4/root/usr/bin/nm\" }\r\n+  tool_path { name: \"objcopy\" path: \"/opt/rh/devtoolset-4/root/usr/bin/objcopy\" }\r\n+  tool_path { name: \"objdump\" path: \"/opt/rh/devtoolset-4/root/usr/bin/objdump\" }\r\n+  tool_path { name: \"strip\" path: \"/opt/rh/devtoolset-4/root/usr/bin/strip\" }\r\n \r\n   # Enabled dynamic linking.\r\n   linking_mode_flags { mode: DYNAMIC }\r\ndiff --git a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\nindex f4f4d0e..b7cf44b 100755\r\n--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n@@ -47,8 +47,8 @@ import pipes\r\n \r\n # Template values set by cuda_autoconf.\r\n CPU_COMPILER = ('%{cpu_compiler}')\r\n-GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\r\n-\r\n+#GCC_HOST_COMPILER_PATH = ('%{gcc_host_compiler_path}')\r\n+GCC_HOST_COMPILER_PATH = ('/opt/rh/devtoolset-4/root/usr/bin/gcc')\r\n NVCC_PATH = '%{nvcc_path}'\r\n PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)\r\n NVCC_VERSION = '%{cuda_version}'\r\n```\r\n\r\n "]}, {"number": 8528, "title": "There is a learning_rate parameter in the Adadelta implementation although there isn't such one in the original paper", "body": "Hi,\r\nI see in the documentation that `tf.train.AdadeltaOptimizer` has a `learning_rate` parameter, but in the original paper that parameter was eliminated from the update rule [see equation 14 in the paper](http://arxiv.org/pdf/1212.5701v1.pdf) (or step 5 in the pseudo code of the algorithm).\r\nIs that a bug in the implementation? \r\nIf you deliberately add a learning_rate parameter, I think that the API documentation should state explicitly that this is NOT the exact implementation of the algorithm as in the paper.", "comments": ["@Mistobaan could you look at this?", "To me it looks like with lr==1 you get exactly the paper. @eranamar, could you submit a pull request to that effect, please?\r\n", "Can you please explain what should I do? (I am new to this...)", "I will take care of updating the docs."]}, {"number": 8527, "title": "configure script shouldn't ask interactive questions when its output isn't a terminal", "body": "It should use the default values.", "comments": ["A workaround for now is to use `yes '' | ./configure`. This is equivalent to accepting all the default values.", "When the default isn't good enough, you can also set the variables before running `./configure`. I used something similar to this to compile TF in Docker:\r\n```\r\nENV TF_NEED_CUDA=1 \\\r\n GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n TF_CUDA_VERSION=8.0 \\\r\n CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n TF_CUDNN_VERSION=5.1.10 \\\r\n CUDNN_INSTALL_PATH=/usr/local/cuda \\\r\n TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.2,6.0 \\\r\n CC_OPT_FLAGS=\"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --copt=-mfpmath=both --config=cuda\" \\\r\n PYTHON_BIN_PATH=\"/usr/bin/python\" \\\r\n USE_DEFAULT_PYTHON_LIB_PATH=1 \\\r\n TF_NEED_JEMALLOC=1 \\\r\n TF_NEED_GCP=0 \\\r\n TF_NEED_HDFS=0 \\\r\n TF_ENABLE_XLA=0 \\\r\n TF_NEED_OPENCL=0\r\n\r\nRUN ./configure && \\\r\n  ......\r\n```", "I can confirm that @phvu answer is working quite well. Just look through the configure script and you will find many lines like \r\n`if [ -z \"$PYTHON_BIN_PATH\" ]; then`\r\nso, it will ask interactively e.g. for the Python path if you do not set the $PYTHON_BIN_PATH variable.\r\n\r\nBut if you set all the variables, it will work without further asking.\r\n"]}, {"number": 8526, "title": "\"Feed\" cannot run in Windows", "body": "Environment:\r\nWindows 10\r\nCPU: i7 6700HQ\r\nGPU: 960M\r\nDriver: 368.71\r\nCUDA: 8.0  \r\nCUDNN: 5\r\nPython 3.5.2\r\nTensorFlow-GPU 1.0.1\r\n-------------------------------------------------------------\r\nWhen I try to run this    \r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ninput1 = tf.placeholder(tf.float32)\r\ninput2 = tf.placeholder(tf.float32)\r\noutput = tf.add(input1, input2)  \r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run([output], feed_dict={input1:32.2, input2:13.35}))\r\n```\r\n\r\nMy program will crash and show that Python has stopped working.  \r\n\r\nThis is trace:\r\n```\r\nE:\\PyCode>python test.py\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 960M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:02:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.35GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)\r\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #0: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #1: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #2: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #3: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #4: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #5: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #6: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #7: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #8: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #9: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #10: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #11: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #12: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #13: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #14: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #15: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #16: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #17: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #18: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #19: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #20: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #21: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #22: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #23: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #24: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #25: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #26: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #27: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #28: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #29: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #30: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #31: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #32: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #33: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #34: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #35: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #36: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #37: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #38: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #39: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #40: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #41: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #42: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #43: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #44: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #45: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #46: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #47: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #48: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #49: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #50: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #51: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #52: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #53: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #54: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #55: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #56: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #57: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #58: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #59: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #60: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #61: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #62: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #63: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #64: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #65: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #66: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #67: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #68: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #69: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #70: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #71: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #72: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #73: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #74: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #75: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #76: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #77: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #78: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #79: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #80: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #81: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #82: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #83: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #84: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #85: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #86: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #87: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #88: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #89: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #90: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #91: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #92: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #93: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #94: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #95: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #96: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #97: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #98: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #99: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #100: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #101: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #102: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #103: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #104: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #105: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #106: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #107: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #108: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #109: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #110: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #111: CUDA driver version is insufficient for CUDA runtime version\r\n```\r\nBut, if change test.py as this\r\n```Python\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n    with tf.device('/cpu:0'):\r\n        input1 = tf.placeholder(tf.float32)\r\n        input2 = tf.placeholder(tf.float32)\r\n        output = tf.add(input1, input2)  \r\n        print(sess.run([output], feed_dict={input1:32.2, input2:13.35}))\r\n```\r\nIt will OK \r\n```\r\n(py35) E:\\PyCode>python test.py\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 960M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:02:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 3.35GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:02:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n[45.550003]\r\n```\r\n\r\n", "comments": ["I think you need upgrade your nv driver and cuda.", "I have similar issue, can't figure out still what's happening.\r\n\r\nCode:\r\n```python\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nwith tf.device('/gpu:0'):\r\n    node = tf.constant(3.0)\r\nprint(sess.run(node))\r\n```\r\n\r\nEnvironment:\r\nWindows 10\r\nGPU: NVIDIA GeForce GTX TITAN X\r\nCUDA: 8.0\r\nCUDNN: 5\r\nPython 3.5.2\r\nTensorFlow-GPU 1.0.1\r\n\r\n```cudaRuntimeGetVersion``` returns 8000;\r\n```cudaDriverGetVersion``` returns 8000;\r\n\r\nOutput:\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:01:00.0\r\nTotal memory: 12.00GiB\r\nFree memory: 8.36GiB\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:590] creating context when one is currently active; existing: 000000AF8F028190\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 1 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:03:00.0\r\nTotal memory: 12.00GiB\r\nFree memory: 8.68GiB\r\nW c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:590] creating context when one is currently active; existing: 000000AF8F02BAB0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 2 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2155\r\npciBusID 0000:02:00.0\r\nTotal memory: 12.00GiB\r\nFree memory: 11.73GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0 1 2\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y N N\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 1:   N Y N\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 2:   N N Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci\r\n bus id: 0000:01:00.0)\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci\r\n bus id: 0000:03:00.0)\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci\r\n bus id: 0000:02:00.0)\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtrem\r\nelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretS\r\ntringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n\r\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #0: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #1: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #2: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #3: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #4: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #5: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #6: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #7: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #8: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #9: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #10: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #11: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #12: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #13: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #14: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #15: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #16: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #17: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #18: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #19: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #20: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #21: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #22: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #23: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #24: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #25: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #26: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #27: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #28: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #29: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #30: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #31: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #32: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #33: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #34: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #35: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #36: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #37: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #38: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #39: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #40: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #41: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #42: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #43: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #44: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #45: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #46: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #47: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #48: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #49: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #50: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #51: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #52: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #53: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #54: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #55: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #56: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #57: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #58: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #59: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #60: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #61: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #62: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #63: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #64: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #65: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #66: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #67: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #68: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #69: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #70: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #71: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #72: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #73: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #74: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #75: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #76: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #77: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #78: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #79: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #80: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #81: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #82: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #83: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #84: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #85: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #86: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #87: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #88: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #89: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #90: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #91: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #92: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #93: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #94: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #95: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #96: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #97: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #98: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #99: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #100: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #101: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #102: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #103: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #104: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #105: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #106: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #107: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #108: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #109: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #110: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #111: CUDA driver version is insufficient for CUDA runtime version\r\n3.0\r\n```", "Looks like it loads all the libraries, recognizes all the GPUs, but then something strange happens.\r\nHow does tensorflow decide if the cuda driver version is insufficient?\r\nThere is another program, which works fine on GPU, so I suppose the problem is on the tensorflow side.", "But can get the ans \"3.0\"", "Yes, but I thought it just fell back to CPU to produce this answer.\r\nIs there a way to debug this?", "Wait for the developer to answer this question ", "\"Driver: 368.71\"\r\n\r\nCould you please upgrade your driver to a newer version?\r\n", "@snnn  YES, I will have a try", "NICE !! I upgrade Driver from 368.71 to 378.78, it run!", "@snnn Could you please point to the exact minimum required version of the driver to install?\r\nBtw, it is not very simple to upgrade drivers in my case. What other solutions do I have? Use older cuda runtime? Downgrade tensorflow? Is there a chance this error can be resolved without upgrading drivers?\r\n\r\nMaybe you could show some code fragment which produces these error messages? So I could compile it and test outside of tensorflow context?\r\nI thought the \"insufficiency\" is determined by returning values of cudaRuntimeGetVersion and cudaDriverGetVersion, but they both return 8000 on my machine.", "Hi @sawlogs , the minimum cuda version is 8.0, which was released 5 months ago.  And the driver should be newer than this.  I don't have an exact answer for you. I suggest you post your question to stackoverflow or just ask your NV customer support.", "@snnn Ok, but I just don't understand how tensorflow fails on this. If it uses cuda api, so does my cuda app, which works. I simply find it strange. Or it uses some really low-level driver API, then it's a different story.\r\nApparently, some calls to api work, and others fail.\r\n\r\nAnd I can't even grep the source code for this error message, which is probably the most annoying thing. Where does this error message live? Nobody knows :)\r\n\r\nWhatever. Thanks and sorry for interrupting, maybe I will investigate it later.", "The error is emitted from CUDA.\r\nYou need to use the GPU drivers recommended by NVIDIA to use CUDA properly.\r\nI am closing this issue, because it is not a TensorFlow bug.\r\n\r\n"]}, {"number": 8525, "title": "Tensorflow first time run error.", "body": "\r\nI installed tensorflow throuh pip3 in windows. And it installed successfully.\r\nThen i run a simple program to check if it is working or not. I got this error.\r\nPython version 35. \r\nCODE :  \r\n```\r\nimport tensorflow as tf  \r\nhello = tf.constant('Hello, TensorFlow!')  \r\nsess = tf.Session()  \r\nprint(sess.run(hello))\r\n```  \r\n\r\nOutput :\r\n\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') fo\r\nr unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_t\r\nype: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"')\r\n for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for\r\nunknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_ty\r\npe: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"')\r\nfor unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"'\r\n) for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') fo\r\nr unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') fo\r\nr unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"\r\n') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\r\n\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"C\r\nPU\"') for unknown op: UpdateFertileSlots\r\nb'Hello, TensorFlow!'", "comments": ["Are you running the latest version of TensorFlow?\r\n\r\nRegardless, if you look at the bottom after the \"unknown op\" messages, it is printing the expected result: `b'Hello, TensorFlow!'` It looks like you're only missing ops for building decision trees, so you should still be able to run (most) TensorFlow models successfully.", "@skye Yes I am running latest version of tensorflow on python 3.5.2 . You know any way to get rid of those unknown op errors. Thanks for replying..", "@poojan124 \r\nEDIT: Can you try out one of the nighties? What pip command did you use to install?\r\nSee if #7859 helps.\r\n\r\niirc they should disappear in 1.0.1. Can you confirm your tensorflow version?", "@jubjamie  i am using 1.0.1  version. I used `pip3 install --upgrade tensorflow` command to install on my windows. I will try nightly build if it take out those op errors.", "Nightly should help with most of those unknown op issues, if not all.\r\nAlso, soon 1.1 will be released, which will be similar to nightlies.", "@poojan124 I had the exact same issue and installing the nightly resolved it. I recommend following the instructions on #8500. \r\n\r\nTensorflow was a pain to install on Windows 10 so I hope 1.1 will fix these issues.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8524, "title": "Asking for TF's official implementation of the inception v3 model's distributed training", "body": "From the recorded video of TF dev summit held about one month ago, I got to know the newest distributed TF 1.0 is able to achieve 57x speed up for the inception v3 model on a server of 8 nodes/ 8 gpus and the codes will be released. Since I am far away from getting such a good scaling performance using my own distributed implementation, I have been looking forward to seeing and studying from the training codes from TF. Could anyone tell me the progress now? Thanks!", "comments": ["duplicate of https://github.com/tensorflow/tensorflow/issues/7679", "@ppwwyyxx sorry, I didn't notice that thread. This one can be closed."]}, {"number": 8523, "title": "Usage of sparse_average_precision_at_k when dense labels are available", "body": "Is there any example for usage of sparse_average_precision_at_k?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8522, "title": "Disable FILE_FLAG_RANDOM_ACCESS in WindowsFileSystem", "body": "You will run out of physical memory if you need to read a large file on Windows.\r\nhttps://support.microsoft.com/en-us/help/2549369\r\nThe behavior looks very like a memory leak, however, the process's memory usage doesn't grow.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Some performance test result after this change:\r\n\r\ninput data: A text file in libsvm format, 10906667862 bytes, 164540416 lines, with CRLF line terminators\r\n\r\ntest code:\r\n```cpp\r\n#include <mutex>\r\n#include <Windows.h>\r\n#include <stdio.h>\r\n\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/io/inputbuffer.h\"\r\nusing namespace tensorflow;\r\n\r\nint test1(const char* filename) {\r\n\tLARGE_INTEGER li;\r\n\tQueryPerformanceFrequency(&li);\r\n\tEnv* env = Env::Default();\r\n\tstd::unique_ptr<RandomAccessFile> file_;\r\n\tconst ::tensorflow::Status status = env->NewRandomAccessFile(filename, &file_);\r\n\tif (!status.ok()) {\r\n\t\treturn -1;\r\n\t}\r\n\tLARGE_INTEGER startTime, endTime;\r\n\tint kBufferSize = 2 * 1024 * 1024;\r\n\tio::InputBuffer* input_buffer = new io::InputBuffer(file_.get(), kBufferSize);\r\n\tQueryPerformanceCounter(&startTime);\r\n\tstring line_contents;\r\n\twhile (true) {\r\n\t\tStatus status = input_buffer->ReadLine(&line_contents);\r\n\t\tif (errors::IsOutOfRange(status)) {\r\n\t\t\tbreak;\r\n\t\t}\r\n\t\tif (!status.ok()) {\r\n\t\t\treturn -1;\r\n\t\t}\r\n\t}\r\n\tQueryPerformanceCounter(&endTime);\r\n\tconst double seconds = ((endTime.QuadPart - startTime.QuadPart) / (double)li.QuadPart);\r\n\tprintf(\"dur = %g\\n\", seconds);\r\n\treturn 0;\r\n}\r\n\r\nint test2(const char* filename) {\r\n\tLARGE_INTEGER li;\r\n\tQueryPerformanceFrequency(&li);\r\n\tint kBufferSize = 2 * 1024 * 1024;\r\n\tstd::unique_ptr<char[]> buf(new char[kBufferSize]);\r\n\tFILE* fd = fopen(filename, \"rb\");\r\n\tif (fd == nullptr) return -1;\r\n\tLARGE_INTEGER startTime, endTime;\r\n\tQueryPerformanceCounter(&startTime);\r\n\twhile (fgets(buf.get(), kBufferSize, fd) != nullptr) {\r\n\r\n\t}\r\n\tQueryPerformanceCounter(&endTime);\r\n\tconst double seconds = ((endTime.QuadPart - startTime.QuadPart) / (double)li.QuadPart);\r\n\tprintf(\"read by fgets, dur = %g\\n\", seconds);\r\n\treturn 0;\r\n}\r\n\r\nint main(int argc,char* argv[]) {\r\n\tconst char* filename = argv[1];\r\n\tconst char* method = argv[2];\r\n\tif (_stricmp(method, \"fgets\")==0) {\r\n\t\treturn test2(filename);\r\n\t}\r\n\treturn test1(filename);\r\n}\r\n\r\n```\r\n\r\nResult:\r\nfile on SSD, read by fgets, dur = 35 seconds\r\nfile on SSD, read by InputBuffer, dur = 19 seconds\r\nfile on HardDisk, read by fgets, dur = 104 seconds\r\nfile on HardDisk, read by InputBuffer, dur = 114 seconds\r\n\r\nSo, I think it's ok to simply remove FILE_FLAG_RANDOM_ACCESS flag.\r\n\r\n\r\n", "I don't have a strong opinion about the change, but it would be great to get an opinion from @vit-stepanovs or @guschmue, who worked on this part of the Windows patch.", "@snnn - can you provide performance numbers before and after your change ?\r\nBasically how much perf hit is it removing FILE_FLAG_RANDOM_ACCESS?\r\nWhat at what filesize do you start seeing issues ?", "Hi @guschmue , Could you tell me why do you need this flag?", "Didn't say we need the flag. I think this code was used for some storage project where random access is common (aka FILE_FLAG_RANDOM_ACCESS is used to not evict behind. I don't think this is the common access pattern for RandomAccessFile in tensorflow and removing FILE_FLAG_RANDOM_ACCESS might be a good thing for sequential reads.\r\nSince you already measure perf after, why not measure before as well to confirm that this also helps performance.", "Hi @guschmue  \r\n\r\n1. FILE_FLAG_RANDOM_ACCESS flag is a hint for Cache Manager to keep mapped views(read blocks) of the file in memory as long as possible, until Memory Manager doesn\u2019t signal low memory condition.  So, if we run tensorflow standalone, without sharing resources with other programs, the performance is almost the same. However, if we run it on yarn, the situation is quite different.  And it's hard to say how much it faster/slower because it number varies, and there is no way to control tensorflow's memory usage.\r\n\r\n2. At the same time, this flag instructs Cache Manager to disable prefetching of file data, which could reduce unnecessary disk io and increase read latency.  However, we already have IoBuffer as the user mode cache. For sequential reads, I can't see any difference.\r\n\r\n@mrry  It would be great if there is a new file type like \"SeqAccessFile\". Actually, FILE_FLAG_RANDOM_ACCESS  is good for random accesses(e.g. KV DBs, sparse model serving). It is just not good for sequential reads.", "Agree, it should be better to turn FILE_FLAG_RANDOM_ACCESS off ... will be more common to have sequential io.", "I'm happy with this change if @guschmue is :). I agree that a specific sequential-access file interface would make sense for most input file processing; I think the main case for random access is when restoring a checkpoint. It's possible that (with this change) restoring a checkpoint will get slower but reading input files will get faster, and we should keep an eye on the former."]}, {"number": 8521, "title": "Add tf.repeat equivalent to np.repeat", "body": "In some applications, we need to repeat Tensors, so hopefully there could be a build-in function tf.repeat which is equivalent to np.repeat. \r\n\r\nExample:\r\n```\r\n np.repeat([1, 2, 3], 2)\r\n>> array([1, 1, 2, 2, 3, 3])\r\n```\r\n", "comments": ["I am not aware of anyone working on this.  We'd happily take a patch!  \r\n\r\nThat said, for simple cases (I understand `np.repeat` supports more general cases) like the one you cited, you could do \r\n```python\r\nwith tf.Session() as sess:\r\n  print sess.run(tf.reshape(\r\n      tf.tile(tf.expand_dims([1, 2, 3], -1),  [1, 2]), [-1]))\r\n```\r\nwhich is both CPU- and GPU-compatible.", "@concretevitamin I am new to contributing to other projects. May i work on this issue?", "I have been working on this issue and have written the c++ code for op.\r\nI encountered the following error which i was unable to resolve.\r\n\r\n1) Upon accessing the 'i'th element of object of tensor class,using\r\n\"object(i)\"\r\nfollowing error was shown:-\r\n**error: no match for call to \u2018(tensorflow::Tensor) (int&)\u2019**\r\n\r\n2)Another one\r\n```\r\ntensorflow/core/framework/op_kernel.h: In member function \u2018tensorflow::int64 tensorflow::PersistentTensor::AllocatedBytes() const\u2019:\r\ntensorflow/core/framework/op_kernel.h:210:49: error: \u2018const class tensorflow::Tensor\u2019 has no member named \u2018AllocatedBytes\u2019\r\n   int64 AllocatedBytes() const { return tensor_.AllocatedBytes(); }\r\n```\r\nThis one is in the header file \"op_kernel.h\" itself.\r\n\r\nI referenced [this](https://www.tensorflow.org/extend/adding_an_op) for adding the op.\r\nPlease help.", "@sidjee Are you still working on this? I have the codes ready. I think I will send a PR tomorrow.", "This is a duplicate of https://github.com/tensorflow/tensorflow/issues/8246. I'm closing this in favor of the other issue, which is older."]}, {"number": 8520, "title": "Fix misalign of code blocks in `tensorflow/tools/ci_build/README.md`", "body": "The code blocks in `tensorflow/tools/ci_build/README.md` are misaligned,\r\ndue to the unaligned pair of \"```bash\" and \"```\".\r\n\r\nThe following was the previous screenshot:\r\n\r\n<img width=\"1101\" alt=\"screen shot 2017-03-18 at 6 06 18 am\" src=\"https://cloud.githubusercontent.com/assets/6932348/24072287/5b68a9c4-0ba2-11e7-944e-9d1a3deabdd2.png\">\r\n\r\n\r\nThis fix fixes the misalign. The following is the new screenshot:\r\n<img width=\"972\" alt=\"screen shot 2017-03-18 at 6 16 47 am\" src=\"https://cloud.githubusercontent.com/assets/6932348/24072302/9f630cd2-0ba2-11e7-8810-032273cef17b.png\">\r\n", "comments": ["Can one of the admins verify this patch?", "PR merged. Thank you for the fix, @yongtang !"]}, {"number": 8519, "title": "error while building tensorflow from source", "body": "_I am trying to install tensorflow from source and getting this error while building tensorflow from bazel using the command:_ \r\n\r\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n_and the error is:_\r\n\r\n`ERROR: /home/prakash/.cache/bazel/_bazel_prakash/8d51bbac70ad4d48b6548f0de307c16b/external/nccl_archive/BUILD.bazel:33:1: error while parsing .d file: /home/prakash/.cache/bazel/_bazel_prakash/8d51bbac70ad4d48b6548f0de307c16b/execroot/tensorflow/bazel-out/local_linux-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/reduce.cu.pic.d (No such file or directory).\r\n\r\n\r\nnvcc fatal   : Unsupported gpu architecture 'compute_21'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 360.810s, Critical Path: 345.61s`\r\n\r\n_I have a Nvidia 720M graphic chip on my laptop is that causing the issue .? any advice would be helpful.\r\nthe configuration i did was this:_\r\n\r\n`prakash@prakash008:~/Downloads/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /home/prakash/anaconda3/bin/python]: \r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] n\r\njemalloc disabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /home/prakash/anaconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/prakash/anaconda3/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /home/prakash/anaconda3/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.5,2.1`", "comments": ["The compute capability of your graphics card is lower than TensorFlow supports. Please see https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 for a possible workaround."]}, {"number": 8518, "title": "tf.contrib.seq2seq Documentation", "body": "Where can I find documentation for `tf.contrib.seq2seq`?\r\n\r\nI get a 404 error on [attention_decoder_fn.py].(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py)\r\n\r\nNamely, I want to hack the `attention_decoder_fn_inference` or `dynamic_rnn_decoder` function during inference in order to sample the outputs of each cell based on output probabilities before feeding into the new cell. The aforementioned functions seem to be a black box that predict the most probable class at each cell.\r\n\r\nThanks", "comments": ["The file's been deleted in master branch.  In general, I'd suggest if you're working on master, just peek into the actual source files for the latest consistent set of docs.", "I think it move into `attention_decoder_fn_inference` or something else?"]}, {"number": 8517, "title": "CUDA_ERROR_DEINITIALIZED running CIFAR10_multi_gpu on CUDA8.0 with cuDNN v5, built from pip", "body": "## Environment info\r\n\r\n### Hardware Platform: \r\nGoogle Cloud Compute Engine\r\n4 vCPUs, 15 GB memory\r\n4 * NVIDIA Tesla K80\r\n\r\n### Software Platform: \r\nOperating System: Ubuntu 16.04\r\ncuDNN v5\r\nCUDA toolkit 8.0\r\nCUDA Compute Capability 3.7\r\n\r\n### Tensor-flow:\r\nInstalled from pip \r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl\r\n(Installing from source also doesn't work)\r\n## Steps to reproduce\r\n\r\npython3 cifar10_multi_gpu_train.py --num_gpus=4\r\n\r\nError occurs after training.\r\n\r\n## What have you tried?\r\n\r\n1. restart machine and rerun several times;\r\n2. --num_gpus=2 --> Segmentation fault\r\n3. Reinstall Tensor-flow from source\r\n\r\n## Logs or other output that would be helpful\r\nLog file is attached.\r\n[Terminal Saved Output.txt](https://github.com/tensorflow/tensorflow/files/852192/Terminal.Saved.Output.txt)\r\n", "comments": ["Is this exception arising spontaneously, or in response to a Ctl-C or similar interrupt?  I can only reproduce it by deliberately killing the program.  ", "Than you poxvoculi, it occurs every time I run the program.\r\n\r\nActually, this issue does not occur on the TensorFlow built from source. But it does occur on pip version.\r\n\r\nBTW, I think it only happens on multi-gpu system.", "In this case, the CUDA_ERROR_DEINITIALIZED error is probably a side effect of some other error that is causing a CUDA kernel to fail, and not a primary error.\r\n\r\nWhen a CUDA kernel fails in a way that can't just return an error status to the next driver call, the driver shuts down.  However, TensorFlow still has some CUDA Event polling threads running (one for each GPU), and these can very quickly field the CUDA_ERROR_DEINITIALIZED error when they can into the terminating driver, and then this error gets propagated back up to python and printed to the terminal.\r\n\r\nI can't reproduce your problem in a clean install in GCE, or building from source in GCE.  I tried pip installing the source you cit and got:\r\n\r\ntensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\n", "Thank you. I am not sure why you cannot reproduce my problem. Here's my infrastructure information and software detail:\r\n\r\nHardware Platform:\r\n4 vCPUs, 15 GB memory\r\n4 * NVIDIA Tesla K80\r\n\r\nSoftware Platform:\r\nOperating System: Ubuntu 16.04\r\ncuDNN v5\r\nCUDA toolkit 8.0\r\nCUDA Compute Capability 3.7\r\n\r\nAnd I am using python3.4.", "@zheng-xq Any ideas?  Is the set of software versions above compatible with the pip wheel?  \r\n[This doc page](https://www.tensorflow.org/versions/r0.10/get_started/os_setup) says that .whl is for CUDA 7.5 only.  Maybe that's what's causing the error, if building from source works.", "I am updating the question that building from source has the same problem with 2 GPUs. It seems to me that Tensor-Flow can only work with single GPU.\r\n\r\nI am posting the newest detail of my infrastructure:\r\n\r\nHardware:\r\nGoogle Cloud Compute Engine\r\nNVIDIA Tesla K80 * 2\r\n\r\nSoftware:\r\nUbuntu 14.04\r\nCUDA 8.0\r\ncuDNN 5.0\r\nTensor-Flow up-to-date with master branch\r\n", "@jw447 are you still experiencing this problem?", "@jw447 I am experiencing the same issue on both a 4 GPUs and a 2 GPUs machine. I have been playing around with configurations and so far I have noticed that the crash only happens if gpu:1 is being called. I tried using only gpu:1 but encountered the same problem. On the other end, the same script using with tf.device('gpu:0') works fine. \r\n\r\nDid you find a solution in the end? Was the problem entirely absent on tensorflow from source?I think the tensorflow install I am working with was done using pip.", "@jw447, @Vargeel, could someone confirm this problem still persists with the latest TensorFlow and models?", "Any updates guys? Having the same problem", "I'm getting it on Tesla K40:\r\n\r\n```\r\n2018-04-11 18:28:02.715560: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\n2018-04-11 18:28:02.715614: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\nProcess exited with return code -6.\r\n```\r\n\r\ntensorflow-gpu==1.4.0 installed with pip\r\nCUDA 8.0\r\ncudNN 6.0.21\r\ncompute capability 3.7\r\npython 3.5.2\r\n\r\nLinux CentOS kernel 3.10.0-693.11.6.el7.x86_64", "I'm having a similar issue in a 4 GPU containerized machine. If I run single job in the node taking 1 GPU, everything works fine. However, if I run two concurrent jobs in the same node, each takes 1 GPU, it ends in \r\n`E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED`\r\nThis happens when the job has finished the container is being stopped.", "Hi Oliver and Adam, \r\nI ended up managing to avoid this issue during its previous occurence. (sorry I went silent on the thread, since this is an issue that arised on a work project it severly limited my ability to share code)\r\nThe core of my problem was that I had some custom CUDA code running for some operators and they internally initialised a device number without receiving it from tensorflow. In the end the only way I managed to run my code efficiently was to have the custom op run on its own GPU while the rest of the graph was parallelized. Definitely hurt perfomance but it did manage to artificially increase my batch size.", "@Vargeel Thanks for the info. For us the GPUs are already containerized but still facing the same issue. The training actually finished successfully but fails at container cleaning up phase.. thinking of manually set the training flag to true if the actual training pass.", "Nagging Assignee @zheng-xq: It has been 120 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could someone confirm this problem still persists ?", "Please open a new issue if it still persists with the latest tensorflow version when CUDA 9 and cuDNN 7 are used. ", "I am facing this issue. TF 1.4", "I am having the same issue on GKE instance with tensorflow 1.12:\r\n\r\n```\r\n2019-02-22 09:04:34.880388: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-02-22 09:04:34.881071: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\nFatal Python error: Aborted\r\n```"]}, {"number": 8516, "title": "Missing pip install file for Python3 GPU version for MacOS platform", "body": "Receive the following error when attempting to install GPU TF 1.0.1\r\n\r\n`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl`\r\n\r\n`Collecting tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl\r\n  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl\r\n  Could not install requirement tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl\r\nCould not install requirement tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl for URL https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl`\r\n\r\n### Environment info\r\nOperating System:  MacOS 10.12.3\r\n\r\nPython 3.5.2 |Anaconda 4.3.1 (x86_64)| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\n\r\n\r\n", "comments": ["Yea, same problem here. \r\n\r\nThe non-gpu versions (ie, tensorflow/mac/cpu/* ) work fine, but the gpu ones are missing.\r\n\r\nPython version is 3.5.2, conda 4.0.5, pip 9.0.1", "@yifeif @av8ramit could you check our packages in storage.googleapis.com?\r\n\r\n@jimthompson5802  @dwillmer you may download the packages from here in the meantime:\r\nhttps://pypi.python.org/pypi/tensorflow-gpu/1.0.1", "Checked and we didn't have the py3 gpu mac pip in google storage. It is uploaded now. Closing this issue. Feel free to reopen if you are still seeing issues.", "I am now able to install Python3 GPU version of TensorFlow 1.0.1.\r\n\r\nThank you."]}, {"number": 8515, "title": "Gradients Tutorial feature requests.", "body": "\r\nI need to modify the tensorflow source for specific ops. When add one more input to the `REGISTER_OP` from\r\n\r\n    .Input(\"feature: T\")\r\n    .Input(\"labels: T\")\r\nto \r\n\r\n    .Input(\"feature: T\")\r\n    .Input(\"labels: T\")\r\n    .Input(\"my_input: T\")\r\n\r\nthe forward version of corresponding codes do work, but the backpropagation version of corresponding codes seem to need further modification. Since the organization of TF source codes seems too complicated to me, I don't what to modify for the next step.\r\n\r\nHere is the Error I got:\r\n  \r\n        grads = optimizer.compute_gradients(total_loss)\r\n      File \"/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 345, in compute_gradients\r\n        colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n      File \"/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 488, in gradients\r\n        _VerifyGeneratedGradients(in_grads, op)\r\n      File \"/home/jake/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 255, in _VerifyGeneratedGradients\r\n        \"inputs %d\" % (len(grads), op.node_def, len(op.inputs)))\r\n    ValueError: Num gradients 2 generated for op name: \"softmax_cross_entropy_loss/xentropy\"\r\n    op: \"SoftmaxCrossEntropyWithLogits\"\r\n    input: \"softmax_cross_entropy_loss/Reshape\"\r\n    input: \"softmax_cross_entropy_loss/Reshape_1\"\r\n    input: \"Reshape_3\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n     do not match num inputs 3\r\n\r\nAny help will be appreciated. Thanks.\r\n", "comments": []}, {"number": 8514, "title": "caffe/tensorflow/mxnet: converter from python to c++", "body": "https://github.com/rmekdma/python_misc/blob/master/caffe/caffe_ftr.py\r\n\r\nI need to do some system integration in C++. Given the above caffe code,\r\n\r\nhow can I track the C++ code being used behind each caffe/tensorflow/mxnet function in python?\r\n\r\nCan I generate the C++ code so that I can compile and run?\r\n\r\nWhat is the python wrapper used by caffe/tensorflow/mxnet? ctype, boot:python or swig?\r\n\r\nSimilarly, it will be interesting to know whether I can do the same from python to c++ in tensorflow , mxnet or other python binding code, if it is easier to do it in tensorflow.\r\n\r\nhttp://stackoverflow.com/questions/42869355/caffe-tensorflow-mxnet-how-to-convert-python-to-c", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8513, "title": "ImportError: cannot import name 'moving_averages' ", "body": "I have a fresh install on AWS EC2 p2.xlarge.\r\n\r\nThe TensorFlow install method was pip:\r\n```\r\nconda list  | grep -E \"keras|teano|tensorflow\"\r\ntensorflow-gpu            1.0.1                     <pip>\r\n```\r\n\r\nHere is the error:\r\n```\r\n/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 from tensorflow.python.training import moving_averages\r\n      3 from tensorflow.python.ops import tensor_array_ops\r\n      4 from tensorflow.python.ops import control_flow_ops\r\n      5 from tensorflow.python.ops import functional_ops\r\n\r\nImportError: cannot import name 'moving_averages'\r\n```\r\nThank you for your support!", "comments": ["The issue can be closed.\r\n\r\nReinstalling with following helped (I passed this particular place).\r\n`pip3 install --upgrade --ignore-installed tensorflow-gpu`\r\n\r\ntensorflow-gpu            1.0.1                     <pip>", "@UkiDLucas  THANK YOU!!!\r\n\r\nI just installed some trash code from github and it broke my codebase entirely. Using this fixed everything along with sourcing my bashrc file. ", "Using TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"<pyshell#2>\", line 1, in <module>\r\n    import keras\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import activations\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\activations.py\", line 3, in <module>\r\n    from . import backend as K\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\backend\\__init__.py\", line 64, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2, in <module>\r\n    from tensorflow.python.training import moving_averages\r\nImportError: cannot import name 'moving_averages'\r\n\r\nSame issue while i had sucessfully installed tensorflow-cpu, keras and theano but it's throwing error while importing keras...\r\n\r\nIs there any solution for that.?"]}]