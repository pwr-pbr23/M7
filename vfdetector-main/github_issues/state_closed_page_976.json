[{"number": 24132, "title": "tf.estimator.Estimator.train fails with 3+ keras layer calls and MirroredStrategy distribution", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nlinux ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary (pip install tensorflow-gpu==1.8)\r\n- TensorFlow version (use command below):\r\n1.8.0\r\n- Python version:\r\n2.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n9.0.176\r\n- GPU model and memory:\r\nGeForce GTX 1080 Ti (11gb memory)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen applying a keras layer 3 or more times using tf.contrib.distribute.MirroredStrategy within the run config, training with a tensorflow estimator hits an exception when applying the layer at graph build time:\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py\", line 314, in __call__\r\n    output = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 671, in __call__\r\n    with scope_context_manager as scope:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1962, in __enter__\r\n    self._current_name_scope.__enter__()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 5974, in __enter__\r\n    % (self._name, self._default_name))\r\nValueError: At least one of name (None) and default_name (None) must be provided.\r\n```\r\n\r\nadditional observations:\r\n- runs fine if the layer is applied 2 times (using MirroredStrategy)\r\n- runs fine if the layer is applied 3 times (using OneDeviceStrategy)\r\n\r\n\r\n**Describe the expected behavior**\r\n`estimator.train` should succeed with multiple gpus and multiple applications of a keras layer.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass ConvLayer(tf.keras.layers.Layer):\r\n    def call(self, image):\r\n        shape = (3, 3, 3, 16)\r\n        stddev = 1\r\n        weights = tf.get_variable(\r\n            name='weights',\r\n            initializer=tf.truncated_normal(shape, stddev=stddev),\r\n        )\r\n        conv = tf.nn.conv2d(image, weights, [1, 1, 1, 1], 'SAME')\r\n        return conv\r\n\r\n\r\ndef get_train_op(loss):\r\n    global_step = tf.train.get_global_step()\r\n    optimizer = tf.contrib.optimizer_v2.AdamOptimizer(.001)  # use distribution-aware optimizer\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n    return train_op\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    image = features['image']\r\n\r\n    # if num_layers >= 3, graph construction will error if more than 1 gpu is used\r\n    num_layers = 3\r\n    layer = ConvLayer()\r\n    values = []\r\n    for _ in xrange(num_layers):\r\n        values.append(layer(image))\r\n    stacked_values = tf.concat(values, axis=0)\r\n\r\n    loss = tf.reduce_sum(stacked_values)\r\n    mode = tf.estimator.ModeKeys.TRAIN\r\n    train_op = get_train_op(loss)\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\r\ndef input_fn():\r\n\r\n    shape = (100, 100, 3)\r\n\r\n    def gen():\r\n        while True:\r\n            image = np.ones(shape, dtype=np.float32)\r\n            yield ({'image': image}, [])\r\n\r\n    ds = tf.data.Dataset.from_generator(\r\n        gen,\r\n        ({'image': tf.float32}, tf.float32),\r\n        output_shapes=({'image': tf.TensorShape(shape)}, tf.TensorShape(None)),\r\n    )\r\n    ds = ds.repeat().batch(4)\r\n    return ds\r\n\r\n\r\n# top-level call\r\ndef run():\r\n    gpus = ['/device:GPU:0', '/device:GPU:1']\r\n    # MirroredStrategy fails\r\n    distribution = tf.contrib.distribute.MirroredStrategy(gpus)\r\n    # OneDeviceStrategy works\r\n    # distribution = tf.contrib.distribute.OneDeviceStrategy(gpus[0])\r\n\r\n    config = tf.estimator.RunConfig(\r\n        train_distribute=distribution,\r\n        model_dir='/path/to/output'\r\n    )\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        config=config,\r\n    )\r\n    estimator.train(\r\n        input_fn=input_fn,\r\n        steps=10,\r\n    )\r\n```\r\n\r\ncalling `run()` after setting `num_layers` to 1 or 2 works, but fails if `num_layers >= 3`.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 465, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 831, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"scott_exp/multigpu.py\", line 31, in model_fn\r\n    values.append(layer(image))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py\", line 314, in __call__\r\n    output = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 671, in __call__\r\n    with scope_context_manager as scope:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1962, in __enter__\r\n    self._current_name_scope.__enter__()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 5974, in __enter__\r\n    % (self._name, self._default_name))\r\nValueError: At least one of name (None) and default_name (None) must be provided.\r\n```\r\n\r\nfull stacktrace: https://gist.github.com/dogbox/dcceda3d7b0c813c139258ea436e73f1\r\n", "comments": ["just noting that this issue isn't pressing - i've been able to work around this by explicitly setting variable scopes instead:\r\n\r\n```\r\nclass ConvLayer(object):\r\n    def __call__(self, image):\r\n        with tf.variable_scope('conv_layer', reuse=tf.AUTO_REUSE):\r\n            shape = (3, 3, 3, 16)\r\n            stddev = 1\r\n            weights = tf.get_variable(\r\n                name='weights',\r\n                initializer=tf.truncated_normal(shape, stddev=stddev),\r\n            )\r\n            conv = tf.nn.conv2d(image, weights, [1, 1, 1, 1], 'SAME')\r\n            return conv\r\n```\r\n\r\nit'd still be nice to use keras layers though, especially since this approach seems to be discouraged (https://github.com/tensorflow/tensorflow/issues/13895#issuecomment-364997554).", "@joshl This user request is about specific combination where 3+ layers of Keras model are imported into estimators and a distribution strategy is employed. Can you please review this condition?", "The usual way of using Keras layers is to have multiple layers (in your case, 3) one after another. What you did was using the same layer 3 times. \r\nI am not sure what caused your problem, but that is definitely not the common pattern of using Keras layers. ", "You are using a Keras Layer.  Please consider the documentation for writing your own layers, including the provided example: https://keras.io/layers/writing-your-own-keras-layers/.  In particular, the weights need to be define as part of `build()` rather than `call()`.  I'd try that.  Please re-open if that doesn't resolve your issue."]}, {"number": 24131, "title": "TFLite error: Op builtin_code out of range: 101.", "body": "Used TF nightly (tf-nightly-gpu-1.13.0.dev20181129) to convert a frozen model to .tflite. When I try to load this model in Android, TFLite throws an error: \"Op builtin_code out of range: 101, are you using old TFLite binary with newer model?\"\r\n\r\nI am loading TFLite nightly in my APK like so:\r\n\r\n```\r\ndependencies {\r\ncompile 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n}\r\n```\r\n\r\nWhat am I missing and what should I do to get this model loaded in Android?\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora r28\r\n- TensorFlow installed from (source or binary): binary(conda)\r\n- TensorFlow version (or github SHA if from source):tf-nightly-gpu-1.13.0.dev20181129\r\n", "comments": ["Hi bug@: the op builtin_code 101 points to the ABS operator as we can see in the TFLite schema [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs#L206).\r\nThe ABS operator was added to the schema only as of 14 days ago in this [commit](https://github.com/tensorflow/tensorflow/commit/22ff3ec66ef0a2453e353d8b1ed21a1866884381#diff-87671811af2daf75c4211e7582e258de).\r\n\r\nI would try recompiling the 0.0.0-nightly build in your demo again and see if it can pick up that commit. Let me know if that works!", "Closing this issue for now. Please reopen if you find that this fix didn't work."]}, {"number": 24130, "title": "TFTRT: Prevent segments with no inputs", "body": "If a segment is created with no inputs, the TRTEngineOp created by TFTRT currently crashes during runtime. This PR simply prevents the engine for that segment from building so that the native TF fallback will be executed instead of crashing.\r\n\r\nThis issue came up for me recently trying to convert ConvS2S which uses weight normalization. I've created an issue for the potential bug in constant folding: https://github.com/tensorflow/tensorflow/issues/24083.\r\n\r\nThis may become redundant when the node validation is working for all nodes - that would disqualify these nodes from even being considered for a segment because their inputs are weights not tensors. Nevertheless it is a good check to have.", "comments": ["Looks good to me.\r\n\r\n@azaks2 @smit-hinsu \r\nCould you please review.", "Looks good to me and will approve once https://github.com/tensorflow/tensorflow/pull/24183 is merged to update the CODEOWNERS file.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Trevor, the commit to use `empty` method has your personal email address as author and that email has not signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "> Trevor, the commit to use `empty` method has your personal email address as author and that email has not signed the CLA.\r\n\r\nOh I see, thanks for pointing that out."]}, {"number": 24129, "title": "Fix broken link in lite api docs", "body": "This fix fixes broken link in lite apis docs\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 24128, "title": "Error when loading frozen graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n> yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\n> CentOS Linux 7\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n\r\n> source\r\n\r\n- TensorFlow version (use command below):\r\n\r\n> 1.8.0\r\n\r\n- Python version:\r\n\r\n> 2.7.15\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n> 4.8.2\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI trined GooglNet model and get the error when importing a frozen graph\r\n\r\n`ValueError: Input 0 of node save/Assign_41 was passed float from auxiliary_classifier_1/classifier/biases/Adam_1:0 incompatible with expected float_ref.\r\n`\r\n\r\n**Code to reproduce the issue**\r\n\r\n`def freeze_graph():\r\n\r\n    # We retrieve our checkpoint fullpath\r\n    checkpoint = tf.train.get_checkpoint_state(SAVE_PATH)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n    print(input_checkpoint)\r\n\r\n    # We precise the file fullname of our freezed graph\r\n    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\r\n    output_graph = absolute_model_dir + \"/frozen_model.pb\"\r\n\r\n    # We clear devices to allow TensorFlow to control on which device it will load operations\r\n    clear_devices = True\r\n\r\n    # We start a session using a temporary fresh Graph\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        # We import the meta graph in the current default Graph\r\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n        # We restore the weights\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        gd = tf.get_default_graph().as_graph_def()\r\n\r\n        # fix nodes\r\n        for node in gd.node:\r\n            if node.op == 'RefSwitch':\r\n                node.op = 'Switch'\r\n                for index in range(len(node.input)):\r\n                    if 'moving_' in node.input[index]:\r\n                        node.input[index] = node.input[index] + '/read'\r\n            elif node.op == 'AssignSub':\r\n                node.op = 'Sub'\r\n                if 'use_locking' in node.attr:\r\n                    del node.attr['use_locking']\r\n            elif node.op == 'AssignAdd':\r\n                node.op = 'Add'\r\n                if 'use_locking' in node.attr:\r\n                    del node.attr['use_locking']\r\n            else:\r\n                print(node.op , node.name)\r\n            # elif node.op == '':\r\n\r\n        output_node_names = [n.name for n in gd.node]\r\n\r\n        # We use a built-in TF helper to export variables to constants\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, # The session is used to retrieve the weights\r\n            gd, # The graph_def is used to retrieve the nodes\r\n            output_node_names # The output node names are used to select the usefull nodes\r\n        )\r\n\r\n        # Finally we serialize and dump the output graph to the filesystem\r\n        with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        print(\"%d ops in the final graph.\" % len(output_graph_def.node))`\r\n\r\n\r\nNone of the answers I found on the net (eg https://github.com/tensorflow/tensorflow/issues/3628) do not answer this specific situation, where the problem is in the save op and Adam optimizer\r\n\r\n", "comments": ["Apologies for the delay in response. Is this still an issue? Can you try with the latest version of TF and test again? Thanks!", "I met this error when trying to convert the froze_eval_graph.pb into .tflite file using 'toco'\r\nI followed the steps in the instruction [here](https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/quantize), and here is my error trace:\r\n```\r\n$ toco --graph_def_file=frozen_eval_graph.pb --output_file=model.tflite --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shapes='1,28,28,1' --input_arrays=Lenet/conv1/Relu --output_arrays=Lenet/fc9/Relu\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node Lenet/conv1/weights_quant/AssignMinLast was passed float from Lenet/conv1/weights_quant/min:0 incompatible with expected float_ref.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/chutz/.local/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 91, in _convert_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 81, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 204, in from_frozen_graph\r\n    import_graph_def(graph_def, name=\"\")\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/chutz/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 422, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node Lenet/conv1/weights_quant/AssignMinLast was passed float from Lenet/conv1/weights_quant/min:0 incompatible with expected float_ref.\r\n```\r\n\r\n\r\nAnd my environments are:\r\nPython3.5\r\nTensorflow-gpu 1.9.0\r\ncuDNN 7.5.0\r\nCUDA 9.0\r\n\r\nAny help is appreciated! Thanks a lot! ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@chutongz TF 1.9 version is low to carry updates for toco file conversions. Please use tf-nightly build and test again. Thanks!"]}, {"number": 24127, "title": "'realdiv' bug in Python tensorflow1.11.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Mojave 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):1.11.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nThe description of 'realdiv' is a little confusing, that is :\r\nr\"\"\"Returns x / y element-wise for real types.\r\n\r\n  If `x` and `y` are reals, this will return the floating-point division.\r\n\r\n  *NOTE*: `Div` supports broadcasting. More about broadcasting\r\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\r\n\r\n  Args:\r\n    **x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.**\r\n    y: A `Tensor`. Must have the same type as `x`.\r\n    name: A name for the operation (optional).\r\n\r\n  Returns:\r\n    A `Tensor`. Has the same type as `x`.\r\n  \"\"\"\r\nI define the dtype of x for int32, the same to y.Then I use tensorflow.realdiv(x, y), it was an error!\r\n\r\n**Describe the expected behavior**\r\nI really hope you can answer my questions!Thank you very much!\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nx = tf.constant([1,2,1])\r\ny = tf.constant([1,1,3])\r\nsess = tf.Session()\r\nsess.run(tf.realdiv(x,y))\r\nsess.close()\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1291     try:\r\n-> 1292       return fn(*args)\r\n   1293     except errors.OpError as e:\r\n\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1274       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1275       self._extend_graph()\r\n   1276       return self._call_tf_sessionrun(\r\n\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1311     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1312       tf_session.ExtendSession(self._session)\r\n   1313 \r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'RealDiv' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n\r\n\t [[{{node RealDiv_2}} = RealDiv[T=DT_INT32](Const_2, Const_3)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-8-465bca8f95b0> in <module>\r\n----> 1 sess.run(tf.realdiv(x,y))\r\n\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    885     try:\r\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 887                          run_metadata_ptr)\r\n    888       if run_metadata:\r\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1109       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1110                              feed_dict_tensor, options, run_metadata)\r\n   1111     else:\r\n   1112       results = []\r\n\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1284     if handle is None:\r\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1286                            run_metadata)\r\n   1287     else:\r\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n~/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1306           self._config.experimental.client_handles_error_formatting):\r\n   1307         message = error_interpolation.interpolate(message, self._graph)\r\n-> 1308       raise type(e)(node_def, op, message)\r\n   1309 \r\n   1310   def _extend_graph(self):\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'RealDiv' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n\r\n\t [[{{node RealDiv_2}} = RealDiv[T=DT_INT32](Const_2, Const_3)]]\r\n\r\nCaused by op 'RealDiv_2', defined at:\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\r\n    self._run_once()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\r\n    handle._run()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/asyncio/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\r\n    ret = callback()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\r\n    self.run()\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\r\n    return runner(coro)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-465bca8f95b0>\", line 1, in <module>\r\n    sess.run(tf.realdiv(x,y))\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5989, in real_div\r\n    \"RealDiv\", x=x, y=y, name=name)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shawn/anaconda3/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'RealDiv' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n\r\n\t [[{{node RealDiv_2}} = RealDiv[T=DT_INT32](Const_2, Const_3)]]", "comments": ["@Ice-Panda I tried with the latest tf-nightly and could not reproduce the error. Could you try with the latest tf-nightly?", "> @Ice-Panda I tried with the latest tf-nightly and could not reproduce the error. Could you try with the latest tf-nightly?\r\n\r\nFirst of all, thank you for your reply. Actually, I would like to know why this problem occurred instead of using another version. As you can see that the end of the log, there is no DT_INT, I think that was the casue of the error, but I have no idea why there is no DT_INT~", "The error is pretty much descriptive: in 1.11 int type was not supported (which looks like it has been supported later).", "Since the issue has been resolved in latest version of tf, I will close the issue now."]}, {"number": 24126, "title": "Compute Capability 3.0 GTX 770 ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.8\r\n- Python version: python2.7\r\n- Installed using virtualenv? pip? conda?: pip (but I don't get that far)\r\n- Bazel version (if compiling from source): 0.13.1\r\n- GCC/Compiler version (if compiling from source): gcc 6.4.0\r\n- CUDA/cuDNN version: Cuda 9.0.176/cuDNN 7.4/NCLL 2.3\r\n- GPU model and memory: GTX 770 2GB 256-Bit GDDR5\r\n\r\n**Config file**\r\n```\r\nYou have bazel 0.13.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]:\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 2.2.12\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/local/cuda/nccl\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.0]\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-7]: /usr/bin/gcc-6\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n --config=mkl          # Build with MKL support.\r\n\r\n --config=monolithic   # Config for mostly static monolithic build.\r\n\r\nConfiguration finished\r\n```\r\n\r\n**Describe the problem**\r\n\r\nI have been struggling to build Tensorflow from source with bazel (I've tried releases 0.19.x - 0.13.x). I need to build from source in order to not have AVX instructions and also to used cuda compute capability 3.0. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nMost recently, I tried building for GPU support using the latest nightly Docker image. That doesn't support compute capability 3.0, though. I tried [this guide](https://stackoverflow.com/questions/39023581/tensorflow-cuda-compute-capability-3-0-the-minimum-required-cuda-capability-is/50592978) which seemed promising, but could not get bazel to build successfully. Note that I'm using different versions of cuDNN and NCLL from that guide. Should I try the exact versions the OP used? Am I doing something obviously wrong? \r\n\r\n**Any other info / logs**\r\n\r\nThere are none to show. The failure with Bazel is literally different everytime. I can provide the next failure if it will help\r\n\r\n**Some ideas I was going to try next**\r\n\r\n-  revert to the versions of cuDNN (7.1.4) and NCCL (2.2.12) that the person from that guide used\r\n- use the versions of gcc (4.8) and bazel (0.10.0) that are listed in the \"Tested build configurations\" table on the official documentation. Although, doesn't CUDA 9.0 need gcc 6.x? \r\n\r\nI've been working on this on/off for about a month now, would really appreciate a push in the right direction. \r\n\r\n", "comments": ["There are several places on github with community built wheels for TF you can download and just install.  The cuda 3 compute capability is a common issue.  You should be able to find a prebuilt if you google for it.", "Currently we only support CUDA Compute Capability 3.5 or higher. To use compute capability 3.0 you have to build it yourself from the sources. Feel free to reopen the issue if you are facing additional problems. Thanks!", "@ymodak I have been trying to build myself from the sources, I'm here for some guidance. Anywhere you can point me to? @bjascob thanks for the tips, I found some public wheels that I will try today.", "@ymodak Basically I just want to know how I would go about building myself from source. is it as simple as changing\r\n```\r\n_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.5,5.2'\r\n```\r\nto\r\n```\r\n_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.0,3.5,5.2'\r\n```\r\n", "I understand. Please take at similar issue #6001. ", "When you run `./configure` and it asks you what your compute capabilities are just enter 3.0.  You don't need the others unless you are building for multiple cards.  `./configure` will produce the file **.tf_configure.bazelrc**.  In case you're interested, here's what that file looks like on my machine where I specifically asked for CC 3.0 and \"haswell\" architecture.  Note this is python 3 and Cuda 10 with gcc 7 (default for UBT 18.04)\r\n```build --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild --define with_ignite_support=true\r\nbuild --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"10.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/local/cuda-10.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env NCCL_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nbuild --action_env NCCL_HDR_PATH=\"/usr/include\"\r\nbuild --action_env TF_NCCL_VERSION=\"2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\":/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=-march=haswell\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```\r\n\r\nThis setup, with bazel 0.18.0, works fine for me so you could try it verbatim if you want.\r\n\r\nNote that .configure also creates a **.bazelrc** file with the content `import /home/bjascob/Libraries/tensorflow-1.12.0/.tf_configure.bazelrc` which is needed to reference that above config file.  There's also a small oddity that if  **.bazelrc** is there, `./configure` wont run so if you rerun this, delete that file first.\r\n\r\nBe sure to run `bazel clean` before building then run...\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nsudo -H pip3 install /tmp/tensorflow_pkg/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n\r\n\r\n\r\n", "@bjascob that is very helpful, thank you so much. Will try ASAP and report back. ", "@ymodak @bjascob it doesn't even ask what compute capability anymore. ./configure just writes 3.5 and above. So what files would I need to edit to compile the latest tensorflow from source for compute capability 3.0? This has to be possible. ", "@meteorcloudy @gunan Can you please take a look? ", "@chsigg Does the sources still work with Compute capability 3.0?\r\n\r\n@mbaroody You can look into the configure script, and change it to use 3.0.\r\nBut as we move forward, our source becomes incompatible with older compute capabilities.\r\nWe have not been actively making sure 3.0 works, so it is highly likely that build can fail, or your binary may crash.", "There is reasonable chance that it still works, because SM 3.5 is relatively similar to SM 3.0.\r\n\r\n@mbaroody, you can change the .tf_configure.bazelrc file to switch to SM 3.0 support after you ran configure.", "@mbaroody Is this still an issue? Were you able to get it running?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24125, "title": "freeze a tensorflow graph which contains a LookupTable and use builder.save() for tf-serving  ", "body": "I have a tf model which contains a LookupTable, and I used\r\n `table_init_op = tf.tables_initializer(name=\"init_all_tables\")\r\n        config.sess.run(table_init_op)\r\n`\r\nto initialize it,  and freezed the graph by\r\n`output_graph_def = convert_variables_to_constants(config.sess,\r\n                                                          config.sess.graph_def,\r\n                                                          output_node_names=['init_all_tables', 'Inference/final_output/output', 'Inference/cur_state'])\r\n`\r\nfinally I save the model by builder.save(), it is ok until i run my tf server, however, when i do inference, I got an error`grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual)]]\"\r\n\tdebug_error_string = \"{\"created\":\"@1543839204.502783269\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1083,\"grpc_message\":\"NodeDef mentions attr 'Truncate' not in Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>; NodeDef: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\\n\\t [[Node: Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/Cast = Cast[DstT=DT_INT32, SrcT=DT_BOOL, Truncate=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Inference/affine_layers2/out_2/affine_transform/dense/Tensordot/GreaterEqual)]]\",\"grpc_status\":3}\"`\r\ndoes the table can freeze to the graph,what should i do to freeze it?\r\nthank you very much for your reply~", "comments": ["Apologies for the delay in response. Is this still an issue for you?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24124, "title": "bazel build issue (does not contain a toolchain for CPU 'k8')", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: `master` (6798656ce64201afe7d8d9ed5445cd5114bfd9b4)\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10\r\n\r\n\r\n**Describe the problem**\r\nWhen building branch `v1.12.0` or current `master` (commit 6798656ce64201afe7d8d9ed5445cd5114bfd9b4) I get the following build error:\r\n\r\n`ERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'k8', you may want to add an entry for 'local|compiler' into toolchains and toolchain_identifier 'local_linux' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).`\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- install bazel as described here: https://docs.bazel.build/versions/master/install-ubuntu.html#install-on-ubuntu\r\n- checkout the `v1.12.0` branch or 6798656ce64201afe7d8d9ed5445cd5114bfd9b4\r\n- bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n", "comments": ["Same issue just came up for me too. Works fine with an older version of Bazel.", "I am having the same issue. Is there a way to make it work with Bazel 0.20.0 ?\r\n\r\n@seank how did you downgrade your bazel version ?", "In my project it was a 1 char fix(dropped the ':').\r\n\"cortex-m3\": \":cc-compiler-cortex-m3\"    ->     \"cortex-m3\": \"cc-compiler-cortex-m3\"\r\n\r\ncc_toolchain_suite(\r\n    name = \"toolchain\",\r\n    # target_cpu | compiler\r\n    toolchains = {\r\n        \"armeabi-v7a|gcc\": \"cc-compiler-armeabi-v7a\",\r\n        \"k8|compiler\": \"cc-compiler-k8\",\r\n        \"cortex-m3\": \"cc-compiler-cortex-m3\",\r\n    },\r\n)", "@seank Which file is that? Grepping for `cc-compiler-k8` doesn't show me any files.", "This is the CROSSTOOL file for tensorflow https://github.com/tensorflow/tensorflow/blob/master/third_party/toolchains/gpus/crosstool/CROSSTOOL\r\n\r\nIt seems to have a valid entry for k8.", "What about the BUILD file?\n\n\ncc_toolchain_suite(\n     name = \"toolchain\",\n     toolchains = {\n         \"local|compiler\": \":cc-compiler-local\",\n         \"darwin|compiler\": \":cc-compiler-darwin\",\n         \"x64_windows|msvc-cl\": \":cc-compiler-windows\",\n     },\n)\n\n\n\nOn 2018-12-04 21:24, Vijayshinva Karnure wrote:\n> This is the CROSSTOOL file for tensorflow\n> https://github.com/tensorflow/tensorflow/blob/master/third_party/toolchains/gpus/crosstool/CROSSTOOL\n> \n> \n> It seems to have a valid entry for k8.\n> \n> --\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub [1], or mute the\n> thread [2].\n> \n> Links:\n> ------\n> [1] \n> https://github.com/tensorflow/tensorflow/issues/24124#issuecomment-444353726\n> [2]\n> https://github.com/notifications/unsubscribe-auth/AACjnVpBRlQ4Dh9zCZTgm0aa1qCRV5T6ks5u10qUgaJpZM4Y-eZE\n", "Downgrading worked. Thanks! Anyone has working solution for bazel 0.20.0?", "bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_packageWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: ae5b088a-4356-4490-ad5c-10b7603e3dc1\r\nDEBUG: /home/zcc/.cache/bazel/_bazel_zcc/ef73f1b213a381f188a64db1d3e2ae98/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'k8', you may want to add an entry for 'local|compiler' into toolchains and toolchain_identifier 'local_linux' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).\r\nINFO: Elapsed time: 0.059s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nI am having the same issue.Is there anyway to solve this problem?\r\n", "This is my work around; it's certainly not the right way to do it, but I needed to get it working and this did the trick.\r\n\r\n```diff\r\ndiff --git a/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh b/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\nindex 864278c647..4991018b5f 100755\r\n--- a/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n+++ b/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n@@ -15,6 +15,10 @@\r\n # ==============================================================================\r\n set -e\r\n\r\n+curl -L https://github.com/bazelbuild/bazel/releases/download/0.19.2/bazel-0.19.2-installer-linux-x86_64.sh > bazel-0.19.2-installer-linux-x86_64.sh\r\n+chmod +x bazel-0.19.2-installer-linux-x86_64.sh\r\n+./bazel-0.19.2-installer-linux-x86_64.sh --user\r\n+\r\n # By default this builds packages for the Pi Two and Three only, since the NEON support\r\n # this allows makes calculations many times faster. To support the Pi One or Zero, pass\r\n # PI_ONE as the first argument to the script, for example:\r\n@@ -102,7 +106,7 @@ fi\r\n export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n\r\n cd ${WORKSPACE_PATH}\r\n-bazel build -c opt ${PI_COPTS} \\\r\n+~/bin/bazel build -c opt ${PI_COPTS} \\\r\n   --config=monolithic \\\r\n   --copt=-funsafe-math-optimizations --copt=-ftree-vectorize \\\r\n   --copt=-fomit-frame-pointer --cpu=armeabi \\\r\n```\r\n", "I also encountered this problem, and reverting to bazel 0.19.2 worked for me.", "07f30871a02654ddecb017bf383a3790d60f9d96 breaks the build again. @mihaimaruseac Have any idea how to fix that?", "I'll have to test building with bazel 0.20 and see where it breaks. It will take a while", "I was able to compile with 0.20 safely, cannot reproduce on my end :(", "How do you install an older version of bazel?", "Run the following\r\n\r\n```\r\ncurl -L https://github.com/bazelbuild/bazel/releases/download/0.19.2/bazel-0.19.2-installer-linux-x86_64.sh > bazel-0.19.2-installer-linux-x86_64.sh\r\nchmod +x bazel-0.19.2-installer-linux-x86_64.sh\r\n./bazel-0.19.2-installer-linux-x86_64.sh --user\r\n```", "Using bazel 0.19.2 works for the latest master branch, while bazel 0.20.0 has k8 issue mentioned above.", "Same problem on ppc64le when trying for bazel test with cuda\r\n\r\nERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'ppc', you may want to add an entry for 'local|compiler' into toolchains and toolchain_identifier 'local_linux' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).", "maybe bazel should have a CI to check that it builds tensorflow before pushing out a release that breaks the biggest user of bazel.. ", "I have somewhat similar issue but downgrading bazel didn't help", "I think @annarev merged a change to fix this at head.\r\nWhat commit are you seeing this at?", "@gunan It certainly happens with https://github.com/tensorflow/tensorflow/releases/tag/v1.12.0. So the currently latest TF release can't be built on a fresh machine. (We got it to work with https://github.com/tensorflow/tensorflow/issues/24124#issuecomment-446701570).", "> @gunan It certainly happens with https://github.com/tensorflow/tensorflow/releases/tag/v1.12.0. So the currently latest TF release can't be built on a fresh machine. (We got it to work with [#24124 (comment)](https://github.com/tensorflow/tensorflow/issues/24124#issuecomment-446701570)).\r\n\r\nIt can be built per se. It is just the Bazel team is releasing frequently, and TF team is releasing frequently based on the lastest Bazel available at that time. If you are building an older release of TF, make sure you are using the corresponding Bazel version, which is probably not the latest one.", "@byronyi Yeah, of course (though 1.12.0 is the newest release, not an older one). I absolutely think the issue lies with Bazel here.\r\n\r\nThe main problem I see is that they regularly roll out non backwards compatible releases without providing packages for older versions. So I can't use a package manager to install that older Bazel version, but I have to curl and then manually install it from somewhere - which is at odds with all the documentation of both TF and Bazel. That's pretty annoying IMHO, because I had to dig through issue reports like this one to find a remedy.\r\n\r\nBut all of this is definitely not the fault of the TensorFlow team or library. And also, my problem is solved, so this is just me ranting aimlessly. Apologies.", "I am getting almost the same issue in MacBook Pro, macOS High Sierra 10.13.6, bazel version 0.21 and CUDA/cuDNN: 10/7.4:\r\n\r\n bazel build --config=cuda --config=opt --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false  --apple_platform_type macos --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/anaconda3/bin/tensorflow/tools/bazel.rc\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: 65975697-0885-4091-a0f4-446cbcab24c7\r\nDEBUG: /1106218c9bb5e3a2c38e96f5f11850c7/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n**ERROR: /1106218c9bb5e3a2c38e96f5f11850c7/external/local_config_cuda/crosstool/BUILD:5:1: in cc_toolchain_suite rule @local_config_cuda//crosstool:toolchain: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for cpu 'darwin'**\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cuda//crosstool:toolchain' failed; build aborted\r\nINFO: Elapsed time: 0,200s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n\r\n\r\nDoes somebody else experience this problem? Is there a solution for it?", "> @byronyi Yeah, of course (though 1.12.0 is the newest release, not an older one). I absolutely think the issue lies with Bazel here.\r\n> \r\n> The main problem I see is that they regularly roll out non backwards compatible releases without providing packages for older versions. So I can't use a package manager to install that older Bazel version, but I have to curl and then manually install it from somewhere - which is at odds with all the documentation of both TF and Bazel. That's pretty annoying IMHO, because I had to dig through issue reports like this one to find a remedy.\r\n> \r\n> But all of this is definitely not the fault of the TensorFlow team or library. And also, my problem is solved, so this is just me ranting aimlessly. Apologies.\r\n\r\nIt is alright. Bazel is still in its 0.x release line so do expect breaking changes. It would be better once it hits 1.0. TF itself is a complex project that is hard to build with any other build systems than the Google internal one (called Blade). I am outside Google, but I guess TF is more or less behind the open source process of Bazel. Bazel itself is a good tool, it is just adapting itself to the open source world.", "The answer from [#24124](https://github.com/tensorflow/tensorflow/issues/24124#issuecomment-446701570) wasn't enough for me, I had to uninstall bazel before.\r\n\r\nTo uninstall it i just removed everything from bazel:\r\n`rm -fr ~/.bazel ~/.bazelrc`\r\n`rm -rf ~/.cache/bazel `\r\n`sudo rm -rf /usr/local/bin/bazel /etc/bazelrc /usr/local/lib/bazel `\r\n\r\nThen reinstalled bazel as @mihaimaruseac said:\r\n`curl -L https://github.com/bazelbuild/bazel/releases/download/0.19.2/bazel-0.19.2-installer-linux-x86_64.sh > bazel-0.19.2-installer-linux-x86_64.sh`\r\n`chmod +x bazel-0.19.2-installer-linux-x86_64.sh`\r\n`./bazel-0.19.2-installer-linux-x86_64.sh --user`\r\n", "Probably just the cache needed to be removed, based on my understanding of  Bazel/Blaze.", "I am closing this since the build script now won't proceed without a compatible bazel version.\r\nThe recommended way is to install the Debian package from https://github.com/bazelbuild/bazel/releases with the corresponding fixed bazel version (`bazel_<version>-linux-x86_64.deb`) instead of using the repository which always updates to the latest version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24124\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24124\">No</a>\n"]}, {"number": 24123, "title": "Error while running sample cpp program in tensorflow", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):r1.11\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19\r\n- GCC/Compiler version (if compiling from source):msvc 2017\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nProblem Statement:\r\nI have build tensorflow r1.11 library. During the build it was giving following error:\r\nhttps://github.com/tensorflow/tensorflow/issues/23402\r\nAs a solution to the problem i used the one of the suggested solution on the forum:\r\nbazel build --define=grpc_no_ares=true -c opt //tensorflow:libtensorflow_cc.so\r\nThe bazel is built successfully and when i try to run a sample program i get the following error:\r\n:1: error: dependent '..\\..\\..\\..\\Downloads\\tensorflow-r1.11\\bazel-tensorflow-r1.11\\external\\eigen_archive\\unsupported\\Eigen\\src\\SpecialFunctions\\arch\\CUDA\\CudaSpecialFunctions.h' does not exist.\r\nEariler i have build other version of library but this was not the issue.\r\n", "comments": ["@HackersSpirit,\r\n\r\nWe see you were using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24123\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24123\">No</a>\n"]}, {"number": 24122, "title": "Build failed on Windows with AVX2, MKL and CUDA support.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 (1809)\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19.2\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7\r\n- GPU model and memory: GTX 1060MQ 6GB\r\n\r\n\r\n**Describe the problem**\r\nI want to build TensorFlow with AVX2, MKL and CUDA support, but I failed (It built successfully without the AVX2 and MKL support). Here is my Bazel build commands:\r\n```\r\nbazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nIt seems that the \"--copt=-mavx2\" argument cannot take effect:\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/batch_matmul_op_complex.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n```\r\nand there are another two error messages:\r\n```\r\nERROR: D:/library/tensorflow/tensorflow/core/kernels/BUILD:6385:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_relu_op' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n```\r\n```\r\nINFO: From Executing genrule //tensorflow/core:version_info_gen:\r\nfatal: Invalid path '/c/users/young/_bazel_young/a3m3eycs/execroot/org_tensorflow/C:': No such file or directory\r\n```\r\nWhat should I do to build the TensorFlow correctly? Thank you very much!\r\n\r\n\r\n**Any other info / logs**\r\n**The \"configure\" command**\r\n```\r\nD:\\Library\\tensorflow>python ./configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nd:\\library\\tensorflow/tools/bazel.rc\r\nnul\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.19.2 installed.\r\nPlease specify the location of python. [Default is C:\\Program Files\\Python36\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Program Files\\Python36\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Program Files\\Python36\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: n\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n```\r\n\r\n\r\n**The \"build\" output (too long and simplified)**\r\n```\r\nD:\\Library\\tensorflow>bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nd:\\library\\tensorflow/.bazelrc\r\nd:\\library\\tensorflow/tools/bazel.rc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nWARNING: C:/users/young/_bazel_young/a3m3eycs/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/young/_bazel_young/a3m3eycs/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/young/_bazel_young/a3m3eycs/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/young/_bazel_young/a3m3eycs/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: C:/users/young/_bazel_young/a3m3eycs/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in C:/users/young/_bazel_young/a3m3eycs/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: D:/library/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (308 packages loaded, 13377 targets configured).\r\nINFO: Found 1 target...\r\nINFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling external/grpc/src/core/tsi/alts/zero_copy_frame_protector/alts_iovec_record_protocol.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Linking external/grpc/libgrpc++_base.a:\r\nserver_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nrpc_method.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\ncreate_channel_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Compiling external/grpc/src/core/tsi/alts/frame_protector/alts_frame_protector.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling external/grpc/src/core/tsi/alts/frame_protector/alts_record_protocol_crypter_common.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling external/grpc/src/core/tsi/alts/zero_copy_frame_protector/alts_grpc_integrity_only_record_protocol.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/map_field.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Linking external/protobuf_archive/libprotobuf_lite.a:\r\narenastring.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/generated_message_table_driven.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/source_context.pb.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Linking external/protobuf_archive/libprotobuf.a:\r\nerror_listener.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Compiling external/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling tensorflow/python/framework/fast_tensor_util.cpp:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nc:\\users\\young\\_bazel_young\\a3m3eycs\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\genfiles\\external\\local_config_python\\numpy_include\\numpy\\npy_1_7_deprecated_api.h(12) : Warning Msg: Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\r\nINFO: From Compiling tensorflow/core/protobuf/queue_runner.pb.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Linking tensorflow/python/framework/fast_tensor_util.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/framework/libfast_tensor_util.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/python/framework/libfast_tensor_util.so.exp\r\nINFO: From Compiling tensorflow/core/framework/remote_fused_graph_execute_info.pb.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling tensorflow/core/example/example.pb_text.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Linking tensorflow/core/liblib_internal_impl.a:\r\nandroid_armv7a_cpu_utils_helper.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nrandom_distributions.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Compiling tensorflow/core/framework/summary.pb_text.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_logical_not.cu.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nhost_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_tan.cu.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nhost_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\r\nINFO: From Compiling tensorflow/contrib/tpu/ops/heartbeat_ops.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/contrib/tpu/ops/tpu_configuration_ops.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/contrib/coder/kernels/pmf_to_cdf_op.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/contrib/tpu/ops/heartbeat_ops.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_log1p.cu.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nhost_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\r\nINFO: From Compiling tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nhost_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\r\nINFO: From Compiling tensorflow/contrib/tpu/ops/outfeed_ops.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling tensorflow/core/kernels/qr_op_double.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/grappler/costs/measuring_cost_estimator.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_bitwise_or.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_conj.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_sigmoid.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_zeta.cu.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nhost_defines.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\r\nINFO: From Compiling tensorflow/python/grappler/cost_analyzer.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\n\r\n...\r\n\r\nINFO: From Compiling tensorflow/core/kernels/mfcc_dct.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nINFO: From Compiling tensorflow/core/kernels/batch_matmul_op_complex.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\nERROR: D:/library/tensorflow/tensorflow/core/kernels/BUILD:6385:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_relu_op' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/young/_bazel_young/a3m3eycs/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=D:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\YOUNG\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\YOUNG\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/mkl_windows /Ibazel-out/x64_windows-opt/genfiles/external/mkl_windows /Ibazel-out/x64_windows-opt/bin/external/mkl_windows /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/genfiles/third_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/mkl_windows/include /Ibazel-out/x64_windows-opt/genfiles/external/mkl_windows/include /Ibazel-out/x64_windows-opt/bin/external/mkl_windows/include /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/genfiles/external/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 -mavx2 -nvcc_options=disable-warnings -DGOOGLE_CUDA=1 -DINTEL_MKL=1 -DEIGEN_USE_VML -DENABLE_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/mkl_relu_op/mkl_relu_op.o /c tensorflow/core/kernels/mkl_relu_op.cc\r\ncl : Command line warning D9002 : ignoring unknown option '-mavx2'\r\ntensorflow/core/kernels/mkl_relu_op.cc(1027): error C2398: Element '1': conversion from 'const std::size_t' to 'int' requires a narrowing conversion\r\ntensorflow/core/kernels/mkl_relu_op.cc(895): note: while compiling class template member function 'void tensorflow::MklReluGradOpBase<Device,T,mkldnn::eltwise_tanh>::Compute(tensorflow::OpKernelContext *)'\r\n        with\r\n        [\r\n            Device=tensorflow::CPUDevice,\r\n            T=float\r\n        ]\r\ntensorflow/core/kernels/mkl_relu_op.cc(1217): note: see reference to class template instantiation 'tensorflow::MklReluGradOpBase<Device,T,mkldnn::eltwise_tanh>' being compiled\r\n        with\r\n        [\r\n            Device=tensorflow::CPUDevice,\r\n            T=float\r\n        ]\r\ntensorflow/core/kernels/mkl_relu_op.cc(1297): note: see reference to class template instantiation 'tensorflow::MklTanhGradOp<tensorflow::CPUDevice,float>' being compiled\r\n.\\tensorflow/core/util/tensor_format.h(451): note: see reference to class template instantiation 'absl::Span<const tensorflow::int64>' being compiled\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 705.262s, Critical Path: 158.20s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 2169 processes: 2169 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Two things:\r\n\r\nFirst: Are you sure that you have the good tensorflow branch (Git)?\r\n\r\nSecond: I highly suggest using CUDA 9.0 instead of CUDA 10.0, because it is not yet officially supported by tensorflow. Try using the older version to see if it works.", "@FredVaugeois Thanks for your reply! \r\n\r\n**1.** I got the TensorFlow source following the [Offical Guide](https://tensorflow.google.cn/install/source_windows) using the following command:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.12\r\n```\r\nThen, to fix the build errors, I made two change following some previous TensorFlow issues:\r\n**(1)** Add \"import D:/Library/tensorflow/tools/bazel.rc\" in the first line of file \"/tensorflow/.bazelrc\" (for solving the Warning from bazel that \"bazel.rc\" is a older file type).\r\n**(2)** Following this [Guide](https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2) to download \"[eigen_half.patch](https://github.com/amsokol/tensorflow-windows-build-tutorial/blob/master/eigen_half.patch)\" to the \"third_party\" folder, and add `patch_file = clean_dep(\u201c//third_party:eigen_half.patch\u201d)` to eigen_archive section of \"tensorflow/workspace.bzl\" file (for fixing the \"Eigen\" build error).\r\nWith these two change, the version without AVX2 and MKL built correctly.\r\n\r\n**2.** It seems that the \"-mavx2\" error results from my incorrect parameter transfer mode, it may reappear using CUDA 9.0 and can be fixed? I am a new learner, or the build error may caused by a serious mistake, and I need change CUDA 10.0 to CUDA 9.0?", "@jayoungo Is this still an issue? Yes you should set cuda 9.0.", "@ymodak I cannot fix the build error, and I had changed the CUDA version to 9.0 and installed the official pip package. It works well now. Thank you!"]}, {"number": 24121, "title": "deeplab v3+ quantization tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (train / export), macOS 10.14.1 (toco) \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): V9.0.176\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version: 0.15.2-homebrew\r\n- GPU model and memory: Nvidia Titan Xp 12GB x 4\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n - I want to make realtime segmentation model using quantization form tensorflow lite on android device. The first step, I success to make quantized tflite. Unfortunately the result is always background. \r\n - We make 2 class dataset background and sky. Here is visualization of tensor, that before argmax. [Graph Link](https://photos.app.goo.gl/ZjcCxstLHJK3dUmx8). Guessing, 8bit quantization make small difference, that makes it miss bias in last stage.\r\n - pb and float tflite(--drop_fake_quant=true) models are works. \r\n\r\n**Describe the expected behavior**\r\n\r\n - Quantized tflite models will works.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was missing information. I use mobilenet v2 as backbone. \r\nThanks.", "@hithere1985 Were you able to extract any segmentation mask image out of it? Could be better if you could share your code, model file, and what kind of changes you incorporated in the existing deeplab model during TFLITE conversion.", "Can you share your frozen graphdef and the command you used to convert so that I can reproduce the issue? Thanks!", "@hithere1985 Gentle ping. Please provide repro instructions.", "I've encounter similar issue with Deeplab.\r\nI print out the value of average pooling, the quantized version's value seems wrong.\r\n\r\nMy decoder structure:\r\n![image](https://user-images.githubusercontent.com/12553389/51158506-c1c14600-18bf-11e9-8bc1-58f31b166244.png)\r\n", "Closing due to inactivity, please create a new issue with details on the issue and how to repro if you are still having an issue.", "@hithere1985, How do you get float tflite model by setting\"--drop_fake_quant=true\". I still see lots of FakeQuant models even I setting this option to toco command. Could you provide more detail on how do you convert it back to floating model from fixed point model?"]}, {"number": 24120, "title": "TFLite Speech recognization app is getting crashed with setUseNNAPI(true)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):[NO]\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):\r\n- Python version: 3.0\r\n- Bazel version (if compiling from source):[bazel release 0.16.1]\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nDescription: \r\nIm using the source code from below repo:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app\r\nCompiled using the readme in above link\r\nThe app works fine without any changes.\r\nHowever when i enable setUseNNAPI(true) for speechActivity the crashes with below logs:\r\n\r\n======================================================================\r\n### tflite  : Custom operations are not supported when using NNAPI.\r\n12-03 14:10:16.545  7947  7969 E tflite  : Returning error since TFLite returned failure nnapi_delegate.cc:745.\r\n12-03 14:10:16.545  7947  7969 E tflite  : Failed to build graph for NNAPI\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: FATAL EXCEPTION: Thread-3\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 7947\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: \r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:140)\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat org.tensorflow.demo.SpeechActivity.recognize(SpeechActivity.java:339)\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat org.tensorflow.demo.SpeechActivity.access$100(SpeechActivity.java:67)\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat org.tensorflow.demo.SpeechActivity$3.run(SpeechActivity.java:290)\r\n12-03 14:10:16.545  7947  7969 E AndroidRuntime: \tat java.lang.Thread.run(Thread.java:764)\r\n==============================================================================\r\n**Other info / logs**\r\nNote:\r\nImage clasification works fine with same changes setUseNNAPI(true)\r\n\r\n", "comments": ["@Umapraveen as the message you posted said, \r\n```\r\nCustom operations are not supported when using NNAPI.\r\n```\r\n\r\nThe model you used had custom OPs so the TFLite interpreter refused to delegate the model to NNAPI.\r\n\r\nTo use NNAPI, try either\r\n1. come up your own model with ops supported by TFLite and NNAPI, or\r\n2. split your model and run custom ops on TFLite only", "Closing this issue since @freedomtan 's explaination is correct. Feel free to reopen if have any follow up questions. Thanks!", "I had similar crash problem with tensor flow tflite android example code when setUseNNAPI to true:\r\n2019-01-05 13:24:13.982 9314-9331/org.tensorflow.lite.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.demo, PID: 9314\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:194)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:247)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\r\nThe phone used is pixel 2 with android 9. Once I setUseNNAPI to false, the tflite example apk runs normally. By the way, the model is NOT customized and is whatever coming with tensor flow's ssdmobilenet object detection.\r\n\r\ndavid \r\n"]}, {"number": 24119, "title": "How to integrate KMeansClustering with other module", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n\r\n**Problem**\r\nWhen I want to integrate KMeansClustering into my model, I want to use minibatch kmeans to cluster the output of 2 fully connected layers. However, the error is one tensor of kmeans must be from the same graph as the input tensor. Is there any way to solve it? Thank you!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24118, "title": "Build failed when using AVX2 and MKL support.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 (1809)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19.2\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7\r\n- GPU model and memory: GTX 1060MQ 6GB\r\n\r\n\r\n**Describe the problem**\r\nI want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:\r\n`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["So sorry for my wrong operation."]}, {"number": 24117, "title": "Build failed when using AVX2 and MKL support.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 (1809)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19.2\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7\r\n- GPU model and memory: GTX 1060MQ 6GB\r\n\r\n\r\n**Describe the problem**\r\nI want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:\r\n`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["So sorry for my wrong operation."]}, {"number": 24116, "title": "Build failed when using AVX2 and MKL support.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 (1809)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19.2\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7\r\n- GPU model and memory: GTX 1060MQ 6GB\r\n\r\n\r\n**Describe the problem**\r\nI want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:\r\n`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["So sorry for my wrong operation."]}, {"number": 24115, "title": "Build failed when using AVX2 and MKL support.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 (1809)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.19.2\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7\r\n- GPU model and memory: GTX 1060MQ 6GB\r\n\r\n\r\n**Describe the problem**\r\nI want to build TensorFlow with AVX2, MKL and CUDA support, Here is my Bazel build instruction:\r\n`bazel build --compilation_mode=opt --config=opt --copt=-mavx2 --copt=-nvcc_options=disable-warnings --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["So sorry for my wrong operation."]}, {"number": 24114, "title": "Fix version number in Bazel downgrade warning", "body": "I have upgraded to Bazel 0.20:\r\n\r\n```bash\r\n$ bazel version\r\nINFO: Invocation ID: 00114105-2567-451d-b22d-bb21e2d14b11\r\nBuild label: 0.20.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Nov 30 14:39:01 2018 (1543588741)\r\nBuild timestamp: 1543588741\r\nBuild timestamp as int: 1543588741\r\n```\r\n\r\nBazel version check is introduced in e7a123f4b361b8104b783d8b1407b45dc087491c. \r\nBefore this patch:\r\n\r\n```bash\r\n$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: b6a7b9a2-db26-4777-a4bb-f5a668e52e82\r\nYou have bazel 0.20.0 installed.\r\nPlease downgrade your bazel installation to version 0.15.0 or lower to build TensorFlow!\r\nConfiguration finished\r\n```\r\n\r\nAfter this patch:\r\n\r\n```bash\r\n$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 34b75ed8-1359-4fa7-ba71-b67fd887ee3a\r\nYou have bazel 0.20.0 installed.\r\nPlease downgrade your bazel installation to version 0.19.2 or lower to build TensorFlow!\r\nConfiguration finished\r\n```", "comments": ["@harshini-gadige Mind to pull?", "> @harshini-gadige Mind to pull?\r\n\r\nSure. Will keep you posted.", "Seems fixed by 3360c72f32894bc12f5592398d8becbc726ab2d1. Closing for now."]}, {"number": 24113, "title": "[Proto/decode]word order fixed", "body": "", "comments": ["@jsimsa Thanks for correcting. Agree with your idea and i think the second one is better ^.^."]}, {"number": 24112, "title": "[INTEL MKL] Temporarily disabling transpose optimization.", "body": "Transpose optimization is causing several regressions.", "comments": ["@penpornk Thanks."]}, {"number": 24111, "title": "Bug when passing multiple output of Lambda Layer to the Model API", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n1.11.0\r\n- Python version:\r\n3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n5.2.0\r\n- CUDA/cuDNN version:\r\n9/7\r\n- GPU model and memory:\r\nGTX 1080, 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nAttempting to receive multiple outputs from a lambda layer.  When passing them through the Model API, causes them to have an unexpected behavior. if all the outputs of that lambda layer are stored into a single variable, passing that variable as output for model fails with \r\n`AssertionError: Could not compute output Tensor...`\r\n\r\nUsing index[0] gives me behavior I would have expected when receiving with no index specified. Using index 1 or 2 gives same error as not giving an index. TLDR: index 0 gives behavior you would expect from not having an index. Index 1,2 give error same as no index \r\n**Describe the expected behavior**\r\nspecifying no index means that I am accessing a list of tensors, where as specifying an index gives me the tensor at that index.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow.python.keras.layers import Input, Lambda\r\nfrom tensorflow.python.keras.models import Model\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nins = Input([1])\r\n\r\ndef test(x):\r\n    return tf.constant(2,name='one'),tf.constant([2]),tf.constant([[2],[4]])\r\n\r\nous = Lambda(test)(ins)\r\nmodel = Model(ins,ous[0]) # changing this 0 to 1 or 2 fails, removing index also fails\r\n# model = Model(ins,ous) # FAILS\r\n# model = Model(ins,ous[1]) # FAILS\r\n\r\nnew = model(np.asarray([1]))\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(new))  #(2, array([2], dtype=int32), array([[2], [4]], dtype=int32))\r\n```\r\n**Other info / logs**", "comments": ["@Anton-Velikodnyy,\r\nSorry for the delayed response. Your code works fine even by replacing 0 with 1 or 2, successfully, in the Tensorflow Version 2.4. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/a7a529e24bc41565a6257fd378d76a7e/gh_24111.ipynb) of the working code. Thanks! ", "While i don't have a chance to test it currently. I'll close for now unless someone finds a bug with it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24111\">No</a>\n"]}, {"number": 24110, "title": "Tensorflow-gpu wont install Windows 10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 10  Home Ver 1803\r\n- TensorFlow installed from pip\r\n- TensorFlow version: 1.12.0 (tensorflow-gpu)\r\n- Python version:3.6.3\r\n- Installed using virtualenv? pip? conda?:pip\r\n- CUDA/cuDNN version: 10.0.132\r\n- GPU model and memory: GTX 1080\r\n\r\n\r\n\r\n**Describe the problem**\r\nTensor flow-gpu is not properly installed.   \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThis is the only code\r\n**import tensorflow**  \r\nThats it!!\r\n\r\nIf I install tensorflow-gpu and install tensor floe for the cpu it all works.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nThe entire traceback is listed below.\r\nInstalling the tensor flow \r\nThat is as basic as you get  it states \r\n**Any other info / logs**\r\nPython 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"copyright\", \"credits\" or \"license()\" for more information.\r\n>>> \r\n RESTART: C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\tensorgputest.py \r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\tensorgputest.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BillS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n", "comments": ["TensorFlow is tested against CUDA 9.0, to use CUDA 10.0 you have to build it yourself from the sources. The easier alternative would be to switch to CUDA 9.0 and build again. Thanks!", "OK,\r\n\r\nAny idea when 10 will be supported.\r\nWill there be any issues with reverting to an older version om my system.\r\nShould I unistall Tensorflow-gpu before switching to ver - 9", "@Bstrum36 cuda 10 support is on our roadmap and it will be officially supported starting from TensorFlow 2.0. Thanks!"]}, {"number": 24109, "title": "Mixture of Multivariate distributions: Could not infer number of classes from cat ", "body": "Hi,\r\nI am facing this issue:\r\nds=tf.contrib.distributions\r\n#input is computed by passing some place_holders for tensors through neural networks\r\nsm_scores=tf.nn.softmax(input)#input=batch_sizexseq\r\ncat=ds.Categorical(probs=sm_scores)\r\nmeans=nmean x batchx dimens\r\nsigs=nsigs x batchx dimens # mean & sigma are from some place_holders\r\ncomps = [ds.MultivariateNormalDiag(loc=means[0], scale_diag=sigs[0]),\r\nds.MultivariateNormalDiag(loc=means[1], scale_diag=sigs[1]),\r\nds.MultivariateNormalDiag(loc=means[2], scale_diag=sigs[2])]\r\nmix=tf.Mixture(cat=cat, components=comps)\r\n\r\nI get the following error:\r\npython3.5/site-packages/tensorflow/contrib/distributions/python/ops/mixture.py\", line 147, in init\r\n\"Could not infer number of classes from cat and unable \"\r\nValueError: Could not infer number of classes from cat and unable to compare this value to the number of components passed in.\r\n\r\nHowever, when I get the output sm_scores after running tf.session, perform the samething outside class, it works fine:\r\n\r\ncat=ds.Categorical(probs=sm_scores)\r\nmeans=nmean x batchx dimens\r\nsigs=nsigs x batchx dimens # mean & sigma are from some place_holders\r\ncomps = [ds.MultivariateNormalDiag(loc=means[0], scale_diag=sigs[0]),\r\nds.MultivariateNormalDiag(loc=means[1], scale_diag=sigs[1]),\r\nds.MultivariateNormalDiag(loc=means[2], scale_diag=sigs[2])]\r\nmix=tf.Mixture(cat=cat, components=comps)\r\n\r\nDo you have any idea, why is this happening?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as a short repro example to reproduce your case. Thanks!\r\n"]}, {"number": 24108, "title": "Update tables_initializer to link to guide", "body": "Links the `tables_initializer` docstring to the Low Level Intro guide.\r\n\r\nResolves  #20629", "comments": ["Oh, that's the problem... this PR is against the r1.11 branch.\r\n\r\nWe don't fix things like this in old releases.\r\n\r\nCan you resubmit this PR against master?", "@MarkDaoust no problem! Sorry for the mistake, I have closed this PR and re-submitting it against master (#24238)"]}, {"number": 24107, "title": "What's the problem? (bazel build)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.12\r\n- Python version:3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0/7.4.1\r\n- GPU model and memory: geforce 750\r\n\r\nINFO: From Compiling external/grpc/third_party/address_sorting/address_sorting_windows.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c99'\r\n\r\nERROR: C:/users/amsokol/development/tensorflow-build/tensorflow/tensorflow/contrib/lite/schema/BUILD:58:1: Generating flatbuffer files for schema_fbs_srcs: //tensorflow/contrib/lite/schema:schema_fbs_srcs failed (Illegal instruction): bash.exe failed: error executing command\r\n \r\n cd C:/users/minsu/_bazel_minsu/lx6zoh4k/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=D:/Program Files\r\n    SET CUDNN_INSTALL_PATH=D:/Program Files\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Users\\amsokol\\tensorflow-v1.12\\Scripts;D:\\Program Files\\bin;D:\\Program Files\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\ffmpeg-4.1-win64-static\\bin;C:\\Users\\minsu\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\minsu\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\Users\\minsu\\AppData\\Local\\cuda\\bin;C:\\bazel;D:\\tensorflow\\tensorflow\\tools\\pip_package;C:\\msys64\\usr\\bin;D:\\tensorflow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC;C:\\Users\\minsu\\Miniconda3;C:\\Users\\minsu\\Miniconda3\\Library\\mingw-w64\\bin;C:\\Users\\minsu\\Miniconda3\\Library\\usr\\bin;C:\\Users\\minsu\\Miniconda3\\Library\\bin;C:\\Users\\minsu\\Miniconda3\\Scripts;C:\\Users\\minsu\\AppData\\Local\\Microsoft\\WindowsApps\r\n    SET PYTHON_BIN_PATH=C:/Users/amsokol/tensorflow-v1.12/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/amsokol/tensorflow-v1.12/Lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for f in tensorflow/contrib/lite/schema/schema.fbs; do bazel-out/x64_windows-opt/bin/external/flatbuffers/flatc --no-union-value-namespacing --gen-object-api  -c -o bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/lite/schema $f; done: bash.exe failed: error executing command\r\n  cd C:/users/minsu/_bazel_minsu/lx6zoh4k/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=D:/Program Files\r\n    SET CUDNN_INSTALL_PATH=D:/Program Files\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Users\\amsokol\\tensorflow-v1.12\\Scripts;D:\\Program Files\\bin;D:\\Program Files\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\ffmpeg-4.1-win64-static\\bin;C:\\Users\\minsu\\AppData\\Local\\Programs\\Python\\Python36;C:\\Users\\minsu\\AppData\\Local\\Programs\\Python\\Python36\\Scripts;C:\\Users\\minsu\\AppData\\Local\\cuda\\bin;C:\\bazel;D:\\tensorflow\\tensorflow\\tools\\pip_package;C:\\msys64\\usr\\bin;D:\\tensorflow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC;C:\\Users\\minsu\\Miniconda3;C:\\Users\\minsu\\Miniconda3\\Library\\mingw-w64\\bin;C:\\Users\\minsu\\Miniconda3\\Library\\usr\\bin;C:\\Users\\minsu\\Miniconda3\\Library\\bin;C:\\Users\\minsu\\Miniconda3\\Scripts;C:\\Users\\minsu\\AppData\\Local\\Microsoft\\WindowsApps\r\n    SET PYTHON_BIN_PATH=C:/Users/amsokol/tensorflow-v1.12/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/amsokol/tensorflow-v1.12/Lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; for f in tensorflow/contrib/lite/schema/schema.fbs; do bazel-out/x64_windows-opt/bin/external/flatbuffers/flatc --no-union-value-namespacing --gen-object-api  -c -o bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/lite/schema $f; done\r\n/usr/bin/bash: line 1:  7444 Illegal instruction     bazel-out/x64_windows-opt/bin/external/flatbuffers/flatc --no-union-value-namespacing --gen-object-api -c -o bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/lite/schema $f\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 22.017s, Critical Path: 2.25s\r\nINFO: 47 processes: 47 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["I see that you are using CUDA 10.0. Latest version of TF is tested against CUDA 9.0. If you want to use CUDA 10.0, you have to build it yourself from the sources. The simpler alternative will be to switch to CUDA 9.0 and build again. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24106, "title": "Duplicate libiomp5.dylib & libmklml.dylib when install TF 1.12 with python 3.6.x using Anaconda3 in Mac OS 10.13", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution : MacOS 10.13 High Sierra\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.x\r\n- Installed using : Anaconda 3, version 5.3.1\r\n\r\n**Describe the problem**\r\nActually, my program meet a issue as [#23999 ](https://github.com/tensorflow/tensorflow/issues/23999)\r\n\r\nI try to deep into the reason which cause the issue, \r\nand found that when I create a env (name 'py36' in my case) in Anaconda3 with Python 3.6 and install TensorFlow 1.12 ,\r\nthere are duplicate files `libiomp5.dylib` & `libmklml.dylib` under path\r\n`~/anaconda3/envs/py36/lib/python3.6/site-packages/_solib_darwin/_U@mkl_Udarwin_S_S_Cmkl_Ulibs_Udarwin___Uexternal_Smkl_Udarwin_Slib/`\r\n\r\nwhich are same files under path \r\n`~/anaconda3/envs/py36/lib`\r\n\r\nNo matter I delete which one, the program cannot run well.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nStep 1. Install Anaconda 3 (Version 5.3.1)\r\nStep2. Create env using Python 3.6\r\nStep3. Install TensorFlow 1.12 and relative package.\r\n\r\nBTW, If I create env (name 'py35' ) using Python 3.5, this issue is gone.\r\nThere are no folder `_solib_darwin` be created under `~/anaconda3/envs/py35/lib/python3.5/site-packages/` and no duplicate files `libiomp5.dylib` & `libmklml.dylib`\r\n\r\n**Any other info / logs**\r\n\r\n\r\n", "comments": ["My program as follow:\r\nThis program will meet issue [#23999](https://github.com/tensorflow/tensorflow/issues/23999)\r\nwhen install TF 1.12 with python 3.6.x using Anaconda3 in Mac OS 10.13\r\n\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Workaround for libiomp5.dylib already initialized.\r\n# import os\r\n# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\r\n\r\n\r\nx_data = np.random.randn(2000, 3)\r\nw_real = [0.3, 0.5, 0.1]\r\nb_real = -0.2\r\n\r\nnoise = np.random.randn(1, 2000) * 0.1\r\n\r\ny_data = np.matmul(w_real, x_data.T) + b_real + noise\r\n\r\nNUM_STEPS = 10\r\n\r\ng = tf.Graph()\r\nwb_ = []\r\n\r\nwith g.as_default():\r\n    x = tf.placeholder(tf.float32, shape=[None, 3])\r\n    y_true = tf.placeholder(tf.float32, shape=None)\r\n\r\n    with tf.name_scope('inference') as scope:\r\n        w = tf.Variable([[0, 0, 0]], dtype=tf.float32, name='weights')\r\n        b = tf.Variable(0, dtype=tf.float32, name='bias')\r\n        y_predict = tf.matmul(w, tf.transpose(x)) + b\r\n\r\n    with tf.name_scope('loss') as scope:\r\n        loss = tf.reduce_mean(tf.square(y_predict - y_true))\r\n\r\n    with tf.name_scope('train') as scope:\r\n        learning_rate = 0.5\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n        train = optimizer.minimize(loss)\r\n\r\n    # Before starting, initialize the variables. We will 'run' this first\r\n    init = tf.global_variables_initializer()\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        for step in range(NUM_STEPS):\r\n            sess.run(train, {x: x_data, y_true: y_data})\r\n            if step % 5 == 0:\r\n                print(step, sess.run([w, b]))\r\n                wb_.append(sess.run([w, b]))\r\n\r\n        print(NUM_STEPS, sess.run([w, b]))\r\n\r\n```\r\n", "I am also having a similar problem! I've noticed that when I use a dataset with 4 features the problem does not occur, however if I use 100 features then I have it.", "I have the same problem. It's related to the anaconda `python3` installation of `tensorflow`. For some reason anaconda recommends installing their version of `tensorflow` because it's faster than the official release since it comes with native `mkl` integration.\r\n\r\nI fixed the issue by running `conda uninstall tensorflow keras`. This seemed to partially remove the packages.  Then I run `pip install tensorflow keras` which worked successfully. \r\nInstalled them back again with `pip install tensorflow keras`.\r\nThis solved my problem.", "Closing this issue since its more a likely an issue with anaconda. You can always set a virtual env and run your code as a workaround. Feel free to reopen if have any further problems. Thanks!"]}, {"number": 24104, "title": "ERROR: Config value monolithic is not defined in any .rc file", "body": "Hello TensorFlow help team,\r\n\r\nI am trying to build Tensorflow for Raspberry pi using instructions given on \r\n\r\nhttps://www.tensorflow.org/install/source_rpi\r\n\r\nI hit to following error:\r\n\r\nmake[1]: Leaving directory `/tmp/openblas_src'\r\nBuilding for the Pi Two/Three, with NEON acceleration\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/workspace/tools/bazel.rc\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  Inherited 'common' options: --color=yes\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  'build' options: --verbose_failures --spawn_strategy=standalone --genrule_strategy=standalone\r\nINFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.4/dist-packages --python_path=/usr/bin/python3 --define with_jemalloc=true --define with_gcp_support=true --define with_hdfs_support=true --define with_aws_support=true --define with_kafka_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0 --define grpc_no_ares=true\r\n**ERROR: Config value monolithic is not defined in any .rc file**\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 4d7baf49-d76f-47e7-9bbb-f2bc6304fe21\r\n\r\n\r\nCould you please help me on this?\r\n\r\nThanks..\r\n", "comments": ["Can you please provide the following information asked by the template: (relevant in your case)\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker on Mac running ubuntu:14.04 \r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: (Building for pi on Mac using Docker using instructions given on https://www.tensorflow.org/install/source_rpi)\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: r1.11\r\nPython version: 3.4\r\nInstalled using virtualenv? pip? conda?: No\r\nBazel version (if compiling from source): 0.19.2\r\nGCC/Compiler version (if compiling from source): 4.8\r\nCUDA/cuDNN version: -\r\nGPU model and memory: -\r\n\r\nDescribe the problem:\r\nI hit to error saying ERROR: Config value monolithic is not defined in any .rc file\r\n\r\nin tensorflow/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh : line 106 --config=monolithic causing error.\r\n\r\nmake[1]: Leaving directory `/tmp/openblas_src'\r\nBuilding for the Pi Two/Three, with NEON acceleration\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/workspace/tools/bazel.rc\r\nINFO: Options provided by the client:\r\nInherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\nInherited 'common' options: --color=yes\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n'build' options: --verbose_failures --spawn_strategy=standalone --genrule_strategy=standalone\r\nINFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:\r\n'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.4/dist-packages --python_path=/usr/bin/python3 --define with_jemalloc=true --define with_gcp_support=true --define with_hdfs_support=true --define with_aws_support=true --define with_kafka_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0 --define grpc_no_ares=true\r\n**ERROR: Config value monolithic is not defined in any .rc file**\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 4d7baf49-d76f-47e7-9bbb-f2bc6304fe21\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem:\r\nfrom tensorflow directory I am running..\r\n\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" \\\r\n    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh", "Hello,\r\n\r\nI am facing the same issue. Tried to install for python 2.7 and its giving me the same error.\r\nDid you figure out any solution to this problem ?\r\n\r\nThanks you.", "@pranoot Not yet, still working on different options. ", "I got the solution.\r\n\r\nBazel 0.20.0 was getting installed which created the problem.\r\n\r\nHow to solve this problem:\r\n1. Remove previously created docker images (docker system prune -a)\r\n2. Remove line rm -rf /usr/local/bin/bazel from /tensorflow/tools/ci_build/install/install_pi_python3_toolchain.sh because bazel 0.15.0 is installed there and we don't want to remove it.\r\n3. Remove \"bazel\" install from /tensorflow/tools/ci_build/install/install_pi_python3_toolchain.sh (last line).\r\n\r\n**At step 14/14 check Bazel version is 0.15.0**", "Hello,\r\n\r\nThanks for the help. I tried to do it for python 2.7 but the build failed again. \r\nAlthough your advice solved \"ERROR: Config value monolithic is not defined in any .rc file\" this error.  \r\n\r\nThanks a lot again.\r\n\r\nRegards", "With Bazel 0.15.2, I'm able to build it.", "What was the resolution for this? I\"m using the Docker / cross-compilation instructions via https://www.tensorflow.org/install/source_rpi with r1.12 branch.  ", "@alexellis Yes, It's for source build for RPI and I had the same error when i compiled r1.12 branch.", "Thanks this discussion helped me a lot and I have summarized all the things in the below link\r\nhttps://stackoverflow.com/questions/53874017/error-while-building-libtensorflow-so-on-raspberry-pi/54202653#54202653", "@alexellis @Kishwar \r\nI have successfully generated the libternsorflow.so but when I tried to use the shared lib in GoLang\r\nI am facing issue\r\n\r\n\r\ngithub.com/tensorflow/tensorflow/tensorflow/go\r\n/home/pi/go/src/github.com/tensorflow/tensorflow/tensorflow/go/attrs.go:173: type [1073741824]_Ctype_longlong larger than address space\r\n/home/pi/go/src/github.com/tensorflow/tensorflow/tensorflow/go/attrs.go:173: type [1073741824]_Ctype_longlong too large\r\n\r\nCould you guys just help me to track down this error?", "@anilknayak Try to build on Raspberry pi instead of Docker. It will take time (few days) but this addressing issue should disappear. I got out of memory address problem when used Odroid to build. ", "I cross-compiled it in Ubuntu 18\r\nI could able to deploy in Raspberry PI 3 B+. No issue with armv7l.\r\n\r\nNow I know what version of bazel, protoc, tensorflow to be used to build from source. I will try to build on raspberry pi that is next step.\r\n\r\nBut could just identify what could be the issue with ```_Ctype_longlong too large```", "I am able to resolve the issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/24450\r\n\r\nby changing for 32 bit arch. change the following line\r\nin github.com/tensorflow/tensorflow/tensorflow/go/attrs.go\r\nfind 1<<30 for int64 slice and change it to 1<<27 it will work fine.\r\n\r\nI think the above issue is addressed but not pushed to master not even to v1.12.0 or v.1.11.0 branch of tensorflow.", "Hey @anilknayak \r\n\r\nDo you have the libtensorflow.so and other artefacts hosted somewhere?", "No, If you need I can publish it.\n\n\nOn Tue, Apr 2, 2019 at 12:31 PM Rushabh Nagda <notifications@github.com>\nwrote:\n\n> Hey @anilknayak <https://github.com/anilknayak>\n>\n> Do you have the libtensorflow.so and other artefacts hosted somewhere?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24104#issuecomment-479159415>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEqtwwKtOqMyDWhw9V0pGTAoVoYAhx6Uks5vc7AogaJpZM4Y9YLl>\n> .\n>\n\n\n-- \nAnil Kumar Nayak\nSoftware Engineer - Computer Vision, Deep Learning and Machine Learning\nOmnify.ai Inc\n1250 Borregas Avenue\nSunnyvale, CA\n\n-- Never give up, there is always a way, find it out.\n", "@anilknayak that'd be very helpful. thanks.", "Find the Tensorflow artifacts for ARM7 in this drive\nhttps://drive.google.com/open?id=1PBFFMCK87FHuiEhJaOqHJDFnxtDUeO9Y\n\nOn Tue, Apr 2, 2019 at 9:10 PM Rushabh Nagda <notifications@github.com>\nwrote:\n\n> @anilknayak <https://github.com/anilknayak> that's be very helpful.\n> thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24104#issuecomment-479329845>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEqtw4QH8nV0OL3d4pMT1p9xFbK9YMUSks5vdCmkgaJpZM4Y9YLl>\n> .\n>\n\n\n-- \nAnil Kumar Nayak\nSoftware Engineer - Computer Vision, Deep Learning and Machine Learning\nOmnify.ai Inc\n1250 Borregas Avenue\nSunnyvale, CA\n\n-- Never give up, there is always a way, find it out.\n"]}, {"number": 24103, "title": "SparseTensor file format", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12+\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently there does not seem to be a way to save just a `SparseTensor`.\r\nThis is problematic as 1. the initialization of an instance can take a _long_ time 2. some models might require a constant sparse tensor. \r\n\r\nThis is further supported by the fact that the Saver class is under the `train` api.\r\n\r\nI propose a `save_variable` method under `tf.data` which can save _any_ TensorFlow native variable to a file for later retrieval.\r\n\r\nIn addition, there should be a corresponding `load_variable` method under `tf.data`. \r\n\r\nNotably, unlike TensorFlow Records, `save_variable` should require nothing more than two arguments, `file` (where to save the variable) and `var` (the variable to save) and `load_varriable` should require nothing more than `file`. \r\n\r\n_Any_ necessary information for reconstructing the variable (e.g. shape, class type) can be hashed as a header in the file.\r\n\r\n\r\nAlternatively, provide users with a serialization function so we can use `pickle` as a short term fix.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt adds 2 methods to `tf.data`\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone.\r\n\r\n**Any Other info.**\r\n", "comments": ["What exactly do you mean by \"initialization of an instance can take a long time\"?\r\n\r\nCould you perhaps illustrate with an example(s) the use case(s) that this API would benefit?\r\n\r\nWhat prevents you from using `pickle`? I was able to use `pickle` for saving and loading a `tf.SparseTensor`:\r\n\r\n```\r\n  import pickle\r\n  import tf as tensorflow\r\n\r\n  tf.enable_eager_execution()\r\n\r\n  st = tf.SparseTensor(indices=[[0, 0], [0, 1], [0, 2], [1, 0], [3, 0], [3, 1]], values=[1, 2, 3, 4, 5, 6], dense_shape=[4, 3])\r\n  f = open('/tmp/tensor.pickle', 'w')\r\n  pickle.dump(st, f)\r\n  f.close()\r\n\r\n  f = open('/tmp/tensor.pickle', 'r')\r\n  st2 = pickle.load(f)\r\n  f.close()\r\n\r\n  print(st)\r\n  print(st2)\r\n```\r\nproduces\r\n```\r\nSparseTensor(indices=tf.Tensor(\r\n[[0 0]\r\n [0 1]\r\n [0 2]\r\n [1 0]\r\n [3 0]\r\n [3 1]], shape=(6, 2), dtype=int64), values=tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32), dense_shape=tf.Tensor([4 3], shape=(2,), dtype=int64))\r\nSparseTensor(indices=tf.Tensor(\r\n[[0 0]\r\n [0 1]\r\n [0 2]\r\n [1 0]\r\n [3 0]\r\n [3 1]], shape=(6, 2), dtype=int64), values=tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32), dense_shape=tf.Tensor([4 3], shape=(2,), dtype=int64))\r\n```", "@jsimsa thanks for chiming in.\r\n\r\nI mean the call `tf.SparseTensor(...)` for relatively large tensors can take a while. For an example, just copy / paste the code from google's rec sys tutorial... although for data I used a matrix with  `shape=(20000, 30000)` and sparsity \u2248`0.001` on a system with 100 Gb Ram, 40 cores and a GPU it took ~ 30 minutes to evaluate just `sparse = tf.SparseTensor(...)`, (not in eager execution mode). \r\n\r\nI was unaware pickle would work... some posts (on S.O. / non-official tutorials) stated they had issue with pickling TF objects (but if it works now, that is nice to know...)\r\n\r\n\r\ne.g. in a colab/  jupyter  notebook \r\n\r\n```\r\n# cell 1\r\n# read or randomly make the dense matrix \r\n\r\n# cell 2\r\nsparse = coo_matrix(dense) \r\n\r\nsparse_indicies = list(zip(\r\n    sparse.row.astype(np.int64).tolist(), \r\n    sparse.col.astype(np.int64).tolist()\r\n))\r\n\r\n\r\n# cell 3 takes forever to return\r\ninput_tensor = tf.SparseTensor(\r\n    indices     = sparse_indicies,\r\n    values      = (sparse.data).astype(np.float32),\r\n    dense_shape = sparse.shape\r\n)\r\n```", "@SumNeuron,\r\nSorry for the delayed response. In **`Tensorflow 2.0`**, as we rarely use [**`Saver`** class is under the **`train`** api](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver) and we use [tf.saved_model.save](https://www.tensorflow.org/api_docs/python/tf/saved_model/save), can you please let us know if this **`Feature`** is relevant? \r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24102, "title": "Tensorflow Prebuilt Binaries are compiled for incompatible CUDA and cuDNN libraries", "body": " Tensorflow Prebuilt Binaries are compiled for CUDA 9.0 and cuDNN 7.2.1, however, cuDNN 7.2.1 download not available for CUDA 9.0", "comments": ["Can you please provide following information?\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "It's not a build issue, I was just pointing out that the prebuilt binaries have CUDA and cuDNN versions that are incompatible with each other(i.e. there is no cuDNN 7.2.1 for CUDA 9.0, at least on the cuDNN archives in the nVidia website)", "@StealthySemicolon Apologies for the delay in response. You can find cuDNN 7.2.1 from [cuDNN archive](https://developer.nvidia.com/rdp/cudnn-archive). Let me know if this doesn't work for you. Thanks!", "cuDNN 7.2.1 does exist, just not for CUDA 9.0. There is no cuDNN 7.2.1 for CUDA 9.0 in the cuDNN archive."]}]