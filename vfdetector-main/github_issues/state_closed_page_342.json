[{"number": 43866, "title": "got error while import keras libraries", "body": "When i run the code to import the keras in anaconda 2020.07 and i am using python 3.8 and tensorflow  2.3.1 and keras 2.4.3  i got the error that is mention below: \r\n\r\nthe solutions i tried from my side : i try to downgrade the version of python and tensorflow and keras but still i got the same error.\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      2 try:\r\n----> 3     from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n      4 except ImportError:\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ankit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-88d96843a926> in <module>\r\n----> 1 import keras\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      3     from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n      4 except ImportError:\r\n----> 5     raise ImportError(\r\n      6         'Keras requires TensorFlow 2.2 or higher. '\r\n      7         'Install TensorFlow via `pip install tensorflow`')\r\n\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`", "comments": ["@ankitkumar7424 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the[ latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nImport keras  using the below command.\r\n`from tensorflow import keras`\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43866\">No</a>\n"]}, {"number": 43865, "title": "Wrong linker flags in Makefile for ARM Compiler", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 917882bfa2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nRecently, --fatal-warnings and --gc-sections was added as linker flags to the Makefile. These flags is not available for ARM Compiler (armclang), the corresponding flags are instead --diag_error=warning and --remove.\r\n\r\nI propose changing the makefile to something like this, in order to use the correct flags when the ARM Compiler is used.\r\nhttps://github.com/jenselofsson/tensorflow/commit/dba2406f6c3396b1d936c53f4dcd6f763b0dafbf\r\n\r\nIf I'm not mistaken, the --remove flag is the default, but could be added in order to be consistent with the gcc-flags for readability.\r\n", "comments": ["Somewhat related to #43726 so tagging @mansnils as well.\r\n\r\nI'd like to keep the `TAGS` list reserved the optimized kernel implementations. I recognize that its a catch-all name and I would like to change that too.\r\n\r\nMy suggestion here would be to add a new command line option called `TOOLCHAIN`, that can help select between armclang, gcc etc. rather than adding armclang to the TAGS.\r\n\r\nThen we would change\r\nhttps://github.com/tensorflow/tensorflow/blob/84202f860fa45b0508a9a23aae4698fc044e0bb0/tensorflow/lite/micro/tools/make/Makefile#L39-L42\r\n\r\nto something like:\r\n\r\n```make\r\nifeq ($(TOOLCHAIN), armclang))\r\n  CXX_TOOL := armclang++?\r\n  CC_TOOL := ?\r\n  AR_TOOL :=?\r\nelse\r\n  # default to gcc\r\n  CXX_TOOL := g++\r\n  CC_TOOL := gcc\r\n  AR_TOOL := ar\r\nendif\r\n```\r\n\r\nAnd for the `LDFLAGS`:\r\n\r\n```make\r\nifeq ($(TOOLCHAIN), armclang))\r\n  LDFLAGS += \\\r\n    -Wl,--diag_error=warning \\\r\n    -Wl,--remove\r\nelse\r\n  # default to gcc\r\n  LDFLAGS += \\\r\n    -Wl,--fatal-warnings \\\r\n    -Wl,--gc-sections\r\nendif\r\n```\r\n\r\nWill this work for you?", "Yes, that should work just fine.", "https://github.com/tensorflow/tensorflow/issues/43887#event-3856301945\r\n\r\nIs related for macos - MacOS uses clang for g++:\r\n```\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/c++/4.2.1\r\nApple clang version 11.0.0 (clang-1100.0.33.8)\r\nTarget: x86_64-apple-darwin19.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```", "@nkreeger that does look related", "https://github.com/tensorflow/tensorflow/pull/43726 is in the right direction but it may not completely address the issue here. Once that PR is merged, let's revisit and see what additional changes are needed.", "Closing since TFLM is now in its own stand-alone repository and I believe that we have addressed this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43865\">No</a>\n"]}, {"number": 43864, "title": "Fix tf.function speacial casing for forwardporp (placeholder issue)", "body": " PR #42048 had some tests running into problems [here](https://github.com/tensorflow/tensorflow/pull/42048#pullrequestreview-463602138). \r\nThere seems to be a problem with tf.function special casing for forwardprop. This PR is to discuss possible solutions and fix that. ", "comments": ["@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4  This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4  This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "@abhichou4  This PR is in draft, any update on this? Please. Thanks!", "@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 43863, "title": "Getting tensorflow.python.framework.errors_impl.NotFoundError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No \r\n- OS Platform and Distribution (e.g., Windows 10):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Not Applicable\r\n- TensorFlow installed from (source or binary):  PyPI\r\n- TensorFlow version (use command below): 2.3.1 (v2.3.0-54-gfcc4b966f1 2.3.1)\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): Not Applicable\r\n- GCC/Compiler version (if compiling from source): Not Applicable\r\n- CUDA/cuDNN version: Not applicable\r\n- GPU model and memory: Not applicable\r\n\r\n**Describe the current behavior**\r\nWhen I try to save a tensorflow model to disk, it has recently started giving this error - \"tensorflow.python.framework.errors_impl.NotFoundError\". It used to work for over a month without issues, and nothing has changed as far as I know; but now, giving the error. I tried saving the model as a pickle file like this --> dnn_classifier.save(\"/path/m.pkl\") and also without the .pkl extension like dnn_classifier.save(\"/path/m\").    I observe that the \"assets\" folder and the \"saved_model.pb\" files are not getting created.\r\n\r\n**Describe the expected behavior**\r\nThe \"assets\" folder and the \"saved_model.pb\" files should get created.\r\n\r\n** Log **\r\n.venvnew\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nNotFoundError: Failed to create a NewWriteableFile: <modelName>\\variables\\variables_temp_85e1300e45534129922b22ad101f5b87/part-00000-of-00001.data-00000-of-00001.tempstate8682566087768856168 : The system cannot find the path specified.\r\n; No such process [Op:SaveV2]\r\n", "comments": ["@bkrishna2006 \r\nFor the error \"tensorflow.python.framework.errors_impl.NotFoundError\" please refer to this links and let us know : [link](https://github.com/tensorflow/models/issues/3762), [link1](https://stackoverflow.com/questions/42928822/tensorflow-python-framework-errors-impl-notfounderror-while-creating-a-custom-in), [link2](https://github.com/tensorflow/models/issues/6595).\r\n\r\nFor the error \"NotFoundError: Failed to create a NewWriteableFile:\" : please verify the solutions and enable memory to run your code : [link](https://stackoverflow.com/questions/45076911/tensorflow-failed-to-create-a-newwriteablefile-when-retraining-inception), [link1](https://github.com/tensorflow/tensor2tensor/issues/253)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43863\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43863\">No</a>\n"]}, {"number": 43862, "title": "Reading cuda array in c++ as tensor and reading that tensor in python", "body": "I am getting frames from camera feed, which is basically a cuda array/vector (not OpenCV mat). I want to read this array periodically in tensoflow python for further operations. To do that, first step is to read this cuda array in the form of GPU tensor in tensoflow, without transferring the array from GPU->CPU->GPU, which is basically an overhead. SInce it is cuda array is already present in the GPU, it can be done GPU->GPU. \r\nThe issue here is which function in tensorflow api supports this GPU->GPU operation? I could not find one, if any.\r\n\r\nThe second issue is, reading this array in array in python side of code. I have gone the https://www.tensorflow.org/guide/create_op, which tells how to create the custom operations in python. But, the template is you send some array, do the operation in cuda, and receive it back in tf. Here, I am trying to read the cuda array in c++ periodically, \r\nPlease help what should be the next step forward. \r\n\r\nBasically my code looks like this:\r\n``\r\n\r\n    #include    <unistd.h>   \r\n    #include    <iostream>\r\n    #include    \"VideoReader.h\"\r\n    #include <cuda_profiler_api.h>\r\n\r\n    int main (int argc, char *argv[])\r\n    {\r\n     void    *fReader = NULL;\r\n \r\n    //// Starting the frameReader to read the camera feed 1. \r\n    fReader = frameReader ::Init (  \"rtsp://admin:........\",  //camera feed url\r\n                                                    ......................);  //some more options to read the frame\r\n    \r\n       do \r\n       {\r\n        pOutData        = 0;\r\n     \r\n        // get frame from camera feed 1\r\n        fResult = fReader::GetVideoFrame (fReader1, &pOutData);\r\n       \r\n       ///// pOutData is the cuda array containing the cuda vector . This vector, I need in tensorflow python for further processing.\r\n      ///// How can I get this vector?     \r\n    \r\n        }\r\n   \r\n    } while (1);\r\n    return 0;\r\n    }\r\n\r\n\r\n``", "comments": ["Hi @gowthamkpr @ravikyram . Please let me know if you need more clarity about the issue or you want me to go through some resources.  ", "@govindamagrawal Please give us more clarity about the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43861, "title": "Unable to Install TF in Conda", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n- Python version:  3.8  (64-bit)\r\n- Installed using virtualenv? pip? conda?: conda\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I try to install tensorflow via conda, the following error message shows.\r\n\r\n------------------------------------------------------------------------------------------------------\r\n------------------------------------------------------------------------------------------------------\r\nUnsatisfiableError: The following specifications were found\r\nto be incompatible with the existing python installation in your environment:\r\n\r\n**Specifications:** \r\n\r\n  **- tensorflow-base -> python[version='>=3.5,<3.6.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0']**\r\n **- tensorflow-datasets -> python[version='3.6.*|>=2.7,<2.8.0a0|>=3.5,<3.6.0a0|3.7.*']**\r\n\r\n**Your python: python=3.8**\r\n\r\nIf python is on the left-most side of the chain, that's the version you've asked for.\r\nWhen python appears to the right, that indicates that the thing on the left is somehow\r\nnot available for the python version you are constrained to. Note that conda will not\r\nchange your python version to a different minor version unless you explicitly specify\r\nthat.\r\n\r\nThe following specifications were found to be incompatible with each other:\r\n\r\nOutput in format: Requested package -> Available versions\r\n\r\nPackage zlib conflicts for:\r\npython=3.8 -> sqlite[version='>=3.33.0,<4.0a0'] -> zlib[version='>=1.2.11,<1.3.0a0']\r\ntensorflow-base -> zlib[version='>=1.2.11,<1.3.0a0']\r\n\r\nPackage requests conflicts for:\r\npython=3.8 -> pip -> requests\r\ntensorflow-datasets -> requests[version='>=2.19.0']The following specifications were found to be incompatible with your system:\r\n\r\n  - feature:/win-64::__cuda==10.2=0\r\n\r\nYour installed version is: 10.2\r\n\r\n------------------------------------------------------------------------------------------------------\r\n------------------------------------------------------------------------------------------------------\r\n\r\nAs far as I know, TF should support Python 3.8. But why does it show the bold error message? That makes me really confused. Anyone can help? Thanks!\r\n\r\n_A stupid update: After creating a new environment in which I set up the Python version to be 3.7, everything works perfectly. Does it mean that TF currently only supports for (up to) Python 3.7? But indeed, I saw online that many ppl mentioned that it does support for Python 3.8 (some even mentioned the newly released Python 3.9). So why? I get even more confused ..._\r\n\r\n", "comments": ["@yiwenxu6,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "> Does it mean that TF currently only supports for (up to) Python 3.7?\r\n\r\n@yiwenxu6,\r\nTensorFlow does support Python 3.8. For more information regarding TensorFlow pip packages, please check the [system requirements](https://www.tensorflow.org/install/pip#system-requirements). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43861\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43861\">No</a>\n"]}, {"number": 43860, "title": "CUDA driver is not load in docker container", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **Linuix Ubuntu 20.04 LTS**:\r\n-   **TensorFlow installed from tensorflow/tensorflow:latest-devel-gpu**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nCUDA ERROR\r\n\r\n### Source code / logs\r\n\r\nin my docker container\r\nroot@jupyter:/# nvidia-smi\r\nThu Oct  8 01:46:15 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: ERR!     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:65:00.0  On |                  N/A |\r\n|  0%   46C    P8    16W / 250W |   2082MiB / 11177MiB |      2%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nin my PC\r\nadminn@adminn-System-Product-Name:~$ nvidia-smi\r\nThu Oct  8 10:54:08 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:65:00.0  On |                  N/A |\r\n|  0%   51C    P0    64W / 250W |   2089MiB / 11177MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1449      G   /usr/lib/xorg/Xorg                102MiB |\r\n|    0   N/A  N/A      2786      G   /usr/lib/xorg/Xorg               1073MiB |\r\n|    0   N/A  N/A      2916      G   /usr/bin/gnome-shell              301MiB |\r\n|    0   N/A  N/A      4045      G   ...AAAAAAAAA= --shared-files      166MiB |\r\n|    0   N/A  N/A      4172      G   ...AAAAAAAAA= --shared-files      429MiB |\r\n+-----------------------------------------------------------------------------+", "comments": ["@kimyoungjin06 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\nPlease share the error log for us to analyse.", "@Saduf2019\r\nIt is an error about docker container, not about the tensorflow itself.\r\nThis is `container` information \r\n\r\nIMAGE: latest-devel-gpu\r\nLast updated16 hours agobytensorflowpackages\r\nDIGEST: 0513e3814d9c\r\nOS/ARCH: linux/amd64\r\nCOMPRESSED SIZE: 3.52 GB", "Can you post a full reproduction example, including the docker invocation, please?", "```bash\r\n$ sudo docker --version\r\n[sudo] password for adminn: \r\nDocker version 19.03.13, build 4484c46d9d\r\n\r\n$ sudo nvidia-docker run -it -p 8889:8888 -p 6007:6006 -h jupyter -v /home/adminn/Jupyter/Workspace:/home/Workspace --name jupyter2 tensorflow/tensorflow:latest-devel-gpu\r\n$ sudo docker start jupyter2\r\n$ sudo docker attach jupyter2\r\n\r\nroot@jupyter:/# nvidia-smi\r\nThu Oct 8 01:46:15 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.66 Driver Version: 450.66 CUDA Version: ERR! |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n| | | MIG M. |\r\n|===============================+======================+======================|\r\n| 0 GeForce GTX 108... Off | 00000000:65:00.0 On | N/A |\r\n| 0% 46C P8 16W / 250W | 2082MiB / 11177MiB | 2% Default |\r\n| | | N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n`container info`\r\n```bash\r\nIMAGE: latest-devel-gpu\r\nLast updated16 hours agobytensorflowpackages\r\nDIGEST: 0513e3814d9c\r\nOS/ARCH: linux/amd64\r\nCOMPRESSED SIZE: 3.52 GB\r\n```\r\n@angerson Do you need any other information?", "Based on your running `nvidia-docker` instead of `--gpus=all`, it looks like you're using nvidia-docker 1.0. Can you try [after migrating to 2.0](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#migration-from-nvidia-docker-1-0) instead?", "I do not remember the version of nvidia-docker, but I try migrating to 2.0\r\nIt is my log\r\n```\r\n$ sudo docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f\r\n$ sudo apt-get purge nvidia-docker\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nPackage 'nvidia-docker' is not installed, so not removed\r\n0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\r\n```\r\nafter that,\r\n```\r\n$ sudo docker run -it --gpus=all -p 8889:8888 -p 6007:6006 -h jupyter -v /home/adminn/Jupyter/Workspace:/home/Workspace --name jupyter2 tensorflow/tensorflow:latest-devel-gpu\r\n```\r\nBut, it comes same result. How can I check nvidia-docker version?", "```\r\n$ nvidia-docker version\r\nNVIDIA Docker: 2.5.0\r\nClient: Docker Engine - Community\r\n Version:           19.03.13\r\n API version:       1.40\r\n Go version:        go1.13.15\r\n Git commit:        4484c46d9d\r\n Built:             Wed Sep 16 17:02:36 2020\r\n OS/Arch:           linux/amd64\r\n Experimental:      false\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          19.03.13\r\n  API version:      1.40 (minimum version 1.12)\r\n  Go version:       go1.13.15\r\n  Git commit:       4484c46d9d\r\n  Built:            Wed Sep 16 17:01:06 2020\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.3.7\r\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\r\n runc:\r\n  Version:          1.0.0-rc10\r\n  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd\r\n docker-init:\r\n  Version:          0.18.0\r\n  GitCommit:        fec3683\r\n```", "Can you run and paste the log of `sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi`, please?", "```bash\r\nsudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\r\n[sudo] password for adminn: \r\nMon Oct 19 06:31:01 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:65:00.0  On |                  N/A |\r\n|  0%   40C    P8    14W / 250W |    607MiB / 11177MiB |      9%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n```", "@kimyoungjin06 Can you please let us know if this issue still persists in latest version 2.6.0 ? Please refer to [link](https://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container) and let us know if it helps?  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43860\">No</a>\n"]}, {"number": 43859, "title": "Autotune Dataset.map when transforming builtin map with autograph", "body": "This PR enables autotuning for `tf.data.Dataset.map()` calls that have been generated from built in `map()` via autograph. This has the potential to significantly increase performance in some cases.", "comments": []}, {"number": 43858, "title": "Alternate approach (to PR #43240) to fixing ethos-u build errors.", "body": "Taking this opportunity to remove variables with the same names but in\r\ndifferent namespaces. This is one of the reasons we are incrementally\r\nmoving to a flat namespace (https://abseil.io/tips/130).\r\n\r\nAdditionally, having the constants in a local scope follow the style of\r\nlocal variables.\r\n\r\nThe fix (hopefully) for Ethos-U is coming from the fact that the array\r\ndimensions are now coming from constexpr variables.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43857, "title": "Fix typos", "body": "Mostly just repeating words.\r\nIf needed I can split it into more PRs as it probably covers too many files (but on the other hand no code changes so shouldn't be too bad maybe?).", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/43857\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "I think it's better to split this by directory.", "Sure, @mihaimaruseac, will do.", "@mihaimaruseac, would this be alright or should I split even further?\r\n| Dirs                                   | # of files  |\r\n|:----------------------------:|:----------:|\r\n| This PR                              | 145         |\r\n|                                           |                |\r\n| c & cc & compiler & core | 62            |\r\n| go & python                      | 43            |\r\n| lite & rest                           | 40            |", "I think that might still have too many files. Splitting by directory is better as that would mean a change will need fewer approvals to merge. So, one PR for `c/`, one for `cc/`, one for `compiler/`, etc. It might be likely that even those PRs will need to be split due to the internal approval hierarchy but we'll cross that bridge when we get to it."]}, {"number": 43856, "title": "Update flatbuffers dependency", "body": "Includes a patch which fixes implicit double promotion causing issues on some platforms.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43855, "title": "Use sysctl for maximum CPU frequency on MacOS", "body": "Hello, I am one of the maintainers of the Tensorflow bindings for Elixir (https://github.com/pylon/extensor) project. We ran into a deadlock on MacOS in a recent release of the Erlang VM related to the `popen` call used to retrieve the maximum CPU frequency when a new Tensorflow session is created. The changes on this PR replace the `popen`/`fork` mechanism used to retrieve these parameters on MacOS with a `sysctl` system call that doesn't fork the process, which avoids problems with the ports mechanism in the Erlang VM.\r\n\r\nIt appears that this was the only use of the `popen` function in the Tensorflow codebase, and there is precedent for using the `sysctl` system calls elsewhere. We also believe that this method is simpler than calling out to the shell to retrieve this parameter. Because the name of the `sysctl` parameter is the same in both versions, this change should be low risk and have the same failure modes as the previous version. That said, I have only tested this change on MacOS Catalina 10.15.7.\r\n\r\nPlease let me know if there is any more information I can provide, and thank you for reviewing this PR.\r\n", "comments": ["I appreciate the review, and no worries about any delay."]}, {"number": 43854, "title": "Issue when convert to tf lite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 1.x\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from models/saved_model/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: input\r\nINFO:tensorflow: tensor name: Reshape_1:0, shape: (1, 1960), type: DT_FLOAT\r\nINFO:tensorflow:output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: output\r\nINFO:tensorflow: tensor name: labels_softmax:0, shape: (1, 4), type: DT_FLOAT\r\nINFO:tensorflow:Restoring parameters from models/saved_model/variables/variables\r\nINFO:tensorflow:Froze 4 variables.\r\nINFO:tensorflow:Converted 4 variables to const ops.\r\nFloat model is 68048 bytes\r\nINFO:tensorflow:Restoring parameters from models/saved_model/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: input\r\nINFO:tensorflow: tensor name: Reshape_1:0, shape: (1, 1960), type: DT_FLOAT\r\nINFO:tensorflow:output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: output\r\nINFO:tensorflow: tensor name: labels_softmax:0, shape: (1, 4), type: DT_FLOAT\r\nINFO:tensorflow:Restoring parameters from models/saved_model/variables/variables\r\nINFO:tensorflow:Froze 4 variables.\r\nINFO:tensorflow:Converted 4 variables to const ops.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n11 frames\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'data_2/wav_filename' with dtype string\r\n\t [[{{node data_2/wav_filename}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'data_2/wav_filename' with dtype string\r\n\t [[node data_2/wav_filename (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\nOriginal stack trace for 'data_2/wav_filename':\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\r\n    self.io_loop.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 132, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\r\n    handler_func(fileobj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 462, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 492, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 444, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-40-582b489634f7>\", line 9, in <module>\r\n    TESTING_PERCENTAGE, model_settings, LOGS_DIR)\r\n  File \"/content/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 203, in __init__\r\n    self.prepare_processing_graph(model_settings, summaries_dir)\r\n  File \"/content/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 398, in prepare_processing_graph\r\n    tf.string, [], name='wav_filename')\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py\", line 2619, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_array_ops.py\", line 6669, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": []}, {"number": 43853, "title": "getting these errors in tensorflow.contrib, TfPoseEstimator etc. while running the command \"python run_webcam.py --model=mobilenet_thin --resize=432x368 --camera=0\" . I have build the pafprocess c++ files swig successfully, but also I'm getting the errors", "body": "Traceback (most recent call last):\r\n  File \"run.py\", line 6, in <module>\r\n    from tf_pose import common\r\n  File \"D:\\tf-pose-estimation\\tf_pose\\__init__.py\", line 5, in <module>\r\n    from tf_pose.runner import infer, Estimator, get_estimator\r\n  File \"D:\\tf-pose-estimation\\tf_pose\\runner.py\", line 8, in <module>\r\n    from tf_pose import eval\r\n  File \"D:\\tf-pose-estimation\\tf_pose\\eval.py\", line 14, in <module>\r\n    from tf_pose.networks import model_wh, get_graph_path\r\n  File \"D:\\tf-pose-estimation\\tf_pose\\networks.py\", line 6, in <module>\r\n    from tf_pose.network_mobilenet import MobilenetNetwork\r\n  File \"D:\\tf-pose-estimation\\tf_pose\\network_mobilenet.py\", line 5, in <module>\r\n    from tf_pose import network_base\r\n  File \"D:\\tf-pose-estimation\\tf_pose\\network_base.py\", line 8, in <module>\r\n    import tensorflow.contrib.slim as slim\r\nModuleNotFoundError: No module named 'tensorflow.contrib'", "comments": ["@jangalasriramd7,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "Also, please go through issue [#40802](https://github.com/tensorflow/tensorflow/issues/40802#issuecomment-651292851), with a similar error and let us know if it helps. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43852, "title": "[ROCm] Fix for ROCM CSB breakage - 201007", "body": "The following commit introduces a new subtest that is failing on the ROCm platform\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/f2ebefba658fb4f424842f03d15f694cb917110f\r\n\r\nThe new subtest ( `NcclManagerTest.Abort` ) requires simulating a multi-node environment on a single-node with mutliple GPUs. This functionality is currently not available on the ROCm platform, and other subtests that require this functionality have already been disabled on the ROCm platform. Doing the same for the newly added subtest as well.\r\n\r\n\r\n------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": []}, {"number": 43851, "title": "AttributeError: 'Tensor' object has no attribute '_lazy_read' inside tf.while_loop containing tf.scatter_nd_update", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n--\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv1.15.0-rc3-22-g590d6ee 1.15.0\r\n- Python version:\r\nPython 3.6.10\r\n- Bazel version (if compiling from source): --\r\n- GCC/Compiler version (if compiling from source): --\r\n- CUDA/cuDNN version:\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 0\r\nCuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory:\r\nAsus Cerberus GTX-1070TI-A8G 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nImplementing a simple `tf.while_loop` which contains a `tf.scatter_nd_update` function throws an error:\r\n\r\n> AttributeError: 'Tensor' object has no attribute '_lazy_read'\r\n\r\nThe behaviour is present only in lazy mode (non-eagar execution). Also, it is not appearing when used outside of `tf.while_loop` with a fixed `j`.\r\n\r\n**Describe the expected behavior**\r\nI should be able to implement this fixed iteration loop without an error. In a [similar issue](https://github.com/tensorflow/tensorflow/issues/21957) the solution suggested converting Tensor to Variable which does not work in my case though.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nref = tf.Variable([[0, 1, 0, 2],\r\n                   [0, 1, 2, 2],\r\n                   [1, 2, 1, 3]], dtype=tf.int32)\r\ntrue_array = tf.Variable([[1, 1, 1, 1]])\r\nfalse_array = tf.Variable([[1, 0, 1, 0]])\r\nnum_iters = tf.Variable(3, dtype=tf.int32)  # 3\r\n\r\n\r\ndef body(ref, true_array, false_array, j, num_iters):\r\n    samples = tf.cond(tf.equal(tf.reduce_sum(ref[j, :], axis=0), 1), lambda: true_array, lambda: false_array)\r\n    ref = tf.scatter_nd_update(ref, [[j]], samples)\r\n     j = tf.add(j, 1)\r\n    return ref, true_array, false_array, j, num_iters\r\n\r\n\r\ncond = lambda ref, true_array, false_array, j, num_iters: tf.less(j, num_iters)\r\nj = tf.Variable(0, dtype=tf.int32)  # tf.constant(0)\r\nref, true_array, false_array, j, num_iters = tf.while_loop(cond, body, [ref, true_array, false_array, j, num_iters])\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print('ref', sess.run(ref))\r\n    print('j', sess.run(j))\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@eypros \r\nThere is no official support for 1.x, please upgrade to 2.x.\r\nWith respect to the error please refer to [this comment](https://stackoverflow.com/questions/54096360/how-to-properly-use-tf-scatter-update-for-n-dimensional-updating) and let us know it helps resolve the issue.", "@Saduf2019 no, the approach mentioned in the stackoverflow comment did not help, since my `ref` was already `tf.Variable`.", "@eypros \r\nCan you please upgrade to 2.x and let us know if you face any issues.", "I have tried tf 2.0 which but in non eager mode (basically in compatibility v1 mode) using either:\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\nor even\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\nbut did not solve my problem.\r\n\r\nInstead a _working solution_ for me was to use `tf.tensor_scatter_nd_update()` in place of `tf.scatter_nd_update()` and it seems to work as expected now.", "i am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/47bc95225276d54ade13f52475a0a6fe/untitled432.ipynb).", "@eypros  Currently we are not making any improvements  for TF 1.x and hence we suggest users to use TF 2.x  version. \r\n\r\nClosing this issue since it was resolved for you.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43851\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43851\">No</a>\n"]}, {"number": 43850, "title": "[XLA/GPU] Size-constrained buffer allocation.", "body": "This change provide the capability to XLA to generate multiple heaps\r\n(i.e., temp buffers) with a size constraint on each heap to avoid Out-of-Memory\r\ndue to memory fragmentation. Note that larger allocations are more subject to\r\nthe effect of fragmentation.\r\n\r\n\r\nThis fixes some TF/XLA OOM issue our clients constantly reported without significant memory use size increase.\r\n\r\nI can share some data points I have at hands:\r\n\r\nRN50 fp16 | Baseline (no constraint) | (constraint=) 2G | 1G | 800M | 700M\r\n-- | -- | -- | -- | -- | --\r\nTotal alloc (reported by XLA) | 10.90GiB | 10.90GiB | 10.92GiB | 10.90GiB | 10.91GiB\r\n\r\nBERT fp16 | Baseline (no constraint) | (constraint=) 1G | 500M | 300M\r\n-- | -- | -- | -- | --\r\nTotal alloc (reported by XLA) | 8.60GiB | 8.60GiB | 8.60GiB | 8.60GiB\r\n\r\n\r\nUNet3d fp16 | Tensorflow(without XLA) | XLA with no size constraint on temp buffer | XLA with size-constraint=1G on temp buffer\r\n-- | -- | -- | --\r\nPeak Memory Use (dumped by bfc_allocator) | 10.02GiB | OOM | 10.02GiB\r\nTotal Alloc(reported by XLA) | \u00a0 | 5.81GiB | 6.09GiB\r\n\r\n", "comments": ["@sanjoy, could you help to take a look of this PR? (or recommend someone to do so.) Thanks!\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I fixed a minor error. Need your approval again.\r\n(I `git commit --amend` [a previous commit](https://github.com/tensorflow/tensorflow/pull/43850/commits/12b38f98bb2914aa3b37a5df544f076925927e81#diff-527519d68fd5b0b89fb2abee61d2b5fe553d5ba9ad2911422718e9d38994f3f7R412) because I did not expect the review so soon.)\r\n```\r\nsize_t total_chunk_count = 0;\r\nabsl::c_accumulate(...)\r\n\r\n```\r\n\r\n=>\r\n\r\n`size_t total_chunk_count = absl::c_accumulate(...)`\r\n\r\n", "\r\nJust check whether there is anything that needs me to do on my side or it is just waiting to be merged?\r\n\r\nI saw two check failures but I cannot see the details so I don't know if they are OK or not.\r\n", "There are some internal build errors. There is an unfortunate long-standing issue that the warning settings differ for the internal and external builds and IIRC the open-source build does not use -Werror (in general, it makes it really helpful to make sure there are no warnings).\r\n\r\nOne error I see is:\r\n\r\n```\r\n./tensorflow/compiler/xla/service/heap_simulator.h:484:9: error: field 'size_limit_per_heap_' will be initialized after base 'GlobalDecreasingSizeBestFitHeap<xla::HloValue>' [-Werror,-Wreorder-ctor]\r\n```", "> There are some internal build errors. There is an unfortunate long-standing issue that the warning settings differ for the internal and external builds and IIRC the open-source build does not use -Werror (in general, it makes it really helpful to make sure there are no warnings).\r\n> \r\n> One error I see is:\r\n> \r\n> ```\r\n> ./tensorflow/compiler/xla/service/heap_simulator.h:484:9: error: field 'size_limit_per_heap_' will be initialized after base 'GlobalDecreasingSizeBestFitHeap<xla::HloValue>' [-Werror,-Wreorder-ctor]\r\n> ```\r\n\r\nFixed the compilation warning.\r\n\r\nIs adding -Werror into .bazelrc a right fix? I see many warnings when I compile the OSS Tensorflow. It is a bit hard for me to believe I can add a global -Werror and succeed the build. Will give it a try later.\r\n", "About using -Werror, I agree in the open source, there is too many warning to be able to use it.\r\nBut we do not use the same compilation as what Google use internally. Maybe this is different default of g++ vs clang.\r\nFor example Google use the flag: `-Wno-sign-compare`.\r\nI already added `build --copt=-Wno-sign-compare` to our config generator of .tf_configure.bazelrc.\r\nBut it didn't worked.\r\n\r\nTo enable -Werror, we would need to find all the flag Google use. Make bazel use them correctly. Then make sure g++ doesn't generates extra warnings vs clang. Then we can enable -Werror.", "Yes, unfortunately we use a different set of warnings internally, and no one bothered so far to port them properly to the OSS build.", "Just check again if there is still any build error in your internal CIs?", "Yes, sorry for being slow (we really ought to fix this!)\r\n\r\n```\r\n/tensorflow/compiler/xla/service/memory_space_assignment.cc:1256:10: error: moving a local object in a return statement prevents copy elision [-Werror,-Wpessimizing-move]\r\n  return std::move(result);\r\n         ^\r\n\r\n/tensorflow/compiler/xla/service/memory_space_assignment.cc:1256:10: note: remove std::move call here\r\n  return std::move(result);\r\n         ^~~~~~~~~~      ~\r\n```", "> Yes, sorry for being slow (we really ought to fix this!)\r\n> \r\n> ```\r\n> /tensorflow/compiler/xla/service/memory_space_assignment.cc:1256:10: error: moving a local object in a return statement prevents copy elision [-Werror,-Wpessimizing-move]\r\n>   return std::move(result);\r\n>          ^\r\n> \r\n> /tensorflow/compiler/xla/service/memory_space_assignment.cc:1256:10: note: remove std::move call here\r\n>   return std::move(result);\r\n>          ^~~~~~~~~~      ~\r\n> ```\r\n\r\nFixed. My oversight. Please help to take a look again.\r\n "]}, {"number": 43848, "title": "Building micro_speech for Arm Mbed OS using ARM Compiler (armclang) fails", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 917882bfa2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS\r\n\r\n**Describe the problem**\r\nBuilding micro_speech with the commands below results in this error:\r\n`./tensorflow/lite/micro/tools/make/downloads/kissfft/kiss_fft.h:45:10: fatal error: 'sys/types.h' file not found`\r\n\r\nSwitching out ARMC6 for GCC_ARM in the final command fixes the issue.\r\nThe issue comes from sys/types.h not being available in the ARM Compiler toolchain, while it is available when using gcc. \r\n\r\nPatching kissfft in the download_and_extract.sh-script to remove the `#include <sys/types.h>` fixes the issue (https://github.com/jenselofsson/tensorflow/commit/080bc19079f25a2f2151e67d303f1f1949bcfc3e), and it compiles using both GCC_ARM and ARMC6.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TAGS=\"cmsis-nn disco_f746ng\" generate_micro_speech_mbed_project\r\n$ cd tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/micro_speech/mbed/\r\n$ mbed config root .\r\n$ mbed deploy\r\n$ mbed compile -m DISCO_F746NG -t ARMC6\r\n", "comments": ["@advaitjain Let me know if the solution is acceptable, and I can create a pull request with the fix.", "Yes, this works. Thanks!", "@advaitjain PR submitted!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43848\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43848\">No</a>\n"]}, {"number": 43847, "title": "Go installation error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 32\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3.0\r\n- Python version:3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: 1660 gtx super (6GB)\r\n- GO version: 1.15.2\r\n\r\nHello i'm trying to install the tensorflow go version to build a little application. So i installed the C library (2.3.0 version tested with classic hello.c example) and after that i run the following command:\r\n\r\n`go get github.com/tensorflow/tensorflow/tensorflow/go`\r\n\r\nbut after this one i get this error:\r\n\r\n```bash\r\ngo: found github.com/tensorflow/tensorflow/tensorflow/go in github.com/tensorflow/tensorflow v2.3.1+incompatible\r\ngo: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\ngo: finding module for package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\n../../../../golang/gopath/pkg/mod/github.com/tensorflow/tensorflow@v2.3.1+incompatible/tensorflow/go/saved_model.go:25:2: module github.com/tensorflow/tensorflow@latest found (v2.3.1+incompatible), but does not contain package github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\r\n```\r\n\r\ncould tou help me?\r\n\r\n\r\nGo ENVS:\r\n```bash\r\nGO111MODULE=\"on\"\r\nGOARCH=\"amd64\"\r\nGOBIN=\"\"\r\nGOCACHE=\"/home/fedyfausto/.cache/go-build\"\r\nGOENV=\"/home/fedyfausto/.config/go/env\"\r\nGOEXE=\"\"\r\nGOFLAGS=\"\"\r\nGOHOSTARCH=\"amd64\"\r\nGOHOSTOS=\"linux\"\r\nGOINSECURE=\"\"\r\nGOMODCACHE=\"/home/fedyfausto/golang/gopath/pkg/mod\"\r\nGONOPROXY=\"\"\r\nGONOSUMDB=\"\"\r\nGOOS=\"linux\"\r\nGOPATH=\"/home/fedyfausto/golang/gopath\"\r\nGOPRIVATE=\"\"\r\nGOPROXY=\"https://proxy.golang.org,direct\"\r\nGOROOT=\"/home/fedyfausto/golang/latest/go\"\r\nGOSUMDB=\"sum.golang.org\"\r\nGOTMPDIR=\"\"\r\nGOTOOLDIR=\"/home/fedyfausto/golang/latest/go/pkg/tool/linux_amd64\"\r\nGCCGO=\"gccgo\"\r\nAR=\"ar\"\r\nCC=\"gcc\"\r\nCXX=\"g++\"\r\nCGO_ENABLED=\"1\"\r\nGOMOD=\"/home/fedyfausto/Lavoro/Project/go_predictor_2/project_name/go.mod\"\r\nCGO_CFLAGS=\"-g -O2\"\r\nCGO_CPPFLAGS=\"\"\r\nCGO_CXXFLAGS=\"-g -O2\"\r\nCGO_FFLAGS=\"-g -O2\"\r\nCGO_LDFLAGS=\"-g -O2\"\r\nPKG_CONFIG=\"pkg-config\"\r\nGOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build398631669=/tmp/go-build -gno-record-gcc-switches\"\r\n```\r\n", "comments": ["@fedyfausto \r\nPlease refer to this issue with same error and let us know :#41808 #34580 #39307", "The issue is same but seems does not be resolved yet", "i tryied to re-install all but seems some files miss (the file `for_core_protos_go_proto`)", "The C library seems work:\r\n```bash\r\n[fedyfausto@desktop-fisso-fedyfausto-fedora Scrivania]$ gcc hello_tf.c -ltensorflow -o hello_tf\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 851 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1959 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2329 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2517 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2732 (>= sh_info of 2)\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2733 (>= sh_info of 2)\r\n[fedyfausto@desktop-fisso-fedyfausto-fedora Scrivania]$ ./hello_tf \r\nHello from TensorFlow C library version 2.3.0\r\n```", "> The C library seems work:\r\n> \r\n> ```shell\r\n> [fedyfausto@desktop-fisso-fedyfausto-fedora Scrivania]$ gcc hello_tf.c -ltensorflow -o hello_tf\r\n> /usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 851 (>= sh_info of 2)\r\n> /usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 1959 (>= sh_info of 2)\r\n> /usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2329 (>= sh_info of 2)\r\n> /usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2517 (>= sh_info of 2)\r\n> /usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2732 (>= sh_info of 2)\r\n> /usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 2733 (>= sh_info of 2)\r\n> [fedyfausto@desktop-fisso-fedyfausto-fedora Scrivania]$ ./hello_tf \r\n> Hello from TensorFlow C library version 2.3.0\r\n> ```\r\n\r\nDo you mean the issue is resolved.", "@Saduf2019 I've met same problem, too.\r\n\r\nThe C library of v2.3.0 works well.\r\n\r\nGo version of tensorflow@v2.3.0 or v2.3.1 does not work in **go mod** environment. \r\n", "Same problem.\r\n\r\nOS ubuntu/amd64\r\nTensorflow v2.3.1\r\nGolang v1.15.1\r\n\r\nThis issue is **STILL NOT SOLVED**. All of us can't say that this trouble is fixed until [this guide](https://www.tensorflow.org/install/lang_go) will be done step by step without any errors or warnings.\r\n\r\n@Saduf2019 ", "@advaitjain @gunan @hawkinsp @qlzh727 anyone?", "@jhseu may be able to help, I do not think anyone else investigated go packages.", "fuck, i accidentally delete /usr/local cause i tried to solve this issue by the hands, lol :laughing: :laughing: :laughing:. Thanks nvidia and tensorflow developers to create simple deb packages that are \"just works\"", "Having the same issue as op", "Having the same issue, after go generate github.com/tensorflow/tensorflow/tensorflow/go/op\r\ncannot find package \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto\" in any of:\r\n\t${GOPATH}/src/github.com/tensorflow/tensorflow/tensorflow/go/vendor/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (vendor tree)\r\n\t/usr/local/Cellar/go/1.15/libexec/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)\r\n\t${GOPATH}/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)\r\nIn the core dir only two dirs:\r\ncore_protos_go_proto\r\nframework\r\n\r\n", "@fedyfausto @Saduf2019 @gowthamkpr  have you found any solutions? \r\n\r\nPerhaps, we need to create custom bindings for tensorflow, cuz usually google fixing their bugs to much time.", "After a lot of try and errors i finally found a little solution:\r\nI installed the last version of TensorFlow C library (2.30.) from BINARY. After that i installed the GO VERSION of tensorflow 2.1.0 and it seems work, so there is some bug in 2.3.0 of go version (anyway the go runs 2.3.0 c libray so it is fine).\r\n\r\nNow i'm facing to another problem, how could i compile a single solution within tensorflow library to distribuite the final solution?", "@fedyfausto i could cooperate with you to fix this problem (cuz i realy need working ml framework on go), so if yo want, text me on email or telegram, links in my profile.", "I will contact you! (Telegram maybe is the perfect solution @fedyfausto)\r\n\r\nDott. Federico Fausto Santoro\r\n\r\n\r\n\r\n\r\n\r\nIl giorno 20 ott 2020, alle ore 15:58, Richard Cooper <notifications@github.com> ha scritto:\r\n\r\n\ufeff\r\n\r\n@fedyfausto<https://github.com/fedyfausto> i could cooperate with you to fix this problem (cuz i realy need working ml framework on go), so if yo want, text me on email or telegram, links in my profile.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43847#issuecomment-712873033>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AA53P4BHIB3HNKGDHDY647DSLWJSDANCNFSM4SHQXQBA>.\r\n", "I solved this problem by changing the TensorFlow version to 1.13.\r\nDownload the corresponding version of TensorFlow, and then go generate $GOPATH/src/......\r\nAt the same time, you need to modify the tensorflow version of require in go.mod\r\n`require github.com/tensorflow/tensorflow v1.13.2`", "@kep-w all participants here are already decided that tensorflow 1.13 is works well, **but latest version is v2.3.1!!!** Also, 2.* versions are mentioned in official guide, so this is not a solution actually.", "@fedyfausto @ololosha228 @kep-w @shravanshetty1 \r\n\r\nI've placed a working Dockerfile for Go 1.15 and TensorFlow 2.3.1 at https://github.com/wamuir/golang-tf . I hope this helps. ", "@wamuir thanks a lot! i'll try to experiment with your image, and leave a feedback in your repo (or here, idk)", "@fedyfausto \r\nDo you still face the issue, could you please try with the latest tf version and update us.", "Yes this is still a problem. tensorflow/go can only be used with bazel, or by building a local version and putting it in vendor.\r\n\r\nFor anyone else struck here, @galeone is maintaining a fork with the propobuf files compiled: https://github.com/galeone/tensorflow (He also have a wrapper for making it even easier to use tensorflow in go: https://github.com/galeone/tfgo )", "@Saduf2019 i copy that, tried to rerun \"getting started\" guide to install and use tensorflow in go, and still lots a lot of problems\r\n\r\n\r\nTo be honest, i prefer to recommend any user who read this comment, use @galeone packages cause... Well, they work, at least...", "Here's a summary for anyone who wants this summarized:  **you cannot use `go get` to obtain a functional copy of `github.com/tensorflow/tensorflow/tensorflow/go`**.  This is because the package relies on compiled protocol buffers, but compiled protobufs cannot be checked into this git repo (see #44655).  Previously, it was fairly straightforward to obtain a working installation by cloning the Tensorflow repo into `${GOPATH}/src` and then using `go generate`  to compile the buffers, but recent Go module changes and the deprecation of GOPATH have all but made this approach impossible (despite what the [Install Guide](https://github.com/tensorflow/tensorflow/blob/3691bc286f4ac0aad8c1cf611b692a22e7eef22a/tensorflow/go/README.md) in this repo would have you believe!).\r\n\r\nThings that flat out won't work:\r\n1. Using `go get github.com/tensorflow/tensorflow/tensorflow/go`\r\n2. Following the [current installation guide in this repo](https://github.com/tensorflow/tensorflow/blob/3691bc286f4ac0aad8c1cf611b692a22e7eef22a/tensorflow/go/README.md), which needs an update\r\n\r\nYou can still use tensorflow/go.  A few solutions that may work for you:\r\n1. Clone the Tensorflow repo, generate the protocol buffers locally and then use go mod's replace directive: instructions are in SIG Build's [Golang Install Guide](https://github.com/tensorflow/build/blob/master/golang_install_guide/README.md) \r\n2. Switch to a fork that includes the generated protocol buffers, such as https://github.com/galeone/tensorflow from @galeone\r\n3. Use Docker images that have Go, the TF shared libs and tensorflow/go installed, such as https://github.com/wamuir/golang-tf\r\n4. Soon, hopefully, build with Bazel; a PR is at #50934", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43847\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43847\">No</a>\n"]}, {"number": 43846, "title": "Access to GPU memory usage", "body": "**System information**\r\n- TensorFlow version (you are using): TensorFlow 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like an easy way to access current/peak GPU memory usage. The TF profiler only allows detailed analysis of a single run, while accessing (and saving) the memory usage would allow users to track and compare the resource consumption of multiple different runs and models. Both PyTorch (`torch.cuda.max_memory_allocated()`) and TensorFlow 1 (`tf.contrib.memory_stats.MaxBytesInUse()`) allow easy access to this, only TensorFlow 2 doesn't.\r\n\r\n**Will this change the current api? How?**\r\nYes, it would add a method that returns current/peak GPU memory consumption.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone caring about memory consumption differences (which are probably a lot of people).\r\n\r\n**Any Other info.**\r\nThis has been discussed in the past, e.g. in https://github.com/tensorflow/serving/issues/1407 or https://stackoverflow.com/questions/59652889/how-to-get-the-exact-gpu-memory-usage-for-keras.\r\n", "comments": ["Hi @klicperajo ! Have you checked [get_memory_usage](https://www.tensorflow.org/api_docs/python/tf/config/experimental/get_memory_usage) feature  from TF 2.7 ?", "Indeed, this seems to have been added already in [TensorFlow 2.4](https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/config/experimental/get_memory_usage), and refined as [get_memory_info in TF 2.5](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/config/experimental/get_memory_info)!\r\n\r\nThank you for pointing me to this!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 43845, "title": "\"No gradients provided\" with GradientDescentOptimizer if not using float variable", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): pip\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version:  10.1\r\n- GPU model and memory: Nvidia Geforce 940MX\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe gradient descent optimizer gives \"No gradients provided\" error when minimizing it, if the variables used in the custom loss function are not explicitly defined as float\r\n\r\n**Describe the expected behavior**\r\nIt should optimize the variable as expected.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training import gradient_descent\r\nx = tf.Variable(np.array([[1,1],[2,2]]), trainable=True)\r\ndef my_loss():\r\n    square = tf.math.square(x)\r\n    return square\r\nopt = gradient_descent.GradientDescentOptimizer(0.1)\r\nfor i in range(20):\r\n    print([x.numpy(), my_loss().numpy()])\r\n    train = opt.minimize(my_loss, var_list=[x])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-165-a3994377e283> in <module>\r\n      1 for i in range(20):\r\n      2     print([x.numpy(), y.numpy(), my_loss().numpy()])\r\n----> 3     train = opt.minimize(my_loss, var_list=[x,y])\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    408           \"No gradients provided for any variable, check your graph for ops\"\r\n    409           \" that do not support gradients, between variables %s and loss %s.\" %\r\n--> 410           ([str(v) for _, v in grads_and_vars], loss))\r\n    411 \r\n    412     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n\r\nValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(2, 2) dtype=int64, numpy=\\narray([[1, 1],\\n       [2, 2]])>\", \"<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\\narray([[9.000003, 9.000003],\\n       [9.000003, 9.000003]], dtype=float32)>\"] and loss <function my_loss at 0x7f5f1c381680>.\r\n```", "comments": ["Can you share a very minimal but complete example that we could copy, past and run to reproduce this?", "@bhack I've shared the reproducible code in the issue ", "`NameError: name 'y' is not defined`", "@bhack Sorry, my bad! I've updated the code. Thank You for looking into it!", "I think the warning is more clear on tf-nightly:\r\n```\r\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\r\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\r\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\r\n````", "@bhack Thanks, that explains it i guess. One more thing, the watched tensor is `x` but what are the target and source tensors here?", "They are the source and target of the gradient function\r\nhttps://github.com/tensorflow/tensorflow/blob/917882bfa26c5f19275346e6c4b0a9fb845f6300/tensorflow/python/eager/backprop.py#L993-L1013\r\nThat is called in your minimize step:\r\nhttps://github.com/tensorflow/tensorflow/blob/917882bfa26c5f19275346e6c4b0a9fb845f6300/tensorflow/python/training/optimizer.py#L475", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@bhack Thanks for the help, man! I will be closing the issue as it's resolved now."]}, {"number": 43844, "title": "fix TFOpLambda name for KerasOpDisparcher", "body": "Try to fix https://github.com/tensorflow/tensorflow/issues/43840", "comments": ["@scd75 The test are failing. I see that the name was specifically removed with this claim https://github.com/tensorflow/tensorflow/blob/f718b7fd197836e335a69844dfec4e2c2ef2a615/tensorflow/python/keras/layers/core.py#L1340-L1344 /cc @tomerk \r\n", "Well I guess that is a big change triggering many issues. Does that mean that the \"name\" parameter in all tf-op layers constructors become useless??\r\nI guess the solution to the original issue would rather be to fix the way tf-ops are automatically named (to make sure they are uniquely identified), rather than totally getting rid of the \"name\" attribute which is very useful in many cases (and which I guess is widely used)", "I am trying to compile this locally to run tests. But as always it is a quite heavy a long process without a [fresh cache](https://github.com/tensorflow/build/issues/5)", "I've replied on issue #43840 and you can find more info about the functional API refactoring in the release notes:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\r\n\r\nThe names of individual ops in the graph generated by a Keras model should not be considered load-bearing, and we have never made guarantees about them. If we did it would prevent us from making any change to Keras layers that alters the actual ops in the TF graph that we generate. That would prevent us from making all sorts of performance improvements, bug fixes, or code cleanups.\r\n\r\n*If* your use case is that you need to identify the generated TFOpLambda layer by its name and the auto-incrementing names aren't sufficient for you then we can explore alternative ways of allowing users to explicitly override the name of the auto-generated layer. But we'd need to understand why you treat the op names generated by tf op layers in a load-bearing way to see what changes/features would even make sense.", "@tomerk What it will be the role of the `name` param in the ticket?\r\n`tf.identity(test, name='abcd')`", "In the linked issue, because `test` is a keras input (which mimics a tensor) rather than actual tf.Tensor, it will create an op layer in the functional model that calls `tf.identity`.\r\n\r\nGenerally when called on standard tensors, `tf.identity` uses the name argument to set the names of any ops in the graphs it creates.\r\nBut, when we create op layers we have to drop the name argument because if the op layer is reused *and* it passes the same `name` arg to `tf.identity` each time it tends to create multiple ops in the same graph that have the exact same name. This is unsupported and causes an error to be raised.\r\n\r\nSo, in the specific `tf.identity(test, name='abcd')` call from the ticket `name` will remain completely unused.\r\n\r\nThe more important question here is *why* is name being set to `abcd`, and what is the user (in this case @scd75 ) hoping to get by having the `name` arg reflected in the ops generated by the Keras model? E.g. is it a slightly more interpretable tensorboard graph? Is there code that is trying to directly manipulating nodes in the graph that ends up treating names as load-bearing? etc.", "E.g, there are things we could to that set `name` as the name of the layer we generate (even though we would still drop `name` before calling the actual tf.identity when running the layer). This would help if the user just cares about identifying the layer w/o using the auto-incremented names, but it wouldn't help if the user is looking to maintain exact compatibility for pre-existing code that treats graph node names in a load-bearing way.", "@tomerk I understand the logic but by an API usability point of view it is not so good to have a parameter that it is more or less silently ignored or at least we could raise a Warning.  \r\nI don't think that the proposed `abcd/Identity:0` it is more consistent but could give a role to the name param.", "Thanks for the reply and detailed explanation on #43840 @tomerk .\r\nTo clarify my use case: I treat some input features differently in my model. Each of those (named) features in the input tf dataset is mapped to a different keras Input layer which is named after the feature name.\r\nI then perform several preprocessing operations on those input layers (which differ depending on the feature), and I end up naming the resulting tensor through the use of a tf.identity layer, which I name \"preprocessed_{feature_name}\".\r\n\r\nin the rest of the model, I am then able to apply different subsequent operations for each preprocessed feature, depending on their name, by relying on the \"name\" attribute of the preprocessed input tensors.\r\nBut maybe there is a better way to do it.. I just found it easy and quite clean to carry over the feature name all along the process, from the very first Input layer, to the entry of the rest of the model.", "@tomerk Can you please take a look on above comments from @bhack, @scd75. Thanks!", "Thanks all for your thoughts! Response to the comments above:\r\n\r\n> by an API usability point of view it is not so good to have a parameter that it is more or less silently ignored or at least we could raise a Warning.\r\n\r\n\r\nYeah, I definitely agree with this in principle.\r\nBut, the key point here is the `tf.Identity` api on actual tensor values *already does* behave that way (hence why it has the `name` argument), while Keras's op layer conversion is a *best effort* attempt to emulate the behavior of the tf apis it traces (but w/o making any guarantees about naming because the simple act of packaging things into layers will change the name scoping / alter the tensor naming semantics).\r\n\r\nWhen we ran various experiments across all of our internal google users of Keras to see what would pose the lowest risk to users, we found that it was safer to blanket-disable the `name` argument than to always leave the `name` argument in.\r\nThe few cases where the name change broke code were generally just unit tests that checked for an exact output name to make sure a layer was used, which could easily be fixed to expect a different name or to check the model layers directly.\r\n\r\nOn the other hand there were a number of cases that started running into breakages if we left `name` in because some tf apis directly pass a name to an op that eventually gets converted to a layer, which then breaks as soon as you used that api multiple times in one model. (One example is some of the various tensor binary operators if I recall correctly)\r\n\r\n-------------\r\nI will say that it's possible we could identify which exact apis it's safe to leave `name` for and which it's not, and then it could behave slightly more like before. We don't have the bandwidth to safely do this analysis though, it would add complexity to the code, and because things get auto-wrapped in layers it might not even match your naming expectations anyway (and naming isn't part of the Keras op layer api spec so it's prone to change anyway)\r\n\r\n---------------\r\n@scd75 it sounds like you fall into the case of needing to know what layers were applied? The way we handled this in the internal cases I mentioned above is by building a temporary model w/ the tensor you're checking as your model output, and then checking `model.layers` for the presence of of specific layers (by type or by name).\r\n"]}, {"number": 43843, "title": "Openpose installation issue ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- \r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?:No\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: [10.18.10.4358]\r\n![Anaconda error](https://user-images.githubusercontent.com/71690984/95325961-80f13800-08bf-11eb-9b28-ee40e005bb03.png)\r\n![anaconda](https://user-images.githubusercontent.com/71690984/95325968-82226500-08bf-11eb-8ca8-21f5b1377ec1.png)\r\n\r\n\r\n\r\n\r\n\r\n**Issue**\r\n\r\n**I constantly encounter issues while installing openpose on windows using Anaconda using this link: https://www.youtube.com/watch?v=4FZrE3cmTPA**\r\n\r\n\r\n****\r\nIn Anaconda, when running this command; python run_webcam.py camera --video.mp4,\r\nerror message comes. Attaching error message.\r\n![Anaconda error](https://user-images.githubusercontent.com/71690984/95326026-98302580-08bf-11eb-85f4-760faf7c2260.png)\r\n![anaconda](https://user-images.githubusercontent.com/71690984/95326030-99f9e900-08bf-11eb-99b7-92fbb013d339.png)\r\n\r\n\r\n", "comments": ["@arulprakash123 \r\nPlease provide complete code for us to replicate the issue faced or if possible share a colab gist with error reported.\r\n\r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43843\">No</a>\n"]}, {"number": 43842, "title": "OP_REQUIRES failed at sparse_tensor_dense_matmul_op.cc:146 : Invalid argument: k (784) from index[4352,1] out of bounds (>=784)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [Referred code repository](https://github.com/WojciechMormul/deep-compression) \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Installed using conda \r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6.12\r\n- CUDA/cuDNN version: 11.1 \r\n- GPU model and memory:\r\n\r\n**Conda list of packages installed**\r\n`# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\n_tflow_select             2.3.0                       mkl  \r\nabsl-py                   0.10.0           py36h9f0ad1d_0    conda-forge\r\nargon2-cffi               20.1.0           py36h7b6447c_1    anaconda\r\nastor                     0.8.1              pyh9f0ad1d_0    conda-forge\r\nasync_generator           1.10             py36h28b3542_0    anaconda\r\nattrs                     20.2.0                     py_0    anaconda\r\nbackcall                  0.2.0                      py_0    anaconda\r\nbleach                    3.2.1                      py_0    anaconda\r\nc-ares                    1.16.1               h516909a_3    conda-forge\r\nca-certificates           2020.7.22                     0    anaconda\r\ncertifi                   2020.6.20                py36_0    anaconda\r\ncffi                      1.14.3           py36he30daa8_0    anaconda\r\ndbus                      1.13.12              h746ee38_0    anaconda\r\ndecorator                 4.4.2                      py_0    anaconda\r\ndefusedxml                0.6.0                      py_0    anaconda\r\nentrypoints               0.3                      py36_0    anaconda\r\nexpat                     2.2.9                he6710b0_2    anaconda\r\nfontconfig                2.13.0               h9420a91_0    anaconda\r\nfreetype                  2.10.2               h5ab3b9f_0    anaconda\r\ngast                      0.2.2                      py_0    conda-forge\r\nglib                      2.56.2               hd408876_0    anaconda\r\ngoogle-pasta              0.2.0              pyh8c360ce_0    conda-forge\r\ngrpcio                    1.31.0           py36h769ab6c_0    conda-forge\r\ngst-plugins-base          1.14.0               hbbd80ab_1    anaconda\r\ngstreamer                 1.14.0               hb453b48_1    anaconda\r\nh5py                      2.10.0          nompi_py36hecadee3_104    conda-forge\r\nhdf5                      1.10.6          nompi_h3c11f04_101    conda-forge\r\nicu                       58.2                 he6710b0_3    anaconda\r\nimportlib-metadata        2.0.0            py36h9f0ad1d_0    conda-forge\r\nipykernel                 5.3.4            py36h5ca1d4c_0    anaconda\r\nipython                   7.16.1           py36h5ca1d4c_0    anaconda\r\nipython_genutils          0.2.0                    py36_0    anaconda\r\nipywidgets                7.5.1                      py_0    anaconda\r\njedi                      0.17.2                   py36_0    anaconda\r\njinja2                    2.11.2                     py_0    anaconda\r\njpeg                      9b                   habf39ab_1    anaconda\r\njsonschema                3.0.2                    py36_0    anaconda\r\njupyter                   1.0.0                    py36_7    anaconda\r\njupyter_client            6.1.7                      py_0    anaconda\r\njupyter_console           6.2.0                      py_0    anaconda\r\njupyter_core              4.6.3                    py36_0    anaconda\r\njupyterlab_pygments       0.1.1                      py_0    anaconda\r\nkeras-applications        1.0.8                      py_1    conda-forge\r\nkeras-preprocessing       1.1.0                      py_0    conda-forge\r\nld_impl_linux-64          2.33.1               h53a641e_7  \r\nlibblas                   3.8.0               17_openblas    conda-forge\r\nlibcblas                  3.8.0               17_openblas    conda-forge\r\nlibedit                   3.1.20191231         h14c3975_1  \r\nlibffi                    3.3                  he6710b0_2  \r\nlibgcc-ng                 9.1.0                hdf63c60_0  \r\nlibgfortran-ng            7.5.0               hdf63c60_16    conda-forge\r\nliblapack                 3.8.0               17_openblas    conda-forge\r\nlibopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge\r\nlibpng                    1.6.37               hbc83047_0    anaconda\r\nlibprotobuf               3.13.0               h8b12597_0    conda-forge\r\nlibsodium                 1.0.18               h7b6447c_0    anaconda\r\nlibstdcxx-ng              9.1.0                hdf63c60_0  \r\nlibuuid                   1.0.3                h1bed415_2    anaconda\r\nlibxcb                    1.14                 h7b6447c_0    anaconda\r\nlibxml2                   2.9.10               he19cac6_1    anaconda\r\nmarkdown                  3.2.2                      py_0    conda-forge\r\nmarkupsafe                1.1.1            py36h7b6447c_0    anaconda\r\nmistune                   0.8.4            py36h7b6447c_0    anaconda\r\nnbclient                  0.5.0                      py_0    anaconda\r\nnbconvert                 6.0.6                    py36_0    anaconda\r\nnbformat                  5.0.7                      py_0    anaconda\r\nncurses                   6.2                  he6710b0_1  \r\nnest-asyncio              1.4.0                      py_1    anaconda\r\nnotebook                  6.1.1                    py36_0    anaconda\r\nnumpy                     1.19.1           py36h3849536_2    conda-forge\r\nopenssl                   1.1.1h               h7b6447c_0    anaconda\r\nopt_einsum                3.3.0                      py_0    conda-forge\r\npackaging                 20.4                       py_0    anaconda\r\npandoc                    2.10.1                        0    anaconda\r\npandocfilters             1.4.2                    py36_1    anaconda\r\nparso                     0.7.0                      py_0    anaconda\r\npcre                      8.44                 he6710b0_0    anaconda\r\npexpect                   4.8.0                    py36_0    anaconda\r\npickleshare               0.7.5                    py36_0    anaconda\r\npip                       20.2.2                   py36_0  \r\nprometheus_client         0.8.0                      py_0    anaconda\r\nprompt-toolkit            3.0.7                      py_0    anaconda\r\nprompt_toolkit            3.0.7                         0    anaconda\r\nprotobuf                  3.13.0           py36h831f99a_0    conda-forge\r\nptyprocess                0.6.0                    py36_0    anaconda\r\npycparser                 2.20                       py_2    anaconda\r\npygments                  2.7.1                      py_0    anaconda\r\npyparsing                 2.4.7                      py_0    anaconda\r\npyqt                      5.9.2            py36h22d08a2_1    anaconda\r\npyrsistent                0.17.3           py36h7b6447c_0    anaconda\r\npython                    3.6.12               hcff3b4d_2  \r\npython-dateutil           2.8.1                      py_0    anaconda\r\npython_abi                3.6                     1_cp36m    conda-forge\r\npyzmq                     19.0.2           py36he6710b0_1    anaconda\r\nqt                        5.9.7                h5867ecd_1    anaconda\r\nqtconsole                 4.7.7                      py_0    anaconda\r\nqtpy                      1.9.0                      py_0    anaconda\r\nreadline                  8.0                  h7b6447c_0  \r\nscipy                     1.5.2            py36h3a855aa_0    conda-forge\r\nsend2trash                1.5.0                    py36_0    anaconda\r\nsetuptools                49.6.0                   py36_1  \r\nsip                       4.19.24          py36he6710b0_0    anaconda\r\nsix                       1.15.0             pyh9f0ad1d_0    conda-forge\r\nsqlite                    3.33.0               h62c20be_0  \r\ntensorboard               1.15.0                   py36_0    conda-forge\r\ntensorflow                1.15.0          mkl_py36h4920b83_0  \r\ntensorflow-base           1.15.0          mkl_py36he1670d9_0  \r\ntensorflow-estimator      1.15.1             pyh2649769_0  \r\ntermcolor                 1.1.0                      py_2    conda-forge\r\nterminado                 0.8.3                    py36_0    anaconda\r\ntestpath                  0.4.4                      py_0    anaconda\r\ntk                        8.6.10               hbc83047_0  \r\ntornado                   6.0.4            py36h7b6447c_1    anaconda\r\ntraitlets                 4.3.3                    py36_0    anaconda\r\nwcwidth                   0.2.5                      py_0    anaconda\r\nwebencodings              0.5.1                    py36_1    anaconda\r\nwerkzeug                  0.16.1                     py_0    conda-forge\r\nwheel                     0.35.1                     py_0  \r\nwidgetsnbextension        3.5.1                    py36_0    anaconda\r\nwrapt                     1.12.1           py36h8c4c3a4_1    conda-forge\r\nxz                        5.2.5                h7b6447c_0  \r\nzeromq                    4.3.2                he6710b0_3    anaconda\r\nzipp                      3.2.0                      py_0    conda-forge\r\nzlib                      1.2.11               h7b6447c_3`\r\n\r\n**Describe the current behavior**\r\nI am working on neural network pruning and the problem occurs when finding accuracy from the pruned weights. \r\n\r\nThe code fails when it runs the `batch_acc = sess.run(accuracy,feed_dict={x_PH: batch_x, labels: batch_y})` in the deploy code. \r\n\r\nThe training goes fine on the MNIST dataset. As I use deploy to find the accuracy from the pruned weights, I get an error on the multiplication between Sparse Tensor and Tensor. \r\n\r\nThere are already issues on this. Found a stackoverflow thread on the same. \r\nhttps://stackoverflow.com/questions/34030140/is-sparse-tensor-multiplication-implemented-in-tensorflow\r\n\r\nThe error occurs at `tf.sparse.sparse_dense_matmul(w,x)` line.\r\n\r\n`\tdef forward_matmul(self, x):\r\n\t\t\r\n\t\tif self.dense == False:\r\n\t\t\tw = tf.sparse.transpose(self.w_matrix, (1, 0))\r\n\t\t\tx = tf.transpose(x, (1, 0))\r\n\t\t\tx = tf.sparse.sparse_dense_matmul(w, x) # only left matrix can be sparse hence transpositions ### Changing tf.sparse.matmul() to tf.sparse.sparse_dense_matmul()\r\n\t\t\tx = tf.transpose(x, (1, 0))\r\n\t\telse:\r\n\t\t\tx = tf.matmul(x, self.w_matrix)\r\n\t\t\r\n\t\treturn x`\r\n\r\n**Describe the expected behavior**\r\nThe code should be able to predict the accuracy from the newly trained weights \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nPlease use the modified repository, which was initially failing because of the type conversion in the default code.\r\nhttps://github.com/sachinkm308/deep-compression_v3/tree/v1.0 \r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n`(mlp8) mohan:~/mlp/git/deep-compression_v3$ python deploy.py \r\nWARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\nlayer: conv1\r\n\tvalid matrix weights: 800\r\n\ttotal tensor weights: 800\r\n\ttotal matrix weights: 4917248\r\nTensor(\"Reshape:0\", shape=(?, 784), dtype=float32)\r\nx -  Tensor(\"Shape:0\", shape=(2,), dtype=int32)\r\nTensor(\"transpose_1:0\", shape=(784, ?), dtype=float32)\r\nlayer: conv2\r\n\tvalid matrix weights: 51200\r\n\ttotal tensor weights: 51200\r\n\ttotal matrix weights: 19668992\r\nx -  Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\r\nTensor(\"transpose_5:0\", shape=(6272, ?), dtype=float32)\r\nlayer: fc1\r\n\tvalid matrix weights: 3211264.0\r\n\ttotal matrix weights: 3211264\r\nWARNING:tensorflow:From /home/mohan/mlp/git/deep-compression_v3/layers.py:223: The name tf.sparse.matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\r\n\r\nlayer: fc2\r\n\tvalid matrix weights: 10240.0\r\n\ttotal matrix weights: 10240\r\nWARNING:tensorflow:From deploy.py:50: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n2020-10-07 12:58:33.749107: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-07 12:58:33.770019: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3999980000 Hz\r\n2020-10-07 12:58:33.770318: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55891ca32840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-07 12:58:33.770345: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nOMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-7\r\nOMP: Info #156: KMP_AFFINITY: 8 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)\r\nOMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 \r\nOMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 \r\nOMP: Info #250: KMP_AFFINITY: pid 2931 tid 2931 thread 0 bound to OS proc set 0\r\n2020-10-07 12:58:33.779852: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nOMP: Info #250: KMP_AFFINITY: pid 2931 tid 3127 thread 1 bound to OS proc set 1\r\nOMP: Info #250: KMP_AFFINITY: pid 2931 tid 3147 thread 2 bound to OS proc set 2\r\nOMP: Info #250: KMP_AFFINITY: pid 2931 tid 3148 thread 3 bound to OS proc set 3\r\nOMP: Info #250: KMP_AFFINITY: pid 2931 tid 3149 thread 4 bound to OS proc set 4\r\n2020-10-07 12:58:39.482282: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at sparse_tensor_dense_matmul_op.cc:146 : Invalid argument: k (784) from index[4352,1] out of bounds (>=784)\r\nTraceback (most recent call last):\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: k (784) from index[4352,1] out of bounds (>=784)\r\n\t [[{{node SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"deploy.py\", line 62, in <module>\r\n    batch_acc = sess.run(accuracy,feed_dict={x_PH: batch_x, labels: batch_y})\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: k (784) from index[4352,1] out of bounds (>=784)\r\n\t [[node SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n\r\nOriginal stack trace for 'SparseTensorDenseMatMul/SparseTensorDenseMatMul':\r\n  File \"deploy.py\", line 25, in <module>\r\n    x = tf.nn.relu(L1.forward_matmul(x))\r\n  File \"/home/mohan/mlp/git/deep-compression_v3/layers.py\", line 353, in forward_matmul\r\n    x = tf.sparse.sparse_dense_matmul(w, x, adjoint_a=False, adjoint_b=False, name=None) # only left matrix can be sparse hence transpositions ### Changing tf.sparse.matmul() to tf.sparse.sparse_dense_matmul()\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/ops/sparse_ops.py\", line 2405, in sparse_tensor_dense_matmul\r\n    adjoint_b=adjoint_b)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_sparse_ops.py\", line 3063, in sparse_tensor_dense_mat_mul\r\n    adjoint_b=adjoint_b, name=name)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/mohan/anaconda2/envs/mlp8/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()`", "comments": ["@sachinkm308 \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "I have created a tag for this repository which is failing.\r\nhttps://github.com/sachinkm308/deep-compression_v3/tree/v1.0\r\n\r\nPlease let me know if you need anything else. I am running the code currently in pycharm and conda environment. I am not sure how to convert it into a google colab file.\r\n\r\nAt the moment, when I train using `train.py` - 2 folders are created on my local directory. One for histograms and one for weights which are pruned. This pruned weights folder is then loaded into `deploy.py`, which is where it is failing as described above. ", "I have tried in colab with TF version 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/161080835014dcdfb3197ab8a7a30398/untitled436.ipynb).Thanks!", "Hey Team, \r\n\r\nI fixed this issue. \r\n\r\n\t`def tensor_to_matrix(self, tensor, prune_mask, H_in, W_in, stride):\r\n\t\t\t\t\r\n\t\t# assume padding type 'SAME' and padding value 0 \r\n\r\n\t\tH_out = int(int(H_in +1)/stride) # padding 'SAME'\r\n\t\tW_out = int(int(W_in +1)/stride) # padding 'SAME'\r\n\t\tH_in = int(H_in)\r\n\t\tW_in = int(W_in)\r\n\t\t\t\t\t\r\n\t\tkH, kW, D_in, D_out = tensor.shape \r\n\t\t\r\n\t\tself.D_out = D_out\r\n\t\tself.H_out = H_out\r\n\t\tself.W_out = W_out\r\n \r\n\t\tif self.dense == False:\r\n\t\t\tindices, values, dense_shape = [], [], [H_in * W_in * D_in, H_out* W_out * D_out] # sparse matrix\r\n\t\telse:\r\n\t\t\tmatrix = np.zeros((H_in * W_in * D_in, H_out* W_out * D_out), dtype=np.float32) # dense matrix\t\t\r\n\t\t\r\n\t\tfor d_in in range(D_in):\r\n\t\t\tfor d_out in range(D_out):\r\n\t\t\r\n\t\t\t\t# tf.nn.conv2d implementation doesn't go from top-left spatial location but from bottom-right\r\n\t\t\t\tfor i_in_center in np.arange(H_in-1, -1, -stride): # kernel input center for first axis\r\n\t\t\t\t\tfor j_in_center in np.arange(W_in-1, -1, -stride): # kernel input center for second axis\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\ti_out = int(i_in_center / stride)\r\n\t\t\t\t\t\tj_out = int(j_in_center / stride)\r\n\r\n\t\t\t\t\t\tfor i in range(kH):\r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\ti_in = int(i_in_center + i - kH/2)  #Changed to int as the indices were generating float values\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\tif i_in < 0 or i_in >= H_in: # padding value 0 \r\n\t\t\t\t\t\t\t\tcontinue \r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\tfor j in range(kW):\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\tj_in = int(j_in_center + j -kW/2) #Changed to int as the indices were generating float values\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\tif j_in < 0 or j_in >= W_in: # padding value 0 \r\n\t\t\t\t\t\t\t\t\tcontinue\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\t# pruning mask: ones - valid weights, zero - pruned weights\r\n\t\t\t\t\t\t\t\tif prune_mask[i][j][d_in][d_out] == 0.0:\r\n\t\t\t\t\t\t\t\t\tcontinue\t\t\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\tpos_in = self.get_linear_pos(i_in, j_in, W_in) + d_in * H_in * W_in\r\n\t\t\t\t\t\t\t\tpos_out = self.get_linear_pos(i_out, j_out, W_out) + d_out * H_out * W_out\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t\tif self.dense == False:\r\n\t\t\t\t\t\t\t\t\tindices.append([pos_in, pos_out])\r\n\t\t\t\t\t\t\t\t\tvalues.append(tensor[i][j][d_in][d_out])\r\n\t\t\t\t\t\t\t\telse:\r\n\t\t\t\t\t\t\t\t\tmatrix[pos_in][pos_out] = tensor[i][j][d_in][d_out]\r\n\r\n\t\tif self.dense == False:\r\n\t\t\treturn indices, values, dense_shape  #Removing np.round as they don't need to be rounded anymore as i_in and j_in are changed to int\r\n\t\telse:\r\n\t\t\treturn matrix`\r\n\r\nPreviously while returning `indices` in the snippet above, I changed to `np.round(indices)` because the default indices pair was returning me a list of float values. But **indices** and **dense_shape** of a SparseTensor is always int and only **values** can be float. ([Reference](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor))\r\n\r\nSo in this code snippet, I type casted `i_in` and  `j_in` values to int and changed `np.round(indices)` to `indices` to ensure that the indices tensor returned int values without any external manipulation .\r\n\r\nThanks for all the help! Atleast I learned to work on google colab in a better way and to solve this bug ;)  \r\n\r\nYou may please close this issue by adding any additional comments if you have from your side. ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43842\">No</a>\n"]}, {"number": 43841, "title": "Unique name in TF ops wrapping layer", "body": "Try to fix https://github.com/tensorflow/tensorflow/issues/43824", "comments": ["@tomerk  Do you plan to serialize/save and recover on load `.backend.PER_GRAPH_OBJECT_NAME_UIDS`?\r\n\r\nI don't know if it has some side effect but it seems to work:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import keras as pykeras\r\nimport weakref\r\nimport copy\r\n\r\ninitial_builing = True\r\nmodel_loading = True\r\n\r\nif initial_builing:\r\n\r\n    layer1 = tf.keras.Input((1,),)\r\n    layer2 = tf.keras.layers.Dense(1)\r\n\r\n    model_output = layer2(layer1)[:,:-1]\r\n\r\n    model = tf.keras.Model(layer1, model_output)\r\n    model.summary()\r\n    model.save('testmodel')\r\n    cache_obj_name_uids = copy.copy(pykeras.backend.PER_GRAPH_OBJECT_NAME_UIDS)\r\n    tf.keras.backend.reset_uids()\r\nif model_loading:\r\n    model = tf.keras.models.load_model('testmodel')\r\n    pykeras.backend.PER_GRAPH_OBJECT_NAME_UIDS=  cache_obj_name_uids\r\n    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])\r\n    model.summary()\r\n```", "No I've tested with `tf.keras.backend.clear_session()` and it doesn't solve the issue cause it will change the default graph `ops.get_default_graph()`.", "it is something like:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import keras as pykeras\r\nimport weakref\r\nfrom tensorflow.python.framework import ops\r\nimport copy\r\nimport gc\r\n\r\ninitial_builing = True\r\nmodel_loading = True\r\ncache_obj_name_uids = None\r\nif initial_builing:\r\n\r\n    layer1 = tf.keras.Input((1,),)\r\n    layer2 = tf.keras.layers.Dense(1)\r\n\r\n    model_output = layer2(layer1)[:,:-1]\r\n\r\n    model = tf.keras.Model(layer1, model_output)\r\n    model.summary()\r\n    model.save('testmodel')\r\n    cache_obj_name_uids = copy.copy(pykeras.backend.PER_GRAPH_OBJECT_NAME_UIDS[ops.get_default_graph()])\r\n    del model, model_output\r\n    tf.keras.backend.clear_session()\r\n    gc.collect()\r\nif model_loading:\r\n    model = tf.keras.models.load_model('testmodel')\r\n    pykeras.backend.PER_GRAPH_OBJECT_NAME_UIDS[ops.get_default_graph()]=  cache_obj_name_uids\r\n    model = tf.keras.Model(model.inputs, model.layers[-1].output[:,:-1])\r\n    model.summary()\r\n```"]}, {"number": 43840, "title": "regression in layer names for tf-operation layers in current tf-nightly (2.4) ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0-dev20201006\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nIn the last tf-nightly release, when attributing a name to a layer created by a tf operation, this naming does not seem anymore to be effective anymore. \r\n\r\n**Describe the expected behavior**\r\nthe naming of the layer shall work as expected, as for tf 2.3\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ntest = tf.keras.Input((1,), name='input1')\r\ntest2 = tf.identity(test, name='abcd')\r\nprint(test.name)\r\nprint(test2.name)\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n2.4.0-dev20201005\r\ninput1\r\ntf.identity/Identity:0\r\n```\r\n", "comments": ["I've tried to create https://github.com/tensorflow/tensorflow/pull/43844 that names `abcd/Identity:0`.", "Thanks, is there a reason why you don't want to keep the current behavior from tf 2.3?", "It is just that I don't know if there was any specific motivation for what we have after `/`. \nSo.. just waiting for feedbacks.", "Hi @scd75 , we actually don't consider this a regression because we never made API guarantees about the exact ops or op names that would be generated for keras models. (and that applies to tf-op layers as well). I would be surprised if any TF code in 2.4/the nightlies relies on these in a load-bearing way, and I would be interested to know what use case you have that treats these names as load-bearing.\r\n\r\nThe release notes actually mention this change:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\r\n\r\n```\r\n* A major refactoring of the internals of the Keras Functional API may affect code that is relying on certain internal details:\r\n...\r\n  * Code that relies on the exact number and names of the op layers that TensorFlow operations were converted into. These may have changed.\r\n```\r\n\r\n```\r\nMajor Features and Improvements\r\n---------------\r\n....\r\n* A major refactoring of the internals of the Keras Functional API has been completed, that should improve the reliability, stability, and performance of constructing Functional models.\r\n```\r\n\r\n```\r\n* Improvements from the functional API refactoring:\r\n  *Functional model construction does not need to maintain a global workspace graph, removing memory leaks especially when building many models or very large models.\r\n  * Functional model construction should be ~8-10% faster on average.\r\n  * Functional models can now contain non-symbolic values in their call inputs inside of the first positional argument.\r\n  * Several classes of TF ops that were not reliably converted to Keras layers during functional API construction should now work, e.g. tf.image.ssim_multiscale\r\n  * Error messages when Functional API construction goes wrong (and when ops cannot be converted to Keras layers automatically) should be clearer and easier to understand.\r\n```\r\n\r\nWe could not complete this refactoring while guaranteeing that the op names produced by layers generated from TF API calls would stay the same (otherwise we would have done so).\r\n\r\nI also see you reported a separate issue #43824. Although I can't repro it in the colabs I would actually expect this to be a non-issue in the nightlies, because the Keras tfoplambda layers after the refactoring don't set the generated op name as their own layer name.\r\n\r\nedit: Actually I was just given a working repro of the issue in #43824 , it looks like there's something weird & unrelated going on w/ the name-gen so I can look into that.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43840\">No</a>\n"]}, {"number": 43837, "title": "[coreml delegate] fix usage of coremltools header", "body": "1. including \"external/....\" will cause problems when using tensorflow\r\n   as a third party / external package\r\n2. there are some redundant includes", "comments": ["Thanks for the fix! deletions looks good, but renames require internal toolchain change. Let me fix that part first, and please update this PR after that."]}, {"number": 43836, "title": "Use enueration for policy items", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43836) for more info**.\n\n<!-- need_sender_cla -->", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43836) for more info**.\n\n<!-- need_sender_cla -->", "@porridge it seems you have changed your email address on GitHub/commits? CLA bot says you have to sign CLA again.", "@porridge  Any update on this PR? Please. Thanks!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43836) for more info**.\n\n<!-- ok -->"]}, {"number": 43835, "title": "tf_record issue while following tutorial ", "body": "ValueError: Shape must be rank 0 but is rank 1 for '{{node DecodeJpeg}} = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](ParseSingleExample/ParseExample/ParseExampleV2)' with input shapes: [?].\r\n\r\n\r\nusing this tutorial: https://keras.io/examples/keras_recipes/tfrecord/", "comments": ["@abhishekbalu,\r\nOn running the Colab Notebook, I am facing a different error due to the invalid `GCS_PATH`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/82868d75e0f6e51d380ec48bc5242e7b/43835.ipynb#scrollTo=YLvGnlUzw8gM). \r\n\r\nCould you please provide the Python script/notebook you are running along with the dataset, so that we can reproduce the issue on our end. Thanks!", "the following is my [gist](https://colab.research.google.com/drive/1RIwxIM0lroyhUmGe_HWeqFq0dq7ZS_cn?usp=sharing). The link to [DATASET](https://drive.google.com/file/d/1gBSp2mhX4p6jlAoxBR55pGX0zeb4dg0q/view?usp=sharing). I am using the [nuscene](https://nuscenes.org/) DATASET\r\n\r\n\r\n", "Hi any updates on this i am kind of stuck here for a week. Please help me out. ", "@abhishekbalu,\r\nSorry for the delayed response. I do not have access to the drive link, could you please grant the required permissions to view the files. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@amahendrakar  sorry for the delay. [dataset](https://drive.google.com/file/d/1gBSp2mhX4p6jlAoxBR55pGX0zeb4dg0q/view?usp=sharing) and [gist](https://colab.research.google.com/drive/1RIwxIM0lroyhUmGe_HWeqFq0dq7ZS_cn?usp=sharing)  these are sharable link", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a43bdce0d63f4d7c0050fc82f8388a30/43835.ipynb). Thanks!", "@abhishekbalu,\r\nPlease check [this comment](https://stackoverflow.com/a/47707733) from a similar StackOverflow query and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@abhishekbalu,\r\nCan you please respond to the @amahendrakar's comment. Thanks! ", "I am passing a tensor and not a scalar but still getting this error ", "@abhishekbalu Can you please share simple dataset. Current dataset is too big (7 GB). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43835\">No</a>\n"]}, {"number": 43834, "title": "Horovod fails to train Keras Preprocessing IntegerLookup Layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7.6\r\n- GPU model and memory: K80, 15 GB of RAM\r\n\r\n**Describe the current behavior**\r\nIn TensorFlow 2.3, Keras Preprocessing Layers were released.  Some of these cause Horovod to fail.  It is not clear if this is a Horovod or TensorFlow issue.\r\n\r\nThe model that fails is:\r\n```python3\r\ndef make_model():\r\n  import tensorflow.keras as keras\r\n  \r\n  vocabulary = range(1, 11)\r\n  return keras.Sequential([\r\n    keras.layers.experimental.preprocessing.IntegerLookup(vocabulary=vocabulary),\r\n    keras.layers.Embedding(len(vocabulary) + 2, 8, input_length=1),\r\n    tf.keras.layers.Dense(1, activation='sigmoid')\r\n  ])\r\n```\r\n\r\nWhen run, the error that is reported is:\r\n```pycon\r\n[1,0]<stderr>:  File \"/databricks/python/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\r\n[1,0]<stderr>:    raise e.ag_error_metadata.to_exception(e)\r\n[1,0]<stderr>:AttributeError: in user code:\r\n[1,0]<stderr>:\r\n[1,0]<stderr>:    /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:138 broadcast_group  *\r\n[1,0]<stderr>:        var.assign(broadcast(var, root_rank))\r\n[1,0]<stderr>:\r\n[1,0]<stderr>:    AttributeError: 'TrackableWeightHandler' object has no attribute 'assign'\r\n```\r\n\r\nAfter fitting the same model locally, we can look at the vairables and see:\r\n```python\r\nfor x in fitted_model.variables:\r\n  print(type(x))\r\n```\r\n```pycon\r\n<class 'tensorflow.python.keras.engine.base_layer_utils.TrackableWeightHandler'>\r\n<class 'tensorflow.python.eager.def_function.UnliftedInitializerVariable'>\r\n<class 'tensorflow.python.eager.def_function.UnliftedInitializerVariable'>\r\n<class 'tensorflow.python.eager.def_function.UnliftedInitializerVariable'>\r\n```\r\n\r\n`TrackableWeightHandler` appears to not support the interface of https://www.tensorflow.org/api_docs/python/tf/Variable - in particular, the `assign` method.\r\n\r\nMight be similar to https://github.com/keras-team/autokeras/issues/1225 .\r\n\r\n**Describe the expected behavior**\r\nHorovod should be able to train with Keras Preprocessing Layers.\r\n\r\nI also expect all objects returned from `keras.Model.variables` to be `tf.Variables`.  I don't know if that expectation is correct.  The API is vague on that part.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport horovod.tensorflow.keras as hvd\r\n\r\nfrom sparkdl import HorovodRunner\r\n\r\ndef make_dataset():\r\n  import tensorflow as tf\r\n  from random import randrange\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(\r\n    ([randrange(1, 11) for p in range(0, 10000)], [randrange(0,2) for p in range(0, 10000)])\r\n  )\r\n  dataset = dataset.repeat().batch(128)\r\n  return dataset\r\n\r\ndef train():\r\n  import tensorflow as tf\r\n  import tensorflow.keras as keras\r\n  import horovod.tensorflow.keras as hvd\r\n  \r\n  # Initialize Horovod\r\n  hvd.init()\r\n\r\n  # Pin GPU to be used to process local rank (one GPU per process)\r\n  gpus = tf.config.experimental.list_physical_devices('GPU')\r\n  for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n  if gpus:\r\n      tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\r\n\r\n  # Build model and dataset\r\n  dataset = make_dataset()\r\n  model = make_model()\r\n  # Horovod: adjust learning rate based on number of GPUs.\r\n  scaled_lr = 0.001 * hvd.size()\r\n  opt = tf.optimizers.Adam(scaled_lr)\r\n\r\n  # Horovod: add Horovod DistributedOptimizer.\r\n  opt = hvd.DistributedOptimizer(opt)\r\n\r\n  # Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\r\n  # uses hvd.DistributedOptimizer() to compute gradients.\r\n  model.compile(\r\n    loss=tf.losses.BinaryCrossentropy(from_logits=True),\r\n    optimizer=opt,\r\n    metrics=['AUC'],\r\n    experimental_run_tf_function=False\r\n  )\r\n\r\n  callbacks = [\r\n      # Horovod: broadcast initial variable states from rank 0 to all other processes.\r\n      # This is necessary to ensure consistent initialization of all workers when\r\n      # training is started with random weights or restored from a checkpoint.\r\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\r\n  ]\r\n\r\n  model.fit(\r\n    dataset,\r\n    steps_per_epoch=500 // hvd.size(),\r\n    callbacks=callbacks,\r\n    epochs=2,\r\n    verbose=1 if hvd.rank() == 0 else 0\r\n  )\r\n  \r\n  return model\r\n\r\nhr = HorovodRunner(np=-1)\r\nfitted_model = hr.run(train)\r\n\r\ndef train_local():\r\n  # Build model and dataset\r\n  dataset = make_dataset()\r\n  model = make_model()\r\n  \r\n  opt = tf.optimizers.Adam(0.001)\r\n  model.compile(\r\n    loss=tf.losses.BinaryCrossentropy(from_logits=True),\r\n    optimizer=opt,\r\n    metrics=['AUC'],\r\n    experimental_run_tf_function=False\r\n  )\r\n\r\n  model.fit(\r\n    dataset,\r\n    steps_per_epoch=100,\r\n    epochs=2,\r\n    verbose=1\r\n  )\r\n  \r\n  return model\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["The potential Horovod defect is available at https://github.com/horovod/horovod/issues/2356 .", "@jeisinge \r\nI ran the code shared and face dependency issues , please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f6c835f03d55d913d0df6c04a886d93d/untitled426.ipynb), if possible share a colab gist with the error reported.", "Thanks for attempting to reproduce.  Databricks allows for running Horovod from the notebook environment.  However, I have been unable to accomplish this in Colab.\r\n\r\nThe good news is that on the [other ticket](https://github.com/horovod/horovod/issues/2356), @tgaddair has been reproduce locally.\r\n\r\n@tgaddair, do you have a local script/gist that can be posted?", "Yes, running this script with a single process was sufficient to repro for me:\r\n\r\n```\r\n        import tensorflow as tf\r\n        import horovod.tensorflow.keras as hvd\r\n\r\n        opt = tf.keras.optimizers.Adam(lr=0.1)\r\n        opt = hvd.DistributedOptimizer(opt)\r\n\r\n        vocabulary = range(1, 11)\r\n        model = keras.Sequential([\r\n            tf.keras.layers.experimental.preprocessing.IntegerLookup(vocabulary=vocabulary),\r\n            tf.keras.layers.Embedding(len(vocabulary) + 2, 8, input_length=1),\r\n            tf.keras.layers.Dense(1, activation='sigmoid')\r\n        ])\r\n\r\n        model.compile(loss=tf.losses.BinaryCrossentropy(from_logits=True),\r\n                      optimizer=opt,\r\n                      metrics=['AUC'],\r\n                      experimental_run_tf_function=False)\r\n\r\n        dataset = tf.data.Dataset.from_tensor_slices(\r\n            ([randrange(1, 11) for _ in range(0, 100)], [randrange(0, 2) for _ in range(0, 100)])\r\n        )\r\n        dataset = dataset.repeat().batch(128)\r\n\r\n        callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(0)]\r\n        model.fit(dataset,\r\n                  steps_per_epoch=10,\r\n                  callbacks=callbacks,\r\n                  epochs=1)\r\n```\r\n\r\nAs I mentioned in the other ticket, the issue is pretty clear.  The `model.variables` is not returning a list of variables here. Instead, one of the variables is this `TrackableWeightHandler`, which does not adhere to the variable interface. As such, we cannot broadcast it and we cannot assign weights to it.\r\n\r\nWe need to determine from the Keras team if this is intended behavior, and if so, how we can workaround this.  But ideally, Keras would not be putting objects that do not adhere to the variable interface in variables, as special-casing is not a good long term solution for consumers of the API like Horovod.", "This is a known issue. However what are the use cases of getting `model.variables` instead of `model.trainable_variables` -- I fail to see the reason of updating non trainable variables during training runtime.", "Hey @tanzhenyu, the motivation is to sync variables across multiple workers to ensure consistency during parallel training.  For example, you may have non-trainable variables initialized randomly that need to have the same values on every worker to prevent divergence.  Does that makes sense?", "FYI - I just tested again with TensorFlow 2.4.1.  This error is still happening.\r\n\r\nAny update?  Any work around?", "FYI - same issue with TF 2.3 -> TF 2.5.  I created a gist for TF 2.5: https://colab.research.google.com/gist/jeisinge/1fe6b14ea960538ce0f14fa6a70ac09e/keraspreprocessing-horovod.ipynb", "Oops!  I just went back to test and I realized the gist I posted appears to be an old version.  I just recreated with https://colab.research.google.com/gist/jeisinge/80fd4e4a4af0c2287072714e7edbd43c/keras_preprocessing-horovod.ipynb for a simple test on the latest released TensorFlow.\r\n\r\nAlso, `tf.keras.layers.experimental.preprocessing.TextVectorization` fails for the same reason.", "@jeisinge \r\nPlease confirm if the issue is resolved as the pr is merged.\r\nAlso, It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version.", "I just tested with rc0 and rc2.  It works.\r\n\r\nAlso, I created I https://colab.research.google.com/gist/jeisinge/10629c26b078ad9f51987cb7acd3ca9a/keras_preprocessing-horovod.ipynb to test for histograms on TensorBoard that had a similar issue.  Everything appears to work!\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/41244 might be fixed as well?!", "@jeisinge \r\nPlease move this to closed status as resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43834\">No</a>\n"]}]