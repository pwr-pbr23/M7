[{"number": 9120, "title": "Fix for Issue 9103, serialization issue with tf.Placeholder", "body": "Removed restriction on Placeholder to have fully defined dimensions. Partially defined dimensions now accepted. Allows Placeholders to now be fully serialized under Protobuf. ", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "I signed it", "@cwhipkey does that look right?\r\n\r\n@karpkarp could you make sure you are pushing with the same email as the one you used for CLA?", "@vrv is working on a more comprehensive fix (that also enables distinguishing between truly unknown and scalar shapes), so I'd be tempted to hold off on this at least until @vrv chimes in.", "I already submitted a fix internally that should fix the problem -- hopefully with the next push it'll all just work?", "Pushed commits which include: https://github.com/tensorflow/tensorflow/pull/9143/commits/6461dd322be3a74f0c20b62a8136bc0d8d8c1bb8 and https://github.com/tensorflow/tensorflow/pull/9143/commits/24a95ae389e1c76e771ac33d66e0ec40a236260f\r\n\r\nPlease pull and check if that worked for you.", "Assuming this has been resolved. Feel free to reopen."]}, {"number": 9119, "title": "Branch 152703253", "body": "Push again, this time do not squash.", "comments": []}, {"number": 9118, "title": "Copy the Keras config file handling from master into the 1.1 release", "body": "Fix for Cloud ML to use 1.1", "comments": ["Current code only handles EEXIST, and doesn't handle unwritable directories.", "Jenkins, test this please.", "/cc: @yifeif "]}, {"number": 9117, "title": "[OpenCL] Implementation improvements", "body": "OpenCL implementation improvements (#22)\r\n\r\n- [Build] Use gcc/g++ as a host compiler to avoid #8394 (#54)\r\n- [Eigen] Version bump\r\n- [OpenCL] Implementation improvements\r\n  - Register SYCL implementations for random ops\r\n    - Simplify by using Eigen math functions\r\n  - Registers Scatter and ScatterNd Ops for SYCL\r\n  - Registers Stack op for SYCL\r\n  - Fixes No sycl buffer found error for debug ops\r\n  - Registers MatMul and Transpose Ops to SYCL device for double\r\n  - Extends analyzer_cli_test.py test to cover SYCL\r\n  - Fixes Transpose Op for double when on SYCL\r\n  - Bumps Eigen version to fix double precision issue on SYCL\r\n  - Extends SessionDebugTestBase to cover SYCL\r\n  - Bumps Eigen Version\r\n  - Refactors Ops registration\r\n  - Introduces workaround for Const Op related to the difference between\r\n   CUDA which uses pointers and OpenCL that uses buffers/accessors\r\n  - Extends memory types to cover DEVICE_SYCL as well\r\n  - Introduces  GetSYCLDevice() method that returns list of supported devices\r\n   with GPU device having the highest priority ( doesn't include blacklisted devices )\r\n  - ::internal::Transpose -> tensorflow::internal::Transpose in order to\r\n   avoid compilation reported error\r\n  - Adds sycl_runtime to bazels ARRAY_DEPS\r\n  - Replicates TF_CALL_GPU_PROXY_TYPES for SYCL\r\n  - Fixes an issue caused by switch to aligned allocator for sycl buffer (#53)\r\n  - Fix testSimple and testConst in stack_op_test (#3)\r\n  - RandomGamma has no GPU friendly implementation (#57)\r\n  - Register batch normalization kernels for OpenCL (#61)\r\n  - Compatibility fixes for TensorFlow 1.1.0-rc1\r\n  - Fixes Scatter Op\r\n  - Implements BatchMatmul Op for SYCL\r\n  - Lowercase the device name when GPU or SYCL returned\r\n  - kernel_estimator_test.py assertEqual-> assertAlmostEqual due to floating point \r\n    representation on the device\r\n", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@ville-k, @Zakor94 can you confirm?", "Sometimes googlebot gets confused, could you `git pull --rebase` and push again?", "@lukeiwanski @googlebot  ok with me.", "@lukeiwanski @googlebot yes ok with me too.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "Jenkins, test this please.", "@lukeiwanski these massive PRs are hard to review since there is a stream of new commits being added, and the PR is already very large.\r\n\r\nWe typically try to stage these changes in small PRs that are more easily reviewable -- can you break this change into smaller requests in some way?  As it is, I don't see anybody being able to safely review a change of this magnitude (117 files!)", "@vrv Ok I will try. What is desired changeset size?\r\n\r\nEDIT: 90 files comes from this commit https://github.com/benoitsteiner/tensorflow-opencl/commit/ecd6f7a0f747f2f602c51214bc9e9cf3ffb45aec . It's due to simple refactoring ot already implemented functionality (moving SYCL registration code at the end of the file instead of between the CPU and the GPU). This apears to by squashed into one massive commit.\r\n\r\nShould I create separate pull request for the refactoring commit?", "Benoit would have a better idea of what he'd like, but it's much easier to review a change that has like, <10 files, < 200-300 LoC change at a time; that we he can review it, it will have fewer things to change, and we can make more iterative progress.\r\n\r\nMight be worth you and Benoit talking offline to see what's going to work well!\r\n\r\n(I'm just on sync rotation this week and trying to help move things along :)", "Appriciate and thanks @vrv !\r\nMy plan for this PR is to revert refactoring changes ( that should knock off 77 files ) and re-introduce them in a new one ( or series ) - I will check with @benoitsteiner on how to proceed.", "@vrv Is current size of the pull request acceptable or should I keep splitting it?\r\n\r\nOr perhaps I should create a new, clean, pull request that follows you guidelines?", "Can one of the admins verify this patch?", "@benoitsteiner could you please take another look?", "@tensorflow-jenkins test this please", "The MacOs CPU Tests seems to be failing due to a missing dependency in gif_archive..\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/external/gif_archive/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@gif_archive//:gif':\r\nthis rule is missing dependency declarations for the following files included by 'external/gif_archive/lib/openbsd-reallocarray.c':\r\n```\r\n \r\nIs that caused by our changeset or is it present in the upstream? I cannot see how we could cause it.", "@lukeiwanski no, the build error on MacOS is unrelated to your changes.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 9116, "title": "Problem with label_image.py", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n![capture](https://cloud.githubusercontent.com/assets/20141573/24884907/07d2d32c-1e4c-11e7-9568-fbaae2459860.PNG)\r\n\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!"]}, {"number": 9115, "title": "Documentation link issue", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\nunder this link \r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/deprecated/scalar_summary\r\n\r\nthe webpage is not found \r\n\r\nhttps://www.tensorflow.org/code/tensorflow/contrib/deprecated/__init__.py\r\n\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["@craigcitro The tensorflow.org/code/... redirector service seems to return 404 for valid `__init__.py` paths. Could App Engine be intercepting those requests?", "Ooh, this is an interesting one. My current hypothesis: app engine routing doesn't like path segments with a leading underscore, maybe an overzealous check for routes like `_ah`?\r\n\r\nAnother failing sample url: https://www.tensorflow.org/code/tensorflow/contrib/labeled_tensor/python/ops/_typecheck.py fails to route to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/labeled_tensor/python/ops/_typecheck.py.\r\n\r\nI'm going to file an internal bug to find out more. (I couldn't turn anything up in a Google search.)", "I believe that's an internal issue to our docs hosting system.  We'll look into it.  Thanks.", "As of now, those links are rewritten to point to github.com/ links, although pre-v1.1 documentation will not  be updated. So, on the scalar_summary page, it now points to:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/deprecated/__init__.py"]}, {"number": 9114, "title": "Help", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["https://cs.nginx.com/static/install-nginx && \u0421\u0443\u0434\u0430 CHMOD + \u0445-\u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 Nginx\r\n                \u0421\u0443\u0434\u043e ./install-nginx 22ee785c8b8eeadc6d0e72f1256779b5"]}, {"number": 9113, "title": "Asynchronous Training Issue for Distributed Tensorflow", "body": "I guess this issue may not be supposed to show up here, and I apologize for opening the ticket here, but really want to be clear about asynchronous training with Distributed Tensorflow. I posted my question on StackOverflow: http://stackoverflow.com/questions/43147435/how-does-asynchronous-training-work-in-distributed-tensorflow, but I got two opposite answers and didn't know which one is the correct. I read the TF docs and example code multiple times, but they're still confused me. So I really appreciate if I could get some official interpretations on asynchronous training.", "comments": ["I posted an answer: http://stackoverflow.com/a/43334912/3574081 \r\n\r\nFeel free to comment on that answer if you have further questions. As you surmised, Stack Overflow is the more appropriate place for this kind of discussion, and issues are for bugs and feature requests."]}, {"number": 9112, "title": "Branch 152703253", "body": "Manually reviewed and merged, from possibly previous faulty merge. Took google internal versions.\r\n\r\nNot sure why it shows the same commits again from previous push (9059).\r\n\r\ntensorflow/tools/graph_transforms/README.md\r\ntensorflow/tensorboard/backend/application.py\r\ntensorflow/tensorboard/backend/application_test.py\r\ntensorflow/core/grappler/optimizers/BUILD\r\ntensorflow/core/grappler/optimizers/auto_parallel.h\r\ntensorflow/core/grappler/optimizers/meta_optimizer.cc\r\ntensorflow/core/ops/array_ops.cc\r\ntensorflow/python/layers/normalization.py\r\ntensorflow/tensorboard/plugins/debugger\r\ntensorflow/core/protobuf/rewriter_config.proto\r\ntensorflow/contrib/layers/python/layers/embedding_ops.py\r\n", "comments": ["Jenkins, test this please."]}, {"number": 9111, "title": "tensorflow installation issue", "body": "I have been following the [Tensorflow installation guide](https://www.tensorflow.org/install/install_linux#InstallingVirtualenv) to install tensorflow r.1.0. Since out network system does not have direct internet access to outside, so I installed it as following in the active virtualenv environment\r\n\r\n```\r\n(virtualenv-test) bash-4.1$ pip3 install -t --upgrade /data/pythonlibs/tensorflow-1.0.1-cp34-cp34m-linux_x86_64.whl\r\nProcessing /data/dsp_emerging/ugwz/pythonlibs/tensorflow-1.0.1-cp34-cp34m-linux_x86_64.whl\r\n```\r\nHowever, I got the following error message, what does it mean?\r\n\r\n> Collecting six>=1.10.0 (from tensorflow==1.0.1)\r\n>   Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x2b623e22bcc0>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/six/\r\n>   Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x2b623e22bd68>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/six/\r\n>   Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x2b623e113fd0>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/six/\r\n>   Retrying (Retry(total=1, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x2b623e115358>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/six/\r\n>   Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x2b623e115438>: Failed to establish a new connection: [Errno -2] Name or service not known',)': /simple/six/\r\n>   Could not find a version that satisfies the requirement six>=1.10.0 (from tensorflow==1.0.1) (from versions: )\r\n> No matching distribution found for six>=1.10.0 (from tensorflow==1.0.1)\r\n> ", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support.", "Hi @wenouyang, please provide all the info the template requires, it helps us better asses what is happening and solve the problem. \r\nPip is trying to download the package `six` and probably due your absence of internet connection TensorFlow is not successfully installing.\r\n\r\nedit: follow @jart instructions, I posted a second after she did so please disconsider", "Thanks"]}, {"number": 9110, "title": "RNN Tutorial references defunct library", "body": "[In the RNN tutorial](https://www.tensorflow.org/tutorials/recurrent), the sample code has line:\r\n\r\n`lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)`\r\n\r\nAccording to issue #6432, this should be replaced by:\r\n\r\n`lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)`", "comments": ["The tutorials are meant to be in sync with the latest release. It does seem that `tf.contrib.rnn.BasicLSTMCell` exists in the 1.0 release.\r\n\r\nIs it not working for you?\r\n", "Sorry, you're right. Looks like I'm running 0.10.0! Thought I had installed the latest version."]}, {"number": 9109, "title": "line<= 80 and RELU to ReLU", "body": "line<= 80 and RELU to ReLU", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9108, "title": "tf.while_loop giving unexpected result when used with tf.assign() during run time", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: 1.0.0\r\n- *Bazel version (if compiling from source)*: NA\r\n- *CUDA/cuDNN version*:  NA\r\n- *GPU Model and Memory*: NA\r\n- *Exact command to reproduce*:\r\n\r\n\r\n### Describe the problem clearly\r\ntf.while_loop with is not working as expected when used with tf.assign()\r\n\r\nIn the below example code, the reset_x operation seems to be preventing the variable 'x' from updating as per per logic in loop1. if I print the output after loop1 has been run, I can see that x is still zero. However I I confirmed that the loop is working by printing output of loop1. It seems resent_x op is being used inside the tf.while_loop() during run time instead of only resetting x back to 0 after loop has finished. If I were to remove reset_x then the variable x is getting updated by the loop.\r\n\r\nbody = lamda x: tf.assign_add(x,1)\r\nloop1 = tf.while_loop(cond,body, [x])\r\nx_op = tf.reduce_sum(tf.abs(x))\r\nreset_x = tf.assign(x,[0])\r\nwith tf.Session() as sess:\r\n......\r\n......\r\n       for in range (100):\r\n              sess.run(loop1)\r\n              print(sess.run(x))\r\n              sess.run(x_op) \r\n              sess.run(reset_x)\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": []}, {"number": 9107, "title": "Tensorflow tests failing on s390x (Farmhash related)", "body": "Below tests are failing on big endian s390x platform.\r\nThe tests are failing due to farmhash doesn't support s390x platform.\r\nWe have already raised an [issue ](https://github.com/google/farmhash/issues/10) with farmhash. Test are passing after applying [patch](https://github.com/google/farmhash/issues/10#issuecomment-272068176) provided there. \r\n\r\nHowever, we would like to know if the testcases are used  for some complex functionality of TensorFlow?\r\n\r\nTests:\r\n```\r\n //tensorflow/contrib/layers:sparse_feature_cross_op_test \r\n //tensorflow/contrib/learn:tensorflow_dataframe_test \r\n //tensorflow/contrib/linear_optimizer:sdca_ops_test \r\n //tensorflow/core:platform_fingerprint_test\r\n\r\n```\r\n\r\n\r\nTensorflow version: v0.10.0", "comments": ["If the fix is available in farmhash upstream, then it probably make sense to update [`workspace.bzl`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L186) so that it includes the fixed version. Would you like to send a PR for that?\r\n\r\n(Also, if possible, I'd suggest moving to TensorFlow 1.0)", "@asimshankar The fix is not yet available in farmhash upstream. \r\nHere are the details: https://github.com/google/farmhash/issues/10\r\n\r\nSo, would like to know if the above mentioned testcases are used to test some complex functionality of TensorFlow? or they can be ignored? ", "@Nayana-ibm - Whether or not they can be ignored would depend on what your use case is. Certainly, these failures cannot be ignored if any of the operations in the model use the fingerprinting function (which would be the contrib ops whose test failed, plus some other operations in core that invoke the `Fingerprint64` and `Fingerprint128` functions).\r\n\r\nThey can be ignored if the codepaths you invoke don't call the `Fingerprint*` functions. Hard to say without more information, though I'd be conservative and suggest that this problem be fixed (and also, that you move to TensorFlow 1.0 instead of 0.10 if possible)", "Tensorflow uses farmhash as an external dependency, it fetches farmhash code through [this ](https://github.com/tensorflow/tensorflow/blob/v0.10.0/tensorflow/workspace.bzl) file.\r\n\r\n```\r\nnative.new_http_archive(\r\n    name = \"farmhash_archive\",\r\n    url = \"https://github.com/google/farmhash/archive/34c13ddfab0e35422f4c3979f360635a8c050260.zip\",\r\n```\r\n    \r\nNow, the required farmhash [patch ](https://github.com/google/farmhash/commit/2bc1fd0d958f6b6b2131ac70484aa81bfaf9b29a)for big endian s390x platform has been merged. \r\n\r\nSo, TensorFlow can use [this](https://github.com/google/farmhash/commit/2bc1fd0d958f6b6b2131ac70484aa81bfaf9b29a) commit id for Tensorflow as to add support for big endian?\r\n\r\n\r\n \r\n", "When [this](https://github.com/google/farmhash/commit/2bc1fd0d958f6b6b2131ac70484aa81bfaf9b29a) commit id will be used in Tensorflow for farmhash?", "@asimshankar \r\nThe farmhash fix (which is required for mentioned test-cases) is now available in farmhash upstream through [this](https://github.com/google/farmhash/commit/2bc1fd0d958f6b6b2131ac70484aa81bfaf9b29a) commit id.  Now, workspace.bzl can be updated with this commit id for farmhash. \r\n\r\n@gunan  Will it be possible to update workspace.bzl file to use [this ](https://github.com/google/farmhash/commit/2bc1fd0d958f6b6b2131ac70484aa81bfaf9b29a)commit id ? ", "Sorry for missing this issue.\r\n@Nayana-ibm please feel free to send a pull request updating the pinned hash in tensorflow/workspace.bzl\r\nYou can assign the pull request to me.", "@gunan Thanks for your reply. \r\nI was a preparing patch for tensorflow/workspace.bzl file for PR however getting an issue with below link: \r\n//mirror.bazel.build/github.com/google/farmhash/archive/23eecfbe7e84ebf2e229bd02248f431c36e12f1a.zip\"\r\n\r\non wget, it gives an error as: \r\n`HTTP request sent, awaiting response... 403 Forbidden`\r\n\r\nAm I missing something here?\r\n\r\nAlso the patch requires sha256 for farmhash commit id.  I have calculated sha256 for farmhash commit id `23eecfbe7e84ebf2e229bd02248f431c36e12f1a` as  `55215f8cd3ddbe9781f6fe5cc228731d6dcc8301b6191c6d420034c3fff1cb8d`. \r\ncan you please confirm if it's correct?\r\n\r\n\r\n\r\n\r\n\r\n", "That is expected to fail. We manuqlly mirror dependencies on GCS once the\nlinks are updated.\n\nOn Jul 18, 2017 8:16 AM, \"Nayana Thorat\" <notifications@github.com> wrote:\n\n@gunan <https://github.com/gunan> Thanks for your reply.\nI was a preparing patch for tensorflow/workspace.bzl file for PR however\ngetting an issue with below link:\n//mirror.bazel.build/github.com/google/farmhash/archive/\n23eecfbe7e84ebf2e229bd02248f431c36e12f1a.zip\"\n\non wget, it gives an error as:\nHTTP request sent, awaiting response... 403 Forbidden\n\nAm I missing something here?\n\nAlso the patch requires sha256 for farmhash commit id. I have calculated\nsha256 for farmhash commit id 23eecfbe7e84ebf2e229bd02248f431c36e12f1a as\n55215f8cd3ddbe9781f6fe5cc228731d6dcc8301b6191c6d420034c3fff1cb8d.\ncan you please confirm if it's correct?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/9107#issuecomment-315960192>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AHlCOdvvl-7raDSq_8i75MzwzP2Pz58Cks5sPD-TgaJpZM4M5Eap>\n.\n", "patch committed to TensorFlow through [PR-11579](https://github.com/tensorflow/tensorflow/pull/11579).\r\nClosing this issue. "]}, {"number": 9106, "title": "add code description", "body": "add code description", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 9105, "title": "Blas SGEMM launch failed", "body": "I just installed  TensorFlow-GPU 1.0.1 on Win10 GTX GEFORCE 850M with CUDA 8.0 and Cudnn v5.1, Anaconda3 4.2.0 64bit\r\nwhen I try to figure out if the installation was successful, I run the \r\n\r\n> mnist_with_summaries.py\r\nin \r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py](url)\r\n\r\nWhen I run the code in Jupyter Notebook, it prints \r\n\r\n> Accuracy at step 0: 0.068\r\nAccuracy at step 10: 0.6795\r\nAccuracy at step 20: 0.8062\r\nAccuracy at step 30: 0.8455\r\nAccuracy at step 40: 0.8737\r\nAccuracy at step 50: 0.8735\r\nAccuracy at step 60: 0.8851\r\nAccuracy at step 70: 0.8815\r\nAccuracy at step 80: 0.8863\r\nAccuracy at step 90: 0.8918\r\n\r\nAnd the kernel just died.\r\n\r\nWhen I try to run the code in command prompt, I get following error:\r\n\r\n> >I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\n> Extracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\n> name: GeForce GTX 850M\r\n> major: 5 minor: 0 memoryClockRate (GHz) 0.9015\r\n> pciBusID 0000:0a:00.0\r\n> Total memory: 4.00GiB\r\n> Free memory: 3.35GiB\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0)\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n> W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n>     return fn(*args)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\n>     status, run_metadata)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n>     next(self.gen)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n>     pywrap_tensorflow.TF_GetCode(status))\r\n> tensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(10000, 784), b.shape=(784, 500), m=10000, n=500, k=784\r\n>          [[Node: layer1/Wx_plus_b/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input/x-input_0/_45, layer1/weights/Variable/read)]]\r\n>          [[Node: accuracy/accuracy/Mean/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"mnist_with_summaries.py\", line 209, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"mnist_with_summaries.py\", line 185, in main\r\n>     train()\r\n>   File \"mnist_with_summaries.py\", line 160, in train\r\n>     summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n>     run_metadata_ptr)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n>     feed_dict_string, options, run_metadata)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n>     target_list, options, run_metadata)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : a.shape=(10000, 784), b.shape=(784, 500), m=10000, n=500, k=784\r\n>          [[Node: layer1/Wx_plus_b/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input/x-input_0/_45, layer1/weights/Variable/read)]]\r\n>          [[Node: accuracy/accuracy/Mean/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n> \r\n\r\n> Caused by op 'layer1/Wx_plus_b/MatMul', defined at:\r\n>   File \"mnist_with_summaries.py\", line 209, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"mnist_with_summaries.py\", line 185, in main\r\n>     train()\r\n>   File \"mnist_with_summaries.py\", line 101, in train\r\n>     hidden1 = nn_layer(x, 784, 500, 'layer1')\r\n>   File \"mnist_with_summaries.py\", line 95, in nn_layer\r\n>     preactivate = tf.matmul(input_tensor, weights) + biases\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1765, in matmul\r\n>     a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1454, in _mat_mul\r\n>     transpose_b=transpose_b, name=name)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n>     op_def=op_def)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\r\n>     original_op=self._default_original_op, op_def=op_def)\r\n>   File \"C:\\Users\\airfo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\r\n>     self._traceback = _extract_stack()\r\n> \r\n> InternalError (see above for traceback): Blas SGEMM launch failed : a.shape=(10000, 784), b.shape=(784, 500), m=10000, n=500, k=784\r\n>          [[Node: layer1/Wx_plus_b/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_input/x-input_0/_45, layer1/weights/Variable/read)]]\r\n>          [[Node: accuracy/accuracy/Mean/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_accuracy/accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n> And this Internal error message appears three times.It shows that all dll files were successfully opened. ( too many error message, I just write down something I think useful. If anyone need more information, tell me).\r\n> \r\n\r\nI found an answer on stackoverflow [http://stackoverflow.com/questions/41117740/tensorflow-crashes-with-cublas-status-alloc-failed](url), and I add :\r\n```\r\n`config = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config, ...)`\r\n```\r\njust after:\r\n`if __name__ == '__main__':`\r\nthis time the error is:\r\n\r\n> > I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\n> name: GeForce GTX 850M\r\n> major: 5 minor: 0 memoryClockRate (GHz) 0.9015\r\n> pciBusID 0000:0a:00.0\r\n> Total memory: 4.00GiB\r\n> Free memory: 3.35GiB\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0)\r\n> Extracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 850M, pci bus id: 0000:0a:00.0)\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n> Accuracy at step 0: 0.0886\r\n> Accuracy at step 10: 0.6844\r\n> Accuracy at step 20: 0.7995\r\n> Accuracy at step 30: 0.8564\r\n> Accuracy at step 40: 0.876\r\n> Accuracy at step 50: 0.8819\r\n> Accuracy at step 60: 0.8892\r\n> Accuracy at step 70: 0.8881\r\n> Accuracy at step 80: 0.8843\r\n> Accuracy at step 90: 0.8912\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cupti64_80.dll\r\n> F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\default\\gpu\\cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: cuptiActivityRegisterCallbacks not found)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n\r\n I am totally lost. Could someone tell me why?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 21 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler Can you count days with a cumulative approach excluding your posts? So instead of 14 days is 14+21+29+14=78 days. I think that it is more informative as notification.", "@drpngx What do you think? This approach was already adopted in `Nagging Assignee`.", "Yeah that makes sense. @yifeif @av8ramit ?", "I think that is supposed to be the behavior by design (see comment on May 4, which says 29 days instead of 15 days). Not sure why it got regressed.", "The origin was in https://github.com/tensorflow/tensorflow/pull/12408#issuecomment-373941577 but then something regressed with `awaiting response` label but on `Nagging Assignee` it still performs well with the new behavior.", "After solving this topic I think that we can close a bug on TF 1.0.1 after more then one year without response.\r\nRemember the bottleneck theory and the \"attention is all you need\" :smile: or we will lost in the information noise expecially if you don't have enough bandwidth for all the `awaiting response` labels.", "I think I know why. Fixing this.", "Should be fixed. Let us know if you see it happening again!", "Can we close this?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 9104, "title": "Add Windows link to \"Common installation problems\"", "body": "The link we provide our users when `import tensorflow` fails only points to Linux and Mac OS. Add a link to the Windows instructions to match.", "comments": []}, {"number": 9103, "title": "BUG: tensorflow.placeholder shape does not serialize with protobuf", "body": "**Profobuf serialization(json)** \r\n{\r\n      \"attr\": {\r\n        \"dtype\": {\r\n          \"type\": \"DT_FLOAT\"\r\n        },\r\n        \"shape\": {\r\n          \"shape\": {}\r\n        }\r\n      },\r\n      \"name\": \"x\",\r\n      \"op\": \"Placeholder\"\r\n    },\r\n\r\n**Tensorflow code** \r\nx = tf.placeholder(tf.float32, shape=None, name=\"x\")", "comments": ["Could you elaborate on the bug here? There is a [known issue with the `Placeholder` op](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/core/ops/array_ops.cc#L2751) where it cannot distinguish between an unknown and a scalar shape, but it does serialize all other shapes correctly.\r\n\r\nThere is some work underway to figure out if that bug can be fixed without requiring the `PlaceholderV2` operation, but all other shapes should be fine regardless.\r\n\r\nCould you elaborate on your concern here?", "Sorry, I copied the wrong line from Python as I was testing. \r\nWhen a placeholder of shape [None, 784] was serialized, the corresponding element in the profobuf json serialization does not contain a shape attribute. \r\n \r\nThis is the python code: \r\n```python\r\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\r\n    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y_\")\r\n    with tf.name_scope(\"first_layer\"):\r\n\r\n        W = tf.Variable(tf.zeros([784,10]), name=\"W\")\r\n        b = tf.Variable(tf.zeros([10]), name=\"b\")\r\n    # Output\r\n        y = tf.matmul(x,W) + b\r\n\r\n    with tf.name_scope(\"softmax_layer\"):\r\n    # Loss Function\r\n        softmax = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_)\r\n    with tf.name_scope(\"error_check\"):\r\n        cross_entropy = tf.reduce_mean(softmax)\r\n\r\n    with tf.name_scope(\"accuracy_check\"):\r\n    #Accuracy Calc\r\n        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    outfile_txt = json_format.MessageToJson(sess.graph_def)\r\n    outfile = open(\"outfile.json\", 'w')\r\n    outfile.write(outfile_txt)\r\n```\r\n\r\nSelect elements from the output json file:\r\n``` JSON\r\n     {\r\n      \"attr\": {\r\n        \"dtype\": {\r\n          \"type\": \"DT_FLOAT\"\r\n        },\r\n        \"shape\": {\r\n          \"shape\": {}\r\n        }\r\n      },\r\n      \"name\": \"x\",\r\n      \"op\": \"Placeholder\"\r\n    },\r\n{\r\n      \"attr\": {\r\n        \"shape\": {\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"size\": \"784\"\r\n              },\r\n              {\r\n                \"size\": \"10\"\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        \"shared_name\": {\r\n          \"s\": \"\"\r\n        },\r\n        \"container\": {\r\n          \"s\": \"\"\r\n        },\r\n        \"dtype\": {\r\n          \"type\": \"DT_FLOAT\"\r\n        }\r\n      },\r\n      \"name\": \"first_layer/W\",\r\n      \"op\": \"VariableV2\"\r\n    }\r\n```\r\nLet me know if this is because of what you said earlier. Im using tensorflow-gpu installed from pip3 on windows. ", "@asimshankar do you know if there's an issue tracking that work?", "@karpkarp : Thanks for the sample code. It seems that if any of the dimensions are unknown is when we end up with an empty shape in the `GraphDef`, which is broader than the problem `PlaceholderV2` is going to address.\r\n\r\nI'll dig in a bit more.\r\n\r\nCC  @vrv ", "Actually I'm trying to change Placeholder itself so no new V2 is needed, but this is precisely correct.  We currently lose shape information when you serialize and deserialize partially known placeholder shapes.  This is fixed in V2 which I am trying to backport to v1.", "It seems in array_ops.py, it sets a requirement for the shape to be fully defined with shape.is_fully_defined() in the placeholder function. Any particular reason for this? Does this mean that a placeholder of shape of [None, SomeNum] will not be enforced?  \r\n\r\nIn any case, I removed the condition where the Placeholder shape has to be fully defined and the serialization issues are fixed. This does break placeholders with no defined shape so I added two additional function in python/framework/tensor_shape.py\r\n\r\npython/framework/tensor_shape\r\n```python\r\n def is_partially_defined(self):\r\n    return self._dims is not None\r\n\r\n  def assert_is_partially_defined(self):\r\n    if not self.is_partially_defined(self):\r\n      raise ValueError(\"Shape %s is not partially defined\" % self)\r\n\r\n```\r\npython\\ops\\array_ops.py\r\n\r\n```python\r\n\r\ndef placeholder(dtype, shape=None, name=None):\r\n  shape = tensor_shape.as_shape(shape)\r\n  if shape.is_partially_defined():\r\n    dim_list = shape.as_list()\r\n  else:\r\n    dim_list = []\r\n  ret = gen_array_ops._placeholder(\r\n      dtype=dtype,\r\n      shape=dim_list,\r\n      name=name)\r\n  ret.set_shape(shape)\r\n  return ret\r\n```\r\n\r\n```JSON \r\n\r\n  \"versions\": {\r\n    \"producer\": 21\r\n  },\r\n  \"node\": [\r\n    {\r\n      \"op\": \"Placeholder\",\r\n      \"name\": \"x\",\r\n      \"attr\": {\r\n        \"shape\": {\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"size\": \"-1\"\r\n              },\r\n              {\r\n                \"size\": \"784\"\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        \"dtype\": {\r\n          \"type\": \"DT_FLOAT\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"op\": \"Placeholder\",\r\n      \"name\": \"y_\",\r\n      \"attr\": {\r\n        \"shape\": {\r\n          \"shape\": {\r\n            \"dim\": [\r\n              {\r\n                \"size\": \"-1\"\r\n              },\r\n              {\r\n                \"size\": \"10\"\r\n              }\r\n            ]\r\n          }\r\n        },\r\n        \"dtype\": {\r\n          \"type\": \"DT_FLOAT\"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      \"op\": \"NoOp\",\r\n      \"name\": \"init\"\r\n    }\r\n  ]\r\n}\r\n"]}, {"number": 9102, "title": "Problems freezing the graph", "body": "- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n   Please see below for how to replicate the problems. Zip attachment contains two pieces of very small code.\r\n- *TensorFlow installed from (source or binary)?*:\r\n   binary - 1.1\r\n- *TensorFlow version*:\r\n     1.1\r\n- *Bazel version (if compiling from source)*:\r\n      Not applicable\r\n- *CUDA/cuDNN version*:\r\n       Recent but not latest.  8.0, V8.0.44 \r\n- *GPU Model and Memory*:\r\n   Tesla K20 4 GB\r\n- *Exact command to reproduce*:\r\n   See below....\r\n### Describe the problem clearly\r\nWe have a problem related to saving the operations as constants while freezing using the algorithm in the attached files.\r\nThe problem can be easily replicated by trying to freeze the graphs generated by the toy example in textsum https://github.com/tensorflow/models/tree/master/textsum .\r\n\r\nModels are trained with the following command:\r\nbazel-bin/textsum/seq2seq_attention --mode=train --article_key=article --abstract_key=abstract --data_path=textsum/data/data --vocab_path=textsum/data/vocab --log_root=textsum/log_root --train_dir=textsum/log_root/train\r\n\r\nThen freeze_2_textsum.py is called with the following syntax:\r\npython freeze_2_textsum.py \r\nCommand in our case was:\r\npython freeze_2_textsum.py --model_folder=./log_root/ --outputnodes=global_step\r\n\r\nIn this case, we are able to find the saved constants in the frozen_model.pb file.\r\nBut when we try the same syntax for the trained graph in our project, we could not find the constants in the frozen model.pb file, while the freeze_2_textsum.py script prints the log message that \"13 ops were converted to constants\"\r\nThis problem leads to the following error while running the session in our test script:\r\n\"Attempting to use uninitialized value model/generate_embedding_RNN_output/BiRNN/BW/BasicLSTMCell/Linear/Bias\"\r\ncmd line for test script:\r\n\r\npython test_tf_frozen_txtsum.py\r\n\r\n\r\n### Source Code / Logs\r\n\r\n[Freezing_problem.zip](https://github.com/tensorflow/tensorflow/files/909961/Freezing_problem.zip)\r\n", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support. If you're sure this is a bug, let me know and I'll reopen.", "Pretty sure its a but that happens in recurrent networks. That's why went through the trouble of making a reproducible case."]}, {"number": 9101, "title": "Why `tf.nn.nce_loss` cannot run on GPU?", "body": "The code `tensorflow/examples/tutorials/word2vec/word2vec_basic.py` has a comment `# Ops and variables pinned to the CPU because of missing GPU implementation`.  I have also found that the operation `tf.nn.nce_loss` cannot be implemented by GPU. So why `tf.nn.nce_loss` cannot run on GPU?  My guess is the `log_uniform_candidate_sampler` cannot run on GPU. But I still don't know which part of it cause this problem. \r\n\r\nTensorflow version: 1.0.1\r\nCUDA version: 7.5\r\nGPU: Tesla K40c\r\n", "comments": ["To debug issues with device placement, use [`log_device_placement`](https://www.tensorflow.org/tutorials/using_gpu#logging_device_placement). This is more of a usage question that is best answered on stackoverflow in the future."]}, {"number": 9100, "title": "Why is there NO input_data in the file?", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\nMy question is irrelevant to these. \r\n\r\n### Describe the problem clearly\r\nI am learning the MNIST based on the tutorial on the [TensorFlow website](https://www.tensorflow.org/get_started/mnist/prosl)\r\nthe first line of code is: \r\n`from tensorflow.examples.tutorials.mnist import input_data`\r\n\r\nBut when I open the file in [GitHub](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py)\r\nI do not see the function Input_data in the file. WHY? \r\n\r\n\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, `input_data` is a module and does exist in its own file. See: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py\r\n\r\nClosing this out since this doesn't seem to be a bug or feature request."]}, {"number": 9099, "title": "Nearest neighbor interpolation method for tf.image.crop_and_resize ?", "body": "At the moment, only bilinear interpolation is supported by crop_and_resize. \r\nHowever, when working with little images (and with labeled images), it sometimes makes more sense to use a nearest neighbor interpolation.\r\nAny plans of adding a nearest neighbor interpolation for the method in the near future ? \r\n", "comments": ["Thanks for reaching out with this feature request. It seems @gpapan added a `method` attribute to this op, so he probably anticipated that people would ask for this.", "Hi, I'm interesting in adding a nearest neighbor interpolation for the method. Have other people also interesting in it?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Hi, I'm also interested in this feature, are there any plans/volunteers for adding this feature soon?\r\nI was also wondering, is back-propagation possible through the nearest neighbor interpolation method?", "@melfm do you have some idea about it ? ", "Well for now I just use the `tf.image.crop_to_bounding_box` with `tf.image.resize_images` inside a `tf.while_loop` which is a bit slow.", "After poking around in the source, I noticed that there is an implementation of nearest neighbor for CPU and it looks like it was intended for GPU as well. [Here](https://github.com/tensorflow/tensorflow/blob/b12c3bb1157245adf6230a2e045831348f679b5b/tensorflow/core/kernels/crop_and_resize_op.cc) is the CPU implementation and you can see there is some intent to implement other resizing methods. Personally I am interested in Bicubic resizing. I've also noticed a bit of a speedup on GPU when using the combined crop_and_resize vs doing a bbox crop and resize separately. I would offer to work on the implementations but I have no experience creating such tensorflow ops from scratch. Specifically in figuring out the back propagation.", "This was added: 349ad798de7f69423e8397c223285ad58238cc31\r\nIssue can be closed."]}, {"number": 9098, "title": "tensorflow/core/util/ctc/ctc_loss_calculator.cc:144] No valid path found", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["why ", "Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support."]}, {"number": 9097, "title": "Android: Invalid argument: No OpKernel was registered to support Op 'PaddingFIFOQueue' with these attrs", "body": "I encountered a problem when using a multibox model to make inference on android platform (using c++ api, just like the android demo). The error says something like this:\r\n\r\ntensorflow_jni.cc:361 Error during inference: Invalid argument: No OpKernel was registered to support Op 'PaddingFIFOQueue' with these attrs\r\n                                                                       \t [[Node: prefetch_queue = PaddingFIFOQueue[capacity=500, component_types=[DT_FLOAT, DT_FLOAT, DT_BOOL, DT_UINT8, DT_STRING, DT_INT64], container=\"\", shapes=[[-1], [-1,4], [-1], [-1,-1,3], [], [-1]], shared_name=\"\"]()]]\r\n", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support. We also ask that people fill out the requested information in the issue template.", "@zgq91 If you build with -D__ANDROID_TYPES_FULL__ does this solve your problem? By default int64 is not supported on mobile to reduce binary size.\r\n\r\nI'd also try updating to the latest TF, as it sounds like you're an older version (we switched from the C++ API to the Java api a few months ago)."]}, {"number": 9096, "title": "android libtensorflow_inference.so run crash (signal 6 (SIGABRT))", "body": "04-10 14:37:38.954   331   331 F DEBUG   : Build fingerprint: 'Xiaomi/virgo/virgo:6.0.1/MMB29M/7.3.30:user/release-keys'\r\n04-10 14:37:38.954   331   331 F DEBUG   : Revision: '0'\r\n04-10 14:37:38.954   331   331 F DEBUG   : ABI: 'arm'\r\n04-10 14:37:38.954   331   331 F DEBUG   : pid: 24044, tid: 24335, name: pool-1-thread-1  >>> cmmc.com.styletransfer <<<\r\n04-10 14:37:38.954   331   331 F DEBUG   : signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n04-10 14:37:38.983   331   331 F DEBUG   :     r0 00000000  r1 00005f0f  r2 00000006  r3 a08bf978\r\n04-10 14:37:38.983   331   331 F DEBUG   :     r4 a08bf980  r5 a08bf930  r6 0000000c  r7 0000010c\r\n04-10 14:37:38.983   331   331 F DEBUG   :     r8 9d8fca90  r9 a06a7bf8  sl a08bdbd0  fp a08bdb30\r\n04-10 14:37:38.985   331   331 F DEBUG   :     ip 00000006  sp a08bd930  lr b6ceec69  pc b6cf1058  cpsr 400f0010\r\n04-10 14:37:39.101   331   331 F DEBUG   :\r\n04-10 14:37:39.101   331   331 F DEBUG   : backtrace:\r\n04-10 14:37:39.101   331   331 F DEBUG   :     #00 pc 00042058  /system/lib/libc.so (tgkill+12)\r\n04-10 14:37:39.101   331   331 F DEBUG   :     #01 pc 0003fc65  /system/lib/libc.so (pthread_kill+32)\r\n04-10 14:37:39.102   331   331 F DEBUG   :     #02 pc 0001c403  /system/lib/libc.so (raise+10)\r\n04-10 14:37:39.102   331   331 F DEBUG   :     #03 pc 000195b5  /system/lib/libc.so (__libc_android_abort+34)\r\n04-10 14:37:39.102   331   331 F DEBUG   :     #04 pc 00017508  /system/lib/libc.so (abort+4)\r\n04-10 14:37:39.102   331   331 F DEBUG   :     #05 pc 00775457  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.103   331   331 F DEBUG   :     #06 pc 0075780d  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.103   331   331 F DEBUG   :     #07 pc 00757855  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.103   331   331 F DEBUG   :     #08 pc 0075794d  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.103   331   331 F DEBUG   :     #09 pc 00757d0f  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.105   331   331 F DEBUG   :     #15 pc 00257b13  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.105   331   331 F DEBUG   :     #16 pc 0063f655  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.105   331   331 F DEBUG   :     #17 pc 0063f8d3  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.105   331   331 F DEBUG   :     #18 pc 006342e3  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.106   331   331 F DEBUG   :     #19 pc 0063c49f  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.106   331   331 F DEBUG   :     #20 pc 0063c591  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.106   331   331 F DEBUG   :     #21 pc 0063c81d  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.106   331   331 F DEBUG   :     #22 pc 0063fc19  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.106   331   331 F DEBUG   :     #23 pc 006390c9  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.106   331   331 F DEBUG   :     #24 pc 0008c0e9  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.107   331   331 F DEBUG   :     #25 pc 0008c4eb  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so\r\n04-10 14:37:39.107   331   331 F DEBUG   :     #26 pc 0008579f  /data/app/cmmc.com.styletransfer-2/lib/arm/libtensorflow_inference.so (Java_org_tensorflow_Session_run+650)\r\n04-10 14:37:39.107   331   331 F DEBUG   :     #27 pc 0056e0c7  /data/app/cmmc.com.styletransfer-2/oat/arm/base.odex (offset 0x391000) (byte[] org.tensorflow.Session.run(long, byte[], long[], long[],\r\nint[], long[], int[], long[], boolean, long[])+306)\r\n\r\nI stylize image crashed((the image size is 576x768 or more bigger, but 384x512 not crash)\r\nThe Model based on A Neural Algorithm of Artistic Style(Stylize) paper\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  **Make sure you also include the exact command if possible to produce  the output included in your test case**.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nIn particular, please include the git version you've compiled from any any other information you can provide to reproduce the problem (have you written custom code, or is this just using the demo application)?", "@hexujun Have you made any modifications to the code? The sizes you list are invalid -- the demo by default only processes square images.", "Closing; please respond with additional details if issue persists.", "@hexujun \r\npls contact me , my qq is 604173084 . we are doing the same thing and facing the same issue"]}, {"number": 9095, "title": "Problem with seq2seq models in prediction", "body": "@mrry Hi, I am confused with a prediction problem implemented in TensorFlow seq2seq. ", "comments": ["Thanks for reaching out. We try to keep this issue tracker limited to bugs and feature requests. We recommend posting on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support. We have this policy so that guys like @mrry will have as much time as possible to make TensorFlow better for everyone. Thank you for your understanding."]}, {"number": 9094, "title": "Uncaught TypeError: Cannot read property 'toString' of undefined when use TensorBoard projector", "body": "- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes. I was training a tree based network using Tensorflow Fold to train word embeddings for SQL parse tree. But this shouldn't be a problem because the TensorBoard can correctly read the checkpoint file.\r\n- *TensorFlow installed from (source or binary)?*: binary (installed using pip)\r\n- *TensorFlow version*: 1.0.0\r\n- *Bazel version (if compiling from source)*: N/A\r\n- *CUDA/cuDNN version*: N/A\r\n- *GPU Model and Memory*: CPU only\r\n- *Exact command to reproduce*:\r\n\r\n  * Open the log folder [tflogs.zip](https://github.com/tensorflow/tensorflow/files/908728/tflogs.zip) using TensorBoard (Seems the absolute path is hardcoded in the checkpoint file, the absolute path should be `/tmp/workspace/tflogs`):\r\n`tensorboard --logdir=tflogs`\r\n  * Switch to embedding tab\r\n  * Enable 3D label on the top left corner of the projector\r\n\r\n### Describe the problem clearly\r\nBefore enabling 3D label\r\n![image](https://cloud.githubusercontent.com/assets/1519759/24842868/0194f2ec-1d6b-11e7-8786-9f83737d99d0.png)\r\n\r\nAfter enabling 3D label\r\n![image](https://cloud.githubusercontent.com/assets/1519759/24842880/18ed6f32-1d6b-11e7-9cfc-81281a074217.png)\r\n\r\n\r\nExpected result: label shown\r\nActual result: the projector becomes blank, while the following error shown in the js console:\r\n```\r\nUncaught TypeError: Cannot read property 'toString' of undefined\r\n    at ProjectorScatterPlotAdapter.getLabelText (tf-tensorboard.html:20587)\r\n    at ProjectorScatterPlotAdapter.generate3DLabelsArray (tf-tensorboard.html:20582)\r\n    at ProjectorScatterPlotAdapter.createVisualizers (tf-tensorboard.html:20613)\r\n    at ProjectorScatterPlotAdapter.set3DLabelMode (tf-tensorboard.html:20245)\r\n    at HTMLElement.<anonymous> (tf-tensorboard.html:24813)\r\nProjectorScatterPlotAdapter.getLabelText @ tf-tensorboard.html:20587\r\nProjectorScatterPlotAdapter.generate3DLabelsArray @ tf-tensorboard.html:20582\r\nProjectorScatterPlotAdapter.createVisualizers @ tf-tensorboard.html:20613\r\nProjectorScatterPlotAdapter.set3DLabelMode @ tf-tensorboard.html:20245\r\n(anonymous) @ tf-tensorboard.html:24813\r\n```", "comments": ["CC @dandelionmane @jart", "Thank you for reporting, and thank you for the repro data and tracebacks!\r\n\r\nI've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/71."]}, {"number": 9093, "title": "Update input.py", "body": "When the input tensor of the tf.train.string_input_producer() is not fully defined (as a tf.placeholder(tf.string, ...)), then the element_shape is set as '[ ]'. There will raise a ValueError in line 150 \"element_shape = input_tensor.get_shape()[1:].merge_with(element_shape)\", since the input of the element is '[ ]', shapes will be not compatible. So I suggest the default 'element_shape' value should be set as 'None'.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "It looks like this was added intentionally at 117583214. @mrry care to comment?", "If you look at the docstring for `tf.train.string_input_producer()`, `string_tensor` is specified as being 1-D, so the element shape is always `[]`. Switching it to `None` would inhibit later shape inference, for no apparent benefit.\r\n\r\nI'm not sure what the original error message is, so it's possible that there's something we could improve, but this isn't the way to do it. Can you share more details or point to a relevant issue?", "Hi mrry, thanks for your reply. I got the original error message when I try to feed a *string tensor* to a *tf.train.string_input_producer()*:\r\n\r\n```python\r\ninput_placeholder = tf.placeholder(tf.string, shape=[None])\r\nname_queue = tf.train.string_input_producer(input_placeholder)\r\n...\r\n\r\nwith tf.Session() as sess:\r\n        \r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        ....\r\n        images = ['1.jpg', '2.jpg', ....]\r\n        sess.run(predict,  feed_dict={input_placeholder:images})\r\n\r\n```\r\nThe program will got the *InvalidArgumentError*, which seems the input images can not be passed to initialized the queue of file names.\r\n\r\nAt first, I think it is because of the shapes are not compatible, so I suggest the default 'element_shape' value should be set as 'None'. Finally I found this not the root of the problem. The non-compatible problem is due to my silly code which set the wrong placeholder shape as *tf.placeholder(tf.string, shape=[None,1])* .....\r\n\r\nCome back to the *InvalidArgumentError* problem, I found it is because when *tf.train.string_input_producer* is called, a queue is created and return back, and implicitly, a *enqueue_many* operation is added into the queue_runner, combined with the queue, as follow:\r\n\r\n```python\r\n    enq = q.enqueue_many([input_tensor])\r\n    queue_runner.add_queue_runner(\r\n            queue_runner.QueueRunner(\r\n            q, [enq], cancel_op=cancel_op))\r\n```\r\nSo in the session module, when tf.train.start_queue_runners(coord=coord) is called, the string_name queue is run as a independent thread, when this thread excute *enqueue_many* operation, there will be a *InvalidArgumentError* problem, since there is no feed_dict.\r\n\r\nSo, as a summary, I think if the  *tf.train.string_input_producer()* is designed to only accept constant tensors (or return from *tf.train.match_filenames_once()*), this is OK. You can just close this pull request :sweat_smile:. If it is also want to accept input from *tf.placeholder*, there maybe need some change...\r\n\r\nThanks for your patience :smile:\r\n", "If I understood your comment correctly, @MartinMoon, I think the next action here is to close this PR without merging. The queue-based input producer methods and placeholders are not really compatible, because the queue runners that get set up in the background do not know how to feed the placeholders, and it would require major API changes to reconcile them."]}, {"number": 9092, "title": "problem with wide_n_deep_tutorial.py on Tensorflow 1.0", "body": "Using Python 3.6.0 (Anaconda x64), Tensorflow 1.0, macOS Sierra version 10.12.4, I get the following error:\r\n\r\n> \r\n> >  python wide_deep.py\r\n> Training data is downloaded to /var/folders/h2/727s56vx40s_6n2z9ldl6kx00000gs/T/tmp76m50o3h\r\n> Test data is downloaded to /var/folders/h2/727s56vx40s_6n2z9ldl6kx00000gs/T/tmpwzhof_zb\r\n> model directory = /var/folders/h2/727s56vx40s_6n2z9ldl6kx00000gs/T/tmpclbsc2wm\r\n> Traceback (most recent call last):\r\n>   File \"wide_deep.py\", line 234, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"/Users/CBrauer/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"wide_deep.py\", line 197, in main\r\n>     FLAGS.train_data, FLAGS.test_data)\r\n>   File \"wide_deep.py\", line 185, in train_and_eval\r\n>     m = build_estimator(model_dir, model_type)\r\n>   File \"wide_deep.py\", line 132, in build_estimator\r\n>     fix_global_step_increment_bug=True)\r\n> TypeError: __init__() got an unexpected keyword argument 'fix_global_step_increment_bug'\r\n> > \r\n\r\nCharles", "comments": ["Thanks for pointing this out. It seems that the tutorial code on the website is linking to the latest version instead of the version compatible with release 1.0.\r\n\r\nCould you try with the tutorial version from the 1.0 branch?\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/learn/wide_n_deep_tutorial.py\r\n\r\nI'll try to look into how to make sure that the links in the website point to the appropriate release branch."]}, {"number": 9091, "title": "Memory Leak Running simple feed_dict graph", "body": "In a series simple tensorflow programs I obtain memory leaks (unbounded growth of CPU memory).\r\nOn original program on a computer with 64GB of RAM this leak is about 640 megabytes per hour (1% of total memory).\r\n\r\n### Plots of computer's memory over time:\r\n#### Long time scale picture:\r\n![unknown-1](https://cloud.githubusercontent.com/assets/1411079/24841025/1064f380-1d2f-11e7-8738-943be211bb1b.png)\r\n####  short time scale picture:\r\n![unknown](https://cloud.githubusercontent.com/assets/1411079/24841026/12d78984-1d2f-11e7-8efd-62bd8e46b46c.png)\r\n\r\n\r\n## Problem description\r\n\r\nThe original program was more advanced and included RNNs/Saving/Loading etc.. but I \"narrowed it down\" to a simple for loop with no gradient descent where memory grows over time without bound. \r\nTested on Fedora 25 and Mac OSX 10.11.5. Issue occurs when running on single GPU (Titan X Pascal) or on CPU. Varying the sizes of the variables in the graph only changes the degree of growth, but does not prevent the effect from occurring. This issue occurs on tensorflow 0.12 and on current tensorflow 1.0.1. No custom code was used. Tensorflow was installed using pip in both cases (pre-compiled binary. Each time this was `pip3 install tensorflow-gpu`). Using CUDA 8.0,  CuDNN v5 [though this should not impact the use-case, since no cudnn kernels are being used]. GPU is a Titan X Pascal 12GB of VRAM (not Titan Xp).\r\n\r\n## To reproduce:\r\n\r\n```\r\nimport argparse\r\nimport psutil\r\n\r\nfrom os import getpid\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef fc(inputs, output_size):\r\n    with tf.variable_scope(\"FC\"):\r\n        input_size = inputs.get_shape()[-1].value\r\n        W = tf.get_variable(\"W\", shape=[input_size, output_size])\r\n        b = tf.get_variable(\"b\", shape=[output_size], initializer=tf.constant_initializer(0))\r\n        out = tf.nn.xw_plus_b(inputs, W, b)\r\n    return out\r\n\r\ndef create_model(input_size, output_size):\r\n    # model placeholders:\r\n    with tf.variable_scope(\"Inputs\"):\r\n        input_placeholder = tf.placeholder(\r\n            tf.float32, [None, input_size], name=\"input_placeholder\"\r\n        )\r\n    # meaningless function of inputs\r\n    op = tf.reduce_mean(tf.reduce_sum(fc(input_placeholder, output_size), 1))\r\n    return input_placeholder, op\r\n\r\ndef parse_args(args=None):\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--max_epochs', type=int, default=1000)\r\n    parser.add_argument('--batch_size', type=int, default=7000)\r\n    parser.add_argument('--input_size', type=int, default=100)\r\n    parser.add_argument('--output_size', type=int, default=100)\r\n    parser.add_argument('--device', type=str, default=\"gpu:0\")\r\n    return parser.parse_args(args=args)\r\n\r\ndef create_batches(inputs, input_size, batch_size, n):\r\n    batches = []\r\n    for i in range(n):\r\n        X = np.random.uniform(-1.0, 1.0, size=(batch_size, input_size))\r\n        batches.append({inputs: X})\r\n    return batches\r\n\r\ndef main():\r\n    args = parse_args()\r\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\r\n    np.random.seed(1234)\r\n    process = psutil.Process(getpid())\r\n\r\n    with tf.Session(config=session_conf) as session, tf.device(args.device):\r\n        inputs, op = create_model(args.input_size, args.output_size)\r\n        session.run(tf.global_variables_initializer())\r\n        batches = create_batches(inputs, args.input_size, args.batch_size, 20)\r\n\r\n        for epoch in range(args.max_epochs):\r\n            before = process.memory_percent()\r\n            for feed_dict in batches:\r\n                session.run(op, feed_dict)\r\n            after = process.memory_percent()\r\n            print(\"MEMORY CHANGE %.4f -> %.4f\" % (before, after))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nOutput will be (exact numbers are percentages of computer's ram, so should change based on hardware, but main point is that memory continues to grow when the program has no variation between graph runs, batches are all the same size, no randomness is left in the program, etc.):\r\n\r\n```\r\nMEMORY CHANGE 1.2427 -> 1.3101\r\nMEMORY CHANGE 1.3101 -> 1.3103\r\nMEMORY CHANGE 1.3103 -> 1.3104\r\nMEMORY CHANGE 1.3104 -> 1.3106\r\nMEMORY CHANGE 1.3106 -> 1.3108\r\nMEMORY CHANGE 1.3108 -> 1.3108\r\nMEMORY CHANGE 1.3108 -> 1.3108\r\n...\r\nMEMORY CHANGE 1.3108 -> 1.3109\r\n...\r\nMEMORY CHANGE 1.3109 -> 1.3110\r\n...\r\n```\r\n\r\nHow can I fix this? I currently suspect a CPU memory pool issue inside tensorflow since the problem is fairly generic, and does not depend on the ops inside the graph (much). From what I've gathered most likely candidate is the `tf.asarray`/copying of numpy arrays in `feed_dict`, leading to memory fragmentation etc. Supposing this were the case, I've heard that `tcmalloc` should alleviate this, but no dice (note: I've also checked that `objgraph` shows no growth in program over time).\r\n\r\n", "comments": ["Thank you @JonathanRaiman for putting a lot of thought into communicating this issue. You've indicated you suspect there is a memory leak in the C++ code. In that case, @zhifengc might be able to advise you on how to troubleshoot down to the precise bug, or perhaps look into it himself.", "@jart Terrific. Thanks!", "I have a similar issue and stumbled upon this report. I used the code supplied by @JonathanRaiman  to quickly try and test what exactly (which instruction) is causing my issue.\r\n\r\nAfter a lot of different tests, evaluating different ops, I got the following code that reliably reproduces this problem:\r\n\r\n```\r\nimport argparse\r\nimport psutil\r\n\r\nfrom os import getpid\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef create_model(input_size, output_size):\r\n    # model placeholders:\r\n    shape = tf.clip_by_value(tf.cast(tf.random_normal([2]) * 38.0 + 64.0, tf.int32), 38, 120)\r\n    shape = tf.concat([[1], shape, [512]], axis=0)\r\n\r\n    return tf.ones(shape, dtype=tf.int32)\r\n\r\ndef parse_args(args=None):\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--max_epochs', type=int, default=10000)\r\n    parser.add_argument('--batch_size', type=int, default=7000)\r\n    parser.add_argument('--input_size', type=int, default=100)\r\n    parser.add_argument('--output_size', type=int, default=100)\r\n    parser.add_argument('--device', type=str, default=\"gpu:0\")\r\n    return parser.parse_args(args=args)\r\n\r\ndef main():\r\n    args = parse_args()\r\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\r\n    np.random.seed(1234)\r\n    process = psutil.Process(getpid())\r\n\r\n    with tf.Session(config=session_conf) as session, tf.device(args.device):\r\n        op = create_model(args.input_size, args.output_size)\r\n        session.run(tf.global_variables_initializer())\r\n        before = process.memory_percent()\r\n\r\n        for epoch in range(args.max_epochs):\r\n            session.run(op)\r\n            \r\n            if epoch % 100 == 0:\r\n                after = process.memory_percent()\r\n                print(\"MEMORY CHANGE %.4f -> %.4f\" % (before, after))\r\n                before = after\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThe tf.ones(shape, dtype=tf.int32) instruction causes the issue. Same with tf.zeros, tf.ones_like and tf.zeros_like. But the interesting part is, this ONLY happens with dtype=tf.int32, it doesn't happen for int64, int16, int8, uints, floats.\r\n\r\nAnother observation is that, while the reported memory usage by python on the first run is roughly the same for all those data types, the memory usage in the XFCE Task manager is more than twice as high for the int32 variant than for other datatypes. So it seems like python is incorrectly reporting memory usage when tf.int32 is used.\r\n\r\nExamples (first bump is int64 with no growth, second bump is int32 with fast growth):\r\n\r\n![selection_001](https://cloud.githubusercontent.com/assets/664486/26070693/cc62a25c-39a5-11e7-9233-b765f16d3813.png)\r\n\r\nPlease also note that the memory usage increases rather quickly (from 2% memory to 8% memory in 10'000 interations, which takes about 10-15 seconds) and that having multiple tf.ones instruction makes it go up even faster, which can have a pretty noticeable effect on larger and more complex models.\r\n\r\nBut this only happens when the input-dimensions are random. If the shape supplied to tf.ones is the same on every run, memory used does not increased. So it only affects variable sized tensors.\r\n\r\nAlso, tf.cast(tf.ones(shape, dtype=tf.int64), tf.int32) works fine\r\n\r\nI'm not 100% sure this is the same issue as @JonathanRaiman's, since he's not using int32 as far as I can tell, but his example does have a \"None\" dimension and the behaviour looks exactly the same.\r\n\r\nAnd I'm using tensorflow built from master yesterday, though the problem also existed in 1.1, on Arch Linux\r\n", "@Panaetius My original memory leak problem arised in code that had several int32 tensors being fed in with varying sizes (inputs for embedding lookup tables that have varying numbers of time steps going into RNNs). This sounds like it might be the same issue", "This is becoming problematic for me as well. I also use non-fixed input dimensions.\r\n\r\nRelated: \r\nhttps://github.com/tensorflow/tensorflow/issues/8560\r\nhttps://stackoverflow.com/questions/42861956/gpu-poolallocator-explodes-the-cpu-memory", "@jart @zhifengc Have these code snippets helped illuminate what might be the source for the leak? Is there any other information we could provide to help fix this?", "@JonathanRaiman I'm facing the same problem and I also suspect that it is due to the copying of numpy arrays in `feed_dict`", "There is indeed different treatment for `dtype=tf.int32 ` for `tf.zeros`, `tf.zeros_like`, `tf.ones`, `tf.ones_like`.\r\n\r\nThese ops are defined in [tensorflow/python/ops/array_ops.py](https://github.com/tensorflow/tensorflow/blob/96f334b0d6ec61f5037b8ddd1860a9aa7874efef/tensorflow/python/ops/array_ops.py#L1433).\r\n\r\nThey all call `fill` function defined in [tensorflow/core/kernels/constant_op.cc](https://github.com/tensorflow/tensorflow/blob/96f334b0d6ec61f5037b8ddd1860a9aa7874efef/tensorflow/core/kernels/constant_op.cc#L168).\r\n\r\nThe `FillOp` is [registered as usual](https://github.com/tensorflow/tensorflow/blob/96f334b0d6ec61f5037b8ddd1860a9aa7874efef/tensorflow/core/kernels/constant_op.cc#L251) for all types except int32:\r\n\r\n```\r\n// A special GPU kernel for int32.\r\n// TODO(b/25387198): Also enable int32 in device memory. This kernel\r\n// registration requires all int32 inputs and outputs to be in host memory.\r\nREGISTER_KERNEL_BUILDER(Name(\"Fill\")\r\n                            .Device(DEVICE_GPU)\r\n                            .TypeConstraint<int32>(\"T\")\r\n                            .HostMemory(\"dims\")\r\n                            .HostMemory(\"value\")\r\n                            .HostMemory(\"output\"),\r\n                        FillOp<CPUDevice, int32>);\r\n```\r\n\r\n`ConstantOp` in the same file also has this special treatment.\r\n\r\n*REGISTER_KERNEL_BUILDER macro is defined in tensorflow/core/framework/op_kernel.h.\r\n", "There may be a relation to [issue 13221](https://github.com/tensorflow/tensorflow/issues/13221).", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Thanks for pointing out that #13221 is likely a duplicate of this. That issue has gained a lot of attention from the development team, with information on what should happen, and we're hoping someone in the community will volunteer to be the one to solve it. Please follow the other issue from here out.", "Appears to be fixed in tf 1.6", "No, not yet. I still found this problem in tf 1.8. ", "I second @hbb21st , I'm facing the same (or very similar) issue in tensorflow 1.8, running a loop of `sess.run` passing via feed dict just a string with the filename to read:\r\n\r\n```\r\nfor image in glob(os.path.join(validationset_path, \"*.png\")):\r\n    embed = sess.run(latent, feed_dict={filename_: image})\r\n``` \r\n\r\nAfter some iteration:\r\n\r\n```\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:967] failed to alloc 20288458752 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\nW ./tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 20288458752\r\n```", "I'm getting a similar problem. I'm generating batches manually from different .h5 files.\r\n```python\r\nwith sess.as_default():\r\n\tfull_start = clock()\r\n\tfor i in range(100):    #epochs\r\n\t\tstart = clock()\r\n\t\tbatch_train_size = 512\r\n\t\tbatch_test_size = 200\r\n\t\tstart_train_index =0\r\n\t\tend_train_index = start_train_index+batch_train_size\r\n\t\tstart_test_index = 0\r\n\t\tend_test_index = start_test_index+batch_test_size\r\n\t\tfor j in range(int(ceil(float(train_len)/batch_train_size))):\t\t\r\n\t\t\tif start_train_index  >= train_len:\r\n\t\t\t\tstart_train_index = 0\r\n\t\t\tif start_test_index >= test_len:\r\n\t\t\t\tstart_test_index = 0\r\n\t\t\tend_train_index = start_train_index+batch_train_size\r\n\t\t\tend_test_index = start_test_index + batch_test_size\r\n\t\t\tif end_train_index >= train_len:\r\n\t\t\t\tend_train_index = train_len\r\n\t\t\tif end_test_index >= test_len:\r\n\t\t\t\tend_test_index = test_len\r\n\t\t\tprint 'epoch:',i+1,'/100 batch_num:',j+1,'/19'\r\n\t\t\tx_train,y_train,x_test,y_test = loadData(start_train_index,end_train_index,start_test_index,end_test_index)\r\n\t\t\tstart_train_index = end_train_index\r\n\t\t\tstart_test_index = end_test_index\r\n                        print x_train.shape,x_test.shape\r\n\t\t\ttrain_step.run(feed_dict={X_train: x_train,\r\n\t                                  labels: y_train})\r\n```\r\nloadData function returns padded input features of different videos from .h5 files\r\n\r\nAfter a few batches \r\n```\r\n2018-07-06 18:04:56.278757: W ./tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 7730940928\r\nKilled\r\n```\r\nCan someone suggest a way to load batches manually and not exhaust memory \r\n@JonathanRaiman @hbb21st Please guide me if you solved your error", "@Satcheel were you able to solve this issue? I am getting a similar memory leak when I change my feed_dict to extract specific tensor values in a single session. Any help in this regards would be really helpful! Many thanks!", "Sorry, I am not very active on GitHub.\r\n\r\nLeaving the answer here for future reference.\r\nMy problem was not memory leak but my huge training data (250GB). So, used keras and steps_for_epoch for training and used a generator function to load data. I was trying to do the same in native tf but couldn't work it out"]}]