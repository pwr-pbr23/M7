[{"number": 12428, "title": "Train our own dataset", "body": "Can you guide me to train our own data-set for TFdetect example??? I read lots of guidelines regarding this, but those are confusing me So can anyone give me well ordered set of instructions to complete this..", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nTF detect? You mean the android demo? If so you can train using the darkflow project.", "The default detector in the demo now uses SSD Mobilenet, which can be retrained with the [TF Object Detection API](https://github.com/tensorflow/models/tree/master/object_detection).\r\n\r\n(The alternate YOLO detector may be retrained using darkflow as suggested)."]}, {"number": 12427, "title": "Error when retraining with retrain.py", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: original one\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.10\r\n- **TensorFlow installed from (source or binary)**: `sudo pip3 install tensorflow`\r\n- **TensorFlow version (use command below)**: latest, `sudo pip3 install tensorflow` before retraining, `v1.0.0-65-g4763edf-dirty 1.0.1`\r\n- **Python version**: Python 3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\n- **Bazel version (if compiling from source)**: not used\r\n- **CUDA/cuDNN version**: not used\r\n- **GPU model and memory**: not used\r\n- **Exact command to reproduce**:\r\n\r\nI know it's Ubuntu 16.10, but mb the problem in something different (that you may already know)?\r\n\r\n### Describe the problem\r\nError when retraining mobilenet\r\n\r\n### Source code / logs\r\n\r\n```\r\n~$ python ~/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py \\\r\n>     --image_dir ~/Desktop/train_tf/images/ \\\r\n>     --learning_rate=0.0005 \\\r\n>     --testing_percentage=20 \\\r\n>     --validation_percentage=20 \\\r\n>     --train_batch_size=32 \\\r\n>     --validation_batch_size=-1 \\\r\n>     --flip_left_right True \\\r\n>     --random_scale=30 \\\r\n>     --random_brightness=30 \\\r\n>     --eval_step_interval=100 \\\r\n>     --how_many_training_steps=10000 \\\r\n>     --architecture mobilenet_1.0_128_quantized\r\nINFO:tensorflow:Looking for images in 'road'\r\nINFO:tensorflow:Looking for images in 'car'\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]\r\nTraceback (most recent call last):\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 1058, in main\r\n    distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\r\n  File \"/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 599, in get_random_distorted_bottlenecks\r\n    {resized_input_tensor: distorted_image_data})\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]\r\n\r\nCaused by op 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise', defined at:\r\n  File \"/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 984, in main\r\n    create_model_graph(model_info))\r\n  File \"/home/osboxes/Desktop/train_tf/tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 282, in create_model_graph\r\n    model_info['resized_input_tensor_name'],\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 288, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/osboxes/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): NodeDef mentions attr 'data_format' not in Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=[\"SAME\", \"VALID\"]>; NodeDef: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise = DepthwiseConv2dNative[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_0/Relu6, MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read/_79__cf__79)]]\r\n\r\n```\r\n", "comments": ["installed fresh with 16.04, seems works ok"]}, {"number": 12426, "title": "tf.losses.log_losses calculation may be incorrect", "body": "https://github.com/tensorflow/tensorflow/blob/a0bbfa6afec617697576210fb22f3ea9c3a57b61/tensorflow/python/ops/losses/losses_impl.py#L409\r\n\r\nhere we are only considering one extreme that the value of prediction can be \"zero\", missing out probably that it can be \"one\" also.\r\n\r\nIf the variables predictions value is 1 then this would return a log loss of \r\n-log(1+1e-7) = -4.3429446044209946e-08\r\nwherein if such occurs multiple times than it can indicate a lower loss.\r\n\r\nI suggest clipping the value between [ 0.0000001 , 0.9999999 ] ", "comments": []}, {"number": 12425, "title": "Feature: TF on mix environnment CPU-GPU", "body": "When using TF-GPU, either switches to GPU depending on GPU availability.\r\n\r\nWhen we have a mixed environnment,\r\nCPU +GPU, it would be better to distribute across CPU and GPU available,\r\nspecify the core available to TF for full usage (keeping one for centralized processing).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["If what I understand is right, this requested feature is already implemented in TensorFlow. GPU version of TensorFlow always assume the \"mixed environment of CPUs and GPUs\", and it is very natural because GPU-only environment doesn't exist. ", "Yes, it seems not using available CPU when GPU is present.\nDo we need to explicitely write distributed code like this ?\n\nfor i in range(num_cpus):\n     with tf.device('/cpu:'+str(i)):\n         outputs.append(tf.matmul(inputs[i], b))\n\n\n\n\n\nChangho Hwang wrote:\n>\n> If what I understand is right, this requested feature is already \n> implemented in TensorFlow. GPU version of TensorFlow always assume the \n> \"mixed environment of CPU and GPUs\", and it is very natural because \n> GPU-only environment doesn't exists.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub \n> <https://github.com/tensorflow/tensorflow/issues/12425#issuecomment-323574987>, \n> or mute the thread \n> <https://github.com/notifications/unsubscribe-auth/AR10pxS50Z30XFU0erhHqfRJ41IJGyBlks5saAJfgaJpZM4O8j4e>.\n>\n", "TensorFlow uses every available CPUs as if they are unified all together as single CPU, which means that there is no such device like `'/cpu:1'` or `'/cpu:2'`, only `'/cpu:0'`is legal. You cannot distinguish each CPU over TensorFlow as far as I know, and maybe, you don't need to in most cases. You can just write:\r\n``` python\r\nwith tf.device('/cpu:0'):\r\n    for i in range(num_cpus):\r\n        outputs.append(tf.matmul(inputs[i], b))\r\n```\r\nthen it will use all available CPUs automatically.", "Even this is GPU (TF GPu) is available,\nCPU will be used ?\n\nIn my case, only GPU seems being used for training.....\n\n\n\n\n\nOn 20 Aug 2017, at 23:58, Changho Hwang <notifications@github.com> wrote:\n\nTensorFlow uses every available CPUs as if they are unified all together as single CPU, which means that there is no such device like '/cpu:1' or '/cpu:2', only '/cpu:0'is legal. You cannot distinguish each CPU over TensorFlow as far as I know, and maybe, you don't need to in most cases. You can just write:\n\nwith tf.device('/cpu:0'):\n    for i in range(num_cpus):\n        outputs.append(tf.matmul(inputs[i], b))\nthen it will use all available CPUs automatically.\n\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n", "Yes, you can always use your CPUs with any version of TensorFlow. It may be much helpful for you to ask in [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) tagging `tensorflow` for further issues.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12424, "title": "Update documentation for contrib", "body": "-Add small details to documentation\r\n-Correct styling for documentation", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ringw, @martinwicke and @vrv to be potential reviewers.", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 12423, "title": "can i used it by C#?", "body": "can i used it by C#?\r\nWhat time can provide C # version\uff1f\r\nWhy isn't the official c # demo?\r\nthanks", "comments": ["The TensorFlow project owners maintain language bindings in a few languages, C# is not one of them (see https://www.tensorflow.org/api_docs/). However, we do encourage the community to build and maintain bindings for other languages. You may find https://github.com/migueldeicaza/TensorFlowSharp to be of interest.", "@asimshankar Why is there no c # version?It is difficult to?", "As you can imagine, we don't have enough cycles to do all that we'd like to. So we try to provide enough that can be extended by others in the community and appreciate efforts like TensorFlowSharp.", "@asimshankar I hope to support the c # version, after all, the usage rate is very high, and I hope that the team of tensorflow should not reject c #. Thank you"]}, {"number": 12422, "title": "[Bug] bazel test kernel_tests:scatter_ops_test on master branch, got ImportError: No module named autograd.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**:\r\n2.7 \r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\n8.0/5.1.10\r\n- **GPU model and memory**:\r\nM40\r\n- **Exact command to reproduce**:\r\nbazel test -c opt --verbose_failures //tensorflow/python/kernel_tests:scatter_ops_test\r\n\r\n\r\n### Describe the problem\r\nI have clone the source and run kernel test with the  above command, got following error.\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1                                     \r\n-----------------------------------------------------------------------------   \r\nTraceback (most recent call last):                                              \r\n  File \"/root/.cache/bazel/_bazel_root/0ccdc6677c64d2c9b51856e8acbdd6d3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/scatter_ops_test.py\", line 23, in <module>\r\n    from tensorflow.python.framework import constant_op                         \r\n  File \"/root/.cache/bazel/_bazel_root/0ccdc6677c64d2c9b51856e8acbdd6d3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py\", line 48, in <module>\r\n    from tensorflow.python.eager import execute                                 \r\n  File \"/root/.cache/bazel/_bazel_root/0ccdc6677c64d2c9b51856e8acbdd6d3/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py\", line 21, in <module>\r\n    from autograd import core as ag_core                                        \r\nImportError: No module named autograd\r\n```\r\n\r\nThen, I check out  branch v1.3, it is ok.\r\n\r\n", "comments": ["Try running\r\n```\r\nsudo pip install autograd\r\n```\r\nBuilding TensorFlow now requires autograd. When I ran the command it above on Ubuntu 14.04, it gave an error about requiring a Fortran compiler, which I installed with\r\n```\r\nsudo apt-get install gfortran\r\n```\r\n\r\n@alextp can you add the autograd dependency under the \"Prepare environment for Linux\" section in the [Installing TensorFlow from Sources](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_sources.md) guide? autograd is currently only listed on the \"Prepare environment for Mac OS\" section.", "@reedwm It works, thanks."]}, {"number": 12421, "title": "Refactor word2vec_basic", "body": "This MR changes two functions on word2vec_basic.py\r\n\r\n*  **create_dataset**: Update how the function search the dictionary for an index value.\r\n*  **generate_batch**:  Update how the context words are obtained. The function originally does that using the following code:\r\n\r\n```python\r\n    targets_to_avoid = [skip_window]\r\n    for j in range(num_skips):\r\n      while target in targets_to_avoid:\r\n        target = random.randint(0, span - 1)\r\n      targets_to_avoid.append(target)\r\n      batch[i * num_skips + j] = buffer[skip_window]\r\n      labels[i * num_skips + j, 0] = buffer[target]\r\n```\r\n\r\nMeaning that it is randomly trying to select a new word in the span and checking if the word is valid to use. If it is, it will be now used to create a batch. I believe that a simpler approach is:\r\n\r\n```python\r\n   target = skip_window  # target label at the center of the buffer\r\n    context_words = [w for w in range(span) if w != target]\r\n    random.shuffle(context_words)\r\n    words_to_use = collections.deque(context_words)\r\n    for j in range(num_skips):\r\n      batch[i * num_skips + j] = buffer[target]\r\n      context_word = words_to_use.pop()\r\n      labels[i * num_skips + j, 0] = buffer[context_word]\r\n```\r\n\r\nIn this approach, an array of valid context words are created without randomly selecting each valid word. This valid context words are shuffled and added to a deque, to make word removal less costly.\r\n\r\nI believe that this modifications make both **create_dataset** and **generate_batch** a little simpler to understand, and although this is not the intention of this MR, but a little faster as well.                                    ", "comments": ["Can one of the admins verify this patch?", "@lucasmoura, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @jhseu and @vrv to be potential reviewers.", "Thanks! Your version is easier to understand.", "@jhseu I think I have fixed the issues in the code. Also, I deleted the **target** variable, which I think is a better approach for the new code. If that is not the case, I can bring it back and change its name.", "Jenkins, test this please"]}, {"number": 12420, "title": "ValueError: No attr named '_XlaCompile' and AttributeError: 'NoneType' object has no attribute 'back_prop' with tf.while_loop and templates", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: 1.2, 1.3\r\n- **Python version**: 3.4\r\n- **CUDA/cuDNN version**: 5, 6\r\n- **GPU model and memory**: GTX 1080 Ti\r\n\r\n### Describe the problem\r\n\r\nWhen I run `tf.train.Optimizer().minimize(loss)` I receive the error message below. The code is a bit unwieldy, so I just provide the overall structure of my code:\r\n\r\n```\r\nx = { ... }\r\ndef some_template(..):\r\n  bar(x[..])\r\ndef foo(..):\r\n  return foobar(x[..])\r\ntemplate = tf.make_template(\"my_template\", some_template)\r\ndef loop_body(inputs, ..):\r\n  for i in range(x[..]):\r\n    net = foo(inputs)\r\n    net = template(net)\r\n  return inputs, ..\r\nnet, *_ = tf.while_loop(cond, loop_body, vars)\r\n```\r\n\r\nThe functions and templates that are called in the loop body access some Python variables from the outer scope (read-only, i.e. without side-effects), and they are, of course, expected to be constant at run-time. The graph construction seems to work perfectly fine. Only when I construct the first minimization operation, it fails at the first graph node that makes use of a variable (`Model/ModuleB_0/b_46/` in this case). One thing that might be unusual about my graph is that it uses an earlier part of the graph as target for the outputs, so the loss is defined as a function of `output_node` and `tf.stop_gradient(earlier_node)`. What might maybe also be interesting is that the `frame_name` below contains the same path concatenated twice `Model/ModuleB_0/while/Model/ModuleB_0/while/`. I also have some `trainable=True` variables which are not in the subgraph of the training op, but they are not in the `var_list` argument for the training op, so that should be fine. Is this a bug or am I doing something wrong?\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.\r\ny\", line 343, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\",\r\nine 1705, in get_attr\r\n    str(self._node_def))\r\nValueError: No attr named '_XlaCompile' in name: \"Model/ModuleB_0/while/Level_4/BottomRight/Conv2D\r\nayer_2/BiasAdd/Enter\"\r\nop: \"Enter\"\r\ninput: \"Model/ModuleB_0/b_46/read\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"Model/ModuleB_0/while/Model/ModuleB_0/while/\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 10\r\n  }\r\n}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 98, in <module>\r\n    tf.app.run()\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", li\r\nne 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 61, in main\r\n    model = Model(tf.flags.FLAGS.__flags)\r\n  File \"/media/other_dir/me/Code/model/model.py\", line 193, in __init__\r\n    scope=\"Model/ModuleA_%i\" % i)\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\r\ny\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\r\ny\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\r\ny\", line 542, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\r\ny\", line 348, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\r\ny\", line 542, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_gra\r\nd.py\", line 208, in _EnterGrad\r\n    if not grad_ctxt.back_prop:\r\nAttributeError: 'NoneType' object has no attribute 'back_prop'\r\n\r\n```", "comments": ["Does it work without XLA?", "The XLA JIT compiler is disabled by default, right? I did not enable it. To be sure I set `sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.OFF`, but the error still occurs.", "It seems the problem was that I defined several losses within the loop body of a while loop and added them to `tf.GraphKeys.LOSSES` collection. I did not realize at that point that `tf.add_to_collection` is not a dynamic graph operation. It is not even a graph operation after all.", "Aha! Yeah, changing the graph during execution should be avoided, I believe. Good thing you solved it. :+1: \r\n\r\nDon't forget to close the issue.", "Excuse me but please explain the changing graph? I have the same error.\r\nWhat you mean by changing the graph during execution?"]}, {"number": 12419, "title": "Non-fused batch norm with NCHW is mush slower than with NHWC", "body": "### Describe the problem\r\nI noticed that in my environment, non-fused batch norm with \"NCHW\" format  run significantly slower \r\n### Environment info\r\n```bash\r\nEnvironment\r\n\r\nGPU: 8x NVIDIA\u00ae Tesla\u00ae M40/P40\r\nOS: Ubuntu 16.04 LTS with tests run via Docker\r\nCUDA / cuDNN: 8.0 / 5.1\r\nTensorflow Version: 1.2.1\r\nDocker Images: docker pull tensorflow/tensorflow:1.2.1-gpu\r\nDataSet: Synthetic images\r\n```\r\n### Source code / logs\r\nHere is my source code\r\n```python\r\nimport time \r\nimport tensorflow as tf\r\nimport pdb \r\n\r\ndata_format = \"NCHW\"\r\nfused=False\r\nbatch_size=512\r\nnclass = 1001\r\nnum_steps = 100 \r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    images = tf.truncated_normal(\r\n            shape=[batch_size, 227, 227, 3], \r\n            dtype=tf.float32,\r\n            stddev=1e-1,\r\n            name=\"fake_images\")\r\n    images = tf.contrib.framework.local_variable(images, name='images')\r\nwith tf.device(\"/cpu:0\"):\r\n    labels = tf.random_uniform(\r\n            [batch_size],\r\n            dtype=tf.int32,\r\n            minval=1,\r\n            maxval=nclass,\r\n            name=\"fake_labels\")\r\n    labels = tf.contrib.framework.local_variable(labels, name='labels')\r\n    labels -= 1\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    images *=  1. / 256\r\n    images = tf.subtract(images, 0.5)\r\n    images = tf.multiply(images, 2.0)\r\n    if data_format == \"NCHW\":\r\n        images = tf.transpose(images, [0, 3, 1, 2])\r\n    logits = tf.contrib.layers.conv2d(images, 64, [11, 11], [4, 4],\r\n                                      padding=\"VALID\", biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\r\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\r\n\r\n    logits = tf.contrib.layers.conv2d(logits, 192, [5, 5],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\r\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\r\n\r\n    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\r\n    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.conv2d(logits, 256, [3, 3],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\r\n\r\n    logits = tf.contrib.layers.flatten(logits)\r\n    logits = tf.contrib.layers.fully_connected(logits, 4096)\r\n    logits = tf.nn.dropout(logits, 0.5)\r\n    logits = tf.contrib.layers.fully_connected(logits, 4096)\r\n    logits = tf.nn.dropout(logits, 0.5)\r\n    logits = tf.contrib.layers.fully_connected(logits, nclass, activation_fn=None)\r\n\r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            logits=logits, labels=labels, name='xentropy')\r\n    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\r\n\r\n    opt = tf.train.GradientDescentOptimizer(0.005)\r\n    training_op = opt.minimize(loss)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    train_op = tf.group(*([training_op] + update_ops))\r\n\r\n    step_train_times = []\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n\r\n        for local_step in xrange(num_steps):\r\n            batch_start = time.time()\r\n            sess.run(train_op)\r\n            step_train_times.append(time.time() - batch_start)\r\n            images_per_sec = batch_size / step_train_times[-1]\r\n            print \"local step %d, %.2f images/sec\" % (\r\n                    local_step+1, images_per_sec)\r\n\r\n```\r\n\r\nAnd my tested results:\r\n\r\n data format |  fused/non-fused batch norm  |  images/sec\r\n------------ | ------------- | --------------\r\nNHWC | fused |  1085.5 \r\nNHWC | non-fused |  969.1\r\nNCHW | fused |  1315.6\r\nNCHW | non-fused | **301.1**\r\n\r\nI test benchmarks with model resnet50 on P40 and get similar results\r\n\r\n data format |  fused/non-fused batch norm  |  images/sec\r\n------------ | ------------- | --------------\r\nNHWC | fused |  65.7 \r\nNHWC | non-fused |  51.0\r\nNCHW | fused |   84.4\r\nNCHW | non-fused | **7.5**\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI found issue https://github.com/tensorflow/tensorflow/issues/7551 similar with my problem but  with opposite result", "comments": ["@zheng-xq Can you comment or re-assign this one?", "Fused batch norm will be enabled by default soon. Is there any reason that you need the non-fused version?", "Using non-fused version is not necessary and just want to know reasons (I will update my old code to fused vserion soon)", "The reason might be related to the reduction dimension in reduce_mean: reducing along the second dimension might be slower than along the last dimension because of the memory access pattern. You can run Timeline or tf prof (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler) to find out more."]}, {"number": 12418, "title": "Feature Request: Add gradient for \"BiasAdd\" op in C++ API", "body": "Add BiasAdd gradient implementation for C++ API so it can be added to graph through AddSymbolicGradients function.", "comments": ["@dguerra I was already planning on tackling this. I may have a PR out in the next ~48 hours.", "@bpiel Ok, the strange thing is that BiasAddGrad already appears in the C++ API documentation. However it's not added properly to the graph through AddSymbolicGradients.  @suharshs do you know why this might be possible?", "@dguerra `BiasAddGrad` already exists as an op, which means the math is already implemented (the hard part). But, `BiasAddGrad` can't be used by `AddSymbolicGradients` without a bit of wrapper logic and then being registered as the gradient of `BiasAdd`.", "@dguerra PR submitted. https://github.com/tensorflow/tensorflow/pull/12448", "@bpiel Great, I see. So other neural networks gradient ops such as Conv2DBackpropFilter can be added in a similar fashion ?", "@dguerra Yes. In fact, I have the code for Conv2D (and MaxPool) in a branch already, but I've only submitted two PRs to TF ever (including this one) and I'd like to wait to get a review on those before submitting more. I don't want to pummel anyone with repeated mistakes.", "@dguerra merged!", "@bpiel  Thank you!", "Hi @bpiel , do you expect to add PR to the other ops you already have in your code?", "@dguerra Yeah, I had been waiting for my other PR to go through, but I didn't expect it to take so long. There's been test infrastructure failures.\r\nhttps://github.com/tensorflow/tensorflow/pull/12391\r\n\r\nI'll give up on that and just get the rest out. Should be in the next 24 hours.\r\nFYI, I have:\r\nConv2D\r\nMaxPool\r\nMean\r\nSum\r\n\r\n", "That'd be great :)"]}, {"number": 12417, "title": "Can't use reshape in a while_loop   InvalidArgumentError ", "body": "tf.reshape( x ,[batch_size,-1])\r\n\r\nBatch_size is a global variable. This instruction within the body of a while loop  produce the error:\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'Reshape' has inputs from different frames. The input 'while/strided_slice' is in frame 'while/while/'. The input 'Reshape/shape' is in frame ''.\r\n\r\nIt's very layered, there are many other similar instructions in the code, reshape that involve global variables, but are compiled correctly.\r\n\r\nI'm using tensorflow 1.1 on python3 installed by Anaconda.", "comments": ["Please fill out the template and include example code the reproduces the problem.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12416, "title": "Outdated Documentation (ImportError: libcudnn.so.6)", "body": "### Problem Description\r\nThe binary version for GPU provided [here](https://www.tensorflow.org/install/install_linux#python_36) on installation page of Ubuntu expects cuDNN v6. However, the documentation mentions the version of cuDNN required as 5.1.\r\n\r\nKindly rectify the error in documentation.\r\n\r\n### System information\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from binary (GPU Version)\r\n- Tensorflow version v1.3.0-rc2-20-g0787eee 1.3.0", "comments": ["I was misled by the documentation as well. Hope it'll be fixed soon.", "Even I spent quite a time to resolve the issue when I received the error that:\r\n```\r\nImportError: libcudnn.so.6 cannot open shared object file: No such file or directory\r\n```\r\n\r\nLater I found a Github thread which was in relation to porting to cuDNN 6 (#8828) and realized that the binary is based on version 6 of cuDNN.\r\n\r\nRaised the issue so that others don't face the same issue.", "I can confirm the same issue. My Linux is Amazon's `Deep Learning AMI Ubuntu Version` which only have cuDNN v5.1 installed. When trying to install with python 3.5 and \r\n`https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp35-cp35m-linux_x86_64.whl`\r\n\r\nThe same error happens:\r\n\r\n```$ python3\r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> exit()\r\n\r\n```", "Added a PR #12463 for the docs change.", "Today I got the same problem. Thanks for solving the issue. ", "I have got the same problem when installing tensorflow into python3.5.\r\nI am quite new to use this site, I may not follow what's going on.\r\nwas this issue solved? if so, how can I fix the issue?\r\nAny helps would be appreciated.\r\nThank you for your help in advance. ", "I had the same problem. \r\nIs there a way to go around the issue? ", "I have cuDNN v6.0 and still have this problem.  Is there another fix?", "@coreyhahn: Check if the symlink for the `libcudnn.so.6` is at appropriate location (usually `usr/local/cuda/lib64/` and `usr/local/cuda/include`). Also, ensure that the `LD_LIBRARY_PATH` is set.", "@coreyhahn \r\nCheck this out \r\n[After building TensorFlow from source, seeing libcudart.so and libcudnn errors\r\n](https://stackoverflow.com/a/45787225/1115215)\r\n\r\nI follow this post to make TensorFlow 1.3 and cudnn 6.0 working on my Ubuntu16.04 + NVIDIA Gtx 660", "@zrcahyx If you could build TensorFlow from source and link against v5 then the binary might be used in your server.", "@yongtang Thanks for your advice. But i can't even link against v5. I am contacting with my colleague and trying to get cudnn v6 installed on the server.", "@zrcahyx what version of tensorflow-gpu are you using ? ", "Install CUDNN6.0. Older version used 5.0. Both 5.0 and 6.0 can co exists. Just download and copy the files to /usr/local/cuda/include and /usr/local/cuda/lib64 the corresponding file. I had this issue upgrading tensorflow to 1.3.", "Hi all,\r\n\r\nAbout `ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory`. In order to solve this problem, you have to download from cuda [https://developer.nvidia.com/cuda-downloads](url) the appropriate package for your system and write in your `~/.bashrc` : \r\n`export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/path/to/your/new/cuda/lib64\"`. \r\n`usr/local/cuda/lib64` is the native path for cuda files and you want add the new files you downloaded. \r\nThat's why you write `:` followed by the new path.\r\n`\"$LD_LIBRARY_PATH` is the expression used in linux for library paths.\r\nBe careful about having the lastest version for tensorflow (1.3.0).\r\nHope it helps.", "We're still getting this error with libcudnn 6 on ubuntu 16.04.2. . We also tried v5 and v7 symlinked to 6. I tried @RaskaFR 's advice above and still no dice. ", "this is not working on ubuntu 17.04.\r\n\r\n", "I just ran into the same issue with my keras docker image... did anyone find the solution yet?", "In my case this command solved the problem\r\nsudo apt-get install libcupti-dev ", "updating to cuDNN v6 for cuda 8 solved my problem", "same problem, i only have <code>libcudnn.so.7</code>.  I edit binary file <code>python2.7/site-packages/tensorflow/libtensorflow_framework.so</code> , change <strong>libcudnn.so.6</strong> \r\n  to <strong>libcudnn.so.7</strong> . surprised, it work.\r\n\r\nSystem information\r\n* Linux OpenSUSE 42.3\r\n* Tensorflow version 1.5.0-dev20171103\r\n* cuDNN v6", "Or just `ln -s YOURS_PATH_TO_cuDNN/libcudnn.so.7.0.3 YOURS_PATH_TO_cuDNN/libcudnn.so.6`", "@wei-yuan Your post worked for me! Thanks!", "Nothing is working for me. I have tried all of the above and the command:\r\n$LD_LIBRARY_PATH/libcudnn.so.6\r\nFinds the file. What can I try next?\r\nYes, I added export to my bashrc file\r\nYes, I installed Cudnn (both 6 and 5)", "I have import error: ImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n1. download cudnn6.0 version (not latest 7.0)\r\n2.sudo rm -rf /usr/local/cuda/include/cudnn.h\r\nsudo rm -rf /usr/local/cuda/lib64/libcudnn*\r\n3. to cudnn untar folders\r\nsudo cp include/cudnn.h /usr/local/cuda/include/\r\nsudo cp lib64/lib* /usr/local/cuda/lib64/\r\n4. cd  /usr/local/cuda/lib64/\r\nsudo chmod +r libcudnn.so.6.0.21  \r\nsudo ln -sf libcudnn.so.6.0.21 libcudnn.so.6  \r\nsudo ln -sf libcudnn.so.6 libcudnn.so  \r\nsudo ldconfig  \r\nIt works for me.\r\nI think @evilkjoker may be useful, @JekaMas It's unsuccessful for me", "Solution by @guoxiaolu worked for me!! Thank you!!", "In my case, the failure only occurs when I start Python3 with `sudo`. Remove `sudo` everything is just fine.", "One thing that worked for me was that rather than using 'export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64\" ' I used 'export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:~/cuda/lib64\" ' and it worked. I have to do it everytime the computer starts. Dont know what is cuda doing in my home folder and don't know why I need to do it everytime.", "@guoxiaolu it worked for me thanks\r\n", "add into env \r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cudnn/c\r\nudnn_v6/cuda/lib64\r\n"]}, {"number": 12415, "title": "Refactor create worker sessions", "body": "Refactoring to OO style for worker group for create worker sessions, and it is more clear for implements.", "comments": ["Can one of the admins verify this patch?", "@horance-liu, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @suharshs and @saeta to be potential reviewers.", "This PR replaces a single function and 58 lines of code with two structs and 122 lines of code to do the same thing. In its current form, I wouldn't be inclined to accept it. Can you explain why it is an improvement?\r\n\r\n(The original function is quite dense, admittedly, but I think a better solution would be to add one or two comments at the appropriate points.)\r\n\r\n/cc @saeta as the original author.", "I\u2018m not familiar with google code style before, but I like \"One Level of Abstraction per Function/Class\". After modify it, every class and function will do one thing, and I will read code quickly and discover potential problems easily.\r\n\r\n I pull request it to make sure the commit criterion, and to decide more commits for this class in the future. I believe that the number of lines will decrease, because it will generate more reusable classes and functions.", "@mrry I give up this PR, it's not a successful case, and I don't satisfy the result. I will continue to commit to tensorflow. \r\n\r\nThanks. \r\n", "@horance-liu Closing as per your comment. Thank you for your contributions to TF :)"]}, {"number": 12414, "title": "tf Dataset / Iterator console flood when using CUDA builds", "body": "### System information\r\n- **Have I written custom code **: Yes\r\n- **OS Platform and Distribution **: Manjaro linux, Arch Linux repo\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version **: 1.3.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version **:\r\n- **CUDA/cuDNN version**: cuda 8.0.61, cudnn 7.0.1 & cudnn6 6.0.21\r\n- **GPU model and memory**: Nvidia 1080 GTX 8GB\r\n- **Exact command to reproduce**:\r\n\r\n### The problem\r\n\r\nWhen using a tensorflow wheel built with cuda support, my app prints the following warning message at the end of a training epoch:\r\n\r\n`2017-08-19 14:01:18.214060: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,132], [?], [?,?], [?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]`\r\n\r\nThe code trains a seq2seq model, and I assume the message gets printed somewhere downstream of seq2seq.dynamic_decode. The message still gets printed even when the NN cells are not wrapped with a tf.contrib.rnn.DeviceWrapper with device field indicating a GPU, only works fine on non-cuda builds.\r\n\r\nAll of this happens while the code is protected with the try/except statements:\r\n```\r\n        for epoch in range(num_epochs):\r\n            session.run(iterator.initializer)\r\n            while True:\r\n                try:\r\n                    session.run([operation])\r\n                except tf.errors.OutOfRangeError:\r\n                    break\r\n```\r\n\r\nNow the only cheeky thing is that I am using the binaries from Arch Linux repositories, but these are far from being dodgy.\r\n[python-tensorflow](https://www.archlinux.org/packages/community/x86_64/python-tensorflow/)\r\n[python-tensorflow-cuda](https://www.archlinux.org/packages/community/x86_64/python-tensorflow-cuda/)\r\n[The build script](https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/tensorflow)\r\n\r\nThis problem was in tensorflow 1.2 and persists in tensorflow 1.3.\r\nAlso tested on a laptop without dedicated gpu, but same OS and packages, works fine.\r\n", "comments": ["this is a side effect of using tf.contrib.data iterators with small batches (as you are doing for inference).  @mrry any way to disable the logging in a configurable way?", "(nothing to do with dynamic_rnn).", "Hi @ebrevdo, by small batches you refer to the size of most batches or the size of the last one ?\r\nThe warning is there in training too, for any batch size apparently, including the case when the batch size evenly divides the training examples.\r\n\r\nI thought it has something to do with dynamic_decode because simply iterating through the dataset and printing it to the console, for example, works fine. ", "How is the dataset defined?\r\n\r\nI think there's an issue in the 1.3 release when you have a `Dataset.map()` that contains a `tf.py_func()` that ultimately raises `StopIteration`... essentially it gets reported as an `errors::OutOfRange()` and logged, rather than `end_of_sequence = true` which exits silently. (I recently noticed fixed that bug in an internal branch, and it should appear in the nightlies soon.)", "```\r\n    dataset = tf.contrib.data.TFRecordDataset(record)\r\n    dataset = dataset.map(lambda p: _parse_function(p, feature_size))\r\n```\r\n_parse_function is a Python function, does tf.py_func get called internally ?\r\nThe problem also existed in TF 1.2, CUDA build only\r\n", "What's the source of `_parse_function()`?", "_parse_function takes an Example and an int, applies tf.parse_example, returns a tuple of one tf.float32 and three tf.int64 (the context and the features)", "Do you get the logged message at every run call or only every once in a\nwhile?\n\nOn Aug 19, 2017 10:36 AM, \"georgesterpu\" <notifications@github.com> wrote:\n\n> _parse_function takes an Example and an int, applies tf.parse_example,\n> returns a tuple of one tf.float32 and three tf.int64 (the context and the\n> features)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12414#issuecomment-323537053>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyk5kfuqOjYeuXaKV0Vs53xuJeyjks5sZx0AgaJpZM4O8Vau>\n> .\n>\n", "I get it at the end of each training epoch.\r\nIf using BeamSearchDecoder with attention, it gets printed multiple times, depending on the beam width.\r\n\r\nOne workaround that I could find is stopping the loop after the last batch, if I know in advance their number (instead of using the infinite loop and stopping on exception)\r\n\r\nHere is the content of my `_parse_function()`, couldn't make it work as a tf.py_func though, with the tuple trick indicated by @mrry:\r\n\r\n```\r\ndef _parse_function(example, feature_size):\r\n\r\n    context_features = {\r\n        \"input_length\": tf.FixedLenFeature(shape=[], dtype=tf.int64),\r\n        \"labels_length\": tf.FixedLenFeature(shape=[], dtype=tf.int64),\r\n    }\r\n    sequence_features = {\r\n        \"inputs\": tf.FixedLenSequenceFeature(shape=[feature_size], dtype=tf.float32),\r\n        \"labels\": tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64)\r\n    }\r\n\r\n    context_parsed, sequence_parsed = tf.parse_single_sequence_example(\r\n        serialized=example,\r\n        context_features=context_features,\r\n        sequence_features=sequence_features\r\n    )\r\n\r\n    return sequence_parsed[\"inputs\"], context_parsed[\"input_length\"], \\\r\n           sequence_parsed[\"labels\"], context_parsed[\"labels_length\"]\r\n\r\n```", "I can confirm the same issue on a couple different Ubuntu versions (and some unknown Linux distro), Python 3.5 and 3.6, TensorFlow 1.2 and 1.3, cudnn 5.1 and cudnn 6, and on Telsa P100, GeForce GTX 1070 and Pascal Titan X.\r\n\r\nI get the message at the end of each epoch, on both training and dev set. Here's my dataset definition:\r\n\r\n```python\r\ndef setup_dataset(filename: str, batch_size: int, shuffle_seed=None):\r\n    n_recs, labels = tfrecord_metadata(filename)\r\n    dataset = tfdata.TFRecordDataset(filename)\r\n    dataset = dataset.skip(1)  # I store some metadata in the first record\r\n    dataset = dataset.map(parse_label_sequence_example if label_seq\r\n                          else parse_example)\r\n    if shuffle_seed is not None:\r\n        dataset = dataset.shuffle(n_recs, shuffle_seed)\r\n\r\n    dataset = dataset.map(lambda x, y: (x, y, build_mask(y)))\r\n    padded_shapes = (tf.TensorShape([None]), tf.TensorShape([None],),\r\n                     tf.TensorShape([None]))\r\n    dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\r\n\r\n    return dataset, labels\r\n```\r\n\r\nI use a batch size of 8, and shuffle the training set but not the dev set (but the warning shows up in all cases).", "What happens to your data once it is fetched and parsed by the iterator ?\r\n", "How much do you need? Here's how it starts:\r\n```python\r\nshuf_seed = tf.placeholder(tf.int64, shape=[])\r\ntrn_data, n_cat = setup_dataset(args.train, args.batch_size, shuf_seed)\r\ndev_data, _ = setup_dataset(args.dev, args.batch_size)\r\n\r\nit = tfdata.Iterator.from_structure(trn_data.output_types,\r\n                                    trn_data.output_shapes)\r\nx, y, y_mask = it.get_next()\r\ny = tf.reshape(y, [-1])\r\ny_mask = tf.cast(tf.reshape(y_mask, [-1]), tf.float32)\r\ntemporal_padding = args.filter_size[0] - 1\r\nt_pad_before = temporal_padding // 2\r\nt_pad_after = temporal_padding - t_pad_before\r\nx = tf.pad(x, [[0, 0], [t_pad_before, t_pad_after]])\r\ntrn_init = it.make_initializer(trn_data)\r\ndev_init = it.make_initializer(dev_data)\r\n<snip>\r\nemb_layer = tf.Variable(embeddings, trainable=args.trainable_embeddings,\r\n                        name='embedding_matrix')\r\nx_embedded = tf.nn.embedding_lookup(emb_layer, x)\r\n```", "Never mind, I made a wrong assumption. Looking at the first comment of @mrry, the issue might be fixed by now in tf nightly 1.4.", "I'd be willing to test it out---I'm not sure what the git model is that's used for TF, but if I compile from master and test that, will that suffice?", "Maybe try `pip install tf-nightly` in a python 3.5 environment first\n\nOn 20 September 2017 at 19:49, Aditya Bhargava <notifications@github.com>\nwrote:\n\n> I'd be willing to test it out---I'm not sure what the git model is that's\n> used for TF, but if I compile from master and test that, will that suffice?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12414#issuecomment-330944089>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFvUy5NVYYdUGT27gEMpRICMnviwrgVEks5skV4igaJpZM4O8Vau>\n> .\n>\n", "This issue is specific to the GPU builds, and the TF README.md says the nightlies are CPU builds only---has that changed?", "I might not be the right person to answer this, but you are correct in my\nopinion; my mistake. Please try compiling from sources.\n\n\nOn 20 September 2017 at 20:44, Aditya Bhargava <notifications@github.com>\nwrote:\n\n> This issue is specific to the GPU builds, and the TF README.md says the\n> nightlies are CPU builds only---has that changed?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12414#issuecomment-330958913>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFvUyxWHvrV01K6gEzSeArThvl3RYE2mks5skWsAgaJpZM4O8Vau>\n> .\n>\n", "I just cloned master yesterday and compiled that and gave it a shot, but the error still shows up for me with that latest build. Not sure how it was for @georgesterpu but I actually get multiple copies of the message:\r\n```\r\n2017-09-23 10:21:59.360135: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,?], [?,?]], output_types=[DT_INT64, DT_INT64, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n2017-09-23 10:21:59.360135: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,?], [?,?]], output_types=[DT_INT64, DT_INT64, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n2017-09-23 10:21:59.360160: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,?], [?,?]], output_types=[DT_INT64, DT_INT64, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n2017-09-23 10:21:59.360195: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,?], [?,?]], output_types=[DT_INT64, DT_INT64, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n```\r\nOn my dev set, which is smaller, I just get one:\r\n```\r\n2017-09-23 10:22:01.758969: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,?], [?,?]], output_types=[DT_INT64, DT_INT64, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n```", "I meet the same problem and fix by flowing.\r\n`tr_data = Dataset.from_tensor_slices((train_images, train_labels)).repeat().batch(100)`\r\nJust add `.repeat()`.\r\n", "@fumihwh That won't work for me, as I need to know when the epoch is done.", "@rightaditya how about this with `.repeat()`. I know your problem now and use following to avoid it. If I do not use this, I got same error log with yours.\r\n```\r\nfor epoch in range(num_epochs):\r\n    session.run(iterator.initializer)\r\n    for _ in range(dataset_size / batch_size):\r\n        session.run([operation])\r\n```", "I have just cloned the master repo and compiled from sources. This time with cuDNN 7 :)\r\nWarning still there", "@fumihwh Is your dataset size divisible by your batch size? Mine isn't, so I'd probably have to `ceil` the division (and make sure to use float division if using python2).\r\n\r\nStill, it's not an ideal solution given the way the API seems to have been designed.", "@fumihwh Is the `.repeat()` function only suitable for training? I suppose it's not suitable for testing, as I only want to evaluate once; will the `make_one_shot_iterator()` only run the data set once even if my dataset size is NOT divisible by my batch size? My test seems that the evaluation would go infinite loops when using `.repeat()`. \r\n\r\nI encountered the same error as described by previous users during testing but not in training. During evaluation, there are thousands of \" Out of range: End of sequence\" errors (I guess the number of errors are the same as the number of my evaluation samples). But correct evaluation results are still printed out after those errors, and the program did not crash and can still continue training. Anyone knows the reason and how to fix it? Thank you.\r\n\r\nI used estimator, and the input functions are\r\n\r\n\tdef input_fn(mode):\r\n\t  \"\"\"Input function which provides a single batch for train or eval.\"\"\"\r\n\t  dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames(mode))\r\n\t  if mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\r\n\t  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)\r\n\r\n\t  if mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdataset = dataset.repeat()\r\n\r\n\t  dataset = dataset.map(lambda value: dataset_parser(value, mode),\r\n\t\t\t\t\t\t\tnum_threads=FLAGS.map_threads,\r\n\t\t\t\t\t\t\toutput_buffer_size=FLAGS.batch_size)\r\n\r\n\t  if mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\r\n\r\n\t  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()\r\n\t  images, labels = iterator.get_next()\r\n\t  return images, labels\r\n\r\nMy model function is\r\n\r\n    _DEVICE_LIST = ['/gpu:0', '/gpu:1']\r\n\tdef imagenet_model_fn(features, labels, mode):\r\n\t  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\r\n\t  tf.summary.image('images', features, max_outputs=6)\r\n\r\n\t  with tf.device('/cpu:0'):\r\n\t\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\r\n\t\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\r\n\t\t\r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  global_step = tf.train.get_or_create_global_step()\r\n\r\n\t\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\r\n\t\t  boundaries = [\r\n\t\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\r\n\t\t  values = [\r\n\t\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\r\n\t\t  learning_rate = tf.train.piecewise_constant(\r\n\t\t\ttf.cast(global_step, tf.int32), boundaries, values)\r\n\r\n\t\t  # Create a tensor named learning_rate for logging purposes.\r\n\t\t  tf.identity(learning_rate, name='learning_rate')\r\n\t\t  tf.summary.scalar('learning_rate', learning_rate)\r\n\r\n\t\t  optimizer = tf.train.MomentumOptimizer(\r\n\t\t\tlearning_rate=learning_rate,\r\n\t\t\tmomentum=_MOMENTUM)\r\n\r\n\t\ttower_grads = []\r\n\t\ttower_cross_entropy = []\r\n\t\ttower_reg_loss = []\r\n\t\ttower_preds = []\r\n\r\n\t\twith tf.variable_scope(tf.get_variable_scope()):\r\n\t\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\r\n\t\t\t_DEVICE_LIST, split_batch, split_labels)):\r\n\t\t\twith tf.device(device):\r\n\t\t\t  with tf.name_scope('device_%d' % dev_idx):\r\n\t\t\t\tlogits = network(inputs=device_features,\r\n\t\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\r\n\t\t\t\ttf.get_variable_scope().reuse_variables()\r\n\t\t  \r\n\t\t\t\ttower_pred = {\r\n\t\t\t\t  'classes': tf.argmax(logits, axis=1),\r\n\t\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n\t\t\t\t}\r\n\r\n\t\t\t\ttower_preds.append(tower_pred)\r\n\r\n\t\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\r\n\t\t\t\t  logits=logits, onehot_labels=device_labels)\r\n\t\t\t\ttower_cross_entropy.append(cross_entropy)\r\n\r\n\t\t\t\treg_loss = FLAGS.weight_decay / len(_DEVICE_LIST) * tf.add_n(\r\n\t\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\r\n\t\t\t\ttower_reg_loss.append(reg_loss)            \r\n\r\n\t\t\t\tloss = cross_entropy + reg_loss\r\n\r\n\t\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \r\n\t\t\t\t  grads = optimizer.compute_gradients(loss)\r\n\t\t\t\t  tower_grads.append(grads)\r\n\r\n\t\tpredictions = {\r\n\t\t  'classes': tf.concat([p['classes'] for p in tower_preds], axis=0),\r\n\t\t  'probabilities':\r\n\t\t\t\ttf.concat([p['probabilities'] for p in tower_preds], axis=0)\r\n\t\t}\r\n\r\n\t\tif mode == tf.estimator.ModeKeys.PREDICT:\r\n\t\t  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)    \r\n\r\n\t\tcross_entropy = tf.add_n(tower_cross_entropy)\r\n\t\ttf.identity(cross_entropy, name='cross_entropy')\r\n\t\ttf.summary.scalar('cross_entropy', cross_entropy)\r\n\r\n\t\treg_loss = tf.add_n(tower_reg_loss)\r\n\t\ttf.summary.scalar('reg_loss', reg_loss)\r\n\r\n\t\tloss = cross_entropy + reg_loss\r\n\t\ttf.summary.scalar('total_loss', loss)\r\n\r\n\t\taccuracy = tf.metrics.accuracy(\r\n\t\t\t\t  tf.argmax(labels, axis=1), predictions['classes'])\r\n\t\tmetrics = {'accuracy': accuracy}\r\n\t\t  \r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  tf.identity(accuracy[1], name='train_accuracy')\r\n\t\t  tf.summary.scalar('train_accuracy', accuracy[1])\r\n\t\t  \r\n\t\t  # Batch norm requires update_ops to be added as a train_op dependency.\r\n\t\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t  with tf.control_dependencies(update_ops):\r\n\t\t\tgrads = average_gradients(tower_grads)\r\n\t\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\r\n\t\telse:\r\n\t\t  train_op = None\r\n\t\t\r\n\t\treturn tf.estimator.EstimatorSpec(\r\n\t\t  mode=mode,\r\n\t\t  predictions=predictions,\r\n\t\t  loss=loss,\r\n\t\t  train_op=train_op,\r\n\t\t  eval_metric_ops=metrics)", "I get flooded the same way using today's tf-nightly-gpu wheel binary on Ubuntu 16.04, Python 2.7, cudnn 6.", "I get the same warning with tensorflow 1.3", "I am experiencing this with tensorflow-gpu on Windows 10, Python 3.6.1, cudnn64_6, using tf.slim.\r\n\r\nStill need to verify this, but it seems the message only comes up when training for the first time, without a checkpoint. The dataset size is divisible by the batch size.", "Same problem with TF1.3", "Same problem. I got a bunch of error logs at the end of each epoch. Very annoying.\r\n\r\n\"2017-10-31 18:59:22.473230: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,120,120,3], [?,120,120,1]], output_types=[DT_FLOAT, DT_UINT8], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\"", "Same problem with TF 1.4\r\n(built from source, CUDA 9.0 cuDNN 7.0, Ubuntu 16.04, Python 3.5)", "I think i can guess out what cause this problems but i do not know how to resolve it. \r\nThat is :\r\nThere are multi threads in our training dataset instances(num_parallel_calls =4 in my config) ,\r\n but during our training process, we catch the except tf.errors.OutOfRangeError .\r\nI guess the try block only deal this exception only once.  So we still get another three excepts below. \r\n\r\n2017-11-04 10:47:47.713841: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n2017-11-04 10:47:47.713915: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n2017-11-04 10:47:47.714404: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\r\n\r\nMaybe we should treat this training process with multi threads, so that each of training threads only consume the return data form just one dataset thread. \r\n", "Maybe there are some other causes because by setting the `num_parallel_calls` argument to `1` in `tf.data.Dataset.map` doesn't prevent the warning from flooding.", "Yes, you can wrap the dataset with a repeat () call and use an outer for\nloop for reach epoch if you know the number of elements per epoch.\n\nOn Sat, Nov 4, 2017, 10:23 PM Jeremy Hsu <notifications@github.com> wrote:\n\n> Maybe there are some other causes because by setting the\n> num_parallel_calls argument to 1 in tf.data.Dataset.map doesn't prevent\n> the warning from flooding.\n>\n> BTW, a very useless tip to avoid the warning is by using for-loops\n> instead of try-except (and remember to run the initializer every time\n> before the loop). However, this only works when the dataset is simple\n> enough that you know the exact number of iterations per epoch...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12414#issuecomment-341950080>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9Lr0GYVCOSb0RWqKvRMSsbLtaB5ks5szUZtgaJpZM4O8Vau>\n> .\n>\n", "I can also confirm this problem and I am also not passing a `py_func` to `dataset.map`. ", "@ybsave \r\ni also meeting this problem same for you,but it is not a bug.\r\nyou can see the doc in https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\r\nabout train()\r\nsteps: Number of steps for which to train model. If None, train forever or train until input_fn generates the OutOfRange error or StopIteration exception. 'steps' works incrementally. If you call two times train(steps=10) then training occurs in total 20 steps. If OutOfRange or StopIteration occurs in the middle, training stops before 20 steps. If you don't want to have incremental behavior please set max_steps instead. If set, max_steps must be None.\r\n", "Thanks for clarifying that @libulin!\r\n\r\nWhile it's true that the log messages are not strictly a bug, we appreciate that it might be confusing or annoying to have a flood of messages in your console from code that's operating correctly :). With the fix in 301a6c4 (and a related fix for the `StopIteration` logging in c154d4719eea88e694f4c06bcb1249dbac0f7877), the logs should be much quieter when using `tf.data`.", "I am using TF 1.4.0. The following works, though weird.\r\n```Py\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ERROR\r\nimport tensorflow as tf\r\n```\r\n", "I am also getting this Warning `W tensorflow/core/framework/op_kernel.cc:1192] Out of range: StopIteration: Iteration finished.`\r\nI am using a **feedable iterator** and the warning appears when the iterator is switching from the train-iterator-handle to the validation-iterator-handle (however not every time it switches)", "@maxfiedler Can you reproduce this with TF 1.5.0? If so, please open a new issue with code to reproduce the warning and we'll take a look.", "@mrry I am still working with TF 1.4.0. I will let you know if the problem persists, once we switched to TF 1.5.0 and produce a code example if so", "A few more things I notice (still on TF 1.4). \r\nI am also getting this Warning\r\n```\r\n2018-01-17 13:35:41.716773: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,90,120,3], [?,10800,21]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n```\r\nPlus a `tf.errors.OutOfRangeError`\r\nwhen I am using a `one_shot_iterator` without repeat() that runs over my test_set AND a batch size that IS cleanly divides the number of elements in the test data\r\n(and I am getting way more than double those warnings when running on 2 GPUs instead of 1 GPU)\r\nRelevant snippet of my code\r\n```\r\nself.test_set = self.create_test_dataset()\r\n        if self.parse_fn:\r\n            self.test_set = self.test_set.map(self.parse_fn, num_parallel_calls=self.num_parallel_processes)\r\n        if self.preprocessing_fn:\r\n            self.test_set = self.test_set.map(self.preprocessing_fn, num_parallel_calls=self.num_parallel_processes)\r\n        # self.test_set = self.test_set.repeat(1)  # this line is not needed, but makes the behavior more explicit\r\n        self.test_set = self.test_set.batch(self.batch_size)\r\n        iterator = self.test_set.make_one_shot_iterator()\r\n        features, targets = iterator.get_next()\r\n``` \r\nand \r\n```\r\nwith self._session as sess:\r\n                while not self._stop_testing:\r\n                    # Start the actual calculation\r\n                    loss, accuracy = sess.run(\r\n                        [self._net.loss, self._net.perf_metrics[0]], feed_dict={\r\n                            self._net.net_is_training: False})\r\n```\r\nIs there a way to signal the while loop that the iterator has reached the end of the dataset instead of throwing an OutOfRangeError?\r\n\r\n\r\nI am following the \"load on batch (i.e. one get_next() op) and distribute it over the GPUs via tf.split\" scheme. When the last batch is not cleanly splittable I get understandably an tf.InvalidArgumentError.\r\n\r\nBut before I get a ton of the following warnings:\r\n```\r\n[[Node: split_batch = Split[T=DT_FLOAT, num_split=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](split_batch_1/split_dim, IteratorGetNext)]]\r\n2018-01-17 14:35:58.198566: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 10) and num_split 3\r\n```", "I have the same _Out of range: End of sequence_ warning with tf 1.4. \r\nAny experience with 1.5 or higher?", "@tanabics it was fixed for me with 1.5+", "Hi - I have a related problem. I am able to run several epochs, but then it suddenly stops  ...\r\n\r\nStart training process ...\r\nIter: 0, Loss: 4.2690\r\nIter: 1, Loss: 0.6611\r\nIter: 2, Loss: 0.3918\r\nIter: 3, Loss: 0.2462\r\nIter: 4, Loss: 0.1588\r\nIter: 5, Loss: 0.1042\r\nIter: 6, Loss: 0.0817\r\nIter: 7, Loss: 0.0698\r\nIter: 8, Loss: 0.0631\r\nIter: 9, Loss: 0.0607\r\nIter: 10, Loss: 0.0573\r\nIter: 11, Loss: 0.0561\r\n\r\nThe process is stopped and I get the error message: \r\n\r\nOutOfRangeError: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,20,1], [?,20,1]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\r\nI have asked for 70 epochs (and data is prepared for that), but something is not working. I use Tensorflow 1.10. The model is not saved, so there must be a strange bug. \r\n\r\nIt looks like some of the same issues as in this thread, but my model is not estimated and finished up by TF. I get the impression that some in this thread get an estimated model, but in addition get a error message. I dont have a model returned by TF. \r\n\r\nAnyone ?\r\n", "I got a similar problem when running serveral test cases, such as `tensorflow/python/data/kernel_tests/list_files_dataset_op_test.py` and `tensorflow/python/data/kernel_tests/reader_dataset_ops_test.py`, although the test cases pass.  \r\n\r\nThe error messages are as following:\r\n```\r\nERROR:tensorflow:End of sequence\r\n\t [[node IteratorGetNext (defined at /anaconda3/lib/python3.6/unittest/case.py:605)  = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.py\", line 242, in <module>\r\n    test.main()\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 64, in main\r\n    return _googletest.main(argv)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 100, in main\r\n    benchmark.benchmarks_main(true_main=main_wrapper)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py\", line 344, in benchmarks_main\r\n    true_main()\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 99, in main_wrapper\r\n    return app.run(main=g_main, argv=args)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 70, in g_main\r\n    return unittest_main(argv=argv)\r\n  File \"/anaconda3/lib/python3.6/unittest/main.py\", line 95, in __init__\r\n    self.runTests()\r\n  File \"/anaconda3/lib/python3.6/unittest/main.py\", line 256, in runTests\r\n    self.result = testRunner.run(self.test)\r\n  File \"/anaconda3/lib/python3.6/unittest/runner.py\", line 176, in run\r\n    test(result)\r\n  File \"/anaconda3/lib/python3.6/unittest/suite.py\", line 84, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/anaconda3/lib/python3.6/unittest/suite.py\", line 122, in run\r\n    test(result)\r\n  File \"/anaconda3/lib/python3.6/unittest/suite.py\", line 84, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/anaconda3/lib/python3.6/unittest/suite.py\", line 122, in run\r\n    test(result)\r\n  File \"/anaconda3/lib/python3.6/unittest/case.py\", line 653, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/anaconda3/lib/python3.6/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.py\", line 49, in testEmptyDirectory\r\n    next_element = itr.get_next()\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/data/ops/iterator_ops.py\", line 420, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/private/var/tmp/_bazel_fei/5dc7b372a7c45427ff30500d3e22fe26/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/kernel_tests/list_files_dataset_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```", "I think that was an effect of 2c97ee31b0cb584b255232d46d4e42db4cbcb1c2. I agree that this makes the output of `tf.data` tests hard to read, because they use `OutOfRangeError` to signal completion.\r\n\r\nIf you'd like to send a PR that disables the logging for `OutOfRangeError`, I'd be happy to approve it.", "@mrry Thanks for your quick reply! Will submit a PR for this.", "Hello\r\nI had opened a Stackoverflow question related to this thread before finding it -- https://stackoverflow.com/questions/52832625/outofrangeerror-logged-at-each-epoch-after-upgrade-from-tensorflow-1-8-0-to-1-11.\r\n@mrry does it mean that this is only a logging issue, and that the Error raised shall not be worried about ?"]}, {"number": 12413, "title": "FIFOQueue '_1_input_producer' is closed.", "body": "I'm using an `input_pipeline` in a `tf.managed_session()`, and encountered the following error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 206, in <module>\r\n    main()\r\n  File \"/home/darth/GitHub Projects/gru_svm/model/gru_svm.py\", line 193, in main\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 238, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1063, in _single_operation_run\r\n    target_list_as_strings, status, None)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.CancelledError: FIFOQueue '_1_input_producer' is closed.\r\n\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/limit_epochs)]]\r\n```\r\n\r\nHere's the session block I wrote:\r\n\r\n```\r\nsv = tf.train.Supervisor(logdir=CHECKPOINT_PATH, summary_op=None)\r\n\r\n    with sv.managed_session() as sess:\r\n\r\n        sess.run(init_op)\r\n\r\n        checkpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\r\n\r\n        if checkpoint and checkpoint.model_checkpoint_path:\r\n            saver.restore(sess, checkpoint.model_checkpoint_path)\r\n\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n        try:\r\n            step = 0\r\n            while not coord.should_stop():\r\n                example_batch, label_batch = sess.run([examples, labels])\r\n\r\n                feed_dict = {x_input: example_batch, y_input: label_batch, state: current_state,\r\n                             learning_rate: LEARNING_RATE, p_keep: DROPOUT_P_KEEP}\r\n\r\n                summary, _, epoch_loss, next_state = sess.run([merged, optimizer, cost, states], feed_dict=feed_dict)\r\n\r\n                accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\r\n\r\n                current_state = next_state\r\n\r\n                if step % 100 == 0:\r\n                    print('step [{}] loss : {}, accuracy : {}'.format(step, epoch_loss, accuracy_))\r\n                    writer.add_summary(summary, step)\r\n                    saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\r\n\r\n                step += 1\r\n        except tf.errors.OutOfRangeError:\r\n            print('EOF -- training done at step {}'.format(step))\r\n        finally:\r\n            writer.close()\r\n            coord.request_stop()\r\n\r\n        coord.join(threads)\r\n\r\n        saver = tf.train.Saver()\r\n        saver.save(sess, CHECKPOINT_PATH + MODEL_NAME, global_step=step)\r\n```\r\n\r\nThank you in advance for your help!\r\n", "comments": []}, {"number": 12412, "title": "Disable denormal_test on s390x platform", "body": "As per the discussion happened on [11902](https://github.com/tensorflow/tensorflow/issues/11902), the mentioned modes are not applicable for s390x architecture as well. So disabling denormal_test for s390x platform.", "comments": ["@shahidhs-ibm, thanks for your PR! By analyzing the history of the files in this pull request, we identified @girving, @jart and @sandipmgiri to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12411, "title": "Update array_ops.py", "body": "-Space import statements according to PEP8\r\n-Remove unnecessary import statements", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @jhseu and @tensorflower-gardener to be potential reviewers."]}, {"number": 12410, "title": "Distributed Tensorflow: data feeding in the beginning", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.31\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am using distributed tensorflow. I have a huge parameter matrix partitioned across multiple parameter servers. In the beginning of training, only some workers will have the following exception. If the failed workers are restarted (possibly need multiple restarts), then the workers can go through and continue the training.\r\n\r\nI have checked the hdfs paths and files. They all look good. It seems the problem is that the failed workers cannot get the parameters from some ps (in the last line, see embedding_lookup/DynamicPartition_S751)\r\n\r\n### Source code / logs\r\n\r\nException thrown:\r\nCaused by op u'ReaderReadV2', defined at:\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 180, in <module>\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 157, in manager\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 61, in worker\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 174, in main\r\n    process()\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 169, in process\r\n    serializer.dunits processed is zero\r\nump_stream(func(split_index, iterator), outfile)\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 345, in func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 793, in func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 240, in _mapfn\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/__pyfiles__/my_distributed_learning.py\", line 105, in map_fun\r\n    ((x, y_), units_completed, records_prodcued, sg_key) = read_csv_examples(skipgrams, None, batch_size, num_epochs, task_index, num_workers)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/__pyfiles__/my_distributed_learning.py\", line 56, in read_csv_examples\r\n    sg_key, sg_csv = sg_reader.read(sg_queue)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\r\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 422, in _reader_read_v2\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2359, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1242, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_1_sg_queue' is closed and has insufficient elements (requested 1, current size 0)\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:worker/replica:0/task:1/cpu:0\"](sg_reader, sg_queue)]]\r\n\t [[Node: embedding_lookup/DynamicPartition_S751 = _Recv [client_terminated=false, recv_device=\"/job:ps/replica:0/task:6/cpu:0\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=6148508285306453080, tensor_name=\"edge_726_embedding_lookup/DynamicPartition\", tensor_type=DT_INT32, _device=\"/job:ps/replica:0/task:6/cpu:0\"]()]]\r\n", "comments": ["@mrry, could you take a look? Feel free to redirect to stack overflow.", "The source of the error is a `ReaderReadV2` op, which suggests that something is failing in your input pipeline. (Perhaps a file cannot be found? Perhaps some of the workers have no work to do?)\r\n\r\nI suspect this is unrelated to the use of distributed TensorFlow or the huge parameter matrix. Try to reproduce this when running on a single machine... it should be easier to see what the problem is in that setting.", "**The quick update: if I increase `num_epochs` to 1000 from 1, it seems to solve the problem (I don't understand why). I can run it more to see if this solves it completely.**\r\n\r\nThe input pipeline is also my initial suspect. I tried and checked the followings when the `num_epochs`=1:\r\n\r\n1. filenames to `string_input_producer` is not empty and they are valid hdfs paths for every worker (each worker will work on multiple files)\r\n2. sanity check on data\r\n3. I am using TensorFlowOnSpark framework so I can relaunch a worker if it fails. When a worker is relaunched due to empty queue, it can possibly read data and train without popping the same error. (There is no randomization in my data pipeline. So I assume the worker tries to read the same data in the same order after relaunch)\r\n\r\nMy pipeline code looks like:\r\n```\r\nf_queue = tf.train.string_input_producer(filenames, shuffle=False, capacity=10, num_epochs=num_epochs)\r\nkey, csv = tf.TextLineReader(name=\"reader\").read(f_queue)\r\nbatch_csv = tf.train.batch(tensors=tf.split(tf.decode_csv(csv, defaults), 2), batch_size=batch_size, capacity=1000+4*batch_size, num_threads=args.readers, allow_smaller_final_batch=True)\r\n```\r\n\r\nI can also try your suggestion to run on a single machine. The error happens on some workers randomly. So not sure if I can reproduce the error not using distributed learning.", "@kinsumliu, let us know when you have more information.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing this due to lack of activity. Feel free to reopen if you can reproduce the problem in a smaller, self-contained example."]}, {"number": 12409, "title": "about fps", "body": "In android device, if I want to know the fps in the detection activity, what should I do in the DetectorActivity.java?\r\n", "comments": ["You can see the inference time (basically 1/fps) in the debug view by pressing the volume key. So I guess you can look at that code to get an idea of what you can do. Looking at the display code here:\r\n`lines.add(\"Inference time: \" + lastProcessingTimeMs + \"ms\");`\r\nIt looks like you can use `lastProcessingTimeMs` as a starting point for your FPS.", "Do I need to add corresponding layout? My phone can't present any information of fps if I don't add layout.", "_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\nI would suggest you go to ask on Stack Overflow. This isn't the place for your question really. Good luck with your project!", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12408, "title": "[XLA][WIP] Add support for Polyhedral compilation through Polly", "body": "Added Bazel build files for compiling Polly with LLVM to enable Polyhedral compilation, as discussed in #8100", "comments": ["@annanay25, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @hawkinsp and @tensorflower-gardener to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "cc @BrianRetford @meinersbur @tobig ", "@googlebot `I signed it!`", "CLAs look good, thanks!\n\n<!-- ok -->", "cc @hawkinsp \r\n\r\nCould you please review this?", "Seems that there is also a recent effort to mainline [Polly in LLVM](http://lists.llvm.org/pipermail/llvm-dev/2017-September/117063.html) /cc @hfinkel", "@annanay25 any chance to pull rebase and address comments? Thank you.", "@drpngx, yes, will do it shortly. Sorry for the delay.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@annanay25 any update?", "@sanjoy did the new commits resolve your comments?", "@annanay25 @arpith-jacob could you resolve the conflicts?", "@annanay25 any luck with addressing the comments? Could you pull rebase and push again?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@annanay25 any progress on this? Please rebase.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jpienaar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jpienaar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can @tensorflowbutler count real days? This second one was probably 28 days so it doesn't make sense to count bot only posts.", "Good point. @av8ramit how hard would it be to ignore bot posts for the purposes of counting days only (the 2 weeks interval in nagging is still good).", "Sure I'll add this to our upcoming feature set. Thanks @bhack ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 12407, "title": "Added a library to convert TensorFlow GraphDef to XLA SessionModule.", "body": "I have only added the minimal code without lot of comments. I wanted to make sure this approach is acceptable. I have made the xlagen library public, so a python library can be built externally by using TensorFlow as a submodule. Please let me know if this is acceptable and I will spruce up the PR with more documentation, comments, and tests.", "comments": ["@keveman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @tensorflower-gardener and @frankchn to be potential reviewers.", "Can one of the admins verify this patch?", "This approach may not be as tenable as I originally anticipated. Even with this in contrib, it is still hard to make use of it from another project that uses TensorFlow as a (git) submodule, and swig wrap it. This is mostly because of the build system, and swig wrapping this causes symbol collisions with tensorflow (can't import tensorflow and this library together). Please let me know what the best way forward for making it easy to extract XLA graphs from GraphDefs in Python.", "Adding myself :)", "Hi Manjunath!\r\n\r\nFYI you might be interested in the following code in contrib, which looks like it does what you want:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/xla_tf_graph/xla_tf_graph_util.h#L62\r\n\r\nI don't think I understand the Python / swig-wrapping issues.  Perhaps you can open a bug to discuss that further, if the problem still exists with the existing contrib code?", "Thanks @tatatodd for the response. I did notice that code before I embarked on this. However, we wanted to be able to call this from Python, so I needed going from GraphDef -> SessionModule. Actually, the better piece of code that I can reuse is a combination of `InitGraph` and `CompileGraph` in your https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/compile.h. Also, the BUILD visibility of the various components is not suitable for making tensorflow a git submodule under a different project. Here is what would be ideal for our (Cerebras) use case:\r\n\r\n    with tf.Graph().as_default() as g:\r\n      a = tf.placeholder(tf.int32, shape=[2, 2])\r\n      b = tf.placeholder(tf.int32, shape=[2])\r\n      c = tf.add(a, b, name=\"c\")\r\n    # xla_graph would be a xla::SessionModule\r\n    xla_graph = tf2xla.GraphDefToXla(g.as_graph_def(), [\"c:0\"])\r\n\r\nNow, `tf2xla` can be something that we build on our own using the stock `tensorflow` repo as a git submodule, or if you think this is going to be more generally useful, this can be in `tf` module itself. The former would require changing the visibility of some of the libraries to `//visibility:public`. Either way, I am willing to do the necessary work. But please let me know what you think the best approach is.\r\n", "Closing this, will make do with a fork for now."]}, {"number": 12406, "title": "compilation error ", "body": "On Ubuntu 16.04 with gcc5\r\n\r\n./configure\r\nYou have bazel 0.5.3 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N]\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N]\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N]\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\r\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\r\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0.1\r\nPlease specify the location where cuDNN 7.0.1 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"6.1\"]:\r\nDo you wish to build TensorFlow with MPI support? [y/N]\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\nBUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1021:50: required from here\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'\r\ncudnnStatus_t retval = ::__name(args...); \r\n^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:234:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\r\n__macro(cudnnSetRNNDescriptor)", "comments": ["We do not support cuDNN 7 yet. See #12052.", "can we ask compilation errors in github or it should be in stackoverflow??\r\nI have [this issue](https://stackoverflow.com/questions/45777660/tensorflow-from-source-compilation-error-about-class-googleprotobufany)", "https://github.com/tensorflow/tensorflow/issues/12474 \r\nInstructions on how to compile with cuDNN 7 and CUDA9RC with an unofficial patch.  \r\n@Goddard @yosoufe "]}, {"number": 12405, "title": "tf.boolean_mask doesn't check array bounds.", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: archlinux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nm = tf.placeholder(tf.bool, shape=[None, 1], name='m')\r\nb = tf.placeholder(tf.float32, shape=[1, None], name='b')\r\noutput = tf.boolean_mask(b, m)\r\nmask = [[True],[True],[True],[True],[True]]\r\ny = np.random.rand(1, 4)\r\n\r\nwith tf.Session() as sess:\r\n    print(y)\r\n    print(sess.run(output, feed_dict={m: mask, b:y}))\r\n```\r\n\r\nThe code runs, while I expect an index out-of-bound error.\r\n\r\nIn fact, even when mask is a 4x1 boolean, I still expect an error (since b is 1x4). But that's somewhat arguable.", "comments": ["/CC @langmore", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "`tf.boolean_mask` calls `tf.gather`, which has the same problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\n\r\nconfig = tf.ConfigProto()\r\n# Disable constant propagation\r\nconfig.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\nconfig.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\r\n\r\nwith tf.device('/gpu:0'):\r\n  y = tf.gather([1., 2.], [3])\r\n  with tf.Session(config=config) as sess:\r\n    print(sess.run(y))\r\n```\r\n\r\n/CC @rryan, can you look into this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "> tf.boolean_mask calls tf.gather, which has the same problem:\r\n\r\nOn GPU this is unfortunately expected behavior, since `Gather` does not currently propagate errors back to the host from the GPU (for performance reasons). This should produce an error on CPU though (and @ppwwyyxx's example does produce an error for me on CPU).\r\n\r\nRelevant TODO:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gather_functor_gpu.cu.h#L56-L60", "Essentially, a dupe of https://github.com/tensorflow/tensorflow/issues/3638.", "Thanks. Closing since it's a duplicate."]}, {"number": 12404, "title": "Update boringssl", "body": "to include \"Work around language and compiler bug in memcpy, etc. \"\r\nDetails in https://github.com/google/boringssl/commit/17cf2cb1d226b0ba2401304242df7ddd3b6f1ff2\r\n\r\nThat change was needed on my system to avoid the following compilation error:\r\nexternal/boringssl/src/crypto/asn1/a_bitstr.c:118:5: error: 'memcpy': specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [-Werror=stringop-overflow=]", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "Jenkins, test this please", "The Windows Cmake build timed out. Might be related. Trying again:\r\n\r\nJenkins, test this please"]}, {"number": 12403, "title": "InvalidArgument error with tf.scan()", "body": "### System information\r\n- No custom code\r\n- Error encountered on Windows 10 and CentOS 6.7\r\n- binary\r\n- TF 1.1.0 and 1.2.1 respectively\r\n- 3.6\r\n- CUDA 8.0, CUDNN 5.1\r\n- GeForce 940MX 2GB (ran out of memory in execution), Titan X 12 GB\r\n\r\n### Describe the problem\r\nI encountered this error when using `tf.scan` on an MultiRNNCell.\r\n\r\n> InvalidArgumentError (see above for traceback): Max scatter index must be <= array size (141 vs. 141)\r\n\r\n### Source code / logs\r\nIf you look here: **[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/tensor_array_ops.cc#L1072](url)**\r\n\r\n``` \r\nOP_REQUIRES(\r\n          ctx, max_index < array_size,\r\n          errors::InvalidArgument(\"Max scatter index must be <= array size (\",\r\n                                  max_index, \" vs. \", array_size, \")\"));\r\n    }\r\n````\r\n\r\nIt seems that the condition (<) is not consistent with the error message (<=).\r\n", "comments": ["I think that this is just a matter of fixing the error message. The error condition seems just to be checking for correct indexing.", "The code's hyperlink address was moved to here\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/tensor_array_ops.cc#L1072\r\n\r\ncan you upload the code that makes the error?", "Yes I think the error message should be fixed as index is 0-based. PR #12800."]}, {"number": 12402, "title": "DataLossError. Checksum does not match when using multiple TFRecordDataset via tf.case", "body": "See below. Please let me know what I can do to provide more information for you.  I am working on pulling out the offending code into a standalone py file to replicate the bug elsewhere, but it might take a few days.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: arch linux (LTS kernel 4.9.44-1)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3 (master branch as of commit `566d167c`)\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: 0.5.2-2\r\n- **CUDA/cuDNN version**: 8.0.61-2/6.0.21-2\r\n- **GPU model and memory**: 1080 GTX Ti\r\n- **Exact command to reproduce**:\r\n\r\nI don't have exact source for you yet of a trimmed down example. I will see if I can put one together. Generally, the steps are\r\n\r\n- Generate a few `TFRecord` files\r\n- Instantiate a few `TFRecordDataset` from the files.\r\n- Perform necessary pre-processing on `TFRecordDataset` entries for each dataset, then on the dataset itself (`.cache`, `.repeat`, `.shuffle`, `.batch`, etc) For example, we might have datasets `train_dataset`, `validate_dataset`, and `test_dataset`.\r\n- Initialize each dataset (`.make_initializable_iterator`)\r\n- Train a model using an input tensor `x` of\r\n\r\n```\r\ntrain_phase = tf.placeholder(dtype=tf.int32, shape=())\r\nx = tf.case([(tf.equal(train_phase, 1), lambda: train_dataset),\r\n             (tf.equal(train_phase, 2), lambda: validate_dataset),\r\n             (tf.equal(train_phase,3), lambda: test_dataset)],\r\n         default=lambda: train_dataset)\r\n# ... construct model_op using tensor x as input ...\r\n# ... call the initializer ...\r\n\r\n# train for a bunch\r\nfor i in ...\r\n     sess.run(main_op, feed_dict={train_phase: 1})\r\n# validate\r\nfor i in ...\r\n     sess.run(main_op, feed_dict={train_phase: 2})\r\n# test\r\nfor i in ...\r\n     sess.run(main_op, feed_dict={train_phase: 3})\r\n```\r\n\r\n### Describe the problem\r\nI am using a single tensor `x` to represent my input values for all three phases of my model training (training, validation, and testing). To alternate between them without duplicating entire graphs, I set the tensor to be conditional on the value of `train_phase` (again, train, validate, or test) by using `tf.case`. \r\n\r\nIf the above steps are followed, my system will eventually (non-deterministically) crash during the training/validating/testing of the model. This is independent of the dataset used (I am using well-scrubbed versions of NYUv2, CIFAR-10, KITTI, etc.) and does not occur in particular records. The error is always long the lines of\r\n\r\n```\r\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n``` \r\n\r\nIt can occur during any phase (that is, on any branch of the `tf.case`). It occurs with multiple versions of CUDA 8 and TF (at least since TFRecordDataset came out). It occurs with single or multiple GPUS. It occurs with different Linux kernel versions (multiple variants of 4.10, 4.11, 4.12, including the 4.9 TLS branch). \r\n\r\nHowever, it does **NOT** occur if I don't use `tf.case` (that is, I use a single TFRecordDataset tensor as the input, i.e., only do training and skip validate/test).\r\n\r\nI would assume this is a complication with the CUDA drivers, as I occasionally (though not always) get kernel panics at the same time. I will also occasionally (though not always) get errors like `malloc(): smallbin double linked list corrupted`.\r\n\r\nI should also add that I've already tested for hardware issues and seem to have ruled all of them out (memory failures, HD/SSD cables, motherboard, PSU spikes, etc.). \r\n\r\n\r\n### Source code / logs\r\nThe full trace/output is\r\n\r\n```\r\nCaught unexpected exception during training: Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n\r\nCaused by op 'input_pipeline/IteratorGetNext_1', defined at:\r\n  File \"train.py\", line 266, in <module>\r\n    datasets = hem.get_datasets(args)\r\n  File \"/mnt/research/projects/autoencoders/hem/util/data.py\", line 64, in get_datasets\r\n    x = iterator.get_next()\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 311, in get_next\r\n    name=name))\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 698, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n```\r\n", "comments": ["@algoterranean Are you specifying a `filename` argument when you use `Dataset.cache()`? Is it possibly the same filename for each dataset? Does the problem go away if you specify an empty filename (and hence cache in memory)?\r\n\r\n@saeta It looks like this error might be coming from the `TensorBundle` used in the `CacheDataset`. Are there any obvious risks in using multiple instances of a dataset derived from the same op concurrently?", "@mrry Yes, I am using separate filenames for each cache/dataset. ~~The error also occurs when I am using in-memory caching.~~", "@algoterranean Do you get the same error when you use in-memory caching? (As far as I can tell, in the in-memory case it should never execute the line of code that returns that `DataLossError`.) What about if you disable caching?", "@mrry Ah. I'm sorry, I seem to have been partially mistaken. I just re-ran the model with in-memory caching, and did not get the error, but I did get a core dump eventually. Trying to pull it now to get some details. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing this due to lack of activity. @algoterranean Feel free to reopen if you can get more details."]}, {"number": 12401, "title": "`import tensorflow.contrib.layers` takes very long time", "body": "I've used Cprofile to test my code, and I find that `import tensorflow.contrib.layers` takes very long time. Here is the quick screenshot of the result:\r\n\r\n```\r\n14495011 function calls (14451561 primitive calls) in 6.324 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n   1023/1    0.058    0.000    6.328    6.328 {built-in method builtins.exec}\r\n        1    0.000    0.000    6.328    6.328 test_import.py:10(<module>)\r\n   1066/1    0.006    0.000    6.328    6.328 <frozen importlib._bootstrap>:966(_find_and_load)\r\n   1066/1    0.005    0.000    6.328    6.328 <frozen importlib._bootstrap>:939(_find_and_load_unlocked)\r\n   1360/1    0.001    0.000    6.328    6.328 <frozen importlib._bootstrap>:214(_call_with_frames_removed)\r\n    759/1    0.002    0.000    6.328    6.328 {built-in method builtins.__import__}\r\n    881/2    0.004    0.000    6.328    3.164 <frozen importlib._bootstrap>:659(_load_unlocked)\r\n    833/2    0.003    0.000    6.328    3.164 <frozen importlib._bootstrap_external>:667(exec_module)\r\n 6575/625    0.005    0.000    6.102    0.010 <frozen importlib._bootstrap>:996(_handle_fromlist)\r\n... \r\n```\r\nAs seen above, it took more than 6 secs to just do `import tensorflow.contrib.layers`, and I wonder if this is expected. Also, I am pretty interested in why it takes so long time to do importing there.\r\n\r\n**P.S** The profiling test runs on MacOS Sierra 10.12.5 with Tensorflow v1.2 and Python 3.5.", "comments": ["@jart, any plans to improve this?", "This looks like a duplicate of #11829 - which was patched on September 18th. \ud83c\udf89 "]}, {"number": 12400, "title": "Update estimator.py", "body": "Replace contrib with training and core framework APIs", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @tensorflower-gardener and @davidsoergel to be potential reviewers.", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 12399, "title": "Eager API inclusion to the dynamic library build target", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "There seems to be a buildifier error:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/6134/consoleFull\r\n\r\n> FAIL: buildifier found errors and/or warnings in above BUILD files.\r\n> buildifier suggested the following changes:\r\n> 472d471\r\n> <         \"//tensorflow/c/eager:c_api\",\r\n> 474a474\r\n> >         \"//tensorflow/c/eager:c_api\",\r\n> Please fix manually or run buildifier <file> to auto-fix.\r\n\r\nTabs?", "@caisq I'm not sure what the problem is exactly. Should I be using tabs for the BUILD files? Currently they are all indented with spaces (4 spaces per indentation level).", "@eaplatanios I'm not sure either. The diff in the error message seems to show two exactly identical lines, which is confusing to me, too. Yes, you should be using 4 spaces, instead of tabs. I'll let you know when I come up other thoughts. ", "@tensorflow-jenkins test this please"]}]