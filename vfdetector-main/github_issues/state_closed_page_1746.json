[{"number": 516, "title": "Feature Request: AdaDelta optimizer", "body": "AdaDelta (http://arxiv.org/abs/1212.5701) is a popular training algorithm for Neural Network. It is available in most other libraries I have used, like Torch or chainer. \n\nIn my small personal experience (and at least for the Neural Networks I have used), AdaDelta is often the optimizer that works best out of the box (RMSProp working quite well as well, while Adam, Adagrad and SGD typically being not as good)\n\nTherefore, it would be nice if AdaDelta could be added to the set of available optimizers.\n", "comments": ["I tried to give it a shot in [this branch](https://github.com/tensorflow/tensorflow/compare/master...Mistobaan:master)\n\nI tried to follow as much as possible the paper and the other training operations, but as Is my first tensorflow operator I would like some feedback and guidelines if anyone has time to review the code.\n", "@martinwicke I think we can close this issue. The adadelta operator is in the codebase\n", "thank you!\n"]}, {"number": 515, "title": "Update mnist_softmax.py", "body": "Closing the interactive session in the end\n", "comments": ["Hi @sdemyanov, thanks (again) for this change.  The same comment as the other pull request applies here :(.\n"]}, {"number": 514, "title": "MNIST works, but word2vec_basic does not", "body": "Ubuntu 15.10, tensorflow 0.6.0, Nvida Titan X\n\nAll of my MNIST example runs work fine. Other examples work great too.\n\nCan confirm they use the GPU fine with log output.\n\nHere's the output of word2vec_basic.py\n\n```\n~/tensorflow/tensorflow/examples/tutorials/word2vec$ python word2vec_basic.py\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nFound and verified text8.zip\nData size 17005207\nMost common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\nSample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n3084 -> 5239\noriginated -> anarchism\n3084 -> 12\noriginated -> as\n12 -> 6\nas -> a\n12 -> 3084\nas -> originated\n6 -> 195\na -> term\n6 -> 12\na -> as\n195 -> 2\nterm -> of\n195 -> 6\nterm -> a\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 11.21GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 10.65GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0xb06c80000 extends to 0xdb07d7e67\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 8\nInitialized\nTraceback (most recent call last):\n  File \"word2vec_basic.py\", line 189, in <module>\n    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'GradientDescent/update_Variable_2/ScatterSub': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n     [[Node: GradientDescent/update_Variable_2/ScatterSub = ScatterSub[T=DT_FLOAT, Tindices=DT_INT64, use_locking=false](Variable_2, gradients/concat_1, GradientDescent/update_Variable_2/mul)]]\nCaused by op u'GradientDescent/update_Variable_2/ScatterSub', defined at:\n  File \"word2vec_basic.py\", line 163, in <module>\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 188, in minimize\n    name=name)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 289, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/training/gradient_descent.py\", line 59, in _apply_sparse\n    return var.scatter_sub(delta, use_locking=self._use_locking)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 392, in scatter_sub\n    use_locking=use_locking)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 227, in scatter_sub\n    name=name)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/paul/.virtualenvs/mypy/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["Just had the same error on Ubuntu 14.04 + NVIDIA GeForce GTX 780 Ti.\n", "I have the same problem (TensorFlow 0.6.0, Ubuntu 15.10, GeForce GTX 750 Ti). `word2vec` works correctly when using CPU-only version of TensorFlow.\n", "I think that's related to #305. You currently need to force the embedding op onto the CPU. Should be an easy fix, I will submit a PR.\n"]}, {"number": 513, "title": "Support for MLP batch normalization", "body": "any plans to support batch normalization for MLP soon?\n", "comments": ["Would be a helpful feature.\n", "https://github.com/tensorflow/tensorflow/issues/804\n"]}, {"number": 512, "title": "Possible bug in attention seq2seq", "body": "source file is seq2seq.py\n\n```\ndef attention(query):\n  \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n  ds = []  # Results of attention reads will be stored here.\n  for a in xrange(num_heads):\n    with vs.variable_scope(\"Attention_%d\" % a): \n      y = rnn_cell.linear(query, attention_vec_size, True)\n      y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n      # Attention mask is a softmax of v^T * tanh(...).\n      s = math_ops.reduce_sum(\n          v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3]) \n      a = nn_ops.softmax(s)\n      # Now calculate the attention-weighted vector d.\n      d = math_ops.reduce_sum(\n          array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n          [1, 2]) \n      ds.append(array_ops.reshape(d, [-1, attn_size]))\n  return ds\n```\n\nat line 440, you compute the energies, ok\u2026 then at line 441 you normalize the energies\u2026 you need to do a -FLT_MAX mask on only the part of the encoder sequence (i.e., you don\u2019t want to pay attention to \u201cempty\u201d part of the sequence)... I don't see the masking op? Or am i missing something?\n\nI can wrap an op patch for this if nobody is working on this (let me know).\n\nThanks all!\n", "comments": ["I was thinking about it and there were 2 things that stopped me from working on it. The first and most important one is that I get good results without any special handling for masks, just as it is. Are you sure that a -FLT_MAX really helps, did you try without it? (I think the net just learns to never attend to the padding easily.) The second reason was that one day, when \"while\" works ok, we might not need to pad at all -- and I did not want to add complexity to the API that is not applicable to that case.\n\nThat said, if you find it important and want to do it -- by all means, send a patch :).\n", "> I get good results without any special handling for masks, just as it is\n\nI too have been using this for about a month, and I also get good results just as it is. I think it learns to never attend to padding as that is a simple if statement. In keras, we also talked about this, and concluded that masking was not necessary because the network learns to ignore padding. \n", "But for very short sequences (sequences with a lot of padding in a mini-batch that also comprises much longer sequences), this will result in an increase in the effective softmax attention temperature. I think this equivalent to having an additional implicit hyperparameter that changes for different input sequence lengths. The network might be somewhat robust to it as long as the input sequences do not vary too much but it is probably misleading for the user to have something that does not follow the mathematical description of most papers in the literature. That might be the source of hard to debug confusion when trying to reproduce the results of experiments done by other researchers with other tools.\n", "Indeed, it's a good argument. Still, as said before, the right solution to be close to the formalism is probably to use \"while\" (i.e., make the graph dynamically unroll) and then avoid padding altogether. We're working on this and did not want to polute the API with elements that will not be used then (as the padding symbol). But it will take a while before it's ready...\n", "@lukaszkaiser even w/ While, wouldnt this still be an issue? esp if the sequences in the minibatch have high variances in sequence length.\n", "> The network might be somewhat robust to it as long as the input sequences do not vary too much  \n\n@ogrisel , if you apply enough different types of bucket sizes then you wouldn't really run into this problem. All the sequences would roughly be within 5 or 7 timesteps (assuming you're doing NLP seq2seq). \n", "You're right Will - if there is variance in minibatches this might always be a problem. And let's not forget there is another problem with padding - we start the encoder from the first batch element, so the state goes through a bunch of pad symbols before the real sentence starts. Both could be corrected if the padding symbol was passed to the function and it constructed appropriate masks - we should probably do that. Are you guys willing to take a shot at it? (I'll probably not manage to do it before leaving for Christmas.)\n", "fyi, i wrote a simple op for this ... needs to be refactored and polished up before a patch can be submitted.\n\nimpl can be found here for those who need it before its pushed upstream:\nhttps://github.com/wchan/tensorflow/blob/master/tensorflow/core/kernels/attention_mask_op_test.cc\n", "FYI, I've added \"MedianMask\" and \"WindowMask\" (same path as above github path); see: http://arxiv.org/pdf/1508.04395.pdf\n\nhowever, I still haven't been able to reduce their results.\n", "@lukaszkaiser: Can this issue be closed? \n", "Yes - let's close this and leave the seq2seq attention model as is (many people are using it successfully). We might implement another attention model with more features at some point, but that's a different issue.\n", "I have to say that fixing this bug helped my results a lot!\r\nI train a model of cnn + attention for OCR, and after adding a change to fix this bug my word error rate improved noticeably.\r\nI know that you are implementing a new dynamic attention mechanism, but until it is stable (documented and tested) many people will probably stay with the old mechanism, so it can be nice if you could write a documentation of the bug in your API.\r\n"]}, {"number": 511, "title": "Large RNN graphs, incredibly slow graph creation and step time...", "body": "When I build a large seq2seq RNN graph (i.e., 3000 timesteps encoder, and 100 time steps decoder), both the graph construction and step time is insanely slow.\n\nFor example, using approx 3000 timesteps of stacked 4 layers of GRUs (in a seq2seq model w/ attention), we get something like this:\n\n**\\* graph creation time ***\ncreate_encoder graph time 77.442740\ncreate_decoder graph time 7.298214\ncreate_loss graph time 0.934293\ncreate_optimizer graph time 349.426908\ncreate_model graph time 489.119399\n\ncreate_optimizer is the part where i call something like this (pulled from the tutorial):\n  def create_optimizer(self):\n    start_time = time.time()\n\n```\nparams = tf.trainable_variables()\n\nself.gradient_norms = []\nself.updates = []\nopt = tf.train.GradientDescentOptimizer(0.001)\n\ngradients = tf.gradients(self.losses, params)\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n\nself.gradient_norms.append(norm)\nself.updates.append(opt.apply_gradients(\n    zip(clipped_gradients, params), global_step=self.global_step))\n\nprint('create_optimizer graph time %f' % (time.time() - start_time))\n```\n\n**\\* step time ***\nstep_time: 142.356251001\n\nWhen I run nvidia-smi while the graph is computing , the utilization is something like 12% ... definitely something wrong...\n\nIn my personal framework (not TF), creating a graph of equal size takes approx 2 secs (as opposed to TF almost 8 mins) and the step time is less 4 seconds (as opposed to TF 2 mins).... something really weird is going on.\n\nI wasn't able to profile / debug it too much (are there public tools for that? i cant find anything on tensorboard). also, is there a command i can run to confirm how many weights i have (and the dimensions and the device location of the variables).\n\nI've confirmed with the tensorboard and log_device_placement=True, and seems like most of the graph is in the GPU (except for embedding layers).\n\nIf you want a link to my graph, send me an email.\n\nThanks All!\n", "comments": ["This is a known issue for large unrolled RNNs in TensorFlow, and we are working on (currently experimental) functionality to express loops directly in the graph, to avoid creating a very verbose representation for those models.\n\nOut of interest, is the step time you report for the first step, or a subsequent step? There is a non-trivial amount of setup work per node in the graph, but this only affects the first step. I would expect it to speed up for subsequent steps.\n", "1st step time: 144.380403996\n2nd step time: 30.2505180836\n\nstill too slow... my framework is around 4 seconds (using even bigger LSTM nodes, so something is definite wrong here)\n", "Is there a way to profile my graph? I want to see which sections of the graph corresponds to what part of the runtime.\n", "FYI... through trial and error, it seems like the encoder is the slower part, which is weird, because I'm passing in the sequence_length and there should be dynamic computation on that part.....\n", "Right now, we don't have TensorFlow-specific profiling tools available, but a sampling profiler (such as [`pprof`](http://goog-perftools.sourceforge.net/doc/cpu_profiler.html)) should give enough information to point you in the right direction.\n", "at each timestep of the backprop of the RNN chain, does it create a new dE/dW and then the optimizer does a reduction? or does the network write to the exact same dE/dW and no reduction is needed? i think this is part of the slowdown.\n", "Can you try to use one of those two (currently not on by default) gradient accumulation methods and see if this improves steptime:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L204\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L231\n\nAlso, for experimentation I would recommend to use a small batch_size=1 and make it bigger when everything works to keep memory down.\n\nOverall, we are fully aware that long unrolled RNNs/LSTMs are still problematic in TensorFlow, we are working on making it better.\n", "@ludimagister , not running the test because even the fprop alone is insanely slow compared to my framework (i.e., I do forward only pass and its still an order of magnitude behind). Right now I'm writing a monolithic RNN Op that does the entire RNN sequence (w/ dynamic computation but not dynamic memory) in 1 block.\n", "Lol @william.  How'd I know you were going to do this.\nOn Dec 15, 2015 12:24 PM, \"William Chan\" notifications@github.com wrote:\n\n> @ludimagister https://github.com/ludimagister , not running the test\n> because even the fprop alone is insanely slow compared to my framework\n> (i.e., I do forward only pass and its still an order of magnitude behind).\n> Right now I'm writing a monolithic RNN Op that does the entire RNN sequence\n> (w/ dynamic computation but not dynamic memory) in 1 block.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-164912084\n> .\n", "@ebrevdo hehehe? Actually, kinda stuck... if my Tensor is in GPU memory, how can I get the value in CPU memory? can't find any API function available in tensor.h or in Eigen Tensor for that, i.e., for the sequence_length to do the dynamic compute.\n", "You mark it as a host input when creating your kernel.  See I think my\nReverseSequence kernel.  That has a bunch of tricks.  You can also just\nregister your kernel as cpu only for now. Everything will stay on cpu.\nOn Dec 15, 2015 7:54 PM, \"William Chan\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo cant be that much harder than\n> DistBelief right? Actually, kinda stuck... if my Tensor is in GPU memory,\n> how can I get the value in CPU memory? can't find any API function\n> available in tensor.h or in Eigen Tensor for that, i.e., for the\n> sequence_length to do the dynamic compute.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165005210\n> .\n", "thanks for the pointer! no, i need GPU for sure... i dont have google infrastructure, definitely need GPUs :p\n", "OK FYI though it's not clear that you can work with multiple gpus in one\nkernel. I may be wrong.  Derek or Sherry can probably say more.  Depending\nhow you need to split data you may be limited with the current\ninfrastructure to tower-like computation.\nOn Dec 15, 2015 8:30 PM, \"William Chan\" notifications@github.com wrote:\n\n> thanks for the pointer! no, i need GPU for sure... i dont have google\n> infrastructure, definitely need GPUs :p\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165013977\n> .\n", "Bit late to this discussion, but I too am doing seq2seq in tf [here](https://github.com/LeavesBreathe/Project_RNN_Enhancement).\n\nI don't want to beat a dead horse, but I have slow step times, though my compilation isn't too bad. I currently run 2 layers of 1536 units (GRU). However, the biggest hurdle by far is the memory.\n\nWhen I run these two layers, each of them on a 980 TI, I get 99% usage, and I'm overclocking my GPUs. So I feel that the GPU is being used. However the usage percent is sinusoidal with a average of 50%, but I also had the same experience in Theano.\n\nBut because of memory usage, I can only use a batch size of 16 whereas in theano I could use a batch size of 256 easily. And it is the batch size that makes tf very slow for me. \n\nI know you guys are working hard on improving the memory allocator. I'm very appreciative of it. Just wanted to give my 2 cents. \n", "i've coded up a monolithic gru op, it compiles but im getting a really weird dynamic link error when I do the python import statement for just \"import tensorflow as tf\":\n\nImportError: /home/wchan/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: _ZN5Eigen8internal14TensorExecutorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLm1ELi1ElEELi1EEEKNS_19TensorCwiseBinaryOpINS0_20scalar_difference_opIKfEEKNS7_INS0_17scalar_product_opIS9_S9_EEKNS3_INS4_IS9_Lm1ELi1ElEELi1EEESF_EESH_EEEENS_9GpuDeviceELb1ELb0EE3runERSL_RKSM_\n\nAnyone see something like that before? I'm guessing this must be related to the Eigen::Tensor?\n\nimpl path: https://github.com/wchan/tensorflow/blob/master/tensorflow/core/kernels/gru_op.cc\n", "```\n  // h[t] = z[t] .* h[t - 1] + (1 - z[t]) .* g[t]\n  dz.vec<float>().device(ctx->eigen_device<Device>()) =\n      dh.vec<float>() * h_prev->vec<float>() -\n      dh.vec<float>() * g.vec<float>();\n\n  dh_prev.vec<float>().device(ctx->eigen_device<Device>()) =\n      dh_prev.vec<float>() + dh.vec<float>() * z.vec<float>();\n\n  dg.vec<float>().device(ctx->eigen_device<Device>()) =\n      dh.vec<float>() * (z.vec<float>().constant(1.0f) - z.vec<float>());\n```\n\nam I allowed to do something like that when Device == GpuDevice\n\n_ZN5Eigen8internal14TensorExecutorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLm1ELi1ElEELi1EEEKNS_19TensorCwiseBinaryOpINS0_20scalar_difference_opIKfEEKNS7_INS0_17scalar_product_opIS9_S9_EEKNS3_INS4_IS9_Lm1ELi1ElEELi1EEESF_EESH_EEEENS_9GpuDeviceELb1ELb0EE3runERSL_RKSM\n\nnotice the \u201cTensor\u201d reference and \u201cGpuDevice\u201d in the name mangling \u2026 I\u2019m guessing this must be a template error, and the GpuDevice specialization failed\n", "ok... I think those are the problem, is there an example I can follow to add a custom CwiseBinaryOp for Eigen::Tensor that will work on the GPU, or should I wrote my own CUDA kernel? What's the proper way to impl this?\n", "You should be able to use Eigen ops with the gpu device. This seems like a\nbug with user op declarations in the build rules.\nOn Dec 16, 2015 7:05 PM, \"William Chan\" notifications@github.com wrote:\n\n> ok... I think those are the problem, is there an example I can follow to\n> add a custom CwiseBinaryOp for Eigen::Tensor that will work on the GPU, or\n> should I wrote my own CUDA kernel? What's the proper way to impl this?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165338454\n> .\n", "FYI, I've got a working implementation for \"Gru\" and \"GruCell\" .. GruCell is 1 timeslice monolithic op, while \"Gru\" operates on a sequence (dynamic computation). GruCell is roughly 20% faster per step (on TITAN X), and both ops have massive edge in the graph creation time (esp if you unroll large timesteps). The code is by no means optimized at this point (using 6 matrices atm, proper impl should concat them and use 3 matrices), and lots of code cleanup -- but if anyone needs/wants to try the code, give me a ping and I can point you to the right direction.\n\nI'll work w/ @ebrevdo to get this pushed to upstream \"soon\", but atm, I have other priorities. Also, not in the mood of learning how to write tests : p\n", "That sounds great. Please send to @evrevdo or me and we'll get this in. As\nof today (I think) we also accept outside contributions so that's another\noption.\n\nOn Sat, Dec 19, 2015 at 12:37 AM, William Chan notifications@github.com\nwrote:\n\n> FYI, I've got a working implementation for \"Gru\" and \"GruCell\" .. GruCell\n> is 1 timeslice monolithic op, while \"Gru\" operates on a sequence (dynamic\n> computation). GruCell is roughly 20% faster per step (on TITAN X), and both\n> ops have massive edge in the graph creation time (esp if you unroll large\n> timesteps). The code is by no means optimized at this point (using 6\n> matrices atm, proper impl should concat them and use 3 matrices), and lots\n> of code cleanup -- but if anyone needs/wants to try the code, give me a\n> ping and I can point you to the right direction.\n> \n> I'll work w/ @ebrevdo https://github.com/ebrevdo to get this pushed to\n> upstream \"soon\", but atm, I have other priorities. Also, not in the mood of\n> learning how to write tests : p\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165960962\n> .\n", "Assigning to @ebrevdo since he's done some work recently on this.\n", "I wrote a bunch of monolithic ops for the RNNs, is there interest for me to\npush this upstream? I can work w/ Eugene on this.\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Wed, Feb 17, 2016 at 7:16 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Assigning to @ebrevdo https://github.com/ebrevdo since he's done some\n> work recently on this.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185524428\n> .\n", "Have you tried dynamic_rnn for comparison?\nOn Feb 17, 2016 7:21 PM, \"William Chan\" notifications@github.com wrote:\n\n> I wrote a bunch of monolithic ops for the RNNs, is there interest for me to\n> push this upstream? I can work w/ Eugene on this.\n> \n> ## \n> \n> William Chan\n> Carnegie Mellon University\n> (650) 450-9455\n> williamchan.ca\n> \n> On Wed, Feb 17, 2016 at 7:16 PM, Vijay Vasudevan <notifications@github.com\n> \n> > wrote:\n> > \n> > Assigning to @ebrevdo https://github.com/ebrevdo since he's done some\n> > work recently on this.\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185524428\n> > \n> > .\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185525264\n> .\n", "i haven't... but looking at the source, it still won't fix the memory issue\n(i.e., for long sequences > 2000 timesteps)? i.e., the monotholic op is\nmuch more efficient in the memory, (at least until TF supports optimization\nby merging ops?)\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Wed, Feb 17, 2016 at 7:37 PM, ebrevdo notifications@github.com wrote:\n\n> Have you tried dynamic_rnn for comparison?\n> On Feb 17, 2016 7:21 PM, \"William Chan\" notifications@github.com wrote:\n> \n> > I wrote a bunch of monolithic ops for the RNNs, is there interest for me\n> > to\n> > push this upstream? I can work w/ Eugene on this.\n> > \n> > ## \n> > \n> > William Chan\n> > Carnegie Mellon University\n> > (650) 450-9455\n> > williamchan.ca\n> > \n> > On Wed, Feb 17, 2016 at 7:16 PM, Vijay Vasudevan <\n> > notifications@github.com\n> > \n> > > wrote:\n> > > \n> > > Assigning to @ebrevdo https://github.com/ebrevdo since he's done\n> > > some\n> > > work recently on this.\n> > > \n> > > \u2014\n> > > Reply to this email directly or view it on GitHub\n> > > <\n> > \n> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185524428\n> > \n> > > .\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185525264\n> > \n> > .\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185527102\n> .\n", "Try it. Should solve a lot of memory issues.\nOn Feb 17, 2016 7:43 PM, \"William Chan\" notifications@github.com wrote:\n\n> i haven't... but looking at the source, it still won't fix the memory issue\n> (i.e., for long sequences > 2000 timesteps)? i.e., the monotholic op is\n> much more efficient in the memory, (at least until TF supports optimization\n> by merging ops?)\n> \n> ## \n> \n> William Chan\n> Carnegie Mellon University\n> (650) 450-9455\n> williamchan.ca\n> \n> On Wed, Feb 17, 2016 at 7:37 PM, ebrevdo notifications@github.com wrote:\n> \n> > Have you tried dynamic_rnn for comparison?\n> > On Feb 17, 2016 7:21 PM, \"William Chan\" notifications@github.com\n> > wrote:\n> > \n> > > I wrote a bunch of monolithic ops for the RNNs, is there interest for\n> > > me\n> > > to\n> > > push this upstream? I can work w/ Eugene on this.\n> > > \n> > > ## \n> > > \n> > > William Chan\n> > > Carnegie Mellon University\n> > > (650) 450-9455\n> > > williamchan.ca\n> > > \n> > > On Wed, Feb 17, 2016 at 7:16 PM, Vijay Vasudevan <\n> > > notifications@github.com\n> > > \n> > > > wrote:\n> > > > \n> > > > Assigning to @ebrevdo https://github.com/ebrevdo since he's done\n> > > > some\n> > > > work recently on this.\n> > > > \n> > > > \u2014\n> > > > Reply to this email directly or view it on GitHub\n> > > > <\n> > \n> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185524428\n> > \n> > > > .\n> > > \n> > > \u2014\n> > > Reply to this email directly or view it on GitHub\n> > > <\n> > \n> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185525264\n> > \n> > > .\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185527102\n> > \n> > .\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-185529697\n> .\n", "Thanks, 45197e07c0dc410bde203462992841b457e0f61c looks great!\n", "It will be awesome to get this working but I've run into multiple problems which I suspect are bugs. I opened a separate issue here #1306 \n", "That bug should be fixed at HEAD\nOn Feb 26, 2016 11:50 AM, \"Mohammed AlQuraishi\" notifications@github.com\nwrote:\n\n> It will be awesome to get this working but I've run into multiple problems\n> which I suspect are bugs. I opened a separate issue here #1306\n> https://github.com/tensorflow/tensorflow/issues/1306\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-189452127\n> .\n", "@ebrevdo: Is this resolved? \n", "Yes, using either functions.Defun or dynamic_rnn.\n", "Hi @ebrevdo ,\n\nI have tried `dynamic_rnn`. But it is still ultra slow when building the graph\n\nMy model is 200~300 steps encoder and 30 steps decoder with attention,\n\nThe mini model of 100 steps encoder and 30 steps decoder takes 400s to build.\n\nI cannot even build the full model on my machine (16 G Memory) due to out of memory, while the same model without attention using mxnet takes approx 20% of the memory.\n\nCould you please provide any solution to that?\n\nThanks all\n", "Can you post a gist with your model so we can look at it and figure out\nwhat's slowing it down?\n\nOn Mon, Jun 13, 2016 at 8:13 PM, lightingghost notifications@github.com\nwrote:\n\n> Hi @ebrevdo https://github.com/ebrevdo ,\n> \n> I have tried dynamic_rnn. But it is still ultra slow when building the\n> graph\n> \n> My model is 200~300 steps encoder and 30 steps decoder with attention,\n> \n> The mini model of 100 steps encoder and 30 steps decoder takes 400s to\n> build.\n> \n> I cannot even build the full model on my machine (16 G Memory) due to out\n> of memory, while the same model without attention using mxnet takes approx\n> 20% of the memory.\n> \n> Could you please provide any solution to that?\n> \n> Thanks all\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-225768998,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtimwbQ_gGcBuel5rBgZ1l8n9dYL79Fks5qLhx0gaJpZM4G1LC9\n> .\n", "Hi @ebrevdo ,\n\nThank you for helping me out. The code was shown in \n\n[https://bitbucket.org/lightingghost/sent2mat](https://bitbucket.org/lightingghost/sent2mat)\n\nwith the model defined in seq2seq.py\n", "I see.  For now you should write your own encoder using tf.nn.dynamic_rnn.\nFor the decoder we are working on something equivalent to dynamic_rnn.\n\nOn Mon, Jun 13, 2016 at 9:21 PM, lightingghost notifications@github.com\nwrote:\n\n> Hi @ebrevdo https://github.com/ebrevdo ,\n> \n> Thank you for helping me out. The code was shown in\n> \n> https://bitbucket.org/lightingghost/sent2mat\n> \n> with the model defined in seq2seq.py\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-225776238,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtim1oVTC2F-LK5AHLVyPAuRMeoqwE2ks5qLiw9gaJpZM4G1LC9\n> .\n", "Oh, Thanks @ebrevdo. I will have a try on that.\n", "@lightingghost just to comment on this as I have heavily worked on seq2seq in tensorflow. I have found that I need at minimum 24gb of ram to have a working model of the timesteps you're describing. The demands go up even further to 32 or 64gb for large sized models (2048 sizes). Your 16gb limitation seems normal to me. \n", "@LeavesBreathe Thanks for letting me know that. Already ordered more RAMs on BestBuy =.=\n", "``` python\nwith tf.device('/cpu:0'):\n            print('create encoder...')\n            with tf.variable_scope(\"lstm_encoder\") as scope:\n                initializer = tf.random_uniform_initializer(-1., 1.)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers, state_is_tuple = True)\n\n                initial_state = cell.zero_state(batch_size, tf.float32)\n\n                encoder_outputs, _ = tf.nn.rnn(cell, inputs, initial_state = initial_state, sequence_length = early_stops)\n\n            print('create decoder...')\n            with tf.variable_scope(\"lstm_decoder\") as scope:\n                initializer = tf.random_uniform_initializer(-1., 1.)\n                # lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n                # cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers)\n                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers, state_is_tuple = True)\n\n                state = cell.zero_state(batch_size, tf.float32)\n\n                weight_input = self._variable_with_weight_decay('weight_context', [self.hidden_units, 1])\n                weight_context = self._variable_with_weight_decay('weight_input', [cell.state_size[0][0], 1])\n\n                for x in range(num_steps):\n                    if x > 0:\n                        tf.get_variable_scope().reuse_variables()\n                    # tanh\n                    w = []\n                    for y in range(num_steps):\n                        w.append(tf.tanh(tf.matmul(encoder_outputs[y], weight_input) + tf.matmul(state[0][0], weight_context)))\n\n                    # softmax\n                    w = tf.reshape(tf.pack(w), [num_steps])\n                    w = tf.exp(w - tf.reduce_max(w))\n                    w /= tf.reduce_sum(w)\n\n                    input_x = tf.zeros_like(encoder_outputs[0])\n                    for y in range(num_steps):\n                        input_x += encoder_outputs[y] * w[y]\n\n                    _, state  = cell(input_x, state)\n\n```\n\nI write a endocer-decoder, the decoder cost too much time,  still can't run, and there's no mistakes, anyway to modify this decoder?\n", "@qingzew you could try to use `tf.nn.dynamic_rnn` instead of `tf.nn.rnn`. I found that graph creation is way faster with dynamic_rnn. \n\nThe only difference is the input format. dynamic_rnn expects the input as a single tensor instead of a list of tensors. The input format as described in the documentation: \n\n```\nIf time_major == False (default), this must be a tensor of shape:\n        `[batch_size, max_time, input_size]`.\n      If time_major == True, this must be a tensor of shape:\n        `[max_time, batch_size, input_size]`.\n```\n", "@jakob-grabner  yes, I saw this function,  it can be used in encoder,  but I don't think it can be used in decoder, maybe this [Dynamic_RNN](https://github.com/qingzew/Dynamic_RNN_Tensorflow/blob/master/LSTM/LSTM.py) would be ok\n", "@jakob-grabner  is `dynamic_rnn` only faster in graph creation or actual computation time? It doesn't seem that it should be \n", "faster graph creation, slightly slower in computation\n\n> On Jun 30, 2016, at 3:15 PM, LeavesBreathe notifications@github.com wrote:\n> \n> @jakob-grabner is dynamic_rnn only faster in graph creation or actual computation time? It doesn't seem that it should be\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@LeavesBreathe definitely faster graph creation time, not sure about computation times. \n", "@wchan  thanks for your response -- Also big thanks for your attempts to make the LSTM faster. Really looking forward to that!\n\n@jakob-grabner  thanks for getting back but I'm pretty sure as @wchan stated its slower on comp time.\n", "by slower, william meant 2-3% (if that)\n\nOn Fri, Jul 1, 2016 at 8:09 AM, LeavesBreathe notifications@github.com\nwrote:\n\n> @wchan https://github.com/wchan thanks for your response -- Also big\n> thanks for your attempts to make the LSTM faster. Really looking forward to\n> that!\n> \n> @jakob-grabner https://github.com/jakob-grabner thanks for getting back\n> but I'm pretty sure as @wchan https://github.com/wchan stated its\n> slower on comp time.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-229971665,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtim3LUoP6XfXbx0qAHETBCoN2Rp2ibks5qRS2agaJpZM4G1LC9\n> .\n", "okay good to know thanks @ebrevdo \n", "I found tf.nn.dynamic_rnn is much slower than tf.nn.rnn in tensorflow_serving inference.\nUsing tf.**version** == 0.9.0\n", "Indeed, for inference you probably want to run some small fixed number of\ntime steps at a time yourself (since you need to decide when to stop based\non your business logic); in which case you just want to build a tf.nn.rnn\nwith that small number of inputs.\n\nOn Mon, Aug 1, 2016 at 1:07 AM, doubler notifications@github.com wrote:\n\n> I found tf.nn.dynamic_rnn is much slower than tf.nn.rnn in\n> tensorflow_serving inference.\n> Using tf._version_ == 0.9.0\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/511#issuecomment-236515239,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim9DtOT_4sFPRQga0tmW4wxghQD6wks5qbakvgaJpZM4G1LC9\n> .\n"]}, {"number": 510, "title": "Sequence labelling tutorial", "body": "Thank you for the great job and the tensorflow. But I don't think the tutorial on LSTM is clearly enough. In addition, I suggest adding sequence labelling tutorial on LSTM tutorial for better explanation.\n", "comments": ["I'm also interested in this.\n", "This request is from last here. Is it still open?\n", "Automatically closing due to lack of activity.\n"]}, {"number": 509, "title": "error with tf.app.flags in fully_connected_feed.py", "body": "I am getting the following error when I try to use flags. Can you please advise?\nThanks\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\nflags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\nflags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\nflags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\nflags.DEFINE_integer('batch_size', 100, 'Batch size.  '\n                     'Must divide evenly into the dataset sizes.')\nflags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\nflags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\n                     'for unit testing.')\n\nThe error is:\n\nusage: **main**.py [-h] [--learning_rate LEARNING_RATE]\n                   [--max_steps MAX_STEPS] [--hidden1 HIDDEN1]\n                   [--hidden2 HIDDEN2] [--batch_size BATCH_SIZE]\n                   [--train_dir TRAIN_DIR] [--fake_data FAKE_DATA]\n                   [--nofake_data]\n**main**.py: error: unrecognized arguments: -f /Users/sergulaydore/.ipython/profile_default/security/kernel-3561b41d-e378-459a-9e96-5333ea079712.json --profile-dir /Users/sergulaydore/.ipython/profile_default\n", "comments": ["How are you invoking your script? From the error message, it looks like you are passing `-f` and `--profile-dir` flags, but neither of them is defined in your code.\n", "Also, @craigcitro, in case he knows of a way to improve our thin flags wrapper in any way.\n", "In particular, from those flags you're passing, I'm guessing you're trying to do something related to IPython? If so, it's probably possible, but we'd need to know what you were trying to do.\n\n@vrv one option would be to use the [parse_known_args](https://docs.python.org/2/library/argparse.html#partial-parsing) method in `_flags.py`, which would let us parse a set of args but return the remaining ones for the caller to use as they'd like (eg pass them on to ipython). this would be a break from the python-gflags behavior, but possibly useful.\n", "Thank you so much for your answers but I am just learning tensorflow and was trying to run the tutorials. I am just using the script in https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py and getting the error above.\nIs it possible that there is a bug in the script provided by tensorflow?\n", "What was the command-line you used to invoke it? (Can you give us a pointer to where in the tutorial you were?)\n", "So, I was going through this tutorial: https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html#tensorflow-mechanics-101 and I got an error when reading the data which was \n\ndata_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)\n\nIt works fine if I just enter the directory instead of FLAGS.train_dir.\n", "Oh, I see -- are you just following the tutorial from within IPython?\n", "Exactly! Is that wrong? Should I run from Python shell?\n", "Well, I think it just means that we didn't anticipate that use case, but should have. :wink: As a short-term workaround, running from the python command-line should get you fixed up.\n\n@vrv it looks like we're going to need something like the fix I was talking about above.\n", "Indeed it worked in IPython command line :) Thank you very much!\n", "I think I fixed this in 1c579361cd1e088dd5e05a394b1561a73e3667ba\n"]}, {"number": 508, "title": "Document the broadcasting behavior for `tf.mul` etc.", "body": "The fact that many of the elementwise operators in TensorFlow support numpy-style broadcasting is not well documented or discoverable. (e.g. [This SO question.](http://stackoverflow.com/questions/34220374/apply-1-channel-mask-to-3-channel-tensor-in-tensorflow/34220963)) We should improve the documentation on this.\n", "comments": ["@mrry: Are you still planning to work on this?\n", "Not urgently, so perhaps \"contributions welcome\" is more appropriate.\n", "If anyone takes this on, it should go in the doc string at the top of `math_ops.py`.\n", "@girving Should this be a generic comment saying that TF uses numpy-style broadcasting? Or should it be specific to the operations that are affected by it (alternatively just mention the operations)?\n", "TF doesn't use numpy style broadcasting for all ops, just for elementwise operations (specifically binary elementwise operations).  Referring to elementwise ops is sufficient; there's no need to list them all.\n", "@girving Should this issue be closed? Or are there other changes that need to be made?\n", "We could have documentation on every elementwise op that supports broadcasting (numpy does mention broadcasting somewhere in the notes of every op.\n", "@vrv Putting it in every op seems unpleasant to maintain.\n", "Final thoughts on this? I agree with @vrv, I think we should have one sentence for each op that supports broadcasting.\n", "I'll reverse my previous position: putting a sentence in every op, ideally with a link to a more detailed discussion of broadcasting, would be great.\n", "@zffchen78 would you update the doc based on the comments in #3534?\n", "@zffchen78 Friendly ping, can you update this issue once it's fixed?\n", "I believe this has been \"fixed\" with a commit that added this to all of the element wise ops in their cc files.  \n"]}, {"number": 507, "title": "New type of decays, esp. for learning rate ", "body": "After playing around TF for multiple CNN models with different optimizers (SGD and Adam) I found it difficult to find the best time to decay from only steps number. \n- It would be nice if you add decay based on changes in a loss, as it was common, if loss doesn't change for a threshold of a step and amount, then decay it. \n- The other feature may be useful is to increase the decay steps as it goes further, for example decay it each 200 steps for the first 2,000 steps and then decay it for each 1,000 steps to the rest or another decay level which can be decay it only each 5,000 after step 100,000.\n", "comments": ["Closing this issue due to lack of activity."]}, {"number": 506, "title": "ptb_word_lm.py error: unsupported operand type(s) for /: 'Tensor' and 'int'", "body": "Hi,\n\nI ran the `ptb_word_lm` example from `models/rnn/ptb`. however I got the `unsupported operand type(s) for /: 'Tensor' and 'int'` for line 139 in `ptb_word_lm.py`.\n\nI also managed to fix it, by changing \n139 `self._cost = cost = tf.reduce_sum(loss) / batch_size`\nto\n139 `self._cost = cost = tf.div(tf.reduce_sum(loss), tf.constant(batch_size, dtype=tf.types.float32))`\n\nAnd it works fine so far.\n", "comments": ["Thanks for letting us know about this. Could you tell me whether you're using Python 2 or 3? (Although we're using `from __future__ import division`, this smacks of a Python 3 compatibility issue....)\n", "I'm using 2.7, however when I commented out the `from __future__ import division` the original code worked fine. So it is some \"v3\" trickery.\n", "This may be a 'feature' -- tensors are intentionally rather strict about\nimplicit conversions, so I think they don't have **olddiv**. Try casting\nthe int to a float.\n\nOn Mon, Dec 14, 2015 at 1:00 PM Martin Tutek notifications@github.com\nwrote:\n\n> I'm using 2.7, however when I commented out the from **future** import\n> division the original code worked fine. So it is some \"v3\" trickery.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/506#issuecomment-164557689\n> .\n", "Attempt 1:\n`from __future__ import division`\n`self._cost = cost = tf.reduce_sum(loss) / float(batch_size)`\n\n> TypeError: unsupported operand type(s) for /: 'Tensor' and 'float'\n\nAttempt 2. maybe numpy?\n`from __future__ import division`\n`self._cost = cost = tf.reduce_sum(loss) / np.array(batch_size, dtype=np.float32)`\n\n> TypeError: unsupported operand type(s) for /: 'Tensor' and 'float'\n\nWith `from __future__ import division` just the way in the original issue I wrote seems to work.\n", "@mttk: what do you get when you print\n`tf.__version__`?\n", "@vrv you got it :)\n\nFirst I got `AttributeError: 'module' object has no attribute '__version__'`\n\nAfter updating to 0.6.0 , the original code works fine. My bad, sorry for the mistake guys!\n"]}, {"number": 505, "title": "Momentum and Adagrad don't work with reshape and embeddings", "body": "Momentum and Adagrad optimizers do not work when I use `tf.reshape` like this:\n\n``` python\nembeddings = tf.Variable(                                                                                                  \n    tf.random_uniform([50000, 50], -1.0, 1.0))\n\nembed = tf.reshape(                                                                                                        \n        tf.nn.embedding_lookup(embeddings, input_op),                                                                          \n        [-1, em_layer_size])\n\n# a few dense layers and a softmax would follow here\n```\n\nThe code above works with SGD and AdamOptimizer. With Momentum or Adagrad it produces this error:\n\n```\n    optimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 188, in minimize\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 289, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/momentum.py\", line 70, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1836, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1476, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/training_ops.py\", line 130, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 527, in merge_with\n    self.assert_same_rank(other)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(50)]) must have the same rank\n```\n\nThis might be connected to #464, the error is similar and also appears with Momentum and Adagrad. \n\nWhen I replace the `tf.reshape` with following code, the error disappears:\n\n``` python\n_emb = []                                                                                                                 \nfor x in tf.split(1, 4, input_op):                                                                          \n    _emb.append(tf.nn.embedding_lookup(embeddings, tf.squeeze(x)))                                                        \nembed = tf.concat(1, _emb) \n```\n", "comments": ["I'll try to track down this issue. Tomas, would you be able to write a self-contained example that fails with this error (including a definition of the input, any size constants, and the momentum optimizer itself)?\n", "Indeed, the graph where I had the bug #464 was also using this mix of embedding layer and reshape. So I might have misinterpreted, and there is actually two bugs: one with \"tf.reshape\" and Momentum and Adagrad, and the other with \"tf.nce_loss\" and RMSProp... I will try to check further...\n", "Here is a small self-contained example that demonstrates the issue:\n\n```\nimport collections\nimport numpy as np\nimport tensorflow as tf\nimport logging\nimport codecs\nimport json\nimport itertools\nimport time\n\ndef device_for_node(n):\n    if n.type == \"MatMul\":\n        return \"/gpu:1\"\n    else:\n        return \"/cpu:0\"\n\nminibatch_size = 128\nhidden_size = 64\nembedding_size = 256\ninput_layer_size = 3\nvocab_size_input = 32\nvocab_size_output = 64\nnce_num_sampled = 16\nlearning_rate = 0.1\nlearning_momentum = 0.9\n\ndummy_input = np.zeros((minibatch_size, input_layer_size), dtype = np.int32)\ndummy_target = np.zeros((minibatch_size, 1), dtype = np.int32)\n\ninput_layer_flattened_size = input_layer_size * embedding_size\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    with graph.device(device_for_node):\n        input_layer = tf.placeholder(tf.int32, shape = (minibatch_size, input_layer_size), name = \"input_layer\")       \n        ref_input = tf.placeholder(tf.int32, shape = (minibatch_size, 1), name = \"ref_input\")\n\n        # Parameters\n\n        input_embeddings = tf.Variable(tf.random_normal([vocab_size_input, embedding_size]), name = \"i_embeddings\")\n\n        Wh_i = tf.Variable(tf.random_normal((input_layer_flattened_size, hidden_size), stddev = 0.2), name = \"Wh_i\")\n        bh_i = tf.Variable(tf.random_normal((hidden_size,), stddev = 0.2), name = \"bh_i\")\n\n        Wh_o = tf.Variable(tf.random_normal((vocab_size_output, hidden_size), stddev = 0.2), name = \"Wh_o\")\n        bh_o = tf.Variable(tf.random_normal((vocab_size_output,), stddev = 0.2), name = \"bh_o\")\n\n        # Layers\n\n        i_embedded = tf.nn.embedding_lookup(input_embeddings, input_layer)\n        i_embedded_flattened = tf.reshape(i_embedded, \n                        (\n                         (minibatch_size if minibatch_size is not None else -1), \n                         input_layer_flattened_size ) \n                        )\n\n        h = tf.tanh(tf.matmul(i_embedded_flattened, Wh_i) + bh_i)\n        nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled, \n                                                     num_classes = vocab_size_output, name = \"nce\"))\n        optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n\n        init_op = tf.initialize_all_variables()\n\n\nwith tf.Session(graph=graph) as session:\n\n    feed_dict = {input_layer : dummy_input, ref_input : dummy_target}\n    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n```\n", "This is the smallest example I was able to write. Embeddings seem to be an important part of the problem.\n\n``` python\n#!/usr/bin/env python\n\nimport tensorflow as tf\n\ninput_op = tf.placeholder(tf.int32, shape=(7, 3))\ntarget_op = tf.placeholder(tf.float32, shape=(7))\n\nembeddings = tf.Variable(tf.random_uniform((13, 5), -1.0, 1.0))\nemb = tf.nn.embedding_lookup(embeddings, input_op)\n\n# This does not work with Momentum:\nreshaped = tf.reshape(emb, (-1, 15))\n\n# This works with Momentum:\n#_emb = []\n#for x in tf.split(1, 3, input_op):\n#    _emb.append(tf.nn.embedding_lookup(embeddings, tf.squeeze(x)))\n#reshaped = tf.concat(1, _emb)\n\nW = tf.Variable(tf.random_uniform([15, 1], -1.0, 1.0))\nloss = tf.nn.sigmoid_cross_entropy_with_logits(tf.matmul(reshaped, W), target_op)\n\n#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\noptimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)\n#optimizer = tf.train.AdagradOptimizer(0.01).minimize(loss)\n#optimizer = tf.train.AdamOptimizer().minimize(loss)\n```\n", "On further investigation, this looks like a bug in `tf.gather()`'s gradient, when the indices are >1-dimensional. I'm working on a fix, and it should be available shortly.\n", "Nice. Thank you.\n"]}, {"number": 504, "title": "Transfer learning tutorial", "body": "The [image recognitions tutorial](https://www.tensorflow.org/versions/master/tutorials/image_recognition/index.html#usage-with-python-api) suggests exploring transfer learning as an exercise. I would like to see a tutorial dedicated to the topic and would be especially interested in transfer learning using the Inception-V3 model.\n", "comments": ["If there is anyone with more knowledge than me, I'd be interested in collaborating on this.\n", "+1\n", "It could be interesting to add something about extension of net archs without scratch retraining if there are plans to introduce [Net2Net](http://arxiv.org/abs/1511.05641) in Tensorflow. There is already an [initial Torch implementation](https://github.com/soumith/net2net.torch).\n", "I think this Caffe tutorial is a pretty good template http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html\n", "+1 A tutorial on transfer learning would be fantastic!\n", "+1\n", "@goodfeli Is there any plan to make a PR of your Net2Net tensorflow implementation?\n", "@bhack Currently there is no plan in place. Tianqi wrote our Net2Net implementation during his summer internship, before TensorFlow was ready for release. API changes to TensorFlow that preceded the release broke the Net2Net code. I currently don't have time to repair it because I am working full time on the Deep Learning textbook (www.deeplearningbook.org).\n\nAlso, Net2Net is \"knowledge transfer\" which is different from \"transfer learning.\" Transfer learning is when one net does well on two tasks, because it can transfer something it has learned from the first task to the second task. Knowledge transfer is when two nets do well on the same task, because the second net has copied the knowledge of how to do the task from the first.\n", "Thank you I did not know this specific sub differentiation. I thought that you can easily mix both: transfer on another task and then let the arch to expand in width and depth to better adapt on the new task.\n", "/cc @tqchen\n", "+1\n", "Check out examples/image_retraining on master. It's not a tutorial yet, but @petewarden is working on it.\n", "@martinwicke This is fantastic! Thanks for alerting us.\n", "To be explicit this is an example of retraining the final layer of Inception and leaving the convolutional weights untouched. We see this as being at one end of the spectrum; it's extremely quick to train (e.g. 30 mins on a single CPU), but won't give quite as good results as fine-tuning all the weights (which takes longer).\n\nWe plan to support full retraining too, and training from scratch, but wanted to make this example available as soon as it was complete. We haven't updated the website yet (we're still sorting out the large protobuf loading issue that currently requires hacks to the protobuf lib source - #836) but the usage guide is here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/image_retraining/index.md\n", "@petewarden Pete, this is a great start. Thanks for your work on this.\n", "Agreed. Thanks Pete\n", "@petewarden Thank you for this great work. We are waiting full retraining too.\n", "@petewarden  I  follow the instructions in order to retrain the model with new flower photos. I put both  new output files (pb and txt file) to the label image example. I get this error\n\n`E tensorflow/examples/label_image/main.cc:302] Running model failed: Not found: FetchOutputs node softmax: not found`\n", "You need to pass in the name of the new output layer, '~~final_output~~' (_edit_ - that should be 'final_result'), as the output_layer flag to label_image. I'll update the tutorial documentation to add this info, apologies for the confusion.\n", "+1\n", "Can anyone help me out in modifying the android demo? I successfully trained with the flowers data set and put the .pb and .txt and in the assets folder and did the respective changes to the names of the files in the source code. When I ran it, I didn't see any labelling happening. I tried changing the number of classes in the TensorFlowListener class but that didn't do anything. I believe I have to do something similar to as @petewarden mentioned with the output_layer flag(?!) but I don't know where to do that in the android demo.\n", "@petewarden Hi pete, I also ran the label image example and the --output_layer flag should point to 'final_result' but not 'final_output' as it is defined in the retrain.py\n", "Some more things I tried to do with the android demo were changing the output_tensor_names output:0 to pool_3, pool_3:0, softmax and input tensor name input:0 to Mul, Mul:0. I am confused that in the main.cc of label images example, Mul and pool3 etc. works whereas output:0 and input:0 don't work and this is the opposite case in the android demo. \n", "Apologies for my typo above, I've corrected it to read final_result. I will also be adding examples of how to load the models in label_image and classify_image.py.\n\nThe port/without port syntax is confusing too, I'll look into why that's different between the demos. In general we use the bare name to reference an operator, and the port version to grab the output of an op.\n", "@petewarden Hi Pete, thank you for your reply. I have been hammering on this all day and still haven't figured it out. I noticed there is a GraphDefBuilder that creates the nodes for the label_image example. Where is that in the Android Demo? Are the nodes created in some different way? If so, is it possible that not having the GraphDefBuilder is causing the port/without port problem? Ty. \n", "The Android code doesn't use GraphDefBuilder, it just loads the file directly into a GraphDef:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/tensorflow_jni.cc#L91\nAs long as you make sure you've added your model file as an asset, that should work to load it.\n\nThere are some additional parameters you'll need to change, like the input image size and the 'mean' to subtract from all the inputs, and you'll need to add a standard deviation parameter too:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/tensorflow_jni.cc#L45\n\nIf you look at the default values for those flags in label_image, that should help.\n\nWhat's the exact error that you're hitting by the way? That may help be more targeted in my ideas.\n", "@petewarden hi Pete, thanks for your help. The error is that when I load the retrained model with the new label file into the Android demo, I don't see any classification happening when I point the camera to a rose or a tulip. \n", "@petewarden I guess I shouldn't be calling it an error. Thing is I want to replicate your Image_retrain example on the android demo. The part of the tutorial where it says to change --outut_layer = final_result,  I want to do that in the android demo after loading retrained model since I'm assuming the android demo's output layer is not pointing to the final_result. \n", "I've filed #1253 to cover the work involved in updating the Android demo to accept a retrained model. There are no major code changes required, but it is a bit fiddly to find all the places where references are made to input and output layer names and other model-specific attributes. I think it will be easier to code up the changes than to try to explain them all, but I don't have an ETA for when I'll be able to get to this unfortunately.\n", "@petewarden Thanks pete. \n", "We would like to see this tutorial compatible with your new tensor flow serving project. Its very important to use this powerful mechanism remotely and get classification results for images. \n\n@petewarden I would greatly appreciate it if you could provide us a tutorial on how to send an image to a tensorflow serving server and get a response with the results.\n\nRegards\n", "@petewarden Hi Pete, I was able to load the newer inception model and a retrained version as well in the android demo. I have written a short reply to #1269 . The kernels BUILD file was missing \"batch_norm_op\" and \"check_numerics_op\". Adding them, changing the input node and output node to \"Mul\" and \"final_result\" (or \"softmax\" for the newer model), and increasing the protobuf file limit fixed the whole issue.\n", "@syed-ahmed \nIs your model just loading or also correctly recognizing your trained images? I looked at the label_image code and there is a difference in mapping rgb->float values in the Android code which I had to change. \nMy current model uses a not very good training set so I'm not 100% sure if the solution is correct...\n\nWhat I did to fix the mapping of the rgb-> float values:\n\nOriginal label_image code after loading the Images:\n`tensorflow::ops::Div(tensorflow::ops::Sub(\n                                              input_tensor, tensorflow::ops::Const({g_image_mean}, b.opts()), b.opts()),\n                         tensorflow::ops::Const({g_image_mean}, b.opts()),\n                         b.opts().WithName(\"final_result\"));`\n\nOriginal Android code snippet:\n `input_tensor_mapped(0, i, j, 0) =  static_cast<float>(src->red) - g_image_mean;`\n\nWhat is missing the the div by g_image_mean in mapping of the int->float rgb values:\n`input_tensor_mapped(0, i, j, 0) =  (static_cast<float>(src->red) - g_image_mean)/g_image_mean;`\n\nNow my images a recognized correctly.\n", "@savage7 yes, the model is correctly recognizing images. You are right that there is a difference in implementation between the label_image and android demo. I have seen that the android demo doesn't use the division op using the standard dev that is done in the label_image. I guess that is because of the way the input tensor is being filled up (mapping RGB values) in the android demo (I don't have a clear idea on the \"why\" of this but continued working with the assumption that the input tensor was being populated correctly). \n\nI tried doing the division you mentioned some days ago but it turned out that wasn't the problem. The BUILD file of the kernels was missing the \"batch_norm_op\" and the \"check_numerics_op\". \n", "The division sounds like it will give the correct results, but for the wrong reasons. :)\n\nThe key difference between the current Android demo code and the label_image example is that they use different versions of the Inception model. The current Android model requires a divisor of 1, and a subtraction of 117. You can see the image mean defined here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorflowImageListener.java#L51\nSince the divisor is 1, it's ignored in the current code.\n\nTo load the model that's used in label_image and image_retraining, you need to use a subtraction of 128 and a division of 128. You can see those values as mean and std in the command-line arguments here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L246\n", "@petewarden Hi Pete, so I loaded the newer inception model in the android demo with the input size of 299 and mean of 128. I got \"correct recognition\" however, I didn't divide the RGB values with 128 in the tensorflow_jni.cc. The division part looks like what we do in standardizing a random variable that is normally distributed? So since I didn't divide it with 128, does it mean that the recognition is still happening but since it's not standardized, it may fail for some cases?\n", "@petewarden Thank you so much for providing this tutorial. I'm working through it, attempting to use my retrained graph with the image_classify.py file, and I'm having issues with reading the graph back. I get this error:\n\n`KeyError: \"The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph.\"`\n\nSince the \"output_graph.pb\" file should in theory be the same as the \"classify_image_graph_def.pb\" file, is there a command that I am missing to make the \"output_graph.pb\" file readable by the image_classify.py file?\n\nThanks\n\nOren\n", "Try \"final_result\" for the tensor name.\n", "@syed-ahmed thanks for your help! That got me a little bit farther, but there's a fun new error now:\n\n`Invalid argument: NodeDef mentions attr 'align_corners' not in Op<name=ResizeBilinear`\n", "I was able to fix this error by updating Tensorflow\u2014however, it does not seem like my new labels are able to be printed. Is there a way to generate a UID lookup file for our new graph?\n", "Sorry you're hitting problems! Are you loading the new labels file with the --labels command line flag? Here's a full example, from https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html#using-the-retrained-model\n\n```\nbazel build tensorflow/examples/label_image:label_image && \\\nbazel-bin/tensorflow/examples/label_image/label_image \\\n--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \\\n--output_layer=final_result \\\n--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg\n```\n", "@petewarden Everything works fantastically if I do it with Bazel, but I'm trying to use the output_graph.pb files and output_labels.txt files with the image_classify.py file, not just the label image example. \n", "@oweingrod Hi, did you get your code working?\n", "Yes. You have to manually write the \".pbtxt\" file so your UIDs can map with your classes.\n", "Hi, I am working on retraining Inception's Final Layer for New Categories. I am following steps at - https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html#training\nThe build step(bazel build -c opt --copt=-mavx tensorflow/examples/image_retraining:retrain)for the retrainer is working good. But when I try to run the second step -   bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos, I encounter below error : -bash: bazel-bin/examples/image_retraining/retrain.py: No such file or directory\nI would appreciate some help or suggestions on this. Thanks.\n", "@dhananjaymehta: I would file a separate bug for that issue. \n", "@martinwicke Thanks for the suggestion, I will open a new bug for the issue.\n", "@oweingrod: can you point me to where i can learn how that is done (re: \"*.pbtxt\")?\ngot past `KeyError: \"The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph.\"` and it's the key to my next problem.\n\n```\n  File \"tensorflow/models/image/imagenet/classify_image.py\", line 82, in __init__\n    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n  File \"tensorflow/models/image/imagenet/classify_image.py\", line 125, in load\n    name = uid_to_human[val]\nKeyError: 'n02119789'\n```\n", "@josefmonje If you're retraining on your own images, then the label files which are generated will also be different. The default code uses two different files to map whatever prediction the model gave, into the human readable form. Is your data set also like that?\n\nThe mapping file which you require should be automatically generated while training with the bazel command. One .pb file, and one .txt file will be generated by default.\n", "@eldor4do: i have output_graph.pb and output_labels.txt from retraining. i'm missing the .pbtxt file this part of the code is supposed to look for it.\n", "@josefmonje Check from here, maybe this will be useful. https://github.com/eldor4do/Tensorflow-Examples/blob/master/retraining-example.py#L82\nI do not think it needs the .pbtxt file, only the output_labels.txt file should be enough.\n", "thanks @eldor4do! worked for me. made some edits to work with Python 3 and sent PR \ud83d\udc4d \n", "That's a very useful example @eldor4do, thanks! I will update the docs to point to that, if you don't mind?\n", "@petewarden Sure,  no problem!\n@josefmonje welcome! and i merged your PR\n", "@petewarden: This thread seems to have deviated quite a bit from the original request.  Should we mark it resolved and reopen other issues as required? \n", "+1\n\nOn Mon, Jun 6, 2016 at 11:51 AM Geoffrey Irving notifications@github.com\nwrote:\n\n> @petewarden https://github.com/petewarden: This thread seems to have\n> deviated quite a bit from the original request. Should we mark it resolved\n> and reopen other issues as required?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/504#issuecomment-224052212,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_SFTinXLttgqlwUgZtN09xXNO6Mgks5qJGw7gaJpZM4G0cHg\n> .\n", "Resolving for now, please open more specific issues if problems persist, or ask on StackOverflow if  you have questions.\n", "@syed-ahmed .\r\nCan you please tell me how you created .pb file from model.ckpt.\r\nThanks.\r\n", "@Bruczzz Hi, you can check out this script: [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py). Also, pete warden has posted a nice documentation on deployment. Check it out: [Graph Transform Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)", "Thanks @syed-ahmed .\r\nI tried retraining model specified in https://github.com/tensorflow/models/blob/master/inception/README.md\r\n and i tried this on CPU version of tensorflow . i can create model.ckpt files and checkpoints. How to convert into .pb file .\r\nAnd I tired https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/image_retraining/index.md\r\nthis way also. gen/head is not declared error occurred ... so i tried running ./configure , this is asking for cuda version and location. but i am not using GPU version. I dont know how to proceed further? Can you Please help?\r\n", "@syed-ahmed  I have figured out the issue. Its running fine now .\r\n\r\nIt will be helpfull ,if you tell how to train a model from scratch with our own image data set.\r\n", "Hi @petewarden ,\r\n\r\nThanks for such a usefull example for retraining inception.\r\nBut this sample uses a .pb file for retraining.Is there a method to use .ckpt for retraining?\r\nIf so any sample code would be of great help.\r\nThere is not much clarity if the same can be done for other opensource models such as Resnet50,101, etc\r\n\r\nIt would be very kind of you if you could guide on this.\r\n\r\nThanks ", "@Bruczzz it sounds like you were able to get around your .ckpt->.pb file problems. Can you please share what the fix was? ", "@cy89 since I also managed to do transfer learning with checkpoints I might as well chime in.\r\n\r\nNote that this is using tf.slim and python.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.slim.nets import inception as nn_architecture\r\nfrom tensorflow.contrib import slim\r\n\r\nwith slim.arg_scope(nn_architecture.inception_v3_arg_scope()):\r\n    logits, endpoints = nn_architecture.inception_v3(images, \r\n                                                     num_classes=10,\r\n                                                     is_training=True)\r\n\r\n#these nodes are not loaded from the checkpoint and are left uninitialized  \r\nretrain = ['InceptionV3/Logits', 'InceptionV3/AuxLogits', 'InceptionV3/Mixed_7c', 'InceptionV3/Mixed_7b'] \r\nvariables_to_restore = slim.get_variables_to_restore(exclude=retrain)  # this checks the current graph, so no custom nodes can be defined at this point, only those from create_network()\r\n\r\nsess = tf.Session()\r\nsaver = tf.train.Saver(variables_to_restore)\r\nsaver.restore(sess, CHECKPOINT_PATH)\r\n\r\n#specify which layers should be trained, all others will not be touched\r\nvariables_to_train = tf.trainable_variables('InceptionV3/Logits') + tf.trainable_variables('InceptionV3/AuxLogits') + tf.trainable_variables('InceptionV3/Mixed_7c') + tf.trainable_variables('InceptionV3/Mixed_7b')\r\n\r\ntrain_step = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=variables_to_train) \r\n```\r\n\r\n[Here](https://github.com/Syzygy2048/OverheadVehicleDetection/tree/master/mnist) you can find a working example for the mnist dataset with inception, resnet and resception"]}, {"number": 503, "title": "Request to add API documentation for using tf.flags", "body": "I saw some tensorflow project contains hyperparameter configuration like:\n`tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")`\n\nI think it is better to add corresponding document for using this feature. \n", "comments": ["Our policy so far has been only to document the features that we intend to support in TensorFlow long-term. Currently we use [`python-gflags`](https://github.com/gflags/python-gflags) as the implementation for `tf.flags`, but this is subject to change in future. In the meantime, the [documentation for that library](https://github.com/gflags/python-gflags/blob/master/gflags.py) is the best source of usage information.\n", "I'm not sure if TensorFlow will use `python-gflags` in long-term or not, it would be better to add the link to refer to `python-glags`'s documentation. Anyway, it seems that project has little document, either :(\n", "Now Is there any documentation for hyperparameter configuration like?"]}, {"number": 502, "title": "Native arm 32 bit compilation", "body": "Having issues compiling natively TensorFlow. The process fails over and over at this point:\n\n```\nerle@erle-brain-2 ~/tensorflow $ ../bazel-0.1.2/output/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --local_resources 1024,.5,1.0 -s --spawn_strategy=standalone --genrule_strategy=standalone --verbose_failures\n.............................................................................................................................................................................................\nINFO: Waiting for response from Bazel server (pid 15928)...\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nERROR: /home/erle/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@accessibility-developer-tools//': SSL peer shut down incorrectly and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 990.385s.\n```\n\nAlso tried with bazel `0.1.1`. Same result.\nThanks,\n", "comments": ["It looks like things are failing when Bazel tries to build TensorBoard. Have you had any success in getting another TensorFlow target to build? (e.g. Does `bazel test //tensorflow/python:constant_op_test` work?)\n", "Thanks @mrry for looking into this. [Here's](https://gist.github.com/vmayoral/ec85d8effa13308d5f66) the build output. Failed at the end. \n\nMore useful seems to be the log cited at the end https://gist.github.com/vmayoral/7665cf9ca9056a15167d. Apparently, there's an `ERROR: testTooLargeGraph (__main__.ConstantTest)`.\n", "These tests check for the correct behavior (the proper exceptions) when\ntrying to create graphs/constants that are too large for protobuf. The\nlimit is 2G/tensor. On your platform, you run out of memory trying to\nallocate/copy that, which is soft of expected given your address space.\n\nI think those two tests should be fixed to catch MemoryError and not count\nthat as a failure (since if your system cannot allocate 2G + 2G, you\nprobably won't be creating graphs that big).\n\nOn Tue, Dec 15, 2015 at 12:40 AM V\u00edctor Mayoral Vilches <\nnotifications@github.com> wrote:\n\n> Thanks @mrry https://github.com/mrry for looking into this. Here's\n> https://gist.github.com/vmayoral/ec85d8effa13308d5f66 the build output.\n> Failed at the end.\n> \n> More useful seems to be the log cited at the end\n> https://gist.github.com/vmayoral/7665cf9ca9056a15167d. Apparently,\n> there's an ERROR: testTooLargeGraph (**main**.ConstantTest).\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/502#issuecomment-164686186\n> .\n", "Thanks @martinwicke for clarifying.\nDoes that help anyhow with the initial issue? I'm still unable to generate a pip package for tensorflow (which AFAIK is needed to get tensorflow installed from the sources).\n", "Looks like this fell through the cracks.  Does anyone know if this is still an issue?\n", "Closing due to lack of response.\n"]}, {"number": 501, "title": "assertion when running on GPU with debug enabled", "body": "when I compile TensorFlow with --config=cuda  -c dbg --strip=never, I get an assertion when running the mnist example (same goes for cifar10. Also tried CUDA 7.5, with same outcome. Reproduces with both TF 0.5.0 and TF 0.6.0.)\n\nThe GPU being used is a Titan X.\n\n[~/tensorflow/tensorflow/models/image/mnist] python convolutional.py\n ......\npython: third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceType.h:734: void Eigen::assertCudaOk(): Assertion `err == cudaSuccess' failed.\nAborted (core dumped)\n\ndmesg shows that an illegal memory access was performed:\n[780540.251853] NVRM: Xid (PCI:0000:03:00): 31, Ch 0000000f, engmask 00000101, intr 10000000\n\nany when running with cuda-memcheck, I get:\n========= CUDA-MEMCHECK\n========= Invalid **global** read of size 4\n=========     at 0x000002f0 in /opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:346:_ZNK5Eigen15TensorEvaluatorIKNS_18TensorCwiseUnaryOpINS_8internal13scalar_log_opIfEEKNS_9TensorMapINS_6TensorIfLm2ELi1ElEELi1EEEEENS_9GpuDeviceEE6packetILi1EEE6float4l\n=========     by thread (15,0,0) in block (0,0,0)\n=========     Address 0x00000000 is out of bounds\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h:122:Eigen::TensorEvaluatorEigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice(long)>::evalPacket (Eigen::TensorEvaluatorEigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice(long)>::evalPacket : 0x350)\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:407:void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op<float>) (void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op<float>) : 0x1460)\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========     Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so (cuLaunchKernel + 0x2cd) [0x15865d]\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 [0x131b0]\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 (cudaLaunch + 0x143) [0x2d653]\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bc036]\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bb2f8]\n.....\n\nThe assertion changes from run to run (non determinism due to multi threading), but the cause is always the NULL pointer dereference in float4 Eigen::TensorEvaluatorEigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, 2ul, 1, long>, 1> const> const, Eigen::GpuDevice>::packet<1>(long) const\n\nCould be an nvcc compiler bug  in debug mode only (kernel runs fine when optimized)\n", "comments": ["Pinging @benoitsteiner, since this sounds like it might be an Eigen issue.\n", "The problem is probably that the scalar_log_op::packetOp method was not marked as EIGEN_DEVICE_FUNC. As a result nvcc doesn't generate the corresponding kernel code in debug mode. In optimized mode, it will inline the code so everything ends up working as expected.\n\nIn change 110406666, I updated the TensorFlow build configuration to pull the Eigen code from the upstream repository, where this problem has been fixed, instead of relying on our own local copy of the code. This issue will therefore be fixed in the next TensorFlow release (0.7). \n", "verified the fix. Thanks.\n"]}, {"number": 500, "title": "MacOS X El Capitan Tensorflow 0.6 upgrade error", "body": "Had a good-running 0.5 tensor flow, now upgrade to 0.6 with command line from Google instruction:\n\"sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl\", run into following exception:\n\"Installing collected packages: setuptools, protobuf, wheel, tensorflow\n  Found existing installation: setuptools 1.1.6\n    Uninstalling setuptools-1.1.6:\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/basecommand.py\", line 211, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/commands/install.py\", line 311, in run\n    root=options.root_path,\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_set.py\", line 640, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_install.py\", line 716, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/req/req_uninstall.py\", line 125, in remove\n    renames(path, new_path)\n  File \"/Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg/pip/utils/**init**.py\", line 315, in renames\n    shutil.move(old, new)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 299, in move\n    copytree(src, real_dst, symlinks=True)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 208, in copytree\n    raise Error, errors\nError: [('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', \"[Errno 1] Operation not permitted: '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', \"[Errno 1] Operation not permitted: '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', \"[Errno 1] Operation not permitted: '/tmp/pip-XBiBee-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib'\")]\n\nIt's stock Mac python 2.7.10 (without ipython).\n", "comments": ["Seems to be caused by new feature \"System Integrity Protection\"(SIP) in El Capitan. What's the best way to work-around it? There is an option to disable the SIP, but seems risky. It's possible to try virtual env. But is there a way in Mac stock python? After all, 0.5 version was working fine there.\n", "As a workaround you can try using homebrew python. It works for me with 2.7.11.\n", "Thanks. homebrew python 2.7.11 works well with 0.6 tensorflow.  \n", "After I update python 2.7.11 through homebrew, I still can't solve this problem.\n", "Finally I found out the root cause is that I install pip via easy_install before and then install homebrew python. The system version of pip would be called when you install tensorflow by pip. \n\nYou can uninstall easy_install and re-install python through homebrew, and then it works.\n", "adding \"--ignore-installed six\" to the \"sudo pip install\" command solved the problem for me\n", "@martin-gorner  Thanks!! I was able to install by adding \"--ignore-installed six\". \nGot the message: \"Successfully installed numpy-1.8.0rc1 protobuf-2.6.1 setuptools-1.1.6 six-1.10.0 tensorflow-0.6.0 wheel-0.26.0\"\n", "Thanks, @martin-gorner!\n", "Hello, I am still getting the error(s) described above, which I presume means that tensorflow 0.6 is not installing, would appreciate any further help.  \n\nThis is my first attempt at tf, and I have not had a successful installation, e.g. tf 0.5 as jwnsu did. \n\nI am using:\nEl Capitan (10.11.2); \n\npip 7.1.0 (doesn\u2019t seem to allow to upgrade further, e.g. to 7.1.2 as I\u2019ve seen others using, or 8.0.2 as the system says is available); \n\npython 2.7.10\n\nI did this:\n$ sudo -H pip install --ignore-installed six\n\nand got this:\n\n\"You are using pip version 7.1.0, however version 8.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nCollecting six\n  Using cached six-1.10.0-py2.py3-none-any.whl\nInstalling collected packages: six\nSuccessfully installed six-1.10.0\"\n\nI did this:\n\n$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl\n\nAnd got this (i.e. the same error messages):\n\n\"The directory '/Users/**_my name**_/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nYou are using pip version 7.1.0, however version 8.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe directory '/Users/m**_my name**_/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nCollecting tensorflow==0.6.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.6.0-py2-none-any.whl (10.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.2MB 50kB/s \nRequirement already up-to-date: six>=1.10.0 in /Library/Python/2.7/site-packages (from tensorflow==0.6.0)\nCollecting protobuf==3.0.0a3 (from tensorflow==0.6.0)\nCollecting wheel (from tensorflow==0.6.0)\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 69kB 1.2MB/s \nCollecting numpy>=1.8.2 (from tensorflow==0.6.0)\n  Downloading numpy-1.10.4-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.7MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.7MB 128kB/s \nCollecting setuptools (from protobuf==3.0.0a3->tensorflow==0.6.0)\n  Downloading setuptools-19.7-py2.py3-none-any.whl (472kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 475kB 713kB/s \nInstalling collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n  Found existing installation: setuptools 1.1.6\n    Uninstalling setuptools-1.1.6:\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip/basecommand.py\", line 223, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip/commands/install.py\", line 299, in run\n    root=options.root_path,\n  File \"/Library/Python/2.7/site-packages/pip/req/req_set.py\", line 640, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/Library/Python/2.7/site-packages/pip/req/req_install.py\", line 726, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/Library/Python/2.7/site-packages/pip/req/req_uninstall.py\", line 125, in remove\n    renames(path, new_path)\n  File \"/Library/Python/2.7/site-packages/pip/utils/**init**.py\", line 314, in renames\n    shutil.move(old, new)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 299, in move\n    copytree(src, real_dst, symlinks=True)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 208, in copytree\n    raise Error, errors\nError: [('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', \"[Errno 1] Operation not permitted: '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', \"[Errno 1] Operation not permitted: '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', \"[Errno 1] Operation not permitted: '/tmp/pip-6s30U8-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib'\u201d)]\"\n", "@mgottsch1  You will not be able to install directly using `pip` (at least, judging from my experience) because El Capitan introduced System Integrity Protection, which does not allow such things.\n\nI was able to install `tensorflow` in [virtual env](https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#virtualenv_install). So far, it runs great. \n\nAdditional tip (just in case): you could use jupyter notebook (which is also impossible to install directly using `pip`) for writing code.\n", "Thank you Oleksandra28, virtualenv did the trick.  Brilliant!\n\n> On Feb 7, 2016, at 9:36 PM, Oleksandra28 notifications@github.com wrote:\n> \n> @mgottsch1 https://github.com/mgottsch1 You will not be able to install directly using pip (at least, judging from my experience) because El Capitan introduced System Integrity Protection, which does not allow such things.\n> \n> I was able to install tensorflow in virtual env https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#virtualenv_install. So far, it runs great.\n> \n> Additional tip (just in case): you could use jupyter notebook (which is also impossible to install directly using pip) for writing code.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub https://github.com/tensorflow/tensorflow/issues/500#issuecomment-181168191.\n", "The command below works for me\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl --ignore-installed six\n\nWe should update the documentation here to avoid confusion.\nhttps://www.tensorflow.org/versions/r0.7/get_started/os_setup.html\n", "The command above works for me too.\n", "--ignore-installed six works for me as well. Thanks!\n", "nothing works for me !\n", "Similar, also got stuck installing tensorflow on Mac OSX El Captain.\nThe numpy version on El Caption is incompatible, and it is non-trivial to update numpy using pip.\n\"RuntimeError: module compiled against API version 0xa but this version of numpy is 0x9\"\n\nCollecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl (11.6MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.6MB 119kB/s \nCollecting six\n  Downloading six-1.10.0-py2.py3-none-any.whl\nCollecting protobuf==3.0.0b2 (from tensorflow==0.7.1)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 1.2MB/s \nCollecting wheel (from tensorflow==0.7.1)\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 10.9MB/s \nCollecting numpy>=1.10.1 (from tensorflow==0.7.1)\n  Downloading numpy-1.11.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.9MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.9MB 333kB/s \nCollecting setuptools (from protobuf==3.0.0b2->tensorflow==0.7.1)\n  Downloading setuptools-20.9.0-py2.py3-none-any.whl (508kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 961kB/s \nInstalling collected packages: six, setuptools, protobuf, wheel, numpy, tensorflow\nSuccessfully installed numpy-1.8.0rc1 protobuf-3.0.0b2 setuptools-1.1.6 six-1.10.0 tensorflow-0.8.0 wheel-0.29.0\n[~]$ python\nPython 2.7.10 (default, Oct 23 2015, 19:19:21) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > RuntimeError: module compiled against API version 0xa but this version of numpy is 0x9\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/Users/erwincoumans/Library/Python/2.7/lib/python/site-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/Users/erwincoumans/Library/Python/2.7/lib/python/site-packages/tensorflow/python/**init**.py\", line 45, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow\n> > >   File \"/Users/erwincoumans/Library/Python/2.7/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n> > >     _pywrap_tensorflow = swig_import_helper()\n> > >   File \"/Users/erwincoumans/Library/Python/2.7/lib/python/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> > > ImportError: numpy.core.multiarray failed to import\n", "--ignore-installed six works for me too. Thanks!\n", "Thank you 1337newbee, that command works for me.\n", "@erwincoumans and anyone else getting the error about:\n\n```\n\"RuntimeError: module compiled against API version 0xa but this version of numpy is 0x9\"\n```\n\n## Simple answer: \n\nrun `sudo easy_install numpy` to update to the correct numpy location.\n\n## Long answer:\n\nI found some good [info here](http://stackoverflow.com/questions/28517937/how-can-i-upgrade-numpy), specifically [here](http://stackoverflow.com/a/34640003/5224806).\n\nThis error occurs when there are two versions of numpy. Removing one doesn't work either. A guy suggests running this in python:\n\n```\nimport numpy\nprint numpy.__path__\n```\n\nTo tell you which numpy path it's using and the `sudo rm -rf`ing the directory, but operation will not be permitted. So alternatively, by running the command below, it will overwrite to the latest numpy path.\n\n`sudo easy_install numpy`\n\nNow, when you `import tensorflow as tf` it should find the right numpy.\n", "`sudo easy_install numpy` fixed the problem for me. Thanks!\n", "sudo easy_install --upgrade numpy fixed the problem for me.Thx\n"]}, {"number": 499, "title": "using #pragma once", "body": "Shouldn't we use both `pragma` once and `ifndef` gaurds ?\n", "comments": ["We can not replace the `#ifndef` guards with `#pragma once` for various reasons. What would be gained concretely from having both?\n", "If both are needed in any case, it would mean that the `#ifdef` guards are wrong and bugs will appear if `#pragma once` doesn't work.\n"]}, {"number": 498, "title": "Installation exception in EI Capitan", "body": "I just installed homebrew, and also set brew python as default.\nwhen I import tensorflow:\n\n`````` javascript\nPython 2.7.11 (default, Dec 13 2015, 19:34:28)\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 43, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 37, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.```\n\nwhat is the problem?\nI can't find the solution anywhere\n``````\n", "comments": ["Try: `brew update && brew reinstall --devel protobuf` see #258.\n", "De-duping with #258 -- reopen if this is different.\n", "`Error importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.`\n\nStill getting the above error on the latest El-Capitan released today\n", "Getting something similar. Any ideas?\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 8, in <module>\n    from google.protobuf import reflection as _reflection\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/google/protobuf/reflection.py\", line 58, in <module>\n    from google.protobuf.internal import python_message as message_impl\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/google/protobuf/internal/python_message.py\", line 53, in <module>\n    from io import BytesIO\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/io.py\", line 51, in <module>\n    import _io\nImportError: dlopen(/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_io.so, 2): Symbol not found: __PyCodecInfo_GetIncrementalDecoder\n  Referenced from: /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_io.so\n  Expected in: flat namespace\n in /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_io.so\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n", "@orcaman @Archivus I was having the same issue after installing tensorflow from bazel, on mac. I fixed it with\n\n```\nsudo pip uninstall tensorflow\nsudo pip uninstall protobuf\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n```\n\nie. Uninstalling both tensorflow and protobuf, then reinstalling\n", "``` bash\nTraceback (most recent call last):\n  File \"cnn_cropping.py\", line 48, in <module>\n    model.add(cropping.Cropping2D(cropping=((2,2),(2,2)) ))\n  File \"/Users/gnu/anaconda/lib/python2.7/site-packages/keras/models.py\", line 146, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/Users/gnu/anaconda/lib/python2.7/site-packages/keras/engine/topology.py\", line 485, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File \"/Users/gnu/anaconda/lib/python2.7/site-packages/keras/engine/topology.py\", line 543, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File \"/Users/gnu/anaconda/lib/python2.7/site-packages/keras/engine/topology.py\", line 148, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File \"/Users/gnu/Gnubox/keras_cropping/cropping.py\", line 56, in call\n    return x[:, :, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:-self.cropping[1][1]]\n  File \"/Users/gnu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 315, in _SliceHelper\n    \"Negative stop indices are not currently supported\")\nNotImplementedError: Negative stop indices are not currently supported\n```\n\nDoes anyone have any idea? I think I should be able to use `self.input_shape` to clarify the index, but \n\n``` python\n(Pdb) self.input_shape\n*** Exception: The layer has never been called and thus has no defined input shape.\n```\n\nit doesn't work. \n"]}, {"number": 497, "title": "Add reference to CONTRIBUTING.md", "body": "Can we add in some way a reference to https://github.com/tensorflow/tensorflow/issues/26\n", "comments": ["I'm not sure what this request was for, but #26 is closed so I assume this one is too.\n"]}, {"number": 496, "title": "tensorflow/core/user_kernels needed", "body": "correct me if I am wrong, but I believe a subdir of tensorflow/core/user_kernels is needed to support user written GPU kernels (CPU is fine), this is due to the build rules in tensorflow/core/BUILD when building the user_ops_op_lib is a cc_library rather than say tf_cuda_library.\n", "comments": ["@keveman is working on the workflow for user written kernels/ops.\n", "I added [instructions](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) for building ops and kernels outside of TensorFlow source tree. Please have a look to see if it fits your need.\n"]}, {"number": 495, "title": "cannot use tf.nn.elu(features, name=None)", "body": "Error Message:\nAttributeError: 'module' object has no attribute 'elu'\n\nWhen I try to use tf.nn.elu. \n\nIs it not written in the package yet?  Why it is included in the API document?\n", "comments": ["You may be looking at the documentation for master or 0.6.0 and running TensorFlow version 0.6.0 or 0.5.0, respectively.  What is `tf.__version__`?\n\nCc @martinwicke in case there's something we can do to make that happen less often.\n", "Default view on docs is at head, which is suboptimal. It should be 0.6, but\nin this case, that may not be the issue.\n\nOn Sat, Dec 12, 2015 at 3:30 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> You may be looking at the documentation for master or 0.6.0 and running\n> TensorFlow version 0.6.0 or 0.5.0, respectively. What is tf.**version**?\n> \n> Cc @martinwicke https://github.com/martinwicke in case there's\n> something we can do to make that happen less often.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/495#issuecomment-164201729\n> .\n", "@girving  even tf.__version__ report the same error. \nHow do I know the version of tensorflow on my computer?\n", "@martinwicke I checked, the document has two version \"master\" and \"0.6.0\",  both version has **tf.nn.elu**. Logically, **tf.nn.elu** should not be in one of them, right?\n", "If `tf.__version__` returns an error, your TensorFlow installation is old and didn't have support for elu() yet. Upgrading to 0.6.0 should solve your problem.\n"]}, {"number": 494, "title": "What is quantized data type? For instance qint8, quint8, qint32", "body": "In tensorflow document of class tf.DType: https://www.tensorflow.org/versions/master/api_docs/python/framework.html#DType\n\nThere are three types:\ntf.qint8: Quantized 8-bit signed integer.\ntf.quint8: Quantized 8-bit unsigned integer.\ntf.qint32: Quantized 32-bit signed integer.\n\nAnd also related method about quantized data type. I searched online, but did not find useful resource about it. \nCould you add some explanation in the document?\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/15\n"]}, {"number": 493, "title": "dual gpu memory allocation problem in 0.6", "body": "For the tensorflow 0.6, when I specifically assign the model on one gpu on a dual-gpu machine, it takes up the memory of both gpus.\n\nThis problem does not happen for tensorflow 0.5\n", "comments": ["Cc @zheng-xq, but I think we may need more information to help here.\n", "@zcyang, did you try to use CUDA_VISIBLE_DEVICES to limit the set of GPUs you want to use with TF? \n\nThe current TensorFlow works by going through all the GPUs visible to itself, check which one is compatible and assign a logical index to it. Note that this logical index might be different from your system device index. Then in your graph, you use the TensorFlow logic index to refer to each device. So all the devices will be occupied, regardless which one will be actually used later. \n\nIf you don't want a device to be used by TensorFlow, one solution is to use CUDA_VISIBLE_DEVICES and make it invisible. \n", "I believe I  fixed this sometime between 0.6 and 0.7 -- you only allocate memory for a GPU on first real use of it.\n"]}, {"number": 492, "title": "memory issues", "body": "Hi,\n\nIt seems the memory allocation of tensorflow is rather inefficient. I have been running a single layer rnn with 256 batch size, 124 length and dim of 512, it constantly gets memory not enough error for my 4GB 980. In theory, the model size is much less than 1GB\n\nno matter how large the batch size I set, it always use up all the 4GB memory, which is unreasonable. I have been compiling tensorflow from the source and BFC memory allocator is set as default.\n\nI think the memory problem was also mentioned here \nhttps://github.com/soumith/convnet-benchmarks/issues/66\nand mentioned by many other users. In compare with Theano and Torch, tensorflow can only experiment with smaller models.\n\nAre there any solutions to this? This is a major problem that stops me from experimenting with tensorflow.\n\nMany thanks!!\n", "comments": ["Hey man, I have had and still have the same issue and asked about it here:\n\nhttps://github.com/tensorflow/tensorflow/issues/352\n\nYou can set the `aggregation_method = 2` and that helped me some. But still you're right. Tensorflow sucks up proportionally way too much memory. It has been difficult to deal with. If they could fix this one aspect, it would be a real game changer. \n", "Stay tuned, improving memory usage and management is at the top of our list.\n", "Sounds great to hear that. I love Tensorflow and with improved memory usage it would be the best deep learning platform in my opinion.\n", "With hundreds of issues still open, this is too general of a request to be useful to keep open -- we're constantly going to be trying to improve performance and memory, of course.\n", "@vrv can you comment if 0.7.0 has improved memory allocation? I have hesitated to upgrade to 0.7.0 due to reported issues with Saver function. \n", "Hello,\n\nI am using the GPU compatible version of TensorFlow 0.8, my system has 4 GPUs with approximately 12GB memory for each. I am running a CNN in TensorFlow for grey scale images (one channel images) whose size is 1024*1024, the total number of training images is 1520 and the batch size is 7, also this CNN has 64 feature maps for each convolutional layer. According to a variety of web pages talking about TensorFlow's memory issue I am using the BFC memory allocator in the session's configuration of my CNN. Unfortunately after around 10 epochs I am getting the \"ran out of memory error\", following is the error log, which doesn't make sense to me considering the batch size, image dimensions, and memory size of my GPU. Any help is appreciated.\n\n Error log:\n\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (256):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (512):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (1024):     Total Chunks: 1, Chunks in use: 0 1.8KiB allocated for chunks. 256B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (2048):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (4096):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (8192):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (65536):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (131072):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (262144):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (524288):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (1048576):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (2097152):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (4194304):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (8388608):     Total Chunks: 1, Chunks in use: 0 8.00MiB allocated for chunks. 400.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (16777216):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (33554432):     Total Chunks: 1, Chunks in use: 0 36.53MiB allocated for chunks. 400.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (67108864):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (268435456):     Total Chunks: 2, Chunks in use: 0 2.21GiB allocated for chunks. 896.00MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:652] Bin for 1.75GiB was 256.00MiB, Chunk State:\nI tensorflow/core/common_runtime/bfc_allocator.cc:658]   Size: 508.67MiB | Requested Size: 448.00MiB | in_use: 0, prev:   Size: 448.00MiB | Requested Size: 448.00MiB | in_use: 1, next:   Size: 1.75GiB | Requested Size: 1.75GiB | in_use: 1\nI tensorflow/core/common_runtime/bfc_allocator.cc:658]   Size: 1.72GiB | Requested Size: 448.00MiB | in_use: 0, prev:   Size: 448.00MiB | Requested Size: 448.00MiB | in_use: 1, next:   Size: 6.2KiB | Requested Size: 6.2KiB | in_use: 1\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80200 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80300 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80400 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80500 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80600 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80700 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80800 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80900 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80a00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80b00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80c00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80d00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80e00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb80f00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81200 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81300 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81400 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81500 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81600 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81700 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb81f00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb82000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb82100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb82200 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb82300 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb82400 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x230fb82500 of size 6400\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x231200b200 of size 29360128\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x231440b200 of size 469762048\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x239e15e100 of size 6400\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x239e15fa00 of size 469762048\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x23ba15fa00 of size 469762048\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x23f5e0b200 of size 1879048192\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x2465e0b200 of size 1879048192\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x24d5e0b200 of size 3733947904\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x230fb81800 of size 1792\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x230fb83e00 of size 38302720\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x2313c0b200 of size 8388608\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x233040b200 of size 1842687744\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x23d615fa00 of size 533379072\nI tensorflow/core/common_runtime/bfc_allocator.cc:685]      Summary of in-use Chunks by size:\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 30 Chunks of size 256 totalling 7.5KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 6400 totalling 12.5KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 29360128 totalling 28.00MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 469762048 totalling 1.31GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 1879048192 totalling 3.50GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3733947904 totalling 3.48GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 8.32GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats:\nLimit:                 11353470976\nInUse:                  8930711040\nMaxInUse:              11319364608\nNumAllocs:                 1495009\nMaxAllocSize:           4804771072\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ****_________________**_*****_____**_***********************************************xxxxxxxxxxxxxxxx\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 1.75GiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[7,64,1024,1024]\nTraceback (most recent call last):\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 342, in <module>\n    tf.app.run()\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 338, in main\n    train()\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 256, in train\n    _, loss_value, mse_value, mymse_value, logits_value, labels_value = sess.run([train_op, loss, mse, mymse, logits, labels])\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[7,64,1024,1024]\n     [[Node: tower_3/gradients/tower_3/pool1_grad/MaxPoolGrad = MaxPoolGrad[data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"SAME\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:3\"](tower_3/conv1/conv1, tower_3/pool1, tower_3/gradients/tower_3/norm1_grad/LRNGrad/_2719)]]\n     [[Node: tower_3/gradients/tower_3/conv1/BiasAdd_grad/tuple/control_dependency_1/_2721 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:3\", send_device_incarnation=1, tensor_name=\"edge_3317_tower_3/gradients/tower_3/conv1/BiasAdd_grad/tuple/control_dependency_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'tower_3/gradients/tower_3/pool1_grad/MaxPoolGrad', defined at:\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 342, in <module>\n    tf.app.run()\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 338, in main\n    train()\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 182, in train\n    grads = opt.compute_gradients(loss)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 251, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\")\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 710, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'tower_3/pool1', defined at:\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 342, in <module>\n    tf.app.run()\n[elided 1 identical lines from previous traceback]\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 338, in main\n    train()\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 173, in train\n    loss, mse, mymse, logits, labels = tower_loss(scope)\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn_multi_gpu_train.py\", line 69, in tower_loss\n    logits = cnn.inference(images)\n  File \"/home/pmobade/project_latest/12july2016/grey/cnn.py\", line 184, in inference\n    padding='SAME', name='pool1')\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 341, in max_pool\n    name=name)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 677, in _max_pool\n    data_format=data_format, name=name)\n  File \"/home/pmobade/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n", "Hi,\nHow did you solve the problem? \nI have a similar problem on nvidid K80 with 11.25 GB of RAM. \n", "Would be amazing if there was some sort of way we could calculate how much memory each tensor is using, including backprop calculations.\n", "Hi,\r\nI use  TensorFlow v0.11.0 RC0.\r\nA simple Alexnet model while training with batch size 128  eats up to 4800 MiB (with config.gpu_options.allow_growth = True),  \r\nwhile the same model on caffe with bach size 128 takes roughly 2700MiB.\r\n\r\nAnd the speed is not fast (~2 times slower than caffe).\r\n", "I'm having similar issue here.\n\nVGG16 (which is around 500MB) + a batch size of 128 samples and I run out of memory. I have a Tesla K40m with 11GB of memory.\n\nHow can I deal with that?\n\nThanks for any answer\n", "Same problem. I can't load the VGG model with my GTX 970, I get 'Resource ExhaustedError: OOM' before the batching even starts.\n", "Similar problem here, keras VGG16 and running out of memory with Quadro K1000M (2GB). It ends execution using CPU instead, I believe. However, it works with a GeForce GTX 960 (4GB). Batch size = 32\n", "Similar problem here. I'm not training on a GPU, but my machine has 64gb RAM. I have to adjust the batch size, or else I get a `Segmentation fault (core dumped)` error on the optimizer (backprop) step. \r\n\r\nHowever, monitoring the RAM usage (e.g. htop), it never crawls above 4GB. So I am inclined to believe this is a bug with tensorflow overestimating the memory required. Backprop should scale linearly with the batch size, and I am seeing this until it faults at an arbitrary point.\r\n\r\nAny ideas?! This is severely limiting my ability to train large models. Using a small batch size is just not going to cut it! -- the estimation of the gradient direction is very poor for large models if the batch size is too small.", "I had a similar problem and solved it by releasing the GPU memory utilized by another process. Use the command nvidia-smi to check the GPU's Memory-Usage. If it is not 0MiB, kill the process that's allocating the GPU's memory even if that process is setting idle. Then run the script that trains your TF model.\r\n\r\nIn many cases, when a process (your TF training program for example) does not end normally, say due to an error, it holds on to the memory allocated for it until the program is manually killed ($kill -9 PID).", "@ashtawy  Absolutely, also I'd recommend using `watch nvidia-smi` to get a live view of GPU memory utilization", "I also have very similar issue, no solutions yet? Thank you for resolving this issue in advance though!", "Is anybody still working in this issue? I get this following error out of nowhere when I'm training a network on multiple GPUs akin to the CIFAR-10 tutorial. \r\n\r\n```\r\nE tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\nW tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes           \r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.                                  \r\nAborted (core dumped)\r\n```", "@kvrd18 I'm working on it (not affiliated with TensorFlow team). Finding memory-efficient execution order is an NP-hard problem, but there are usable heuristics.\r\n\r\nOne piece is a better node scheduling algorithm -- https://github.com/yaroslavvb/stuff/tree/master/linearize\r\n\r\nThis algorithm will add control dependencies to force a particular order on a graph. In particular, graph like below will always use constant memory to execute after running through `linearize`. (TensorFlow default algorithm uses memory proportional to length of the graph in worst case)\r\n\r\n <img src=https://cloud.githubusercontent.com/assets/23068/21414408/06033508-c7b4-11e6-939b-f0607d8e2a3a.png>", "Try to set a session config:\r\n\r\n```\r\nsession_config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)    \r\n# please do not use the totality of the GPU memory\r\nsession_config.gpu_options.per_process_gpu_memory_fraction = 0.90\r\n```", "@kvrd18  BTW, for tracking down out of memory errors, here are two tools to make it easier:\r\n\r\n1. https://github.com/yaroslavvb/memory_probe_ops -- it's a tensorflow op you can insert somewhere in the graph and evaluate in `sess.run` to get the amount of memory allocated at that point in time\r\n\r\n2. https://github.com/yaroslavvb/memory_util -- this tool lets you see timeline of all tensor allocations and deallocations.\r\n\r\nAfter looking at out of memory situations from various large networks (densenet, pixelnet), I found no \"low-hanging fruit\" aside from the \"linearize\" tool mentioned above. IE, TensorFlow only allocates memory for tensors that are required for computation, and this memory is released as soon as the tensor has been consumed.\r\n\r\nOut of memory situations tend to occur while computing gradients. If you have a computation with \"k\" operations in a sequence, each operation producing B bytes, then you need to save their outputs in order to compute backprop. This means k*B memory in peak. This is different from forward prop where older tensors can be discarded. So for instance, a network with 100 operations, with each processing 1GB of data only needs 2GB to run forward prop, but 100GB to do backprop.\r\n\r\nThere are some \"high-hanging fruit\" I'm looking on atm, in particular, such as discarding some parts of computation, and then recomputing it. One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example -- https://github.com/yaroslavvb/notebooks/blob/master/saving%20memory%20by%20using%20functions.ipynb\r\n", "@yaroslavvb, Thanks for great tips!\r\n> One way to do this in current tensorflow to wrap blocks into functions, this way intermediate values inside of each function block will be recomputed and hence don't need to be stored in memory, here's an example\r\n\r\nThats a bit counterintuitive. How is it controlled? Why does TF discard intermediate values inside the function (despite all intermediate ops will be added to the graph anyhow)?", "@asanakoy a `function.Defun` node is treated as a single tensorflow node for the purpose of backprop. IE, similar to what happens in a single op launch -- there may be intermediate temporary variables that could've been useful for the backprop kernel, but they get recomputed rather than reused. I haven't checked precisely how it's implemented with `function.Defun`, but I suspect that the backprop graph has a copy of `function.Defun` graph with same input as original `function.Defun`. ", "@yaroslavvb, I have a question about backprop, based on your comment above. For a given layer, don't we only need to keep the gradients on the outputs long enough to calculate the gradients on the inputs (and weights)?", "Correct, the gradients (backprop values) can be released quickly and don't affect peak memory much. It's the activations (forward prop values) that have to be kept in memory for a long time, here's a diagram of how things work -- https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-269241038", "@yaroslavvb, where is `function.Defun` defined? I cannot get how you imported it.", "That example is incomplete, but this information can be easily found on the internet (here's a complete example https://github.com/yaroslavvb/stuff/blob/master/node-merge.ipynb)", "@yaroslavvb @asanakoy my program using tensorflow , with the increase of iteration , the consume memory will increase. Can you help me to explain it?", "@zhangqianhui there are many ways for memory usage to grow without being a bug in TensorFlow (ie, your taining may be requesting increasing amounts of memory from TF). You could be allocating new variables, or modifying your graph between iterations. One possible (but unverified) possibility is that if you request many different sizes of tensors, your memory fragmentation could grow which increases memory usage", "@yaroslavvb Thanks", "@yaroslavvb The key , my mean is CPU memory . not GPU memory.", "I have found the reason , In my program , every iteration will call the function.\r\nthe code\r\n\r\n```\r\ndef sample_prior(self, batch_size):\r\n        ret = []\r\n        for dist_i in self.dists:\r\n            ret.append(tf.cast(dist_i.sample_prior(batch_size), tf.float32))\r\n        return tf.concat(1, ret)\r\n```\r\n\r\n@yaroslavvb  the function will consume my gpu mempry . Do you know why? And how to sovle it ?", "@yaroslavvb Does the current tensorflow version release the gradOutputs and the activations from the memory as soon as the gradInputs are computed? The peak memory will be achieved when the first gradInput is computed, if I'm correct.", "The memory is released as soon as soon as the tensor is not needed by downstream consumers. So it depends how your gradInput/gradOutputs are wired (ie, you can rewire them to be more memory efficient, like [here](https://github.com/yaroslavvb/notebooks/blob/master/simple_rewiring.ipynb)) . \r\n\r\nEven though TensorFlow releases memory right after it's needed, but there's nothing forcing TensorFlow to allocate memory right before it's needed. So TF could compute an op early and hold it's output in memory longer than necessary, which can increase peak memory. This greedy approach favors speed in multi-device setting over memory efficiency.\r\n\r\nIn a single device setting, a simple heuristic of executing an op as late as possible saves memory without affecting computation speed, I use this utility to force TensorFlow to compute ops as late as possible -- https://github.com/yaroslavvb/stuff/blob/master/linearize", "@yaroslavvb When executing on the GPU is there any way to interrogate the current memory pool allocated by tensorflow when using `gpu_options.allow_growth = True`. You must keep somewhere the full pool of the device memory manager, but how can we access the number (here I'm referring without logging it to some ifle).", "@botev you can use https://github.com/yaroslavvb/memory_util to see timeline of all allocations/deallocations (CPU/GPU), and you can use https://github.com/yaroslavvb/memory_probe_ops to query memory usage at point during session.run call (GPU only)", "@yaroslavvb I'm assuming that if I run the probe op in a session together with computation of a model this would return me the peek memory usage, is that correct?", "it would give you memory usage at the time when the op was executed. If you want peak memory usage, you can use MaxBytesInUse added in https://github.com/tensorflow/tensorflow/commit/ccf9a752\r\n\r\n```\r\ntensorflow.contrib.memory_stats.python.ops.memory_stats_ops\r\nmax_bytes_in_use = sess.run(memory_stats_ops.MaxBytesInUse())\r\n```", "I have been trying to find the largest models my box will run (neural_gpu) given that my box has 128gb ram and 3x 1080 gpu's, but tensorflow seems to run out of memory arbitrarily when it is only using about 71.5gb (minus a bit for the OS)... the dumps say that the total allocs = 64gb, so I wonder if it is self limiting to 64gb somewhere... when I built tensorflow and the box it only had 64gb... swap stays at zero... would rebuilding tensor flow help or is there a setting that could increase this?  ", "I also face memory thing when I train a transfer learning based model. I solve it by freezing some layers by pre-trained weights and fine tuning layer from layer. It works in this way.  But when I train the entire model, too many trainable parameters will still cause the OOM in AWS.", "@ericloveland there was some memory issues in older version, maybe trying the latest tf would help", " @yaroslavvb I'm running tf 1.0 from about a month ago... are you saying there have been relevant changes since 1.0 came out??  Thanks!", "@ericdnielsen in last 2 weeks there was a change that made all cwise ops run in place when possible, so that will reduce usage. But I suspect you have a deeper problem with your architecture requiring too much memory to evaluate in some cases\r\n\r\nIt's weird that you are getting dumps for running out of CPU memory. On my machine TensorFlow just uses as much CPU memory as is available, until the machine freezes.\r\n\r\nThe order of execution is random, so different runs may use different amounts of memory. You could use my [linearize](https://github.com/yaroslavvb/stuff/tree/master/linearize) util to fix a single memory efficient order, and debug your excessive memory usage from there. (ie, using http://github.com/yaroslavvb/memory_util)", "I face a similar memory issue while trying to train my tensorflow model. I use a Tesla K80 and the error I get is \"Ran out of memory trying to allocate 102.65MiB.\" Any general way to avoid this? ", "I think it was an architecture issue... I chose some different params (smaller nmaps, and more layers) and tensorflow happily ran even when it had to start using the swap file.  It appears that nmap size is limited by the memory size of the GPU, while the number of layers is limited by the amount of CPU memory... other things like (batch size, and sample size) being the same.  The seemingly sporadic running out of memory appears to be due to when the model runs up on a larger sample, it sometimes needs to create a larger RNN model/cell under the covers... I think. Thanks!", "you could try shrinking your batch size down to 1 and then shrinking other params down until memory issues stop...  so far it seems that models are limited by  GPU memory for some params and by CPU memory for other params.", "@yaroslavvb It seems that the `memory_stats_op.MaxBytesInUse()` is wrong. I'm running a single tensorflow instance and that being the only thing running on that GPU. Under `nvidia-smi` there are 8459MB memory used on the tensorflow, the op returns 4254MB. I have no idea what is going wrong but this is clearly wrong\r\n```\r\nmemory_stats_ops = tf.load_op_library(memory_stats_ops_loc)\r\nstats_op = memory_stats_ops.max_bytes_in_use()\r\nmemory = session.run((train, cost, stats_op), feed_dict=feed_dict)[2] // (1024 * 1024)\r\n```\r\nPS: I'm loading it as an external library, as we are using `r1.0` which do not have the op and we can not use non stable releases.", "Tensorflow uses its own caching allocator, so nvidia-smi gives incorrect\nresult\n\nOn Mar 11, 2017 5:35 AM, \"Alexander Botev\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> It seems that the\n> memory_stats_op.MaxBytesInUse() is wrong. I'm running a single tensorflow\n> instance and that being the only thing running on that GPU. Under\n> nvidia-smi there are 8459MB memory used on the tensorflow, the op returns\n> 4254MB. This seems criminally like the tensorflow op is returning exactly\n> half the memory actually being used. I'm using it like this:\n>\n> memory_stats_ops = tf.load_op_library(memory_stats_ops_loc)\n> stats_op = memory_stats_ops.max_bytes_in_use()\n> memory = session.run((train, cost, stats_op), feed_dict=feed_dict)[2] // (1024 * 1024)\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/492#issuecomment-285866983>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYoxVJzBid9_o5mfSczGZtDl8QXLjyfZks5rkqMYgaJpZM4G0MRi>\n> .\n>\n", "@yaroslavvb2 is that 100% sure. Because we observe on smaller Feed Forward models Tensorflow to be able to measure about a half memory usage than theano and torch, but on larger models it uses the same. \r\nAdditionally, we run everything with the flag `allow_growth`, so there is no reason why tensorflow should allocate x2 more memory than needed?", "@botev correct, even with `allow_growth`, the amount of available memory reported by nvidia-smi does not represent the amount of memory available", "My solution is somehow tricky. You can build your network with standard [**Keras**](https://github.com/fchollet/keras/) backend. \r\n**During development,** you can use Tensorflow as backend because compiling a model in Theano takes more time than you can endure, especially when the optimization option is on. You might suffer from small batch size and slow computation with Tensorflow, but what you have to do is just to make sure your network structure is correct.\r\n**After that**, your can simply [switch to Theano backend](https://keras.io/backend/) in the configuration file of Keras. Since you used the standard Keras backend when build the network, all your function calls of Tensorflow API will automatically switch to corresponding ones of Theano API. It might take a while to compile the model in Theano, but you might use a larger batch size and enjoy a fast computation.\r\n\r\n**Notice that** Theano and Tensorflow differ on the arrangement of data, such as the axis of filter channel. I use the Theano's way in both development and deployment in that it can boost Theano's computation.\r\n**And that** Theano does not have versatile tools such as TensorBoard in TensorFlow. And hence if you want to visualize or analyze the trained model, feel free and safe to switch back to TensorFlow. \r\n**And that** if some function is not implemented by Keras backend, you can write a simple condition statement to do that. Such as eigenvalue decomposition of a self-adjoint matrix:\r\n```\r\n  import keras.backend as K\r\n  # K.eig() does not exist\r\n  if K.backend()=='theano':\r\n    eig, eigv = K.theano.tensor.nlinalg.eig(matrix)\r\n  elif K.backend() == 'tensorflow':\r\n    eig, eigv = K.tf.self_adjoint_eig(matrix)\r\n  else:\r\n    raise NotImplemented\r\n```", "@kvrd18 I also ran into this issue today and I fixed it. The error I made is I try to access a array index out of the array I declare, that is, I try to access a gpu memory space that I haven't assign values. That's why the error message is like ` tried to allocate 0 bytes`. Hope this also help you.\r\n\r\n> E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\nW tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes           \r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.                                  \r\nAborted (core dumped)", "@jasonwu0731 Can you please help me to solve this issue? getting the same error. Where exactly did you fixed?\r\n\r\n", "I still have the memory problem. It uses so much ram that the OS kills the process.", "@modanesh Could you explain your problem with a little bit more explanation?", "TensorFlow uses up all the memory by default.\r\nyou can set the memory tensorflow uses by the flowing settings\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.visible_device_list = \"0\"  (the gpu device that can be used)\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5 (the percentage of memory used)\r\nsess = tf.Session(graph=tf.get_default_graph(), config=config)", "> Same problem. I can't load the VGG model with my GTX 970, I get 'Resource ExhaustedError: OOM' before the batching even starts.\r\n\r\ni'd suggest you check how many parameters your network is trying to learn, most likely you are above 100 million for lack of use of MaxPooling2D layers and high resolution in general.", "Try reducing the Batch size as at the start i tried dumping a lot of data onto the GPU memory which caused the OOM to throw up. It was solved when i tried reducing the number of BS to 5 or 4. So yeah it depends upon the model and the size of the data which is used to train the network.", "For those not satisfied with decreasing BS and number of neurons I can recommend looking into https://github.com/openai/gradient-checkpointing"]}, {"number": 491, "title": "Mac OS X GPU Support Request", "body": "Hi Team\n\nCurrently I've been doing some work with TensorFlow (some of my work on http://blog.otoro.net/) and have been developing everything a Macbook Pro running the IPython stack.  My Mac is fairly fast for regular sized tasks, and even has an NVIDA chip (NVIDIA GeForce GT 750M 2048 MB).  Although the current build doesn't support GPU's for Macs, it seems to me from reading earlier threads that it is not possible to custom build for GPU support on the Mac, despite having CUDA libraries installed.\n\nThis would really help out my work flow, especially I can get other frameworks to utilize my GPU / CUDA on the Mac.  The Macbook Pro is a great machine to develop use and develop stuff on, and lots of people I know also use MBP's to develop ML algorithms, and then send jobs off to AWS for the heavy-lifting after it works locally.\n\nI think having GPU support for Mac would really help in not just the performance front, but allows us to debug and check any GPU specific issues when running some script locally, and be able to fix them quickly, before sending them to a EC2 or some remote GPU server.  It would make the workflow a lot smoother for developers who use Macbook Pro's with the NVIDIA chip.\n\nThanks in advance!\n", "comments": ["Hi hardmaru!\n\nI'm in the same situation  as you and was able to build TensorFlow on my Mac with GPU support. I have an open pull request for the changes:\n\nhttps://github.com/tensorflow/tensorflow/pull/475\n", "Hey Ville!\n\nThanks- I missed that pull request when browsing the history of issues.  Hopefully they will merge your request!  If not I'll try to do it manually.\n\nCheers.\n", "Since the PR is closed (hopefully temporarily) I thought a little reminder that a lot of us want this, would be helpful.\n", "(it is just temporary, we may re-open it soon).\n", "This would be _very_ useful.\n", "Agreed all comments. \n\nTensorflow is a great tool but I see two bottlenecks for it to be widely used:\n1. Many clusters have the old CentOS on which TF doesn't work\n2. Many developers use MacBook Pro, and TF doesn't support GPU integration. \n\nI hope the team considers these issues. \n", "Let me just add to @ueser's comments, that many of us are on CUDA 7.5.  There are a lot of reasons for this, not the least of which is that if you're setting up a new machine, its a lot easier to get a working 7.5 installation with a current distro than a working 7.0 installation.  So part of what's needed is permitting compilation with 7.5.\n", "@elbamos I have a new open PR for OSX Cuda support that uses Cuda 7.5: https://github.com/tensorflow/tensorflow/pull/664\n\nThe PR makes it easier to test with different versions of cuDNN and Cuda by making the version of each framework a variable in the configure script. \n", "Would running it in docker make any difference as a workaround? The base is the same since tensorflow container inherits from the same linux distro as the gpu enabled one in the link below.\n\nhttp://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container\n", "@hurshprasad I think those solutions are meant for Linux Docker hosts running Linux Docker images.\n\nBut Docker doesn't run natively on OS X like it does on Linux. When you run Docker on OSX, it actually boots a tiny Linux VM inside either VirtualBox or VMWare Fusion. Then that Linux VM is then what actually runs Docker. It's Russian nesting dolls :)\n\nSo to share an OS X GPU with a running Docker image, there's actually two levels of indirection: VirtualBox has to allow the Linux VM to access the host GPU. Then the Linux VM has to allow the docker images running inside of it to access that GPU. \n\nI'm not an expert, but I'd guess that making that work would be way harder than just supporting OS X GPUs directly in Tensorflow.\n", "+1 for this feature.  With a thunderbolt e-gpu, you can do some solid work on a top spec macbook pro.\n", "+1 for this. I have a 2014 model with an NVIDIA GPU, it shouldn't be a super hard port. \n", "+1, for all the reasons mentioned above\n", "+1 - Had this partical MacBook spec'ed because of the NVIDIA GeForce GT 750M 2048 MB (MacBook Pro (Retina, 15-inch, Mid 2014))  Oddly enough it seems on the latest models Apple stopped supplying NVIDIA graphics cards.\n", "+1\nYes, the latest MacBook Pro / Mac Pro models do not offer NVIDIA as an option, but, apparently there's a MacBook Pro in the works for 2016 that will come with NVIDIA graphics. \n", "+1\n", "In order to keep the thread compact and easy to read, please consider just hitting \"subscribe\" instead of +1ing. (Also consider deleting previous posts that only have a +1 etc.)\n", "+1\n", "I use a linux machine for running TensorFlow, but I'm developing on MacBookPro (Early 2013) which has NVIDIA GeForce GT 650M. Though I have installed CUDA on my mac, TensorFlow wouldn't use GPU. Any support will help me quite a lot.\n", "+1\n", "+1\n", "+1\n", "+1\n", "+1\n", "+1\n", "It seems that there has been a pull request pending on this for several months: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/yF6g1pfesCM\nDoes anyone know the status of this?\n", "If you read through, Google felt there were issues with the PR, but this has really been in their hands for a long time. I gave in and installed Linux on my Mac Pro.\n\nIt turned out that running data science in Linux was 4x-6x faster than with OS X on the same hardware, for both gpu and non-gpu tasks. Because the Mac multithreading just isn't as efficient.\n\nSo, while I totally get the issue folks have, honestly getting tf to use laptop gpus on OS X I don't think is going to buy you what you want, and you're gonna be happier switching to Linux for your computations.\n\n> On Apr 8, 2016, at 3:20 PM, Ron Cohen notifications@github.com wrote:\n> \n> It seems that there has been a pull request pending on this for several months: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/yF6g1pfesCM\n> Does anyone know the status of this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n", "Honestly talking about \"data science\" as if it was just one thing that supposedly runs \"4x faster\" sounds like a very broad generalization, especially when the GPU is doing most of the work.\n", "@elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n", "Check out this thread...\n\nhttps://github.com/tensorflow/tensorflow/pull/664\n\nhttps://gist.github.com/ageitgey/819a51afa4613649bd18\n\nReportedly, it should work.  Unfortunately, I tried it and although I got really close to doing a complete Bazel PIP build, it failed when trying near the end when going through the cuDNN v5 library. I'm not sure if it would work for you, but you should give it a try.\n", "I don't have any links to provide or anything like that. It's just my personal experience. \n\nBut I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw. \n\nAn example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4. \n\nProcessing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\n\nI should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux. \n\nI was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X. \n\n> On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> \n> @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n", "Very interesting, indeed, but it doesn\u2019t proof that the problem is fundamental to OS X\u2019s architecture. We should try to isolate the problem and study it once we get a reliable way to compile TF under Mac. Maybe it was specific only to your particular configuration. I\u2019ll try the instructions above later.\n\n> On Apr 9, 2016, at 00:19, elbamos notifications@github.com wrote:\n> \n> I don't have any links to provide or anything like that. It's just my personal experience. \n> \n> But I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw. \n> \n> An example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4. \n> \n> Processing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\n> \n> I should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux. \n> \n> I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X. \n> \n> > On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> > \n> > @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly or view it on GitHub\n> > \n> > \u2014\n> > You are receiving this because you are subscribed to this thread.\n> > Reply to this email directly or view it on GitHub https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207609941\n", "You may be right. I will say, I observed this with torch, R, and tensorflow, all of them.\n\nMy point is, the folks who are very eager for Mac-tf-gpu support so they can run tf on the gpus in apple laptops, which aren't remotely cuda-optimized anyway, I think they're going to find that the book isn't worth the candle, and there are easier ways to accomplish what they want.\n\n> On Apr 8, 2016, at 5:27 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> \n> Very interesting, indeed, but it doesn\u2019t proof that the problem is fundamental to OS X\u2019s architecture. We should try to isolate the problem and study it once we get a reliable way to compile TF under Mac. Maybe it was specific only to your particular configuration. I\u2019ll try the instructions above later.\n> \n> > On Apr 9, 2016, at 00:19, elbamos notifications@github.com wrote:\n> > \n> > I don't have any links to provide or anything like that. It's just my personal experience. \n> > \n> > But I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw. \n> > \n> > An example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4. \n> > \n> > Processing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\n> > \n> > I should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux. \n> > \n> > I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X. \n> > \n> > > On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> > > \n> > > @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n> > > \n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly or view it on GitHub\n> > > \n> > > \u2014\n> > > You are receiving this because you are subscribed to this thread.\n> > > Reply to this email directly or view it on GitHub https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207609941\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n", "@elbamos: this is most likely not true. Can you please run the CUDA benchmarks on OSX and Linux for something simple like repeated GEMM calls (to ensure proper burn in) and report back?  CUDA provides examples that can natively be built on OSX and Linux. If you are seeing more cpu cores being used you are most likely using CPU BLAS and might be experiencing performance gains due to lack of host <-> gpu transfers. \n", "I'm positive that I wasn't using cpu blas. Because I tried forcing it to use cpu and that was an order of magnitude slower. I also fully believe that the performance on unit benchmarks is identical.  \n\nIf I have the time this weekend I may try to take some video of it.  And if there are particular benchmarks you'd like me to run, lay it out for me exactly what you'd like me to do and I'll give it a try.\n\n> On Apr 8, 2016, at 5:37 PM, Jason Ramapuram notifications@github.com wrote:\n> \n> @elbamos: this is most likely not true. Can you please run the CUDA benchmarks on OSX and Linux for something simple like repeated GEMM calls (to ensure proper burn in) and report back? CUDA provides examples that can natively be built on OSX and Linux. If you are seeing more cpu cores being used you are most likely using CPU BLAS and might be experiencing performance gains due to lack of host <-> gpu transfers.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n", "In reality, people using mac os x should be exploiting the tremendous development platform which Apple has created. Using the XCode IDE, Instrument Profiler, Swift Language and new Metal API, developers could easily program script level, super-optimized (Clang LLVM), super-fast CPU/GPU compute software.\n\nIf you are curious about the Metal API, you can watch some of the videos from WWDC (https://developer.apple.com/videos/play/wwdc2015/610/). Unfortunately, I think you might have to watch them on Safari, because for some reason the videos only stream on safari.\n\nIn fact, I highly recommend checking out this guys, Amund Tveit, https://github.com/atveit, work...\nhttps://github.com/DeepLearningKit/DeepLearningKit\nhttps://github.com/atveit/SwiftMetalGPUParallelProcessing\n", "Well, I wouldn't be surprised if the CUDA drivers (or the NVIDIA drivers for that matter) for Mac aren't all that optimized at this point in time, considering Apple is not offering any laptop or workstation with NVIDIA video cards.\nThis could change in the near future though.\nBut in any event, using the Mac as a development environment (including GPU support) should be supported. \n", "I really did not intend to send things in this direction \n\n> On Apr 8, 2016, at 6:17 PM, esd100 notifications@github.com wrote:\n> \n> In reality, people using mac os x should be exploiting the tremendous development platform which Apple has created. Using the XCode IDE, Instrument Profiler, Swift Language and new Metal API, developers could easily program script level, super-optimized (Clang LLVM), super-fast CPU/GPU compute software.\n> \n> If you are curious about the Metal API, you can watch some of the videos from WWDC (https://developer.apple.com/videos/play/wwdc2015/610/). Unfortunately, I think you might have to watch them on Safari, because for some reason the videos only stream on safari.\n> \n> In fact, I highly recommend checking out this guys, Amund Tveit, https://github.com/atveit, work...\n> https://github.com/DeepLearningKit/DeepLearningKit\n> https://github.com/atveit/SwiftMetalGPUParallelProcessing\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n", "From my understanding, there is not a very huge difference in GPU compute performance of an NVIDIA card accepting assembly language instructions compiled form OpenCL (now the Metal API) libraries and assembly language instructions compiled from CUDA libraries. \n\nMy conclusion is based on doing a google search for openCL vs. CUDA. For example, this post in 2010 from Accelereyes (now ArrayFire) shows subtle, but not huge differences in performance for various mathematical operations.\nhttp://blog.accelereyes.com/blog/2010/05/10/nvidia-fermi-cuda-and-opencl/\n", "With introduction of Thunderbolt 2 and 3 technology, external GPUs will likely be supported in the near future. Even now I was able to connect a GPU to my MacBook Pro externally via Thunderbolt 2 and Akitio Thunder 2. So even if Mac is not optimized enough right now, this is most likely to change in the nearest future.\n\n> On Apr 9, 2016, at 02:23, Steven notifications@github.com wrote:\n> \n> Well, I wouldn't be surprised if the CUDA drivers (or the NVIDIA drivers) for Mac aren't all that optimized at this point in time, considering Apple is not offering any laptop or workstation with NVIDIA as an option anymore.\n> This could change in the near future though.\n> But in any event, using the Mac as a development environment (including GPU support) should be supported.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207644451\n", "In #664 @ville-k added support and a few have verified it works -- we'll try our best to keep it running but we don't have a test machine so please continue to contribute to keep it working!   \n", "This press release from NVIDIA may be relevant to the performance issues mentioned.\r\n\r\nhttps://blogs.nvidia.com/blog/2015/08/31/mac-driver/", "+1 on this!\r\n", "+1 on this. This issue especially important in the light of spreading eGPU support, like on these videos:\r\nhttps://www.youtube.com/watch?v=jFHjT50M7_Y\r\nhttps://www.youtube.com/watch?v=vvAB3U5umug\r\nhttps://www.youtube.com/watch?v=JpR8_cS-168\r\nhttps://www.youtube.com/watch?v=LN7prQBjRNc\r\nhttps://www.youtube.com/watch?v=pNd5_kxpPc4", "+1", "+1", "For everyone interested in official eGPU support in macOS, I recommend creating a ticket in Apple's bug reporter: https://bugreport.apple.com/logon\r\nAnd mentioning my ticket via \"Also see rdar://29836015\"", "[Bender](https://github.com/xmartlabs/Bender) supports running TensorFlow models over the Metal API. It was tested on iPhone, not on Mac. But maybe with little changes it works for Mac!", "@bryant1410 No, it uses MetalPerformanceShaders and thus allows to execute *already trained* networks, not training with Metal.", "Oh, yeah. It only allows doing inference, not training."]}, {"number": 490, "title": "Error in TF.pad API docs", "body": "Please note the following error in tf.pad in \"array_ops.md\":\n\nExample 1 currently reads as follows:\n\n```\n# 't' is [[1, 1], [2, 2]]\n# 'paddings' is [[1, 1]], [2, 2]] ##This is not a valid tensor\n# rank of 't' is 2\npad(t, paddings) ==> \n     [[0, 0, 0, 0, 0] ##This is not the correct output\n      [0, 0, 0, 0, 0]\n      [0, 1, 1, 0, 0]\n     [[0, 2, 2, 0, 0]\n      [0, 0, 0, 0, 0]]\n```\n\nIt can be fixed as follows:\n\n```\n# 't' is [[1, 1], [2, 2]]\n# 'paddings' is [[1, 1], [2, 2]]\n# rank of 't' is 2\npad(t, paddings) ==> \n    [[0, 0, 0, 0, 0, 0]\n     [0, 0, 1, 1, 0, 0]\n     [0, 0, 2, 2, 0, 0]\n     [0, 0, 0, 0, 0, 0]]\n```\n", "comments": ["Thanks for letting us know. This is now fixed upstream, and should be released soon.\n"]}, {"number": 489, "title": "tensorflow hello world catch error", "body": "this is my installation info\n\n```\nsudo -H pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl in /Library/Python/2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /Library/Python/2.7/site-packages (from tensorflow==0.5.0)\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.9.2 in /Library/Python/2.7/site-packages (from tensorflow==0.5.0)\n```\n\nbut when I run the code in readme I got this error\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 8, in <module>\n    from google.protobuf import reflection as _reflection\n  File \"/Library/Python/2.7/site-packages/google/protobuf/reflection.py\", line 58, in <module>\n    from google.protobuf.internal import python_message as message_impl\n  File \"/Library/Python/2.7/site-packages/google/protobuf/internal/python_message.py\", line 59, in <module>\n    import six.moves.copyreg as copyreg\nImportError: No module named copyreg\n```\n\nsix i has installed.\nI'm a python fresher.\n", "comments": ["Did you try upgrading six with pip? Just run pip --upgrade six\n", "I find this on https://www.tensorflow.org/versions/master/get_started/os_setup.html#mac-os-x-importerror-no-module-named-copyreg\n", "```\nsudo easy_install -U six\n```\n"]}, {"number": 488, "title": "when i install i got this error", "body": "", "comments": []}, {"number": 487, "title": "TypeError: metaclass conflict in Python 2 and 3", "body": "TF 0.6.0 installed using PIP gives the following error on import in both Python 2 and 3:\n\n```\n>>> import tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/user/.local/lib/python3.4/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/user/.local/lib/python3.4/site-packages/tensorflow/python/__init__.py\", line 37, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/home/user/.local/lib/python3.4/site-packages/tensorflow/core/framework/graph_pb2.py\", line 10, in <module>\n    from google.protobuf import descriptor_pb2\n  File \"/home/user/.local/lib/python3.4/site-packages/google/protobuf/descriptor_pb2.py\", line 1533, in <module>\n    __module__ = 'google.protobuf.descriptor_pb2'\n  File \"/home/user/.local/lib/python3.4/site-packages/google/protobuf/reflection.py\", line 123, in __new__\n    new_class = superclass.__new__(cls, name, bases, dictionary)\nTypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\n```\n", "comments": ["This error seems to be in the protobuf library (and wasn't something we ran into when testing python 3).  Do you think you could file this bug over at https://github.com/google/protobuf ? I don't think we know enough about protobuf ... :(\n", "Ok, I have no idea what that was, but deleting and re-installing protobuf solved the problem. \n", "Pronobis can you tell how you deleted and re-installed protobuf?\n\nI uninstalled and re-installed protobuf with pip3, but got a new error out of that (\"AttributeError: 'module' object has no attribute 'MutableMapping').\n\nYour help would be greatly appreciated :-).\n", "sudo pip uninstall protobuf\nsudo pip install protobuf==3.0.0a3 (should be 3.0.0b2 if tensorflow's version is 0.7.0, see @hsaito comment below)\nsudo pip uninstall tensorflow\nsudo pip install tensorflow-xxx-xxx.whl\n\nWorks for me. @PeterARBork \n", "Thank you @Vesnica !\n", "+1 \ud83d\ude00\n", "+1\n", "Thanks, @Vesnica \nI encountered the same issue, and solved by following your instruction.\n", "thanks\n", "I'm encountering this problem as of 0.7.0 (was working fine on 0.6.0) which seems to be not resolving even after reinstallation of protobut as shown above. Anyone else seeing the same problem?\n", "^Hsaito, this didn't fix the problem for me either\n", "After some investigation, I think I see what's going on. You have to uninstall protobuf (and you may have to run this several types just to make sure it's completely gone.) and then install protobuf==3.0.0b2 instead of protobuf==3.0.0a3, and looks like it's working.\n", "+1 I just uninstalled protobuf and reinstalled with protobuf==3.0.0b2 and it's working now\n", "+1 I had the same issue. \nI just had to reinstall protobuf : pip uninstall protobuf then pip install protobuf==3.0.0b2 and it would be able to import TensorFlow;) \n", "Would there be any reason that this procedure would be different for python 2.7 instead of python3? I've gone through all the steps above, but am still getting this error:\n\n```\nPython 2.7.10 (default, Oct 23 2015, 18:05:06) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 10, in <module>\n    from google.protobuf import descriptor_pb2\n  File \"/Library/Python/2.7/site-packages/google/protobuf/descriptor_pb2.py\", line 1533, in <module>\n    __module__ = 'google.protobuf.descriptor_pb2'\n  File \"/Library/Python/2.7/site-packages/google/protobuf/reflection.py\", line 123, in __new__\n    new_class = superclass.__new__(cls, name, bases, dictionary)\nTypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\n```\n\nAny ideas?  Thanks for any pointers.\n", "@theludachris As long as you are doing the procedure with correct version of pip (and pip3 for Pythob3) it should work. Is python symlinked to the same version of python as shown on the very first line of pip script?\n", "+1 @hsaito 's solution works for me too. Using Ubuntu 14.04.4 and Python 2.7\n", "Thanks for the tips. I think I figured it out - separately, I was getting\nsome earlier errors in the install process ('operation not permitted'), and\ndiscovered that OSX El Capitan last month added something called SIP which\nwas partially screwing up my installation.  I went ahead and tried the\nvirtualenv installation and everything is working.\n\nOn Mon, Feb 22, 2016 at 1:21 AM Sean Saito notifications@github.com wrote:\n\n> +1 @hsaito https://github.com/hsaito 's solution works for me too.\n> Using Ubuntu 14.04.4\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/487#issuecomment-187086353\n> .\n", "+1\ngreat thanks !\n"]}]