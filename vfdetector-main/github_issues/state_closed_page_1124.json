[{"number": 19512, "title": "Fix pylint issue in the master branch", "body": "\r\nThe latest master branch has the following pylint failure that caused:\r\n\r\n`Ubuntu Sanity \u2014 Internal CI build failed`:\r\n```\r\n53 FAIL: Found 2 non-whitelited pylint errors:\r\n54 tensorflow/contrib/cmake/tools/create_def_file.py:47: [C0301(line-too-long), ] Line too long (106/80)\r\n55\r\n56 tensorflow/contrib/cmake/tools/create_def_file.py:61: [C0301(line-too-long), ] Line too long (90/80)\r\n```\r\n\r\nThis PR addresses the above issues.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc @gunan @case540 please take a look.", "thank you!"]}, {"number": 19511, "title": "Error in tensorflow in python2.7 ubuntu", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n(tens) sagar@sagar-HP-Pavilion-Notebook:~/Desktop/neural-vqa-tensorflow/tens$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n/home/sagar/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/sagar/Desktop/neural-vqa-tensorflow/tens/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/sagar/Desktop/neural-vqa-tensorflow/tens/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\r\n    from tensorflow.python.estimator import estimator_lib as estimator\r\n  File \"/home/sagar/Desktop/neural-vqa-tensorflow/tens/lib/python2.7/site-packages/tensorflow/python/estimator/estimator_lib.py\", line 41, in <module>\r\n    from tensorflow.python.estimator.inputs import inputs\r\n  File \"/home/sagar/Desktop/neural-vqa-tensorflow/tens/lib/python2.7/site-packages/tensorflow/python/estimator/inputs/inputs.py\", line 22, in <module>\r\n    from tensorflow.python.estimator.inputs.numpy_io import numpy_input_fn\r\n  File \"/home/sagar/Desktop/neural-vqa-tensorflow/tens/lib/python2.7/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\", line 26, in <module>\r\n    from tensorflow.python.estimator.inputs.queues import feeding_functions\r\n  File \"/home/sagar/Desktop/neural-vqa-tensorflow/tens/lib/python2.7/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\r\n    import pandas as pd\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/pandas/__init__.py\", line 23, in <module>\r\n    from pandas.compat.numpy import *\r\n  File \"/home/sagar/miniconda2/lib/python2.7/site-packages/pandas/compat/__init__.py\", line 420, in <module>\r\n    if LooseVersion(dateutil.__version__) < LooseVersion('2.5'):\r\nAttributeError: 'module' object has no attribute '__version__'\r\n>>> \r\n\r\n", "comments": ["Looks like you are running into this:\r\nhttps://github.com/spyder-ide/spyder/issues/3616", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19510, "title": "The name 'softmax:0' refers to a tensor which does not exist.", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary (pip)\r\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0\r\nPython version: 3.5.2\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\nI have this error: \r\n`\r\nKeyError: \"The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph.\"\r\n`\r\n\r\nHere's a part of my code:\r\n```Python\r\nglobal num_top_predictions\r\n  if not tf.gfile.Exists(image):\r\n    tf.logging.fatal('File does not exist %s', image)\r\n  image_data = tf.gfile.FastGFile(image, 'rb').read()\r\n  with tf.Session() as sess:\r\n    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\r\n    predictions = sess.run(softmax_tensor,\r\n                           {'DecodeJpeg/contents:0': image_data})\r\n    predictions = np.squeeze(predictions)\r\n    node_lookup = NodeLookup()\r\n    top_k = predictions.argsort()[-num_top_predictions:][::-1]\r\n    results = []\r\n    for node_id in top_k:\r\n      human_string = node_lookup.id_to_string(node_id)\r\n      score = predictions[node_id]\r\n      results.append((human_string, score))\r\n    return results\r\n```\r\nI've made some research and found out that this can be a tensorflow version problem. But I installed a latest version using pip.\r\nThank you for your help", "comments": ["If you have pbtxt file, just verify the node name. There must be some variable scope attached to your node.", "Nagging Assignee @bignamehyp: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since we don't help you debug your custom code here. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The problem was in tensorflow versions", "Could you clarify what the difference was between versions?   \r\nI have a similar problem working from an example,  but I can't seem to figure out what the version differentiation is?\r\n\r\nDo we need to use a different assignment variable or do we actually use a different API function?\r\n\r\nWhen calling the function \r\n`softmax_tensor = tf.get_default_graph().get_tensor_by_name('softmax:0')`\r\n\r\nit responds with \r\n\r\n> KeyError: \"The name 'softmax:0' refers to a Tensor which does not exist. The operation, 'softmax', does not exist in the graph.\"\r\n\r\nI have checked the documentation at https://www.tensorflow.org/api_docs/python/tf/Graph\r\nand the it still implies that mechanism for calling the said function is the same.  I just want know if there is change in passing in the argument?  \r\n\r\nThe documentation for the get_tensor_by_name does not appear to mention or have any signifiicant breaking changes but I could be wrong \r\nhttps://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name", "@mashkovtsevlx @garywoodfine am seeing something similar, did either of you get the bottom of this?", "I am getting a similar issue. Did you find the fix?"]}, {"number": 19509, "title": "Update docstring for tf.reduce_sum/reduce.", "body": "This fix tries to address the issue raised in #19498 where the description for reduction:\r\n```\r\nIf axis has no entries, all dimensions are reduced, and a tensor with a single element is returned.\r\n```\r\n\r\ndoes not match the current behavior. The current behavior (matches `np.sum` as well) is actually:\r\n```\r\nIf axis is None, all dimensions are reduced, and a tensor with a single element is returned.\r\n```\r\n\r\nThis fix fixes the discrepancy for tf.reduce_sum/mean/max/min/prod.\r\n\r\nThis fix fixes #19498.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the doc update!"]}, {"number": 19508, "title": "tf.keras: Fix plot_model for Sequential model", "body": "Description:\r\n\r\nSolves issue https://github.com/tensorflow/tensorflow/issues/18908.\r\n\r\nThe Keras implementation of the Sequential model has a deprecated ``model.model`` property. The TensorFlow implementation of the Keras API does not have the deprecated ``model.model`` property.\r\n\r\nThis leads to the error: ``AttributeError: 'Sequential' object has no attribute 'model'``.\r\n\r\nThis PR mirrors the PR in the Keras repository: https://github.com/keras-team/keras/pull/10256.\r\n\r\n", "comments": []}, {"number": 19507, "title": "Eager metrics not saving as summaries", "body": "I am writing a piece of tf.eager code on the Iris dataset and I'm facing issues when trying to save `Mean` and `Accuracy` as summaries. Here is the piece of code I am using:\r\n```\r\ndef evaluate(model, iterator, logdir=None):\r\n    avg_loss = tfe.metrics.Mean('loss')\r\n    accuracy = tfe.metrics.Accuracy('accuracy')\r\n\r\n    for inputs, labels in iterator:\r\n        avg_loss(sce(model, inputs, labels))\r\n        accuracy(tf.argmax(model(inputs), axis=1, output_type=tf.int64),\r\n                 tf.argmax(labels, axis=1))\r\n    print(f\"Dev set: Average loss: {avg_loss.result()},\\\r\n    Accuracy: {100 * accuracy.result()}\\n\")\r\n    with tf.contrib.summary.always_record_summaries():\r\n        tf.contrib.summary.scalar('loss', avg_loss.result())\r\n        tf.contrib.summary.scalar('accuracy', accuracy.result())\r\n```\r\n\r\nThis is almost a copy from the MNIST Eager tutorial (line 82).\r\nThe print statement is working great but when it gets into the `with` block I get:\r\n```\r\nFile \"./iris.py\", line 131, in train_iris_model\r\n    logdir=logdir, summary_freq=summary_freq)\r\n  File \"./iris.py\", line 98, in fit\r\n    evaluate(model, dev_iterator, logdir=logdir)\r\n  File \"./iris.py\", line 61, in evaluate\r\n    Accuracy: {100 * accuracy.result()}\\n\")\r\n  File \"/Users/thms/.virtualenvs/unbabel3/lib/python3.6/site-packages/tensorflow/contrib/eager/python/metrics_impl.py\", line 341, in result\r\n    t = self.numer / self.denom\r\nAttributeError: 'Mean' object has no attribute 'numer'\r\n```\r\n\r\nIf I understand correctly it means that is is trying to call `.result()` on an unitialized `Mean` object but I don't get why. Is this normal behaviour?\r\n\r\nFull code can be found [here](https://github.com/Threynaud/tf-deep-learning/blob/master/tensorflow/tf-eager/iris/iris.py) and run with `./iris.py`\r\n\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6.5", "comments": ["@Threynaud did you find a solution for this? I'm experiencing the same error.", "I am experiencing the same error too. Was it closed with no solution?", "Hi! I completely forgot to share the solution, very sorry for that. It seems like you need to use `tf.contrib.metrics` instead of `tf.metrics` which doesn't support eager execution correctly for some reason... It worked for me at least, tell me if it solves your problem as well!", "No, it did not help. I got the following error:\r\n\r\nAttributeError: module 'tensorflow.contrib.metrics' has no attribute 'Mean'", "> Hi! I completely forgot to share the solution, very sorry for that. It seems like you need to use `tf.contrib.metrics` instead of `tf.metrics` which doesn't support eager execution correctly for some reason... It worked for me at least, tell me if it solves your problem as well!\r\n\r\nI had the same problem.\r\nSOLUTION:\r\nCheck the iterator in for loop,\r\nit may not be initialized and no value would be appended to metrics,\r\ncausing error.  "]}, {"number": 19506, "title": "Numa device prototype", "body": "This pull request is to get the comments for the prototype to support NUMA aware CPU device in TensorFlow.\r\n \r\nFrom the performance profiling, we found TensorFlow does not scale to multi sockets, the inter-core memory traffic is high.\r\n\r\nThis pull request has a prototype to support NUMA aware CPU device. A new device, called NumaDevice, is a collection of CPU cores that belongs to the same NUMA node.  The graph running on the NumaDevice will only use the memory close to that Numa node.  Our experiment showed this prototype dramatically reduced the inter-core memory traffic, and improved the training and inference performance.  \r\n\r\nSince this is only a prototype, there are some hacked code to just make it run. \r\n\r\nTo make it compile, copy /usr/lib64/libnuma.so to bazel cache directory/external/mkl_linux/lib, and copy NonBlockingThreadPool.h to bazel cache directory/external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool. \r\n\r\nThe TensorFlow benchmark is modified a little bit to make it run. The branch numa-device-prototype under https://github.com/Intel-tensorflow/benchmarks.git has the modified script. There is an example in scripts/tf_cnn_benchmarks/run.sh to run ResNet50 with two NUMA devices. \r\n\r\nFeel free to contact me @ sheng.fu@intel.com if you have any questions. Looking forward to your comments. \r\n", "comments": ["+ jart@ to get her opinions on this. ", "It seems a friend from Intel shared an experimental performance enhancement.\r\n\r\nI've read other teams at Google got 10-20% gains by modifying scheduling to be NUMA-aware. What were your benchmark numbers here? (I ask because your App Engine demo appears down.)\r\n\r\nI see you added `pthread_setaffinity` to the Eigen thread pools. Here's your diff: https://gist.github.com/jart/84b96f56b92f21eb9ed2ffc4a372dedf @rmlarsen might be able to help you get those changes upstreamed. We can't vendor sources. We'll also want portability before merging.\r\n\r\nI can't say for certain if the TensorFlow team would consider adopting OpenMP language extensions. GCC and Clang don't support these out of the box.\r\n\r\nLastly, would it be possible for Intel to distribute MKL-DNN to include `.a` files built with `-ffunction-sections` and `-fdata-sections`? I'm not sure if we can copy 180MB of .so files into our pip wheels. These libraries are also still BSD-3, correct? I heard Intel announced a new license last month.\r\n\r\ncc: @rajatmonga @m3bm3b ", "cc: @saeta @zheng-xq since they have been thinking about NUMA", "CC @mrry ", "See https://github.com/tensorflow/tensorflow/pull/10629#issuecomment-312032067 for valuable suggestion from @poxvoculi. Maybe your execution environment was different, while most of us use 2-socket CPU platform.\r\n\r\nLooking forward to a (hopefully) more interesting discussion on NUMA specific designs.", "@byronyi, this prototype is for the case that TensorFlow runs on CPU only with MKLDNN as the backend.  #10629 (comment) tried to address NUMA issue between the host and the GPU. I think they are different issues.", "Sounds like maybe this is related to #19136 and #19391.  It would not be surprising that binding threads to sockets and using socket-local memory yields performance gains.  One reason we haven't done this so far has been lack of a reliable mechanism for doing the thread-socket binding.   Shall we follow up with discussion of support for hwloc on #19391 ?", "@poxvoculi, this prototype is not the same as #19391. It also requires the change in how the training and the inference run. It splits the data in one batch evenly on the available NUMA nodes, computes the loss and gradients on each node (no communication between NUMA nodes), and then averages the gradients from each node and apply the update (need to communicate between NUMA nodes).  ", "This PR could be related to or could be the starting point for distributed \"mirrored\" strategy, but for CPUs. So far, it is 2 CPUs on one machine, similar to 8 GPU cards on one machine? ", "@shengfuintel Above you say this dramatically reduces inter-core memory traffic.  Did you test any benchmark model, e.g. resnet50?  By your measurements what ratio of cpu reads/writes are cross socket with/without this change?   ", "@poxvoculi, In my NUMA device prototype, with Resnet50, the average UPI (Intel UltraPath Interconnect ) memory bandwidth dropped to 7G from 35G. So it is about 5x lower. The data is in my slides.", "Nagging Assignee @case540: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A NUMA aware CPU device change is due to be pushed by Google, so this PR should not be merged. "]}, {"number": 19505, "title": "Tensorflow Lite Python Interpreter Crashes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: b'v1.8.0-2191-gc36266e' 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.13\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\nmkvirtualenv tf -p /usr/bin/python3\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout 4dbaa65\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\ncd ~\r\npip install /tmp/tensorflow_pkg/tensorflow-1.8.0-cp35-cp35m-linux_x86_64.whl\r\nwget http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz\r\ntar -zxf mobilenet_v1_1.0_224.tgz\r\npython\r\n> import tensorflow as tf\r\n> a = tf.contrib.lite.Interpreter('mobilenet_v1_1.0_224.tflite')\r\n> a.allocate_tensors()\r\n> a.get_input_details()\r\nSegmentation fault (core dumped)\r\n\r\n### Describe the problem\r\nSegmentation fault when running the tflite python interpreter.  The Interpreter initializes and allocate_tensors() runs, but other commands result in segfault.\r\n\r\nIs the python tflite interface only intended for ARM systems, or should it run on x64 as well?  (I'm running on x64).\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["I'm not sure of a proper fix but adding a call to `tensorflow::ImportNumpy();` in this function https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/interpreter_wrapper/interpreter_wrapper.cc#L111 seems to work.", "The fix noted by @parvizp is correct. It has been fixed internally and should be fixed externally next week.", "Great, fixed it for me!\r\nThanks!"]}, {"number": 19504, "title": "Tensorflow build on Bazel takes longer than 24Hrs", "body": "Hello Guys, I got a question.. I want to get Tensorflow on my Rasberypi 3b+ and found here the Tutorial to build Tensorflow on Bazel.. After several attempts i got it.. to the point wherenitiate TensorFlow  the build with: bazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package\r\n\r\n![img_6831](https://user-images.githubusercontent.com/39053923/40437580-feae0630-5eb5-11e8-86db-e0bed0d5f56b.JPG)\r\n\r\nnow it stuck since 20 Hours maybe here .. \r\nit is normal? or im just should still waiting", "comments": ["its not really stuck the seconds they go up", "My assumption is you don't have enough memory to run the build. Do \"htop\" and see if you have enough memory/free threads to run the build.", "Nagging Assignee @cy89: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@alex2425 does @achalshah20 's suggestion shed any light?", "Nagging Assignee @cy89: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@cy89  and @achalshah20 So guys, sorry for the late response... but i killed the project. Its not working.. finally i found a soloution which is called tensorflow for poets. It works quite well but not fast enough. Its done with native pip. But i killed it to because of the \"slow\" perfomance, but waaay better than you use the normal TF. Now i just use the rasberry pi as a \"connector\" and my tf runs on a vm server. Its  fast and stable and i can train my own models. I get a connection with a rest api (flask) and can send some picture to it from a smartglas or a mobilephone, notebook and so on. But thanks for help guys. The rasberry pi is not really good for it because the perfomance is really really bad.. maybe i am done somthing wrong but now it work with my solution :P", "@alex2425 that makes sense. Raspberry Pis aren't known for their processing power. I'll close; reopen if you'd like to revive."]}, {"number": 19503, "title": "TOCO Quantized InceptionV3 Error: \"tensorflow/contrib/lite/kernels/pooling.cc:116 input->params.scale != output->params.scale\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Minor custom code for quantization but using provided InceptionV3 checkpoints and models/research/slim scripts.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.8.0-2169-gb84878e63e', '1.8.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 7.2.0\r\n- **CUDA/cuDNN version**: 9.1/7.1.3\r\n- **GPU model and memory**: NVIDIA P40 (24 GB)\r\n- **Exact command to reproduce**: I link to my script on GitHub below.\r\n\r\n### Describe the problem\r\nI've been trying to produce a quantized InceptionV3 model since one is not provided (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md) but it currently fails in the `label_image` demo application due to a MaxPool that has different input/output quantization ranges.\r\n\r\nIn this TensorFlow Lite announcement video, quantized InceptionV3 is shown to be ~3x faster than floating point (https://youtu.be/FAMfy7izB6A?t=8m40s), so I thought it might work \"out of the box\".\r\nAdditionally, in this Google quantization paper the accuracy is shown to be fairly high (https://arxiv.org/abs/1712.05877), which is pretty motivating. \r\n\r\n### Source code / logs\r\nI have a branch of `tensorflow/models` with very minor changes to support quantization: https://github.com/parvizp/models/tree/quantize\r\nYou can run the whole script to see the training, TOCO call and call to `label_image`: https://github.com/parvizp/models/blob/279e458ac99da67e405ac74bc5e4583d5111c1bb/research/slim/scripts/quantize_inception_v3_on_imagenet.sh\r\n```\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/examples/label_image/label_image '--tflite_model=/tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite' '--image=/tmp/tensorflow/tensorflow/contrib/lite/examples/label_image/testdata/grace_hopper.bmp' '--labels=/tmp/imagenet/labels.txt'\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model /tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite\r\nresolved reporter\r\ntensorflow/contrib/lite/kernels/pooling.cc:116 input->params.scale != output->params.scale (-1454358704 != 734155808)\r\nFailed to allocate tensors!\r\n```\r\nThe error seems to stem from this MaxPool in the center of:\r\n<img width=\"850\" alt=\"inception_v3 quantized maxpool-issue\" src=\"https://user-images.githubusercontent.com/926261/40436998-d7f6ff38-5e79-11e8-9b1e-6e65f2f20ec9.png\">\r\nTo reach the accuracy reported in (https://arxiv.org/abs/1712.05877) should we:\r\n- Add support for TF-Lite MaxPool kernel so it can perform requantization as needed (i.e. for this case)?\r\n- Make TOCO nudge the ranges so all things forking from the cancat and joining have the same range? Could also make the graph re-writer emulate this with shared ranges? \r\n\r\nSeems to preserve more precision and easier to do the first option. I'm happy to provide a patch if you can provide some insight into which direction to pursue.", "comments": ["@suharshs Should the current version of TOCO/TF-Lite be able to quantize vanilla InceptionV3?", "Hi @parvizp ,\r\n\r\nThis was a recent issue we found and fixed. The issue has to do with TOCO trying to guarantee that the input and output scales of Concatentation operations match, which stomps on other guarantees TOCO tries to make. This should be resolved by providing `--change_concat_input_ranges=false` to your commandline TOCO invocation.\r\n\r\nWe plan to make this default to false soon, but were worried about breaking users that may rely on that feature.\r\n\r\nThanks!\r\n-Suharsh", "Thanks @suharshs !"]}, {"number": 19502, "title": "Fix ovic typo", "body": "1. pre-requesits -> pre-requisite\r\n2. --cxxopt--std=c++11 -> --cxxopt=--std=c++11", "comments": ["Thanks!"]}, {"number": 19501, "title": "Tensorflow Estimator API doesn't work in distributed mode", "body": "here is my test code \r\n\r\n```\r\nfrom tensorflow.python.keras.layers import Conv1D, MaxPooling1D\r\nfrom tensorflow.python.keras.models import Model\r\nimport logging\r\nlevel = logging.getLevelName('INFO')\r\nlogging.getLogger().setLevel(level)\r\nmodel = tf.keras.Sequential()\r\noutput = Dense(2, activation=\"softmax\")\r\nmodel.add(Dense(64, activation=\"relu\", input_shape=(10,)))\r\nmodel.add(output)\r\nmodel.compile('rmsprop', 'categorical_crossentropy')\r\nest_model = tf.keras.estimator.model_to_estimator(keras_model=model)\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"dense_2_input\": np.random.randint(10, size=(320, 10))},\r\n        y=np.random.rand(320, 2),\r\n        num_epochs=10000,\r\n        shuffle=False)\r\nest_model.train(train_input_fn)\r\n```\r\n\r\nMy TF_CONFIG is like is\r\n```\r\nTF_CONFIG={\r\n\"cluster\": {\"chief\": [\"localhost:2223\"], \r\n\"worker\": [\"localhost:2221\"], \r\n\"ps\": [\"lcoalhost:2222\"]}, \r\n\"task\": {\"index\": \"0\", \"type\": \"chief\"}\r\n}\r\n```\r\n\r\nThe chief is stuck on logging \r\n`INFO:tensorflow:Restoring parameters from /tmp/tmpe_c82nqn/keras_model.ckpt` \r\nand no ports is listening.\r\nMaybe I can't simulate cluster in local? \r\n\r\nWhen I using strace to found out why? I saw a lots \r\n`[pid 16719] futex(0x7f4c39cab604, FUTEX_WAIT_BITSET_PRIVATE, 850, {4401792, 658466075}, ffffffff) = -1 ETIMEDOUT (Connection timed out)`\r\n\r\nWithout TF_CONFIG set, the code is working fine.\r\n\r\nAny suggestion? ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The checkpoints need to be on a distributed filesystem, like NFS. Are you doing that?", "Thanks for the reply.\r\nNo, I'm not using any distributed filesystem, but I'm trying to simulate distributed training on a single node. while starting training, the checkpoints are created on my local filesystem, I'm confused, Is a distributed filesystem still needed?", "Yes, I think so. We do have `tf.contrib.distribute`, that will move to core once it's stable. Could you take a look at that?", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "If you want to run your model across multiple machines you need to use Estimator's train_and_evaluate API.  train_and_evaluate uses a between graph async parameter server approach.\r\n\r\nIf you want to train your model across multiple GPUs you can use the new [DistributionStrategy APIs](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/distribute/DistributionStrategy). Use model_to_estimator to convert your keras model to an estimator instance and use train as you have above. ", "Thanks, I'll look into it later.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@anj-s  clarfications needed. Please help.\r\nIf I created a model using estimator then.\r\n\r\n1. If i use distribution startegy as CollectiveAllReduceStrategy or ParameterServerStrategy through RunConfig, then and estimator can be trained distributedly using estimator.train(), there is no need to use train_and_evaluate().\r\n2. When specifying model_dir in RunConfig using parameter-server appraoch through estimator, the path to directory can be local machine path(chief node), it need not to be a shared file-system path.\r\n3. When using ParameterServerStrategy, how to use synchronous and stale-synchronus approach, are SyncReplicasOptimizer and DropStaleGradientOptimizer compatible with Distribution strategy ParameterServerStrategy API."]}, {"number": 19500, "title": "i took the mine and rock problem based ANN program from edureka and edited it to suit my need which is tripping the transformer for fault current. for training purpose i used a csv input in which i passed 6 featured input values, while passing one desirable output. Now the problem is i want the trained model to work for live values for which i don't have desired output ... i only want to predict the output with the help of trained model ... but it keeps giving an error at the Y input ... How can i make the program run without passing a desired output?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 19499, "title": "tf.data.Dataset iterators are not cleaned when the loop ends with a break", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:CentOS7\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ntf.data.Dataset iterators are not cleaned when the loop ends with a `break`.\r\n\r\nThe code below opens one file per epoch. This eventually hits a system limit (maximum number of open files).\r\n\r\nReplacing the `break` by a `continue` works better since the files are closed. However, this is inefficient if we only need to iterate over a small fraction of the data\r\n\r\n### Source code / logs\r\n```\r\ndataset = tf.data.TextLineDataset(fp)\r\n...\r\nfor epoch in xrange(epochs):\r\n    ...\r\n    batches = 0\r\n    for (x, y) in dataset:\r\n        batches += 1\r\n        if batches > MAX_BATCHES:\r\n            break\r\n        ...\r\n```", "comments": ["@allenlavoie It looks like you've done something related to Eager resource garbage collection in 309e340619ab922f1ecb51b8f142283e09bda07d, and this is still used in the current `iterator_ops.EagerIterator`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2c9d129cb8f6e3f6a27d074664033e4fccd83a63/tensorflow/python/data/ops/iterator_ops.py#L480-L482\r\n\r\nCan you please take a look and see if there's a reference leak here? ", "There is a reference leak, but I don't think it's in Python. DestroyResourceOp gets run and Unrefs the resource, but it looks like IteratorHandleOp is [keeping a reference to the resource in its OpKernel](https://github.com/tensorflow/tensorflow/blob/f8b74d642420dcf2f5cab763b41884a05777ea45/tensorflow/core/kernels/data/iterator_ops.cc#L510). AFAIK kernels are never deleted when executing eagerly, they just sit around in the kernel cache.\r\n\r\nI've verified that removing the reference from IteratorHandleOp fixes the \"files not closed\" issue (they get closed when DestroyResourceOp runs). I can think of horrible hacks to get this to happen only when executing eagerly, but maybe we should discuss tomorrow.", "Ugh, yes, whatever we do to that kernel implementation, the current version will still leak the `OpKernel` object and related guff for each iterator. As a strawman, we could solve it with (i) a new version of `IteratorHandleOp` that creates the handle doesn't retain a resource, and (ii) some API for creating-and-running-but-not-caching a kernel in eager mode. (CCing @asimshankar since kernel lifetimes in eager is something we've talked about in the past.)\r\n\r\n@LionelCons In the meantime, here's a workaround that should alleviate the file handle leak:\r\n\r\n```python\r\ndataset = tf.data.TextLineDataset(fp)\r\n# ...\r\nfor epoch in xrange(epochs):\r\n  # ...\r\n  for (x, y) in dataset.take(MAX_BATCHES):\r\n    # ...\r\n```"]}, {"number": 19498, "title": "tf.reduce_sum not consistent with documentation when axis==[ ]", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  9.0.176 / 7.0.5\r\n- **GPU model and memory**: GeForce GTX 1050 Ti/PCIe/SSE2 4GB\r\n- **Exact command to reproduce**: custom script\r\n\r\n### Describe the problem\r\ndocumentation says:\r\nIf axis has no entries, all dimensions are reduced, and a tensor with a single element is returned.\r\n\r\nIn practice:\r\nIf axis == []: nothing is reduced (the output is the input itself)\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\n\r\nconstant = tf.constant([[1., 2.], [3., 4.]])\r\nreduce = tf.reduce_sum(constant, axis=[])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(reduce))", "comments": ["The current behavior of tensorflow matches numpy:\r\n```\r\n# python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> x = np.asarray([[1., 2.], [3., 4.]])\r\n>>> np.sum(x, ())\r\narray([[1., 2.],\r\n       [3., 4.]])\r\n>>> \r\n```\r\n\r\nSo I think this is a documentation (not implementation) issue. Created a PR #19509 for the documentation fix."]}, {"number": 19497, "title": "NHWC convolution sometimes incorrectly considered NCHW", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  9.0.176 / 7.0.5\r\n- **GPU model and memory**: GeForce GTX 1050 Ti/PCIe/SSE2 4GB\r\n- **Exact command to reproduce**: custom script\r\n\r\n### Describe the problem\r\nWhen using dilated conv2d and bias_add, the conv2d is incorrectly considered NCHW, so the normal dilation format is not accepted. Strangely when there is no bias_add, the problem does not happen.\r\n\r\n### Source code / logs\r\nThis gives ERROR:\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\ndef test_add_conv_transform1():\r\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\r\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\r\n    \r\n    conv = tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\r\n\r\n    bias1 = tf.get_variable(name='bias', shape=[16], dtype=tf.float32)\r\n    return tf.nn.bias_add(conv, bias1)\r\n        \r\no = test_add_conv_transform1()\r\nwith tf.Session() as sess:\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\r\n```\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support dilations in the batch and depth dimensions.\r\n         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 4, 4, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, filter/read)]]\r\n\r\nInterestingly this works as intended:\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\ndef test_add_conv_transform1():\r\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\r\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\r\n\r\n    return tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\r\n        \r\no = test_add_conv_transform1()\r\nwith tf.Session() as sess:\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\r\n```\r\n", "comments": ["I tried with v1.8.0 and it works fine. Maybe the issue has been resolved?", "Hi! Thanks for trying! I'll check out the 1.8 version tomorrow at work, and then confirm the results to you. Cheers", "It is fixed in 1.8. Thank you.", "It was good only in tf 1.8 CPU version, but in the GPU version it is still erroneous. \r\n@yongtang  please confirm", "@zheng-xq Can you take a look?", "This seems to be a layout optimizer issue. Adding Yao. ", "Thanks for reporting the bug. I have submitted a fix, which will be pushed to TensorFlow github and reflected in nightly build in a day or two; it will also be part of the next release.\r\n\r\nMore info on nightly build: https://github.com/tensorflow/tensorflow/", "Hi, I am also using 1.7 version and facing the same issue. Is there any fix for it?"]}, {"number": 19496, "title": "Segfault with rpc ops in eager mode", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary, pip\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nRpc ops (`rpc`, `try_rpc`)  are not working in eager mode. See minimal examples below.\r\n\r\n### Source code / logs\r\n\r\nNon-eager version works\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rpc.python.ops.gen_rpc_op import try_rpc\r\n\r\nwith tf.Graph().as_default():\r\n    response = try_rpc('localhost:80', '/Test', 'some simple message', protocol='grpc')\r\n    session = tf.InteractiveSession()\r\n    print( session.run([response], feed_dict={}) )\r\n```\r\nEager version does not (results in segfault):\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rpc.python.ops.gen_rpc_op import try_rpc\r\ntf.enable_eager_execution()\r\nresponse = try_rpc('localhost:80', '/Test', 'some simple message', protocol='grpc')\r\n```\r\n\r\n", "comments": ["@jsimsa : Mind taking a look?", "will do", "This was just fixed internally and will be pushed to master in the next sync."]}, {"number": 19495, "title": "How to optimize for inference , from frozen graph?", "body": "```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\ninputGraph = tf.GraphDef()\r\nwith tf.gfile.Open(\"frozen.pb\", \"rb\") as f:\r\n    data2read = f.read()\r\n    inputGraph.ParseFromString(data2read)\r\n\r\noutputGraph = optimize_for_inference_lib.optimize_for_inference(\r\n                inputGraph,\r\n                [\"inputTensor\"], # an array of the input node(s)\r\n                [\"output/output\"], # an array of output nodes\r\n                tf.int32.as_datatype_enum)\r\n\r\n        # Save the optimized graph\r\n\r\nf = tf.gfile.FastGFile(\"outputOptimizedGraph.pb\", \"w\")\r\nf.write(outputGraph.SerializeToString())    \r\n\r\n```\r\nOutput : /home/bibhu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"convert_frozen.py\", line 14, in <module>\r\n    tf.int32.as_datatype_enum)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py\", line 111, in optimize_for_inference\r\n    placeholder_type_enum)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 86, in strip_unused\r\n    output_node_names)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 174, in extract_sub_graph\r\n    _assert_nodes_are_present(name_to_node, dest_nodes)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-**packages/tensorflow/python/framework/graph_util_impl.py\", line 133, in _assert_nodes_are_present\r\n    assert d in name_to_node, \"%s is not in graph\" % d\r\nAssertionError: output/output is not in grap**", "comments": ["@achalshah20  Here is the new issue!", "```\r\noutputGraph = optimize_for_inference_lib.optimize_for_inference(\r\n                inputGraph,\r\n                [\"inputTensor\"], # an array of the input node(s)\r\n                [\"output/predictions\"], # an array of output nodes\r\n                tf.int32.as_datatype_enum)\r\n```"]}, {"number": 19494, "title": "Correct type error : Pre-requesits -> Pre-requests", "body": "please, correct this error.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 19493, "title": "How to get the output of a layer when making predictions?", "body": "My code is something like this:\r\n```\r\n        from tensorflow.contrib.layers import fully_connected\r\n\r\n        hidden1 = fully_connected(train_inputs, hidden[0], scope=\"hidden1\") \r\n        hidden2 = fully_connected(hidden1, hidden[1], scope=\"hidden2\")\r\n        u = fully_connected(hidden2, hidden[2], scope=\"u\")\r\n\r\n        loss = tf.reduce_mean(\r\n            tf.nn.sampled_softmax_loss(\r\n                weights=nce_weights,\r\n                biases=nce_biases,\r\n                labels=train_labels,\r\n                inputs=u,\r\n                num_sampled=sample_num,\r\n                num_classes=class_num,\r\n                remove_accidental_hits=True),\r\n            name='loss')\r\n```\r\n\r\nAfter training, i loaded the model and make predictions for my data. I want to get the 'u' scope layer output in the net as output. How can i achieve that?\r\nNotice that 'u' is a scope define, so i can not just use sess.run(['u:0'])\r\n\r\nI load the model as follows:\r\n```\r\n    sess = tf.Session()\r\n    saver = tf.train.import_meta_graph(meta_path)\r\n    saver.restore(sess, model_path)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 19492, "title": "How to freeze the graph ? ", "body": "```\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\ntf.train.write_graph(sess.graph.as_graph_def(),'.','tensorflowModel.pbtxt', as_text=True)\r\n\r\nsaver.save(sess, './my_test_model.ckpt')\r\n\r\nfreeze_graph.freeze_graph('./tensorflowModel.pbtxt', \"\",False,'./my_test_model.ckpt', \"output/predictions\", \"save/restore_all\",  \"save/Const:0\",'frozen.pb', True,\"\")\r\n\r\n```\r\n\r\nError:\r\nfreeze_graph.freeze_graph('./tensorflowModel.pbtxt', \"\",False,'./my_test_model.ckpt', \"output/predictions\", \"save/restore_all\",  \"save/Const:0\",'frozen.pb', True,\"\")\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py\", line 153, in freeze_graph_with_def_protos\r\n    variable_names_blacklist=variable_names_blacklist)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 232, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 174, in extract_sub_graph\r\n    _assert_nodes_are_present(name_to_node, dest_nodes)\r\n  File \"/home/bibhu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 133, in _assert_nodes_are_present\r\n    **assert d in name_to_node, \"%s is not in graph\" % d\r\nAssertionError: output/predictions is not in graph**\r\n \r\n", "comments": ["Check your pbtxt file and find valid output node. \"output/predictions\" might have been changed to \"output/predictions:0\" or something else. Just verify from pbtxt file or attach that file so I can take a look. ", "@achalshah20  Actually there is a problem in converting the frozen.pb file( generated after freeze_graph.py) to optimize inference file so it can be deployed on Android.\r\n\r\n```\r\nnode {\r\n  name: \"output/predictions\"\r\n  op: \"ArgMax\"\r\n  input: \"output/scores\"\r\n  input: \"output/predictions/dimension\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"Tidx\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"output_type\"\r\n    value {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\n\r\n\r\n```\r\n", "DONE!\r\n@achalshah20 "]}, {"number": 19491, "title": "Performance problem with TensorFlow training", "body": "Hello,\r\n    We are running a not-very-complex 3D convolution problem had we have extremely poor performance. Here is the summary of our problem\r\n\r\nNow the technical part.\r\n\r\nI am running on an Haswell CPU in a Mac OS running High Sierra. \r\n\r\nModel Name:\tMacBook Pro\r\nModel Identifier:\tMacBookPro11,5\r\nProcessor Name:\tIntel Core i7\r\nProcessor Speed:\t2.5 GHz\r\nNumber of Processors:\t1\r\nTotal Number of Cores:\t4\r\nL2 Cache (per Core):\t256 KB\r\nL3 Cache:\t6 MB\r\nMemory:\t16 GB\r\n\r\n\r\nTensorflow performance\r\n---------------------------------\r\n\r\n1.) Memory allocation\r\n\r\nMemory allocation seems highly unoptimized. I see an allocation of ~80GB (78M allocations) out of which we are left with 37GB persistent (corresponding to 575k permanent allocations). The memory churn is enormous and this may affect very seriously performance. Most of those are very small allocation / deallocation which happen here\r\n\r\nEigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::Schedule(std::__1::function<void ()>)\r\n\r\nI have tried to run with tcmalloc from google hoping to improve memory allocation and handling. tcmalloc complains that there are the following \u201clarge allocation\u201d from TF even before starting the epochs: (23+23+2+18+4+9+2) = 81GB of allocation even before starting the epoch\u2019s. After that my disk is full of swap files and my machine dies.\r\n\r\nThen I went back running with the Mac allocator, which surprisingly seems to be more robust. \r\n\r\n2.) Code performance\r\n\r\nA careful VTune analysis performed by Sofia has identified Eigen as the major source of CPU consumption.  All the time is wasted simply in repacking (gemm_pack_rhs).\r\n\r\nTo look at the code I attempted to compile with -g, however the default compilation is with -g0 and I could not find yet a way to replace this default on bazel. I added -g3 that, according to the manual (and to a small test I have made) should override -g0. However the Mac Instrument (a poor relation of VTune on Mac) could not find out the source. The library should be _pywrap_tensorflow_internal.so. Then I went looking for the source and I found that gemm_pack_rhs::operator() is defined in the following files\r\n\r\n./bazel-tensorflow/external/eigen_archive/Eigen/src/Core/products/GeneralBlockPanelKernel.h\r\n./bazel-tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h\r\n\r\nThe last two are identical. Putting good old printf\u2019s I discovered we are calling the GeneralBlockPanelKernel.h version. The operator works with packets of size 8, which is fine for AVX2 (256) and float32 as we are using. However I am not sure that the compiler manages to vectorize this procedure. Indeed, most of the time is spent in line 559 of eigen_volume_patch.h. \r\n\r\n   for (int i = 0; i < PacketSize; ++i) {\r\n      values[i] = coeff(index + i);\r\n    }\r\n\r\nThe packet structure of the code is meant to have each packet treated as a unit. This for loop simply destroys all possibility of optimisation. There is a lot of room for optimisation in tensorflow before we get really serious about performance with a problem like ours. But who is going to pick up the tab? \r\n\r\n3.) MKL or not MKL. \r\n\r\nWhen bringing up this problem, I have been answered that tensorflow in Mac does not support the usage of MKL, and therefore, till then, my findings were not entirely relevant. MKL for Mac exists, however clang does not support OMP (or rather the default version of clang distributed with Mac does not have OMP support enabled). So the only way to compile tensorflow on the Mac with MKL was to change compiler. \r\n\r\nUnfortunately changing compiler with bazel on the Mac seems a very ambitious proposition. After posting to and perusing stackoverflow, bazel forum and tensorflow forum, I came to the following recipe\r\n\r\nexport BAZEL_USE_CPP_ONLY_TOOLCHAIN=1 \r\nexport CC=/path/to/compiler\r\nbazel build [\u2026]\r\n\r\ndoes indeed force Bazel to use a new compiler, however controlling the compiler switches is much more complicated. The two compiler flags -Wthread-safety and -Wself-assign, as well as the linker flag \u201c-no-as-needed\u201d and \u201c-z\u201d are incompatible with g++ linker. The CROSSTOOL.tpl are automatically generated during configuration. The only occurrences of (for instance) -Wself-assign in the TF code are in \r\n\r\nthird_party/gpus/crosstool/CROSSTOOL_nvcc.tpl\r\nthird_party/toolchains/clang6/CROSSTOOL.tpl\r\nthird_party/toolchains/cpus/arm/CROSSTOOL.tpl\r\n\r\nbut even if I comment the lines:\r\n\r\n-  compiler_flag: \"-Wthread-safety\"\r\n-  compiler_flag: \"-Wself-assign\"\r\n+#  compiler_flag: \"-Wthread-safety\"\r\n+#  compiler_flag: \"-Wself-assign\u201d\r\n\r\nin all three of them \u201csomething\u201d creates a CROSSTOOL.tpl with these flags in. The hack I am trying now is to configure and then edit the file\r\n\r\n./bazel-tensorflow/external/local_config_cc/cc_wrapper.sh\r\n\r\nadding the following line\r\n\r\n/sw/bin/gcc-fsf-7 `echo \"$@\" | sed -e 's/-Wself-assign//' | sed -e 's/-Wthread-safety//' | sed -e 's/-Wl,-no-as-needed//' | sed -e 's/-Wl,-z,relro,-z,now//\u2018`\r\n\r\nwhich is a very poor hack. \r\n\r\nWith this I could build a version of tensorflow using the Mac MKL, but to no avail. Performance is still abysmal with the same bottleneck. \r\n\r\nThanks for reading up to here...", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and distribution\r\nPlatform:\t\tMacBook Pro\r\nDistro:  High Sierra. 10.13.4 (17E202)\r\nModel Identifier:\tMacBookPro11,5\r\nProcessor Name:\tIntel Core i7, Haswell\r\nProcessor Speed:\t2.5 GHz\r\nNumber of Processors:\t1\r\nTotal Number of Cores:\t4\r\nL2 Cache (per Core):\t256 KB\r\nL3 Cache:\t6 MB\r\nMemory:\t16 GB\r\n\r\nTensorflow installed from git master\r\nTensorflow version: head \r\nBazel version: bazel release 0.13.1-homebrew\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: \r\n \r\nWe  use keras version 2.1.2 \r\n \r\nThe model : https://github.com/svalleco/3Dgan/blob/master/keras/EcalEnergyGan.py\r\nThe test training script: https://github.com/svalleco/3Dgan/blob/master/keras/EcalEnergyTrain.py\r\nTraining dataset: https://cernbox.cern.ch/index.php/s/XIKU7gZA7jjzKsf\r\n \r\nYou\u2019ll need to change the input file path in line 96\r\nhttps://github.com/svalleco/3Dgan/blob/master/keras/EcalEnergyTrain.py#L96\r\n \r\nThe scripts uses a small subset of data and runs for only two epochs to speedup profiling.\r\nThe keras config file attached sets the data format and the backend (TF). It goes in the .keras directory.\r\n", "Forgot the keras file. I had to attach .txt at the end to be able to upload it\r\n\r\n[keras.json.txt](https://github.com/tensorflow/tensorflow/files/2044780/keras.json.txt)\r\n", "A small update on this matter. When we make the filters multiple of 16 and the dataset size multiple of 32, the training speed improves by a factor 30. Apparently when these two conditions are met, the data are repacked by MKL, which is much faster. Otherwise some slower functions are used, and the difference is massive. \r\n", "@fcacarminati, Filters being a multiple of 16 is recommended to get the best performance on systems with Intel AVX512 instruction set. For systems with Intel AVX2 support (like your Macbook) multiple of 8 is enough. There's no restrictions related to the dataset size or the minibatch size, though the larger minibatch will results in better performance.", "I'm not sure I understand the core of this issue. Can you explain what kind of response you are looking for here?", "Thanks for your comment. My point is that under some conditions (i.e. filters not being multiple of 8 or 16), the training performance is sub-optimal and half of the time is spent repacking tensors. ", "Thanks a lot\u2026 still the performance should be better, and I am pointing out where I think it could be improved. Best,\r\n\r\nFederico Carminati\r\nChief Innovation Officer\r\nCERN openlab\r\n1211 Geneva 23\r\nSwitzerland\r\nTel: +41 22 76 74959\r\nFax: +41 22 76 68505\r\nMobile: +41 75 411 4843\r\n\r\nOn Jun 1, 2018, at 19:12, Vadim Pirogov <notifications@github.com<mailto:notifications@github.com>> wrote:\r\n\r\n\r\n@fcacarminati<https://github.com/fcacarminati>, Filters being a multiple of 16 is recommended to get the best performance on systems with Intel AVX512 instruction set. For systems with Intel AVX2 support (like your Macbook) multiple of 8 is enough. There's no restrictions related to the dataset size or the minibatch size, though the larger minibatch will results in better performance.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/19491#issuecomment-393946011>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AEBwoW4S4-D1KhbeSeDXPLTl7SxLsC4yks5t4XX7gaJpZM4UJ8uZ>.\r\n\r\n", "@tfboyd Can you take a look at this?", "@tatianashp    Assigning to the CPU performance lead.  No low hanging fruit for me to grab.", "Actually I would really like an answer... thanks!", "@fcacarminati Thank you for pointing out a performance problem. I apologize that this issue fell through the cracks. If I understand your request correctly, you are asking if we could improve performance of\r\nthe loop in line 559 of eigen_volume_patch.h.\r\n    for (int i = 0; i < PacketSize; ++i) {\r\n        values[i] = coeff(index + i);\r\n    }\r\nIs this correct?\r\n\r\n/cc @ezhulenev @rmlarsen ", "Thanks for looking into my problem. In my problem 50% of the time is spent in this line. The packet should be executed in a single vector instruction, ideally. There it is executed in a loop calling other procedures. This kills optimisation, as far as I understand. Regards,\r\n\r\nFederico Carminati\r\nChief Innovation Officer\r\nCERN openlab\r\n1211 Geneva 23\r\nSwitzerland\r\nTel: +41 22 76 74959\r\nFax: +41 22 76 68505\r\nMobile: +41 75 411 4843\r\n\r\nOn Jul 20, 2018, at 20:07, Tatiana Shpeisman <notifications@github.com<mailto:notifications@github.com>> wrote:\r\n\r\n\r\n@fcacarminati<https://github.com/fcacarminati> Thank you for pointing out a performance problem. I apologize that this issue fell through the cracks. If I understand your request correctly, you are asking if we could improve performance of\r\nthe loop in line 559 of eigen_volume_patch.h.\r\nfor (int i = 0; i < PacketSize; ++i) {\r\nvalues[i] = coeff(index + i);\r\n}\r\nIs this correct?\r\n\r\n/cc @ezhulenev<https://github.com/ezhulenev> @rmlarsen<https://github.com/rmlarsen>\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/19491#issuecomment-406754541>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AEBwoSkbGTNyGyErk9qy8-Qq9nh-dH7kks5uInCvgaJpZM4UJ8uZ>.\r\n\r\n", "@ezhulenev Could you take a look at what's going on here?", "@fcacarminati Unfortunately in your case the volume patch data is scattered across all of the input buffer, and there is no contiguous blocks of 8+ scalars, so TensorVolumePatchOp packet access has to construct packet by loading scalars one by one.\r\n\r\nOne option to make it faster, is to create custom LHS/RHS packer, similar to what you can find in eigen_spatial_convolutions, I'll take a look on it, it does seem like relatively simple thing. That change made Conv2D ~2x faster, so I expect to see somewhat similar results for Conv3D.", "[just back from vacation] Thanks a lot for your answer. I am really glad you acknowledged this issue. We are really dependent on the training performance for our studies and any improvement could be a big help. Please keep us informed about your progress. Best regards, ", "I'm working on it right now, have 10x-125x improvements for the Conv3D and Conv3DBackward, should land in master soon.", "Hello, \r\n   this is fantastic news and we cannot wait to test it. If you want, you can give us the code and we will try it out and let you know. Thanks again for your help. Best, ", "@fcacarminati I've submitted couple of changes that should make Conv3D performance much better (I'd expecte something like ~5x). One of them is custom kernels for backprop input and filter: https://github.com/tensorflow/tensorflow/commit/e183b8d0328d7398cb6ffc530d1ae8fdbd2111c0\r\n\r\nThese kernels make less smaller allocations, instead they allocate quite large temporary buffers, so you might see increased peak memory usage. It's possible to fallback on original Eigen kernels using `kernel_label_map` (see https://github.com/tensorflow/tensorflow/blob/8cb0558da924e891aa1bb5d79a6c0c846301e4eb/tensorflow/python/framework/ops.py#L3311), old kernels registered with a \"eigen_tensor\" label.\r\n\r\nI'd be super interested to know how much this helps in your specific case.", "Thanks a lot! I have tested immediately your changes and I have a 2.5x speedup on my Mac (haswell). We will make more conclusive tests on a GPU system, but this is already very very good. More news soon. Thanks again for your help, this makes the difference already for us, ", "Another remark, which I do not know whether is correlated or not. The famed method gemm_pack_rhs taking up 50% of the time in my original post, is now taking 3%, which explains a large part of the speedup. A question, does this mod also applies to GPU code? Best", "No, these changes are for CPU only, GPU kernels are the same cudnn base code as before.\r\n\r\nCan you upload the graph def that you are executing on CPU, TF CPU convolutions require NDHWC* data format, and from your code I see that Keras is using channels_first. I guess there might be some unnecessary layout conversions. Also I don't have a good representative Conv3D dimensions for benchmarks: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_benchmark_cpu_test.cc#L403\r\n\r\n*) Channels first works best for GPU (aka NCDHW).", "> @fcacarminati I've submitted couple of changes that should make Conv3D performance much better (I'd expecte something like ~5x). One of them is custom kernels for backprop input and filter: [e183b8d](https://github.com/tensorflow/tensorflow/commit/e183b8d0328d7398cb6ffc530d1ae8fdbd2111c0)\r\n> \r\n> These kernels make less smaller allocations, instead they allocate quite large temporary buffers, so you might see increased peak memory usage. It's possible to fallback on original Eigen kernels using `kernel_label_map` (see\r\n> \r\n> [tensorflow/tensorflow/python/framework/ops.py](https://github.com/tensorflow/tensorflow/blob/8cb0558da924e891aa1bb5d79a6c0c846301e4eb/tensorflow/python/framework/ops.py#L3311)\r\n> Line 3311 in [8cb0558](/tensorflow/tensorflow/commit/8cb0558da924e891aa1bb5d79a6c0c846301e4eb)\r\n>  def _kernel_label_map(self, op_to_kernel_label_map): \r\n> ), old kernels registered with a \"eigen_tensor\" label.\r\n> I'd be super interested to know how much this helps in your specific case.\r\n\r\nI have a factor 3 improvement! And I have a factor 2.5 with the 3D patch changes. All in all this would give me a fantastic factor close to 10. When can we expect these two changes to get into the same branch? We are *really* depending on performance here and the work seems to be mostly done. One more little effort please :-) Best regards\r\n", "@fcacarminati,\r\nSorry for the delayed response. \r\n\r\nAs there were many upgrades in **`Tensorflow, CUDA and CuDNN`** versions since past 3 years, can you please confirm if your issue still persist? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19491\">No</a>\n"]}, {"number": 19489, "title": "TF eager backdrop does not co-locate gradient computation correctly.  Results in Placement warning.  ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.08\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:  Tesla V100-SXM2-16GB\r\n- **Exact command to reproduce**: - \r\n\r\n### Describe the problem\r\nWhile moving a model to eager execution, I encountered an error using gradient_tape for back propagation.  While as far as I can tell all operations are taking place on the GPU, during back prop I get the following error:\r\n\r\n```\r\n>   File \"tf_registration_continuous.py\", line 128, in single_registration_step\r\n>     elastic_grads = tape.gradient(loss_value, elastic_variable_list)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 767, in gradient\r\n>     output_gradients=output_gradients)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 63, in imperative_grad\r\n>     tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 147, in grad_fn\r\n>     op_inputs, op_outputs, orig_outputs)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 115, in _magic_gradient_function\r\n>     return grad_fn(mock_op, *out_grads)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 427, in _GatherV2Grad\r\n>     params_shape = math_ops.to_int32(params_shape)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 875, in to_int32\r\n>     return cast(x, dtypes.int32, name=name)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 787, in cast\r\n>     x = gen_math_ops.cast(x, base_type, name=name)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1548, in cast\r\n>     _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n>   File \"/share/software/user/open/py-scipystack/1.0_py27/lib/python2.7/site-packages/six.py\", line 718, in raise_from\r\n>     raise value\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu() methods, or transparently copied by using tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: ToInt32/\r\n```\r\n\r\nI might also put in a feature request to make backdrop error messages more verbose -- since I can not tell what operation in the model is actually causing this error.  ", "comments": ["Could you provide more information - such as something to reproduce the error?\r\n\r\nAs a workaround, you can do what the error message suggests, i.e., use:\r\n`tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_SILENT)`. Tensors will still be copied between CPU/GPU if needed, but it won't fail. Though, that should be happening by default anyway, so are you explicitly changing the default device placement policy?\r\n\r\nThat said, this seems to be coming from the gradient of a `tf.gather`. I'll be curious to dig into this further, but there is a reasonable chance that you're seeing this on the CPU because int32 tensors are often kept in host memory. `DEVICE_PLACEMENT_SILENT` mimics the behavior of TensorFlow graph execution (where tensors are copied between devices if needed).", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19488, "title": "tensorflow 'killed' issue on Jetson TX1 ", "body": "i use tensorflow v1.3.0 on Jetson TX1.\r\nwhen i use tensorflow to train model repeatedly, occur 'killd' message or 'system stop' after one normal training. this error occurs during the model builing phase.\r\ni reboot TX1 then i can train model without any problem.\r\nall the time, i need to reboot TX1 every model training.\r\n\r\n in other words, i can train model with tensorflow only once without any error. after training, if i want to train again, i need to reboot TX1.\r\n\r\ni think that memory can't be released after processs died or closed.\r\nso, i force to refresh cache memory and swap memory directly. but it still doesn't work normally.\r\nin other deivce such as JTX2, desktop PC, i can train model with tensorflow without any error.\r\nalso, i reinstalled tensorflow after format, but i still this issue.\r\nhelp me, please.\r\n", "comments": ["Do you try to add more swap space on TX1? \r\n\r\nIf you don't need Desktop environment when you training. You can disable it.\r\n$ sudo service lightdm stop", "@peterlee0127  thank you for your answer.\r\ni use 119GB SD card for swap memory so i think that enough to train model.\r\ni execute train command in the Desktop PC to JTX1 by using ssh\r\nso i stop lightdm service with your comment but i still get killed error.\r\ni enforce to terminate training by using ctl+c and use run_cotext.request_stop() but i get same killd error.\r\nwhy can't release tensorflow resource normally when  process died.....\r\n\r\n\r\n", "Before you start to training model, you can run jetson_clocks.sh, it will let your tx1 enable high performance. \r\n$ ./jetson_clocks.sh\r\n\r\n I am not sure tx1 is same or not.\r\n$ sudo nvpmodel -m 0 \r\nWhen you running your training, you can run tegrastats to check your memory.\r\n$ sudo ./tegrastats\r\n\r\nCould you run this to check your available memory ? It can check the max amount memory for your gpu.\r\nhttps://devtalk.nvidia.com/default/topic/974063/jetson-tx1/caffe-failed-with-py-faster-rcnn-demo-py-on-tx1/post/5013784/#5013784\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "i can't solve this problem....", "Have you solve the problem with \"killed\" on Jetson TX1\uff1f\r\nWill adding more swap space on TX1 solve the issue?\r\nThank you for reply.", "hello\ni could't solve this issue.\ni reboot jtx1 after training to repeat training....\ni hope averything goes well.\n \n-----Original Message-----\nFrom: \"TianChenone\"<notifications@github.com>\nTo: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\nCc: \"GUBONTAK\"<ku9203@naver.com>; \"State change\"<state_change@noreply.github.com>;\nSent: 2018-08-15 (\uc218) 21:43:04\nSubject: Re: [tensorflow/tensorflow] tensorflow 'killed' issue on Jetson TX1 (#19488)\n \nHave you solve the problem with \"killed\" on Jetson TX1\uff1f\nWill adding more swap space on TX1 solve the issue?\nThank you for reply.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread."]}, {"number": 19487, "title": "platform.syscofig.has no get_compile_flags()and get_link_flags() function after v1.*,why\uff1f is it  implemented in other places\uff1f", "body": "platform.syscofig.has no get_compile_flags()and get_link_flags() function after v1.*,why\uff1f is it  implemented in other places\uff1f Thanks!", "comments": []}, {"number": 19486, "title": "Python Dropout op will not return directly when keep_prob =1", "body": "Python Dropout op uses the following code to check keep_prob value:\r\n`if tensor_util.constant_value(keep_prob) == 1:\r\n  return x`\r\nIf keep_prob is placeholder,  tensor_util.constant_value(keep_prob) will return None,  if statement will always be false.\r\n\r\nIn python/ops/nn_test.py\r\nWhen test keep_prob value (testDropoutPlaceholderKeepProb), it only test keep_prob in [0.1, 0.5, 0.8],\r\nWe should add the case of keep_prob = 1.0 test. \r\n\r\n \r\n\r\n", "comments": ["> If keep_prob is placeholder, tensor_util.constant_value(keep_prob) will return None, if statement will always be false.\r\n\r\nYou're right and this is exactly the expected behavior.", "@ppwwyyxx , do you think we should fix this? Thanks.", "I don't because I didn't see a bug. The behavior given a placeholder is correct.", "If the value of keep_prob placeholder is 1, dropout should return directly. But the current code will not. \r\nFor inference, it is reasonable to set keep_prob to 1 to skip dropout op. The current code cannot skip dropout.", "If you know the value **will**  always be 1, you should use a constant to enable optimization.", "Nagging Assignee @tatatodd: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19485, "title": "Update supervisor.py", "body": "This was quite confusing (encountered when modifying original tutorial https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py )", "comments": ["Thanks for the fix!"]}, {"number": 19484, "title": "Question about using of TFRecordDataset", "body": "Hi every body:\r\n   I am new to tensorflow, I am now try to read tfrecords file by TFRecordDataset,  I use the code like follows:\r\n      traindataset = tf.data.TFRecordDataset(train_file) \r\n      traindataset = traindataset.map(dataset._parse_record)\r\n    \r\n        \r\n    iterator = traindataset.make_one_shot_iterator()\r\n    images, labels, comment = iterator.get_next()\r\n\r\n   raw_images, raw_labels, comments = sess.run([images, labels, comments])\r\n   for img, lex, comment in zip(raw_images, raw_labels, comments):\r\n          ......\r\n      \r\n   And I am not sure that img got like above are regular images or the serialized version of images? And should I use the following statement to convert them like:\r\n          img_decoded = tf.image.decode_image(img) \r\n        \r\n          \r\n    ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 33 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19483, "title": "Fix BFCAllocator::Extend alignment issues", "body": "The BFCAllocator::Extend method used an incorrect harcoded alignment\r\nof 32 in two different places.  This led to alignment check assertions when\r\nExtend was called, as all memory is now expected to be 64 byte aligned.\r\n\r\nSigned-off-by: Mark Ryan <mark.d.ryan@intel.com>", "comments": []}, {"number": 19482, "title": "CUDA cannot create more than one session", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Xenial\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**: \r\n1.8\r\n- **Python version**:\r\n2.7 \r\n- **Bazel version (if compiling from source)**:\r\n0.13\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nTegra x2\r\n- **Exact command to reproduce**:\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4 # I have also tried with allow memory growth\r\nsess1 = tf.Session(config=config)\r\nsess2=tf.Session(config=config) # Cannot create the session\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have a Jetson TX2 with updated drivers and the last jetpack provided by Nvidia, I have built tensorflow (r.1.5 and r.18) and I'm not able to create more than one session, I can execute operations and everything with only one session, but once I create a new session, I encounter that tensorflow cannot create a new session, which I suspect is Nvidia fault, but the error is not that informative:\r\n\r\n```\r\n  File \"object_detection.py\", line 183, in detection\r\n    with tf.Session(graph=detection_graph,config=config) as sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1509, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 628, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n```\r\nIs there any way I can get more information about the cuda error or status? So I can complement my bug report?\r\n\r\nThanks\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@tfboyd do we have a way to reproduce this?\r\n\r\n@Davidnet just to clarify, do you not experience this problem with a CPU-only build?", "I use a Jetson TX2, so its quite difficult to use a cpu only build (takes a lot of time). I'm concerned since I do not know how to fill a proper bug report. How to debug programs with CUDA and CUdnn ?", "I had not used or seen multiple sessions in a script but indeed it happens and there as an issues resolved a long time ago.  I would test the code on a regular GPU just to rule in or out Jetson being the issue.  We do not have a Jetson TX2 or TX setup in our area.  I would also wrap the session in a with.device just to make sure it is going where you want, but that is likely not needed really.  If there was a CUDA issue I would expect to see a CUDA error but expectation does not always match reality.  \r\n\r\n[Here is the multi-session ](https://github.com/tensorflow/tensorflow/issues/9205)example I found while looking for issues.  They call run before starting the next session but I do not see why that would matter.  \r\n\r\nFor debugging CUDA, I do not have direct knowledge.  You could try the tfdebugging as a starting point.  https://www.tensorflow.org/programmers_guide/debugger  I also do not have experience with it.  Huge help right? \r\n\r\nThat is where I would start.  Finding out it is a Jetson specific issue would be my first thought, then going from there.  \r\n", "I'm now getting error on the CUDA runtime implicit initialization on GPU\r\n\r\n\r\n```\r\n2018-05-31 01:23:28.378527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-31 01:23:28.378766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4744 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\r\nBuilding Graph\r\n2018-05-31 01:23:40.001179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-31 01:23:40.001338: E tensorflow/core/common_runtime/direct_session.cc:154] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: unknown error\r\nTraceback (most recent call last):\r\n  File \"real_time_detection.py\", line 174, in <module>\r\n    main()\r\n```", "@Davidnet There is likely not much I can debug.  I would suggest the following:\r\n\r\n- Share the code you are running.  All of it so someone else could cut/paste and run it.  If someone has a few minutes to look at the issue they will get a lot farther, before giving up, if they have code to run and a lot of details stated in a concise manner.\r\n\r\n- Be clear about what you tried.  In this case, without the code it is hard to know why you saw no initialization message before but you do not.  What did you change?\r\n\r\n-  Share the full log and command-line that was run.  Link to a file or just paste it in.  \r\n\r\nFinally, keep in mind this is not a help desk and mostly it is about giving you ideas.  In the case of the Jetson even more so as we do no have those sitting around like we do GPUs or CPUs on our local machines.  \r\n\r\nDo not read this as not wanting to help.  Everyone wants to help and the biggest frustration is not having enough information.  ", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}]