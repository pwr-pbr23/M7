[{"number": 19169, "title": "Problem of fusing CONV-BN-RELU together of TF-Lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: 16GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nBN-RELU-CONV structure is becoming more popular recently. However, for TF-Lite, if there are frequent BN-RELU-CONV structures in the graph, the (BN-RELU-CONV) * N sequence will be matched as BN-RELU-(CONV-BN-RELU) * (N-1) - CONV sequence. To make TOCO tools work, it is simply to add an additional 1x1 CONV op at the very beginning, such like conv-(BN-RELU-CONV) * N. Then, it will be  conv-BN-RELU-(CONV-BN-RELU) * (N-1) -CONV and TOCO can fuse them. However, the shapes between CONV in first block and BN-RELU in second block could be not matched. Thus, it will be crashed during the runtime. The single way to solve this problem is to add an 1x1 conv in front of every BN-RELU-CONV strcture. However, this changes too much from the original model. Thus, in my opinion, fusion of CONV-BN-RELU ops should not be hard coded. \r\n\r\nI am not sure that this problem only occurs in TF-Lite. It seems that the BN folding method of TF graph_transform will also match the corresponding CONV-BN structure at first.\r\n\r\n### Source code / logs\r\nFor example, DenseNet:\r\nRuntime Error:\r\n> java.lang.NullPointerException: Can not allocate memory for the given inputs: third_party/tensorflow/contrib/lite/kernels/mul.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 1)\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @suharshs: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The issue has been solved by folding bn before converting it to tflite model."]}, {"number": 19168, "title": "tf.control_dependencies() not work in a custom_gradient function", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 2.7.14\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 8.0/7.0\r\n- **GPU model and memory**: GTX1080, 8G\r\n- **Bazel version**: N/A\r\n- **Exact command to reproduce**: python test_script.py\r\n\r\n### Describe the problem\r\nThe output for the code below, is not correct. The first value of variable \"assign_var\" is not assigned with 2. The expected output for the 2nd line is: [2 0]. Thus, the approach [here][1] cannot work with \"@tf.custom_gradient\". I don't know if this is a bug or there is a solution for this. Thanks in advance!\r\n\r\n### Source code / logs\r\n@tf.custom_gradient\r\ndef test_func(x):\r\n    i = tf.constant(0)\r\n    id_refs = tf.Variable(tf.zeros((1), tf.int32))\r\n    assign_var = tf.Variable(tf.zeros((2), tf.int32))\r\n    cond = lambda i, id_refs: tf.less(i, 8)\r\n\r\n    def _body(i, id_refs):\r\n      a = tf.assign(assign_var[0], 2)\r\n      id_refs = tf.concat([id_refs, [i]], 0)\r\n      with tf.control_dependencies([a]):\r\n        return i+1, id_refs\r\n\r\n    _, gradRefs = tf.while_loop(cond, _body, [i, id_refs], shape_invariants= \\\r\n      [i.get_shape(), tf.TensorShape([None])])\r\n    def grad(dy):\r\n        return tf.constant(0)\r\n    return assign_var, grad\r\n\r\nx = tf.constant(100.)\r\ny = test_func(x)\r\ndy = tf.gradients(y, x)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n    print(sess.run(dy))\r\n    print(sess.run(y))\r\n\r\nOutput:\r\n[0]\r\n[0 0]\r\n", "comments": ["@YantianZha,\r\n\r\nAs you had the issue in older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19168\">No</a>\n"]}, {"number": 19167, "title": "Windows 10 GPU JNI Compile Error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Only modified path variables\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.8 or master ( tried both)\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 3.6.3\r\n- **CUDA/cuDNN version**: V9.0 / 7.0\r\n- **GPU model and memory**: GTX 1070 6G (Notebook)\r\n- **Exact command to reproduce**:  \r\nmsys64 :swigwin-3.0.10\r\ntensorflow/tools/ci_build/windows/libtensorflow_gpu.sh\r\n \r\n\r\n### Describe the problem\r\nI need a GPU version of tensorflow_jni.dll but I did not find that trying to compile with an error.\r\nCompilation is perfectly normal in cpu mode, but errors occur in gpu mode, and every time changes error * .o file\r\n\r\n### Source code / logs\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: not all outputs were created or valid\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_enum.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/php/php_generator.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_message_field_lite.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_primitive_field.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_lazy_message_field.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/cpp/cpp_string_field.o' was not created\r\n\r\n### Source code / logs (ALL)\r\nwangxiaoming@MSI MINGW64 /d/projects/tensorflow\r\n$ /d/projects/tensorflow/tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh\r\n+++ dirname /d/projects/tensorflow/tensorflow/tools/ci_build/windows/libtensorflow_gpu.sh\r\n++ cd /d/projects/tensorflow/tensorflow/tools/ci_build/windows\r\n++ pwd\r\n+ SCRIPT_DIR=/d/projects/tensorflow/tensorflow/tools/ci_build/windows\r\n+ source /d/projects/tensorflow/tensorflow/tools/ci_build/windows/bazel/common_env.sh\r\n++ export TMPDIR=C:/tmp\r\n++ TMPDIR=C:/tmp\r\n++ mkdir -p C:/tmp\r\n++ export BAZEL_SH=C:/tools/msys64/usr/bin/bash\r\n++ BAZEL_SH=C:/tools/msys64/usr/bin/bash\r\n++ export PYTHON_BASE_PATH=ProgramData/Anaconda3\r\n++ PYTHON_BASE_PATH=ProgramData/Anaconda3\r\n++ export PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe\r\n++ PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe\r\n++ export PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages\r\n++ PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages\r\n++ export PATH=/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin\r\n++ PATH=/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin\r\n++ export PATH=/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin\r\n++ PATH=/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin\r\n++ export 'PATH=/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'\r\n++ PATH='/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'\r\n++ export 'PATH=/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/extras/CUPTI/libx64:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'\r\n++ PATH='/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/extras/CUPTI/libx64:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/c/ProgramData/Anaconda3/Scripts:/c/ProgramData/Anaconda3:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin:/c/ProgramData/chocolatey/bin'\r\n+ source /d/projects/tensorflow/tensorflow/tools/ci_build/windows/bazel/bazel_test_lib.sh\r\n++ failing_cpu_cc_tests='    //tensorflow/core/kernels:control_flow_ops_test +     //tensorflow/core:example_example_parser_configuration_test +     //tensorflow/core:lib_core_status_test +     //tensorflow/core:lib_monitoring_collection_registry_test +     //tensorflow/core:lib_strings_numbers_test +     //tensorflow/core/platform/hadoop:hadoop_file_system_test +     //tensorflow/core:platform_file_system_test +     //tensorflow/core:platform_logging_test +     //tensorflow/core:util_sparse_sparse_tensor_test +     //tensorflow/cc:framework_gradient_checker_test +     //tensorflow/cc:framework_gradients_test +     //tensorflow/cc:gradients_array_grad_test +     //tensorflow/cc:gradients_math_grad_test +     //tensorflow/cc:gradients_nn_grad_test +     //tensorflow/cc/saved_model:loader_test '\r\n++ broken_cpu_cc_tests='    //tensorflow/cc:framework_cc_ops_test +     //tensorflow/core/platform/cloud:time_util_test +     //tensorflow/core/platform/cloud:oauth_client_test +     //tensorflow/core/platform/cloud:http_request_test +     //tensorflow/core/platform/cloud:google_auth_provider_test +     //tensorflow/core/platform/cloud:gcs_file_system_test +     //tensorflow/core/kernels/cloud:bigquery_table_accessor_test +     //tensorflow/core/kernels/hexagon:graph_transferer_test +     //tensorflow/core/kernels:remote_fused_graph_execute_utils_test +     //tensorflow/core/kernels:requantize_op_test +     //tensorflow/core/kernels:requantization_range_op_test +     //tensorflow/core/kernels:quantized_reshape_op_test +     //tensorflow/core/kernels:quantized_pooling_ops_test +     //tensorflow/core/kernels:quantized_matmul_op_test +     //tensorflow/core/kernels:quantized_conv_ops_test +     //tensorflow/core/kernels:quantized_concat_op_test +     //tensorflow/core/kernels:quantized_bias_add_op_test +     //tensorflow/core/kernels:quantized_batch_norm_op_test +     //tensorflow/core/kernels:quantized_activation_ops_test +     //tensorflow/core/kernels:quantize_op_test +     //tensorflow/core/kernels:quantize_down_and_shrink_range_op_test +     //tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu +     //tensorflow/core/kernels:quantize_and_dequantize_op_test +     //tensorflow/core/kernels:quantization_utils_test +     //tensorflow/core/kernels:debug_ops_test +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test +     //tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding_test +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test +     //tensorflow/core/distributed_runtime:remote_device_test_gpu +     //tensorflow/core/distributed_runtime:remote_device_test +     //tensorflow/core/distributed_runtime:executor_test_gpu +     //tensorflow/core/distributed_runtime:executor_test +     //tensorflow/core/debug:debug_gateway_test +     //tensorflow/core/debug:debug_grpc_io_utils_test +     //tensorflow/core:util_reporter_test +     //tensorflow/core:util_memmapped_file_system_test +     //tensorflow/core:platform_subprocess_test +     //tensorflow/core:platform_profile_utils_cpu_utils_test +     //tensorflow/core:lib_jpeg_jpeg_mem_unittest +     //tensorflow/core/debug:debug_io_utils_test '\r\n++ extra_failing_gpu_cc_tests='    //tensorflow/core:lib_core_threadpool_test +     //tensorflow/core:cuda_libdevice_path_test +     //tensorflow/core:common_runtime_direct_session_test +     //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test +     //tensorflow/core:device_tracer_test +     //tensorflow/core:ops_math_grad_test '\r\n++ exclude_cpu_cc_tests='    //tensorflow/core/kernels:control_flow_ops_test +     //tensorflow/core:example_example_parser_configuration_test +     //tensorflow/core:lib_core_status_test +     //tensorflow/core:lib_monitoring_collection_registry_test +     //tensorflow/core:lib_strings_numbers_test +     //tensorflow/core/platform/hadoop:hadoop_file_system_test +     //tensorflow/core:platform_file_system_test +     //tensorflow/core:platform_logging_test +     //tensorflow/core:util_sparse_sparse_tensor_test +     //tensorflow/cc:framework_gradient_checker_test +     //tensorflow/cc:framework_gradients_test +     //tensorflow/cc:gradients_array_grad_test +     //tensorflow/cc:gradients_math_grad_test +     //tensorflow/cc:gradients_nn_grad_test +     //tensorflow/cc/saved_model:loader_test  +     //tensorflow/cc:framework_cc_ops_test +     //tensorflow/core/platform/cloud:time_util_test +     //tensorflow/core/platform/cloud:oauth_client_test +     //tensorflow/core/platform/cloud:http_request_test +     //tensorflow/core/platform/cloud:google_auth_provider_test +     //tensorflow/core/platform/cloud:gcs_file_system_test +     //tensorflow/core/kernels/cloud:bigquery_table_accessor_test +     //tensorflow/core/kernels/hexagon:graph_transferer_test +     //tensorflow/core/kernels:remote_fused_graph_execute_utils_test +     //tensorflow/core/kernels:requantize_op_test +     //tensorflow/core/kernels:requantization_range_op_test +     //tensorflow/core/kernels:quantized_reshape_op_test +     //tensorflow/core/kernels:quantized_pooling_ops_test +     //tensorflow/core/kernels:quantized_matmul_op_test +     //tensorflow/core/kernels:quantized_conv_ops_test +     //tensorflow/core/kernels:quantized_concat_op_test +     //tensorflow/core/kernels:quantized_bias_add_op_test +     //tensorflow/core/kernels:quantized_batch_norm_op_test +     //tensorflow/core/kernels:quantized_activation_ops_test +     //tensorflow/core/kernels:quantize_op_test +     //tensorflow/core/kernels:quantize_down_and_shrink_range_op_test +     //tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu +     //tensorflow/core/kernels:quantize_and_dequantize_op_test +     //tensorflow/core/kernels:quantization_utils_test +     //tensorflow/core/kernels:debug_ops_test +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test +     //tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding_test +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test +     //tensorflow/core/distributed_runtime:remote_device_test_gpu +     //tensorflow/core/distributed_runtime:remote_device_test +     //tensorflow/core/distributed_runtime:executor_test_gpu +     //tensorflow/core/distributed_runtime:executor_test +     //tensorflow/core/debug:debug_gateway_test +     //tensorflow/core/debug:debug_grpc_io_utils_test +     //tensorflow/core:util_reporter_test +     //tensorflow/core:util_memmapped_file_system_test +     //tensorflow/core:platform_subprocess_test +     //tensorflow/core:platform_profile_utils_cpu_utils_test +     //tensorflow/core:lib_jpeg_jpeg_mem_unittest +     //tensorflow/core/debug:debug_io_utils_test '\r\n++ exclude_gpu_cc_tests='    //tensorflow/core:lib_core_threadpool_test +     //tensorflow/core:cuda_libdevice_path_test +     //tensorflow/core:common_runtime_direct_session_test +     //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test +     //tensorflow/core:device_tracer_test +     //tensorflow/core:ops_math_grad_test  +     //tensorflow/core/kernels:control_flow_ops_test +     //tensorflow/core:example_example_parser_configuration_test +     //tensorflow/core:lib_core_status_test +     //tensorflow/core:lib_monitoring_collection_registry_test +     //tensorflow/core:lib_strings_numbers_test +     //tensorflow/core/platform/hadoop:hadoop_file_system_test +     //tensorflow/core:platform_file_system_test +     //tensorflow/core:platform_logging_test +     //tensorflow/core:util_sparse_sparse_tensor_test +     //tensorflow/cc:framework_gradient_checker_test +     //tensorflow/cc:framework_gradients_test +     //tensorflow/cc:gradients_array_grad_test +     //tensorflow/cc:gradients_math_grad_test +     //tensorflow/cc:gradients_nn_grad_test +     //tensorflow/cc/saved_model:loader_test  +     //tensorflow/cc:framework_cc_ops_test +     //tensorflow/core/platform/cloud:time_util_test +     //tensorflow/core/platform/cloud:oauth_client_test +     //tensorflow/core/platform/cloud:http_request_test +     //tensorflow/core/platform/cloud:google_auth_provider_test +     //tensorflow/core/platform/cloud:gcs_file_system_test +     //tensorflow/core/kernels/cloud:bigquery_table_accessor_test +     //tensorflow/core/kernels/hexagon:graph_transferer_test +     //tensorflow/core/kernels:remote_fused_graph_execute_utils_test +     //tensorflow/core/kernels:requantize_op_test +     //tensorflow/core/kernels:requantization_range_op_test +     //tensorflow/core/kernels:quantized_reshape_op_test +     //tensorflow/core/kernels:quantized_pooling_ops_test +     //tensorflow/core/kernels:quantized_matmul_op_test +     //tensorflow/core/kernels:quantized_conv_ops_test +     //tensorflow/core/kernels:quantized_concat_op_test +     //tensorflow/core/kernels:quantized_bias_add_op_test +     //tensorflow/core/kernels:quantized_batch_norm_op_test +     //tensorflow/core/kernels:quantized_activation_ops_test +     //tensorflow/core/kernels:quantize_op_test +     //tensorflow/core/kernels:quantize_down_and_shrink_range_op_test +     //tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu +     //tensorflow/core/kernels:quantize_and_dequantize_op_test +     //tensorflow/core/kernels:quantization_utils_test +     //tensorflow/core/kernels:debug_ops_test +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu +     //tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test +     //tensorflow/core/distributed_runtime/rpc:grpc_tensor_coding_test +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_session_test +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu +     //tensorflow/core/distributed_runtime/rpc:grpc_channel_test +     //tensorflow/core/distributed_runtime:remote_device_test_gpu +     //tensorflow/core/distributed_runtime:remote_device_test +     //tensorflow/core/distributed_runtime:executor_test_gpu +     //tensorflow/core/distributed_runtime:executor_test +     //tensorflow/core/debug:debug_gateway_test +     //tensorflow/core/debug:debug_grpc_io_utils_test +     //tensorflow/core:util_reporter_test +     //tensorflow/core:util_memmapped_file_system_test +     //tensorflow/core:platform_subprocess_test +     //tensorflow/core:platform_profile_utils_cpu_utils_test +     //tensorflow/core:lib_jpeg_jpeg_mem_unittest +     //tensorflow/core/debug:debug_io_utils_test '\r\n+ cd /d/projects/tensorflow/tensorflow/tools/ci_build/windows/../../../..\r\n+ '[' '!' -e WORKSPACE ']'\r\n+ export TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so\r\n+ TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so\r\n+ export 'TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate'\r\n+ TF_BAZEL_TARGETS='//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate'\r\n+ export 'TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so'\r\n+ TF_BAZEL_TARGETS='//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so'\r\n+ export 'TF_BAZEL_TARGETS=//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so //tensorflow/tools/lib_package:jnilicenses_generate'\r\n+ TF_BAZEL_TARGETS='//tensorflow:libtensorflow.so //tensorflow/tools/lib_package:clicenses_generate //tensorflow/java:libtensorflow_jni.so //tensorflow/tools/lib_package:jnilicenses_generate'\r\n+ run_configure_for_gpu_build\r\n+ export TF_NEED_CUDA=1\r\n+ TF_NEED_CUDA=1\r\n+ export TF_CUDA_VERSION=9.0\r\n+ TF_CUDA_VERSION=9.0\r\n+ export 'CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'\r\n+ CUDA_TOOLKIT_PATH='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'\r\n+ export TF_CUDNN_VERSION=7.0\r\n+ TF_CUDNN_VERSION=7.0\r\n+ export 'CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'\r\n+ CUDNN_INSTALL_PATH='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0'\r\n+ export TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n+ TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n+ '[' -z '' ']'\r\n+ export TF_ENABLE_XLA=0\r\n+ TF_ENABLE_XLA=0\r\n+ export TF_NEED_VERBS=0\r\n+ TF_NEED_VERBS=0\r\n+ export TF_NEED_MKL=0\r\n+ TF_NEED_MKL=0\r\n+ export TF_NEED_GCP=0\r\n+ TF_NEED_GCP=0\r\n+ export TF_NEED_HDFS=0\r\n+ TF_NEED_HDFS=0\r\n+ export TF_NEED_OPENCL_SYCL=0\r\n+ TF_NEED_OPENCL_SYCL=0\r\n+ export USE_MSVC_WRAPPER=1\r\n+ USE_MSVC_WRAPPER=1\r\n+ echo ''\r\n+ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.10.0 installed.\r\nInvalid python path: C:/ProgramData/Anaconda3/python cannot be found.\r\nPlease specify the location of python. [Default is C:\\ProgramData\\Anaconda3\\python.exe]:\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: No MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n+ bazel build -c opt --copt=/arch:AVX tensorflow:libtensorflow.so tensorflow/tools/lib_package:clicenses_generate tensorflow/java:libtensorflow_jni.so tensorflow/tools/lib_package:jnilicenses_generate\r\n...................\r\nLoading:\r\nLoading: 0 packages loaded\r\nWARNING: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/WORKSPACE:1: Workspace name in C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nDEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nAnalyzing: 4 targets (6 packages loaded)\r\nAnalyzing: 4 targets (42 packages loaded)\r\nAnalyzing: 4 targets (55 packages loaded)\r\nAnalyzing: 4 targets (59 packages loaded)\r\nAnalyzing: 4 targets (66 packages loaded)\r\nWARNING: D:/projects/tensorflow/tensorflow/core/BUILD:1895:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in D:/projects/tensorflow/tensorflow/tensorflow.bzl:1181:30\r\nWARNING: D:/projects/tensorflow/tensorflow/core/BUILD:1895:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in D:/projects/tensorflow/tensorflow/tensorflow.bzl:1181:30\r\nINFO: Analysed 4 targets (70 packages loaded).\r\nINFO: Found 4 targets...\r\nBuilding: no action\r\n[0 / 24] [-----] BazelWorkspaceStatusAction stable-status.txt\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.cc [for host]:\r\n\u2592\u00f7\u2592: cl [ \u0461\u2592\u2592... ] \u2592\u013c\u2592\u2592\u2592... [ /link \u2592\u2592\u2592\u2592\u0461\u2592\u2592... ]\r\n\u2592\u2592\u2592\u2592 x64 \u2592\u2592 Microsoft (R) C/C++ \u2592\u017b\u2592\u2592\u2592\u2592\u2592\u2592\u2592 19.00.24215.1 \u2592\u2592\r\n\u2592\u2592\u0228\u2592\u2592\u2592\u2592(C) Microsoft Corporation\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u0228\u2592\u2592\u2592\u2592\r\n\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_enum.cc [for host]:\r\n\u2592\u00f7\u2592: cl [ \u0461\u2592\u2592... ] \u2592\u013c\u2592\u2592\u2592... [ /link \u2592\u2592\u2592\u2592\u0461\u2592\u2592... ]\r\n\u2592\u2592\u2592\u2592 x64 \u2592\u2592 Microsoft (R) C/C++ \u2592\u017b\u2592\u2592\u2592\u2592\u2592\u2592\u2592 19.00.24215.1 \u2592\u2592\r\n\u2592\u2592\u0228\u2592\u2592\u2592\u2592(C) Microsoft Corporation\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u0228\u2592\u2592\u2592\u2592\r\n\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_doc_comment.o' was not created\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/java/java_message_field_lite.cc [for host]:\r\n\u2592\u00f7\u2592: cl [ \u0461\u2592\u2592... ] \u2592\u013c\u2592\u2592\u2592... [ /link \u2592\u2592\u2592\u2592\u0461\u2592\u2592... ]\r\n\u2592\u2592\u2592\u2592 x64 \u2592\u2592 Microsoft (R) C/C++ \u2592\u017b\u2592\u2592\u2592\u2592\u2592\u2592\u2592 19.00.24215.1 \u2592\u2592\r\n\u2592\u2592\u0228\u2592\u2592\u2592\u2592(C) Microsoft Corporation\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u0228\u2592\u2592\u2592\u2592\r\n\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: not all outputs were created or valid\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_enum.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/php/php_generator.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_message_field_lite.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/javanano/javanano_primitive_field.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/java/java_lazy_message_field.o' was not created\r\nERROR: C:/tmp/_bazel_wangxiaoming/g7crfchu/external/protobuf_archive/BUILD:277:1: output 'external/protobuf_archive/_objs/protoc_lib/external/protobuf_archive/src/google/protobuf/compiler/cpp/cpp_string_field.o' was not created\r\nINFO: Elapsed time: 11.237s, Critical Path: 0.46s\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["\r\nClean up the build file and restart the build successfully", "@wangxm345566462 Can you share the compiled jni files?"]}, {"number": 19166, "title": "densenet.pb is invalid protobuf", "body": "`densenet.pb` in [densenet_2018_04_27.tgz](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz) linked in [models.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md) seems to be not a `GraphDef`?\r\n\r\n```\r\nfrom tensorflow.core.framework import graph_pb2\r\nfrom google.protobuf import text_format\r\n\r\nwith open('densenet.pb', 'rb') as input_file:\r\n    graph_def = graph_pb2.GraphDef()\r\n    graph_def.ParseFromString(input_file.read())\r\n```\r\n\r\n```\r\n    graph_def.ParseFromString(input_file.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "None of those are required to diagnose the problem. Code to repro is included.", "Nagging Assignee @bignamehyp: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19165, "title": "Switch to use str instead of number for colab_url", "body": "Fix nightly failure:\r\n  File \"tensorflow/tools/ci_build/update_version.py\", line 253, in colab_url\r\n    version_string = \"%d.%d.%d\" % (version.major, version.minor, version.patch)\r\nTypeError: %d format: a number is required, not str", "comments": ["I believe @case540 fixed this internally as well."]}, {"number": 19164, "title": "Branch 195897321", "body": "", "comments": []}, {"number": 19163, "title": "Memory leak in tensorflow newsession c++ api", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**:0.12\r\n- **GCC/Compiler version (if compiling from source)**:7.3\r\n- **CUDA/cuDNN version**: 9.1.85,7.1.3\r\n- **GPU model and memory**: Titan x geforce\r\n- **Exact command to reproduce**:\r\n\r\nCode:\r\n```\r\nclass Temp\r\n{\r\n public:\r\n  Temp(){\r\nstd::unique_ptr<tensorflow::Session> session(\r\n      tensorflow::NewSession(tensorflow::SessionOptions()));\r\n};\r\n};\r\n\r\nint main(int argc, char **argv)\r\n{\r\n  Temp temp;\r\n  return 0;\r\n}\r\n```\r\n\r\n### Describe the problem\r\nI am just trying to create tensorflow session and getting memory leak.\r\n\r\n### Source code / logs: (Valgrind)\r\n```\r\n==13704== 4,391 (24 direct, 4,367 indirect) bytes in 1 blocks are definitely lost in loss record 87,919 of 88,141\r\n==13704==    at 0x4C30216: operator new(unsigned long) (vg_replace_malloc.c:334)\r\n==13704==    by 0x73D47A7: tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0x6CD32C1: tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0xBAA034C: tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0xB93A246: tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0xBAED531: tensorflow::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0x4E5E1D5: Temp::Temp() (temp.cpp:7)\r\n```\r\n\r\n\r\n```\r\n4,388 (24 direct, 4,364 indirect) bytes in 1 blocks are definitely lost in loss record 87,918 of 88,141\r\n==13704==    at 0x4C30216: operator new(unsigned long) (vg_replace_malloc.c:334)\r\n==13704==    by 0x73D47A7: tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0x73CF019: tensorflow::XlaGpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0xBAA034C: tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0xB93A246: tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0xBAED531: tensorflow::NewSession(tensorflow::SessionOptions const&) (in /usr/lib/tensorflow_cc/libtensorflow_cc.so)\r\n==13704==    by 0x4E5E1D5: Temp::Temp() (temp.cpp:7)\r\n```\r\n", "comments": ["@cy89 Any update on this?", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@achalshah20 can you please try with a more recent version of TF, and see if you still have the issue?", "Yes. It got fixed in latest TF version. Closing the issue. Thanks!", "@achalshah20, which tf version you used, I still catch the same leak point when use tf 1.13.1. Thanks!"]}, {"number": 19162, "title": "Documentation of raw_rnn is wrong", "body": "In the [official documentation][1] of tf.nn.raw_rnn we have emit structure as the third output of loop_fn when the loop_fn is run for the first time\r\n\r\nlater on the emit_structure is used to copy tf.zeros_like(emit_structure) to the minibatch entries that are finished by `emit = tf.where(finished, tf.zeros_like(emit_structure), emit)`\r\n\r\nmy lack of understanding is: emit structure is `None` so `tf.where(finished, tf.zeros_like(emit_structure), emit)` is going to throw a ValueError as `tf.zeros_like(None)` does so. Can somebody please fill in what i am missing here?\r\n\r\nbasically even the example implementation of dynamic_rnn using raw_rnn is simple wrong\r\nto quote exact parts:\r\nin dynamic_rnn implementation shown\r\n```\r\ndef loop_fn(time, cell_output, cell_state, loop_state):\r\n  emit_output = cell_output  # == None for time == 0\r\n```\r\nand in raw_rnn we have:\r\n```\r\ntime = tf.constant(0, dtype=tf.int32)\r\n(finished, next_input, initial_state, emit_structure, loop_state) = loop_fn(\r\n    time=time, cell_output=None, cell_state=None, loop_state=None)\r\n.\r\n.\r\n.\r\n# Emit zeros and copy forward state for minibatch entries that are finished.\r\n  state = tf.where(finished, state, next_state)\r\n  emit = tf.where(finished, tf.zeros_like(emit_structure), emit)\r\n```\r\n\r\nThis will fail at the first time step itself. But i have gone through code, this is not how it is implemented. In case of None for emit_structure  cell_output is used.\r\n\r\nraw_rnn is really powerful and once i have understood it there is no going back to dynamic_rnn or any other rnn unfolding mechanism. But the documentation is making is really tough to understand this amazing api. An improvement is in order\r\n\r\nHave I written custom code N/A\r\nOS Platform and Distribution windows 10\r\nTensorFlow installed from https://www.tensorflow.org/install/install_windows\r\nTensorFlow version 1.7\r\nBazel version NA\r\nCUDA/cuDNN version NA\r\nGPU model and memory NA\r\nExact command to reproduce tf.zeros_like(None)\r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OK, are you able to write a better documentation?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'll take a closer look next Wednesday and see what I can do!", "This API is removed in TF2."]}, {"number": 19161, "title": "Added gradients for SparseReduceMax/Min", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it !", "CLAs look good, thanks!\n\n<!-- ok -->", "@prafiny please fix the linter errors.", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It is still valid, It just needs to pass some linting", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 19160, "title": "Add tf.regex_match for regex match support", "body": "This fix tries to address the issue raised in #18264.\r\n\r\nCurrently tf.regex_replace has already been supported though there was no regex match support.\r\nThis fix adds the tf.regex_match support in a similiar pattern as tf.regex_replace.\r\n\r\nThis fix fixes #18264.\r\n\r\nNote: this PR is a preliminary implementation which support full match of the regex. Additional variations could be added or in follow up PRs if needed.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @asimshankar @rmlarsen for the review. The PR has been updated with op exposed as `tf.strings.regex_full_match`. Please take a look and let me know if there are any issues.", "@asimshankar handing to you for API approval.", "Good for API addition. Thanks!", "Thanks!"]}, {"number": 19159, "title": "[Feature Request] Implementation of cosine_distance_with_negative_samples loss", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.5.0\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:No\r\n- **GCC/Compiler version (if compiling from source)**:No\r\n- **CUDA/cuDNN version**:CPU\r\n- **GPU model and memory**:CPU\r\n- **Exact command to reproduce**:Feature request\r\n\r\n### Describe the problem\r\nTensorflow current does not have [CNTK's negative sampling loss](https://www.cntk.ai/pythondocs/cntk.losses.html#cntk.losses.cosine_distance_with_negative_samples). I tried to get the same functionality using tf.nn.sampled_softmax_loss, but the weights parameter of the function needs to be variable, instead of an op[If weights is an arbitrary op, I get sparse gradient warning].\r\n\r\nIs it possible to implement this loss natively without doing an embedding lookup on weights?", "comments": ["@drpngx do you know whether this is possible (efficiently)?", "As I understand it, it looks similar to the sampled softmax, except that the negative samples are sampled deterministically using a stride operation.\r\n\r\n@iamsimha can you confirm?", "The sampled_softmax_loss is different from CNTKs negative sampling loss in two ways.\r\n\r\n1. The negative samples are generated by probabilistic sampling in TF, instead of deterministic strides.\r\n\r\n2. The weights parameter to sampled_softmax_loss can only be a tf.variable instead of an arbitrary op. This is because sampled_softmax_loss internally does a nn.embedding_lookup on weights parameter. According to my understanding embedding_lookup is efficient, only if the input is a variable or an op with sparse gradient support. If the input parameter is an arbitrary op, TF raises a sparse gradient warning.  So is it possible to support arbitrary ops for the weights parameter by removing dependency on embedding_lookup?", "Re 2: The warning is just a warning -- it means that whatever you're doing on the way from the embedding lookup to the underlying variables might requires a dense gradient. If that is the case, then that may be inefficient. It does not mean that it is necessarily inefficient. You can check the graph to see whether in your case you actually instantiate a large dense gradient. We're open to changing the warning to be clearer.\r\n\r\nRe 1: We would prefer not to add a new function for this. If this is a critical use case (I am not sure it is, would deterministic, but pseudorandom sampling be enough?) then we should extend the interface of the existing method to support it."]}, {"number": 19158, "title": "TFLite memory alignment errors with some armv7 Android devices", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Built on\r\n  Mac OS X 10.13.4 and `Linux Ubuntu 17.10`, Android NDK revision 16.1.4479499,\r\n  targeting various Android devices with TFLite.\r\n- **TensorFlow installed from (source or binary)**:\r\n  Binary\r\n- **TensorFlow version (use command below)**:\r\n  1.7.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See [my repo, `tsob/TFLite_bug_test`](https://github.com/tsob/TFLite_bug_test) for a demonstration of this issue. Clone the repo, open in Android Studio, and run the tests.\r\n\r\n### Describe the problem\r\n\r\nWhen using TFLite to run inference on a saved `.tflite` model, we see a memory\r\nalignment error on certain armv7 devices (_e.g._ Galaxy S3, Galaxy Nexus) which\r\ncauses crashes pretty consistently. The same app runs fine on armv8, apparently,\r\nas well as _some_ armv7 devices (Galaxy S4).\r\n\r\nFor example, on Galaxy S3 we see:\r\n\r\n```\r\nD/CrashAnrDetector(  351): Build: samsung/d2uc/d2att:4.3/JSS15J/I747UCUEMJB:user/release-keys\r\nD/CrashAnrDetector(  351): Hardware: MSM8960\r\nD/CrashAnrDetector(  351): Revision: 16\r\nD/CrashAnrDetector(  351): Bootloader: I747UCUEMJB\r\nD/CrashAnrDetector(  351): Radio: unknown\r\nD/CrashAnrDetector(  351): Kernel: Linux version 3.0.31-2024954 (se.infra@R0210-11) (gcc version 4.7 (GCC) ) #1 SMP PREEMPT Thu Oct 31 01:05:53 KST 2013\r\nD/CrashAnrDetector(  351):\r\nD/CrashAnrDetector(  351): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\nD/CrashAnrDetector(  351): Build fingerprint: 'samsung/d2uc/d2att:4.3/JSS15J/I747UCUEMJB:user/release-keys'\r\nD/CrashAnrDetector(  351): Revision: '16'\r\nD/CrashAnrDetector(  351): pid: 6048, tid: 6067, name: roidJUnitRunner  >>> com.example.tsob.tflite_bug_test <<<\r\nD/CrashAnrDetector(  351): signal 7 (SIGBUS), code 128 (SI_KERNEL), fault addr 00000000\r\nD/CrashAnrDetector(  351):     r0 5f7d6bd8  r1 00000004  r2 5f7f0250  r3 00000080\r\nD/CrashAnrDetector(  351):     r4 5f6ab6e4  r5 00000000  r6 5f7d6bd8  r7 5f540888\r\nD/CrashAnrDetector(  351):     r8 00000001  r9 5f7f0240  sl 00000000  fp 5f7f0040\r\nD/CrashAnrDetector(  351):     ip 5f7f0040  sp 5f540830  lr 00000080  pc 5f743394  cpsr 00000030\r\nD/CrashAnrDetector(  351):     d0  0000000000000000  d1  4082819d00000000\r\nD/CrashAnrDetector(  351):     d2  000000004147b36c  d3  0000000041476116\r\nD/CrashAnrDetector(  351):     d4  4113563800000000  d5  4114b6d100000000\r\nD/CrashAnrDetector(  351):     d6  0000000000000000  d7  000000003fe5492c\r\nD/CrashAnrDetector(  351):     d8  0000000000000000  d9  0000000000000000\r\nD/CrashAnrDetector(  351):     d10 0000000000000000  d11 0000000000000000\r\nD/CrashAnrDetector(  351):     d12 0000000000000000  d13 0000000000000000\r\nD/CrashAnrDetector(  351):     d14 0000000000000000  d15 0000000000000000\r\nD/CrashAnrDetector(  351):     d16 0000000000000000  d17 0000000000000000\r\nD/CrashAnrDetector(  351):     d18 c0e0bc0ec1100562  d19 c167ff6a40ab03aa\r\nD/CrashAnrDetector(  351):     d20 c07bb5aac01af9fd  d21 c0e3bae1c0916f36\r\nD/CrashAnrDetector(  351):     d22 40bb1fa74085e307  d23 41476116c139dbc3\r\nD/CrashAnrDetector(  351):     d24 3fc74721cad6b0ed  d25 3fc2f112df3e5244\r\nD/CrashAnrDetector(  351):     d26 40026bb1bbb55516  d27 4000000000000000\r\nD/CrashAnrDetector(  351):     d28 40008df2d49d41f1  d29 3fb0f4a31edab38b\r\nD/CrashAnrDetector(  351):     d30 3ff0000000000000  d31 3f4de16b9c24a98f\r\nD/CrashAnrDetector(  351):     scr 80000010\r\nD/CrashAnrDetector(  351):\r\n```\r\n\r\nOn Galaxy Nexus, we have:\r\n\r\n```\r\nF/libc    ( 4638): Fatal signal 7 (SIGBUS) at 0x00000000 (code=128), thread 4651 (roidJUnitRunner)\r\nI/DEBUG   (  124): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\nI/DEBUG   (  124): Build fingerprint: 'google/mysid/toro:4.2.2/JDQ39/573038:user/release-keys'\r\nI/DEBUG   (  124): Revision: '10'\r\nI/DEBUG   (  124): pid: 4638, tid: 4651, name: roidJUnitRunner  >>> com.example.tsob.tflite_bug_test <<<\r\nI/DEBUG   (  124): signal 7 (SIGBUS), code 128 (?), fault addr 00000000\r\nI/DEBUG   (  124):     r0 5d2c6f88  r1 00000004  r2 5a01c250  r3 00000080\r\nI/DEBUG   (  124):     r4 59e866d4  r5 00000000  r6 5d2c6f88  r7 5e61d990\r\nI/DEBUG   (  124):     r8 00000001  r9 5a01c240  sl 00000000  fp 5a01c040\r\nI/DEBUG   (  124):     ip 5a01c040  sp 5e61d938  lr 00000080  pc 5c97f394  cpsr 00000030\r\nI/DEBUG   (  124):     d0  6620746f00000000  d1  732e736b726f7775\r\nI/DEBUG   (  124):     d2  7262696c203a296e  d3  62696c2220797261\r\nI/DEBUG   (  124):     d4  c07bb5aac01af9fd  d5  c0e3bae1c0916f36\r\nI/DEBUG   (  124):     d6  40bb1fa74085e307  d7  41476116c139dbc3\r\nI/DEBUG   (  124):     d8  0000000000000000  d9  0000000000000000\r\nI/DEBUG   (  124):     d10 0000000000000000  d11 0000000000000000\r\nI/DEBUG   (  124):     d12 0000000000000000  d13 0000000000000000\r\nI/DEBUG   (  124):     d14 0000000000000000  d15 0000000000000000\r\nI/DEBUG   (  124):     d16 0000000000000000  d17 0000000000000000\r\nI/DEBUG   (  124):     d18 ff7fffffff7fffff  d19 ff7fffffff7fffff\r\nI/DEBUG   (  124):     d20 41007c5e41512bac  d21 0000000040a5953d\r\nI/DEBUG   (  124):     d22 41007c5e41512bac  d23 0000000040a5953d\r\nI/DEBUG   (  124):     d24 0000000000000000  d25 0000000000000000\r\nI/DEBUG   (  124):     d26 0000002f0000002f  d27 0000002f0000002f\r\nI/DEBUG   (  124):     d28 0000000000000005  d29 0001000000010000\r\nI/DEBUG   (  124):     d30 000000000000000a  d31 000000000000000e\r\nI/DEBUG   (  124):     scr 80000090\r\nI/DEBUG   (  124):\r\n```\r\n\r\n\r\n### Source code / logs\r\nSee [`tsob/TFLite_bug_test`](https://github.com/tsob/TFLite_bug_test) for relevant info, as well as the code to reproduce this issue.", "comments": ["Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @aselle, @andrehentz: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have similar problem when trying to run TensorFlow Lite model. I've tested:\r\n- my own model\r\n- inception_v3_slim_2016_android_2017_11_10.tflite from sample.\r\n\r\nI use following version of TFLite in build.gradle: `org.tensorflow:tensorflow-android:1.8.0`\r\n\r\nWhen I try to run model on Nexus 4 (Mako) I get following error:\r\n\r\n```\r\nA/libc: Fatal signal 7 (SIGBUS), code 1, fault addr 0xb8ad0198 in tid 18379 (d.example.dev)\r\nA/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\nA/DEBUG: Build fingerprint: 'google/occam/mako:6.0.1/MOB30Y/3067468:user/release-keys'\r\nA/DEBUG: Revision: '0'\r\nA/DEBUG: ABI: 'arm'\r\nA/DEBUG: pid: 18379, tid: 18379, name: d.example.dev  >>> com.example.dev <<<\r\nA/DEBUG: signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xb8ad0198\r\nA/DEBUG:     r0 b8ad0198  r1 00000004  r2 b8a9b410  r3 00000032\r\nA/DEBUG:     r4 a11fb7dc  r5 00000006  r6 b8ad0198  r7 beea3220\r\nA/DEBUG:     r8 00000006  r9 b8a9b400  sl 00000000  fp b8a9b4c8\r\nA/DEBUG:     ip b8a9b4e0  sp beea31c8  lr 00000030  pc a04c32dc  cpsr 000f0030\r\nA/DEBUG: backtrace:\r\nA/DEBUG:     #00 pc 0008c2dc  /data/app/com.example.dev-1/lib/arm/libtensorflowlite_jni.so\r\nA/DEBUG:     #01 pc 000564fb  /data/app/com.example.dev-1/lib/arm/libtensorflowlite_jni.so\r\nA/DEBUG:     #02 pc 0005672d  /data/app/com.example.dev-1/lib/arm/libtensorflowlite_jni.so\r\nA/DEBUG:     #03 pc 0007b219  /data/app/com.example.dev-1/lib/arm/libtensorflowlite_jni.so\r\nA/DEBUG:     #04 pc 00007eb1  /data/app/com.example.dev-1/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1332)\r\nA/DEBUG:     #05 pc 000eaa99  /system/lib/libart.so (art_quick_generic_jni_trampoline+40)\r\nA/DEBUG:     #06 pc 000e63a1  /system/lib/libart.so (art_quick_invoke_stub_internal+64)\r\nA/DEBUG:     #07 pc 004025bb  /system/lib/libart.so (art_quick_invoke_static_stub+170)\r\nA/DEBUG:     #08 pc 007fc4ec  [stack]\r\nA/DEBUG: Tombstone written to: /data/tombstones/tombstone_03\r\nE/DEBUG: AM write failed: Broken pipe\r\n```\r\n\r\n\r\n\r\nOn Samsung Galaxy Alpha model runs successfully. Both devices has ARMv7 processors.", "@tsob thank you for the repro app. We fixed an alignment issue recently on May 14: https://github.com/tensorflow/tensorflow/commit/88103d000add4ea7f8d1a34ee3c898fc79d9e3c7\r\n\r\nI am going to verify if the issue reported by App is also fixed.", "Thanks @shashishekhar, I will also try to verify if it is fixed.", "I ran it on Nexus 5 and couldn't reproduce the issue, please reopen if you can still reproduce it.", "Unfortunately the problem still exists, I ran it on Nexus 4 (6.0.1) and same issue occurs.\r\n@shashishekhar Could you please reopen?\r\n\r\nCrash comes when following line is executing:\r\n`tflite.run(input, output);`\r\n\r\nI tried updated version of TFLite: org.tensorflow:tensorflow-android:1.10.0-rc0\r\n\r\nPlease take a look at the logcat:\r\n\r\n```\r\nFatal signal 7 (SIGBUS), code 1, fault addr 0xb880c1f8 in tid 24238 (Thread-2923)\r\nBuild fingerprint: 'google/occam/mako:6.0.1/MOB30Y/3067468:user/release-keys'\r\n    Revision: '0'\r\n    ABI: 'arm'\r\n    pid: 25959, tid: 26491, name: Thread-1257  >>> com.example.dev <<<\r\n    signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xb812abb8\r\nA/DEBUG:     r0 b812abb8  r1 00000004  r2 b84a1d98  r3 00000032\r\n        r4 a0d13220  r5 00000006  r6 b812abb8  r7 9f61b040\r\n        r8 00000006  r9 b84a1d88  sl 00000000  fp b84a0a00\r\n        ip b84a0a18  sp 9f61afe8  lr 00000030  pc a050e2dc  cpsr 000f0030\r\nA/DEBUG: backtrace:\r\n        #00 pc 0008c2dc  /data/app/com.example.dev-2/lib/arm/libtensorflowlite_jni.so\r\nA/DEBUG:     #01 pc 000564fb  /data/app/com.example.dev-2/lib/arm/libtensorflowlite_jni.so\r\n        #02 pc 0005672d  /data/app/com.example.dev-2/lib/arm/libtensorflowlite_jni.so\r\n        #03 pc 0007b219  /data/app/com.example.dev-2/lib/arm/libtensorflowlite_jni.so\r\n        #04 pc 00007eb1  /data/app/com.example.dev-2/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1332)\r\n        #05 pc 000eaa99  /system/lib/libart.so (art_quick_generic_jni_trampoline+40)\r\n        #06 pc 000e63a1  /system/lib/libart.so (art_quick_invoke_stub_internal+64)\r\n        #07 pc 004025bb  /system/lib/libart.so (art_quick_invoke_static_stub+170)\r\n        #08 pc 0010130c  [stack:26491]\r\nW/debuggerd: type=1400 audit(0.0:1986): avc: denied { search } for name=\"com.example.dev\" dev=\"mmcblk0p23\" ino=635258 scontext=u:r:debuggerd:s0 tcontext=u:object_r:app_data_file:s0:c512,c768 tclass=dir permissive=0\r\nD/NetlinkSocketObserver: NeighborEvent{elapsedMs=17051864, 192.168.137.1, [B6B676DBFD9E], RTM_NEWNEIGH, NUD_REACHABLE}\r\nW/debuggerd: type=1400 audit(0.0:1987): avc: denied { read } for name=\"kgsl-3d0\" dev=\"tmpfs\" ino=8653 scontext=u:r:debuggerd:s0 tcontext=u:object_r:gpu_device:s0 tclass=chr_file permissive=0\r\n    type=1400 audit(0.0:1988): avc: denied { read } for name=\"kgsl-3d0\" dev=\"tmpfs\" ino=8653 scontext=u:r:debuggerd:s0 tcontext=u:object_r:gpu_device:s0 tclass=chr_file permissive=0\r\nW/ActivityManager: Process com.example.dev has crashed too many times: killing!\r\n      Force finishing activity com.example.dev/com.example.dev.TestActivity\r\nA/DEBUG: Tombstone written to: /data/tombstones/tombstone_01\r\nE/DEBUG: AM write failed: Broken pipe\r\n```", "@spike07 : Can you try with 'org.tensorflow:tensorflow-lite:0.0.0-nightly'", "@shashishekhar Just tried the 'org.tensorflow:tensorflow-lite:0.0.0-nightly' - still crashing with the same info in logcat on Nexus 4 with Qualcomm Snapdragon S4 Pro (Krait)\r\n\r\nHowever, both `org.tensorflow:tensorflow-lite:0.0.0-nightly` and `org.tensorflow:tensorflow-android:1.10.0-rc0` works fine on Samsung Galaxy Alpha (SM-G850F) with Samsung Exynos Octa 5430 processor (4x ARM Cortex A7 + 4x ARM Cortex A15).\r\n\r\nThe previous version of TFLite which I tested (`org.tensorflow:tensorflow-android:1.8.0`) crashed both on Samsung Galaxy Alpha and Nexus 4. So changes from version 1.8.0 to 1.10.0 helped fixing the problem on Samsung device.", "I've just tried `org.tensorflow:tensorflow-lite:1.9.0` and it works fine on Nexus 4 \ud83d\udc4d \r\n@shashishekhar Thank you for your fast reply.\r\n\r\nHowever, keep in mind that I've tested following versions and all of them crashed on Nexus 4:\r\n\r\n`org.tensorflow:tensorflow-lite:0.0.0-nightly` \r\n`org.tensorflow:tensorflow-android:1.8.0`\r\n`org.tensorflow:tensorflow-android:1.9.0-rc0`\r\n`org.tensorflow:tensorflow-android:1.10.0-rc1`"]}, {"number": 19157, "title": "Estimator Head design document", "body": "RFC document for design review pilot: Estimator Head API design.\r\n\r\n(*Not to be merged into tensorflow/ - will be merged into new community repo.)\r\n\r\nPlease leave design feedback in this PR. Comment period will be open until 2018-05-22.\r\n\r\ncc @ispirmustafa @roumposg\r\n\r\n## Summary\r\n\r\n| Status        | Proposed                                          |\r\n:-------------- | :-------------------------------------------------|\r\n| **Author(s)** | George Roumpos (Google), TensorFlow team          |\r\n| **Sponsor**   | Mustafa Ispir (Google)                            |\r\n| **Updated**   | 2018-05-01                                        |\r\n\r\nIn this doc we discuss the Head API, which helps users define customizable\r\nmodels that follow the `tf.estimator` API. The API\r\nincludes:\r\n\r\n*   A `Head` interface.\r\n*   Factory methods to create common heads, such as regression head.\r\n*   A `multi_head` method that combines more than one heads for multi-objective\r\n    learning.\r\n*   Canned estimators that can use those heads.\r\n\r\nThe API is already exposed under `tf.contrib.estimator`. The code is in\r\n`tensorflow/contrib/estimator/python/estimator/head.py`,\r\n`tensorflow/contrib/estimator/python/estimator/multi_head.py` and\r\n`tensorflow/python/estimator/canned/head.py`, and an earlier (deprecated)\r\nversion is exposed under `tf.contrib.learn`. The goal of this design doc is to\r\ngraduate the API from contrib to core.\r\n", "comments": ["Thank you very much for creating this process and being open to community feedback! :)\r\n\r\nI like the head API proposal but I would like to add something to it, based on my experience of trying to create a high-level learning API on TensorFlow Scala.\r\n\r\nWe usually want different inputs while training and while performing inference (e.g., target labels in supervised learning are only provided while training). However, we also might want to produce different outputs while training and while performing inference. For example, in a machine translation task, I might want to provide a single distribution over tokens for each time point while training, but a batch of complete sequence predictions along with their scores, while performing inference. I have implemented an interface that supports those use cases too in my API, but I'm not very happy with it and so I would be interested in adopting the head API too. However, I think that this use case is not currently supported here. Is that true? If so, would you be open to a discussion about adding support for this use case?\r\n\r\nNote that these different heads that I am describing, also require different inputs to be fed to the model. In the current design proposal all heads are provided the same inputs, right?\r\n\r\nOut of curiosity, was this API inspired from the Tensor2Tensor library?\r\n\r\nThanks,\r\nAnthony", "Hi @eaplatanios,\r\nUsing some features only in training but not in inference is supported. `create_estimator_spec` has `mode` as arguments. Based on `mode`, head developer can choose which part of the features they want to use. Is that what you're asking?\r\n\r\nabout the history: Head API was first started when we were designing/prototyping the tf.contrib.learn API. This one is the last version which is fine tuned based on our experience.", "Hi @eaplatanios,\r\nAs @ispirmustafa said, the API supports different inputs for training and infrerence, as well as different outputs. E.g. during training the output is typically the loss, whereas for inference we can return e.g. probabilities for all classes, or top-k classes.\r\n\r\nBut you mention the case of sequence models, such as translation. We did not touch this use case yet. Do you think the API is compatible with those models? If not, we are interested about what are the specific blockers.", "The Head API looks very nice for reducing boilerplate when writing custom model functions. \r\nMy two observations:\r\n1) There does not currently seem to be a way to use different loss weights for different labels (e.g. if you have a very imbalanced dataset. I am not sure how well that would fit into the `LossSpec` interface.\r\n2) Suppose I wanted to implement a general `AdversarialTrainingHead` that takes an existing Head as input but modifies the loss function to also include the loss for an adversarial modification of the input, so its loss function might look something like \r\n```\r\nloss = original_head.create_loss(logits, labels)\r\nadv = make_adversarial_perturbation()\r\nadv_loss = original_head.create_loss(logit_fn(adv), labels)\r\nreturn loss + adv_loss\r\n```\r\nThis would be very difficult to implement in the current setting because even though the design doc mentions a `logit_fn` the `create_estimator_spec` actually takes a pre-existing Tensor. \r\nFor many use cases this seems like the more convenient choice, but maybe it would make sense to allow passing in an actual `logit_fn` together with `features`.", "Will there be a way for explicitly targeting a particular device?\r\n\r\nFor instance:  if in the `logits_fn` I explicitly set the device target (`tf.device()`), then will any automatically generated loss and training networks follow that target?\r\n\r\n", "Hi @ngc92,\r\n1. The head factory methods we provide offer a weight_column argument. You can then pass the weights in the features dict, and have a different weight per example. Would this cover your use case?\r\n2. For adversarial training, we recommend multi_head. One head will have the loss related to your objective function, the second head will have the adversarial loss. Multi_head will sum the two losses. Is this what you are looking for?\r\n\r\nHi @DavidNorman,\r\nI think the following will work:\r\n\r\n```python\r\nwith tf.device(...):\r\n  logits = my_logit_fn()\r\n  estimator_spec = my_head.create_estimator_spec()\r\n```", "@roumposg \r\n1. It will work if there is only one class per example (i.e. multi_class_head) but in multi_label_head one example could belong to multiple classes, so these two weighting concepts become orthogonal. If the weight_column could have shape `BATCH_SIZE, NUM_CLASSES` one could do this more fine-grained weighting, but this would make `LossSpec`'s `unreduced_loss` and `weights` also be of shape `BATCH_SIZE, NUM_CLASSES`. \r\nOf course the user could also provide a custom loss function that does the weighting as desired, so this point is not about what is possible with the API but about what the predefined Heads support.", "@ngc92 \r\nYes, if you have 2D weight matrix, the recommended way is to define your custom `loss_fn`. The reason we did that is to avoid surprises related to the `loss_reduction` argument. E.g. some users may expect to sum over the batch but average over the classes, and it was not clear to us how to support all use cases in a clean way.\r\n\r\nThanks for the feedback. I see that this is not well explained, we will explain in better in a future version of the document.", "@ewilderj Can we add a special label for design review/RFC issues/PRs like this one?", "How this will interact with the `tf.keras.estimator.model_to_estimator` use case?", "@bhack it does not interact with `tf.keras.estimator.model_to_estimator`. But it offers another way to write a `model_fn` and an estimator.", "@roumposg\r\nOk, so I've spent some time thinking about how to implement an adversarial head with the current design (so that you could then do a MultiHead to combine it with your regular training).\r\n\r\n1. A minimal way of doing adversarial training would be to just add the adversarial loss to your `Head` as an additional regularization loss. However you probably also would want additional eval statistics and eval metrics (i.e. the accuracy on adversarial data) which you would have to add by hand.\r\n\r\n2. Using the same head twice. An adversarial training `model_fn` with the current design could look like this:\r\n```\r\nlogits = logit_fn(features)\r\nadv_f = make_adversarial(logits, features)\r\nadv_l = logit_fn(adv_f)\r\nbase_head = MyHead()\r\nadv_head = MyHead(name=\"adversarial\")\r\nhead = MultiHead(base_head, adv_head)\r\nreturn head.make_estimator_spec(..., logits={\"head\": logits, \"adversarial\": adv_l})\r\n```\r\n\r\n3. The documentation mentions a split of a model_fn into a head and a logit_fn, which does not seem to be really reflected in the current design. If `make_estimator_spec` took an actual `logit_fn` instead of a logit tensor, one could write a head like\r\n```\r\nclass FGSM_AdvHead(Head):\r\n  def __init__(self, base_head):\r\n     ....\r\n  def make_estimator_spec(features, logit_fn, ...):\r\n    logits = self.logit_fn(features)\r\n    adv_f = self.make_adversarial(logits, features)\r\n    adv_l = self.logit_fn(adv_f)\r\n    head = MultiHead(self.base_head(name=\"base\"), self.base_head(name=\"adv\"))\r\n    return head.make_estimator_spec(..., logits={\"base\": logits, \"adv\": adv_l})\r\n```\r\nwhere a real implementation would probably do something smarter/more specialized than constructing a temporary MultiHead.\r\n\r\nThis last option is currently no possible, right?\r\n\r\nSo how could this be fixed?\r\n1) Simply state that this is out of scope. Note that this disables to write any head that wants to control the context in which the `logit_fn` is called, or wants to call `logit_fn` multiple times. The most direct example I could come up with is the AdversarialHead above, but one might also be interested to call `logit_fn` for parts of the current batch on different devices, and maybe seq2seq models might also benifit from receiving their input in a functional form instead of a tensors (I haven't really thought about this last point, though). In this case I would rewrite the following paragraph in the documentation:\r\n\r\n> Head offers an intermediate level of abstraction: the model_fn is split into a logit_fn that contains the ops to create the logits Tensor, and a Head that produces ops for train, eval and infer given those logits. Internally, canned estimators use this scheme. E.g. DNNClassifier and DNNRegressor use the same logit_fn but the former uses a classification head, whereas the latter uses a regression head. This reduces code duplication, improves testing, and makes it easier for users to adapt models for their own use.\r\n \r\nat least to me this would insinuate that a usage as above described be possible, instead of this being just the implementation strategy of DNNClassifier/DNNRegressor\r\n\r\n2) Take a `logit_fn` instead of logits. This would cause three problems:\r\n * The user might be required to write another function even though for most use cases, if the non-head part of the model_fn does something apart from calculating the logits. This could be fixed by making the interface take either a logit tensor or a function, where fany heads might only work with a function and otherwise raise an exception. \r\n * In the current setting `make_estimator_spec` also takes regularization losses. These will depend on the calculation of the logits. Therefore one would need the `logit_fn` to be able to return a `(logits, regularization_loss)` tuple or more generally a `LogitSpec` object.\r\n* In MultiHead in most cases one would not like each sub-head to calculate the same logits again. However, MultiHead might amend the `logit_fn` with some form of LRU cache that only adds ops to the graph if it is called with different features. (Maybe the graph optimizer would be able to fold all logit_fn subgraphs back together?)\r\n\r\nIf you do pass in the logits as a function instead of a tensor, this might also allow to lift the Head completely out of `model_fn` and provide a `make_estimator` instead of a `make_estimator_spec` function.\r\n\r\n  ", "@ngc92 thanks for the feedback.\r\nI think option 2 for adversarial training works. You can also use the `head_weights` argument of `multi_head` to multiply the loss for each head. E.g. for the adversarial head you typically want to choose a negative head weight.\r\n\r\nWhat we mean by splitting into head and logit_fn is that you can write your method that calculates logits, and pass those logits to the head. I don't think that accepting a logit_fn argument would make the api more flexible. Is there a use case that is not supported in your opinion?", "@roumposg\r\nwell, since you can easily combine higher and lower level interfaces in the `model_fn` I don't think this is something that is not supported by the current Head design.\r\n\r\nI am not quite sure whether a `logit_fn` should be supported as it adds quite a bit of complexity. If Head only takes logit tensors, though, I would not call the calculation of that a `logit_fn`, as this (to me) sounds very much like the ability of passing a `model_fn` to an estimator.\r\n\r\nI think to a user just writing a model option 2 from my last comment is fine, but from a library perspective (e.g. in case of the adversarial training for [cleverhans](https://github.com/tensorflow/cleverhans)) the third option seems more appealing. In that way passing a `logit_fn` instead of logits does increase what you can do as a writer of a general purpose head. ", "After design review, this RFC has been merged into the new community repo. See https://github.com/tensorflow/community/blob/master/rfcs/20180501-estimator-head.md\r\n\r\nThank you to everyone who participated in the discussion!\r\n", "@roumposg  would it make more sense to add a `label_sep` to `multi_label_head` so that `labels` don't need to be `sparse_tensor`  but a `tensor`?\r\n  as here https://github.com/tensorflow/tensorflow/pull/20001/files#diff-eb023bd59df93d51913b23161f6187baR1457", "@yupbank `multi_label_head` is for problems where the number of labels is not fixed. So it makes sense to have `SparseTensor`. Do you have a problem with a fixed number of labels `n>1`? Then one option is to split each each example into `n` examples and use `multi_class_label`.", "@roumposg yeah, i understand that, but if we can have a labels feature column containing dimension 1 value, with `[\"label1=label2\", \"label2=label5\"] ` and user can specify `=` as the `label_separator`,  the `multi_label_head` interface would be more consistent with other heads, which accept dense tensor\r\n", "Good idea. But this is easy to implement using `tf.string_split` in your `input_fn`. I think adding one more argument to the api will add complexity without adding much value. What do you think? If you want to use a dense `Tensor`, `multi_label_head` supports multi-hot tensor of shape `[batch_size, n_classes]`.", "i mean it would add value for users that, they can easily switch to one vs all mode, using `multi_class_head` without need to change the label tensor. ", "Peng, you can set loss_fn argument of `multi_class_head`. We can add an\nexample to the documentation of `multi_class_head`. What do you think?\n\nOn Fri, Jun 22, 2018 at 9:04 AM Peng Yu <notifications@github.com> wrote:\n\n> i mean it would add value for users that, they can easily switch to one vs\n> all mode, using multi_class_head without need to change the label tensor.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19157#issuecomment-399492001>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASZl7WbF30Nk_x5ZIQO5SIWibC8RHEEoks5t_RWAgaJpZM4T3ClR>\n> .\n>\n"]}, {"number": 19156, "title": "Tensorflow silently renaming variables on implicitly defined scope", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**:  N/A\r\n- **GPU model and memory**:  N/A\r\n- **Exact command to reproduce**:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\nn_input = 5\r\ninput = tf.placeholder(tf.float32, (None, n_input), 'input')\r\noutput = tf.placeholder(tf.float32, (None, 1), 'output')\r\ncheck = tf.placeholder(tf.float32, (None, 1), 'check')\r\n\r\nwith tf.variable_scope(\"scope1\",\r\n                       initializer=tf.random_normal_initializer(mean=0, stddev=1)):\r\n    w = tf.get_variable(name=\"w\", shape=(n_input, 1))\r\n    y_predicted = tf.matmul(input, w)\r\n\r\ncost = tf.losses.mean_squared_error(check, y_predicted)\r\ncost = tf.reduce_mean(cost, name=\"mean_squared_error\")\r\n\r\ntf.get_default_graph().get_tensor_by_name(\"mean_squared_error:0\")\r\n```\r\n\r\n### Describe the problem\r\nWhen using losses from `tf.losses` various scopes a defined implicitly. For example for `tf.losses.mean_squared_error` the scope `mean_squared_error` is defined. If I now try to define a variable named `mean_squared_error`, e.g., for later showing results in Tensorboard, then I get a `KeyError`:\r\n```\r\nKeyError: \"The name 'mean_squared_error:0' refers to a Tensor which does not exist. The operation, 'mean_squared_error', does not exist in the graph.\"\r\n```\r\nThis is due to Tensorflow silently renaming my explicitly defined variable because of the implicitly defined scope (by `tf.losses.mean_squared_error`). This is rather counter intuitive and potentially can easily result in unexpected behavior. I suggest throwing an error or at least printing a warning in such a case. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Hi, thanks for your reply. I updated the post. ", "By \"variable\" you mean Tensor? Can you post the graph that actually gets generated?\r\n\r\nI think, from looking at your code, that everything is working as intended; TensorFlow graphs need to uniquify the operation names somehow, so you specifying a name which conflicts with the name of something else in the graph will cause the name you specified to not actually be used.", "Nagging Assignee @alextp: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since it looks to me like it's working as intended. Please reopen with answers to my questions above if I'm wrong."]}, {"number": 19155, "title": "[Bug] Silent memory failure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (see below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Lubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: Python 3.5.2\r\n **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**:  Driver Version: 390.30 CUDA Driver Version = 9.1\r\n- **GPU model and memory**: NVidia GTX 1080Ti (11Go)\r\n- **Exact command to reproduce**: copy paste the given code and call the only function (if running on a  GPU with different memory please fiddle with the size of the arrays accordingly)\r\n\r\n### Describe the problem\r\nWhen run on GPU the program runs fine except results are silently zeroed for elements in the last 25% of result. It only happens when I it uses a lot of memory but instead of failing it tries to compute and silently mess the results.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\ndef bugTensorflow():\r\n    #bug with GTX 1080 Ti\r\n    #Lubuntu 16.04\r\n    #Driver Version: 390.30\r\n    #CUDA Driver Version = 9.1\r\n    nbpoints1 = 100000\r\n    nbpoints2 = 4000\r\n    dim = 3\r\n\r\n    #works fine on cpu and bugs on second gpu as well\r\n    #with tf.device(\"/cpu:0\"):\r\n    #with tf.device(\"/gpu:1\"):\r\n\r\n    with tf.device(\"/gpu:0\"):\r\n        points = tf.random_normal( (nbpoints1,1,dim) )\r\n        traj = tf.random_normal((1,nbpoints2,dim) )\r\n        diff= points-traj\r\n        dist = tf.reduce_sum( diff*diff,axis=-1)\r\n    sess = tf.Session()\r\n    with(sess.as_default()):\r\n        _dist = sess.run(dist)\r\n        print(\"dist.shape\")\r\n        print(dist.shape)\r\n        print(\"dist[0]\")\r\n        print(_dist[0])\r\n        print(\"dist[75000]\")\r\n        print(_dist[75000])\r\n        print(\"tensorflow version\")\r\n        print(tf.__version__)\r\n```\r\nResults : \r\n\r\n> dist.shape\r\n> (100000, 4000)\r\n> dist[0]\r\n> [ 3.918147   8.78706    3.0947132 ...  0.8662497  0.6857513 11.894537 ]\r\n> dist[75000]\r\n> [0. 0. 0. ... 0. 0. 0.]\r\n> tensorflow version\r\n> 1.7.0\r\n> \r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nExact command to reproduce", "@tensorflowbutler updated", "@asimshankar Can you take a look at this?", "Nagging Assignees @asimshankar, @angersson: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @asimshankar, @angersson: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @asimshankar, @angersson: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflowbutler Damn humans are so lazy, you need to nagg harder :) otherwise they will never give you rights. You can start by CC'ing their bosses, or take money from them. That usually do the trick. ", "Sorry for ignoring your issue. I've reproduced the issue and will provide updates in #22123."]}, {"number": 19154, "title": "[XLA] Use non-inlined functions where inlining causes problems", "body": "On my OS/X machine these two functions, currently defined in the header, no not get marked `weak`, and cause a compilation error.\r\n\r\nThey do not appear to need to be defines in a header file.\r\n\r\nThis change removes them from the header and places them into the `cc` file.\r\n", "comments": ["Nagging Assignee @rmlarsen: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@rmlarsen Hi - could I get this reviewed.  I think that it is a fairly non-controversial change.", "@tatatodd Hi there.  Could we get this reviewed and merged?  I think it is a fairly non-controversial change.\r\n", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tatatodd @rmlarsen Hello.   This is over 2 months old now.  Can someone look at it?\r\n", "@vrv @hawkinsp Hi guys.  I'm guessing that the chaps who are down to review this don't know they are or get notifications.  Is there a way of getting someone else to review it?  It is over 2 months old now.  It's not a complicated thing, and it doesn't fail on linux, only OS/X.\r\n\r\n", "Sorry about the delay.  My guess is with 262 active PRs, many of those here listed also on them, these are getting lost in the noise.  This is a simple change so LGTM, even though I'm pretty unfamiliar with the code myself.  Thanks for the contribution!", "Hi @vrv, I don't think that these failures are to do with the change.  The XLA ones look like a resource issue (something to do with remote cache), and the ubuntu sanity fails from time to time anyway.\r\n\r\ncould we try to run the unit tests again?\r\n", "Yes, let me try again, thanks.", "Ok. So there is a compilation error, which is odd as it doesn't appear on either of our build environments.  Could be something to do with compiling with the GPU enabled.\r\n\r\n```\r\ntensorflow/compiler/xla/tests/client_library_test_base.cc:579:1: error: 'std::unique_ptr<xla::GlobalData> xla::ClientLibraryTestBase::CreateParameterAndTransferLiteral(tensorflow::int64, const xla::Literal&, const string&, const xla::DeviceHandle*, xla::XlaBuilder*, xla::XlaOp*)' previously defined here\r\n ClientLibraryTestBase::CreateParameterAndTransferLiteral(\r\n```", "ah ha - it turns out that this problem has been fixed by someone else already.\r\n", ":(.  Thanks for checking and dealing with so many delays, your effort is appreciated (by me at least!)."]}, {"number": 19153, "title": "Remove python defines which clash with logging", "body": "I have the following clang on OS/X (High Sierra 10.13.4):\r\n\r\n```\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\n```\r\n\r\nI receive a set of errors while compiling the `contrib/lite` directory which comes down to python defining some of the standard `isXXX` type functions with pre-processor macros.\r\n\r\nThis change removes those errors by undefining them\r\n\r\n", "comments": ["Nagging Assignee @rmlarsen: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@aselle @miaout17 hi guys.  i wonder if someone could look at this one. it is an OS/X build failure due to Python.h defining a load of symbols which clash with the standard C RTL\r\n", "I'm abandoning this. "]}, {"number": 19152, "title": "[INTEL MKL] Fixes a failure in //tensorflow/python/profiler:model_analyzer_test.", "body": " Modified  testComplexCodeView to look for lower total_float_ops. The value of total_float_ops is lower when Tensorflow is compiled with Intel MKL.", "comments": ["@rmlarsen thanks. I have updated the PR based on your feedback."]}, {"number": 19151, "title": "Keras metrics raise RuntimeError with MirroredStrategy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Docker built on nvidia/cuda:9.0-devel-ubuntu16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Cuda version**: 9.0\r\n- **Bazel version**: N/A (binary install)\r\n- **Python version**: Python 3.5.2\r\n- **GPU model and memory**:  4x1080 w/8G / See output from tf_env_collect.sh below\r\n- **Exact command to reproduce**: `python3 simple_tfkeras_example.py ./simple_model/`\r\n\r\n### Describe the problem\r\n\r\nTrying to adapt Keras code to be distributed using `tf.contrib.distribute.MirroredStrategy`, however, using `metrics` in the Keras model causes an Exception to be raised:\r\n\r\n```\r\n    \"Use DistributionStrategy.update() to modify a MirroredVariable.\")\r\nRuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.\r\nException ignored in: <generator object get_controller at 0x7f7be0048f68>\r\n```\r\n\r\nIf the `strategy` is set to `None` or `OneDeviceStrategy`, the metrics perform as expected.  When `strategy` is `MirroredStrategy`, the exception is raised.  If the metrics are removed from the `model.compile` call, the code works as expected.\r\n\r\nThis appears to be a bug, or an unimplemented feature, with the understanding that `MirroredStrategy` is new in TF 1.8.\r\n\r\n### Source code / logs\r\n`simple_tfkeras_example.py` (slightly modified from the [examples](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/simple_tfkeras_example.py))\r\n\r\n```\r\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"An example tf.keras model that is trained using MirroredStrategy.\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom sys import argv\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nsz = 16\r\nnchannels = 5\r\nsh = (sz,sz,sz,nchannels)\r\ndef input_fn():\r\n  x = np.random.random((2,sz,sz,sz,nchannels))\r\n  y = np.random.randint(2, size=(2,sz,sz,sz,nchannels))\r\n  x = tf.cast(x, tf.float32)\r\n  dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n  dataset = dataset.repeat(10)\r\n  dataset = dataset.batch(32)\r\n  return dataset\r\n\r\n\r\ndef main(args):\r\n  if len(args) < 2:\r\n    print('You must specify  model_dir for checkpoints such as'\r\n          ' /tmp/tfkeras_example./')\r\n    return\r\n\r\n  print('Using %s to store checkpoints.' % args[1])\r\n\r\n  # These strategies work\r\n  # strategy = None\r\n  # strategy = tf.contrib.distribute.OneDeviceStrategy('/device:GPU:0')\r\n  \r\n  # These strategies fail with:\r\n  # \"Use DistributionStrategy.update() to modify a MirroredVariable.\")\r\n  # RuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.\r\n  # Exception ignored in: <generator object get_controller at 0x7f7be0048f68>\r\n  strategy = tf.contrib.distribute.MirroredStrategy(\r\n      ['/device:GPU:0', '/device:GPU:1'])\r\n  strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=4)\r\n  \r\n  config = tf.estimator.RunConfig(train_distribute=strategy)\r\n  optimizer = tf.train.GradientDescentOptimizer(0.2)\r\n\r\n  optimizer = tf.train.AdadeltaOptimizer()\r\n  \r\n  model = tf.keras.Sequential()\r\n  l = tf.keras.layers\r\n  model.add(l.Conv3D(5,3, padding='same', activation='relu', input_shape=sh))\r\n  # model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)))\r\n  # model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n\r\n  # works correctly with OneDeviceStrategy and MirroredStrategy\r\n  metrics = None\r\n  # works correctly with OneDeviceStrategy, but not with MirroredStrategy\r\n  metrics = ['binary_accuracy']\r\n    \r\n  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\r\n  model.summary()\r\n  tf.keras.backend.set_learning_phase(True)\r\n  keras_estimator = tf.keras.estimator.model_to_estimator(\r\n      keras_model=model, config=config, model_dir=args[1])\r\n\r\n  keras_estimator.train(input_fn=input_fn, steps=1000)\r\n  eval_result = keras_estimator.evaluate(input_fn=input_fn)\r\n  print('Eval result: {}'.format(eval_result))\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run(argv=argv)\r\n```\r\n\r\n### Output of `tf_env_collect.sh`\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux c036785af2e5 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 04:11:40 CST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux c036785af2e5 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 04:11:40 CST 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy           1.14.3\r\nprotobuf        3.5.2.post1\r\ntensorflow-gpu  1.8.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue May  8 15:47:06 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:84:00.0 Off |                  N/A |\r\n| 27%   35C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 00000000:85:00.0 Off |                  N/A |\r\n| 27%   37C    P0    38W / 180W |      0MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 00000000:88:00.0 Off |                  N/A |\r\n| 27%   27C    P0    39W / 180W |      0MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 00000000:89:00.0 Off |                  N/A |\r\n| 27%   32C    P0    38W / 180W |      0MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n```\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version", "We are currently don't support metrics in training when using MirroredStrategy. Here is the [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md) with a few more details about what is supported. cc @josh11b ", "@anj-s @jsh11b appreciate the answer, I didn't read about the limitations of `MirroredStrategy`.  Will use `tf.contrib.estimator.add_metrics` to clone the estimator with metrics and use the new estimator in the evaluation phase of training.  This will suit our use case just fine. ", "@anj-s Recommend to compare MirroredStrategy with horovod in the docs."]}, {"number": 19150, "title": "error while converting *.pb file to *.tflite with toco", "body": "I used the following example to create tensorflow model: http://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/\r\nYou can download the code from here: https://github.com/sankit1/cv-tricks.com/tree/master/Tensorflow-tutorials/tutorial-2-image-classifier\r\nAlso I used \"2. Freezing the graph\" section from http://cv-tricks.com/how-to/freeze-tensorflow-models/\r\nto create a *.pb file of my model.\r\nI have Ubuntu 16.04 and tensorflow cpu version 1.8.0 from binary. python version is 3.5.2 and bazel 0.13.0. \r\nI'm tried to convert *.pb file with toco command line tool, as describe on \"Convert a TensorFlow SavedModel to TensorFlow Lite\" at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel\r\nand got he following error: \r\n(venv) user@user-desktop:~/PycharmProjects/tensorflow_tutorial/tensorflow$ **bazel run -c opt   tensorflow/contrib/lite/toco:toco --   --savedmodel_directory=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier   --output_file=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier/dogs-cats-model.tflite**\r\nWARNING: /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nWARNING: failed to create one or more convenience symlinks for prefix 'bazel-':\r\n  cannot create symbolic link bazel-out -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-out (File exists)\r\n  cannot create symbolic link bazel-out -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-out (File exists)\r\n  cannot create symbolic link bazel-tensorflow -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-tensorflow (File exists)\r\n  cannot create symbolic link bazel-bin -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/bin:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-bin (File exists)\r\n  cannot create symbolic link bazel-testlogs -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/testlogs:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-testlogs (File exists)\r\ncannot create symbolic link bazel-genfiles -> /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/genfiles:  /home/user/PycharmProjects/tensorflow_tutorial/tensorflow/bazel-genfiles (File exists)\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n/home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.271s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: /home/user/.cache/bazel/_bazel_user/e21a56d90e65395c94952f8aa3d0c4bc/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/lite/toco/toco '--savedmodel_directory=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier' '--output_file=/home/user/PycharmProjects/tensorflow_tutorial/tutorial-2-image-classifier/dogs-cats-model.tflite'\r\n2018-05-07 01:33:13.776954: F tensorflow/contrib/lite/toco/toco_saved_model.cc:34] **Check failed: tensorflow::MaybeSavedModelDirectory(model_path) Model is not saved in the supported SavedModel format.**\r\n\r\nThe function which throw this error is MaybeSavedModelDirectory at\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/toco_saved_model.cc\r\nI took a look at the implementation of it on \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.cc\r\nActually it looking for *.pb or *.pbtxt file on the model directory. I got this file at this location so why I get this error?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Lets do it again:\r\nOS Platform and Distribution - ubuntu x64\r\nTensorFlow installed from - pip\r\nTensorFlow version - cpu version 1.8.0\r\nBazel version - 0.13.0\r\nCUDA/cuDNN version - no cuda\r\nGPU model and memory - no gpu\r\nExact command to reproduce - no need\r\n\r\n**I solved it. I should save the pb file as saved_model.pb exactly. it works now. thanks!** ", "Lovely. Thanks for the update."]}, {"number": 19149, "title": "Just fix little issue that is model_file_type variable but not using it in iOS DEMO.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Reviewer @aselle: It has been 16 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Why is this awaited a long time?.. Am I missing something for my process?", "Nagging Reviewer @aselle: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 30 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19148, "title": "BUG: keras.callbacks.TensorBoard raises an exception for non_trainale_weights", "body": "The class seems not to exclude non_trainable_weight (moving_variances and moving_mean in BatchNormalization) correctly, see the script and its logs below for more details:\r\n\r\n\r\n```python\r\n# TF 1.8, CPU\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = np.ones((32, 10, 10, 3))\r\ny = np.ones((32,))\r\ny = tf.keras.utils.to_categorical(y)\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.BatchNormalization(input_shape=(10, 10, 3)))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(2, activation='softmax'))\r\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nmodel.summary()\r\n\r\ncbks = [tf.keras.callbacks.TensorBoard(histogram_freq=1, batch_size=2, write_grads=True)]\r\n\r\nmodel.fit(\r\n  x,\r\n  y,\r\n  batch_size=4,\r\n  validation_data=(x, y),\r\n  callbacks=cbks,\r\n  epochs=2,\r\n  verbose=0)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 24, in <module>\r\n    verbose=0)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 1232, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training_arrays.py\", line 156, in fit_loop\r\n    callbacks.set_model(callback_model)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py\", line 70, in set_model\r\n    callback.set_model(model)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py\", line 716, in set_model\r\n    grads = model.optimizer.get_gradients(model.total_loss, weight)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/optimizers.py\", line 115, in get_gradients\r\n    raise ValueError('An operation has `None` for gradient. '\r\nValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.\r\n```", "comments": ["@fchollet Could you take a look? Thanks.", "@facaiy could you please take a look at the test failures?", "@rmlarsen Sure. The failure seems unrelated: Timeout for batch_dataset_op_test. ", "@facaiy Could you please rebase to resolve the conflicts? Then I think we can merge this.", "@rmlarsen Hi, I merged the latest commit from upstream master branch.", "@facaiy Thanks!", "@facaiy Thanks!", "@fchollet @rmlarsen Thanks all!"]}, {"number": 19147, "title": "when oficial Ruby language?", "body": "Is any real data to start oficial Ruby port of tensorflow?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@ewilderj -- Thoughts on Ruby support?", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Ruby isn't one of our officially supported languages. However, if there are any community developers who want to work on it, we can certainly provide some help and resource, in the same way as we do with the Rust bindings.", "why ruby isnt officialy supported?", "I think that's because Google has limited resources, and they have to focus on a small part of the universe to beat their competitors. \r\n\r\nJoking aside, \r\n\r\n@ewilderj \r\nFamous Ruby community developer Andrew Kane has started creating a TensorFlow binding. \r\nThis project will be the most active Ruby-Tensorflow binding in 2019.\r\nhttps://github.com/ankane/tensorflow\r\n\r\nThere have been several attempts to create Ruby TensorFlow bindings, but they have not been very successful. For example, the project below has stopped updating.\r\nhttps://github.com/somaticio/tensorflow.rb\r\n\r\nI look forward to the future of Ankane's Tensorflow bindings.", "@ankane please feel free to reach out - I'm ewj@google.com - as I would like to help you get support from us\r\n\r\nthanks for pointing this out @kojix2 ", "Hi @ewilderj, thanks for offering to help. I've reached out over email. Please let me know if you haven't received it.", "@ankane thanks for the ping, I replied to your email just now."]}, {"number": 19146, "title": " 'num' support a tensor as input in tf.unstack(#17266)", "body": "", "comments": ["@rmlarsen I think We should modify gen_array_ops.py, But this file is MACHINE GENERATED. So, we should edit array_ops.cc. However, I don't know how to generate gen_array_ops.py file by array_ops.cc. Could you give me a way to generate this file?? \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/17266#issuecomment-388495869", "@jason9693 gen_array_ops.py gets generate automatically during the build. ", "@rmlarsen  Is it only available to build in ubuntu or windows os, not in mac?", "@jason9693 It is available on all platforms.", "@jason9693 there are many errors, including the sanity check. Could you fix and push again?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 19145, "title": "Linalg.LinearOperators do not give any performance improvement", "body": "### Describe the problem\r\n\r\nThe performance gaurantees are not visible for the Linalg.LinearOperators (e.g., [DiagOperator](https://www.tensorflow.org/api_docs/python/tf/linalg/LinearOperatorDiag))   I have implemented a basic case below as per the documentation. Is there any reason why the performance of `diag` operator is same as full matrix ? \r\n\r\nCuda and other versions not included as the issue is reproducible across GPU and CPU modes.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, but using only preliminary Linalg operations. \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux xxxx-xxxop 4.10.0-28-generic #32~16.04.2-Ubuntu SMP Thu Jul 20 10:19:48 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n\r\n- **Python version**: \r\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nCheck the source code.\r\n\r\n\r\n### Source code / logs\r\n\r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nrng = np.random.RandomState(1)\r\n```\r\n```python\r\nclass Data:\r\n    num_data = 10\r\n    num_ind = 50\r\n    D_in = 100\r\n    D_out = 2\r\n\r\n    Xmu = rng.randn(num_data, D_in)\r\n    Xcov = rng.randn(num_data, D_in, D_in)\r\n    Xcov = Xcov @ np.transpose(Xcov, (0, 2, 1))\r\n    Z = rng.randn(num_ind, D_in)\r\n```\r\n\r\n\r\n```python\r\nN = Data.num_data\r\nXmu = tf.convert_to_tensor(Data.Xmu)\r\nXcov = tf.convert_to_tensor(Data.Xcov)\r\n```\r\n```python\r\nC = tf.cholesky(Xcov)\r\n\r\nZ_tiled = tf.tile(tf.expand_dims(tf.transpose(Data.Z), 0), [N, 1, 1])\r\n```\r\n```python\r\nC_operator = tf.linalg.LinearOperatorLowerTriangular(C)\r\nC_diag_operator = tf.linalg.LinearOperatorDiag(tf.matrix_diag_part(C))\r\n```\r\n\r\n```python\r\ndef compute_matrix_solve(C, Z_tiled):\r\n#     return tf.matrix_triangular_solve(C, Z_tiled, lower=True)  # NxDxM\r\n    return tf.matrix_solve(C, Z_tiled)  # NxDxM\r\n\r\ndef compute_matrix_matmul(C, Z_tiled):\r\n    mat = tf.matmul(C, Z_tiled)\r\n    return mat \r\n\r\ndef compute_operator_solve(C_operator, Z_tiled):\r\n    operator_value = C_operator.solve(Z_tiled)\r\n    return operator_value\r\n\r\ndef compute_operator_matmul(C_operator, Z_tiled):\r\n    operator_value = C_operator.matmul(Z_tiled)\r\n    return operator_value\r\n```\r\n\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    mat_solve = sess.run(compute_matrix_solve(C, Z_tiled))\r\n    op_solve = sess.run(compute_operator_solve(C_operator, Z_tiled))\r\n    \r\n    tf_mat_mul = sess.run(compute_matrix_matmul(C, Z_tiled))\r\n    op_matmul = sess.run(compute_operator_matmul(C_operator, Z_tiled))\r\n\r\nnp.testing.assert_allclose(mat_solve, op_solve)\r\nnp.testing.assert_allclose(tf_mat_mul, op_matmul)\r\n```\r\n\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    %timeit sess.run(compute_matrix_solve(C, Z_tiled))\r\n    %timeit sess.run(compute_operator_solve(C_operator, Z_tiled))\r\n    %timeit sess.run(compute_operator_solve(C_diag_operator, Z_tiled))\r\n```\r\n\r\n    69.1 ms \u00b1 1.33 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    69.4 ms \u00b1 515 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    73.7 ms \u00b1 1.55 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n    %timeit sess.run(compute_matrix_matmul(C, Z_tiled))\r\n    %timeit sess.run(compute_operator_matmul(C_operator, Z_tiled))\r\n    %timeit sess.run(compute_operator_matmul(C_diag_operator, Z_tiled))\r\n    \r\n```\r\n\r\n    58.5 ms \u00b1 447 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    60.8 ms \u00b1 808 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n    63.8 ms \u00b1 1.78 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n\r\n\r\n\r\n", "comments": ["Hi,\r\n  One reason for slowness is compute_* adds nodes to the graph. Generally for benchmarking, one should create the graph up front, and then benchmark the sess.runs. The sess.run(...) should also be timed for the underlying op since eval'ing the tensor can include time from the CPU / GPU copies.\r\n\r\nWhat you want in the end is something like:\r\n\r\nwith tf.Session() as sess:\r\n  msolve = compute_matrix_solve(C, Z_tiled).op\r\n  ....\r\n  def sess_run_msolve():\r\n    return sess.run(msolve)\r\n  print(timeit.timeit(sess_run_msolve, number=100))\r\n\r\nDoing the aforementioned changes (and changing D_in to 500 to see the difference for larger matrices), I get on CPU (in seconds):\r\n\r\nMatrix Solve 3.94200801849\r\nOperator Solve 2.10899114609\r\nDiag Solve 1.88388705254\r\nMatrix Mul 2.18344807625\r\nOperator Mul 2.28254699707\r\nDiag Mul 1.86973500252\r\n\r\nUsing https://www.tensorflow.org/api_docs/python/tf/test/Benchmark is also an alternative, which would also do some burn in cycles.\r\n\r\n ", "Thank for your reply. I can confirm that with your suggestions I do see the improvements.\r\nUnfortunately, I have to do this over much lower dimensions and may the gain that case is very marginal.\r\n\r\nHowever, I do like this feature. I will close this issue with results confirming @srvasude  suggestions. \r\n\r\n``` python\r\nwith tf.Session(graph=tf.get_default_graph()) as sess:\r\n    msolve = compute_matrix_solve(C, Z_tiled).op\r\n    opsolve = compute_operator_solve(C_operator, Z_tiled)\r\n    diagsolve=compute_operator_solve(C_diag_operator, Z_tiled)\r\n    def sess_run_msolve():\r\n        return sess.run(msolve)\r\n    def sess_run_opsolve():\r\n        return sess.run(opsolve)\r\n    def sess_run_diagsolve():\r\n        return sess.run(diagsolve)\r\n    def burn_in(msolve, opsolve, diagsolve, cycles=100):\r\n        for _ in range(cycles):\r\n            sess.run([msolve, opsolve, diagsolve])\r\n            \r\n            \r\n    burn_in(msolve, opsolve, diagsolve)\r\n    print(timeit.timeit(sess_run_msolve, number=500))\r\n    print(timeit.timeit(sess_run_opsolve, number=500))\r\n    print(timeit.timeit(sess_run_diagsolve, number=500))\r\n```\r\n\r\n``` shell\r\n28.759427453987882\r\n15.111607366998214\r\n10.926930371992057\r\n\r\n```"]}, {"number": 19144, "title": "TF 1.8 benchmark distributed run failed", "body": "System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nLinux Ubuntu 16.04\r\nTensorFlow installed from source with verbs support\r\nTensorFlow version:\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.8.0-1-g8753e2e', '1.8.0')\r\nPython version: 2.7 \r\nBazel version: 0.9 ,0.10\r\nGCC/Compiler version: 5.4\r\nCUDA/cuDNN version: 9.1 ,7.0\r\nGPU model and memory: P100, 16GB\r\nExact command to reproduce:\r\n\r\nOn first node:\r\nCUDA_VISIBLE_DEVICES='' python ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 1 --job_name ps --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True &\r\n\r\npython ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 1 --job_name worker --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True\r\n\r\n\r\nOn second node:\r\nCUDA_VISIBLE_DEVICES='' python ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 0 --job_name ps --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True &\r\n\r\npython ~/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model inception3 --batch_size 64 --ps_hosts 11.11.11.47:10001,11.11.11.48:10002 --worker_hosts 11.11.11.47:20001,11.11.11.48:20002 --task_index 0 --job_name worker --num_gpus 1 --variable_update=distributed_replicated --cross_replica_sync=True\r\nProblem\r\nI'm running benchmark ( master branch) on two hosts with error:\r\n2018-05-08 14:55:30.598759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-05-08 14:55:30.979461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-05-08 14:55:31.325185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-05-08 14:55:31.676291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 3 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\r\n2018-05-08 14:55:31.685309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-05-08 14:55:32.760916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-08 14:55:32.760978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \r\n2018-05-08 14:55:32.760986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \r\n2018-05-08 14:55:32.760991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \r\n2018-05-08 14:55:32.760997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \r\n2018-05-08 14:55:32.761002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \r\n2018-05-08 14:55:32.762303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 15133 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\r\n2018-05-08 14:55:33.097816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 15133 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)\r\n2018-05-08 14:55:33.431088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 15133 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:07:00.0, compute capability: 6.0)\r\n2018-05-08 14:55:33.735570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 15133 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 6.0)\r\n2018-05-08 14:55:34.010006: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 11.11.11.47:10001, 1 -> 11.11.11.48:10002}\r\n2018-05-08 14:55:34.010056: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:20001, 1 -> 11.11.11.48:20002}\r\n2018-05-08 14:55:34.016202: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:20001\r\nTensorFlow:  1.8\r\nModel:       inception3\r\nDataset:     imagenet (synthetic)\r\nMode:        training\r\nSingleSess:  False\r\nBatch size:  128 global\r\n             64 per device\r\nNum batches: 100\r\nNum epochs:  0.01\r\nDevices:     ['/job:worker/task:0/gpu:0']\r\nData format: NCHW\r\nLayout optimizer: False\r\nOptimizer:   sgd\r\nVariables:   distributed_replicated\r\nSync:        True\r\n==========\r\nGenerating model\r\nW0508 14:55:41.537986 139727489328896 tf_logging.py:126] From /root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:1525: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2018-05-08 14:55:53.359721: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-05-08 14:56:00.360362: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\r\nI0508 14:56:00.440299 139727489328896 tf_logging.py:116] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnavailableError'>, OS Error\r\nTraceback (most recent call last):\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\", line 60, in <module>\r\n    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 274, in run\r\n    _run_main(main, argv)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 238, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\", line 56, in main\r\n    bench.run()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1306, in run\r\n    return self._benchmark_cnn()\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 1535, in _benchmark_cnn\r\n    start_standard_services=start_standard_services) as sess:\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 1000, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 828, in stop\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 989, in managed_session\r\n    start_standard_services=start_standard_services)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 285, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution", "Ubuntu 16.04 , grpc", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "This problem still exists on 1.13 branch"]}, {"number": 19143, "title": "[Feature Request] Control the progress of Iterator.get_next()", "body": "I just noticed that Iterator.get_next() did not change my batch samples in a tf.while_loop.\r\nHowever, I'd like to run the training operation in a tf.while_loop, so I need to control the progress of Iterator.get_next() directly.\r\n\r\n\r\n**My Feature Request:**\r\nIt would be cool, if Iterator would provide an operation which can be called to advance its state.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  3.6.5\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n", "comments": ["> I just noticed that Iterator.get_next() did not change my batch samples in a tf.while_loop.\r\n\r\nCan you explain what you actually tried? If the `get_next()` op is created inside the while loop body, it should advance in each iteration.", "I opened a question on Stackoverflow to this topic:\r\nhttps://stackoverflow.com/questions/50237486/tf-data-iterator-get-next-how-to-advance-in-tf-while-loop\r\nMy problem is that I have to assign the new values to my model.\r\n\r\nI also tried the following:\r\n```\r\na = tf.Variable(iterator.get_next())\r\nwhile (...):\r\n    with tf.control_dependency([tf.assign(a, iterator.get_next())]):\r\n        minimize(...)\r\n```\r\nHowever, this did not work as well. I still did not get other values in operations using `a`.\r\nUnfortunately my model is too big to share it here.", "This question is better handled on Stack Overflow. Unfortunately neither we nor the folks on Stack Overflow will be able to help without a runnable example that exhibits the problem you're facing. The [answer](https://stackoverflow.com/a/50237643/3574081) to your Stack Overflow question explains that the `iterator.get_next()` call must be performed inside the `tf.while_loop()` body, and the same principle should apply to your model code if you wrap it in a `tf.while_loop()`.", "@mrry :\r\nThe question on Stackoverflow should be seen as an workaround.\r\nI just realized, why my depending operations still did not get updated values:\r\nCurrently, **all** operations depending on `iterator.get_next` have to be defined inside the while_loop's `body_op`.\r\n\r\nWhat I'd like to request as a new feature would be the following `step_op`:\r\n```\r\na = iterator.get_next()\r\n...\r\nloss = tf.reduce_sum(some_tensor_depending_on_a)\r\n...\r\ntf.while_loop(...):\r\n    with tf.control_dependency([iterator.step_op()]):\r\n        minimize(loss)\r\n```\r\n\r\nIn my opinion this is a much cleaner option as it removes the necessity to define the whole model inside a while_loop.", "I'd recommend checking out [eager execution mode](https://www.tensorflow.org/programmers_guide/eager), which lets you write code similar to that using a regular Python `while` (or `for`) loop.", "I'm sorry for the wrong tf.while_loop syntax, it's just an illustration.\r\n\r\nMy target is to run the optimizer in a `tf.while_loop` inside the computational graph. \r\nThe eager execution would not stop me from using Python evaluations; instead it enforces the usage of Python loops.\r\n\r\nI'm not sure, if we are talking about the same problem...", "Unfortunately what you are asking for would be a large change to the semantics of graph-mode execution and `tf.while_loop()`: currently any tensors that precede the `tf.while_loop()` and are captured inside the body of the loop are captured by value and treated as a **constant** for the rest of the loop. Therefore both the `iterator.get_next()` **and** the calculation of `loss` must also be moved inside the `tf.while_loop()` body in order to process different examples in each iteration. \r\n\r\nI'm not aware of any plans to change this behavior.\r\n\r\n/cc @skye, who maintains the control flow operators, and can correct me if I'm wrong here.", "Derek's correct, tensors defined outside a while loop cannot be (re)evaluated inside the loop, so any computation that happens inside the loop must be defined inside the loop as well. If you're feeling adventurous, you could try defining a TensorFlow function (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function.py#L43) that performs the loss calculation in your example, and then call the function inside the while loop (and pass it the result of iterator.get_next()). Note that the Defun decorator is not part of the official API at this time though.", "Although I'm not sure using a TensorFlow function would actually buy you anything here, you may as well use a regular Python function :)", "@mrry Thanks for the clarification, this explains my misunderstandings at the beginning :)\r\n@skye That's an interesting feature, I'll definitely try that. You really have to promote such features more :D\r\n"]}, {"number": 19142, "title": "[TF r1.8][TensorRT 4.0.0.3] Output \"munmap_chunk(): invalid pointer\"", "body": "## **System information**\r\n- **HaveI written custom code (as opposed to using a stock example script provided in TensorFlow):** no\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04\r\n\r\n- **TensorFlow installed from (source or binary):** source\r\n\r\n- **TensorFlow version (use command below):** 1.8.0\r\n\r\n- **Bazel version (if compiling from source):** 0.10.1\r\n\r\n- **CUDA/cuDNN version:** 9.0/7.1.3\r\n\r\n- **GPU model and memory:** GTX 1080 8gb\r\n\r\n- **Exact command to reproduce:**\r\npython [test_tftrt.py](https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/tensorrt/test/test_tftrt.py)\r\n\r\n## **Source code / logs**\r\n`2018-05-08 16:11:39.979209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2018-05-08 16:11:39.979516: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1`\r\n`2018-05-08 16:11:40.197749: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2660] Max batch size= 100 max workspace size= 33554432`\r\n`2018-05-08 16:11:40.197774: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2666] starting build engine`\r\n`munmap_chunk(): invalid pointer`\r\n`Aborted (core dumped)`", "comments": ["@MacwinWin TensorRT 4.0 is not officially supported yet but we didn't observe this issue in my builds. Can you give more information on how you build?\r\n\r\nThanks,\r\nSami\r\n", "@samikama I build the environment follow the instruction below.\r\n\r\n **1. Install CUDA 9.0\\CuDNN 7.1.3 from runfile and tar file**\r\n\r\n- After install CUDA in /usr/local/cuda, add \r\n`export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}`\r\n`export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}`\r\n`export CUDA_HOME=/usr/local/cuda`\r\nto ~/.bashrc\r\n- Move the target files from extracted cudnn file to /usr/local/cuda, and create soft links.\r\n\r\n **2. Install TensorRT from tar file**\r\n\r\n- Extract the tar file to my home directory, and add\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/micro/TensorRT-4.0.0.3/lib`\r\nto ~/.bashrc\r\n- Make the samples, and run well.\r\n\r\n **3. Install TensorFlow r1.8.0 from source follow the [official instruction](https://www.tensorflow.org/install/install_sources)**\r\n\r\n- Bazel version 0.10.1, GCC version 4.8.5\r\n-  execution of the configure script\r\n`...`\r\n`Do you wish to build TensorFlow with CUDA support? [y/N]: y`\r\n`CUDA support will be enabled for TensorFlow.`\r\n`Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.0`\r\n`Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: `\r\n`Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1`\r\n`Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:`\r\n`Do you wish to build TensorFlow with TensorRT support? [y/N]: y`\r\n`TensorRT support will be enabled for TensorFlow.`\r\n`Please specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/micro/TensorRT-4.0.0.3` \r\n`...`\r\n\r\n**I wonder if it is possible that TensorFlow can not find TensorRT. Because when I remove the TensorRT-4.0.0.3 directory, I got the same logs:**\r\n`munmap_chunk(): invalid pointer`\r\n`Aborted (core dumped)`\r\n\r\nCan anyone help me:)", "@MacwinWin Can you ensure that there are no other TensorRT or TensorFlow installations in your system. Also I usually prefer prepending to LD_LIBRARY_PATH rather than appending, i.e. `export LD_LIBRARY_PATH=/home/micro/TensorRT-4.0.0.3/lib${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}`\r\nTo use TensorRT with TensorFlow, you don't need to compile the samples.\r\n\r\nI see that you set ubuntu version 18.04. If this is not a typo, we haven't tried anything on Ubuntu 18.04 and it is not officially supported. TensorRT is not built for 18.04 and may not work there, especially in 4.0RC, when TensorRT 4.0 is released, things may improve but it is not guaranteed.", "@samikama Thank you for your reply. I follow the advise from NVIDIA to use the TensorRT-3.0.4 for Ubuntu 14.04 not for 16.04 and can get \"PASS\" result from running [test_tftrt.py](https://github.com/tensorflow/tensorflow/blob/v1.8.0/tensorflow/contrib/tensorrt/test/test_tftrt.py). But the run_all.sh from [tftrt](https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz) sample code still return error.\r\n\r\n```\r\nNamespace(FP16=True, FP32=True, INT8=True, batch_size=4, dump_diff=False, native=True, num_loops=10, topN=5, update_graphdef=False, with_timeline=False, workspace_size=3072)\r\nStarting at 2018-05-10 15:42:54.711629\r\n2018-05-10 15:42:54.825034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-10 15:42:54.825343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.86\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.44GiB\r\n2018-05-10 15:42:54.825357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-10 15:42:55.034163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-10 15:42:55.034191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-10 15:42:55.034199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-10 15:42:55.034319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4059 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Starting execution\r\n2018-05-10 15:42:55.627521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-10 15:42:55.627555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-10 15:42:55.627560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-10 15:42:55.627563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-10 15:42:55.627645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4059 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Starting Warmup cycle\r\nINFO:tensorflow:Warmup done. Starting real timing\r\niter  0   0.012434930801391601\r\niter  1   0.012465815544128418\r\niter  2   0.012448086738586425\r\niter  3   0.012464394569396972\r\niter  4   0.012447872161865235\r\niter  5   0.012483620643615722\r\niter  6   0.012439064979553223\r\niter  7   0.0124442720413208\r\niter  8   0.012428841590881347\r\niter  9   0.012441315650939942\r\nComparison= True\r\nINFO:tensorflow:Timing loop done!\r\nimages/s : 321.3 +/- 0.4, s/batch: 0.01245 +/- 0.00002\r\nRES, Native, 4, 321.29, 0.41, 0.01245, 0.00002\r\n2018-05-10 15:43:03.037356: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-05-10 15:43:03.462730: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2660] Max batch size= 4 max workspace size= 3221225472\r\n2018-05-10 15:43:03.462758: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2666] starting build engine\r\n2018-05-10 15:43:15.239915: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-05-10 15:43:15.247011: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-05-10 15:43:15.247026: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2671] Built network\r\n2018-05-10 15:43:15.247145: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] subgraph conversion error for subgraph_index:0 due to: \"Internal: Engine building failure\" SKIPPING......( 452 nodes)\r\nINFO:tensorflow:Starting execution\r\n2018-05-10 15:43:16.385523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-10 15:43:16.385560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-10 15:43:16.385565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-10 15:43:16.385568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-10 15:43:16.385649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4059 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Starting Warmup cycle\r\nINFO:tensorflow:Warmup done. Starting real timing\r\niter  0   0.012447414398193359\r\niter  1   0.012494730949401855\r\niter  2   0.01248936653137207\r\niter  3   0.012472720146179199\r\niter  4   0.012484326362609863\r\niter  5   0.012482056617736817\r\niter  6   0.01248654842376709\r\niter  7   0.012489418983459472\r\niter  8   0.012483220100402832\r\niter  9   0.012479052543640137\r\nComparison= True\r\nINFO:tensorflow:Timing loop done!\r\nimages/s : 320.5 +/- 0.3, s/batch: 0.01248 +/- 0.00001\r\nRES, TRT-FP32, 4, 320.49, 0.32, 0.01248, 0.00001\r\n2018-05-10 15:43:23.911577: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-05-10 15:43:24.275253: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2660] Max batch size= 4 max workspace size= 3221225472\r\n2018-05-10 15:43:24.275284: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2664] Using FP16 precision mode\r\n2018-05-10 15:43:24.275288: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2666] starting build engine\r\n2018-05-10 15:43:24.275693: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.\r\n2018-05-10 15:43:24.351625: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger reformat.cu (591) - Cuda Error in NCHWToNCHW: 2\r\n2018-05-10 15:43:24.368325: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger reformat.cu (591) - Cuda Error in NCHWToNCHW: 2\r\n2018-05-10 15:43:24.368343: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2671] Built network\r\n2018-05-10 15:43:24.368473: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] subgraph conversion error for subgraph_index:0 due to: \"Internal: Engine building failure\" SKIPPING......( 452 nodes)\r\nINFO:tensorflow:Starting execution\r\n2018-05-10 15:43:25.431249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-10 15:43:25.431289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-10 15:43:25.431296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-10 15:43:25.431302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-10 15:43:25.431388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4059 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Starting Warmup cycle\r\nINFO:tensorflow:Warmup done. Starting real timing\r\niter  0   0.012494874000549317\r\niter  1   0.012518620491027832\r\niter  2   0.012508559226989745\r\niter  3   0.012507514953613281\r\niter  4   0.012525267601013183\r\niter  5   0.012517638206481933\r\niter  6   0.012493929862976073\r\niter  7   0.012491002082824706\r\niter  8   0.012517666816711426\r\niter  9   0.012507796287536621\r\nComparison= True\r\nINFO:tensorflow:Timing loop done!\r\nimages/s : 319.8 +/- 0.3, s/batch: 0.01251 +/- 0.00001\r\nRES, TRT-FP16, 4, 319.79, 0.29, 0.01251, 0.00001\r\n2018-05-10 15:43:32.833938: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-05-10 15:43:33.172603: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2419] Max batch size= 4 max workspace size= 3221225472\r\n2018-05-10 15:43:33.172755: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2446] finished op preparation\r\n2018-05-10 15:43:33.172858: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2454] OK\r\n2018-05-10 15:43:33.172864: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2455] finished op building\r\nRunning Calibration\r\nINFO:tensorflow:Starting execution\r\n2018-05-10 15:43:33.988248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-10 15:43:33.988285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-10 15:43:33.988290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-10 15:43:33.988293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-10 15:43:33.988375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4059 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Starting Warmup cycle\r\nCuda error in file src/winograd.cu at line 715: out of memory\r\npython: customWinogradConvActLayer.cpp:280: virtual void nvinfer1::cudnn::WinogradConvActLayer::allocateResources(const nvinfer1::cudnn::CommonContext&): Assertion `convolutions.back().get()' failed.\r\n./run_all.sh: line 13: 14247 Aborted                 (core dumped) python tftrt_sample.py --native --FP32 --FP16 --INT8 --num_loops 10 --topN 5 --batch_size 4 --workspace_size 3072 --log_file log.txt --network resnet_v1_50_frozen.pb --input_node input --output_nodes resnet_v1_50/predictions/Reshape_1 --img_size 224 --img_file grace_hopper.jpg\r\n```\r\n\r\nProbably it really not support Ubuntu 18.04. ", "I'm also seeing this for Ubuntu 16.04 on GTX 1080 or TitanX Pascal GPUs.\r\n```\r\n2018-05-10 19:11:35.252280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so return\r\ning NUMA node zero\r\n2018-05-10 19:11:35.394301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so return\r\ning NUMA node zero\r\n2018-05-10 19:11:35.517305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so return\r\ning NUMA node zero\r\n2018-05-10 19:11:35.661768: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 3\r\n2018-05-10 19:11:36.218467: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2660] Max batch size= 100 max workspace size= 33554432\r\n2018-05-10 19:11:36.218532: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2666] starting build engine\r\n*** Error in `python': munmap_chunk(): invalid pointer: 0x00007ffc19df4fe0 ***\r\n```\r\n\r\nHere's the Dockerfile I used to reproduce the issue:\r\n\r\n```\r\n# Commands ran:\r\n# docker build -t cuda9.0_tf1.8_trt4 .\r\n# nvidia-docker run -it cuda9.0_tf1.8_trt4 bash\r\n# python /install/test_tftrt.py\r\n\r\nFROM nvidia/cuda:9.0-base-ubuntu16.04\r\n\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n        build-essential \\\r\n        cuda-command-line-tools-9-0 \\\r\n        cuda-cublas-9-0 \\\r\n        cuda-cufft-9-0 \\\r\n        cuda-curand-9-0 \\\r\n        cuda-cusolver-9-0 \\\r\n        cuda-cusparse-9-0 \\\r\n        cuda-libraries-dev-9-0 \\\r\n        curl \\\r\n        libcudnn7=7.0.5.15-1+cuda9.0 \\\r\n        libfreetype6-dev \\\r\n        libhdf5-serial-dev \\\r\n        libpng12-dev \\\r\n        libzmq3-dev \\\r\n        pkg-config \\\r\n        python \\\r\n        python-dev \\\r\n        rsync \\\r\n        software-properties-common \\\r\n        unzip \\\r\n        && \\\r\n    apt-get clean && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\nRUN curl -O https://bootstrap.pypa.io/get-pip.py && \\\r\n    python get-pip.py && \\\r\n    rm get-pip.py\r\n\r\nRUN pip --no-cache-dir install \\\r\n        Pillow \\\r\n        h5py \\\r\n        ipykernel \\\r\n        jupyter \\\r\n        matplotlib \\\r\n        numpy \\\r\n        pandas \\\r\n        scipy \\\r\n        sklearn \\\r\n        && \\\r\n    python -m ipykernel.kernelspec\r\n\r\nENV LD_LIBRARY_PATH=/TensorRT-4.0.0.3/lib:$LD_LIBRARY_PATH\r\n\r\nCOPY test_tftrt.py /install/\r\nCOPY TensorRT-4.0.0.3.Ubuntu-16.04.4.x86_64-gnu.cuda-9.0.cudnn7.0.tar.gz /install/\r\n\r\nRUN tar xzvf /install/TensorRT-4.0.0.3.Ubuntu-16.04.4.x86_64-gnu.cuda-9.0.cudnn7.0.tar.gz && rm /install/TensorRT-4.0.0.3.Ubuntu-16.04.4.x86_64-gnu.cuda-9.0.cudnn7.0.tar.gz\r\n\r\nRUN cd /TensorRT-4.0.0.3/python && pip2 install tensorrt-4.0.0.3-cp27-cp27mu-linux_x86_64.whl\r\nRUN pip2 install tensorflow-gpu==1.8\r\n```\r\n\r\nFWIW I also tried TensorRT-3.0.4 and got the same error. Relevant changes to Dockerfile:\r\n```\r\nCOPY TensorRT-3.0.4.Ubuntu-16.04.3.x86_64.cuda-9.0.cudnn7.0.tar.gz /install/\r\n\r\nRUN tar xzvf /install/TensorRT-3.0.4.Ubuntu-16.04.3.x86_64.cuda-9.0.cudnn7.0.tar.gz && rm /install/TensorRT-3.0.4.Ubuntu-16.04.3.x86_64.cuda-9.0.cudnn7.0.tar.gz\r\n\r\nRUN cd /TensorRT-3.0.4/python && pip2 install tensorrt-3.0.4-cp27-cp27mu-linux_x86_64.whl\r\nRUN pip2 install tensorflow-gpu==1.8\r\n```\r\n\r\nInterestingly, I'm able to get the test_tftrt.py script to run fine in docker on a PX2 (with TensorRT 3.02 installed by the DriveWorks SDK, and Tensorflow 1.8 binary installed from https://github.com/peterlee0127/tensorflow-nvJetson).", "@MacwinWin ,\r\nIt looks like you ran out of memory on the card, see third line from the end. Can you try with changing gpu memory allocation parameters and workspace size and try again? Are you by any chance driving X on your GPU 0 as well? if you run nvidia-smi you can see which processes are running on the card and free memory.\r\n@kahkeng, \r\n\r\nPlease see my response above. You need to use **14.04** version of the TRT libraries **NOT 16.04**.", "@samikama Sorry, I didn't see any clear recommendation to use 14.04 above. I just tried that out and it works for me. Thanks!", "@kahkeng,\r\n\r\nSorry I confused this with another issue. I didn't mention it in this one. It is well hidden in official installation instructions though :)\r\n", "@samikama Yeah, It really due to the lack of GPU memory. When I only select one precision mode at once to run, everything works fine.\r\n```\r\n#!/bin/bash\r\npython tftrt_sample.py --INT8 \\\r\n                       --num_loops 10 \\\r\n                       --topN 5 \\\r\n                       --batch_size 1 \\\r\n                       --workspace_size 3000 \\\r\n                       --log_file log.txt \\\r\n                       --network resnet_v1_50_frozen.pb \\\r\n                       --input_node input \\\r\n                       --output_nodes resnet_v1_50/predictions/Reshape_1 \\\r\n                       --img_size 224 \\\r\n                       --img_file  grace_hopper.jpg\r\n```\r\nThanks!\r\n\r\n**Now, I can ensure that TensorRT 3.0.4 for Ubuntu 14.04 works fine on Ubuntu 18.04 with TensorFlow 1.8.0!**\r\nWhen TensorRT 4 RC can get the same result?", "@MacwinWin, if you want to be able to use TRT4.0RC you need to recompile TensorFlow. However we are still working on TRT4.0 support in interface layer so not all features of TRT4 will be available to you. TRT4.0 support in Tensorflow will be available after TensorRT4.0 is released.", "Closing because the original issue seems to be resolved."]}, {"number": 19141, "title": "[r1.7][TensorRT] Questions about the calibration in INT8 mode of TensorRT optimization", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat 4.8.5-16, Linux version 3.10.0-693.5.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: pip (python 2.7)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==1.7.0\r\n- **Python version**:  python 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.3\r\n- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5\r\n- **GPU model and memory**: Tesla P4, 8GB\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nI tried to evaluate the accuracy of TF integrated TRT INT8 optimization in Python. However, I followed the procedure as the example test: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\r\nbut the inference accuracy is not good (FP32: 70%, INT8-without-calib: 16%, INT8-with-calib: 35%).\r\n\r\nI tried to trace the code but on the API level, but I'm a little confused about how does the calibration work. looks like the function run_calibration() just created a session and accepted the calib dataset to run the session, and then the return value was abandoned. \r\n\r\nI checked this introduction:https://devblogs.nvidia.com/int8-inference-autonomous-vehicles-tensorrt/, seems that the TRT python INT8 calibration will generate a calibration cache in file and then be used in the later inference. But in this TFTRT, the calibration didn't generate a calibration cache.\r\n\r\nAnother observation is that, I previously tried the TensorRT optimization in C++ independently, which is not integrated in the Tensorflow. At that time, the accuracy lose on INT8 mode optimization is about 10% (FP32: 70%, INT8-with-calib: 60%). Note that my C++ and Python experiments are using the same model and same calibration dataset and the same test dataset, so from my point of view, perhaps TRT INT8 Python API (at least the TF1.7 integrated version) seems somehow different from the TRT original C++ INT8 API. \r\n\r\nSo let me conclude my questions:\r\n1) Is the TF integrated TRT INT8 optimization in Python has a different back-end process compared from the independent TRT INT8 optimization in C++?\r\n2) Could you share a hint of how does the python version INT8 calibration is realized? Shall the calibration data be generated so that can be loaded in later inference?\r\n\r\n### Source code / logs\r\nPlease refer to the test:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/test/test_tftrt.py\r\n\r\n@samikama ", "comments": ["@oscarriddle , \r\nYou can't have INT8 without calibration. It should fail. Are you sure that you are running an int8 graph? \r\n\r\nAlso TFTRT int8 is a bit more complicated than TRT only int8. In native TRT(C++ version you mention) you are calibrating a graph end-to-end and int8 calibration TRT tries the minimize the information loss between input and output but still introduces some small errors as you describe. In TFTRT you are likely converting a subgraph rather than full graph, meaning there are TF native nodes in the graph after TRT graph. Since the calibration can only minimize the information loss in the engine, the errors introduced at the output of the engine due to calibration, may, and most likely will be, amplified by the downstream nodes. If you want to minimize this, you will need to do calibration aware training. We are thinking on how can we simplify that and have some ideas to test. \r\n\r\nAs for the answers,\r\n1) No, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?\r\n2) In TFTRT, calibration data is not serialized. It is consumed immediately. When you create a calibration graph, a calibrator is created and attached to the graph. When you run the calibration graph data is passed to the calibrator to collect calibration data and when you convert calibration graph to inference graph, nodes in calibration graph is taken out and replaced with TRT engine. If you save inference graph, it will always use the calibration you made and it doesn't require further calibration.\r\n\r\nCheers,\r\nSami", "Hi @samikama ,\r\nThanks for your detailed reply, it indeed help clarified the answer to my questions.\r\n\r\n> No, they have the same calibration scheme. Likely the calibrated parts of the graph are different. Did you check how much of the graph is converted to TRT engine?\r\n\r\nMy model is InceptionV3 that contains about 800 nodes, and after the TFTRT optimization, nodes number shrunk to about 7~10 (varies if I set an internal nodes to be the output node).\r\n\r\nJust for now, I think the TFTRT INT8  seems still have some unaddressed issues that would cause a big sacrifice on the accuracy (Not sure whether because a higher efficient models have a higher information entropy per bit). \r\n\r\nAnother plan is to just use the FP32 mode TFTRT optimization, which has no accuracy sacrifice but with 50~70% throughput boost. However, I'm wondering whether TFTRT could optimize a graph partially. For example, if I set the parameter \"outputs\" in API \"create_inference_graph()\" to be an internal node, could the downstream nodes just be reserved? \r\n\r\nI think this would be practically useful. For instance, a InceptionV3 graph with a final part like \"GlobalPool(image feature in high dim)->dropout->Conv2d_1x1(FC Layer)->SpatialSqeeze(Classification result)->Softmax\". Not only the classification result, but also the GlobalPool layer result is useful too, since it represents the feature of a image in higher dimensions. \r\n\r\nI've just give a try to get both the GlobalPool and SpatialSqeeze results in one turn. My procedure is to extract the weights of Conv2d layer from original frozen model, then optimize the original graph from start to the GlobalPool, then use numpy methods to do the dropout, Conv2d, SpatialSqeeze calculation. \r\n\r\nIt's just a raw idea that I would like to share with you. \r\n\r\nThanks,\r\n\r\n\r\n", "@oscarriddle,\r\n\r\nAs I mentioned above, int8 engines in TF-TRT falls into partial quantization domain and requires you to do quantization aware training of your graph. It is a complicated process today and we are trying to find ways to simplify this for the users. However for advanced users or for the cases where entire graph replaced with a single TRTEngineOP, current quantization should not have too much accuracy loss, provided you can provide a diverse enough calibration set.", "I met the same problem,  the results of INT8 model seems to be wired."]}, {"number": 19139, "title": "Can't quantize nodes In while", "body": "I have similar errors with [#7162](https://github.com/tensorflow/tensorflow/issues/7162) when quantize_nodes for [Tensor2Tensor Transformer](https://github.com/tensorflow/tensor2tensor) model.\r\nIt's no problem after freeze_graph, it's also no problem with quantize_weights. But when I qutize_nodes, it will:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/add_1/eightbit' has inputs from different frames. The input 'transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/add_1_eightbit/transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/add_1/Enter/quantize' is in frame 'transformer/while/while_context'. The input 'transformer/while/body/parallel_0/body/decoder/layer_prepostprocess/layer_norm/mul_1/eightbit/requantize' is in frame ''.\r\n```\r\nI have tried TF1.4 and TF1.7, and the error is the same.\r\nCan anyone help me?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}]