[{"number": 55192, "title": "Add a root cause arg to the fallback converter", "body": "Please check https://github.com/keras-team/keras-cv/pull/146#issuecomment-1063974863\r\n\r\nAny improvement feedback on the `root_cause` message description and fix hints to the developer is appreciated.\r\n\r\nSide note: I am going to \"abuse\" the CI here a little bit as I'am not compiling and testing this locally cause the build overhead on every PR with the daily [LLVM updates](https://discuss.tensorflow.org/t/llvm-updates-and-bazel-cache/2060) and cause [I cannot use the nightly remote cache](https://github.com/tensorflow/build/issues/72)", "comments": ["/cc @wangpengmit", "Can we give a better self-explainable string to the \"avg\" developer?\r\nI think that the current strings are a little bit cryptic and we don't have any hints on what the user need to check/change to workaround the fallback.", "> Can we give a better self-explainable string to the \"avg\" developer? I think that the current strings are a little bit cryptic and we don't have any hints on what the user need to check/change to workaround the fallback.\r\n\r\nYeah good point. Even I don't understand the strings. I don't have a quick suggestion. Please file a bug against them so we can look into pfor's UX issue closer.", "> > Can we give a better self-explainable string to the \"avg\" developer? I think that the current strings are a little bit cryptic and we don't have any hints on what the user need to check/change to workaround the fallback.\r\n> \r\n> Yeah good point. Even I don't understand the strings. I don't have a quick suggestion. Please file a bug against them so we can look into pfor's UX issue closer.\r\n\r\nDone: https://github.com/tensorflow/tensorflow/issues/55425", "Is this ready to pull?", "@wangpengmit Can you check why feedback/copybara is failing?"]}, {"number": 55191, "title": "split and fix exception message corresponding to each condition", "body": "The message does not reflect the entire condition. Giving a more specific message to each part of the condition is more accurate.", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55191/checks?check_run_id=5496107286)."]}, {"number": 55190, "title": "Got this warning while using @tf.function on tensorflow_probability.", "body": "I am trying to train a reinforcment learning agent on BipedalWalker. To Speed things up I used @tf.function wrapper on my gradient calculations and then I got this warning.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04LTS\r\n- Device: Lenovo Legion Y540\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): v2.4\r\n- TensorFlow-Probability: v0.12.2\r\n- Python version: v3.9\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: Nvidia Geforce Gtx 1650 4GB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <bound method A3CAgent.act of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f20540f1940>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <bound method A3CAgent.act of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f20540f1940>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:From /home/himanshu/anaconda3/envs/myenv/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:298: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\r\nInstructions for updating:\r\n`scale_identity_multiplier` is deprecated; please combine it with `scale_diag` directly instead.\r\nWARNING:tensorflow:From /home/himanshu/anaconda3/envs/myenv/lib/python3.9/site-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDo not pass `graph_parents`.  They will  no longer be used.\r\n```\r\n\r\n**Describe the expected behavior**\r\n   Tensorflow should not give these warnings.\r\n\r\n\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): None\r\n\r\n**Standalone code to reproduce the issue**\r\n  Link to the jupyter notebook can be found [here](https://drive.google.com/file/d/1OzDI00TQ1U1g63UR7HxuKhEzAQ3nMwug/view?usp=sharing).\r\n\r\n\r\n", "comments": ["Hi @HimGautam ! Did you check in TF 2.8 version and tensorflow probability nightly? You can add this line at the beginning of code to disable the warnings. Thanks!\r\n`os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'`\r\n", "No, v2.8 is not supported by CUDA 10.1. So that's why I have not checked it.\r\n", "Can you switch to Cuda 11.2 and CudNN 8.1 for TF 2.8 and let us know whether the issue still persists.", "I tried but my machine when I install these versions of cuda and cudnn, tensorflow is not able to access gpu on my system.", "@HimGautam,\r\nThere are the Deprecation Warnings, which we can suppress using one of the below code snippets \r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\nimport tensorflow as tf\r\n```\r\nOR\r\n\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nimport tensorflow as tf\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Ok got it, I think when I will update to higher versions of tensorflow then these warning will go away automatically. Thanks for the assist.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55190\">No</a>\n"]}, {"number": 55189, "title": "tensorflow training: result is different if model passed as argument to training loop", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes and no. the training loops are stock but the rest of code is not\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): microsoft windows 10 pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: NVIDIA QUATRO RTX 4000 with max-q design\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have experimented with training a simple tensorflow model using two scenarios: passing my model to my training loop (and to the subfunctions which are called from training loop), versus not passing my model to the training loop. The two cases result in different results. When passing my model to the training functions, the model is trained properly. But in the second scenario, something is wrong because the model is apparently not trained. I am baffled, and I wonder if it's a scope thing.\r\n\r\nTo be more specific, my setup involves dynamically creating a new model of larger size (adding some layers at each iteration of a for loop), and then training the resulting model. As stated before, I train the model in two scenarios: passing the model to training subfunctions and not doing so, and I obtain different results depending on which one I do. I verify this by giving the model a test sample (class 0 MNIST images) and checking if the correct classification is output. The models trained by passing the model as an argument is trained correctly, but, if I do not do this, then only the first model created by the for loop is correctly trained, as verified by incorrect class predictions. Can this be explained?\r\n\r\n**Describe the expected behavior**\r\nTraining a model should not be different whether or not I pass the model as an argument to the custom training function.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\nno\r\n\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n`\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\nimport time\r\n\r\nepochs = 200\r\ninput_shape = (28,28,1)\r\nnum_classes=10\r\nbatch_size = 64\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\nx_val = x_train[-10000:]\r\ny_val = y_train[-10000:]\r\nx_train = x_train[:-10000]\r\ny_train = y_train[:-10000]\r\n\r\nx_train = np.expand_dims(x_train, -1)\r\nx_val = np.expand_dims(x_val, -1)\r\nx_test = np.expand_dims(x_test, -1)\r\nx_train = x_train.astype(\"float32\") \r\nx_test = x_test.astype(\"float32\") \r\nx_val = x_val.astype(\"float32\") \r\n\r\ny_test_sorted_args_0=np.where(y_test == 0)\r\nx_test_0=x_test[y_test_sorted_args_0]\r\ny_test_0=np.full( (x_test_0.shape)[0],0)\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\r\n\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\nval_dataset = val_dataset.batch(batch_size)\r\n\r\nacc_metric = keras.metrics.SparseCategoricalAccuracy()\r\nval_acc_metric = keras.metrics.SparseCategoricalAccuracy()\r\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\r\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n    \r\n@tf.function\r\ndef train_step(x, y):\r\n    with tf.GradientTape() as tape:\r\n        mod_output = model(x, training=True)\r\n        loss_value = loss_fn(y, mod_output)\r\n    grads = tape.gradient(loss_value, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n    acc_metric.update_state(y, mod_output)\r\n    return loss_value\r\n\r\n@tf.function\r\ndef test_step(x, y):\r\n    val = model(x, training=False)\r\n    acc_metric.update_state(y, val)\r\n\r\ndef train( epochs):\r\n    for epoch in range(epochs):\r\n        print(\"\\nStart of epoch %d\" % (epoch,))\r\n        start_time = time.time()\r\n        \r\n        # Iterate over the batches of the dataset.\r\n        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\r\n            loss_value = train_step( x_batch_train, y_batch_train)\r\n    \r\n            # Log every 200 batches.\r\n            if step % 200 == 0:\r\n                print(\r\n                    \"Training loss (for one batch) at step %d: %.4f\"\r\n                    % (step, float(loss_value))\r\n                )\r\n                print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\r\n    \r\n        # Display metrics at the end of each epoch.\r\n        train_acc = acc_metric.result()\r\n        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\r\n    \r\n        # Reset training metrics at the end of each epoch\r\n        acc_metric.reset_states()\r\n    \r\n        # Run a validation loop at the end of each epoch.\r\n        for x_batch_val, y_batch_val in val_dataset:\r\n            test_step(x_batch_val, y_batch_val)\r\n    \r\n        val_acc = acc_metric.result()\r\n        val_acc_metric.reset_states()\r\n        print(\"Validation acc: %.4f\" % (float(val_acc),))\r\n        print(\"Time taken: %.2fs\" % (time.time() - start_time))\r\n\r\nmax_hidden=7\r\nfor num_hidden_layers in range(1,max_hidden,3):    \r\n    model1 = keras.Sequential(\r\n        [\r\n            keras.Input(shape=input_shape),\r\n            layers.Flatten(),\r\n        ]\r\n    )\r\n    \r\n    for i in range(1, num_hidden_layers+1):\r\n        model1.add(layers.Dense(150, activation=\"relu\"))\r\n    model1.add(layers.Dense(num_classes, activation=\"softmax\"))\r\n    \r\n    model=model1\r\n    \r\n    train(epochs)\r\n    \r\n    #verify that the model is properly trained by checking that the model correclty predicts images from class 0.  \r\n    #The more class 0 predictions we have, the better.\r\n    for sample_index in range(0,10): \r\n        x_sample=x_test_0[sample_index,:,:,:]\r\n        x_sample=np.expand_dims(x_sample, 0)\r\n        print(tf.math.argmax(model(x_sample),axis=1))\r\n    time.sleep(1)\r\n        \r\n`\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["For some reason I cannot properly add my code to this log. Some of it is correctly recognized and put in gray, but a lot of it is not", "@animalcroc  We see that you are using older version of TF (2.1) which is not actively supported.We recommend you to kindly upgrade to 2.4 or later versions and let us know if the issue still persists ?Thanks!", "I will update you soon", "@animalcroc Could you please let us know if there is any update on this issue?\r\nThanks!", "Yes, I will tomorrow. Thanks for checking with me.", "I have an update. I ran the code on tensorflow 2.4, and I get the same issue as I did on 2.1. ", "@animalcroc \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I included code in my original post, but the formatting is not correct. Only part of the code is gray, and the rest is in text format. I can't fix the formatting even though I try", "@animalcroc \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "I just posted there. Here is the link: https://github.com/keras-team/keras/issues/16240", "@animalcroc Thank you for the confirmation!\r\nCan you please move this issue to closed status as we will track the other ticket there.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55189\">No</a>\n", "Closed per request from sushreebarsa"]}, {"number": 55188, "title": "Inconsistent behavior betweek tf.nn.leaky_relu and keras.layers.LeakyReLU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen giving `tf.nn.leaky_relu()` a `-np.Inf` array and set the `alpha=0.0`. `tf.nn.leaky_relu` will output `nan`. However, since `-np.Inf` is still a valid array, would it be more reasonable that leaky_relu output 0? For comparison, I also check the result of `keras.layers.LeakyReLU` and set `alpha=0.0`, `keras.layers.LeakyReLU` will output 0 instead of `nan`.\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease run below code or directly access this colab link: https://colab.research.google.com/drive/1ILYlPKkKlbLcPNq9VTEBJTlHURGBbMG2?usp=sharing\r\n```\r\nimport keras\r\nimport tensorflow as tf\r\ninput_shape = (1, 3, 3, 3)\r\nx = keras.layers.Input(input_shape[1:])\r\ny = keras.layers.LeakyReLU(alpha=0.0)(x)\r\nmodel = keras.Model(x,y)\r\nmodel.summary()\r\nimport numpy as np\r\nx = np.random.rand(*input_shape)\r\nx *= -np.Inf\r\nkeras_output = model.predict(x)\r\nprint(\"Keras's result\", keras_output)\r\n\r\ntf_output = tf.nn.leaky_relu(x, alpha=0.0).numpy()\r\nprint(\"TensorFlow's result\", tf_output)\r\n```\r\nProgram output:\r\n```\r\nKeras's result [[[[0. 0. 0.]\r\n   [0. 0. 0.]\r\n   [0. 0. 0.]]\r\n\r\n  [[0. 0. 0.]\r\n   [0. 0. 0.]\r\n   [0. 0. 0.]]\r\n\r\n  [[0. 0. 0.]\r\n   [0. 0. 0.]\r\n   [0. 0. 0.]]]]\r\nTensorFlow's result [[[[nan nan nan]\r\n   [nan nan nan]\r\n   [nan nan nan]]\r\n\r\n  [[nan nan nan]\r\n   [nan nan nan]\r\n   [nan nan nan]]\r\n\r\n  [[nan nan nan]\r\n   [nan nan nan]\r\n   [nan nan nan]]]]\r\n```\r\n", "comments": ["Moreover, I have also tried sending -Inf value to `tf.nn.relu` and `tf.nn.relu` can output 0.0 instead of NaN.", "@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6770078abca8dff5207f47152ed9b53f/55188.ipynb).", "For Leaky relu:\r\n```\r\n  f(x) = alpha * x if x < 0\r\n  f(x) = x if x >= 0\r\n```\r\nAccording to the law of Zero, anything multiplied by zero is zero. \r\nIn your example `Alpha(0.0)` which is multiplied by -`inf` outputs zero which is showing correctly in `tf.keras.LeakyRelu`.\r\nBut the same situation according to the law of infinity, anything multiplied by `infinity` is `infinity`, considering both the above cases this situation becomes undefined and `nan` output in `tf.nn.leaky_relu`  is also correct.\r\n\r\nConsidering example for `tf.nn.relu` the condition is:\r\n```\r\n  f(x) = 0 if x < 0\r\n  f(x) = x if x >= 0\r\n```\r\n\r\nThe above explained ambiguity situation doesn't occur for this.\r\n\r\nYou can try any value other than zero for Alpha to get the consistent behavior.\r\n\r\n```\r\nx = np.random.rand(*input_shape)\r\nx *= -np.Inf\r\nlayer = keras.layers.LeakyReLU(alpha=0.1)\r\noutput = layer(x)\r\nlist(output.numpy())\r\n<[array([[[-inf, -inf, -inf],\r\n         [-inf, -inf, -inf],\r\n         [-inf, -inf, -inf]],\r\n \r\n        [[-inf, -inf, -inf],\r\n         [-inf, -inf, -inf],\r\n         [-inf, -inf, -inf]],\r\n \r\n        [[-inf, -inf, -inf],\r\n         [-inf, -inf, -inf],\r\n         [-inf, -inf, -inf]]], dtype=float32)]>\r\n\r\ntf.nn.leaky_relu(x,alpha=0.1).numpy()\r\n\r\n<array([[[[-inf, -inf, -inf],\r\n         [-inf, -inf, -inf],\r\n         [-inf, -inf, -inf]],\r\n\r\n        [[-inf, -inf, -inf],\r\n         [-inf, -inf, -inf],\r\n         [-inf, -inf, -inf]],\r\n\r\n        [[-inf, -inf, -inf],\r\n         [-inf, -inf, -inf],\r\n         [-inf, -inf, -inf]]]])>\r\n```\r\n", "Hi @sachinprasadhs,\r\nThanks for your reply! I understand that in general, these two sentences hold:\r\n```\r\nanything multiplied by zero is zero\r\nanything multiplied by infinity is infinity\r\n```\r\nHowever, when `0*Infinity` the result is **_indeterminate_**, that means the result of `0*Infinity` can be anything. Here are some links that support this argument:\r\nhttps://www.quora.com/What-is-0-times-infty-Why-isn%E2%80%99t-it-defined\r\nhttps://www.youtube.com/watch?v=bSZiEZzwB7Q\r\n\r\nI guess this is why `tf.nn.leaky_relu` outputs `nan`. I also test `0*np.Inf` in numpy and the result is also `nan`\r\n```\r\nimport numpy as np\r\n0*np.Inf\r\n```\r\n\r\nTo further check why `keras.layers.LeakyReLU` output 0 instead, I take a look at its code and find out that it actually does not perform the `0*Infinity` operation when the `alpha=0`\r\nhttps://github.com/keras-team/keras/blob/dd0f1f480613ceffd994658e5632d54c660d1e73/keras/backend.py#L4964\r\n\r\n```\r\n  dtype = getattr(x, 'dtype', floatx())\r\n  if alpha != 0.:  # this branch will not covered\r\n    if max_value is None and threshold == 0:\r\n      return tf.nn.leaky_relu(x, alpha=alpha)\r\n\r\n    if threshold != 0:\r\n      negative_part = tf.nn.relu(-x + threshold)\r\n    else:\r\n      negative_part = tf.nn.relu(-x)\r\n\r\n  clip_max = max_value is not None\r\n```\r\n\r\nTherefore, I guess this issue is a potential bug inside Keras instead of TensorFlow. I will post an issue in the keras repository", "the bug isn't inside keras it's a reaction to the vast framework of indirect input of domain localization of data input. its a simple fix giving the right amount of time and handle with none subjective actions on its module plug-in", "This issue will be taken care in the keras-team/keras repo and `tf.nn.leaky_relu` is working as intended. Could you please move this issue to closed. Thanks!", "Thanks for reminding me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55188\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55188\">No</a>\n"]}, {"number": 55187, "title": "Will 64-bit support ever be added to the TPU driver API?", "body": "Hi, this is my first GitHub issue so please let me know if this is an innapropriate place to ask this question.\r\n\r\nWhen I try to create an array of type `float64` in JAX I get the following message:\r\n\r\n`RuntimeError: INVALID_ARGUMENT: 64-bit data types are not yet supported on the TPU driver API. Convert inputs to float32/int32_t before using.`\r\n\r\nwhich from what I can tell is from the file `tensorflow/compiler/xla/python/tpu_driver/client/tpu_client.cc` in the Tensorflow repo, which I assume is part of the TPU driver API code. The verbiage here suggests this support may one day be added to the TPU driver API, so I'd like to know if this is something that's planned to be added in the future. If it is planned, is there an ETA on when this feature may be available?\r\n\r\nFor a university project I'm trying to implement a Mersenne prime number search program which runs on the TPU. These calculations are very sensitive to precision errors, and we've found that although we may be able to use the TPU to improve performance over existing GPU-based programs, the precision of `bfloat16` or even `float32` are too low for the calculations we're trying to run.", "comments": ["Hi @WillFarris ! Can you please share a simple stand alone code to replicate this issue? ", "@mohantym sure thing, running the code below in a Google Colab notebook with the TPU runtime selected should produce the error:\r\n```\r\nimport jax\r\nimport jax.tools.colab_tpu\r\njax.tools.colab_tpu.setup_tpu()\r\njax.config.update(\"jax_enable_x64\", True)\r\nx = jax.numpy.array([1, 2, 3, 4], dtype=jax.numpy.float64) # Should throw an error on Colab\r\n```\r\n\r\n\r\nThe error message:\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n[<ipython-input-1-a3700b59f8b9>](https://localhost:8080/#) in <module>()\r\n      3 jax.tools.colab_tpu.setup_tpu()\r\n      4 jax.config.update(\"jax_enable_x64\", True)\r\n----> 5 x = jax.numpy.array([1, 2, 3, 4], dtype=jax.numpy.float64) # Should throw an error on Colab\r\n\r\n10 frames\r\n\r\n[/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py](https://localhost:8080/#) in _device_put_array(x, device)\r\n    671   if x.dtype is dtypes.float0:\r\n    672     x = np.zeros(x.shape, dtype=np.dtype(bool))\r\n--> 673   return (backend.buffer_from_pyval(x, device),)\r\n    674 \r\n    675 def _device_put_scalar(x, device):\r\n\r\nRuntimeError: INVALID_ARGUMENT: 64-bit data types are not yet supported on the TPU driver API. Convert inputs to float32/int32_t before using.\r\n```", "@WillFarris ! I think this issue will suit more in [Jax repo ](https://github.com/google/jax/issues).  Could you please post your query there?", "Sure thing, thanks! Should I go ahead and close this issue then?", "Ok @WillFarris . Moving this to closed status. Thanks!", "It appears the answer is no, this feature is not planned. See the issue below for details.\r\n\r\nhttps://github.com/google/jax/issues/9862"]}, {"number": 55186, "title": "tensorflow core while do stdthread create", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (yes):\r\n- OS Platform and Distribution (e.g., Linux Red Hat 4.8.5-16) (GCC) ):\r\n- Mobile device (no) if the issue happens on mobile device:\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (use command below):1.15.0\r\n- Python version:no\r\n- Bazel version (3.1.0):\r\n- GCC/Compiler version (5.4.0):\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwhile I load savedmodel,tensorflow core dump,the detail gdb info at below.\r\n\r\n**Describe the expected behavior**\r\nload saved model success\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs**\r\ncore dump stack info:\r\n#0  0x0000000000000000 in ?? ()\r\n#1  0x00007f201fbfbaf3 in __gthread_create (__args=<optimized out>, __func=0x7f201fbfb990 <std::execute_native_thread_routine(void*)>, __threadid=0x2ab77d8)\r\n    at /usr/gcc-5.4.0/gcc-build/x86_64-unknown-linux-gnu/libstdc++-v3/include/x86_64-unknown-linux-gnu/bits/gthr-default.h:662\r\n#2  std::thread::_M_start_thread (this=0x2ab77d8, __b=...) at ../../../../../libstdc++-v3/src/c++11/thread.cc:149\r\n#3  0x00007f2016dc92c1 in std::thread::thread<std::function<void ()>&>(std::function<void ()>&) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#4  0x00007f2016dc8027 in tensorflow::(anonymous namespace)::StdThread::StdThread(tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void ()>) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#5  0x00007f2016dc83d6 in tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void ()>) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#6  0x00007f2016db46b5 in tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#7  0x00007f2016db517b in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tensorflow::thread::EigenEnvironment) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#8  0x00007f2016db1c9c in tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, Eigen::Allocator*) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#9  0x00007f2016850084 in tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&, int, tensorflow::Allocator*)\r\n    () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#10 0x00007f201684f9f9 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#11 0x00007f20168ca7d1 in tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>, tensorflow::DeviceLocality const&, tensorflow::Allocator*)\r\n    () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#12 0x00007f20168cbcb6 in absl::memory_internal::MakeUniqueResult<tensorflow::ThreadPoolDevice>::scalar absl::make_unique<tensorflow::ThreadPoolDevice, tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_---Type <return> to continue, or q <return> to quit---\r\n, long long>, tensorflow::DeviceLocality, tensorflow::Allocator*>(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>&&, tensorflow::DeviceLocality&&, tensorflow::Allocator*&&) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#13 0x00007f20168cb92a in tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#14 0x00007f20167f3272 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#15 0x00007f2015eb4a5d in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#16 0x00007f20168aa054 in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#17 0x00007f200f324559 in tensorflow::(anonymous namespace)::LoadMetaGraphIntoSession(tensorflow::MetaGraphDef const&, tensorflow::SessionOptions const&, std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >*) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#18 0x00007f200f32617a in tensorflow::(anonymous namespace)::LoadSavedModelInternal(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::SavedModelBundle*) () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#19 0x00007f200f3268e1 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::SavedModelBundle*) () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#20 0x00007f200f10a683 in fg_framework::TFGraphModel::LoadFromSavedModelBundle(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >---Type <return> to continue, or q <return> to quit---\r\n const&, fg_framework::LocalModelBundle*) () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#21 0x00007f200f10ac77 in fg_framework::TFGraphModel::Load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()\r\n   from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n#22 0x00007f200f1077ad in fg_framework::FeatureGenerator::InitFgConfig(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /.../bazel-bin/src/lib/lib_common/xxx.so\r\n", "comments": ["@goddie1 \r\nWe see that you are using old version of tensorflow 1,x(1.15)  which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Thanks!", "ok,I will try  new version.", "@goddie1 Could you please move this issue to closed status if it is resolved for you ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55186\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55186\">No</a>\n"]}, {"number": 55183, "title": "Targets in `tensorflow/core/kernels/fuzzing` directory.", "body": "Hello. I'm trying to build fuzzing targets from `tensorflow/core/kernels/fuzzing` directory, and by default all targets from there are built as libraries. Was there some idea in this way of building, and is there any way to build them not like a library?\r\n", "comments": ["The question is, how to link build .so or .lo files with libfuzzer, tried something like `clang -o bin -fsanitize=fuzzer libdecode_csv_fuzz_lib.so`, but got linking errors. How it was conceived to fuzz with such targets?", "Hi.\r\n\r\nThese targets build internally with additional configs. We have integrated them in OSSFuzz in the past, see https://github.com/google/oss-fuzz/pull/1937 for the additional things we've done to make it buildable in OSS.\r\n\r\nWe are in the process of migrating these to something that can be properly built.", "@mihaimaruseac\r\nHi. We have already found this patch and we succeeded to build these targets. We added `cc_test` like you did in this patch and added `linkopts = [\"-fsanitize=fuzzer,address\"]` into it. Also we used `-config=monilithic`, `-dynamic_mode=off` and added `libclang_rt.asan_cxx-x86_64.a` and `libclang_rt.asan-x86_64.a` to link options and succeeded to build targets only with `bazel build` command without linking targets by ourselves after linking fail (so we don't have linking fail). The only problem is `decode_base64` target, we decided not to build it, because it is linking too long.\r\nTo minimize logs from TF we used environment variable `TF_CPP_MIN_LOG_LEVEL=3`.", "Awesome.\r\n\r\nWe're planning to move these to rules that are better suited for OSS development, but it would take some time"]}, {"number": 55182, "title": "You have two different versions of our example for tf.transpose(x) , only one of your tutorial examples  of the exact same thing is correct!", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nTutorial Links:\r\nCorrect example of x @ tf.transpose(x)\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/transpose?authuser=2\r\n\r\nIncorrect example of x @ tf.transpose(x)\r\nhttps://www.tensorflow.org/guide/basics?authuser=2\r\n\r\n## Description of issue (what needs changing):\r\nNeed to decide which result it correct when performing the x @ tf.transpose(x)\r\n\r\n### Clear description\r\nYou have two different outcomes for the same x @ tf.transpose(x) [command]\r\nYou can't have it both ways.\r\n", "comments": ["@SynapticHorizons,\r\n\r\nSince `python >= 3.5` the `@` operator is supported (see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow, it simply calls the [tf.matmul()](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)\r\n\r\n>Incorrect example of x @ tf.transpose(x)\r\nhttps://www.tensorflow.org/guide/basics?authuser=2\r\n\r\n No, here the example describes `matmul` operation between `x` and `tf.transpose(x)`. Please find the gist [here](https://colab.research.google.com/gist/chunduriv/23ce2ccf04a43aad61084fb25677d037/55182.ipynb) for reference. \r\n\r\nFor more information please refer [here](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul).Thanks!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Chunduriv\nThanks for your response on the TensorFlow new syntax\ndoug\n\nFrom: chunduriv ***@***.***>\nSent: Thursday, March 10, 2022 9:55 AM\nTo: tensorflow/tensorflow ***@***.***>\nCc: SynapticHorizons ***@***.***>; Mention ***@***.***>\nSubject: Re: [tensorflow/tensorflow] You have two different versions of our example for tf.transpose(x) , only one of your tutorial examples of the exact same thing is correct! (Issue #55182)\n\n\n@SynapticHorizons<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FSynapticHorizons&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052393512%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=%2F1HeZrRHlU7eBshNrvYYvjJ3YYIefMaUwK1uwEJZD1M%3D&reserved=0>,\n\nSince python >= 3.5 the @ operator is supported (see PEP 465<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.python.org%2Fdev%2Fpeps%2Fpep-0465%2F&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052393512%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=LDGmOYu0uYVK3y5%2F2B5N5L%2Bv4oP5uWBp5DxEu%2Fl3mis%3D&reserved=0>). In TensorFlow, it simply calls the tf.matmul()<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.tensorflow.org%2Fapi_docs%2Fpython%2Ftf%2Flinalg%2Fmatmul&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052393512%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=vb0eakwbj15l3v%2BlcYYmey3JPZqO3MfZwKJYakcIybM%3D&reserved=0>\n\nIncorrect example of x @ tf.transpose(x)\nhttps://www.tensorflow.org/guide/basics?authuser=2<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.tensorflow.org%2Fguide%2Fbasics%3Fauthuser%3D2&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052393512%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=lAAxsJbWqAyb%2BJl%2FQMb%2BwRjU78RbDTbwAza9KLYcXRg%3D&reserved=0>\n\nNo, here the example describes matmul operation between x and tf.transpose(x). Please find the gist here<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcolab.research.google.com%2Fgist%2Fchunduriv%2F23ce2ccf04a43aad61084fb25677d037%2F55182.ipynb&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052393512%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=AlgujFFP5p7Bw96y4SPjYrUQe3qS%2BpImSQ9mpcNfx7M%3D&reserved=0> for reference.\n\nFor more information please refer here<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.tensorflow.org%2Fapi_docs%2Fpython%2Ftf%2Flinalg%2Fmatmul&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052549733%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=Wwu%2BZwt7WmqoSEBf38J7x7eLcKImOYC2QCeykerogvk%3D&reserved=0>.Thanks!\n\n-\nReply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F55182%23issuecomment-1064148837&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052549733%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=NVqQHgiKalgHvoD6UBas7xiSLk5ysjSgbVgt7jDpuXw%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAMIIACJXBBKTYUNCRX3U7WTU7IEMLANCNFSM5QJYTVEA&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052549733%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=RVd7%2B42Nbf0CGG%2BcND3a29VakLmHVNAuR5EiZiOPeR0%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052549733%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=q7253Pq0EJNjGU8SADokCffWBXp0yu0aeY0rgTCzwjc%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7C%7C8d9e6bb37f53458c9c3508da02a5f45a%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637825209052549733%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=aE%2Ff2CkXWm6SpkKx2vUebcCdEfCgHUlIqC3utGA24wE%3D&reserved=0>.\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\n", "@SynapticHorizons,\r\n\r\nIf your issue resolved. Please feel free to close. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55182\">No</a>\n"]}, {"number": 55179, "title": "[TfLite] C++ tflite::InterpreterBuilder is missing in version r1.12", "body": "Hi, I'm using the tflite library from version [r1.12](https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite). However, I followed the guide in [official doc](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter):\r\n\r\n```\r\n// Create model from file. Note that the model instance must outlive the\r\n// interpreter instance.\r\nauto model = [tflite::FlatBufferModel::BuildFromFile](https://tensorflow.google.cn/lite/api_docs/cc/class/tflite/flat-buffer-model#classtflite_1_1_flat_buffer_model_1aa38cad2bc62ae19a647c64f10226862b)(...);\r\nif (model == nullptr) {\r\n  // Return error.\r\n}\r\n// Create an [Interpreter](https://tensorflow.google.cn/lite/api_docs/cc/class/tflite/interpreter#classtflite_1_1_interpreter) with an [InterpreterBuilder](https://tensorflow.google.cn/lite/api_docs/cc/class/tflite/interpreter-builder#classtflite_1_1_interpreter_builder).\r\nstd::unique_ptr interpreter;\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nif (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {\r\n  // Return failure.\r\n}\r\nif (interpreter->AllocateTensors() != kTfLiteOk) {\r\n  // Return failure.\r\n}\r\n```\r\n\r\nIt seems that the `interpreterBuilder.h` file is missing in the r1.12 version. In this case, what should I do to create an interpreter with a specified model in r1.12? Thanks.\r\n", "comments": ["@BILLXZY1215 We could see you are using tflite under contrib directory which is not actively supported, and `contrib` is `depreciated`. We recommend you to kindly  use the latest version.Thanks!", "Okay, I see, thanks!", "@BILLXZY1215 Could you please move this issue to closed  status if it is resolved for you?\r\nThanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "I have found class `InterpreterBuilder` in r1.12, it's hardcoded in `model.h`. Thanks again."]}, {"number": 55178, "title": "tf.distribute.MultiWorkerMirroredStrategy() not getting initialized(RuntimeError: Collective ops must be configured at program startup)", "body": "I am running a training job on vertex AI. \r\nVersions:\r\nTF = 2.7\r\npython = 3.8\r\n\r\nThe model runs fine for mirrored strategy where we have only one node with multiple gpu attached to it.\r\n\r\nFor MultiWorkerMirroredStrategy I am using\r\nchief - n1 32 with 4V100 gpu - count 1(default)\r\nworker n1 32 with 4V100 gpu - count 1\r\n\r\nWhen I am trying to run the code using multiple nodes using MultiWorkerMirroredStrategy, It is giving following error\r\n\r\nRuntimeError: Collective ops must be configured at program startup\r\n\r\nI found some suggestions to put the strategy at the program beginning  but that also didn't help.\r\nVertex AI is setting the TF_CONFIG correctly.\r\nBut it is not able to instantiate the MultiWorkerMirroredStrategy\r\n\r\nThe stack trace\r\n\r\n mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\"\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 348, in new_func\"\r\n\r\nworkerpool0-0\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 253, in __init__\"\r\n\" super(_CollectiveAllReduceStrategyExperimental,\"\r\nworkerpool0-0\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 186, in __init__\"\r\nworkerpool0-0\r\n\" CollectiveAllReduceExtended(\"\r\nworkerpool0-0\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 330, in __init__\"\r\nworkerpool0-0\r\n\" self._initialize_strategy(self._cluster_resolver)\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 342, in _initialize_strategy\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\" self._initialize_multi_worker(cluster_resolver)\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 463, in _initialize_multi_worker\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\" context.context().configure_collective_ops(\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\" File \"/opt/conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 817, in configure_collective_ops\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\" raise RuntimeError(\"Collective ops must be configured at program startup\")\"\r\nError\r\n2022-03-09 09:57:09.622 IST\r\nworkerpool0-0\r\n\"RuntimeError: Collective ops must be configured at program startup\"", "comments": ["Can you put a breakpoint before this line `mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()` and check if any gpu memory is allocated already by TF? if so you accidentally created a tf.Var or Tensor which you are not allowed to before creating MultiWorkerMirroredStrategy", "Instantiating the strategy is first line of code, I don't think there are any tensors created before.", "@ranvirdesai, \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@andre-bauer Thanks for the reply, any other issue you could think of....", "@chunduriv \r\nIts just this code raises error while initializing strategy.\r\n\r\nif __name__==\"__main__\":\r\n    mirrored_strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n\r\n", "This is not the first line of code... Did you check the GPU memory at this line? \r\n\r\nyou could have \r\n```\r\ndef do_something(var_name=tf.constant(0)):\r\n   pass\r\n```\r\nsomewhere above main or in any imported module and it would already fail ", "@ranvirdesai,\r\n\r\nCould you please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/55178#issuecomment-1063131241) and let us know ?Thanks!", "@chunduriv @andre-bauer \r\nThanks for the response.\r\n\r\nI tried running vertex ai custom job using suggested method.\r\nI initialized strategy right after importing tf, it did not give error but training was also not done.\r\ntraining was for only one epoch and takes 2min in my local, but I think it went in never ending loop on vertex ai.", "Make sure you have the proper configuration, follow [this](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration) document for reference. Thanks!", "@chunduriv @andre-bauer \r\nThe comment helped.\r\nI rearranged imports and it worked", "@ranvirdesai , If your issue is resolved, could you please close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55178\">No</a>\n"]}, {"number": 55177, "title": "[TF:TRT] Update test helpers to handle bool tensors and weights", "body": "Enable the handling of bool tensors for TF-TRT test helper subroutines.\r\n", "comments": ["@bixia1 : This is a substitute for [PR#54452](https://github.com/tensorflow/tensorflow/pull/54452) and I need it for converters for LogicalNot and other logical operators, which are almost ready.", "@bixia1 : All checks have passed. Could you, please, merge this one?", "@bixia1 : I'm fully prepared to submit a PR for `LogicalNot`, but it would be better if this one was merged.", "@gbaned: @bixia1 already approved these changes, but she is still mentioned as 1 pending reviewer.\r\nCould you, please, fix that?", "@gbaned : I am sorry, but @bixia1 already approved these changes twice.", "@drivanov this test fail, see\r\n[log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)", "> @drivanov this test fail, see [log](https://github.com/tensorflow/tensorflow/files/8265934/boolpr.log)\r\n\r\nFixed.", "This PR is already merged, see [here](https://github.com/tensorflow/tensorflow/commit/06833608a4eabdcd42133efedce009235df2f792) don't understand why the status shown here is not correct.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR.\r\n"]}, {"number": 55176, "title": "Fix out size eqn in atrous_conv2d doc", "body": "The following code can be found in [this colab notebook](https://colab.research.google.com/drive/1g9FNR-3g4KIxWdUw8q5hGJATFMAhKElU?usp=sharing). It shows that the equation for the height and width of the output of `tf.nn.atrous_conv2d` reported in the documentation is incorrect when `padding` is `'VALID'`, and demonstrates the correct equation, which this PR provides.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef out_size(in_size, filter_size, rate, padding):\r\n  value = tf.ones((1, in_size, in_size, 1))\r\n  filters = tf.ones((filter_size, filter_size, 1, 1))\r\n  actual_out = tf.nn.atrous_conv2d(value, filters, rate, padding)\r\n  actual_out_size = tf.shape(actual_out)[1]\r\n  if padding == 'VALID':\r\n    current_doc = in_size - 2 * (filter_size - 1)\r\n    proposed_doc = in_size - rate * (filter_size - 1)\r\n  else: # padding == 'SAME'\r\n    current_doc = in_size\r\n    proposed_doc = in_size\r\n  return actual_out_size, current_doc, proposed_doc\r\n  \r\nfor padding in ('VALID', 'SAME'):\r\n  for in_size in (8, 9, 10, 11):\r\n    for filter_size in (1, 2, 3):\r\n      for rate in (1, 2, 3):\r\n        actual, current, proposed = out_size(in_size, filter_size, rate,\r\n                                             padding)\r\n        print(\"pad: %-5s, in: %2d, f: %d, r: %d / \"\r\n              \"actual: %2d, current doc: %2d, proposed doc: %2d\" %\r\n              (padding, in_size, filter_size, rate, actual, current, proposed))\r\n```\r\n\r\nOutput:\r\n```\r\npad: VALID, in:  8, f: 1, r: 1 / actual:  8, current doc:  8, proposed doc:  8\r\npad: VALID, in:  8, f: 1, r: 2 / actual:  8, current doc:  8, proposed doc:  8\r\npad: VALID, in:  8, f: 1, r: 3 / actual:  8, current doc:  8, proposed doc:  8\r\npad: VALID, in:  8, f: 2, r: 1 / actual:  7, current doc:  6, proposed doc:  7\r\npad: VALID, in:  8, f: 2, r: 2 / actual:  6, current doc:  6, proposed doc:  6\r\npad: VALID, in:  8, f: 2, r: 3 / actual:  5, current doc:  6, proposed doc:  5\r\npad: VALID, in:  8, f: 3, r: 1 / actual:  6, current doc:  4, proposed doc:  6\r\npad: VALID, in:  8, f: 3, r: 2 / actual:  4, current doc:  4, proposed doc:  4\r\npad: VALID, in:  8, f: 3, r: 3 / actual:  2, current doc:  4, proposed doc:  2\r\npad: VALID, in:  9, f: 1, r: 1 / actual:  9, current doc:  9, proposed doc:  9\r\npad: VALID, in:  9, f: 1, r: 2 / actual:  9, current doc:  9, proposed doc:  9\r\npad: VALID, in:  9, f: 1, r: 3 / actual:  9, current doc:  9, proposed doc:  9\r\npad: VALID, in:  9, f: 2, r: 1 / actual:  8, current doc:  7, proposed doc:  8\r\npad: VALID, in:  9, f: 2, r: 2 / actual:  7, current doc:  7, proposed doc:  7\r\npad: VALID, in:  9, f: 2, r: 3 / actual:  6, current doc:  7, proposed doc:  6\r\npad: VALID, in:  9, f: 3, r: 1 / actual:  7, current doc:  5, proposed doc:  7\r\npad: VALID, in:  9, f: 3, r: 2 / actual:  5, current doc:  5, proposed doc:  5\r\npad: VALID, in:  9, f: 3, r: 3 / actual:  3, current doc:  5, proposed doc:  3\r\npad: VALID, in: 10, f: 1, r: 1 / actual: 10, current doc: 10, proposed doc: 10\r\npad: VALID, in: 10, f: 1, r: 2 / actual: 10, current doc: 10, proposed doc: 10\r\npad: VALID, in: 10, f: 1, r: 3 / actual: 10, current doc: 10, proposed doc: 10\r\npad: VALID, in: 10, f: 2, r: 1 / actual:  9, current doc:  8, proposed doc:  9\r\npad: VALID, in: 10, f: 2, r: 2 / actual:  8, current doc:  8, proposed doc:  8\r\npad: VALID, in: 10, f: 2, r: 3 / actual:  7, current doc:  8, proposed doc:  7\r\npad: VALID, in: 10, f: 3, r: 1 / actual:  8, current doc:  6, proposed doc:  8\r\npad: VALID, in: 10, f: 3, r: 2 / actual:  6, current doc:  6, proposed doc:  6\r\npad: VALID, in: 10, f: 3, r: 3 / actual:  4, current doc:  6, proposed doc:  4\r\npad: VALID, in: 11, f: 1, r: 1 / actual: 11, current doc: 11, proposed doc: 11\r\npad: VALID, in: 11, f: 1, r: 2 / actual: 11, current doc: 11, proposed doc: 11\r\npad: VALID, in: 11, f: 1, r: 3 / actual: 11, current doc: 11, proposed doc: 11\r\npad: VALID, in: 11, f: 2, r: 1 / actual: 10, current doc:  9, proposed doc: 10\r\npad: VALID, in: 11, f: 2, r: 2 / actual:  9, current doc:  9, proposed doc:  9\r\npad: VALID, in: 11, f: 2, r: 3 / actual:  8, current doc:  9, proposed doc:  8\r\npad: VALID, in: 11, f: 3, r: 1 / actual:  9, current doc:  7, proposed doc:  9\r\npad: VALID, in: 11, f: 3, r: 2 / actual:  7, current doc:  7, proposed doc:  7\r\npad: VALID, in: 11, f: 3, r: 3 / actual:  5, current doc:  7, proposed doc:  5\r\npad: SAME , in:  8, f: 1, r: 1 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 1, r: 2 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 1, r: 3 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 2, r: 1 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 2, r: 2 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 2, r: 3 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 3, r: 1 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 3, r: 2 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  8, f: 3, r: 3 / actual:  8, current doc:  8, proposed doc:  8\r\npad: SAME , in:  9, f: 1, r: 1 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 1, r: 2 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 1, r: 3 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 2, r: 1 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 2, r: 2 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 2, r: 3 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 3, r: 1 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 3, r: 2 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in:  9, f: 3, r: 3 / actual:  9, current doc:  9, proposed doc:  9\r\npad: SAME , in: 10, f: 1, r: 1 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 1, r: 2 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 1, r: 3 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 2, r: 1 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 2, r: 2 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 2, r: 3 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 3, r: 1 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 3, r: 2 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 10, f: 3, r: 3 / actual: 10, current doc: 10, proposed doc: 10\r\npad: SAME , in: 11, f: 1, r: 1 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 1, r: 2 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 1, r: 3 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 2, r: 1 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 2, r: 2 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 2, r: 3 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 3, r: 1 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 3, r: 2 / actual: 11, current doc: 11, proposed doc: 11\r\npad: SAME , in: 11, f: 3, r: 3 / actual: 11, current doc: 11, proposed doc: 11\r\n```", "comments": ["Good catch! The documentation has been wrong for a long time."]}, {"number": 55175, "title": "Add link to padding notes to depthwise_conv2d doc", "body": null, "comments": []}, {"number": 55174, "title": "Silent reset_states() error in custom model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Monterey 12.1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.8\r\n- Python version: 3.9\r\n\r\n**Describe the current behavior**\r\nI am developing a custom model by inheriting from the `tf.keras.Model` class, in particular I am developing a custom LSTM residual network. Following the documentation and code, if calling `model.reset_states()` on a stateless layer should trigger an exception present in the layer class (in my case LSTM). \r\n\r\nHowever, such call is obscured by a conditional statement that prevents the raising of such exception in any case (at least the one I was trying). So basically model.reset_states() runs silently without errors and without resetting the states. \r\n\r\nThis is the function present inside tf.keras.Model class. The conditional statement prevents the condition from layer.reset_states() to be raised. By manually forcing the evaluation  of `layer.reset_states()` inside a debugger, the exception is correctly raised.\r\n\r\n```\r\n  def reset_states(self):\r\n    for layer in self.layers:\r\n      if hasattr(layer, 'reset_states') and getattr(layer, 'stateful', False):\r\n        layer.reset_states()\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nRaise the exception and make explicit whether reset_states() is called on layers that are not stateful\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```\r\nclass ResNetModel(Model):\r\n    def __init__(self, num_inputs, **kwargs):\r\n        \"\"\"\r\n        The class initialiser should call the base class initialiser, passing any keyword\r\n        arguments along. It should also create the layers of the network according to the\r\n        above specification.\r\n        \"\"\"\r\n        super(ResNetModel, self).__init__(**kwargs)\r\n        self.lstm_1 = tf.keras.layers.LSTM(units=32, input_shape=(None, num_inputs), return_sequences=True, stateful = False)\r\n        self.dense = tf.keras.layers.Dense(units=1, activation=None)\r\n\r\n    def call(self, inputs, training=False):\r\n        \"\"\"\r\n        This method should contain the code for calling the layer according to the above\r\n        specification, using the layer objects set up in the initialiser.\r\n        \"\"\"\r\n        x = self.lstm_1(inputs)\r\n        y = self.dense(x)\r\n        return y + inputs\r\n\r\nmodel = ResNetModel(1)\r\nmodel.reset_states() # does not raise any errors\r\n```\r\n", "comments": ["@mattiasu96, Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999. Thanks!", "Oh sorry, should I copy the issue to Keras then? ", "@mattiasu96,\r\n\r\nYes, please post your issue in https://github.com/keras-team/keras/issues repo and close this issue. Thanks!", "Done!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55174\">No</a>\n"]}, {"number": 55173, "title": "[PluggableDevice] Add DEVICE_DEFAULT registration for ZerosLike and OnesLike", "body": null, "comments": []}, {"number": 55172, "title": "Expose xla custom_call op to tensorFlow", "body": "Expose xla custom_call op to tensorFlow\n\nCurrently xla custom_call op can only be created indirectly from some tf ops.\nThis patch exposed this op to tensorflow directly and added the xla kernel to\nconvert to the xla custom call. This is an useful feature to preserve the high\nlevel tensorflow ops when tf2xla is applied.\n", "comments": []}, {"number": 55171, "title": "Add dtensor to the build tree of TensorFlow.", "body": "Add dtensor to the build tree of TensorFlow.\n", "comments": []}, {"number": 55170, "title": "[mhlo] Give names to RngBitGenerator outputs", "body": "[mhlo] Give names to RngBitGenerator outputs\n", "comments": []}, {"number": 55169, "title": "Fixing naming of metric to be consistent with style", "body": "Fixing naming of metric to be consistent with style\n", "comments": []}, {"number": 55168, "title": "Change xprof field \"time\" to \"time_fraction\", since that's what it represents.", "body": "Change xprof field \"time\" to \"time_fraction\", since that's what it represents.\n\nThis avoids confusing code that references time but is unit-less.\n", "comments": []}, {"number": 55167, "title": "Add captured_function.cc to libtensorflow.so.", "body": "Add captured_function.cc to libtensorflow.so.\n", "comments": []}, {"number": 55165, "title": "Avoid cached definition and signature in __init__ of eagerly created function", "body": "Avoid cached definition and signature in __init__ of eagerly created function\n\nWhen a concrete function is created but never called from Python, this change\nsaves us an extra copy of fdef. This tends to happen frequenctly in functions\nloaded from a saved model, where only the top-level 'inference' concrete\nfunctions are directly called from Python, whilst other supporting functions\nare called as part of the graph execution.\n", "comments": []}, {"number": 55164, "title": "Add GetReallyOpaqueDelegateCreators method to OpResolver.", "body": "Add GetReallyOpaqueDelegateCreators method to OpResolver.\nThis is like GetDelegateCreators except that it creates an opaque type TfLiteReallyOpaqueDelegate rather than TfLiteDelegate.\n", "comments": []}, {"number": 55163, "title": "lite: Add C API to support custom op", "body": "lite: Add C API to support custom op\n\nAdded new experimental C API to support custom op.\n\n- Since TfLiteRegistration has new field, make sure all code uses\n  TfLiteRegistration with initializer likes \"TfLiteRegistration reg{};\".\n  It makes \".registration_external' field initialized as nullptr.\n\n- 'custom_sinh.bin' is a simple graph with custom op \"Sinh\" which is generated\n  as the following.\n\n  @tf.function(\n    input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\n  def sin(x):\n    return tf.sinh(x)\n\n  converter = tf.lite.TFLiteConverter.from_concrete_functions([sin.get_concrete_function()], sin)\n  converter.allow_custom_ops = True\n  tflite_model = converter.convert()\n", "comments": []}, {"number": 55162, "title": "Implement parser/printer as member methods.", "body": "Implement parser/printer as member methods.\n\nSee https://reviews.llvm.org/D121090.\n", "comments": []}, {"number": 55161, "title": "Implement concatenate3", "body": "Implement concatenate3\n\nIntroduce helper xnn_define_concatenate_n to check and set up opdata for concatenate nodes of various number of inputs.\n", "comments": []}, {"number": 55160, "title": "[XLA:SPACE_TO_BATCH] Propagate trivial slices in transformed spatial", "body": "[XLA:SPACE_TO_BATCH] Propagate trivial slices in transformed spatial\ndimensions.\n", "comments": []}, {"number": 55159, "title": "Mieosis", "body": null, "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55159/checks?check_run_id=5469111554)."]}, {"number": 55158, "title": "Revert back a change that runs CreateGuaranteeAllFuncsOneUsePass pass unconditionally.", "body": "Revert back a change that runs CreateGuaranteeAllFuncsOneUsePass pass unconditionally.\n", "comments": []}]