[{"number": 45629, "title": "The current TensorFlow lite version cannot be built with Visual Studio 2017.", "body": "**System information**\r\n- OS Platform and Distribution: Windows SDK version 10.0.17763.0 to target Windows 10.0.19042\r\n- TensorFlow installed from (source): master, commit 67a9764695429fc5963aeeb905526bfe5af45739\r\n- TensorFlow version: 2.4?\r\n- CMake version: 3.18.5\r\n- Compiler version: Visual Studio 2017 (MSVC 19.16.27045.0)\r\n- CUDA/cuDNN version: 10.2, 11.1\r\n- GPU model and memory: GeForce 1080 Ti\r\n\r\n**Describe the problem**\r\nThe current TensorFlow lite version cannot be built with Visual Studio 2017.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nD:\r\nmkdir TF\r\ncd TF\r\ngit clone https://github.com/tensorflow/tensorflow.git src\r\nmkdir build\r\ncd build\r\ncmake -G \"Visual Studio 15\" -A x64 -D CMAKE_DEBUG_POSTFIX:STRING=d -D CMAKE_INSTALL_PREFIX:PATH=D:\\TF\\install -D TFLITE_ENABLE_GPU:BOOL=OFF -D GIT:FILEPATH=D:\\dev\\Tools\\GIT\\bin\\git.exe D:\\TF\\src\\tensorflow\\lite\r\nmsbuild INSTALL.vcxproj /maxcpucount /nodeReuse:false /nologo /p:Configuration=Debug\r\n```\r\n\r\n**Any other info / logs (translated from german)**\r\n```\r\n       \"D:\\TF\\build\\INSTALL.vcxproj\" (default target) (1) ->.\r\n       \"D:\\TF\\build_BUILD.vcxproj\" (default target) (3) ->.\r\n       \"D:\\TF\\build\\tensorflow-lite.vcxproj\" (default target) (4) ->\r\n       (ClCompile target) ->\r\n         D:\\TF\\src\\tensorflow-lite\\kernels\\elementwise.cc(299): fatal error C1001: Internal compiler error. [D:\\TF\\build\\tensorflow-lite.vcxproj]\r\n```\r\n\r\nProject file built with Visual Studio:\r\n```\r\n42>D:\\TF\\src\\tensorflow\\lite\\kernels\\elementwise.cc(299): fatal error C1001: Internal compiler error.\r\n42>(Compiler file \"msc1.cpp\", line 1518)\r\n42> Simplify or modify the program in the environment of the items listed above. Select\r\n42>Use the \"Technical Support\" command in the \"Help\" menu of Visual C++,\r\n42>or open the technical support help file for more information.\r\n42>INTERNAL COMPILER ERROR in \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX86\\x64\\CL.exe\".\r\n42> From the Visual C++ Help menu, click Technical Support,\r\n42> or open Technical Support Help for more information.\r\n```\r\n", "comments": ["Can you check our Visual Studio compatibility matrix for each TF version at https://www.tensorflow.org/install/source_windows#tested_build_configurations ?\r\n\r\nThanks", "Hi bhack,\r\n\r\nthe list gives an overview of TensorFlow, not TensorFlow lite. For lite a C++11 compiler is supposedly sufficient as it can be found e.g. in the Micro documentation (https://www.tensorflow.org/lite/microcontrollers#supported_platforms). Also, there is a CMake build instruction https://www.tensorflow.org/lite/guide/build_cmake without any restrictions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45629\">No</a>\n"]}, {"number": 45628, "title": "RNN Using TensorFlow 2.0", "body": "Please let us know anything wrong in below code, not getting desire result -\r\n\r\n```\r\nfrom numpy import sqrt\r\nfrom numpy import asarray\r\nfrom pandas import read_csv\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.layers import LSTM\r\nimport tensorflow as tf\r\nfrom sklearn import metrics\r\nfrom sklearn.model_selection import train_test_split\r\n```\r\n\r\nAssign the value as 40 to the variabel RANDOM_SEED which will be the seed value.\r\nSet the random seed value using the value stored in the variable RANDOM_SEED.\r\n\r\n```\r\nRANDOM_SEED = 40\r\ntf.random.set_seed(RANDOM_SEED)\r\n```\r\n\r\nsplit a univariate sequence into samples\r\n\r\n```\r\ndef split_sequence(sequence, n_steps):\r\n    X, y = list(), list()\r\n    for i in range(len(sequence)):\r\n        # find the end of this pattern\r\n        end_ix = i + n_steps\r\n        # check if we are beyond the sequence\r\n        if end_ix > len(sequence)-1:\r\n            break\r\n        # gather input and output parts of the pattern\r\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\r\n        X.append(seq_x)\r\n        y.append(seq_y)\r\n    return asarray(X), asarray(y)\r\n```\r\n\r\n\r\nRead the dataset airline-passengers.csv and give parameter index_col as 0 and save it in variable df.\r\n\r\n`df = read_csv(\"airline-passengers.csv\", index_col=0)\r\n`\r\n\r\nConvert the data type of the values dataframe df to float32 and save it in variable values.\r\nAssign the value 5 to the variable n_steps which is the window size.\r\nSplit the samples using the function split_sequence and pass the parameters values and n_steps and save it in variables X and y\r\n\r\n```\r\nvalues = df.values.astype('float32')\r\nn_steps = 5\r\nX, y = split_sequence(values, n_steps)\r\n```\r\n\r\nSplit the data X,y with the train_test_split function of sklearn with parameters test_size=0.33 and random_state=RANDOM_SEED.**\r\n\r\n`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_SEED)\r\n`\r\n\r\nConstruct a fully-connected network structure defined using dense class\r\nCreate a sequential model\r\nAdd a LSTM layer which has 200 nodes with activation function as relu and input shape as (n_steps,1).\r\nThe first hidden layer has 100 nodes and uses the relu activation function.\r\nThe second hidden layer has 50 nodes and uses the relu activation function.\r\nThe output layer has 1 node.\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM(200, activation='relu',  input_shape=(n_steps,1)))\r\nmodel.add(Dense(100, activation='relu'))\r\nmodel.add(Dense(\r\n50, activation='relu'))\r\nmodel.add(Dense(1))\r\n```\r\n\r\nWhile comipling the model pass the following parameters -\r\n-optimizer as Adam\r\n-loss as mse\r\n-metrics as mae\r\n\r\n`model.compile(optimizer='Adam', loss='mse', metrics=['mae'])`\r\n\r\nfit the model with X_train, y_train, epochs=350, batch_size=32,verbose=0.\r\n\r\n`model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=0)\r\n`\r\n\r\nPerform prediction on the test data (i.e) on X_test and save the predictions in the variable y_pred.\r\n\r\n`y_pred = model.predict( X_test)`\r\n\r\nCalculate the mean squared error on the variables y_test and y_pred using the mean_squared_error function in sklearn metrics and save it in variable MSE.\r\nCalculate the Root mean squared error on the variables y_test and y_pred by performing square root on the above result and save it in variable RMSE.\r\nCalculate the mean absolute error on the variables y_test and y_pred using the mean_absolute_error function in sklearn metrics and save it in variable MAE.\r\n\r\n```\r\nMSE  = metrics.mean_squared_error(y_test,y_pred)\r\nRMSE = sqrt(metrics.mean_squared_error(y_test,y_pred))\r\nMAE  = metrics.mean_absolute_error(y_test,y_pred)\r\nprint('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (MSE, RMSE,MAE))\r\n\r\n```\r\n\r\n**MSE: 665.522, RMSE: 25.798, MAE: 17.127 ... this we getting and it is wrong.**\r\n\r\n```\r\nwith open(\"MSE.txt\", \"w\") as text_file:\r\n        MSE=str(MSE)\r\n        text_file.write(MSE)\r\nwith open(\"RMSE.txt\", \"w\") as text_file:\r\n        RMSE=str(RMSE)\r\n        text_file.write(RMSE)\r\nwith open(\"MAE.txt\", \"w\") as text_file:\r\n        MAE=str(MAE)\r\n        text_file.write(MAE)\r\n# serialize model to JSON\r\nmodel_json = model.to_json()\r\nwith open(\"model.json\", \"w\") as json_file:\r\n    json_file.write(model_json)\r\n```\r\n\r\n[airline-passengers.zip](https://github.com/tensorflow/tensorflow/files/5650585/airline-passengers.zip)\r\n[RNN_Question.zip](https://github.com/tensorflow/tensorflow/files/5650599/RNN_Question.zip)\r\n\r\nAfter running code then we execute\r\n\r\n```\r\nfrom hashlib import md5\r\nf = open(\"MSE.txt\", \"r\")\r\ns=f.read()\r\ns=float(s)\r\ns=round(s,3)\r\nf1=open(\"RMSE.txt\",\"r\")\r\ns1=f1.read()\r\ns1=float(s1)\r\ns1=round(s1,3)\r\nf2=open(\"MAE.txt\",\"r\")\r\ns2=f2.read()\r\ns2=float(s2)\r\ns2=round(s2,3)\r\nif (md5(str(s).encode()).hexdigest() == '51ad543f7ac467cb8b518f1a04cc06af') and (md5(str(s1).encode()).hexdigest() == '6ad48a76bec847ede2ad2c328978bcfa') and (md5(str(s2).encode()).hexdigest() == '64bd1e146726e9f8622756173ab27831'):\r\n\r\n\tprint(\"Your MSE,RMSE and MAE Scores matched the expected output\")\r\nelse :\r\n\tprint(\"Your MSE,RMSE and MAE Scores does not match the expected output\") \r\n```\r\n\r\nHere our output should be match but coming as unmatched.\r\n", "comments": ["please help us", "@ashishsme14,\r\nLooks like this is a duplicate of [#45411](https://github.com/tensorflow/tensorflow/issues/45411#issuecomment-742214832). \r\n\r\nAnd as mentioned, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "No help from that\r\n", "Hey , had you solved this problem. I'm also getting the same issue", "@jagadeeshjr5  did u followed same steps and getting same output?\r\n\r\nMSE: 665.522, RMSE: 25.798, MAE: 17.127 .", "@ashishsme14 yes I followed my steps and got the same output\r\n\r\nHad you completed the handson ", "@jagadeeshjr5  can u please tell ur steps n output please", "@ashishsme14 I too got the same output, there's may be wrong in testcode or in the question", "@jagadeeshjr5  so u passed test case?", "@ashishsme14 for RNN only I'm not able to pass the testcase ", "please help us to review steps which we followed \r\n", "@ashishsme14 raise a feedback in fresco regrading the issue ", "@jagadeeshjr5  can u try now n see any luck for me no", "@ashishsme14 I already did, you also raise a feedback once ", "@ashishsme14 there's mistake from their end ", "@jagadeeshjr5  they saying they corrected it but no luck for me... that why any luck for u?", "@ashishsme14 when did they told you ", "@jagadeeshjr5 today", "@ashishsme14 try again, if not getting passed tell them there's an issue with the question", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45628\">No</a>\n", "@ashishsme14 still not able to pass\r\n", "@jagadeeshjr5 reset and install n follow rest process", "@ashishsme14 have you completed ", "@ashishsme14 can you give me your number I'll text you, after that you can delete it ", "@ashishsme14 Still not able to pass ", "@ashishsme14 can you tell me what output you got (mse, rmse, mae) ", "@ashishsme14 Can you help me with this ", "@jagadeeshjr5  Once launch do not install anything. 1st re start then install n follow rest process ", "@ashishsme14 I did it bro, but still no luck, can you tell me what output you got(mse, rmse, mae)", "@jagadeeshjr5 not rem it ..", "@ashishsme14 did you get the same as before or different ", "@jagadeeshjr5 different", "@ashishsme14 can you tell me the exact procedure what you did ", "same steps as mentioned here\r\nnothing different", "@ashishsme14 I mean exactly what after clicking on start test ", "yes same steps  RESET -> Install ? Run -> Test", "@ashishsme14 on getting into the environment Reset -> Install -> Run -> Open preview -> write code -> run -> test is this the process ", "yes", "@ashishsme14 Still getting the same output bro, not able to pass", "@ashishsme14 If you don't mind, can you reattempt it and tell me the output you got bro, that'll be very helpful", "Hi, is this issue was resolved successful? \r\nWhat was the solution? can you share please?\r\nI has the same problem", "MSE: 333.545, RMSE: 18.263, MAE: 14.169\r\ncorrect one", "some please help me with mlp binary classification im unable to clear that one", "MSE: 1544.323, RMSE: 39.298, MAE: 37.612\r\n\r\nI am getting these forMLP regression.This is njot matching . Can someone help me with the correct ones ?"]}, {"number": 45627, "title": "Potential GPU Memory Leak", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab (couldn't find distro from this)**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **Python 3.8.5 (64-bit)**\r\n- CUDA/cuDNN version: **10.1**\r\n- GPU model and memory: **Nvidia Tesla T4 16GB**\r\n\r\n**Describe the current behavior**\r\n\r\nHalf way through an epoch the network appears to run out of memory despite no new memory requirements i.e. nothing extra is being created that isn't deleted (I think). See ipynb below for a detailed error message. Furthermore, when checking the memory free after the program has finished executing (using `nvidia-smi`) it gives `14599MiB / 15079MiB` used and `No running processes found`.\r\n\r\n**Describe the expected behavior**\r\nThe network should continue to train as normal.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/IlleQuiProgrammat/cd37e1b0767e0c859fef922387284898\r\n\r\n*Note: the full error is shown at the bottom*\r\n", "comments": ["@IlleQuiProgrammat \r\n\r\nI ran the code shared an face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f5856ebfa522d94eb93cb30b2ff52624/untitled485.ipynb).\r\nPlease share all dependencies for us to replicate the issue faced.", "Apologies for not explaining more clearly, on my drive I have copied the celeb a img_align data set to `/MLProj/img_align_celeba.zip` from https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=drivesdk\n\nAnd the other error regarding the latest checkpoint can be solved by not running that cell - I haven't trained a model yet it is there for using it in the future for inference.\n\nIn addition, you don't appear to be running this on a gpu or colab has missed copying the `pip install tensorflow-gpu` command from my gist. Running this on cpu would be far too slow and I believe this issue is related to managing the gpu memory but I'm not sure.", "@Saduf2019 mentioning you as, from the previous issues I've seen, tensorflow butler normally removes the awaiting response tag but this hasn't seemed to happen so making sure you've seen my response. ", "@IlleQuiProgrammat\r\nApologies for the delayed response.\r\nCan you please share a colab gist, i am unable to download from the link shared.", "I'm not quite sure how you would like me to attach the dataset to the colab gist as it is very large. If there is a way I don't know, please tell me. \n\nIf it is just an issue of the Google drive link not working, you can download the dataset from any of the links on this page: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n\nThank you in advance. ", "Since you are hitting OOM error I would recommend [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and also try reducing your batch size to a smaller value.\r\n```python\r\n# On top of your script \r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n# Rest of your code\r\n...\r\n\r\n", "Apologies, I missed your response, I shall try that now and update you to the results.", "Thank you, this seems to be working @ymodak - is there a reason why this is not enabled by default?", "Glad it worked. As per the [Limiting GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) docs,\r\n> By default, TensorFlow maps nearly all of the GPU memory of all GPUs  visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45627\">No</a>\n"]}, {"number": 45626, "title": "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028717BD2C18> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause:", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hello, want to use the Model.predict from tensorflow but i see the issu", "@alinouruzi\r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45626\">No</a>\n"]}, {"number": 45625, "title": "How to set the signature of keras.models.save_model", "body": "Maybe this problem is not suitable for this module\r\n\r\n# My environment\r\n\r\ntf:2.3\r\nsystem:ubuntu 18\r\n\r\n# My question\r\n\r\nI updated from tf14 to tf2.3. The model I used is a model of keras type. After viewing the official document, adding signature failed\r\n\r\n# My main code\r\n\r\n```\r\nmodel = VGG16(weights = weights_dir)\r\n...\r\nkeras.models.save_model(model, model_dir_saved_model)\r\n```\r\nThis function has the input of signature, but I don't know how to organize it\r\n\r\n# Here's my try\r\n\r\n```\r\n\r\ndef saveKerasModelAsProtobuf(model, outputPath):\r\n    inputs = {'image': utils.build_tensor_info(model.input)}\r\n    outputs = {'scores': utils.build_tensor_info(model.output)}\r\n\r\n    signature = tf.saved_model.signature_def_utils.build_signature_def(\r\n        inputs, outputs, 'name')\r\n\r\n    builder = tf.saved_model.builder.SavedModelBuilder(outputPath)\r\n    builder.add_meta_graph_and_variables(\r\n        sess=keras.backend.get_session(),\r\n        tags=['serving_default'],\r\n        signature_def_map={'serving_default': signature})\r\n    builder.save()\r\n```\r\n\r\nSo, what's the right way to keep it\r\n\r\n", "comments": ["Hmm I am not sure whether I understand your question. Could you provide a copy-paste code example or colab which reproduces your error?", "> Hmm I am not sure whether I understand your question. Could you provide a copy-paste code example or colab which reproduces your error?\r\n\r\nThank you very much for your reply.I'll sort out the code\r\n\r\nThe problem is simply as follows:\r\n\r\nIn this environment, the version of tensorflow is 2.3\r\n\r\nThe storage type of the model is required to be keras\r\n\r\nThe main save function is keras.model.save_ model\r\n\r\nHow to specify the signature of the model so that it has the following structure\r\n\r\n```\r\nsignature_def['serv']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_x'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 200, 60, 3)\r\n        name: input_images:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1)\r\n        name: output_scores:0\r\n  Method name is: tensorflow/serving/classify\r\n\r\n```", "> Hmm I am not sure whether I understand your question. Could you provide a copy-paste code example or colab which reproduces your error?\r\n\r\nhere is my code,the question at the last line\r\n```\r\n'''\r\nAuthor: your name\r\nDate: 2020-12-12 20:42:51\r\nLastEditTime: 2020-12-12 21:12:33\r\nLastEditors: your name\r\nDescription: In User Settings Edit\r\nFilePath: /full_class/models/test/test_signature.py\r\n'''\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport PIL\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nimport pathlib\r\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\r\ndata_dir = tf.keras.utils.get_file('flower_photos',\r\n                                   origin=dataset_url,\r\n                                   untar=True)\r\ndata_dir = pathlib.Path(data_dir)\r\n\r\nimage_count = len(list(data_dir.glob('*/*.jpg')))\r\nprint(image_count)\r\n\r\nroses = list(data_dir.glob('roses/*'))\r\nPIL.Image.open(str(roses[0]))\r\nPIL.Image.open(str(roses[1]))\r\ntulips = list(data_dir.glob('tulips/*'))\r\nPIL.Image.open(str(tulips[0]))\r\nPIL.Image.open(str(tulips[1]))\r\nbatch_size = 32\r\nimg_height = 180\r\nimg_width = 180\r\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n    data_dir,\r\n    validation_split=0.2,\r\n    subset=\"training\",\r\n    seed=123,\r\n    image_size=(img_height, img_width),\r\n    batch_size=batch_size)\r\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n    data_dir,\r\n    validation_split=0.2,\r\n    subset=\"validation\",\r\n    seed=123,\r\n    image_size=(img_height, img_width),\r\n    batch_size=batch_size)\r\nclass_names = train_ds.class_names\r\nprint(class_names)\r\nimport matplotlib.pyplot as plt\r\n\r\nplt.figure(figsize=(10, 10))\r\nfor images, labels in train_ds.take(1):\r\n    for i in range(9):\r\n        ax = plt.subplot(3, 3, i + 1)\r\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\r\n        plt.title(class_names[labels[i]])\r\n        plt.axis(\"off\")\r\nfor image_batch, labels_batch in train_ds:\r\n    print(image_batch.shape)\r\n    print(labels_batch.shape)\r\n    break\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\r\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\r\nnormalization_layer = layers.experimental.preprocessing.Rescaling(1. / 255)\r\nnormalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\r\nimage_batch, labels_batch = next(iter(normalized_ds))\r\nfirst_image = image_batch[0]\r\n# Notice the pixels values are now in `[0,1]`.\r\nprint(np.min(first_image), np.max(first_image))\r\nnum_classes = 5\r\n\r\nmodel = Sequential([\r\n    layers.experimental.preprocessing.Rescaling(1. / 255,\r\n                                                input_shape=(img_height,\r\n                                                             img_width, 3)),\r\n    layers.Conv2D(16, 3, padding='same', activation='relu'),\r\n    layers.MaxPooling2D(),\r\n    layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n    layers.MaxPooling2D(),\r\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n    layers.MaxPooling2D(),\r\n    layers.Flatten(),\r\n    layers.Dense(128, activation='relu'),\r\n    layers.Dense(num_classes)\r\n])\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    metrics=['accuracy'])\r\n\r\nmodel.summary()\r\nepochs = 10\r\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\r\n\r\n# here is the problem,how to set the signatures\r\nmodel_export_dir = '.'\r\nkeras.models.save_model(model, model_export_dir)\r\n\r\n```", "[This docpage](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) explains how to specify model signatures during export. See also the [SavedModelCLI](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface) which should already be installed if you use the TF pip package.", "@wangwenchao-job,\r\nAs mentioned by @aaronmondal, please go through the [specifying signatures during export](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) guide to set signatures in your model. \r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45625\">No</a>\n", "This is not clear in the documentation that is referenced above. This Link in the models API is more useful\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#save_spec"]}, {"number": 45624, "title": "Update RELEASE.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45624) for more info**.\n\n<!-- need_sender_cla -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac", "Furthermore, the fix is invalid, mostly spam."]}, {"number": 45623, "title": "NaN predictions/gradients/losses in basic GAN training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- TensorFlow installed from (source or binary):\r\nConda\r\n- TensorFlow version (use command below):\r\n2.2.0\r\n- Python version:\r\n3.7.9\r\n- CUDA/cuDNN version:\r\n10.1.243\r\n7.6.5\r\n- GPU model and memory:\r\nRTX3080 10GB VRAM\r\n\r\n**Describe the current behavior**\r\nWhen training simple GAN models using the exact code from the documentation ([\"DCGAN\"](https://www.tensorflow.org/tutorials/generative/dcgan) & [\"Writing a training loop from scratch\"](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)), the generator predicts \"nan\" after one or two training steps. \r\n\r\nWhen debugging and looking at the gradients, I see that some layers' gradients have nan values\r\n\r\n**Describe the expected behavior**\r\nThe model should predict actual numbers and not GANs, or 2 different documentation articles need to be fixed.\r\n\r\n**Standalone code to reproduce the issue**\r\nI can't reproduce it on Colab, but here's the code I run on my PC:\r\nhttps://colab.research.google.com/gist/asaf92/8de8b9fc0e9e34c9079ebbe54f41e0b4/copy-of-gan-bug.ipynb\r\nI get nan losses after the first epoch.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n```\r\nConnected to pydev debugger (build 202.7660.27)\r\n2020-12-12 11:52:37.370048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-12 11:52:40.950277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-12-12 11:52:40.987435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-12-12 11:52:40.988331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-12 11:52:40.997389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-12 11:52:41.004564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-12 11:52:41.006971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-12 11:52:41.013938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-12 11:52:41.016784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-12 11:52:41.028209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-12 11:52:41.028663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-12-12 11:52:41.030082: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-12-12 11:52:41.049835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c04f8b7f70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-12 11:52:41.050215: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-12 11:52:41.050521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-12-12 11:52:41.050776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-12 11:52:41.051112: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-12 11:52:41.051189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-12 11:52:41.051269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-12-12 11:52:41.051347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-12 11:52:41.051767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-12 11:52:41.051965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-12 11:52:41.052387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-12-12 11:52:42.074102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-12 11:52:42.074197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-12-12 11:52:42.074417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-12-12 11:52:42.075088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8454 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:65:00.0, compute capability: 8.6)\r\n2020-12-12 11:52:42.082271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1c004532730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-12 11:52:42.082872: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3080, Compute Capability 8.6\r\nModel: \"discriminator\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 14, 14, 64)        640       \r\n_________________________________________________________________\r\nleaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \r\n_________________________________________________________________\r\nleaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \r\n_________________________________________________________________\r\nglobal_max_pooling2d (Global (None, 128)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 129       \r\n=================================================================\r\nTotal params: 74,625\r\nTrainable params: 74,625\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nStart epoch 0\r\n2020-12-12 11:52:43.175082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-12 11:52:43.567573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-12 11:52:46.072136: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\ndiscriminator loss at step 0: 0.69\r\nadversarial loss at step 0: 0.76\r\n\r\nStart epoch 1\r\ndiscriminator loss at step 0: nan\r\nadversarial loss at step 0: nan\r\n\r\nStart epoch 2\r\ndiscriminator loss at step 0: nan\r\nadversarial loss at step 0: nan\r\n\r\nStart epoch 3\r\ndiscriminator loss at step 0: nan\r\nadversarial loss at step 0: nan\r\n\r\n```\r\n", "comments": ["My initial guess would be that training is unstable because your GPU uses TensorFloat32, which has the same precision as half precision (10 bits).\r\n\r\nIn the case of half precision this instability is addressed by adaptively scaling the loss and weight updates inversely so that mathematically nothing changes but the available numeric ranges are better utilized.\r\n\r\nMaybe try to use something like [tf.mixed_precision.experimental.DynamicLossScale](https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/DynamicLossScale) on the loss and gradients. See [here](https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/LossScale) for usage instructions.\r\n\r\nAnother problem may simply be that you are using Windows, as indicated by the warning you got.", "I tried your solution and although it sometimes helps with eliminating the nans, it seems that RTX3080 requires CUDA11 which is not used by TensorFlow2.2 version. \r\nThere are many bugs with Cuda 10.1 and Ampere (RTX30xx) cards", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45623\">No</a>\n"]}, {"number": 45622, "title": "tf.test.gpu_device_name() breaks tensorflow", "body": "**System information**\r\n- I am working on a vae in tensorflow using probability layers\r\n- Ubuntu 20.04\r\n- tensorflow 2.5 compiled from source\r\n- Python version: 3.8.5\r\n- CUDA 11.1\r\n- RTX 2070 Super with 8gb of ram\r\n\r\nI want to reproduce this [example](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE). When running it I get the following exception:\r\n```\r\nBlas xGEMM launch failed : a.shape=[1,300,4096], b.shape=[1,4096,512], m=300, n=512, k=4096\r\n\t [[node encoder/dense/MatMul (defined at <ipython-input-8-24f1e62ee75b>:15) ]] [Op:__inference_train_function_1790]\r\n```\r\nJupyter also throws a ```CUBLAS_STATUS_NOT_INITIALIZED``` error. NVTOP also show that all of the VRAM is being used. I run ``` tf.config.experimental.set_memory_growth(gpu, True)``` right after tensorflow import.\r\n\r\nRemoving\r\n```\r\nif tf.test.gpu_device_name() != '/device:GPU:0':\r\n  print('WARNING: GPU device not found.')\r\nelse:\r\n  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\r\n```\r\nfixes the problem. When adding this snippet to any other tensorflow program it breaks as well.\r\n\r\nMy theory is that ```tf.test.gpu_device_name()``` somehow breaks or resets ```tf.config.experimental.set_memory_growth(gpu, True)```.", "comments": ["It Is probably a https://github.com/tensorflow/tensorflow/issues/26275 duplicate.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45622\">No</a>\n"]}, {"number": 45621, "title": "Updated README.md", "body": "Just a grammatical error", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45621) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot\r\n\r\nI signed it!", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}, {"number": 45620, "title": "[TFTRT] Disable non TRT optimization fix", "body": "This PR fixes OfflineConversion not respecting properly: `DisableNonTrtOptimizers()`.\r\nIn addition it provides some cleanup, and a \"private\" function to get a rewriter_config with all non-TRT grappler opt disabled.", "comments": ["CC: @bixia1 @tfeher for review.\r\n\r\nThis basically fixes some of the issue I found in the `ConvertCast` where some grappler optimizations where applied even when I deactivated them.\r\n\r\nI reviewed the whole python code base. It seems to me that there's no more corner cases that we forgot once this PR is merged.", "This PR fixes the following cases:\r\n- Convert Offline wasn't properly deactivating Non TRT Grappler Opt: \r\n  - https://github.com/tensorflow/tensorflow/pull/45620/files#diff-39ad5be340981b91107e595a47a42caf7da5a8436448d64a8914c4696384d5a2R336 \r\n  - https://github.com/tensorflow/tensorflow/pull/45620/files#diff-8e470ff4b5b82fda4fb3fe008b694323d63484f3cbf490d4e3ba104894f914e6R676-R681\r\n\r\n- During GraphState !=  Inference, same issue: \r\nhttps://github.com/tensorflow/tensorflow/pull/45620/files#diff-39ad5be340981b91107e595a47a42caf7da5a8436448d64a8914c4696384d5a2R336\r\n\r\n- A constfold was systematically added even when non TRT Grappler Opt were deactivated: https://github.com/tensorflow/tensorflow/pull/45620/files#diff-8e470ff4b5b82fda4fb3fe008b694323d63484f3cbf490d4e3ba104894f914e6R306-R308\r\n\r\n-----------\r\n\r\nWe also forgot to force deactivate: \r\n- `auto_mixed_precision=off`: https://github.com/tensorflow/tensorflow/pull/45620/files#diff-8e470ff4b5b82fda4fb3fe008b694323d63484f3cbf490d4e3ba104894f914e6R237\r\n- `min_graph_nodes=-1`: https://github.com/tensorflow/tensorflow/pull/45620/files#diff-8e470ff4b5b82fda4fb3fe008b694323d63484f3cbf490d4e3ba104894f914e6R248\r\n\r\nAs a note, I also took the opportunity to sort alphabetically the rewriter_cfg flags that we deactivate. It was difficult to read and find something.", "@bixia1 I pushed the changes we talked about this morning:\r\n\r\n- `_disable_non_trt_optimizers_in_rewriter_config()` doesn't use `MergeFrom()` or `CopyFrom()` but directly modifies the `rewriter_config` as it used to do.\r\n\r\n- `_disable_non_trt_optimizers_in_rewriter_config()` is defined in `trt_convert.py` but does not requires calling the converter (useful for `tf_trt_integration_test_base.py`).\r\n\r\n- For your remark about needing the `rewriter_config` in Offline mode. I want to be able to disable \"non-TRT optimizers\" in both Online & Offline Mode. And it needs to be working in all GraphStates.\r\n\r\n--------------\r\n\r\nLet me know if there's anything I can do. This PR doesn't introduce any new behavior, only fixes loopholes/bugs. If we want to change Offline/Online Conversion behavior, I would vote to create a new PR about that. But first let's make sure it's bug-free.", "@bixia1 I pushed a fix with the requested changes. We shall be good now :+1: ", "@DEKHTIARJonathan  Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 45619, "title": "Build Xtensa kernels for Vision P6", "body": "This PR currently includes commits from #45619.\r\n\r\nConfirmed that all tests pass for vision P6:\r\n ```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=vision_p6 XTENSA_CORE=P6_200528 test\r\n```\r\n\r\nOnce this PR is merged we can turn on continuous builds for P6.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving", "> I'm going to wait until #45618 and this is rebased. Seems like that will provide a clearer picture of the delta here.\r\n\r\nThe quantize refactor PR is merged so this PR is ready for review now.", "copybara import error occurred. retrying with \"kokoro:force-run\" and \"ready to pull\" labels."]}, {"number": 45618, "title": "Refactor quantize (reference and xtensa) for a complete fallback.", "body": "PR #45464 missed a proper reference fallback for the quantize kernel and prior to the current PR we can not build the quantize kernel for the Vision P6.\r\n\r\nFollow on PR #45619 makes additional changes to be able to build all the kernels for the Vision P6 but this change was separated out into its own PR to keep individual PRs smaller and focused.\r\n\r\nTested:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=Google_F1 test_keyword_benchmark -j8\r\n```\r\n\r\nGives:\r\n```\r\nInitializeKeywordRunner() took 239589 ticks (239 ms)\r\nKeywordRunNIerations(1) took 171203 ticks (171 ms)\r\nKeywordRunNIerations(10) took 1711501 ticks (1711 ms)\r\n```\r\n\r\nThis is a small increase in the latency but is not important since we are still only using the reference implementation.\r\n\r\nAnd the kernel test passes for fusion_f1i (13 cases) and hifimini (2 cases):\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=Google_F1 test_kernel_quantize_test -j8\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_quantize_test\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving\r\n\r\nAlso tagging @yair-ehrenwald -- this is one example of the type of refactor that we are incrementally going through to improve code sharing (and reference fallbacks) between optimized kernels and reference kernels.\r\n", "> Looks reasonable. Just to double check, this has no effect on the hifimini cycle count and binary size, since everything new is hidden behind ifdefs, right?\r\n\r\nIt shouldn't. I decided not to try the full size and cycle check because it is a bit laborious and would like to get a process going where we use the continuous builds to check for these regressions.\r\n"]}, {"number": 45617, "title": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xXX in position YY : invalid continuation byte", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I'm trying to adapt a code that I found the code [here](https://medium.com/@martin.lees/image-recognition-with-machine-learning-in-python-and-tensorflow-b893cd9014d2). But this code doesn't work (well, I have an error)\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow version (use command below):TF 2.0\r\n- Python version: 3.8\r\n\r\n\r\n**Describe the current behavior**\r\nRun, but got this UnicodeDecodeError at the end so it stops. \r\n\r\n**Describe the expected behavior**\r\nShould run without the error at the end. \r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nATTENTION RESULTAT  0.8888889 #% it's not a mistake, I just put 5 pic in each folder just to see if it was working\r\nTraceback (most recent call last):\r\n\r\n  File \"D:\\pokemon\\PogoBot\\PoGo-Adb\\ml_test_data_test.py\", line 108, in <module>\r\n    tf.app.run(main=main)\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n\r\n  File \"D:\\pokemon\\PogoBot\\PoGo-Adb\\ml_test_data_test.py\", line 104, in main\r\n    saver.save(sess, \"./model\")\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1183, in save\r\n    model_checkpoint_path = sess.run(\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 957, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1180, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1358, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1349, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n\r\n  File \"C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1441, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 109: invalid continuation byte\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n````\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport cv2\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\nimport numpy as np\r\nimport tensorflow as tf2\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nimport math\r\n\r\n\r\n\r\nclass Capchat:\r\n    data_dir = \"data_test//\"\r\n    nb_categories = 9\r\n    X_train = None # X is the data array\r\n    Y_train = None # Y is the labels array, you'll see this notation pretty often\r\n    \r\n    train_nb = 0 # number of train images\r\n    X_test = None\r\n    Y_test = None\r\n    test_nb = 0 # number of tests images\r\n    index = 0 # the index of the array we will fill \r\n    def readimg(self, file, label, train = True):\r\n        im = cv2.imread(file); # read the image to PIL image\r\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY).flatten() # put it in black and white and as a vector\r\n        # the train var definies if we fill the training dataset or the test dataset\r\n        if train : \r\n            self.X_train[self.index] = im\r\n            self.Y_train[self.index][label - 1] = 1\r\n        else :\r\n            self.X_test[self.index] = im\r\n            self.Y_test[self.index][label - 1] = 1\r\n        self.index += 1\r\n    def __init__(self):\r\n        total_size = [f for f in listdir(self.data_dir + \"1/\") if isfile(join(self.data_dir + \"1/\", f))].__len__() # ge the total size of the dataset\r\n        self.train_nb = math.floor(total_size * 0.8) # we get 80% of the data to train\r\n        self.test_nb = math.ceil(total_size *0.2) # 20% to test\r\n        \r\n        # We fill the arrays with zeroes 840 is the number of pixels in an image\r\n        self.X_train = np.zeros((self.train_nb*self.nb_categories, 735), np.int32)\r\n        self.Y_train = np.zeros((self.train_nb*self.nb_categories, 3), np.int32)\r\n        self.X_test = np.zeros((self.test_nb*self.nb_categories, 735), np.int32)\r\n        self.Y_test = np.zeros((self.test_nb*self.nb_categories, 3), np.int32)\r\n        # grab all the files \r\n        files_1 = [f for f in listdir(self.data_dir+\"1/\") if isfile(join(self.data_dir+\"1/\", f))]\r\n        files_2 = [f for f in listdir(self.data_dir+\"2/\") if isfile(join(self.data_dir+\"2/\", f))]\r\n        files_3 = [f for f in listdir(self.data_dir+\"3/\") if isfile(join(self.data_dir+\"3/\", f))]\r\n\r\n        for i in range(self.train_nb):\r\n            # add all the files to training dataset\r\n            self.readimg(self.data_dir+\"1/\"+files_1[i], 1)\r\n            self.readimg(self.data_dir+\"2/\"+files_2[i], 2)\r\n            self.readimg(self.data_dir+\"3/\"+files_3[i], 3)\r\n\r\n        self.index = 0\r\n        \r\n        for i  in range (self.train_nb, self.train_nb + self.test_nb):\r\n            self.readimg(self.data_dir+\"1/\" + files_1[i], 1, False)\r\n            self.readimg(self.data_dir+\"2/\" + files_2[i], 2, False)\r\n            self.readimg(self.data_dir+\"3/\" + files_3[i], 3, False)\r\n        print(\"donn\u00e9e tri\u00e9e\")\r\n\r\n\r\ndef main(_):\r\n  # Import the data\r\n  cap = Capchat()\r\n  # Create the model\r\n  x = tf.placeholder(tf.float32, [None, 735])\r\n  W = tf.Variable(tf.zeros([735, 3]), name=\"weights\")\r\n  b = tf.Variable(tf.zeros([3]), name=\"biases\")\r\n  mult = tf.matmul(x, W) # W * X...\r\n  y = tf.add(mult, b, name=\"calc\") # + b\r\n  # Define loss and optimizer\r\n  y_ = tf.placeholder(tf.float32, [None, 3])\r\n  # cost function\r\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\r\n  # optimizer\r\n  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n  # allows to save the model later\r\n  saver = tf.train.Saver()\r\n  # start a session to run the network on\r\n  sess = tf.InteractiveSession()\r\n  # initialize global variables\r\n  tf.global_variables_initializer().run()\r\n  # Train for 1000 steps, notice the cap.X_train and cap.Y_train\r\n  for _ in range(1000):\r\n    sess.run(train_step, feed_dict={x: cap.X_train, y_: cap.Y_train})\r\n  # Extract one hot encoded output via argmax\r\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n  # Test for accuraccy on the testset, notice the cap.X_test and cap.Y_test\r\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n  print(\"\\nATTENTION RESULTAT \",sess.run(accuracy, feed_dict={x: cap.X_test,\r\n                                      y_: cap.Y_test}))\r\n  # save the model learned weights and biases\r\n  saver.save(sess, \"./model\")\r\n  \r\n  \r\nif __name__ == '__main__':\r\n  tf.app.run(main=main)\r\n", "comments": ["I see you are using TF 2.0. Can you try TF 2.3 please? Or, 2.4, it is supposed to come out next Monday (but there are release candidates now)", "> I see you are using TF 2.0. Can you try TF 2.3 please? Or, 2.4, it is supposed to come out next Monday (but there are release candidates now)\r\n\r\nI just did this, I still got the problem sir :( ", "Could it be that you are using invalid folder names or printing non-standard symbols? A `UnicodeDecodeError` may indicate that some string (e.g. a filename, foldername or print statement) you are using is not a valid UTF-8 string: https://stackoverflow.com/questions/5552555/unicodedecodeerror-invalid-continuation-byte", "> Could it be that you are using invalid folder names or printing non-standard symbols? A `UnicodeDecodeError` may indicate that some string (e.g. a filename, foldername or print statement) you are using is not a valid UTF-8 string: https://stackoverflow.com/questions/5552555/unicodedecodeerror-invalid-continuation-byte\r\n\r\nAt the end, the error was caused by this line : \r\n`saver.save(sess, \"./model\")`\r\nBecause I'm on windows, the computer didn't like this line, so I changed it with \r\n`saver.save(sess, \"model\\\\model\")`\r\n\r\nThank you for helping guys, this message really helped me ! :) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45617\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45617\">No</a>\n", "OS Platform and Distribution : Windows 10\r\nTensorFlow version (use command below):TF 2.2.0\r\nPython version: 3.7\r\n\r\nwhen copy project: [tensorflow/object_detection](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb)\r\n`label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\r\n`\r\noccur the same error, I solved the bug by chage path:\r\n`PATH_TO_LABELS = models/research/object_detection/data/mscoco_label_map.pbtxt`\r\nto\r\n`PATH_TO_LABELS = D:\\\\Users\\\\models-master\\\\models-master\\\\research\\\\object_detection\\\\data\\\\mscoco_label_map.pbtxt`\r\n"]}, {"number": 45616, "title": "tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "## My machine have - Cuda Version - 10.1.105, ##cuDNN Version - 7.6 , ##Device - GeForce GTX 1660 SUPER \r\n##Tensorflow - 2.3.1 , ##Python 3.8.3 , O.S - Windows 10\r\n=====================================================\r\nE:\\xtreme_vision_yolo>python train_yolov4.py\r\n2020-12-12 02:49:01.254777: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\nclass path E:/xtreme_vision_yolo/classes.names\r\ntrain images file E:/xtreme_vision_yolo/train_images_file.txt\r\ntrain image dir E:/xtreme_vision_yolo/train_img_dir/\r\n2020-12-12 02:49:18.534478: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-12-12 02:49:18.585778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 SUPER computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2020-12-12 02:49:18.594476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-12 02:49:19.157955: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-12 02:49:19.495738: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-12 02:49:19.529529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-12-12 02:49:19.880640: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-12 02:49:20.170330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-12 02:49:20.477234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-12 02:49:20.482540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-12-12 02:49:20.487618: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-12 02:49:20.508225: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25fa6271050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-12 02:49:20.516217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-12-12 02:49:20.524054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 SUPER computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2020-12-12 02:49:20.536073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-12-12 02:49:20.543032: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-12-12 02:49:20.548638: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-12 02:49:20.554466: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-12-12 02:49:20.559750: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-12 02:49:20.566821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-12-12 02:49:20.572949: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-12 02:49:20.580348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-12-12 02:49:22.507429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-12 02:49:22.513254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-12-12 02:49:22.517366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-12-12 02:49:22.521455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4618 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-12 02:49:22.533883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25fd40d41e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-12-12 02:49:22.542623: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1660 SUPER, Compute Capability 7.5\r\nDownloading weights file...\r\nPlease wait...\r\n2020-12-12 02:49:34.825447: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\n2020-12-12 02:49:34.829432: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs\r\n2020-12-12 02:49:34.835081: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found\r\n2020-12-12 02:49:34.843967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2020-12-12 02:49:34.850964: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n\r\nEpoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(9.0483736e-05, shape=(), dtype=float32).\r\nEpoch 1/500\r\n2020-12-12 02:50:13.332541: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-12-12 02:50:14.779540: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-12-12 02:50:14.789174: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\nTraceback (most recent call last):\r\n  File \"train_yolov4.py\", line 18, in <module>\r\n    model.train(epochs=500, lr=1e-4, steps_per_epoch=400)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xtreme_vision\\Detection\\Custom\\yolo.py\", line 186, in train\r\n    self.model.fit(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xtreme_vision\\Detection\\yolov4\\tf\\__init__.py\", line 260, in fit\r\n    self.model.fit(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 840, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1843, in _filtered_call\r\n    return self._call_flat(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1923, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 545, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node YOLOv4/CSPDarknet53/yolo_conv2d/sequential/conv2d/Conv2D (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xtreme_vision\\Detection\\yolov4\\model\\common.py:92) ]] [Op:__inference_train_function_54586]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node YOLOv4/CSPDarknet53/yolo_conv2d/sequential/conv2d/Conv2D:\r\n IteratorGetNext (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xtreme_vision\\Detection\\yolov4\\tf\\__init__.py:260)\r\n\r\nFunction call stack:\r\ntrain_function\r\n## Kindly suggest solution", "comments": ["@zaheerbeg21 \r\nPlease refer to these resolved issue with same error and let us know: #44885 #43764 #45458 ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\r\n\r\nNo issue is same as before..\r\n", "@zaheerbeg21 Can you please explain your issue with little more details? Do you have any issue running TF only on GPU? Did you try to run any simple model with GPU? Can you please share any simple standalone code to reproduce the issue? thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45616\">No</a>\n"]}, {"number": 45615, "title": "metrics  in the Classification on imbalanced data tutorial ", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/structured_data/imbalanced_data\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe example uses the AUC-ROC metric, both for evaluation and as the criterion for early stopping. However, for imbalanced datasets the Precision-Recall curve may be a better choice. In fact, this is the metric that is recommended in the Kaggle dataset (https://www.kaggle.com/mlg-ulb/creditcardfraud), see also:\r\n\r\nhttps://www.biostat.wisc.edu/~page/rocpr.pdf\r\n\r\n### Clear description\r\n\r\nInstead of using the ROC-AUC, the PRC Area Under the Curve should be used instead. In particular, \r\n\r\n* The `METRICS` array should include:\r\n\r\n    `keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve`\r\n\r\n* `early_stopping` should be defined as:\r\n\r\n    ```\r\n    early_stopping = tf.keras.callbacks.EarlyStopping(\r\n        monitor='val_prc', \r\n        verbose=1,\r\n        patience=10,\r\n        mode='max',\r\n        restore_best_weights=True)\r\n   ```\r\n\r\n* The first line of `plot_metrics(history)` should become:\r\n\r\n    ```\r\n    metrics = ['loss', 'prc', 'precision', 'recall']\r\n    ```\r\n\r\n* Instead of having a `plot_auc()` we should have:\r\n\r\n    ```\r\n    def plot_prc(name, labels, predictions, **kwargs):\r\n        precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\r\n\r\n        plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\r\n        plt.xlabel('Recall')\r\n        plt.ylabel('Precision')\r\n        plt.grid(True)\r\n        ax = plt.gca()\r\n        ax.set_aspect('equal')\r\n    ```\r\n\r\n### Submit a pull request?\r\n\r\nNo, because I have also made some other changes, the most important of which is that oversampling does not seem to give very good results, but a combination of SMOTE oversampling and undersampling does give better results. This is easy to do with imbalanced-learn (I might submit another issue). Also, I am not sure why the adjusted output bias is used when we reweight the classes, as the effect is the same as with oversampling, and we zero out the last bias in oversampling.", "comments": ["@louridas,\r\nThanks for the suggestion.  Can you please click on \"**`Run in Google Colab`**\" option of the [Tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data), do the respective changes and share the **`Github Gist`** (File -> Save a Copy as Github Gist). \r\n\r\nAlso, if possible, please highlight how this change is better over the existing approach.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hello, I did not have time to do the respective changes as of yet, but please do not close the issue, I will do it.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Same, will get to it as soon as I find some time.", "Hi @rmothukuru, I am very interested in this issue. Can you assign this issue to me? Thanks!", "Hi @rmothukuru, I have already made the changes in my own gist. I don't use gist very often... I don't know how to commit my changes through gist. \r\nMy former experience is :\r\npull request my own branch, commit to my branch, pull request to the main branch.\r\n\r\nBut for this doc, I don't know where it is located. Need help here. Thanks!", "@samaritanhu,\r\nThank you for your contribution. \r\n\r\nThe Tutorial is located in the path, https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb.\r\n\r\nPlease find the below links for the doc changes:\r\n\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style", "Thanks, @rmothukuru! I have already pull request into the tensorflow/doc here: https://github.com/tensorflow/docs/pull/1847.", "@rmothukuru  I would love to work on this issue.Can you assign this issue to me?", "This was fixed by tensorflow/docs#1847"]}, {"number": 45614, "title": "Only use site.USER_SITE if site.ENABLE_USER_SITE is set", "body": "Currently, TensorFlow does not respect the virtual environment borders, loading dynamic kernels from the user site even in a virtual environment, causing a hard crash during start when for example TF 2.3 is installed in a user site and TF 2.4 in a virtual environment.\r\n\r\nThis pull request is a cherry pick of a commit from `master` which adds the `site.USER_SITE` to the list of paths to be loaded from only if `site.ENABLE_USER_SITE` is set, which fixes the issue.\r\n\r\nThe pull request on master is #45399, the original issue #42978.", "comments": ["In the original pull request #45399, @mihaimaruseac said that we should probably wait for a build of tf-nightly, to see if everything is OK.\r\n\r\nBut I created the cherry-pick to TF 2.4 already to keep the issue visible.", "The `tf_nightly_cpu-2.5.0.dev20201213` contains the fix, and it works (so TF-nightly in a venv does not clash with a TF in user site).\r\n\r\nPlease consider backporting for TF 2.4.\r\n\r\nCheers :-)"]}, {"number": 45613, "title": "Fix the issue of tf.image.central_crop when central_fraction is a tensor", "body": "This PR tries to address the issue raised in #45324 where\r\ntf.image.central_crop throws out an error when central_fraction is a tensor.\r\nThis PR addresses the issue by capturing the scenario where\r\ncentral_fraction is a tensor.\r\n\r\nThis PR fixes #45324.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 45612, "title": "micro: RFC: port op FILL to micro", "body": "This is an RFC on how to best chunk, into reviewable PRs, the port of operator FILL from lite to micro as discussed in issue #45306. Below is the current plan from that issue and a mapping to the commits in this draft PR. Please review to ensure this is on target, creating small-enough PRs with understandable diffs.\r\n\r\nFollowing the plan in issue #45306, the following PRs have already been submitted:\r\n1. ~~**Extract the code for parsing the op from a flatbuffer**~~. Merged as #45307.\r\n2. ~~**Extract the reference implementation into its own header**.~~ Merged as #45311.\r\n3. **Copy operator from lite to micro without making any changes**. Open as #45457. Note I've copied both the kernel and the test.\r\n\r\nThis RFC contains the changes from plan steps PR 3 to PR 5:\r\n\r\n4. **Delete extra code from the micro copy of the operator**: b5b6071. As I understand it, the intent is to remove some of the old code so that the diff in the following PR is more readable. The code isn't yet included in the build and doesn't function, so what is right and wrong is a little subjective. Note I've modified both the kernel and the test.\r\n5. **Port micro copy of operator as necessary and add a corresponding test**: At the moment, this change is in three commits: 046eb2e containing the port and addition to the build, 9919a96 adding the op to the resolver (which doesn't get tested, BTW); and 6fe4e69 adding new datatypes specific to micro. Is this too large for a single PR, or do I have the right idea? It's hard to see how to get more atomic than these three commits, unless the first is broken up into small unbuilt and untested changes (like the copy PR 3 above).\r\n\r\nOf course, the content needs reviewing too, but let's start by getting the chunking right.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Thanks for helping us work through this process. We're still figuring it out so happy to change things, and take it on a case-by-case basis depending on the complexity of the OP.\r\n\r\n> \r\n> This RFC contains the changes from plan steps PR 3 to PR 5:\r\n> \r\n> 1. **Delete extra code from the micro copy of the operator**: [b5b6071](https://github.com/tensorflow/tensorflow/commit/b5b60711283082c5087a7fa2a6d515886f200a63). As I understand it, the intent is to remove some of the old code so that the diff in the following PR is more readable. The code isn't yet included in the build and doesn't function, so what is right and wrong is a little subjective. Note I've modified both the kernel and the test.\r\n\r\nYes, this is indeed subjective and what you have looks good to me as an example for PR4.\r\n\r\nThe only thing I might add is removing support for all types other than `int8` and `float32`. Everything but these should be added to TFLM on an as needed basis and motivated by specific use-cases. We should probably add this note to the OP porting guide.\r\n\r\nAdditional context:\r\n * https://github.com/tensorflow/tensorflow/pull/42452#discussion_r473414356\r\n * https://github.com/tensorflow/tensorflow/issues/44912\r\n\r\n> 2. **Port micro copy of operator as necessary and add a corresponding test**: At the moment, this change is in three commits: [046eb2e](https://github.com/tensorflow/tensorflow/commit/046eb2ec35c915864917e1e13d8235b7935a93e0) containing the port and addition to the build, [9919a96](https://github.com/tensorflow/tensorflow/commit/9919a968a98fb7a16dc88beca842220e8bb7b12e) adding the op to the resolver (which doesn't get tested, BTW); and [6fe4e69](https://github.com/tensorflow/tensorflow/commit/6fe4e69dfb2e15cc74d1d4259601042b2e965cad) adding new datatypes specific to micro. Is this too large for a single PR, or do I have the right idea? It's hard to see how to get more atomic than these three commits, unless the first is broken up into small unbuilt and untested changes (like the copy PR 3 above).\r\n> \r\n> Of course, the content needs reviewing too, but let's start by getting the chunking right.\r\n\r\nI think all three commits in a single PR5 is fine.\r\n\r\nMy only comment is that I would expect (hope?) that the datatypes supported in Lite are a superset of the datatypes needed for micro, so I'm not 100% sure what the 3rd commit means. Regardless, this last PR is (non-mechanical) the set of changes needed to port an OP to TFLM so it will be involved and require careful reviewing.", "Thanks for the feedback. I'm closing this RFC and have launched PR4 #45646 and PR5 #45647 for review and merger.\r\n\r\n> The only thing I might add is removing support for all types other than `int8` and `float32`. Everything but these should be added to TFLM on an as needed basis and motivated by specific use-cases. We should probably add this note to the OP porting guide.\r\n\r\nChanges made, and I will add a note.\r\n \r\n> My only comment is that I would expect (hope?) that the datatypes supported in Lite are a superset of the datatypes needed for micro, so I'm not 100% sure what the 3rd commit means.\r\n\r\nLite did not handle `int8`, so I've had to add it here. If you wish, I can go back and add it to lite for consistency's sake.\r\n"]}, {"number": 45611, "title": "Tensorflow crashes python when using Keras Convolution/Max Pooling layers on rtx 3070", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): pip install tensorflow-gpu==2.4.0-rc0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1/8\r\n- GPU model and memory: RTX 3070 (8gb)\r\n\r\n**Current Behavior**\r\nRunning **\"Code 1\"** [in standalone code sect.] works fine and engages the GPU, obviously the results are garbage but it is an illustrative example. \r\n\r\nBy adding either a convolutional layer, max pooling layer, or both and running **\"Code 2\"** [in standalone code sect.] causes python to crash and yields the terminal output in the \"other info\" section. The error comes when the ```model.fit(...)``` call is made; the model is able to compile successfully but crashes on the first epoch of training. \r\n\r\n**Expected behavior**\r\nSince the standard sequential ANN trains fine on the GPU i would expect the CNN to be able to train as well. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n**Code 1** - works fine\r\n```import tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nprint(tf.config.list_physical_devices('GPU'))\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\n\r\nmodel.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\r\n\r\nhistory = model.fit(train_images, train_labels, batch_size=1, epochs=100)\r\n```\r\n**Code 2** - crashes when .fit(...) is called on model\r\n```import tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nprint(tf.config.list_physical_devices('GPU'))\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\n\r\nmodel.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\r\n\r\nhistory = model.fit(train_images, train_labels, batch_size=1, epochs=100)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2020-12-11 14:28:08.841765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-11 14:28:11.095172: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-11 14:28:11.096076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-11 14:28:11.124073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-11 14:28:11.124461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-11 14:28:11.136398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-11 14:28:11.136622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-11 14:28:11.139581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-11 14:28:11.140535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-11 14:28:11.147342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-11 14:28:11.149809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-11 14:28:11.150352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-11 14:28:11.150569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2020-12-11 14:28:11.153569: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-11 14:28:11.154591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-11 14:28:11.154972: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-11 14:28:11.155203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-11 14:28:11.155383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-11 14:28:11.155567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-11 14:28:11.155753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-11 14:28:11.155934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-11 14:28:11.156112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-11 14:28:11.156288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-11 14:28:11.156488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-11 14:28:11.595698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-11 14:28:11.595908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-12-11 14:28:11.596028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-12-11 14:28:11.596248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6177 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2020-12-11 14:28:11.597093: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-11 14:28:12.139175: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/100\r\n2020-12-11 14:28:12.386035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-11 14:28:13.019914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-11 14:28:13.024711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```", "comments": ["@taylrcawte,\r\nCould you please update TensorFlow to v2.4.0rc4 and check if it works.\r\n\r\nAlso, try setting a hard limit on the total GPU memory as per [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if you are facing the same issue. Thanks!", "@amahendrakar \r\nNo changes in behavior following the above recommendations; Version=2.4.0rc4 and limiting GPU memory growth with the below yields a python crash with ```Process finished with exit code -1073740791 (0xC0000409)```\r\n\r\nLimit memory growth with: \r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\r\n  try:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Virtual devices must be set before GPUs have been initialized\r\n    print(e)\r\n```", "@taylrcawte,\r\nI was able to run the code with TF v2.4 stable version without any issues. Please check the attached screenshot for reference. \r\n![Screenshot 2020-12-16 at 11 14 46 PM](https://user-images.githubusercontent.com/57165142/102387587-90df7380-3ff6-11eb-8fd0-aa65fadbadf3.png)\r\n\r\nCould you please try running the code in a virtual environment and check if it works. Thanks!", "it is being run in a venv already, and still crashes python with stable release.", "@amahendrakar i reinstalled Cuda11.0/cudnn8, and uninstalled and reinstalled tensorflow 2.4 and then ran the code again, i previously had cuda 11.1 and 10.2 installed on the same machine; is there a way to tell if the previous versions of cuda are interfering? \r\n\r\n```\r\n2020-12-17 11:04:21.846887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-17 11:04:23.182121: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-17 11:04:23.182665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-17 11:04:23.209308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-17 11:04:23.209700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-17 11:04:23.212767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-17 11:04:23.212993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-17 11:04:23.214872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-17 11:04:23.215484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-17 11:04:23.219111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-17 11:04:23.220411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-17 11:04:23.221049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-17 11:04:23.221260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-17 11:04:23.221809: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-17 11:04:23.222777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-17 11:04:23.223118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-17 11:04:23.223343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-17 11:04:23.223522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-17 11:04:23.223707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-17 11:04:23.223889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-17 11:04:23.224068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-17 11:04:23.224248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-17 11:04:23.224419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-17 11:04:23.224607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-17 11:04:23.643601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-17 11:04:23.643789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-12-17 11:04:23.643925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-12-17 11:04:23.644157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6177 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2020-12-17 11:04:23.644947: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n1 Physical GPUs, 1 Logical GPUs\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2020-12-17 11:04:24.926289: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/100\r\n2020-12-17 11:04:25.241425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-17 11:04:25.827191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-17 11:04:25.831846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```", "I was experiencing the exact same problem. The process will crash only when using a Convolution Layer, with\r\n`Process finished with exit code -1073740791 (0xC0000409)`\r\n\r\n\r\nSystem information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- TensorFlow version (use command below): pip install tensorflow-gpu==2.4.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: 11.0/8.0.1\r\n- GPU model and memory: RTX 2060 (6gb)\r\n\r\nI tried everything that I can find on the internet, but nothing solved the problem.\r\nIn the end I just downgraded to: \r\n- TensorFlow version (use command below): pip install tensorflow-gpu==2.3.0\r\n- CUDA/cuDNN version: 10.1/7.6.0\r\n\r\nThis did solve the issue and my model will compile and fit without problem.\r\n", "@MattYoon Thanks for the input but the 3000 series RTX cards are not compatible with Cuda < 11 because they are running on ampere architecture rather than turing; so for other users this may be helpful however for me it is not. ", "@taylrcawte,\r\nTensorFlow v2.4 is build and tested against CUDA 11 (not CUDA 11.1). \r\n\r\nIn this case could you please uninstall CUDA and cuDNN completely. Then install CUDA 11 with cuDNN 8 and check if it works. Thanks!", "@amahendrakar ; TF is being run with 11.0 and cudnn 8.0.3 and it still does not work. No other versions of cuda or cudnn are present on local machine anymore. \r\n\r\n> @amahendrakar i reinstalled Cuda11.0/cudnn8, and uninstalled and reinstalled tensorflow 2.4 and then ran the code again, i previously had cuda 11.1 and 10.2 installed on the same machine; is there a way to tell if the previous versions of cuda are interfering?\r\n> \r\n> ```\r\n> 2020-12-17 11:04:21.846887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n> 2020-12-17 11:04:23.182121: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2020-12-17 11:04:23.182665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n> 2020-12-17 11:04:23.209308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\n> coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n> 2020-12-17 11:04:23.209700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n> 2020-12-17 11:04:23.212767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n> 2020-12-17 11:04:23.212993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2020-12-17 11:04:23.214872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-12-17 11:04:23.215484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n> 2020-12-17 11:04:23.219111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-12-17 11:04:23.220411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n> 2020-12-17 11:04:23.221049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n> 2020-12-17 11:04:23.221260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n> 2020-12-17 11:04:23.221809: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-12-17 11:04:23.222777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\n> coreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n> 2020-12-17 11:04:23.223118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n> 2020-12-17 11:04:23.223343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n> 2020-12-17 11:04:23.223522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2020-12-17 11:04:23.223707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n> 2020-12-17 11:04:23.223889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n> 2020-12-17 11:04:23.224068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020-12-17 11:04:23.224248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n> 2020-12-17 11:04:23.224419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n> 2020-12-17 11:04:23.224607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n> 2020-12-17 11:04:23.643601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-12-17 11:04:23.643789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n> 2020-12-17 11:04:23.643925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n> 2020-12-17 11:04:23.644157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6177 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n> 2020-12-17 11:04:23.644947: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 1 Physical GPUs, 1 Logical GPUs\r\n> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n> 2020-12-17 11:04:24.926289: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n> Epoch 1/100\r\n> 2020-12-17 11:04:25.241425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n> 2020-12-17 11:04:25.827191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n> 2020-12-17 11:04:25.831846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n> Process finished with exit code -1073740791 (0xC0000409)\r\n> ```\r\n\r\n", "@ymodak update; i did a clean install of windows, reinstalled cudnn 8 and cuda 11 and still get the same crash and error message. ", "@ymodak update; when running the program from terminal and not in an IDE the error message is \r\n\r\n```\r\nCould not load library cudnn_ops_infer64_8.dll. Error code 126\r\nPlease make sure cudnn_ops_infer64_8.dll is in your library path!\r\n```", "@ymodak \r\nUPDATE:\r\n\r\nFor whatever reason when run in the IDE terminal an error message was being suppressed and ```Process finished with exit code -1073740791 (0xC0000409)``` was logged as the error message. \r\n\r\nWhen run from the command line the below error messages were displayed instead of logging the exit code error.\r\n```\r\nCould not load library cudnn_ops_infer64_8.dll. Error code 126\r\nPlease make sure cudnn_ops_infer64_8.dll is in your library path!\r\n```\r\nI recognized this was a package included in the cudnn library and copy and pasted it from the bin folder in cudnn to NVIDIA GPU computing toolkit > CUDA > V11.0 > bin. This process was repeated for the below packages and the issue was resolved.\r\n```\r\ncudnn_adv_infer64_8.dll\r\ncudnn_adv_train64_8.dll\r\ncudnn_cnn_infer64_8.dll\r\ncudnn_cnn_train64_8.dll\r\ncudnn_ops_infer64_8.dll\r\ncudnn_ops_train64_8.dll\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45611\">No</a>\n", "> @ymodak\r\n> UPDATE:\r\n> \r\n> For whatever reason when run in the IDE terminal an error message was being suppressed and `Process finished with exit code -1073740791 (0xC0000409)` was logged as the error message.\r\n> \r\n> When run from the command line the below error messages were displayed instead of logging the exit code error.\r\n> \r\n> ```\r\n> Could not load library cudnn_ops_infer64_8.dll. Error code 126\r\n> Please make sure cudnn_ops_infer64_8.dll is in your library path!\r\n> ```\r\n> \r\n> I recognized this was a package included in the cudnn library and copy and pasted it from the bin folder in cudnn to NVIDIA GPU computing toolkit > CUDA > V11.0 > bin. This process was repeated for the below packages and the issue was resolved.\r\n> \r\n> ```\r\n> cudnn_adv_infer64_8.dll\r\n> cudnn_adv_train64_8.dll\r\n> cudnn_cnn_infer64_8.dll\r\n> cudnn_cnn_train64_8.dll\r\n> cudnn_ops_infer64_8.dll\r\n> cudnn_ops_train64_8.dll\r\n> ```\r\n\r\nAfter struggling to get this work for the last 3 weeks,  Installing different versions of CUDA and Tensorflow over and over again finally it worked.\r\n\r\nI can't believe I was making such a silly mistake over and over again the solution was right in front of me but I chose to ignore it.\r\n\r\nThank you @taylrcawte for sharing the update.", "The problem is caused due to OOM issues\r\nMy GPU RTX 3060 jumped to 12 GB as soon as a started training a simple CNN-\r\n\r\nThe only thing that helped in my case was:\r\npip install tf-nightly-gpu\r\n\r\nResolved all the problems"]}, {"number": 45610, "title": "Fix convert_variables_to_constants_v2 for scalar valued captures", "body": "tensor.numpy() can return a scalar, not a numpy array.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45610) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 45609, "title": "Tensorflow_datsaet api fails to cache my training data.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: V100 (16GB)\r\n\r\n**Describe the current behavior**\r\nHere is the error I get:\r\n![Screenshot from 2020-12-11 16-39-07](https://user-images.githubusercontent.com/70702200/101952699-727d1080-3bcf-11eb-8aaf-cb55fe37bec4.png)\r\n\r\n\r\nThe calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nI run a custom data loader:\r\n```\r\nstart_time = time.clock()\r\n\r\ndata_loader = None\r\n\r\nif translation_direction in [Translation_Direction.English_to_Farsi,Translation_Direction.Farsi_to_English]:\r\n    data_loader = Data_loader_persian(translation_direction, fa_vocab_size=12000, en_vocab_size=24000)\r\n    \r\nelse:\r\n    raise Exception(\"Requested Translation Direction is not supported!\")\r\n    \r\ntrain_examples, val_examples, test_examples, dataset_size_dict, tokenizer = data_loader.run()\r\n\r\nend_time = time.clock()\r\nprint(f\"Execution time: {end_time - start_time} seconds.\")\r\n```\r\n\r\nThen I try to cache and prefetch the dataset:\r\n```\r\ntrain_dataset = train_examples.cache()\r\ntrain_dataset = train_dataset.padded_batch(global_batch_size)\r\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n```\r\n\r\nThen when I try to read the dataset:\r\n```\r\nfor i,x in enumerate(train_examples):\r\n    if i==5:\r\n        break\r\n```\r\nHere is when the error comes up! No matter how many times I try to read, every times it reads from file (no caching happens!)\r\n\r\nNote: Screenshot of the profiling window is attaches. As could be seen, the program is HIGHLY input-bound.\r\nI would appreciate if you have any idea. Thanks :) ", "comments": ["It would be helpful if you could provide a standalone reproducible code example. Without being able to see custom components these things are hard to debug.\r\n\r\nAlso note that `cache` works at the **second** epoch of training. The data is read from disk (or cachefile, if specified), cached in your memory during the first epoch (provided your memory is large enough, otherwise the code will kill itself) and then read from memory from the second epoch onwards. This is at least the case for a \"regular\" dataset, where each datapoint is fed into the model once per epoch.", "@Arman-IMRSV \r\nI ran the code shared its incomplete , please share complete code or if possible share a  colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45609\">No</a>\n"]}, {"number": 45608, "title": "Micro: port op CAST from Lite", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): master\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n\r\n**Describe the problem**\r\nI am about to port The TF Lite kernel op CAST to TF Lite Micro.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nPR 1: refactor flatbuffer_conversions parsing function\r\nPR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes.\r\nPR 3: copy the reference kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build.\r\n", "comments": ["The operator CAST has been ported to TFLIte Micro. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45608\">No</a>\n", "There is a bug in the micro test code. Re-opening the issue to fix it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45608\">No</a>\n"]}, {"number": 45606, "title": "TFLM Added optimized fully connected (float32 and int8) for CEVA-BX1", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Github issue: https://github.com/tensorflow/tensorflow/issues/45607", "Also, I have created a new label (comp:micro:ceva). If you could apply that to all your PRs and github issues, that would make tracking this work easier. And link the issue from your PR as well.\r\n\r\nYou should have the permissions needed to be able to apply labels.\r\n\r\nYou can directly add me as a reviewer, have yourself be the assignee for the github issues.", "I may have reviewed this PR before it was ready -- sorry about that. If possible, please explicitly add a comment letting me know that the PR is ready for review (similar to https://github.com/tensorflow/tensorflow/pull/46411#issuecomment-761210727)", "> I may have reviewed this PR before it was ready -- sorry about that. If possible, please explicitly add a comment letting me know that the PR is ready for review (similar to [#46411 (comment)](https://github.com/tensorflow/tensorflow/pull/46411#issuecomment-761210727))\r\n\r\nThis is my fault, I should have closed this one - I created a new PR after our chat -  incorporating code sharing with the reference kernel - see https://github.com/tensorflow/tensorflow/pull/46500 and also https://github.com/tensorflow/tensorflow/pull/46226 for Quantize.\r\nI'll go over your comments here and apply them to those PRs where applicable.\r\n", "sounds good, I'll go ahead and close this PR then. Let me know when the others are ready for review."]}, {"number": 45605, "title": "No documentation on how to access the ram:// filesystem of TPU's", "body": "According to this comment there is support to access the RAM filesystem of TPU's: https://github.com/tensorflow/tensorflow/issues/32651#issuecomment-534685146\r\n\r\nHowever, the documentation seems to be lacking information on the subject (although it's a bit hard to search, as the search term 'ram' (special characters are stripped) returns a lot of irrelevant results.\r\n\r\nIs the documentation indeed missing, or is the feature maybe not implemented yet?\r\n\r\n\r\n", "comments": ["TF filesystems use a URI to refer to paths. The scheme of the URI specifies which filesystem to use. So\r\n\r\n```\r\nwith open(\"ram://path/to/file\") as f:\r\n  print(f.read())\r\n```\r\n\r\nWill use `/path/to/file` from the RAM filesystem. Changing the path to be `s3://path/to/file` will use the S3 filesystem, etc.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you. And how would one move data to the ram filesystem (as opposed to reading)?", "All methods under [`tf.io.gfile`](https://www.tensorflow.org/api_docs/python/tf/io/gfile) can be used transparently across filesystems.\r\n\r\n```\r\ntf.io.gfile.copy(\"/tmp/local/file\", \"ram://ram_file\")\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45605\">No</a>\n"]}, {"number": 45604, "title": "No gradients available error", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n\r\nUsing a minus sign before the loss in tape.gradient gives a ValueError: No gradients provided for any variable whereas there should be.\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nlayers = tf.keras.layers\r\n\r\n\r\ndef _create_mlp(\r\n        hidden_size: int = 1024, output_size: int = 256\r\n):\r\n    return tf.keras.Sequential(\r\n        [\r\n            layers.Dense(hidden_size, activation=None, use_bias=True),\r\n            layers.BatchNormalization(),\r\n            layers.Activation(\"relu\"),\r\n            layers.Dense(hidden_size, activation=None, use_bias=True),\r\n            layers.BatchNormalization(),\r\n            layers.Activation(\"relu\"),\r\n            layers.Dense(output_size, activation=None, use_bias=True)\r\n        ]\r\n    )\r\n\r\n\r\n\r\nclass ModelTest(tf.keras.Model):\r\n    def __init__(self):\r\n        super(ModelTest, self).__init__()\r\n\r\n        self.generator = _create_mlp(output_size=128)\r\n\r\n    def train_step(self, inputs):\r\n\r\n        with tf.GradientTape() as gen_tape:\r\n\r\n            z = self.generator(inputs)\r\n\r\n            final_loss = tf.reduce_mean(z)\r\n\r\n        gen_gradients = gen_tape.gradient(-final_loss, self.generator.trainable_variables)\r\n\r\n        self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\r\n\r\n        return {}\r\n\r\n\r\ninput_data = np.random.uniform(size=(200, 32))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(input_data)\r\n\r\ndataset = dataset.batch(20)\r\n\r\nmodel = ModelTest()\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n\r\nmodel.compile(optimizer=optimizer, run_eagerly=True)\r\n\r\nmodel.fit(dataset, epochs=20)\r\n```\r\n\r\nThis code gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tests/model_test.py\", line 57, in <module>\r\n    model.fit(dataset, epochs=20)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 572, in train_function\r\n    self.train_step, args=(data,))\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 951, in run\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2290, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2649, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 282, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"tests/model_test.py\", line 40, in train_step\r\n    self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 472, in apply_gradients\r\n    grads_and_vars = _filter_grads(grads_and_vars)\r\n  File \"/home/guillaume/miniconda3/envs/tensorflow-gpu/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1219, in _filter_grads\r\n    ([v.name for _, v in grads_and_vars],))\r\nValueError: No gradients provided for any variable: ['sequential/dense/kernel:0', 'sequential/dense/bias:0', 'sequential/batch_normalization/gamma:0', 'sequential/batch_normalization/beta:0', 'sequential/dense_1/kernel:0', 'sequential/dense_1/bias:0', 'sequential/batch_normalization_1/gamma:0', 'sequential/batch_normalization_1/beta:0', 'sequential/dense_2/kernel:0', 'sequential/dense_2/bias:0'].\r\n```\r\n\r\nIf you remove the minus sign before final loss in gen_tape.gradient, the code works fine.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nlayers = tf.keras.layers\r\n\r\n\r\ndef _create_mlp(\r\n        hidden_size: int = 1024, output_size: int = 256\r\n):\r\n    return tf.keras.Sequential(\r\n        [\r\n            layers.Dense(hidden_size, activation=None, use_bias=True),\r\n            layers.BatchNormalization(),\r\n            layers.Activation(\"relu\"),\r\n            layers.Dense(hidden_size, activation=None, use_bias=True),\r\n            layers.BatchNormalization(),\r\n            layers.Activation(\"relu\"),\r\n            layers.Dense(output_size, activation=None, use_bias=True)\r\n        ]\r\n    )\r\n\r\n\r\n\r\nclass ModelTest(tf.keras.Model):\r\n    def __init__(self):\r\n        super(ModelTest, self).__init__()\r\n\r\n        self.generator = _create_mlp(output_size=128)\r\n\r\n    def train_step(self, inputs):\r\n\r\n        with tf.GradientTape() as gen_tape:\r\n\r\n            z = self.generator(inputs)\r\n\r\n            final_loss = tf.reduce_mean(z)\r\n\r\n        gen_gradients = gen_tape.gradient(final_loss, self.generator.trainable_variables)\r\n\r\n        self.optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\r\n\r\n        return {}\r\n\r\n\r\ninput_data = np.random.uniform(size=(200, 32))\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(input_data)\r\n\r\ndataset = dataset.batch(20)\r\n\r\nmodel = ModelTest()\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n\r\nmodel.compile(optimizer=optimizer, run_eagerly=True)\r\n\r\nmodel.fit(dataset, epochs=20)\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\n\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 2/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 3/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 4/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 5/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 6/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 7/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 8/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 9/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 10/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 11/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 12/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 13/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 14/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 15/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 16/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 17/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 18/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 19/20\r\n10/10 [==============================] - 0s 11ms/step\r\nEpoch 20/20\r\n10/10 [==============================] - 0s 11ms/step\r\n\r\n```", "comments": ["Why you don't negate the loss inside the gradient tape?", "I have tried in colab with TF 2.2, 2.3, nightly version(`2.5.0-dev20201213`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5e41564c21c67bb16b6056d58c88e28c/untitled575.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Without negating the loss in gradient_tape, the code works fine but it raises an error \"No gradients provided for any variable\" with the minus sign. This behavior is clearly not the one expected.", "> Without negating the loss in gradient_tape, the code works fine but it raises an error \"No gradients provided for any variable\" with the minus sign. This behavior is clearly not the one expected.\n\nWhy you don't negate the loss in the gradient_tape?", "I don't understand your question. Please elaborate.", "I meant to negate `\u200btf\u200b.\u200breduce_mean\u200b(\u200bz\u200b)`", "Ok yes I understand. Because the last operation (multiplication by -1) is done outside of the  \"with tf.GradientTape() as gen_tape:\", the gradients cannot be computed.\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45604\">No</a>\n"]}, {"number": 45603, "title": "BoostedTreesClassifier segmentation fault when evaluate or predict is called", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Os X 10.15.7\r\n- TensorFlow installed from (source or binary): binary (pip installed)\r\n- TensorFlow version (use command below): tried with both 2.3.1 and 2.3.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: running on Mac, no GPU support\r\n- GPU model and memory: as above\r\n\r\nusing this [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n```\r\n== check python ===================================================\r\npython version: 3.8.5\r\npython branch:\r\npython build version: ('default', 'Sep 30 2020 08:41:45')\r\npython compiler version: Clang 11.0.0 (clang-1100.0.33.17)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple clang version 11.0.0 (clang-1100.0.33.17)\r\nTarget: x86_64-apple-darwin19.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                    1.18.5\r\nprotobuf                 3.14.0\r\ntensorflow               2.3.0\r\ntensorflow-estimator     2.3.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.3.0\r\ntf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nrun.sh:145: command not found: nvidia-smi\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.3.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/cesco/Desktop/venv/lib/python3.8/site-packages\r\nRequired-by:\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 5, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nI took [this notebook](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding) and tried to run it on my local machine but it repeatedly crashed the jupyter kernel when trying to evaluate the model (the same happens with the .predict() method).\r\nI then converted the notebook into a simple python script (I thought that perhaps is an issue with jupyter) using the command `ipython nbconvert --to python` and tried to run it.\r\nThis is the full output I got:\r\n```\r\nFeature value: \"Third\"\r\n2020-12-11 14:07:55.153062: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-11 14:07:55.167575: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd8db882480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-11 14:07:55.167595: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nOne-hot encoded:  [[0. 0. 1.]]\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/4w/m2lqr9ts0q1dtnzgsx9gr3_40000gn/T/tmpy159zb62\r\nWARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\nWARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1471: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.add_weight` method instead.\r\nWARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:111: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\naccuracy                  0.765152\r\naccuracy_baseline         0.625000\r\nauc                       0.832844\r\nauc_precision_recall      0.789631\r\naverage_loss              0.478908\r\nlabel/mean                0.375000\r\nloss                      0.478908\r\nprecision                 0.703297\r\nprediction/mean           0.350790\r\nrecall                    0.646465\r\nglobal_step             100.000000\r\ndtype: float64\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/4w/m2lqr9ts0q1dtnzgsx9gr3_40000gn/T/tmpwwx0o5_p\r\nWARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:398: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:From /Users/cesco/Desktop/venv/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/head.py:637: auc (from tensorflow.python.ops.metrics_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe value of AUC returned by this may race with the update so this is deprecated. Please use tf.keras.metrics.AUC instead.\r\nWARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\r\nWARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\r\n[1]    76205 segmentation fault  python boosted_trees.py\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect the jupyter notebook kernel not to crash, and the script not to have a segmentation fault.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe part that cause the crash is when the BoostedTreesClassifier is used for predict or estimate.\r\nThe shortest script I could come up to reproduce is:\r\n```python\r\nimport pandas as pd\r\n\r\n# Load dataset.\r\ndftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\r\ndfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\r\ny_train = dftrain.pop('survived')\r\ny_eval = dfeval.pop('survived')\r\n\r\n\r\nimport tensorflow as tf\r\ntf.random.set_seed(123)\r\n\r\n\r\nCATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\r\n                       'embark_town', 'alone']\r\nNUMERIC_COLUMNS = ['age', 'fare']\r\n\r\ndef one_hot_cat_column(feature_name, vocab):\r\n  return tf.feature_column.indicator_column(\r\n      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\r\n                                                 vocab))\r\nfeature_columns = []\r\nfor feature_name in CATEGORICAL_COLUMNS:\r\n  # Need to one-hot encode categorical features.\r\n  vocabulary = dftrain[feature_name].unique()\r\n  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n\r\nfor feature_name in NUMERIC_COLUMNS:\r\n  feature_columns.append(tf.feature_column.numeric_column(feature_name,\r\n                                           dtype=tf.float32))\r\n\r\n\r\nexample = dict(dftrain.head(1))\r\nclass_fc = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('class', ('First', 'Second', 'Third')))\r\n\r\n\r\n# Use entire batch since this is such a small dataset.\r\nNUM_EXAMPLES = len(y_train)\r\n\r\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n  def input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\r\n    if shuffle:\r\n      dataset = dataset.shuffle(NUM_EXAMPLES)\r\n    # For training, cycle thru dataset as many times as need (n_epochs=None).\r\n    dataset = dataset.repeat(n_epochs)\r\n    # In memory training doesn't use batching.\r\n    dataset = dataset.batch(NUM_EXAMPLES)\r\n    return dataset\r\n  return input_fn\r\n\r\n# Training and evaluation input functions.\r\ntrain_input_fn = make_input_fn(dftrain, y_train)\r\neval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\r\n\r\n\r\nn_batches = 1\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\r\n                                          n_batches_per_layer=n_batches)\r\n\r\n# The model will stop training once the specified number of trees is built, not\r\n# based on the number of steps.\r\nest.train(train_input_fn, max_steps=100)\r\n\r\n# Eval.\r\nest.evaluate(eval_input_fn)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nFrom pip freeze:\r\n```\r\nabsl-py==0.10.0\r\nappnope==0.1.2\r\nargon2-cffi==20.1.0\r\nastunparse==1.6.3\r\nasync-generator==1.10\r\nattrs==20.3.0\r\nbackcall==0.2.0\r\nbleach==3.2.1\r\ncachetools==4.2.0\r\ncertifi==2020.12.5\r\ncffi==1.14.4\r\nchardet==3.0.4\r\ncycler==0.10.0\r\ndecorator==4.4.2\r\ndefusedxml==0.6.0\r\ndill==0.3.3\r\nentrypoints==0.3\r\nfuture==0.18.2\r\ngast==0.3.3\r\ngoogle-auth==1.23.0\r\ngoogle-auth-oauthlib==0.4.2\r\ngoogle-pasta==0.2.0\r\ngoogleapis-common-protos==1.52.0\r\ngrpcio==1.34.0\r\nh5py==2.10.0\r\nidna==2.10\r\nimportlib-resources==3.3.0\r\nipykernel==5.4.0\r\nipython==7.19.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\njedi==0.17.2\r\nJinja2==2.11.2\r\njoblib==0.17.0\r\njsonschema==3.2.0\r\njupyter==1.0.0\r\njupyter-client==6.1.7\r\njupyter-console==6.2.0\r\njupyter-core==4.7.0\r\njupyterlab-pygments==0.1.2\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.1\r\nMarkdown==3.3.3\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.3.3\r\nmistune==0.8.4\r\nnbclient==0.5.1\r\nnbconvert==6.0.7\r\nnbformat==5.0.8\r\nnest-asyncio==1.4.3\r\nnotebook==6.1.5\r\nnumpy==1.18.5\r\noauthlib==3.1.0\r\nopt-einsum==3.3.0\r\npackaging==20.7\r\npandas==1.1.5\r\npandocfilters==1.4.3\r\nparso==0.7.1\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==8.0.1\r\nprometheus-client==0.9.0\r\npromise==2.3\r\nprompt-toolkit==3.0.8\r\nprotobuf==3.14.0\r\nptyprocess==0.6.0\r\npyarrow==2.0.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycparser==2.20\r\nPygments==2.7.3\r\npyparsing==2.4.7\r\npyrsistent==0.17.3\r\npython-dateutil==2.8.1\r\npytz==2020.4\r\npyzmq==20.0.0\r\nqtconsole==5.0.1\r\nQtPy==1.9.0\r\nrequests==2.25.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.6\r\nscikit-learn==0.23.2\r\nscipy==1.4.1\r\nSend2Trash==1.5.0\r\nsix==1.15.0\r\nsklearn==0.0\r\ntensorboard==2.4.0\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow==2.3.0\r\ntensorflow-estimator==2.3.0\r\ntermcolor==1.1.0\r\nterminado==0.9.1\r\ntestpath==0.4.4\r\nthreadpoolctl==2.1.0\r\ntornado==6.1\r\ntqdm==4.54.1\r\ntraitlets==5.0.5\r\nurllib3==1.26.2\r\nwcwidth==0.2.5\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.1\r\nwidgetsnbextension==3.5.1\r\nwrapt==1.12.1\r\n```\r\n", "comments": ["Further info. I now tried to run it inside docker python:3.8 and it works.\r\nIt seems to be an issue with Mac Os X.\r\n\r\nI asked a colleague of mine (who also has a mac) to try the same and he got this error instead (again at the `.evaluate()` point):\r\n[libprotobuf FATAL ./google/protobuf/arena_impl.h:160] CHECK failed: (limit_) >= (ptr_):\r\nlibc++abi.dylib: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: (limit_) >= (ptr_):", "@francesco086,\r\nI was able to run the code with TensorFlow v2.3 on Linux without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1734a4115adbd4ac9ac526a10e45c805/45603.ipynb#scrollTo=BjqXqPiOEbUE&line=4&uniqifier=1).\r\n\r\nCould you please try running the code in a new virtual environment and with TF v2.4.0rc4 or TF-nightly and check if you are facing the same issue? Thanks!", "Please read the message above yours - with Docker it worked. It is an issue on Mac Os X.", "@francesco086,\r\nThank you for the update. The gist was just to verify if there is an issue with the code itself, which does not seem to be the case here. \r\n\r\nCould you please check if you are facing the same issue with TF v2.4 and TF-nightly as well. Thanks!", "@amahendrakar \r\n\r\nTried with v2.4.0 and it works. Thanks :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45603\">No</a>\n"]}, {"number": 45602, "title": "keras load_model Could not find matching function when compute_mask is used", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nhttps://colab.research.google.com/drive/1Q9kiMw5k1uVtCpIbOEhui3h83_8IfN3t?usp=sharing\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\ncolab\r\n\r\n- TensorFlow installed from (source or binary):\r\ntf-nightly\r\n\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nValueError exception is raised when attempting to load a saved model that uses a ```compute_mask``` method.\r\n\r\nIn this example https://colab.research.google.com/drive/1Q9kiMw5k1uVtCpIbOEhui3h83_8IfN3t?usp=sharing\r\na custom layer copied from the documentation is added to a Sequential model. The model is compiled and saved. The function ```keras.models.load_model``` raises and exception when attempting to reconstruct the model.\r\n\r\nIt seems probable that the model is not taking computed masks into account when attempting to reconstruct the graph. The error message leads me to believe that it is unaware that a mask producer can pass a mask to a mask consumer.\r\n\r\n**Describe the expected behavior**\r\nCanonical examples described in the documentation should be able to be saved and loaded.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Q9kiMw5k1uVtCpIbOEhui3h83_8IfN3t?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (2 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 16, 8), dtype=float32)\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (2 total):\r\n    * TensorSpec(shape=(None, 16, 8), dtype=tf.float32, name='inputs')\r\n    * TensorSpec(shape=(None, 16), dtype=tf.bool, name='mask')\r\n  Keyword arguments: {}\r\n```\r\n\r\n**Workaround:**\r\nIf the model is saved with the ```save_traces``` flag set to False  it can be successfully saved and loaded.\r\nExample:\r\nhttps://colab.research.google.com/drive/1WRPqSzu41waymZRr_9wqpojhVEi7EsCV?usp=sharing", "comments": ["You need to use custom object on load. Please see Docs at:\r\nhttps://keras.io/api/utils/serialization_utils/\r\n\r\nP.s. /cc @MarkDaoust I think this Is one of the edge cases where we could improve the error with a more user friendly message.", "@bhack I've confirmed that using custom_objects works. i.e. the \"workaround\" colab works even if save_traces is True.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45602\">No</a>\n"]}, {"number": 45601, "title": "Cannot convert numpy array to tensor in Image Data Generator", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04\r\n-   **TensorFlow installed from (source or binary)**: Installed using Lambda Stack\r\n-   **TensorFlow version (use command below)**: 2.3.1\r\n-   **Python version**: 3.8.5\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 11\r\n-   **GPU model and memory**: RTX 3080 10GB\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to use keras image data generator to feed data into a pretrained mobilenet v2 model followed by global average pooling and one dense layer. Inputs are 224 * 224 * 3 images and outputs are the coordinates of a rectangular enclosure in the image. The coordinates are in a numpy array. But when the data generator tries to load data during model.fit, the following error is shown.\r\n\r\n\r\n### Source code / logs\r\n```\r\ndatagen = ImageDataGenerator(brightness_range=[0.2,1.2])\r\ntrain_generator = datagen.flow_from_dataframe(dataframe=df, \r\n                                              directory='./',\r\n                                              x_col=\"filename\", \r\n                                              y_col=\"diagonals\", \r\n                                              class_mode=\"raw\", \r\n                                              target_size=(224, 224), \r\n                                              batch_size=16,\r\n                                              rescale=1.0/255.0)\r\n\r\n```\r\n```\r\n\r\nmodel.compile(optimizer=Adam(lr=0.001),loss='mse')\r\nmodel.fit(train_generator,\r\n                    epochs = 200,\r\n                    batch_size = 16,\r\n                    callbacks=[es, checkpoint]\r\n)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-31-4293be728923> in <module>\r\n----> 1 history = model.fit(train_generator,\r\n      2                     epochs = 200,\r\n      3                     batch_size = 16,\r\n      4                     callbacks=[es, checkpoint]\r\n      5 )\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1047          training_utils.RespectCompiledTrainableState(self):\r\n   1048       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\r\n-> 1049       data_handler = data_adapter.DataHandler(\r\n   1050           x=x,\r\n   1051           y=y,\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\r\n   1103 \r\n   1104     adapter_cls = select_data_adapter(x, y)\r\n-> 1105     self._adapter = adapter_cls(\r\n   1106         x,\r\n   1107         y,\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\r\n    907     self._keras_sequence = x\r\n    908     self._enqueuer = None\r\n--> 909     super(KerasSequenceAdapter, self).__init__(\r\n    910         x,\r\n    911         shuffle=False,  # Shuffle is handed in the _make_callable override.\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\r\n    786     peek, x = self._peek_and_restore(x)\r\n    787     peek = self._standardize_batch(peek)\r\n--> 788     peek = _process_tensorlike(peek)\r\n    789 \r\n    790     # Need to build the Model on concrete input shapes.\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _process_tensorlike(inputs)\r\n   1019     return x\r\n   1020 \r\n-> 1021   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)\r\n   1022   return nest.list_to_tuple(inputs)\r\n   1023 \r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x)\r\n   1014       if issubclass(x.dtype.type, np.floating):\r\n   1015         dtype = backend.floatx()\r\n-> 1016       return ops.convert_to_tensor(x, dtype=dtype)\r\n   1017     elif scipy_sparse and scipy_sparse.issparse(x):\r\n   1018       return _scipy_sparse_to_sparse_tensor(x)\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1497 \r\n   1498     if ret is None:\r\n-> 1499       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1500 \r\n   1501     if ret is NotImplemented:\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n     54 \r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    261     ValueError: if called on a symbolic tensor.\r\n    262   \"\"\"\r\n--> 263   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    264                         allow_broadcast=True)\r\n    265 \r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    273       with trace.Trace(\"tf.constant\"):\r\n    274         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 275     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    276 \r\n    277   g = ops.get_default_graph()\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    298 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    299   \"\"\"Implementation of eager constant.\"\"\"\r\n--> 300   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    301   if shape is None:\r\n    302     return t\r\n\r\n/usr/lib/python3/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n     97   ctx.ensure_initialized()\r\n---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n    100 \r\n\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\r\n\r\n```", "comments": ["Found a very similar issue on stack overflow without solution [here](https://stackoverflow.com/questions/64978209/valueerror-when-trying-to-execute-model-fit-failed-to-convert-a-numpy-array/)", "@smdshakeelhassan \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45600, "title": "[XLA] [Docs] Fix typo in operation semantics", "body": "All of `T_N` occurred in the operation semantics doc should be `T_{N-1}` actually, because the first index is 0, and the lengths of the arrays are N.\r\nThe format of the subscript referred to `Map Section`", "comments": []}, {"number": 45599, "title": "TF-TRT Test ConvertQuantize in dynamic shape mode", "body": "This PR adds explicit batch and dynamic shape mode test to ConvertQuantize.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n\r\nTracker:  #45481", "comments": ["@tfeher Can you please check @DEKHTIARJonathan's comments and keep us posted ? Thanks!", "@tfeher Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@tfeher Any update on this PR? Please. Thanks!", "@tfeher Any update on this PR? Please. Thanks!"]}]