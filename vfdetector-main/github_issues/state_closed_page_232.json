[{"number": 47591, "title": "Fix reported snyk vulnerabilities with the recommended patch", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47591) for more info**.\n\n<!-- need_sender_cla -->", "Can you list the vulnerabilities discovered?", "Here's the Snyk scan report for r2.2: [TF 2.2.2 Snyk Scan Report](https://snyktf22.tiiny.site/)\r\n\r\nThese seem to have been fixed in the recent TF 2.4.x. The following are the respective commits for these fixes. I put together all the individual fixes into one in this PR.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/c1e1fc899ad5f8c725dcbb6470069890b5060bc7\r\nhttps://github.com/tensorflow/tensorflow/commit/eccb7ec454e6617738554a255d77f08e60ee0808\r\nhttps://github.com/tensorflow/tensorflow/pull/42143/commits/3ade2efec2e90c6237de32a19680caaa3ebc2845\r\n\r\nGetting rid of these vulnerabilities would be beneficial for users who still don't want to upgrade to the latest TF r2.4\r\n\r\n", "Something is wrong though. c1e1fc8 is included in TF 2.2.2, see [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.2)\r\n\r\nWill pick the other two when we make a new patch release on the branch.", "Thank you @mihaimaruseac Sorry for not noticing that c1e1fc8 was already a part of r2.2.\r\n\r\nDoes this also mean the PyPi packages of r2.2 will be updated when you apply the patch? And, any idea when we can expect the patched release to happen? This information will be helpful to make decisions on my current work as it needs r2.2 to be clear of vulnerabilities. Thanks in advance! \ud83d\udc4d ", "We don't release timelines for the patch releases, but most likely it will happen by end of month/early April.\r\n\r\n2.2 will get patched once this lands."]}, {"number": 47590, "title": "TF-TRT SpaceToDepth and DepthToSpace op converters for dynamic shape mode.", "body": "This PR updates the converter for the SpaceToDepth and DepthToSpace ops  to work with explicit batch and dynamic shape mode.\r\n\r\nWhile the explicit batch converter needs only a slight modification to insert the batch dim, the dynamic shape converter has to redefine the same target shape calculation using TRT shape tensor arithmetic.\r\n\r\nThe unit test are updated to run all three trt modes. The parameters are not modified, just refactored to allow execution in all trt modes.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\n\r\nTracker:  #45481", "comments": ["https://github.com/tensorflow/tensorflow/pull/47590#discussion_r599123547\r\n\r\nPrefer using return values over output parameters: they improve readability, and often provide the same or better performance"]}, {"number": 47589, "title": "CMSIS-NN - Missing compiler warning flags", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  184cf2d642885013d533227d189232d9a5b7ffab\r\n- Python version: NA\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): ARMCLANG6.15\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\nCompiler warnings are not enabled for CMSIS-NN. I believe it should be the same for other optimized libraries as well.\r\n\r\n\r\nTFLM kernels source files have the following warning flags enabled: TFLM          -> -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter\r\n\r\nCMSIS-NN have these set for warnings: -Wno-type-limits -Wno-unused-private-field\r\n\r\n(A fuill example line: arm-none-eabi-gcc -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -DCORTEX_M_GENERIC -DCMSIS_NN -mcpu=cortex-m7+nofp -mfpu=auto -DTF_LITE_MCU_DEBUG_LOG -mthumb -mfloat-abi=soft -funsigned-char -mlittle-endian -Wno-type-limits -Wno-unused-private-field -fomit-frame-pointer -MD -DCPU_M7=1 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/cmsis -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/ConvolutionFunctions/arm_depthwise_conv_s8_opt.c -o tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m7_default/obj/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/ConvolutionFunctions/arm_depthwise_conv_s8_opt.o\r\n)\r\n\r\n **Is the intention here that , warnings are not enabled for external libraries or is it something that ought to be fixed?**\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m7 OPTIMIZED_KERNEL_DIR=cmsis_nn microlite\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The problem seems to be that by default tensorflow/lite/micro/examples/micro_speech/Makefile.inc is included in the main Makefile and in this file, warnings are suppressed, i.e.:\r\nCCFLAGS := $(filter-out $(CC_WARNINGS),$(CCFLAGS))\r\nSince only CCFLAGS, which the CMSIS kernels use is filtered it gives this behavior.\r\n@advaitjain  Do you think we can just remove the suppressed warnings in the micro speech example or will that break other targets? Or do we need a mechanism to only do this filtering specifically if building the micro speech example?\r\n\r\nAt least currently to build CMSIS-NN for target cortex_m_generic (when removing this filtering) these are needed:\r\n  -Wno-implicit-fallthrough \\\r\n  -Wno-strict-aliasing\r\nThose could easily be added to cortex_m_generic_makefile.inc.", "I would guess that removing the suppressed warnings would result in some other failure. Adding in additional flags (e.g. what you have for cortex_m_generic) is very reasonable. If you want, you can certainly give this approach a try.\r\n\r\nIf it turns out that we need a mechanism in the Makefiles to selectively add or remove compiler flags, then the direction that I would like to go in, but have not had time to pursue is to have separate build targets for each of the third_party libraries instead of building everything via THIRD_PARTY_CC_SRCS.\r\n\r\nSomething like:\r\n * have cmsis_nn.inc define a make target (say `cmsis:`) with full control over the CFLAGS etc.\r\n * have a common THIRD_PARTY_TARGETS variable that `cmsis` is appended to, and is used to make sure the dependencies are properly built before creating the final static library.\r\n * we would still want to provide a single libtensorflow-microlite.a generated from all the object files\r\n\r\nLet me know if you would like to go down a path like this and I can come up with some more concrete suggestions.", "I am not sure if there is a need for such a mechanism since every target can control the warning level.\r\nSince one can anyway only build the tflu-lib with the cortex_m_generic target, I propose to just filter out the micro speech example test.", "Ah yes, for the cortex_m_generic target filtering out the test works.", "@felix-johnny,\r\n\r\nAs @mansnils has submitted a PR which was already merged. Can you confirm if we are good to close this issue? Thanks!", "Yes we can close it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47589\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47589\">No</a>\n"]}, {"number": 47588, "title": "My python version is 3.6. I successfully installed tensorflow, but when I imported in idle it gives error as following :", "body": ">>> import tensorflow\r\n2021-03-05 18:13:41.002427: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-03-05 18:13:41.003385: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sushi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\sushi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\sushi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"C:\\Users\\sushi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\sushi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 48, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.", "comments": ["@oberon98 \r\nPlease refer to these resolved issues and let us know: #47517, #45398", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47587, "title": "Failed to build TF on Ubuntu 18.04 due to C++ compilation of rule '//tensorflow/stream_executor/cuda:cusparse_stub' failed (Exit 1)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.8\r\n- Installed using virtualenv? pip? conda?: conda (conda version 4.8.2)\r\n- Bazel version (if compiling from source): 3.1.0 (Bazelisk v1.7.5)\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: CUDA Version 10.1.105, cuDNN version 7.6.5\r\n- GPU model and memory: 2 x NVIDIA Quadro RTX 8000 (48 GB per card, 96 GB total)\r\n\r\n\r\n\r\n**Describe the problem**\r\nBazel build step fails when building from source\r\n**ERROR: /home/user/Downloads/tensorflow-2.4.1/tensorflow/tools/pip_package/BUILD:69:1 C++ compilation of rule '//tensorflow/stream_executor/cuda:cusparse_stub' failed (Exit 1)**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Installing Bazelisk\r\nsudo curl -Lo /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.7.5/bazelisk-linux-amd64\r\n2. Downloaded TF v2.4.1 source\r\ncurl -LO https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n3. tar -xvzf v2.4.1.tar.gz && cd tensorflow-2.4.1\r\n4. Install bazel\r\nbazel version (This command automatically installed bazel version 3.1.0)\r\n5. ./configure.py\r\nOutput of configure.py attached in file configure_output.txt\r\n6. bazel build //tensorflow/tools/pip_package:build_pip_package\r\nOutput of build attached in file bazel_build_output.txt\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[bazel_build_output.txt](https://github.com/tensorflow/tensorflow/files/6089929/bazel_build_output.txt)\r\n[configure_output.txt](https://github.com/tensorflow/tensorflow/files/6089930/configure_output.txt)\r\n", "comments": ["@div3125,\r\nTensorFlow **v2.4** was built and tested against **CUDA 11.0** and **cuDNN 8**. Every TensorFlow release is compatible with a certain CUDA/cuDNN version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu). \r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow-2.2.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\n\r\n\r\nCould you please try building **TensorFlow v2.4** with **CUDA 11.0** and **cuDNN 8** and check if you are facing the same error. Thanks!", "On my system, I have other versions (<=2.3) of Tensorflow running which are built against CUDA 10, which is currently installed. I can install CUDA 11.2 side by side but my NVIDIA driver version is **418.39** which is not compatible with cuDNN 8. Is it not possible to build v2.4 against CUDA 10?", "> Is it not possible to build v2.4 against CUDA 10?\r\n\r\n@div3125,\r\nYou will most likely face errors while building the package since TF v2.4 was intended to be built with CUDA 11.0.\r\n\r\nIn this case, is it possible for you to update the NVIDIA driver on your machine?\r\n\r\nAlso, CUDA 11.2 support is already being tracked in issue [#47568](https://github.com/tensorflow/tensorflow/issues/47568).\r\n\r\nThanks!", "My bad, I typed CUDA 11.2 by mistake, I actually meant 11.0. I can't update the NVIDIA driver because it will break compatibility with other TensorFlow installations on the system. If there is no other solution then I will close the issue.", "@div3125,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/45205#issuecomment-747440700) from issue #45205 with a similar error log and check if it helps. Thanks!", "I deleted Bazel cache directory, reinstalled Bazel and created new source directory for TensorFlow v2.4.1 and ran the compilation process again but encountered this same error again. So, Bazel caching doesn't seem like the root cause. \r\n\r\nAlso, in issue [#45205](https://github.com/tensorflow/tensorflow/issues/45205), the author is trying to compile TensorFlow version 2.3.0 against CUDA version 10.1 which is a tested build configuration. I am trying to build v2.4.1 against CUDA 10.1 which is not an officially tested configuration according [the link](https://www.tensorflow.org/install/source#gpu)  you mentioned in a comment above.\r\n\r\nAt the moment, I am unable to upgrade NVIDIA driver on my system from its current version r418.39 because it will break compatibility with Tensorflow installations < v2.4 in different Anaconda environments on my system which use CUDA v10.0 and v10.1 which in turn need this version of NVIDIA driver.\r\n\r\nI am currently trying Docker build process mentioned in [this comment](https://github.com/tensorflow/tensorflow/issues/45205#issuecomment-735217803) of the same issue  [#45205](https://github.com/tensorflow/tensorflow/issues/45205) and I will share the results in a few days.", "@amahendrakar I found a solution, Anaconda has added tensorflow-gpu v2.4.1 package. I was able to install and run it with the following command in my Anaconda environment (Python 3.8.8)\r\n\r\n`conda install -c anaconda tensorflow-gpu==2.4.1`\r\n\r\nAnaconda has built TensorFlow v2.4.1 against CUDA 10.1 and cuDNN 7.6.5 so I didn't need to update my NVIDIA Driver.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47587\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47587\">No</a>\n"]}, {"number": 47586, "title": "[Keras] Model does not learn at all - loss and val_acc remains constant - what other problems can be there?", "body": "I am doing text classification and have made a dummy dataset to aid in debugging. The problem is that the model does not learn/converge. I am trying to predict one out of 20 labels and have tried most fixes on the net to no avail. \r\n```py\r\ntrain_text = [['a'],['a'],['a']]\r\nval_text = [['a'],['a'],['a']]\r\ntrain_label = np.asarray([5,5,5])\r\nval_label = np.asarray([5,5,5])\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n#Implementing a Transformer block as a layer\r\n\r\nclass TransformerBlock(layers.Layer):\r\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n        super(TransformerBlock, self).__init__()\r\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n        self.ffn = keras.Sequential(\r\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\r\n        )\r\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n        self.dropout1 = layers.Dropout(rate)\r\n        self.dropout2 = layers.Dropout(rate)\r\n\r\n    def call(self, inputs, training):\r\n        attn_output = self.att(inputs, inputs)\r\n        attn_output = self.dropout1(attn_output, training=training)\r\n        out1 = self.layernorm1(inputs + attn_output)\r\n        ffn_output = self.ffn(out1)\r\n        ffn_output = self.dropout2(ffn_output, training=training)\r\n        return self.layernorm2(out1 + ffn_output)\r\n\r\n\r\nclass TokenAndPositionEmbedding(layers.Layer):\r\n    def __init__(self, maxlen, vocab_size, embed_dim):\r\n        super(TokenAndPositionEmbedding, self).__init__()\r\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\r\n\r\n    def call(self, x):\r\n        maxlen = tf.shape(x)[-1]\r\n        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n        positions = self.pos_emb(positions)\r\n        x = self.token_emb(x)\r\n        return x + positions\r\n\r\nvocab_size = 40000  # Only consider the top 20k words :)\r\nmaxlen = 10  # for my task, ideally 500\r\n\r\ntrain_label = tf.keras.utils.to_categorical(train_label, 20)\r\nval_label = tf.keras.utils.to_categorical(val_label, 20)\r\n\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000, lower=True, oov_token='<OOV>')\r\ntokenizer.fit_on_texts(train_text)\r\ntrain_sequences = tokenizer.texts_to_sequences(train_text)\r\n\r\nval_sequences = tokenizer.texts_to_sequences(val_text)\r\n\r\ntrain_text = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen, dtype='int32',\r\n                                           padding='post', truncating='post')\r\n\r\nval_text = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=maxlen, dtype='int32',\r\n                                           padding='post', truncating='post')\r\n\r\nfrom sklearn.preprocessing import normalize\r\n\r\ntrain_text_n = normalize(train_text)\r\nval_text_n = normalize(val_text)\r\n\r\ndata_set = tf.data.Dataset.from_tensor_slices((train_text_n , train_label))\r\ndata_set\r\n\r\nfrom tensorflow.keras.regularizers import l2\r\n \r\nembed_dim = 128  # Embedding size for each token\r\nnum_heads = 8  # Number of attention heads\r\nff_dim = 256  # Hidden layer size in feed forward network inside transformer\r\nbatch_size = 32\r\nfactor = 0.0001\r\n \r\ninputs = layers.Input(shape=(maxlen,))\r\n\r\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\r\nx = embedding_layer(inputs)\r\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\r\n\r\nx = transformer_block(x)\r\n\r\nx = layers.GlobalAveragePooling1D()(x)\r\n\r\nx = layers.Dense(160, activation=\"relu\", kernel_initializer='glorot_uniform')(x)\r\nx = layers.Dropout(0.15)(x)\r\nx = layers.BatchNormalization()(x)\r\n\r\nx = layers.Dense(90, activation=\"relu\", kernel_initializer='glorot_uniform')(x)\r\nx = layers.Dropout(0.1)(x)\r\nx = layers.BatchNormalization()(x)\r\n\r\n \r\nx = layers.Dense(40, activation=\"relu\", kernel_initializer='glorot_uniform')(x)\r\nx = layers.Dropout(0.1)(x)\r\n\r\noutputs = layers.Dense(20)(x)\r\n \r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n \r\nadamopt = tf.keras.optimizers.Adam(learning_rate=1e-4)\r\n \r\n \r\nmodel.compile(optimizer='adamopt', loss=\"categorical_crossentropy\", metrics=[\"acc\"])\r\n \r\nmodel.summary()\r\n \r\nhistory = model.fit(\r\n    train_text_n, train_label, batch_size=batch_size, epochs=20, validation_data=(val_text_n, val_label), verbose=1)\r\n```\r\nAny Idea on how to do I fix this network? I am totally stumped after trying to fix it for 4 days :(\r\n\r\nI posted this on Stack Overflow also, but seeing the lack of traffic, I doubt I would receive any response https://stackoverflow.com/questions/66471955/network-does-not-learn-what-are-the-other-things-that-can-go-wrong \r\n\r\n> There is a list of all the stuff I have tried on that link ^^^", "comments": ["@neel04 \r\nI tried your code in Colab, here's the [gist](https://colab.research.google.com/gist/AdityaKane2001/f2b018a594becbdc6bf2e411f3881d98/47586.ipynb).  As you mentioned in your SO issue,\r\n\r\n> BUT the accuracy and val_accuracy remain constant.\r\n\r\nIn the above gist, the metrics keep changing, but as the dataset is quite small, they don't make any sense.  Is this the problem you were facing?\r\nAlso, if possible, could you please upload the logs and configuration details?\r\n\r\nEDIT: I have little experience using TensorFlow for NLP, but I don't think it's an issue here.", "The problem was that even after removing the dropout and regularizations - the network does not overfit\r\nAnd I can't figure out why. \r\n\r\nthis reproduces the issue, so I don't think there is any need for further seed setting or configs\r\n\r\nThe dataset is small to ensure that the model overfits\r\n", "Ok, I found the issue\r\nSo the `train_text` has only one character each sample, and thus the vectorized form is something as follows:\r\n```\r\nTrain text: \r\n [[2 0 0 0 0 0 0 0 0 0]\r\n [3 0 0 0 0 0 0 0 0 0]\r\n [4 0 0 0 0 0 0 0 0 0]\r\n [3 0 0 0 0 0 0 0 0 0]\r\n [2 0 0 0 0 0 0 0 0 0]\r\n [4 0 0 0 0 0 0 0 0 0]\r\n [2 0 0 0 0 0 0 0 0 0]\r\n [3 0 0 0 0 0 0 0 0 0]\r\n [4 0 0 0 0 0 0 0 0 0]]\r\n```\r\n\r\nThus the normalized form becomes:\r\n```\r\n [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r\n```\r\nHence, the inputs are same for all samples, and thus the model is not training. \r\nPlease see the updated gist.", "Good one, but it still does not overfit even if we don't normalize.\r\n```py\r\nTrain text: \r\n [[2 0 0 0 0 0 0 0 0 0]\r\n [3 0 0 0 0 0 0 0 0 0]\r\n [4 0 0 0 0 0 0 0 0 0]\r\n [3 0 0 0 0 0 0 0 0 0]\r\n [2 0 0 0 0 0 0 0 0 0]\r\n [4 0 0 0 0 0 0 0 0 0]\r\n [2 0 0 0 0 0 0 0 0 0]\r\n [3 0 0 0 0 0 0 0 0 0]\r\n [4 0 0 0 0 0 0 0 0 0]]\r\n\r\nEpoch 1/50\r\n1/1 [==============================] - 2s 2s/step - loss: 2.9162 - acc: 0.0000e+00 - val_loss: 2.7745 - val_acc: 0.4000\r\nEpoch 2/50\r\n1/1 [==============================] - 0s 182ms/step - loss: 2.7857 - acc: 0.3333 - val_loss: 2.6827 - val_acc: 0.4000\r\nEpoch 3/50\r\n1/1 [==============================] - 0s 175ms/step - loss: 2.6982 - acc: 0.3333 - val_loss: 2.6225 - val_acc: 0.4000\r\nEpoch 4/50\r\n1/1 [==============================] - 0s 164ms/step - loss: 2.6455 - acc: 0.3333 - val_loss: 2.5740 - val_acc: 0.4000\r\nEpoch 5/50\r\n1/1 [==============================] - 0s 171ms/step - loss: 2.6054 - acc: 0.3333 - val_loss: 2.5275 - val_acc: 0.4000\r\nEpoch 6/50\r\n1/1 [==============================] - 0s 161ms/step - loss: 2.5602 - acc: 0.3333 - val_loss: 2.4861 - val_acc: 0.4000\r\nEpoch 7/50\r\n1/1 [==============================] - 0s 157ms/step - loss: 2.5179 - acc: 0.3333 - val_loss: 2.4451 - val_acc: 0.4000\r\nEpoch 8/50\r\n1/1 [==============================] - 0s 175ms/step - loss: 2.4768 - acc: 0.3333 - val_loss: 2.4071 - val_acc: 0.4000\r\nEpoch 9/50\r\n1/1 [==============================] - 0s 168ms/step - loss: 2.4402 - acc: 0.3333 - val_loss: 2.3674 - val_acc: 0.4000\r\nEpoch 10/50\r\n1/1 [==============================] - 0s 168ms/step - loss: 2.4031 - acc: 0.3333 - val_loss: 2.3306 - val_acc: 0.4000\r\nEpoch 11/50\r\n1/1 [==============================] - 0s 170ms/step - loss: 2.3681 - acc: 0.3333 - val_loss: 2.2973 - val_acc: 0.4000\r\nEpoch 12/50\r\n1/1 [==============================] - 0s 167ms/step - loss: 2.3383 - acc: 0.3333 - val_loss: 2.2596 - val_acc: 0.4000\r\nEpoch 13/50\r\n1/1 [==============================] - 0s 160ms/step - loss: 2.3057 - acc: 0.3333 - val_loss: 2.2246 - val_acc: 0.4000\r\nEpoch 14/50\r\n1/1 [==============================] - 0s 165ms/step - loss: 2.2751 - acc: 0.3333 - val_loss: 2.1900 - val_acc: 0.4000\r\nEpoch 15/50\r\n1/1 [==============================] - 0s 166ms/step - loss: 2.2480 - acc: 0.3333 - val_loss: 2.1565 - val_acc: 0.4000\r\nEpoch 16/50\r\n1/1 [==============================] - 0s 166ms/step - loss: 2.2219 - acc: 0.3333 - val_loss: 2.1236 - val_acc: 0.4000\r\nEpoch 17/50\r\n1/1 [==============================] - 0s 161ms/step - loss: 2.1939 - acc: 0.3333 - val_loss: 2.0941 - val_acc: 0.4000\r\nEpoch 18/50\r\n1/1 [==============================] - 0s 154ms/step - loss: 2.1696 - acc: 0.3333 - val_loss: 2.0660 - val_acc: 0.4000\r\nEpoch 19/50\r\n1/1 [==============================] - 0s 163ms/step - loss: 2.1468 - acc: 0.3333 - val_loss: 2.0401 - val_acc: 0.4000\r\nEpoch 20/50\r\n1/1 [==============================] - 0s 167ms/step - loss: 2.1248 - acc: 0.3333 - val_loss: 2.0156 - val_acc: 0.4000\r\nEpoch 21/50\r\n1/1 [==============================] - 0s 174ms/step - loss: 2.1022 - acc: 0.3333 - val_loss: 1.9897 - val_acc: 0.4000\r\nEpoch 22/50\r\n1/1 [==============================] - 0s 162ms/step - loss: 2.0783 - acc: 0.3333 - val_loss: 1.9629 - val_acc: 0.4000\r\nEpoch 23/50\r\n1/1 [==============================] - 0s 175ms/step - loss: 2.0550 - acc: 0.3333 - val_loss: 1.9407 - val_acc: 0.4000\r\nEpoch 24/50\r\n1/1 [==============================] - 0s 185ms/step - loss: 2.0357 - acc: 0.3333 - val_loss: 1.9219 - val_acc: 0.4000\r\nEpoch 25/50\r\n1/1 [==============================] - 0s 176ms/step - loss: 2.0175 - acc: 0.3333 - val_loss: 1.9039 - val_acc: 0.4000\r\nEpoch 26/50\r\n1/1 [==============================] - 0s 177ms/step - loss: 1.9978 - acc: 0.3333 - val_loss: 1.8855 - val_acc: 0.4000\r\nEpoch 27/50\r\n1/1 [==============================] - 0s 164ms/step - loss: 1.9777 - acc: 0.3333 - val_loss: 1.8642 - val_acc: 0.4000\r\nEpoch 28/50\r\n1/1 [==============================] - 0s 162ms/step - loss: 1.9569 - acc: 0.3333 - val_loss: 1.8429 - val_acc: 0.4000\r\nEpoch 29/50\r\n1/1 [==============================] - 0s 163ms/step - loss: 1.9374 - acc: 0.3333 - val_loss: 1.8196 - val_acc: 0.4000\r\nEpoch 30/50\r\n1/1 [==============================] - 0s 167ms/step - loss: 1.9164 - acc: 0.3333 - val_loss: 1.8020 - val_acc: 0.4000\r\nEpoch 31/50\r\n1/1 [==============================] - 0s 166ms/step - loss: 1.9002 - acc: 0.3333 - val_loss: 1.7826 - val_acc: 0.4000\r\nEpoch 32/50\r\n1/1 [==============================] - 0s 182ms/step - loss: 1.8810 - acc: 0.3333 - val_loss: 1.7618 - val_acc: 0.4000\r\nEpoch 33/50\r\n1/1 [==============================] - 0s 170ms/step - loss: 1.8608 - acc: 0.3333 - val_loss: 1.7380 - val_acc: 0.4000\r\nEpoch 34/50\r\n1/1 [==============================] - 0s 171ms/step - loss: 1.8397 - acc: 0.3333 - val_loss: 1.7171 - val_acc: 0.4000\r\nEpoch 35/50\r\n1/1 [==============================] - 0s 165ms/step - loss: 1.8186 - acc: 0.3333 - val_loss: 1.6967 - val_acc: 0.4000\r\nEpoch 36/50\r\n1/1 [==============================] - 0s 173ms/step - loss: 1.7979 - acc: 0.3333 - val_loss: 1.6786 - val_acc: 0.4000\r\nEpoch 37/50\r\n1/1 [==============================] - 0s 168ms/step - loss: 1.7781 - acc: 0.3333 - val_loss: 1.6615 - val_acc: 0.4000\r\nEpoch 38/50\r\n1/1 [==============================] - 0s 176ms/step - loss: 1.7596 - acc: 0.3333 - val_loss: 1.6408 - val_acc: 0.4000\r\nEpoch 39/50\r\n1/1 [==============================] - 0s 188ms/step - loss: 1.7395 - acc: 0.3333 - val_loss: 1.6238 - val_acc: 0.4000\r\nEpoch 40/50\r\n1/1 [==============================] - 0s 182ms/step - loss: 1.7220 - acc: 0.3333 - val_loss: 1.6017 - val_acc: 0.4000\r\nEpoch 41/50\r\n1/1 [==============================] - 0s 181ms/step - loss: 1.7007 - acc: 0.3333 - val_loss: 1.5844 - val_acc: 0.4000\r\nEpoch 42/50\r\n1/1 [==============================] - 0s 171ms/step - loss: 1.6814 - acc: 0.3333 - val_loss: 1.5681 - val_acc: 0.4000\r\nEpoch 43/50\r\n1/1 [==============================] - 0s 166ms/step - loss: 1.6637 - acc: 0.3333 - val_loss: 1.5490 - val_acc: 0.4000\r\nEpoch 44/50\r\n1/1 [==============================] - 0s 165ms/step - loss: 1.6426 - acc: 0.3333 - val_loss: 1.5322 - val_acc: 0.4000\r\nEpoch 45/50\r\n1/1 [==============================] - 0s 170ms/step - loss: 1.6219 - acc: 0.3333 - val_loss: 1.5135 - val_acc: 0.4000\r\nEpoch 46/50\r\n1/1 [==============================] - 0s 168ms/step - loss: 1.5993 - acc: 0.3333 - val_loss: 1.4937 - val_acc: 0.4000\r\nEpoch 47/50\r\n1/1 [==============================] - 0s 169ms/step - loss: 1.5788 - acc: 0.3333 - val_loss: 1.4737 - val_acc: 0.4000\r\nEpoch 48/50\r\n1/1 [==============================] - 0s 168ms/step - loss: 1.5575 - acc: 0.3333 - val_loss: 1.4542 - val_acc: 0.4000\r\nEpoch 49/50\r\n1/1 [==============================] - 0s 162ms/step - loss: 1.5382 - acc: 0.3333 - val_loss: 1.4347 - val_acc: 0.4000\r\nEpoch 50/50\r\n1/1 [==============================] - 0s 170ms/step - loss: 1.5165 - acc: 0.3333 - val_loss: 1.4196 - val_acc: 0.4000\r\n\r\n\r\n```\r\n\r\nAlso, shouldn't we use Softmax here, or should I use `Sigmoid`?\r\n```py\r\noutputs = layers.Dense(20,activation='sigmoid')(x)\r\n```", "Yes, I tried that. The reason being, the function we are now trying to fit to is (for one sample):\r\n```\r\noutput = np.zeros((20),dtype=np.float32)\r\noutput = np.put(sample, sample[0]+3 , 1 )\r\n```\r\n\r\nwhich I don't think is the best function to fit using a transformer :)", "I tried the sigmoid, but nothing changed\r\n", "> I tried the sigmoid, but nothing changed       \r\n\r\nyeah, but I heard I have to use `categorical_crossentropy` for classification. Plus for me, I tried that too with my main dataset but it didn't help.", "> yeah, but I heard I have to use categorical_crossentropy for classification. \r\n\r\n\r\nThat's correct, but the the function you are trying to fit is not possible, because it is not a curve that you can plot in some n-dimensional space.\r\n\r\n\r\n", "no, I meant for the actual dataset I am using, should I use sigmoid?\r\n", "> no, I meant for the actual dataset I am using, should I use sigmoid?\r\n\r\nyup\r\n", "Else you are just training a linear regression model \ud83d\ude05 ", "yeah, I didn't see it that way. I didn't pick that up because in the examples I looked, they all used softmax. Well, it is training with sigmoid with a pretty small network (without normalization) so I expect the accuracy to scale up.\r\n\r\nThanx a lot Aditya!! :+1: :100: :rocket: \r\n\r\nJust one last thing - I heard that normalization can help in converging since there would be less interference of weights since the range is [1,40000]. Is there some other way/resource you recommend that I use to normalize my data?", "> yeah, I didn't see it that way. I didn't pick that up because in the examples I looked, they all used softmax. Well, it is training with sigmoid with a pretty small network (without normalization) so I expect the accuracy to scale up.\r\n\r\nOk, sorry about that. You must use softmax in your case, because each of your tokens have some probability associated with them. Sigmoid and softmax are close enough, just that sigmoid is for binary classification whereas softmax is for multilabel classification. \r\nBoth are used to introduce non linearity in gradient losses.", "About normalization: normalization works as it keeps the weights from overflow or underflow. Input data normalization can be done as you have done in the above code.", "Hmm...sigmoid was working pretty well. If my input data normalization is correct, why am I having problems whenever I use the normalized data?", "Ok, I got a bit confused.\r\nSoftmax is used for `multi-label` classification : which means if output consists of more than one label per sample. \r\nSigmoid is used for `multi-class` classification: which means if output has exactly one label out of many possible labels.\r\nMost tutorials don't show the bigger picture: they show only binary  classification.\r\n\r\nThus, use sigmoid for your task. Sorry for the prior confusion. \ud83d\ude05 \r\n\r\nComing to normalization, make sure that you normalize along the correct axis. Secondly, note that dropout works effectively on extremely large networks (approx >1000 nodes per layer). This is just from my experience and not a mathematical fact. Also there's a difference between normalization and regularization: make sure you understand that. ", "Right now, situation is something like this:-\r\n```py\r\nEpoch 1/30\r\n34/34 [==============================] - 13s 321ms/step - loss: 3.6056 - acc: 0.0571 - val_loss: 3.1510 - val_acc: 0.1003\r\nEpoch 2/30\r\n34/34 [==============================] - 11s 311ms/step - loss: 3.3849 - acc: 0.0598 - val_loss: 3.1064 - val_acc: 0.1003\r\nEpoch 3/30\r\n34/34 [==============================] - 11s 311ms/step - loss: 3.2823 - acc: 0.0480 - val_loss: 3.0968 - val_acc: 0.0418\r\nEpoch 4/30\r\n34/34 [==============================] - 11s 311ms/step - loss: 3.2642 - acc: 0.0633 - val_loss: 3.1047 - val_acc: 0.0362\r\nEpoch 5/30\r\n34/34 [==============================] - 11s 311ms/step - loss: 3.4409 - acc: 0.0542 - val_loss: 3.1140 - val_acc: 0.0306\r\n```\r\nit was working well when I initially used sigmoid, but after that, I made a lot of changes to try to get softmax working so now I don't know what exact things I used :(\r\nAny possible insight?\r\n\r\nCheers,\r\nNeel Gupta", "When using `one_hot` encoding, use `categorical_crossentropy`. Use sigmoid in last layer.\r\nAlso, please close this issue as we have now pinpointed the root cause.", "Could you get it to work?", "no, it seems to be randomly guessing. ", "Can you provide a small dataset so I can try on Colab?\r\n", "[val_text.txt](https://github.com/tensorflow/tensorflow/files/6092019/val_text.txt)\r\n[val_label.txt](https://github.com/tensorflow/tensorflow/files/6092020/val_label.txt)\r\n[train_text.txt](https://github.com/tensorflow/tensorflow/files/6092021/train_text.txt)\r\n[train_label.txt](https://github.com/tensorflow/tensorflow/files/6092022/train_label.txt)\r\n\r\nI have attached 5 examples of each - you can try and see if it overfits.", "I don't know what's going on in these files \ud83d\ude05 . Can you share some data source that is easy to load?", "In each file, there is a nested list that contains 5 other lists. they have been already converted to integers to facilitate mode training. you can simply copy the list, put it in a variable and train the model."]}, {"number": 47585, "title": "TensorFlow Lite currently doesn't support control flow ops: Merge, Switch.", "body": "TensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, WHERE. Here is a list of operators for which you will need custom implementations: DecodeBmp, DecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, ParseSingleExample, Substr.\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Sorry for encountering this issue. We do not have plans to support TF v1 control flow ops but we do support v2 control flow ops. Please consider the migration to TF v2 control flow ops or you can enable v2 control flow ops through the following option during graph construction time. https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47584, "title": "CANNOT FIND the source code of \u201cgen_candidate_sampling_ops\u201d", "body": "I\u2019m reading the source code(python version) of sampled softmax loss(tf.nn.sampled_softmax_loss) since I want to write my own \u201csampled_values\u201d, and it leads me to the function \u201clog_uniform_candidate_sampler\u201d in candidate_sampling_ops.py.\r\n\r\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\r\n@tf_export(\r\n    'random.log_uniform_candidate_sampler',\r\n    v1=[\r\n        'random.log_uniform_candidate_sampler',\r\n        'nn.log_uniform_candidate_sampler'\r\n    ])\r\n@dispatch.add_dispatch_support\r\n@deprecation.deprecated_endpoints('nn.log_uniform_candidate_sampler')\r\ndef log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique,\r\n                                  range_max, seed=None, name=None):\r\n\r\n  seed1, seed2 = random_seed.get_seed(seed)\r\n  return gen_candidate_sampling_ops.log_uniform_candidate_sampler(\r\n      true_classes, num_true, num_sampled, unique, range_max, seed=seed1,\r\n      seed2=seed2, name=name)\r\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\r\n\r\nHOWEVER, I cannot find the gen_candidate_sampling_ops.py in source code, which confused me.\r\n\r\n[Link:](url)\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/candidate_sampling_ops.py#L99\r\n", "comments": ["@JulieYao0754 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "These (gen_*_.py) are automatically generated files and can be found in your local TF installation.\r\nFor me on macOS (PyCharm IDE) it is located at:\r\n```python\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_candidate_sampling_ops.py\r\n```", "@ymodak thanks a lot!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 47583, "title": "how can i access the dataset on each worker node when using MultiWorkerMirroredStrategy training?", "body": "when i use MultiWorkerMirroredStrategy to distributed training,\r\ni want to know how the dataset shard with multiple workers in each batch-train,\r\nbut there is no way to debug...\r\n\r\nthanks for your help!", "comments": ["@junneyang,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "> @junneyang,\r\n> In order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n\r\n1. tensorflow: 2.3.0\r\n\r\n2. worker nodes: 2, running as:\r\n```\r\npython train-dist.py 0 \r\npython train-dist.py 1\r\n```\r\n\r\n3. complete code(train-dist.py):\r\n```\r\nimport os\r\nimport sys\r\nimport json\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation\r\n\r\nper_replica_batch_size = 10\r\nnum_workers = 2\r\nglobal_batch_size = per_replica_batch_size * num_workers\r\ndataset_size = 100\r\nsteps_per_epoch = 100 / global_batch_size\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"localhost:20000\", \"localhost:20001\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': int(sys.argv[1])}\r\n})\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ndef get_model():\r\n    model = Sequential()\r\n    model.add(Dense(1, input_shape=(1,)))\r\n    model.compile(loss='mse', optimizer='sgd')\r\n    return model\r\n\r\nif __name__ == \"__main__\":\r\n    X_train = np.random.rand(dataset_size, 1)\r\n    noise = np.random.normal(0, 0.01, X_train.shape)\r\n    Y_train = 10 * X_train + 2 + noise\r\n    dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(global_batch_size)\r\n\r\n    with strategy.scope():\r\n        model = get_model()\r\n    model.fit(dataset, epochs=1, steps_per_epoch=steps_per_epoch, verbose=2)\r\n    print(model.get_weights())\r\n\r\n```\r\n", "@amahendrakar could you please give me some instructions, thank you !", "@junneyang,\r\nThank you for the update. Could you please go through the below links regarding \r\n\r\n- [Dataset sharding](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding) \r\n- [Autosharding](https://www.tensorflow.org/tutorials/distribute/input#sharding)\r\n\r\nand let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47583\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47583\">No</a>\n", "> @junneyang,\r\n> Thank you for the update. Could you please go through the below links regarding\r\n> \r\n> * [Dataset sharding](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding)\r\n> * [Autosharding](https://www.tensorflow.org/tutorials/distribute/input#sharding)\r\n> \r\n> and let us know if it helps. Thanks!\r\n\r\nthanks !"]}, {"number": 47582, "title": "Ghost bug on macOS Big Sur - TypeError: tf__gradient_update() missing n required positional arguments", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 11.2.1\r\n- TensorFlow installed from (source or binary): pip (binary)\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.7\r\n\r\n```\r\n== check python ===================================================\r\npython version: 3.8.7\r\npython branch: \r\npython build version: ('default', 'Dec 30 2020 10:14:55')\r\npython compiler version: Clang 12.0.0 (clang-1200.0.32.28)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple clang version 12.0.0 (clang-1200.0.32.29)\r\nTarget: x86_64-apple-darwin20.3.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                      1.19.5\r\nprotobuf                   3.14.0\r\ntensorflow                 2.4.1\r\ntensorflow-addons          0.12.0\r\ntensorflow-estimator       2.4.0\r\ntensorflow-probability     0.12.1\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.4.1\r\ntf.version.GIT_VERSION = v2.4.0-49-g85c8b2a817f\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./collect_tf.sh: line 145: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.4.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python3.8/site-packages\r\nRequired-by: yolo-tf2\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(2, 7, 16, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe issue sometimes happens and sometimes it doesn't. It usually occurs when I run some perfectly working code then make some minor changes followed by a re-run. I get the error / a variation of the very same error message referring to missing positional arguments while they are there and there is absolutely nothing wrong.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/emadboctor/Desktop/code/drl-algos/acer.py\", line 273, in <module>\r\n    ]\r\n  File \"/Users/emadboctor/Desktop/code/drl-algos/base_agent.py\", line 346, in fit\r\n    self.train_step()\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3887, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n\r\n    TypeError: tf__gradient_update() missing 5 required positional arguments: 'states', 'rewards', 'actions', 'dones', and 'action_probs'\r\n```\r\nThen, the error is usually gone in the following run.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@emadboctorx Can you please share a simple standalone code to reproduce the error? Thanks!", "@jvishnuvardhan The error does not recur. It sometimes happens between re-runs and whenever I encounter it, I just re-run (with zero modifications), then it runs perfectly fine. Therefore I don't think sharing the code is likely to reproduce the error.", "@emadboctorx in order to understand what might be going wrong, its important we can reproduce the error. Even if its a workflow that you do i.e. start with code X, make modification Y, re-run is good enough for us but without a reproduction, its really hard to understand what is going on.", "@rohan100jain well I see your point but the error sometimes occurs and sometimes it doesn't and if it did, it won't repeat itself again unless you changed a random part of the code. I don't know this is me totally speculating but I think it has something to do with compilation under `tf.function`, the error happens when modifications are made to the code and then run and compiled for the first time. If the error was reproduced, it does not recur in the following run otherwise, it would be easy to fix. Therefore I have to make some random modifications that sometimes reproduce the error and sometimes they don't or compile just fine according to my theory. I will try to reproduce on my own and see which parts of the code are involved and I may edit the issue accordingly.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47582\">No</a>\n"]}, {"number": 47581, "title": "[StreamExecutor C API] Lazy getting device count to provide more feasibility", "body": "This PR allows plugins provide the device count at runtime instead of loading stage, which provides more feasibility, for example, plugin can register a virtual device during the loading time(import tensorflow), and can select another backend through some plugin specific API, so the device count is possible to be changed.", "comments": ["@penpornk can you help to have a review or suggest other reviewers? Thanks. ", "Removing the API review tag. Will do this internally instead."]}, {"number": 47580, "title": "[TFL] Optimize ArgMinMax for float when axis=dims-1.", "body": "Fixes #47555.\r\n\r\nBenchmark on argmax of 1-D float32 tensor on Pixel 3a android 11:\r\n\r\n| Size   | Reference (min/max/avg microsecs) | Optimized (min/max/avg microsecs) |\r\n| ------ | --------------------------------- | --------------------------------- |\r\n| 10     | 0/63/0.260737                     | 0/49/0.216432                     |\r\n| 100    | 1/101/1.17919                     | 0/77/0.271518                     |\r\n| 1000   | 9/98/9.80665                      | 0/65/1.07653                      |\r\n| 10000  | 95/190/96.2557                    | 8/110/9.63559                     |\r\n| 100000 | 952/1102/967.445                  | 88/215/94.8766                    |\r\n\r\n/cc @abattery for visibility.", "comments": ["@renjie-liu could you review this performance improvement proposal?", "@WindQAQ do you think the current set of test cases for arg min max covers the cases for float when axis=dim - 1? If not, please add some test cases.", "> @WindQAQ do you think the current set of test cases for arg min max covers the cases for float when axis=dim - 1? If not, please add some test cases.\r\n\r\nThere is one test with size=4, but I can add more. BTW, do you think it's better to put `ArgMaxVector` to `tensor_utils`? Though there is no other uses other than in `ArgMinMax` I think.\r\n\r\nI also leave the reference implementation unchanged for backward compatibility. Looks like TFLite micro depends on it.", "> do you think it's better to put ArgMaxVector to tensor_utils?\r\n\r\nI think we don't need to move them to the tensor_utils as you said.", "Generally, LGTM. To make sure the validity of the implementation, could you add some test cases under lite/op_tests for cases when axis = dims - 1?", "Resolve the conflict problem in the internal/optimized/legacy_optimized_ops.h. There are two ArgMinMax declarations from the headers for reference ops and optimized ops.\r\n\r\n", "Legacy ops cannot use optimized version because they accept a cmp functor as argument. I add a function that takes bool as argument, which is the same as the one in reference_ops. Let me know if it's correct. Also port ArgMax for backward compatibility. Reference ops have ArgMax function that is separated from ArgMinMax.", "tensorflow/lite/testing:zip_test_arg_min_max is not buildable.", "> tensorflow/lite/testing:zip_test_arg_min_max is not buildable.\r\n\r\nDo you have error msg? Thanks!", "```\r\ntensorflow/lite/testing/generate_examples_report.py, in make_report_table\r\n    fp.write(\"  <td>%s</td>\\n\" % html.escape(repr(params[p]), quote=True))\r\nKeyError: 'axis'\r\n```", "Let's see if it works \ud83d\ude03 ", "Looks like the PR gets rolled back. Let me know if there's anything that I can help."]}, {"number": 47578, "title": "Gradient Accumulation with Custom fit in tf.keras?", "body": "## Documents: \r\n[Customizing fit : TF.Keras](https://keras.io/guides/customizing_what_happens_in_fit/)\r\nThis a great article and one of the best deep learning practices. However, with this reading intuition, I was trying to implement Gradiant Accumulation (**GA**) by customizing the `.fit()` method, overriding the `train_step` within `tf.keras.Model` class. But I couldn't implement it yet. \r\n\r\nI don't want it in a custom training loop (like [this](https://stackoverflow.com/questions/59893850/how-to-accumulate-gradients-in-tensorflow-2-0)) but with customized `.fit()`. I also asked in [SO](https://stackoverflow.com/q/66472201/9215780), no response yet. I've found some workaround [here](https://github.com/keras-team/keras/issues/14483), but it's too messy. There should be a convenient way to achieve this. How to achieve this? Please find the gist [here](https://colab.research.google.com/drive/1hr9kIhTlkCxlW_md3CXWREor-6TE6RFq?usp=sharing). \r\n\r\nAlso, I know the pros of using **GA** but what are the major **cons** of using it? Why it's not come as a default but an optional feature with the framework?", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/88e42704e1f5f9f9ed14766013e25e0e/47578.ipynb). Thanks!", "Any effective workaround? ", "Solved. [Discussion](https://stackoverflow.com/questions/66472201/how-to-do-gradient-accumulation-with-custom-fit-in-tf-keras). Closing this issue. "]}, {"number": 47577, "title": "[INTEL MKL] Execute small gemm's single threaded.", "body": "This PR introduces a simple cost metric to determine if dnnl_sgemm needs to be run with 1 thread or the whole threadpool.\r\n\r\nPlease note: Please dont merge this PR until https://github.com/tensorflow/tensorflow/pull/47543 is merged", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47577) for more info**.\n\n<!-- need_author_cla -->", "@penpornk I tried to fix the merge conflict online and it has cancelled the cla check. Can you please help resolve this?", "@googlebot I fixed it", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47577) for more info**.\n\n<!-- need_author_cla -->", "Manually changing CLA to yes because the two commits in this PR are from the same github account which has CLA. ", "@penpornk , Yes 8 is just a magic number that worked with internal workloads. I had made the changes internally and was testing them on specific models. Thanks!"]}, {"number": 47576, "title": "Make circular buffer op configurable via flatbuffers.", "body": "Make circular buffer op configurable via flexbuffer arguments.\r\n\r\nFix circular_buffer test on bluepill.\r\n\r\nBase change: http://cl/360707883\r\n\r\nhttp://b/143286954\r\nhttp://b/150001379", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47575, "title": "Exception related symbols appear to be showing up in the final binary with the xtensa toolchain", "body": "The tests described in this issue are with the RI-2020.4-linux toolchain and the Fusion F1 core.\r\n\r\nBuild the keyword_benchmark with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\n```\r\n\r\nList all the symbols:\r\n```\r\nXTENSA_CORE=F1_190305_swupgrade xt-nm --print-size --size-sort --radix=d -C tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nWe see symbols that appear to be related to exception handling:\r\n```\r\n00082100 00000304 T _Unwind_ForcedUnwind\r\n00070712 00000305 T __cxxabiv1::__vmi_class_type_info::__do_upcast(__cxxabiv1::__class_type_info const*, void const*, __cxxabiv1::__class_type_info::__upcast_result&) const\r\n00076024 00000307 T __divdf3\r\n00082796 00000310 T _Unwind_Resume_or_Rethrow\r\n00104616 00000326 t _xa_nn_dot_product_4_rows_1_vec_mat_aligned_vec_aligned\r\n00080392 00000340 t uw_update_context_1\r\n00081736 00000364 T _Unwind_RaiseException\r\n00082404 00000392 T _Unwind_Resume\r\n00081140 00000408 t uw_advance_context\r\n00080732 00000408 t uw_update_context\r\n00072160 00000413 t get_ttype_entry(lsda_header_info*, unsigned int)\r\n00089100 00000415 T _FDscalex\r\n00085964 00000444 t add_fdes\r\n00086600 00000456 t linear_search_fdes\r\n00085504 00000457 t classify_object_over_fdes\r\n00084100 00000526 t fde_single_encoding_compare\r\n00070064 00000584 T _FExp\r\n00079584 00000806 t uw_frame_state_for\r\n00078136 00001446 t execute_cfa_program\r\n00116320 00002048 b emergency_buffer_72\r\n```\r\n\r\nSome of the full command line options (to confirm that we are passing in `-fno-exceptions` when building the .cc files):\r\n```\r\nxt-clang++ -std=c++11 -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DNDEBUG -DTF_LITE_STRIP_ERROR_STRINGS -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=F1_190305_swupgrade -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DFUSION_F1 -Wno-unused-private-field -DNNLIB_V2 -Wno-shadow -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/kernels/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/nnlib/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/common/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/include/ -c tensorflow/lite/micro/kernels/xtensa/fully_connected.cc -o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/kernels/xtensa/fully_connected.o\r\n\r\nxt-clang -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DNDEBUG -DTF_LITE_STRIP_ERROR_STRINGS -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=F1_190305_swupgrade -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DFUSION_F1 -Wno-unused-private-field -DNNLIB_V2 -Wno-shadow -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/kernels/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/nnlib/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/common/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/include/ -c tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/src/scl_tanhf_hifi4.c -o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/src/scl_tanhf_hifi4.o\r\n\r\nxt-clang++ -std=c++11 -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DNDEBUG -DTF_LITE_STRIP_ERROR_STRINGS -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=F1_190305_swupgrade -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DFUSION_F1 -Wno-unused-private-field -DNNLIB_V2 -Wno-shadow -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/kernels/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/nnlib/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/common/include/ -Itensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/algo/ndsp/hifi4/include/ -o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/benchmarks/keyword_benchmark.o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/obj/tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm\r\n```\r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47575\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47575\">No</a>\n"]}, {"number": 47573, "title": "Handle null pointer parameters in C functions", "body": "**System information**\r\n- TensorFlow 2.4.0.\r\n- Are you willing to contribute it? Yes.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, when a null pointer (that should not be null) is passed to a C function, segmentation fails, causing the program to be interrupted. It would be good to handle this case to report the error in the `TF_Status` object.\r\n\r\n**Will this change the current api? How?**\r\nThe `TF_Status` object would contain information related to the segmentation fault error.\r\n\r\n**Who will benefit with this feature?**\r\nTensorFlow C API users.", "comments": []}, {"number": 47572, "title": "Use nn.softmax in keras.activations.softmax", "body": "There are two issues with [tf.keras.activations.softmax](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py#L47-L84 ) :\r\n\r\n**First**: \r\nit implements softmax for 3D+ tensors in not optimal way due to old workarounds:\r\n- **Keras**:   Support for 3D+ tensors was added on Nov 29, 2015 [https://github.com/keras-team/keras/commit/8f2d6d2714aa1b60950a2fc355d39297b7f2cdfb](https://github.com/keras-team/keras/commit/8f2d6d2714aa1b60950a2fc355d39297b7f2cdfb)\r\n- **TF**: In that time tensorflow did not support multiple dimensions: issue was reported on Jun 29, 2016 (Softmax for multiple dimensions) [https://github.com/tensorflow/tensorflow/issues/3101](https://github.com/tensorflow/tensorflow/issues/3101)\r\n- **TF**: Support for multiple dimensions was added on Aug 29, 2016 [https://github.com/tensorflow/tensorflow/commit/aeac274ed160618afa242512c38d1fd496466136](https://github.com/tensorflow/tensorflow/commit/aeac274ed160618afa242512c38d1fd496466136)\r\n- **TF**: Improved kernel accepting shapes with higher ranks was added on Aug 6, 2018 [https://github.com/tensorflow/tensorflow/commit/f3aa5d4717deac34b00979fdc4f13c3c9170e819](https://github.com/tensorflow/tensorflow/commit/f3aa5d4717deac34b00979fdc4f13c3c9170e819)\r\n\r\nFor 3D tensor, the softmax gradient is getting really complicated which seriously hurts performance. example:\r\n![image](https://user-images.githubusercontent.com/37601244/110015738-834ba700-7d24-11eb-9d21-41a6869c3c9c.png)\r\n\r\n**Second**:\r\nIt returns wrong results for 2D tensors, axis=0. I also prepared unit test for this scenario.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47572) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 47571, "title": "LSTM conversion into Tflite. Error: This is not a valid Tensorflow lite model file -- Android Studio", "body": "### 1. System information\r\n\r\nOn Google colabs\r\n\r\n### 2. Code\r\n\r\n`import tensorflow as tf \r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nwith open('model_tensor.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n`\r\n\r\n### 3. Model Creation\r\nmodel = keras.Sequential()\r\nmodel.add(\r\n    keras.layers.Bidirectional(\r\n      keras.layers.LSTM(\r\n          units=128, \r\n          input_shape=[X_train.shape[1], X_train.shape[2]]\r\n      )\r\n    )\r\n)\r\nmodel.add(keras.layers.Dropout(rate=0.5))\r\nmodel.add(keras.layers.Dense(units=128, activation='tanh'))\r\nmodel.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n\r\n### 3. Failure after conversion\r\n\r\nThis is not a valid Tensorflow lite model file -- Android Studio\r\n\r\n[Question on StackOverflow](https://stackoverflow.com/questions/66473725/unable-to-import-tensor-flow-lite-model-in-android-after-converting-from-keras-l)", "comments": ["@lu-wang-g do you know when the android studio reject the given TFLite?", "When I try to add a Tensor flow Lite Model.\r\nIn Android Studio:\r\nNew -> Other -> Tensor flow Lite Model\r\n\r\nhttps://i.stack.imgur.com/whxCq.png", "@AraibKarim could you share your TFLite file? And which TF version did you use?", "And which android studio version are you using?", "Android Studio 4.1.2\r\nTF 2.4.1 in Google Colab\r\nSee the zip file. Unzip to find the model.\r\n\r\n[My_TFlite_Model.zip](https://github.com/tensorflow/tensorflow/files/6086781/My_TFlite_Model.zip)\r\n", "Thanks for sharing! The converted model looks fine. I will ask @lu-wang-g about this behavior.", "This is a known issue for AS 4.1. LSTM models may have multiple subgraphs, where the Android Studio ML binding expects only one. This issue has been fixed in AS 4.2.", "So should I update my Android Studio to 4.2 and it will work?", "According to @lu-wang-g 's words, it should be fixed in AS 4.2. Could you try again with Android Studio 4.2?", "I tried with Android Studio 4.2 Beta and it was able to load the model.\r\n\r\nThank you for the support! closing the issue now", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47571\">No</a>\n"]}, {"number": 47570, "title": "tensorflow.keras and tensorflow.python.keras", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux centos\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.4.1\r\n- Python version: 3.6.2\r\n- Installed using virtualenv? pip? conda?: virtualenv conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):7.2.0\r\n- CUDA/cuDNN version:cudnn-6.0.21-cuda8.0_0\r\n- GPU model and memory:Tesla K20Xm 4G\r\n\r\n\r\n\r\n**Describe the problem**\r\nI was wondering why keras is under /path/tensorflow/python instead of /path/tensorflow/ anymore?\r\nIs it because of new version?\r\n\r\nI asked this question is because one package I am trying to use call tensorflow.keras when I import it, while my keras is actually under tensorflow.python.keras, and I want to find the reason before I reinstall everything.\r\n\r\nThanks.  \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I remember like last year, my keras was under tensorflow/keras, but I reinstalled my environment, after that my keras was under tensorflow/python/keras. I don't remember their versions. Is it caused by tensorflow version? I will appreciate if you have any idea of importing tensorflow.keras directly. Thanks!", "@XXZhou25 \r\nWe see that you are using an old version of tensorflow, we do not have support for 1.x, could you please upgrade to 2.x and let us know if you face any issues.\r\nYes you would see changes due to the new version and because there is no support for 1.x.", "Hi @Saduf2019 ,\r\nI see. I just updated tf to 2.x and it works perfect. Thank you so much! ", "@XXZhou25 \r\nCan you please move this to closed status as the issue is resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47570\">No</a>\n"]}, {"number": 47568, "title": "Update TF with CUDA 11.2 and cuDNN 8.1", "body": "Update TF with CUDA 11.2 and cuDNN 8.1\r\n", "comments": ["May I know if we have support for CUDA 11.2 and cuDnn 8.* - please see this [issue  ](https://github.com/tensorflow/tensorflow/issues/47669)", "Support for Ubuntu has landed in the nightlies (for CUDA 11.2 and cuDNN 8.1).", "Would do you think it will be possible for Windows 10 as well?", "Windows support has landed too.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47568\">No</a>\n", "I tried installing on UBUNTU 20.04 : Cuda 11.2 and the latest Cudnn 8.1.1, python 3.7, Anaconda.\r\nI installed tf-nightly-gpu 2.5.0.dev20210320, and tf-nightly 2.5.0.dev20210320\r\n\r\nand got the following crash on import tensorflow as tf: Illegal instruction (core dumped)\r\n\r\nMy cpu has AVX but not AVX2 :  There was an issue back end of last year which was fixed by Jan regarding tf nightly binaries accidently starting to use AVX2 by mistake. It seemed this issue has now come back??\r\n\r\nI will try Windows 10 later on. \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/45866\r\n"]}, {"number": 47567, "title": "Update resize_bilinear_op.cc", "body": "Fix #47464 . It removes the redundant code in `resize_bilinear_op.cc`", "comments": []}, {"number": 47566, "title": "Addition of TanhExp activation function - issue #45929", "body": "Addition of TanhExp activation function based on issue #45929 that leads changes to tensorflow/python/keras/activations.py and tensorflow/python/ops/nn_impl.py", "comments": ["We are not sure what exactly oneDNN unit tests failed checks are talking about. I could see failure in compiling programs and some functionality tests. are these failed checks meant for our code changes?", "The API owners are moving to a rotation based system instead of the meetings. Looks like @fchollet is already a reviewer and adding @manivaradarajan. Please review on behalf of the API Owners.", "@sangireddysiva  Can you please address Ubuntu Sanity errors? Thanks!", "> @sangireddysiva Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\n@gbaned , Adjusted the documentation to address the Ubuntu Sanity errors", "@gbaned  , Added Pylint disable line too long statement to address the Ubuntu Sanity errors", "@gbaned , fixed Ubuntu CPU Doctest errors, thanks", "@mihaimaruseac addresses the requested changes to the documentation, thanks", "@gbaned , There are still some failed checks, do we need to look into them before merging? Thanks,", "@mihaimaruseac Can you please assist on above comments from @sangireddysiva. Thanks!", "approved by @tensorflow/api-owners internally , trying again to merge this internally \r\n", "@sangireddysiva Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "@sangireddysiva To minimize the redundancy between the Keras API and the tensorflow ops,\r\nthe best solution should be remove the Keras layer but keep the tf ops.", "@gbaned this PR adds a new activation function in `tf.nn`. Let's ask a TF core team member to review it.", "> @gbaned this PR adds a new activation function in `tf.nn`. Let's ask a TF core team member to review it.\r\n\r\n@fchollet Thank you for the update. ", "@sangireddysiva  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@sangireddysiva  Any update on this PR? Please. Thanks!", "Removing myself as reviewer since the Keras code has been moved to github.com/keras-team/keras. Please split the keras change into a new PR and sent it to keras-team, if you are still interested in contributing this change. Thanks.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 47565, "title": "TensorBoard Embedding Projector Metadata filepath format not clear", "body": "\r\n## URL(s) with the issue:\r\n\r\n- https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\r\n- https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\r\n  - description of argument *embeddings_metadata*; See details link is gone: https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAt the moment i try to use the embedding projector when using multiple runs (with a subfolder named by date for every training run)\r\n\r\nI am not able to get the embedding projection including the metadata.tsv file for description labels (instead of just showing the index).\r\n\r\nEverything does work as expected when i follow this tutorial for only one run: https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\r\n\r\nThe embedding projector does show my custom labels when hovering over points.\r\n\r\nBut the embedding projector was not able to read the metadata.tsv file when i saved multiple runs in different subfolders.\r\nI tried a lot of combinations (storing the metadata.tsv file in the root logs folder or in the subfolder of the specific run and so on)\r\nIt does not recognise the metadata file. So when hovering over points it only shows the index.\r\n\r\nAt first i used the manual process described in the tensorboard_projector_plugin tutorial.\r\n\r\nLater i tried to use the TensorBoard callback with the arguments *embeddings_freq=1, embeddings_metadata=(tried different parameters as dict or string)*\r\n\r\nI would be very happy to get clear instruction on how to define the metadata file path in the current version of tensorflow/tensorboard. It seems that a few other people have the same problem. Perhaps it did change with a newer version.\r\n\r\nI am not sure how to reference the path:\r\n- relative path from subfolder of run\r\n- relative path from tensorflow log folder root / (experiment root in case of multiple subfolders)\r\n- use backslash or forward slash (does it make a difference when using windows vs. unix)\r\n- how to reference the different layers when using the dict format in the TensorBoard callback instead of a string (in case of different embedding metadata for different embedding layers\r\n  - just use layer name as string which ist defined in model?\r\n  - or use some string with the suffix */.ATTRIBUTES/VARIABLE_VALUE* (as described in the projector tutorial)?\r\n\r\nI really tried a lot of things so i would be very happy to finally get a solution or clear description.\r\n\r\nThank you.\r\n\r\nReference links to issues with the same problem:\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/33967\r\n- https://stackoverflow.com/questions/41708106/linking-tensorboard-embedding-metadata-to-checkpoint\r\n- https://stackoverflow.com/questions/42679552/how-can-i-select-which-checkpoint-to-view-in-tensorboards-embeddings-tab", "comments": ["@suiluj I think this is more related to Tensorboard (TB) repo where experts will resolve issues related to TB. [Here](https://github.com/tensorflow/tensorboard/issues) is the repo.\r\n\r\nAFAIK, you can create multiple log files and load each one as shown in that example. I tried three different training runs and collected logs files as shown below. I was able to open each log to visualize it.\r\n\r\n<img width=\"324\" alt=\"Screen Shot 2021-03-05 at 3 40 21 PM\" src=\"https://user-images.githubusercontent.com/46058173/110186362-d6c40080-7dc9-11eb-85c8-d02fcdb4efe0.png\">\r\n \r\nMy question is do you want to load all three (or many) logs and compare them?\r\n[Here](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_projector_plugin.ipynb#scrollTo=PtL_KzYMBIzP) is a gist for reference. Thanks! \r\n\r\nPlease post this in TB repo and close here. Thanks!", "hello @jvishnuvardhan yes i know how to train multiple runs and log everything in subfolders. Event the embeddings are visible.\r\nThe main point is that i was not able to show the visualizations including my metadata.\r\n\r\nI found an older issue which had kind of the same problem with the projector_config which includes the metadata path.\r\n\r\nShould i create a completely new issue in the tensorboard repo either way or is it enough to link these issues like i did?\r\n", "@suiluj Please create a completely new issue in the Tboard repo. When you create a new issue it can be assigned to an expert who can resolve the issue faster. \r\n\r\nPlease close the issue here after creating issue in Tboard repo. Thanks!", "@suiluj I am closing this issue here as you are going to open it in TB repo. Thanks!"]}, {"number": 47564, "title": "pip3 install tensorflow-cpu fails on AArch64, Fedora 33 ", "body": "I guess it is because there is no 'wheel' defined yet. 'pip3 install tensorflow' is available, works. Is there a difference, if you have no (dedicated) GPU? ", "comments": ["@LutzWeischerFujitsu,\r\nPlease try installing the Linux aarch64 release under the Community Supported Builds from [this page](https://github.com/tensorflow/tensorflow#community-supported-builds) and check if it helps. Thanks!", "I tried pip3 install tf-nightly instead, but it fails, too. However, pip3 install tensorflow works. ", "@LutzWeischerFujitsu,\r\nWith respect to your question, \r\n> Is there a difference, if you have no (dedicated) GPU? \r\n\r\nThere is no difference with respect to `Installation` of `Tensorflow` if we have `GPU` or don't have a `GPU`. \r\n\r\nSo, you can proceed with `pip3 install tensorflow`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47564\">No</a>\n"]}, {"number": 47563, "title": "\"Status: device kernel image is invalid\" with A100", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source (pip)\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 (also have 10.1 installed)\r\n- GPU model and memory: A100-SXM4-40GB\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\ntf.constant(0)\r\n```\r\n\r\nyields the error \"Status: device kernel image is invalid\".  [This issue](https://github.com/tensorflow/tensorflow/issues/41990)  mentions that tensorflow==2.3 no longer support some older GPUs, but this shouldn't apply to this case. Is it some cuda library confusion? \r\n\r\n\r\n\r\n**Detailed log**\r\n\r\n```\r\nPython 3.6.3 (default, Mar 20 2018, 13:50:41)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-03-04 14:19:04.766920: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> x = tf.constant(0)\r\n2021-03-04 14:19:31.998272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-03-04 14:19:32.186620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.188740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:\r\npciBusID: 0000:41:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.190843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties:\r\npciBusID: 0000:81:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.192913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties:\r\npciBusID: 0000:c1:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.192970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-04 14:19:32.195229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-03-04 14:19:32.196491: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-03-04 14:19:32.196916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-03-04 14:19:32.198940: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-04 14:19:32.200061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-04 14:19:32.204798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-04 14:19:32.221788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3\r\n2021-03-04 14:19:32.222380: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-04 14:19:32.238093: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3200000000 Hz\r\n2021-03-04 14:19:32.246203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x484daa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-04 14:19:32.246307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-03-04 14:19:32.624138: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40cc380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-04 14:19:32.624199: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): A100-SXM4-40GB, Compute Capability 8.0\r\n2021-03-04 14:19:32.624218: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): A100-SXM4-40GB, Compute Capability 8.0\r\n2021-03-04 14:19:32.624235: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): A100-SXM4-40GB, Compute Capability 8.0\r\n2021-03-04 14:19:32.624286: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): A100-SXM4-40GB, Compute Capability 8.0\r\n2021-03-04 14:19:32.633961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.636050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:\r\npciBusID: 0000:41:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.638161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties:\r\npciBusID: 0000:81:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.640212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 3 with properties:\r\npciBusID: 0000:c1:00.0 name: A100-SXM4-40GB computeCapability: 8.0\r\ncoreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\r\n2021-03-04 14:19:32.640284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-04 14:19:32.640334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-03-04 14:19:32.640364: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-03-04 14:19:32.640394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-03-04 14:19:32.640421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-04 14:19:32.640449: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-04 14:19:32.640476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-04 14:19:32.656584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1, 2, 3\r\n2021-03-04 14:19:32.656644: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n  File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/opt/app-root/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 539, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n\r\n```\r\n", "comments": ["@cgebbe \r\nCould you please refer to these existing resolved issues and let us know: #43911, #43701,#41990,#41132,#42428", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47563\">No</a>\n"]}, {"number": 47562, "title": "Didnt find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3' when trying to run on Android", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (Windows 10):\r\n- TensorFlow installation (pip package):\r\n- TensorFlow library (2.3.1, pip package):\r\n\r\n### 2. Code\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()  #### Returns error here\r\n\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 151 (FlexSize) failed to prepare.\r\n\r\n", "comments": ["Since you are using the Select TF option, please refer to the following documents to enable Flex delegate for FlexSize op:\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\n\r\nYou need to link the additional aar for the Select TF option. There are two ways, one using the prebuilt aar and one using a custom built aar to reduce the unused parts for a small footprint.", "> Since you are using the Select TF option, please refer to the following documents to enable Flex delegate for FlexSize op:\r\n> https://www.tensorflow.org/lite/guide/ops_select\r\n> \r\n> You need to link the additional aar for the Select TF option. There are two ways, one using the prebuilt aar and one using a custom built aar to reduce the unused parts for a small footprint.\r\n\r\nHi, thanks very much for your reply. I will try it.\r\nFor the solutions you mentioned, does it also apply for the issue of 'RESIZE_NEAREST_NEIGHBOR' version '3' when deploying on Android?", "For the RESIZE_NEAREST_NEIGHBOR issue, could you file a separate issue with the reproducible steps?", "> For the RESIZE_NEAREST_NEIGHBOR issue, could you file a separate issue with the reproducible steps?\r\n\r\nThank you for your reply. It is actually the same issue. After I converted the model to .tflite, when I try to deploy the model on Android. I encounter the issue of RESIZE_NEAREST_NEIGHBOR. Any ideas? :)", "Can you share the model creation script or the TFLite model?", "> Can you share the model creation script or the TFLite model?\r\n\r\nI may not be able to share the model creation script or the model. It is a custom-trained model based on YOLO V3. After training, I get the checkpoint (3 files: checkpoint, .index & .data). Then a script (convert_to_pb.py from https://github.com/pythonlessons/TensorFlow-2.x-YOLOv3/tree/master/tools) is used to convert it to .pb, and then another script (shown in original post as 2. Code) to convert it to .tflite. Is this information enough?", "I am not sure that it can be reproducible on my side. If possible, could you provide minimal reproducible steps for reducing the graph structure and carrying dumb weights?\r\n\r\nI saw that you are using TF 2.3.1. I would highly recommend using TF 2.4.1 and tf-nightly both for the TFLite conversion since there are a lot of improvements in the conversion. Your problem may be already resolved at HEAD.", "Thank you for the suggestions. But the code currently is not compatible with TF 2.4.1 yet. I'll try to share the model in some way.", "Will close this issue for now, as I couldn't share the code or model. Thanks for all the inputs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47562\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47562\">No</a>\n"]}, {"number": 47561, "title": "tf.train.Checkpoint.read breaking inside a @tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 11.2.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\n\"Tensor is unhashable. Instead, use tensor.ref() as the key.\" error when running tf.train.Checkpoint.read() inside a @tf.function.\r\nI am running this on jupyter notebook and I am getting the error whenever I run the following function.\r\nword2vec.load_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=\"file\")))\r\n\r\n**Describe the expected behavior**\r\nIt was expected to work without errors as the tf.train.Checkpoint.write() does and nothing in the documentation indicates that the tf.train.Checkpoint().read() should be different.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n    import tensorflow as tf    \r\n    from tensorflow.keras import Model\r\n    from tensorflow.keras.layers import Dot, Embedding, Flatten\r\n\r\n    class Word2Vec(Model):\r\n        def __init__(self, vocab_size, embedding_dim):\r\n            super(Word2Vec, self).__init__()\r\n            self.target_embedding = Embedding(vocab_size, \r\n                                          embedding_dim,\r\n                                          input_length=1,\r\n                                          name=\"w2v_embedding\", )\r\n\r\n            self.context_embedding = Embedding(vocab_size, \r\n                                           embedding_dim, \r\n                                           input_length=5)\r\n            self.dots = Dot(axes=(3,2))\r\n            self.flatten = Flatten()\r\n\r\n        @tf.function\r\n        def call(self, pair):\r\n            target, context = pair\r\n            we = self.target_embedding(target)\r\n            ce = self.context_embedding(context)\r\n            dots = self.dots([ce, we])\r\n            return self.flatten(dots)\r\n    \r\n        @tf.function\r\n        def save_variables(self, file):\r\n            tf.train.Checkpoint(step=self.variables).write(\"variables\")\r\n            return 0\r\n    \r\n         @tf.function\r\n        def load_variables(self, file):\r\n            tf.train.Checkpoint(step=self.variables).read(\"variables\")\r\n            return 0\r\n    \r\n    #Word2Vec\r\n    embedding_dim = 300\r\n    vocab_size = 50000\r\n    word2vec = Word2Vec(vocab_size, embedding_dim)\r\n    opt = tf.keras.optimizers.SGD(learning_rate=1.0)\r\n    word2vec.compile(optimizer=opt,\r\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n    target = tf.constant([[1]])\r\n    context = tf.constant([[[2],[3],[4],[5],[6]]])\r\n    predict_example = (target,context)\r\n    word2vec(predict_example)\r\n\r\n    call_output = word2vec.call.get_concrete_function((tf.TensorSpec([None,1], tf.int32, name='target'), tf.TensorSpec([None,5,1], tf.int32, name='context')))\r\n    save_variables = word2vec.save_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=\"file\")))\r\n    load_variables = word2vec.load_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=\"file\")))\r\n\r\n    word2vec.save(\"w2v_shakespeare.tf\", save_format=\"tf\", signatures={'predict': call_output, 'save': save_variables, 'load': load_variables})\r\n`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-2-f49e7051c166>:31 load_variables  *\r\n        tf.train.Checkpoint(step=self.variables).read(\"variables\")\r\n    /Users/maurodlt/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py:2148 read  **\r\n        return self._saver.restore(save_path=save_path, options=options)\r\n    /Users/maurodlt/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py:1322 restore\r\n        file_prefix_feed_dict = {self._file_prefix_placeholder: save_path}\r\n    /Users/maurodlt/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:830 __hash__\r\n        raise TypeError(\"Tensor is unhashable. \"\r\n\r\n    TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\r\n\r\n\r\n[traceback_error.txt](https://github.com/tensorflow/tensorflow/files/6084255/traceback_error.txt)\r\n**Background** \r\nI am creating a TF model in Python which I am training using the C API. After training in the C API, I want to save and load the trained model. Considering the limitations in the C API, I need to run @tf.functions defined in python to save and load my model. However, the only function that I found that saves variables and can be used inside a @tf.function is the tf.train.Checkpoint.write() which can only be read using the tf.train.Checkpoint.read(), which is returning the reported error.\r\n\r\nThanks : ) \r\n\r\n", "comments": ["@maurodlt,\r\nI am facing a different error stating `NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for variables` on running the code. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2575450e985220c21ea919d2f2691436/47561.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Hi @amahendrakar,\r\n\r\nThe code was trying to access a folder that was not created at that point. Please find the corrected notebook [here](https://colab.research.google.com/gist/maurodlt/68cda6c01348981f08e31dc75831b82f/47561.ipynb) (same code below). \r\n\r\nThanks! : )\r\n\r\n\r\n    import tensorflow as tf    \r\n    from tensorflow.keras import Model\r\n    from tensorflow.keras.layers import Dot, Embedding, Flatten\r\n\r\n    class Word2Vec(Model):\r\n        def __init__(self, vocab_size, embedding_dim):\r\n            super(Word2Vec, self).__init__()\r\n            self.target_embedding = Embedding(vocab_size, \r\n                                      embedding_dim,\r\n                                      input_length=1,\r\n                                      name=\"w2v_embedding\", )\r\n\r\n            self.context_embedding = Embedding(vocab_size, \r\n                                       embedding_dim, \r\n                                       input_length=5)\r\n            self.dots = Dot(axes=(3,2))\r\n            self.flatten = Flatten()\r\n\r\n        @tf.function\r\n        def call(self, pair):\r\n            target, context = pair\r\n            we = self.target_embedding(target)\r\n            ce = self.context_embedding(context)\r\n            dots = self.dots([ce, we])\r\n            return self.flatten(dots)\r\n\r\n        @tf.function\r\n        def save_variables(self, file):\r\n            tf.train.Checkpoint(step=self.variables).write(\"variables\")\r\n            return 0\r\n\r\n         @tf.function\r\n        def load_variables(self, file):\r\n            tf.train.Checkpoint(step=self.variables).read(\"variables\")\r\n            return 0\r\n\r\n    #Word2Vec\r\n    embedding_dim = 300\r\n    vocab_size = 50000\r\n    word2vec = Word2Vec(vocab_size, embedding_dim)\r\n    opt = tf.keras.optimizers.SGD(learning_rate=1.0)\r\n    word2vec.compile(optimizer=opt,\r\n          loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n          metrics=['accuracy'])\r\n\r\n    target = tf.constant([[1]])\r\n    context = tf.constant([[[2],[3],[4],[5],[6]]])\r\n    predict_example = (target,context)\r\n    word2vec(predict_example)\r\n    word2vec.save_variables(\"\")\r\n\r\n    call_output = word2vec.call.get_concrete_function((tf.TensorSpec([None,1], tf.int32, name='target'), \r\n    tf.TensorSpec([None,5,1], tf.int32, name='context')))\r\n    save_variables = word2vec.save_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=\"file\")))\r\n    load_variables = word2vec.load_variables.get_concrete_function((tf.TensorSpec([], tf.string, name=\"file\")))\r\n\r\n    word2vec.save(\"w2v_shakespeare.tf\", save_format=\"tf\", signatures={'predict': call_output, 'save': save_variables, 'load': load_variables})\r\n", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6a32ac9fcb448df77c53f4d38413b3a6/47561.ipynb#scrollTo=-FwYKenh34En). Thanks!", "@maurodlt,\r\nAs the error suggests, can you please use `tensor.ref()` as shown in the [Example Code of tensor.ref()](https://www.tensorflow.org/api_docs/python/tf/Tensor#ref) and see if the error still persists? Thanks!", "> @maurodlt,\r\n> As the error suggests, can you please use `tensor.ref()` as shown in the [Example Code of tensor.ref()](https://www.tensorflow.org/api_docs/python/tf/Tensor#ref) and see if the error still persists? Thanks!\r\n\r\nHi @rmothukuru,\r\nI cannot because this error occurs inside the `tf.train.Checkpoint.read` function and I do not have control of the tensor that should be used with '`tensor.ref()'`. The only parameter that is passed to this function is the name of the file that it should read. \r\n\r\nMy Checkpoint object is defined as `tf.train.Checkpoint(step=self.variables)`, the same construction used on the `tf.train.Checkpoint(step=self.variables).write()`, which works well on the example provided. I tried another two approaches as well: to define the checkpoint using a list of '`tensor.ref()`' as suggested, and to define it without any arguments `tf.train.Checkpoint()`. None worked, indicating that the problem is indeed inside the `tf.train.Checkpoint.read` method. \r\n\r\nThanks!", "@maurodlt \r\nCan you check on the latest tf version and let us know if this is still an issue.", "@Saduf2019 \r\nThe error still persists on TF 2.6.0. We can see this by running the code example on [this](https://colab.research.google.com/gist/amahendrakar/6a32ac9fcb448df77c53f4d38413b3a6/47561.ipynb#scrollTo=-FwYKenh34En) gist. \r\nThanks!", "Yes, this is still a problem.\r\n\r\nThere is a workaround, use a TF1 name-based checkpoint from `raw_ops`.\r\n\r\nThere's a complete example here:\r\n\r\nhttps://www.tensorflow.org/lite/examples/on_device_training/overview\r\n\r\nCaution: Name-based checkpoints can be tricky.", "@maurodlt,\r\n\r\nCan you take a look at this above comment from @MarkDaoust and let us know if it helps in resolving your issue? Thanks!", "Thanks @MarkDaoust, now I can load the weights back in the model!\r\n\r\n@sanatmpa1, his response did help me as a workaround to the issue.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47561\">No</a>\n"]}, {"number": 47560, "title": "passing constants to costume rnn cell not working", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro 20.2.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.4.1\r\n- Python version:3.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.2\r\n- GPU model and memory: GeForce GTX 1080;  6858 MB memory\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nwhen passing the \"constants\" argument to a tf.keras.layers.RNN model instance with a costume rnn cell, the python script crashes\r\n**Describe the expected behavior**\r\nthe \"constants\" argument is passed to the costume RNN cells call function, and no error occurs\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nclass dummy_rnn_cell(tf.keras.Model):\r\n  def __init__(self, units):\r\n    super(dummy_rnn_cell, self).__init__()\r\n    self.gru = tf.keras.layers.GRUCell(units,\r\n                                   recurrent_initializer='glorot_uniform')\r\n    self.units = self.gru.units\r\n    self.state_size = self.gru.state_size\r\n    self.output_size = self.gru.output_size\r\n\r\n  def call(self, input_at_t, states_at_t, constants):\r\n    output, state = self.gru(input_at_t, states_at_t)\r\n    return output, state\r\n  \r\n  def get_initial_state(inputs=None, batch_size=None, dtype=None):\r\n      return self.gru.get_initial_state(inputs, batch_size, dtype)\r\n\r\nclass dummy_model(tf.keras.Model):\r\n    def __init__(self, units):\r\n        super(dummy_model, self).__init__()\r\n        self.rnn = tf.keras.layers.RNN(dummy_rnn_cell(units), True)\r\n\r\n    def call(self, inp):\r\n        seq = inp[0]\r\n        const = inp[1]\r\n        out = self.rnn(seq, constants=const)\r\n        return out\r\n\r\nmodel = dummy_model(1)\r\nseqs = [[[[1],[2]],[[1],[2]]],[[[1],[2]],[[1],[2]]]]\r\nconsts = [[[3],[3]],[[3],[3]]]\r\ndataset = tf.data.Dataset.from_tensor_slices((seqs,consts))\r\nfor inp in dataset:\r\n    print(model(inp))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n2021-03-04 13:59:52.420241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-04 13:59:53.466141: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-04 13:59:53.466826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-04 13:59:53.492760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.493343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:0d:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.797GHz coreCount: 20 deviceMemorySize: 7.91GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-04 13:59:53.493366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-04 13:59:53.495658: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-04 13:59:53.495704: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-04 13:59:53.496468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-04 13:59:53.496635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-04 13:59:53.497347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-03-04 13:59:53.497917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-04 13:59:53.498048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-04 13:59:53.498137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.498663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.499094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-04 13:59:53.499318: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-04 13:59:53.500255: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-04 13:59:53.500322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.500769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:0d:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.797GHz coreCount: 20 deviceMemorySize: 7.91GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-04 13:59:53.500786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-04 13:59:53.500798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-04 13:59:53.500809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-04 13:59:53.500820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-04 13:59:53.500830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-04 13:59:53.500840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-03-04 13:59:53.500850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-04 13:59:53.500859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-04 13:59:53.500902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.501396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.501948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-04 13:59:53.501973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-04 13:59:53.884072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-04 13:59:53.884104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-04 13:59:53.884110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-04 13:59:53.884278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.884769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.885213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-04 13:59:53.885636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6858 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:0d:00.0, compute capability: 6.1)\r\n2021-03-04 13:59:53.885875: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nTraceback (most recent call last):\r\n  File \"/home/moritz/Projects/Bitblade/tensorflow/./dummy.py\", line 39, in <module>\r\n    print(model(inp))\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1012, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/moritz/Projects/Bitblade/tensorflow/./dummy.py\", line 31, in call\r\n    out = self.rnn(seq, constants=const)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 717, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1008, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2710, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 578, in build\r\n    self.cell.build(step_input_shape)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\", line 407, in build\r\n    raise ValueError(\r\nValueError: Currently, you cannot build your model if it has positional or keyword arguments that are not inputs to the model, but are required for its `call` method. Instead, in order to instantiate and build your model, `call` your model on real tensor data with all expected call arguments.\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47560\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47560\">No</a>\n", "the minimum working example and problem description is wrong, sorry for the inconvenience. I will update and open a new one once i am sure everything is correct."]}, {"number": 47559, "title": "Fails to run the model in an Android app", "body": "Hi, I've created a model using keras. The model works fine in google-colab but when I attach it to my app in Android Studio I get the following comment:\r\n\r\n`java.lang.AssertionError: Error occurred when initializing ImageClassifier: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.`\r\n\r\nMy app works fine with a different model so I believe the problem is in the model (in the metadata to my understanding).\r\n\r\nThis is the metadata code:\r\n# Creates model info.\r\nmodel_meta = _metadata_fb.ModelMetadataT()\r\nmodel_meta.name = \"Pedastrian Lights classifier\"\r\nmodel_meta.description = (\"checks what color is the light if exists in frame\")\r\nmodel_meta.version = \"v1\"\r\nmodel_meta.author = \"Neta Kletshevsky\"\r\n# Creates input info.\r\ninput_meta = _metadata_fb.TensorMetadataT()\r\n\r\n# Creates output info.\r\noutput_meta = _metadata_fb.TensorMetadataT()\r\ninput_meta.name = \"image\"\r\ninput_meta.description = (\r\n    \"Input image to be classified.\".format(160, 160))\r\ninput_meta.content = _metadata_fb.ContentT()\r\ninput_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\r\ninput_meta.content.contentProperties.colorSpace = (\r\n    _metadata_fb.ColorSpaceType.RGB)\r\ninput_meta.content.contentPropertiesType = (\r\n    _metadata_fb.ContentProperties.ImageProperties)\r\ninput_normalization = _metadata_fb.ProcessUnitT()\r\ninput_normalization.optionsType = (\r\n    _metadata_fb.ProcessUnitOptions.NormalizationOptions)\r\ninput_normalization.options = _metadata_fb.NormalizationOptionsT()\r\ninput_normalization.options.mean = [127.5]\r\ninput_normalization.options.std = [127.5]\r\ninput_meta.processUnits = [input_normalization]\r\ninput_stats = _metadata_fb.StatsT()\r\ninput_stats.max = [255]\r\ninput_stats.min = [0]\r\ninput_meta.stats = input_stats\r\n\r\n# Creates output info.\r\noutput_meta = _metadata_fb.TensorMetadataT()\r\noutput_meta.name = \"probability\"\r\noutput_meta.description = \"Probabilities of the 1001 labels respectively.\"\r\noutput_meta.content = _metadata_fb.ContentT()\r\noutput_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\r\noutput_meta.content.contentPropertiesType = (\r\n    _metadata_fb.ContentProperties.FeatureProperties)\r\noutput_stats = _metadata_fb.StatsT()\r\noutput_stats.max = [1.0]\r\noutput_stats.min = [0.0]\r\noutput_meta.stats = output_stats\r\n#label_file = _metadata_fb.AssociatedFileT()\r\n#label_file.name = \"labels\"#label_file.description = \"Labels for objects that the model can recognize.\"\r\n#label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\r\n#output_meta.associatedFiles = [label_file]\r\n\r\n# Creates subgraph info.\r\nsubgraph = _metadata_fb.SubGraphMetadataT()\r\nsubgraph.inputTensorMetadata = [input_meta]\r\nsubgraph.outputTensorMetadata = [output_meta]\r\nmodel_meta.subgraphMetadata = [subgraph]\r\n\r\nb = flatbuffers.Builder(0)\r\nb.Finish(\r\n    model_meta.Pack(b),\r\n    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\r\nmetadata_buf = b.Output()\r\n\r\npopulator = _metadata.MetadataPopulator.with_model_file(\"converted_model.tflite\")\r\npopulator.load_metadata_buffer(metadata_buf)\r\n#populator.load_associated_files('/content/drive/MyDrive/android/assets')\r\npopulator.populate()\r\n\r\n**What NormalizationOptions do I need to add and how?**", "comments": ["@lu-wang-g could you take a look at this? Thanks", "I would highly appreciate if someone can help! ", "You should be able to pack the normalization params with the following lines of code:\r\n```\r\ninput_normalization.options = _metadata_fb.NormalizationOptionsT()\r\ninput_normalization.options.mean = [127.5]\r\ninput_normalization.options.std = [127.5]\r\n```\r\nNot sure why it doesn't work for you.\r\n\r\nPlease try the MetadataWriter Library to populate the metadata:\r\n1. Install the TFLite Support nightly Pypi package:\r\n```\r\npip install tflite_support_nightly\r\n```\r\n2. Write metadata to the model using the following script:\r\n```\r\nfrom tflite_support.metadata_writers import image_classifier\r\nfrom tflite_support.metadata_writers import writer_utils\r\nfrom tflite_support import metadata\r\n\r\nImageClassifierWriter = image_classifier.MetadataWriter\r\n_MODEL_PATH = \"mobilenet_v1_1_default_1.tflite\"\r\n_LABEL_FILE = \"labelmap.txt\"\r\n_SAVE_TO_PATH = \"mobilenet_v1_1_default_1_metadata.tflite\"\r\n_MEAN = 127.5\r\n_STD = 127.5\r\n\r\nwriter = ImageClassifierWriter.create_for_inference(\r\n    writer_utils.load_file(_MODEL_PATH), [_MEAN], [_STD], [_LABEL_FILE])\r\nwriter_utils.save_file(writer.populate(), _SAVE_TO_PATH)\r\n\r\n# Verify the populated metadata and associated files.\r\ndisplayer = metadata.MetadataDisplayer.with_model_file(_SAVE_TO_PATH)\r\nprint(\"Metadata populated:\")\r\nprint(displayer.get_metadata_json())\r\nprint(\"Associated file(s) populated:\")\r\nprint(displayer.get_packed_associated_file_list())\r\n```\r\n\r\nUsing mean=127.5 and std=127.5 will normalize your image from [0, 255] to [-1, 1]. Some models may normalize it to [0, 1], so mean=0, std=255. Make sure the values are set properly for your case.", "Thank you very much, it's working now. Very appreciate your help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47559\">No</a>\n"]}]