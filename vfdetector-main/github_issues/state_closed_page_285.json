[{"number": 45764, "title": "Hexagon Delegate of fully connected layer ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHello, with the attached example tflite model, it runs on our device cpu good, output matches with expected. However if we use hexagon delegate to run, the output do not match, every 0x200 blocks have the same data.\r\nThe model is generated with tf1.15\r\nI see that there's a test code at lite/delegates/hexagon/builders/tests/matmul_test.cc, however it asks the weight to be const, while in our model we need both inputs to matmul to be dynamic (output from previous nodes).\r\nThe matmul inputs are both at 512x256.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["How do I attached the tflite model? it only allows for txt file for attachment.", "@tensorbuffer,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.4 and check if you are facing the same issue?\r\n\r\n> How do I attached the tflite model? it only allows for txt file for attachment.\r\n\r\nYou can drag and drop the file in the comment box or upload the file on Google Drive and share the link with us. \r\n\r\nAlso, please provide the complete code and sequence of commands / steps that you executed before running into the issue. Thanks! ", "Ok I have found out the root cause, it's a mismatch between tflite and nnlib.\r\nI will close this issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45764\">No</a>\n"]}, {"number": 45763, "title": "Update release notes for TensorFlow 1.15.5", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.15.5\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 45762, "title": "CUDA 10.0 support has been dropped in tf 2.4", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.0/7.6.3\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nWhen building tf 2.4.0 from source on CUDA 10.0 the build fails with\r\n\r\n```\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  /tensorflow_src/third_party/gpus/cuda_configure.bzl:1430:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 1400\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 1075, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 606, in _find_libs\r\n\t\t_check_cuda_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/tensorflow_src/third_party/gpus/cuda_configure.bzl\", line 501, in _check_cuda_libs\r\n\t\texecute(repository_ctx, <1 more arguments>)\r\n\tFile \"/tensorflow_src/third_party/remote_config/common.bzl\", line 217, in execute\r\n\t\tfail(<1 more arguments>)\r\nRepository command failed\r\nNo library found under: /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcublasLt.so.10.0\r\n```\r\n\r\nThe dependency on cublasLt was added in 592947d1a6c4123d8aec8362d74b020c8b4e2b1a but cublasLt was added only in CUDA 10.1 and later.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe dependency on cublasLt is modelled such that on CUDA 10.0 it is not required.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@janbernloehr,\r\nEvery TensorFlow release is compatible with a certain CUDA and cuDNN version. Please go through the [tested build configurations](https://www.tensorflow.org/install/source#gpu) for more information. \r\n\r\nPlease try building TensorFlow 2.4 with CUDA 11 and cuDNN 8 and check if you are facing the same error. Thanks! ", "Building with CUDA 11 works fine, also 10.2 works. I was not sure if dropping CUDA 10.0 was intentional but I get from your response that it is not supported anymore. Thanks!", "@janbernloehr,\r\nYes, CUDA 10.0 is not supported with TensorFlow v2.4. \r\n\r\n> Building with CUDA 11 works fine, also 10.2 works\r\n\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45762\">No</a>\n"]}, {"number": 45761, "title": "Add exception for .resource and .md files from the style check.", "body": "These are config files for renode / robot and the exception is needed to\r\nmerge PRs like https://github.com/tensorflow/tensorflow/pull/44729 and https://github.com/tensorflow/tensorflow/pull/44726\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45760, "title": "Progress bar silence during validation for model.fit()", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen using model.fit(), even setting `verbose=2` will still disable progress bar in the validation steps. Only the final progress bar with metric shows up when validation steps are finished. After digging the cause, it turns out that an attribute called `_called_in_fit ` is set to `True`, as seen in the following lines. I am not sure what is the reasoning behind disabling progress bar in validation step. But IMO enabling/disabling progress bar option should be left for user to choose.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d3e40668078c96ef46cc3bd59b91e222852d4214/tensorflow/python/keras/callbacks.py#L1001-L1003\r\n\r\n**Will this change the current api? How?**\r\n\r\nPotentially adding an argument called `val_verbose` in `model.fit()` to enable/disable the progress bar.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone whose val dataset is large and requires long time to run.\r\n\r\n**Any Other info.**\r\n", "comments": ["@wuhy08 Sorry for the late response. As keras code moved to separate repo [keras-team/keras](https://github.com/keras-team/keras/issues), can you please open the same issue there. Thanks!\r\n\r\nYou could also raise a PR if you want to contribute. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45758, "title": "[TensorFlow 2.x]'s model from OFFICAL DOC cannot freeze in any way( included [frozen_graph.py] in offical doc) [TenseorFlowSharp]. [C#] [Python][frozen model] CSharp Transfer Learning", "body": "Sorry to trouble you guys, but i cost days and problem dosen't gone :(\r\n\r\nDescription: As a outsider of mechine learning, i use ml.net before. And there's a project need ML on Raspberry Pi, so it won't work( Linux Arm is not sueported).\r\nInstead of ML.net, i plan to use **TensorFlowShar****p**(TF#, 'cause i muna use C# to write GUI and other things), and it **need a FROZEN MODEL**. So as a idot of mechin-learning, i followed officical docs to built a image classifier.(Details: _Anaconda_, _TensorFlow 2.3_, docs here: [https://tensorflow.google.cn/tutorials/images/transfer_learning](url) )\r\n\r\nThen just to use a code like this of the end of the codes to get the model file:\r\nkeras_model_path = './keras_save'\r\nmodel.save(keras_model_path)\r\nh5_save_path = 'model.h5'\r\nmodel.save(h5_save_path)\r\n\r\nThe difficulity is to make the 'h5' or savedmodel to be a frozen model.\r\nThe official code \"frozen_graph\" dosent work, and I used many scripts but failed.\r\nThe last code in my latop looks like this( From Lei Mao, thank you).\r\n\r\nFollow the code, i put the model firstly to create ConcreteFunction, and then get frozen ConcreteFunction, finally write it on the hard drive.\r\n\r\nFor this clip of code, question is : **tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.**\r\nIn this place\r\n `frozen_func = convert_variables_to_constants_v2(full_model) # Error`\r\n\r\nFreeze code like this:\r\n`import matplotlib.pyplot as plt`\r\n`import numpy as np`\r\n`import os`\r\n`import tensorflow  as tf`\r\n`from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2`\r\n`from tensorflow import keras`\r\n\r\n`model = keras.models.load_model('keras_save')`\r\n\r\n`# \u5c06 Keras \u6a21\u578b\u8f6c\u6362\u4e3a ConcreteFunction`\r\n`full_model = tf.function(lambda x: model(x))`\r\n`full_model = full_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))`\r\n\r\n`# \u83b7\u53d6 \u51bb\u7ed3\u7684 ConcreteFunction`\r\n`frozen_func = convert_variables_to_constants_v2(full_model) # Error`\r\n`frozen_func.graph.as_graph_def()`\r\n\r\n`layers = [op.name for op in frozen_func.graph.get_operations()]`\r\n`print(\"-\" * 50)`\r\n`print(\"Frozen model layers: \")`\r\n`for layer in layers:`\r\n  `  print(layer)`\r\n\r\n`print(\"-\" * 50)`\r\n`print(\"Frozen model inputs: \")`\r\n`print(frozen_func.inputs)`\r\n`print(\"Frozen model outputs: \")`\r\n`print(frozen_func.outputs)`\r\n\r\n`# \u5c06\u51bb\u7ed3\u7684\u56fe\u4ece\u51bb\u7ed3\u7684ConcreteFunction\u4fdd\u5b58\u5230\u786c\u76d8`\r\n`tf.io.write_graph(graph_or_graph_def=frozen_func.graph,logdir=\"./frozen_models\",name=\"frozen_graph.pb\",as_text=False)`\r\n\r\nI'm doubt that is there any bugs? I hope you guys could pull me a hand ...\r\n\r\nBy the way, i also TIRED to use a TensorFlowSharp method called \"FromSavedModel\", it returns a TFSession type object, but icannot use this way to classify image. Only secess is to make image classification from google inception .... I hope you guys could give me a hand, and now the difficultiy is to <1 export a frozen model and then use it in TF# OR\r\n<2 use a SavedModel in TF# .\r\n\r\nIn C# of TF#\uff0c error here\uff1a\r\nTensorFlow.TFException:\u201cOp type not registered 'StatefulPartitionedCall' in binary running on LAPTOP-8FU28J1C. Make sure the Op and Kernel are registered in the binary running in this process.\u201d\r\ncode is\uff1a\r\n\r\n`//load SavedModel`\r\n`            TFGraph _tfGraph = new TFGraph();`\r\n`            TFSession _tfSession;`\r\n`           using (var tmpSess = new TFSession(_tfGraph))`\r\n`            using (var tfSessionOptions = new TFSessionOptions())`\r\n`           using (var metaGraphUnused = new TFBuffer())`\r\n`            {`\r\n`                //for some reason FromSavedModel is not static`\r\n`                _tfSession = tmpSess.FromSavedModel(tfSessionOptions, null, \"tmp/keras_save\", new[] { \"serve\" }, _tfGraph, `metaGraphUnused);`\r\n`            }`\r\ncode is from Github also, links here:\r\n[https://github.com/migueldeicaza/TensorFlowSharp/issues/265](url)\r\nmigueldeicaza/TensorFlowSharp/issues/265\r\n\r\n\r\nLast recorded 12.17( Dec,17,2020) Peking Time.\r\n", "comments": ["Sorry, # in Python make words so huge ...", "@Rick-712N,\r\nfrozen_graph is a TF 1.x concept. In TF2.x, the primary export format is SavedModels.\r\n\r\nIs there any specific reason you want to export a frozen graph? Thanks!", "> @Rick-712N,\r\n> frozen_graph is a TF 1.x concept. In TF2.x, the primary export format is SavedModels.\r\n> \r\n> Is there any specific reason you want to export a frozen graph? Thanks!\r\n\r\nAppreciate for your commit! :)  I wanna run the model in **TensorFlowSharp**(TF#) which is a C# version tensorflow, but it need Frozen model. (Saved model is available but i cannot load my tf2 SavedModel in it, it might be not support TF2.x)", "BTW, i ran the official freeze_graph.py tool to transform, but error:\r\nCode like this:\r\n\r\nimport tensorflow as tf\r\nfrom  tensorflow.python.tools import freeze_graph\r\n\r\n\r\n`with tf.compat.v1.Session() as sess:`\r\n`    freeze_graph.freeze_graph(`\r\n`\t\t\tinput_graph=\"\", `\r\n `           input_saver=\"\", `\r\n  `          input_binary=False, `\r\n   `         input_checkpoint=\"\", `\r\n    `        output_node_names=\"dense/BiasAdd\", `\r\n     `       restore_op_name=\"\", `\r\n      `      filename_tensor_name=\"\", `\r\n       `     output_graph=\"frozen_graph.pb\",`\r\n        `    clear_devices=\"\", `\r\n         `   initializer_nodes=\"\", `\r\n          `  variable_names_whitelist=\"\", `\r\n           ` variable_names_blacklist=\"\", `\r\n            `input_meta_graph=None, `\r\n`            input_saved_model_dir=\"keras_save\" `\r\n`\t\t\t)`\r\n `   print('Over !')`\r\n\r\noutput node name is the real output node name in my model, which is the model of the TensorFlow offical transferlearning tutorial.\r\n\r\nerror: `AssertionError: dense/BiasAdd:0 is not in graph`\r\n\r\ni donnot know what's wrong with it, cuz im a idot of mechine learning ...", "Finally, I muna use C# to make other things, and use python to do Mechine-Learning. I have no time to stuck in this terrible stuff... and I have a tech competition to prepare.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45758\">No</a>\n"]}, {"number": 45757, "title": "Support channel wise fill value", "body": "Closes #45717. ", "comments": ["/cc @tanzhenyu for visibility. Can you take a look at it? Thank you!", "Hi @tanzhenyu, can I get the failed internal checks if there is something wrong on my part? Thank you!", "@WindQAQ  Can you please resolve conflicts? Thanks!", "> @WindQAQ Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned Done!", "@WindQAQ  Can you please resolve conflicts? Thanks!", "@WindQAQ  Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 45755, "title": "fix memory_space_assignment_best_fit_repacker_test", "body": "in BestFitRepacker::Repack, the call to Finish moves the 'result_'\r\ndata to the values returned. This means that result_ is empty\r\nfor the remainder of the code in Repack.\r\nUse the result returned form Finish instead.", "comments": ["If it's a fix, is it possible to add a test?", "the test is already there. it's memory_space_assignment_best_fit_repacker_test.\r\nW/o this change it fails", "The fix looks correct, but I was not able to reproduce the test failure:\r\n\r\n```\r\nbazel test -c opt --config=cuda --nodistinct_host_configuration //tensorflow/compiler/xla/service:memory_space_assignment_best_fit_repacker_test\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [v2, cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=362\r\nINFO: Reading rc options for 'test' from /mnt/Code/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'test' from /mnt/Code/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'test' from /mnt/Code/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/bin/python3 --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11.0 --action_env TF_CUDNN_VERSION=8 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Reading rc options for 'test' from /mnt/Code/tensorflow/.bazelrc:\r\n  'test' options: --config=v2\r\nINFO: Reading rc options for 'test' from /mnt/Code/tensorflow/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium --test_env=LD_LIBRARY_PATH\r\nINFO: Found applicable config definition build:short_logs in file /mnt/Code/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /mnt/Code/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file /mnt/Code/tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nINFO: Found applicable config definition build:xla in file /mnt/Code/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:tensorrt in file /mnt/Code/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /mnt/Code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /mnt/Code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:v2 in file /mnt/Code/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file /mnt/Code/tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nINFO: Found applicable config definition build:cuda in file /mnt/Code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /mnt/Code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /mnt/Code/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /mnt/Code/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/b7593a7688ee632b794500a46dd0810f/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /mnt/Code/tensorflow/WORKSPACE:16:10: in <toplevel>\r\n  /mnt/Code/tensorflow/tensorflow/workspace0.bzl:65:34: in workspace\r\n  /root/.cache/bazel/_bazel_root/b7593a7688ee632b794500a46dd0810f/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/b7593a7688ee632b794500a46dd0810f/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/compiler/xla/service:memory_space_assignment_best_fit_repacker_test (150 packages loaded, 6708 targets configured).\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/compiler/xla/service:memory_space_assignment_best_fit_repacker_test up-to-date:\r\n  bazel-bin/tensorflow/compiler/xla/service/memory_space_assignment_best_fit_repacker_test\r\nINFO: Elapsed time: 154.691s, Critical Path: 92.83s\r\nINFO: 4786 processes: 2069 internal, 2717 local.\r\nINFO: Build completed successfully, 4786 total actions\r\n//tensorflow/compiler/xla/service:memory_space_assignment_best_fit_repacker_test PASSED in 0.3s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 4786 total actions\r\n```", "Bas, actually I don't the `std::move` of `result_`, can you share where you see it?  I believe [this](https://github.com/tensorflow/tensorflow/blob/673079ce30f04324cda9b8a276a4ff7eb38bbacd/tensorflow/compiler/xla/service/heap_simulator.cc#L818) is the `Finish` method we're calling here right?", "@bas-aarts  Can you please check @sanjoy's comments and keep us posted ? Thanks!", "@sanjoy, you are correct. Possibly a botched integration on our end.\r\nDiscussing with @trentlo if the \"std:move\" that is present in our code is required.  ", "> @sanjoy, you are correct. Possibly a botched integration on our end.\r\n> Discussing with @trentlo if the \"std:move\" that is present in our code is required.\r\n\r\nThe std::move was originally part of a NVidia PR, but was removed during the review process.\r\nIt remained in our code, which caused the failure of the new test.\r\n\r\nWe'll remove the 'move' on our end. I'll leave it up to you to either accepts or ignore this PR. Since the test is the only not explicitly using the return value from 'Finish', I would prefer to accept the PR to match other tests.\r\n\r\nApologies for the inconvenience", "Let's close this PR for now, as per https://github.com/tensorflow/tensorflow/pull/45755#issuecomment-771230764."]}, {"number": 45754, "title": "fix infinite loop when using fuel for multi-output fusion", "body": "", "comments": []}, {"number": 45753, "title": "Tensorflow lite C++ library release default sample crashes at runtime on Windows build", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):  2.3.1 and 2.4.0\r\n- Python version: 3.6 (not relevant)\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): MSVC++ 14.28  (visual studio 2019 version 16.8.2)\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia RTX 2080 Ti 11Go\r\n\r\n**Describe the current behavior**\r\n\r\nMy build commmands are the following building both tensorflow lite c++ and c libraries\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow\r\ncd tensorflow\r\ngit checkout 2.3.1\r\nconfigure.cmd\r\n   CUDA 7.5 compute capability activated\r\n   all the rest is default\r\nbazel build --config=opt //tensorflow/lite/c:tensorflowlite_c.dll //tensorflow/lite:tensorflowlite.dll\r\n```\r\n\r\nThe headers are collected with the following commands:\r\n```\r\nxcopy tensorflow\\*.h include\\tensorflow\\ /sy\r\npushd tensorflow\\lite\\tools\\make\r\nc:\\msys64\\mingw64.exe ./download_dependencies.sh\r\npopd\r\nxcopy tensorflow\\lite\\tools\\make\\downloads\\flatbuffers\\include\\*.h include\\ /sy\r\n```\r\n\r\nUsing the minimal C++ sample [tensorflow\\tensorflow\\lite\\examples\\minimal\\minimal.cc,](https://github.com/tensorflow/tensorflow/blob/9754a22a8091af2943ad479d5e5a56e84f58a701/tensorflow/lite/examples/minimal/minimal.cc#L67)\r\njust uncommenting the line to get the input tensor pointer as in:\r\n`float* input = interpreter->typed_input_tensor<float>(0);`\r\nthere is an exception within the call due to unallocated memory in ..include\\vector:\r\n```\r\nException thrown: read access violation.\r\n_My_data was 0xFFFFFFFFFFFFFFF7.\r\n```\r\nCall stack was:\r\n```\r\nTestTensorFlow.exe!std::vector<int,std::allocator<int>>::operator[](const unsigned __int64 _Pos) Line 1510\tC++\r\nTestTensorFlow.exe!tflite::Interpreter::typed_input_tensor<float>(int index) Line 292\tC++\r\nTestTensorFlow.exe!main(int argc, char * * argv) Line 68\tC++\r\n```\r\n\r\nTesting other models yield the same error. I used pre-trained models downloaded from tensorflow such as this small one: [mobilenet_v1_0.25_128](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.25_128.tgz)\r\nTesting the same models with the c library built with at the same time with the c++ library works fine.\r\n\r\n_Important remark: the debug build of the lib works fine_ \r\nThe debug lib built with the below command does not have the issue:\r\nbazel build -c dbg //tensorflow/lite:tensorflowlite\r\n\r\n**Describe the expected behavior**\r\n\r\nThe vector used internally by the tflite::interpreter should be initialized so that I can get the pointer to input tensor.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHere is the smallest visual studio project I could create - the code minimal.cc is not modifed in any significant way, and the vcproj simply include the headers, add the directory containing the build c++ tensorflowlite.dll and libtensorflowlite.dll.ifso (that i renamed with a .lib extension) and set the file libtensorflowlite.dll.if.lib as an input library.\r\nTo use it you need to change the path in the project to the relevant ones for your tensorflow build location.\r\n\r\n[TestTensorflowCC.zip](https://github.com/tensorflow/tensorflow/files/5705055/TestTensorflowCC.zip)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["What's the build command you used to build the minimal?", "I also wonder if you're seeing the same issue with CMake build.\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal", "I will try with the cmake build. \r\nI did not use a build command to build the minimal example, I created the attached VisualStudio project - which run fine when building the library with debug symbols and without optimization with bazel flag \"-c dbg\", but which fails when building with \"-c opt\" option.", "Hello, I tried the cmake build, for now the build fails on the pthreadpool project due to an include of stdatomic.h:\r\n`minimal_build\\pthreadpool-source\\src\\threadpool-atomics.h(134,1): fatal error C1083: Cannot open include file: 'stdatomic.h': No such file or directory`\r\nI'm fiddling around to try and make it work - I suppose the cmake build is designed to work with clang or mingw compiler but not MSVC yet - if you have any advice this would be very much appreciated. Thanks", "Ok so creation of the visual studio project files with cmake is fine, I just had to modify the pthreadpool project to include a spoofed version for [stdatomic.h](http://docs.ros2.org/latest/api/rcutils/win32_2stdatomic_8h_source.html) found on another project [ROS 2](https://index.ros.org/doc/ros2/), since this is a part of C11/C17 standard that is still not implemented by Visual Studio 2019 16.8.2.\r\n\r\nBoth the release and debug .lib version are working fine. I will suppose this is the recommended way of building tensorflowlite c++ library on window, thus I won't try to make the bazel release build work as expected.\r\nI will point out that maybe the documentation on building tensorflowlite library for windows is too light, I would've never suspected that the 'minimal_build' cmake tutorial was a mean of building this library, if it were not for @terryheo nice suggestion.\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45753\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45753\">No</a>\n", "@spikegee Try to build Tensorflow lite using this bazel command\r\n\r\nbazel build --cxxopt='--std=c++11' -c opt //tensorflow/lite:tensorflowlite\r\n\r\nMy issue got resolved while using this method.", "Still have this issue, the above bazel build command somehow does not solve the issue.\r\n\r\nHere is the error:\r\n```\r\nThe thread 0x2ebc has exited with code 0 (0x0).\r\nException thrown at 0x00007FF7A7EB6DE3 in FYDP_Demo.exe: 0xC0000005: Access violation reading location 0xFFFFFFFFFFFFFFFF.\r\nUnhandled exception thrown: read access violation.\r\n**_My_data** was 0xFFFFFFFFFFFFFFF7.\r\n```"]}, {"number": 45752, "title": "Allowing dict key mismatch among dataloader, keras model output, loss, and metric", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, when tf.keras.Model.call is called and return type is dictionary, it requires that the keys of the dictionary matches the keys of the Dataloader's sample output, which also need to be sames as the keys of losses and metrics during compile. \r\n\r\nWhen training a complex model, it might be the case that only certain branches of the model matches the dataloader, losses and metrics. It will be great if only the intersect of all the key sets are used, potentially with warning, instead of throwing exceptions like `The two structures don't have the same sequence length. Input structure has length 11, while shallow structure has length 10.`\r\n\r\n**Will this change the current api? How?**\r\n\r\nPotentially. During compilation phrase, a flag argument like `allow_key_mismatch` can be passed to suppress the error.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who train a complex model with a lot of branches (outputs used for different tasks)\r\n\r\n**Any Other info.**\r\n", "comments": ["@wuhy08 \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "Hi @ravikyram \r\nI will take a look at the code. Thank you!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45751, "title": "Unexpected kernel_shape (and trainable parameters) in Keras group convolution", "body": "In Google Colab (tensorflow version 2.3.0) I am getting an unexpected kernel shape (and trainable params in the layer) for keras group convolution. \r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tensorflow.__version__)\r\n\r\nbatch = 15\r\nchannels = 6\r\nrows = 10\r\ncols = 10\r\nkernel = (3, 3)\r\ngroups = 2\r\nfilters = 32\r\n\r\nconv_layer = tf.keras.layers.Conv2D(filters, kernel, groups)\r\n\r\noutput = conv_layer(tf.ones([batch, rows, cols, channels]))\r\n\r\nprint(conv_layer.get_weights()[0].shape)\r\nprint(conv_layer.get_weights()[1].shape)\r\n```\r\n\r\ngives me the following output:\r\n\r\n```\r\n2.3.0\r\n(3, 3, 6, 32)\r\n(32,)\r\n```\r\n\r\nReading the documentation for the groups params in Conv1D/Conv2D/Conv3D, I was expecting a kernel shape of the following dimensions: weights (3, 3, 6, 16) and biases (16,). Is that a bug or I am missing something?\r\n\r\nThanks!\r\n", "comments": ["I have tried in colab with TF version 2.2, 2.3 and nightly version(`2.5.0-dev20201216`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/191e8f4f718524085cdad0f5ea139be8/untitled581.ipynb). Thanks!", "Sorry, my previous code snippet had a bug since groups is not the third param in Conv2D. Here is an updated version:\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nbatch =15\r\nchannels=6\r\nrows = 10\r\ncols = 10\r\nkernel = (3, 3)\r\ngroups = 2\r\nfilters = 32\r\n\r\nconv_layer = tf.keras.layers.Conv2D(filters, kernel, groups=groups)\r\n\r\noutput = conv_layer(tf.ones([batch, rows, cols, channels]))\r\n\r\nprint(conv_layer.get_weights()[0].shape)\r\nprint(conv_layer.get_weights()[1].shape)\r\n```\r\n\r\nThis prints\r\n\r\n```\r\n2.4.0\r\n(3, 3, 3, 32)\r\n(32,)\r\n```\r\n\r\nwhich corresponds to: kernel + channels/2 + filters.\r\n\r\nThis kernel_shape would look okey to me if the params of the convolution are NOT shared across the different groups. This seems to be the case, right?", "@raulsoutelo,\r\nThe behavior is expected because, as per the [Documentation of Groups Argument](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#arguments_1), \r\n\r\n> The output is the concatenation of all the groups results along the channel axis.\r\n\r\nand as per [this blog](https://blog.yani.ai/filter-group-tutorial/) , \r\n\r\n> each filter group are convolved with only half the previous layer\u2019s featuremaps\r\n\r\nFrom the above statements, **`Feature Maps`** from previous layers are divided among 2 **`Groups`** but finally, they are concatenated. \r\n\r\nHence, the **`Number of Parameters`** with and without **`Groups`** (by the end of the Layer) will be same but the Representational Power is better.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@raulsoutelo,\r\nCan you please respond to the [above comment](https://github.com/tensorflow/tensorflow/issues/45751#issuecomment-750035881)? Thanks!", "Thanks for the clarification @rmothukuru. I initially thought this group parameter was the size of the group in Group Equivariant Convolutional Networks which is not the case. The previous comment is clear to me. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45751\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45751\">No</a>\n"]}, {"number": 45750, "title": "Fix sanity", "body": "", "comments": []}, {"number": 45749, "title": "micro: port op FLOOR_MOD from lite", "body": "\r\n@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator FLOOR_MOD from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/floor_div.cc into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro without making any changes or including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45749\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45749\">No</a>\n"]}, {"number": 45747, "title": "Disable test which now fails as code is too old.", "body": "", "comments": []}, {"number": 45746, "title": "Tensorflow compile failure on Power9/PPC64LE, how to pass compile flags to NVCC?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RedHat 8, PPC64LE\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): gcc 8.3.1\r\n- CUDA/cuDNN version: 10.2/7.6\r\n- GPU model and memory: Tesla V100\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI get the following error when  compiling Tensorflow for IBM Power9 architectures (PPC64LE):\r\n\r\n```/usr/include/bits/floatn.h(79): error: identifier \"__ieee128\" is undefined```\r\n```/usr/include/bits/floatn.h(82): error: invalid argument to attribute \"__mode__\"```\r\n\r\nPer this PyTorch github thread that had the same issue (https://github.com/pytorch/pytorch/issues/45073) you apparently need to pass in some compile flags, in particular \"-mno-float128\" to CC and \"-Xcompiler -mno-float128\" to NVCC. From the Bazel documentation you can use ``--copt/--cxxopt`` to pass in compile flags for CC but is there a way to pass in compile flags to NVCC? \r\n\r\nThe related issue thread at https://github.com/tensorflow/tensorflow/issues/38912 unfortunately suggests this is still an open problem..\r\n\r\nAny help would be appreciated!\r\n", "comments": ["Facing the same problem.\r\n\r\nAccording to some docs from tensorflow, we can pass ` --copt=-nvcc_options=disable-warnings ` to bazel to disable nvcc warnings. How about passing `--copt=-nvcc_options=Xcompiler --copt=-nvcc_options=mno-float128` to  bazel?\r\n", "So I still run into compiling errors when trying to pass in ``-nvcc_options`` through ``--copt``, it seems like those options don't get fully propagated through bazel, specifically I ran into problems compiling nccl-related files\r\n\r\nI was however able to successfully compile Tensorflow 2.3 with GPU support on PPC64LE by directly modifying the bazel build files for tensorflow and nccl. In particular:\r\n\r\n1. Replace these lines of tensorflow.bzl (https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/tensorflow.bzl#L1380-L1383) with\r\n```\r\n        \"@local_config_cuda//cuda:using_nvcc\": ([\r\n            \"-nvcc_options=relaxed-constexpr\",\r\n            \"-nvcc_options=ftz=true\",\r\n            \"-nvcc_options=compiler-options=-mno-float128\",\r\n        ]),\r\n```\r\n2. Replace these lines of build_defs.bzl.tpl from nccl (https://github.com/tensorflow/tensorflow/blob/r2.3/third_party/nccl/build_defs.bzl.tpl#L46-L51) with\r\n\r\n```\r\n        \"@local_config_cuda//cuda:using_nvcc\": [\r\n            \"-nvcc_options\",\r\n            \"relocatable-device-code=true\",\r\n            \"-nvcc_options\",\r\n            \"-nvcc_options=compiler-options=-mno-float128\",\r\n            \"ptxas-options=\" + maxrregcount,\r\n        ],\r\n```\r\n\r\n3. Then compile Tensorflow with the following call:\r\n```\r\n bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt='--machine-no-float128' --host_cxxopt='--machine-no-float128' --copt='--machine-no-float128' --host_copt='--machine-no-float128' --verbose_failures\r\n```\r\n\r\nThe key is passing in the compile flag ``-mno-float128`` since GCC 8.3.1 for RHEL 8 for PPC64LE architecture is configured with ``--with-long-double-128`` which is causing problems when compiling certain ops. I suspect this should also work for Tensorflow 2.4, but I haven't tried it yet. \r\n\r\nPerhaps there should be a way to automatically add these flags when compiling Tensorflow for PPC64LE...", "You can add configs into `.bazelrc`", "@vlawhern If you want you can try prepare a PR for that specific GCC version on:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/aa92d1d7a9bd24e157326bd3dc18945ab5ca5b78/configure.py#L522-L524", "Also probably it seems it is fixed with CUDA11 (and so with TF 2.4.0). Please check https://github.com/LLNL/blt/issues/341#issuecomment-620810990\r\n\r\nIf this is fixed with TF 2.4.0 we could close this as we will not do specific TF 2.3.x backports. @vlawhern Can you try with plain TF 2.4.0?", "@bhack I tried compiling TF 2.4.0 but was unable to due to a separate issue with Eigen (see #44626), which appears to be unresolved at this point. So I'm not sure if this issue is fixed with TF 2.4 right now..", "@vlawhern You can ping that ticket for an update.", "Just circling back to this...\r\n\r\nI successfully compiled a newer version of TF (TF 2.5.2, CUDA 11, cuDNN 8) without problems and without having to do the fixes I described above. So I believe this is no longer an issue (and the problems I had with Eigen were resolved in https://github.com/tensorflow/tensorflow/issues/44626). ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45746\">No</a>\n"]}, {"number": 45744, "title": "Illegal instruction (core dumped) in a CPU with AVX support", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Linux Ubuntu 20.10\r\n- TensorFlow installed from (source or binary): pip, tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl\r\n- TensorFlow version (use command below): tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI was using tensorflow 2 without any issue until a few days ago. I installed it using pip in a new virtualenv environment and now it throws illegal instruction error. My CPU is an i5-3230M, which, according to /proc/cpuinfo supports AVX, so it seems not related to #19584 .\r\n\r\nMy CPU (according to `/proc/cpuinfo`):\r\nmodel name      : Intel(R) Core(TM) i5-3230M CPU @ 2.60GHz\r\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n\r\nSystem:\r\n```\r\n$ uname -a\r\nLinux e431 5.8.0-33-generic #36-Ubuntu SMP Wed Dec 9 09:14:40 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe import should work and not give the `Illegal instruction (core dumped)` error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n$ python3 --version\r\nPython 3.8.6\r\n$ python3 -m venv test\r\n$ . test/bin/activate\r\n$ pip install --upgrade pip\r\nCollecting pip\r\n  Using cached pip-20.3.3-py2.py3-none-any.whl (1.5 MB)\r\nInstalling collected packages: pip\r\n  Attempting uninstall: pip\r\n    Found existing installation: pip 20.1.1\r\n    Uninstalling pip-20.1.1:\r\n      Successfully uninstalled pip-20.1.1\r\nSuccessfully installed pip-20.3.3\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl (394.8 MB)\r\nCollecting gast==0.3.3\r\n  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\nCollecting absl-py~=0.10\r\n  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\r\nCollecting astunparse~=1.6.3\r\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nCollecting flatbuffers~=1.12.0\r\n  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\nCollecting google-pasta~=0.2\r\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\nCollecting grpcio~=1.32.0\r\n  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\r\nCollecting h5py~=2.10.0\r\n  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\r\nCollecting keras-preprocessing~=1.1.2\r\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\nCollecting numpy~=1.19.2\r\n  Using cached numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\r\nCollecting opt-einsum~=3.3.0\r\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\nCollecting protobuf>=3.9.2\r\n  Using cached protobuf-3.14.0-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\r\nCollecting six~=1.15.0\r\n  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\r\nCollecting tensorboard~=2.4\r\n  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\r\nRequirement already satisfied: setuptools>=41.0.0 in ./test/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (44.0.0)\r\nCollecting google-auth<2,>=1.6.3\r\n  Using cached google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.2.0-py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\r\nCollecting markdown>=2.6.8\r\n  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nCollecting pyasn1<0.5.0,>=0.4.6\r\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\nCollecting requests<3,>=2.21.0\r\n  Using cached requests-2.25.0-py2.py3-none-any.whl (61 kB)\r\nCollecting certifi>=2017.4.17\r\n  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\r\nCollecting chardet<4,>=3.0.2\r\n  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\r\nCollecting idna<3,>=2.5\r\n  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nCollecting rsa<5,>=3.1.4\r\n  Using cached rsa-4.6-py3-none-any.whl (47 kB)\r\nCollecting tensorboard-plugin-wit>=1.6.0\r\n  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\r\nCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\r\n  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\r\nCollecting termcolor~=1.1.0\r\n  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\r\nCollecting typing-extensions~=3.7.4\r\n  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\nCollecting urllib3<1.27,>=1.21.1\r\n  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\r\nCollecting werkzeug>=0.11.15\r\n  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\r\nCollecting wheel~=0.35\r\n  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\r\nCollecting wrapt~=1.12.1\r\n  Using cached wrapt-1.12.1.tar.gz (27 kB)\r\nUsing legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\r\nUsing legacy 'setup.py install' for wrapt, since package 'wheel' is not installed.\r\nInstalling collected packages: urllib3, pyasn1, idna, chardet, certifi, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, wheel, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\r\n    Running setup.py install for wrapt ... done\r\n    Running setup.py install for termcolor ... done\r\nSuccessfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 certifi-2020.12.5 chardet-3.0.4 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.2 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1\r\n$ python --version\r\nPython 3.8.6\r\n$ python\r\nPython 3.8.6 (default, Sep 25 2020, 09:36:53) \r\n[GCC 10.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nIllegal instruction (core dumped)\r\n```\r\n\r\n**Other info / logs**\r\n", "comments": ["[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5703406/tf_env.txt)\r\n", "Here I attach the output of running an interactive python shell with `python -W all -d -v -X dev` just to import tensorflow.\r\n[py_output.txt](https://github.com/tensorflow/tensorflow/files/5705940/py_output.txt)\r\n", "@cserpell,\r\nCould you please create a new virtual environment and check if you are facing the same issue in that as well? Thanks!", "Sure, I already did it in a new virtualenv, but here it goes again in a brand new one.\r\n\r\n[new_output.txt](https://github.com/tensorflow/tensorflow/files/5709622/new_output.txt)\r\n", "I am having the same problem (also on Ubuntu 20.10, also installed through pip). Whenever I import tensorflow and python crashes and I look in journalctl I see something like this:\r\n\r\n`Dec 17 19:30:06 erebus kernel: traps: python[193520] trap invalid opcode ip:7fd16f267c48 sp:7ffd11713f50 error:0 in libtensorflow_framework.so.2[7fd16e4e0000+1c55000]`", "I also see the same issue on an older server after upgrading to 2.4.0. Crashes consistently at same place while importing tensorflow according to `dmesg`.\r\n```\r\n 8996.519179] traps: python3[2587] trap invalid opcode ip:7f19106a8c48 sp:7ffc8d486220 error:0 in libtensorflow_framework.so.2[7f190f921000+1c55000]\r\n```\r\n\r\nHere's one of the cores from `/proc/cpuinfo`, should have AVX:\r\n```\r\nprocessor       : 0                                                                                                                                                                             \r\nvendor_id       : GenuineIntel                                                                                                                                                                  \r\ncpu family      : 6                                                                                                                                                                             \r\nmodel           : 45                                                                                                                                                                            \r\nmodel name      : Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz                                                                                                                                      \r\nstepping        : 7                                                                                                                                                                             \r\nmicrocode       : 0x71a                                                                                                                                                                         \r\ncpu MHz         : 1199.954                                                                                                                                                                      \r\ncache size      : 20480 KB                                                                                                                                                                      \r\nphysical id     : 0                                                                                                                                                                             \r\nsiblings        : 16                                                                                                                                                                            \r\ncore id         : 0                                                                                                                                                                             \r\ncpu cores       : 8                                                                                                                                                                             \r\napicid          : 0                                                                                                                                                                             \r\ninitial apicid  : 0                                                                                                                                                                             \r\nfpu             : yes                                                                                                                                                                           \r\nfpu_exception   : yes                                                                                                                                                                           \r\ncpuid level     : 13                                                                                                                                                                            \r\nwp              : yes                                                                                                                                                                           \r\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon\r\n pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer \r\naes xsave avx lahf_lm epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid xsaveopt dtherm ida arat pln pts md_clear flush_l1d                                                    \r\nbugs            : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit                                                                                            \r\nbogomips        : 5199.91                                                                                                                                                                       \r\nclflush size    : 64                                                                                                                                                                            \r\ncache_alignment : 64                                                                                                                                                                            \r\naddress sizes   : 46 bits physical, 48 bits virtual                                                                                                                                             \r\npower management:                                                                                                                                                                               \r\n```\r\nThis is running Ubuntu 18.04.4 LTS.\r\n", "@necromuralist did you check AVX support on your CPU? Otherwise, please check #19584", "Same error for me in `dmesg`:\r\n```\r\ntraps: python[434569] trap invalid opcode ip:7fb785029c48 sp:7ffd15a859a0 error:0 in libtensorflow_framework.so.2[7fb7842a2000+1c55000]\r\n```", "Just to pile on, same problem here, 2.4.0 doesn't work, 2.3.0 and earlier do. CPU supports AVX:\r\n\r\n```processor\t: 31\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 45\r\nmodel name\t: Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz\r\nstepping\t: 7\r\nmicrocode\t: 0x718\r\ncpu MHz\t\t: 1197.138\r\ncache size\t: 20480 KB\r\nphysical id\t: 1\r\nsiblings\t: 16\r\ncore id\t\t: 7\r\ncpu cores\t: 8\r\napicid\t\t: 47\r\ninitial apicid\t: 47\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx lahf_lm epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\r\nbogomips\t: 4395.39\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 46 bits physical, 48 bits virtual\r\npower management:\r\n```", "Couple of things to add though:\r\n\r\n1. For me it's a problem with a debian buster/sid container, not Ubuntu (I assume this is not distro related)\r\n2. I don't see how this is in any way a build/install issue. We aren't building it and it installs fine. It isn't until use time (import) that things fail.", "@cserpell \r\n\r\nIf I do `sudo lscpu | grep avx` I get:\r\n` fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonst\r\nop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave **avx** f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit \r\nwdt fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold`\r\n\r\n(asterixes added)  I get `avx` entries when I grep `/proc/cpuinfo` as well, which I assumed meant that I have AVX support.", "I think I also have the same problem as @alex-orange. 2.4 results in illegal instruction error, whereas 2.3 works. Well, the GPU doesn't work since my CUDA version is incompatible with 2.3 but at least tensorflow works. \r\n\r\n`flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts md_clear flush_l1d\r\n`\r\n\r\n", "I just tried the nightly build of tensorflow and it doesn't crash. I used the [nvidia-container ](https://github.com/NVIDIA/nvidia-docker) to get the Nvidia toolkit instead of using Ubuntu's packange and tried to install the latest release of tensorflow via both pip and the [tensorflow-container](https://www.tensorflow.org/install/docker) and both still crashed the python interpreter, but when I did `pip install tf-nightly` it didn't crash the interpreter, but it also couldn't find the Nvidia libraries since they're in the container, but I just ran \r\n\r\n`docker run --gpus all -it tensorflow/tensorflow:nightly-gpu bash`\r\n\r\nAnd imported tensorflow into a python intepreter with no errors. Unfortunately in experimenting to get to that state I crashed my server and had to re-install Ubuntu so I don't have anything on it to test whether tensorflow actually runs, but it seems promising.", "@necromuralist Sadly I don't think that's progress. #44668 seems to suggest this problem was introduced when nightly was split off to make the release candidate (or whatever the exact process is). Also worth mentioning #45872 and #45866 seem to be the exact same bug.\r\n\r\nAdding more info: #45866 seems to suggest that this might be a problem with processors supporting AVX but not AVX2. Is it possible that this requirement got bumped (perhaps by accident)?", "I've just tried building from source with:\r\n\r\n    bazel build --config=avx_linux //tensorflow/tools/pip_package:build_pip_package\r\n\r\nand the resulting binary works fine. Is it possible someone is building the packages with --config=opt or --config=avx2_linux by accident?", "@alex-orange Is it somehow possible that you share your custom-built package or does everyone need to build for themselves at the moment?", "@moorugi98 Sure, here's my setup: https://gitlab.flux.utah.edu/alex_orange/tensorflow-2.4.0-build . That includes the Dockerfile I use for my build environment and the command history for building and testing. Not exactly perfect but you should be able to figure out how to make your own build (and the maintainers can hopefully use this to get an idea of what's different from the official build).", "Didn't say it very well, but that repo also includes the binary.", "It seems we accidentally built the official wheels using AVX2. Now we are during holiday period so not many people are present but we will revisit this situation in January and issue a patch which either will build using only AVX or will contain release notes updates that from 2.4 onwards AVX2 is required.\r\n\r\nI don't know yet which path will be taken. Nor do I know why we got to use AVX2.", "Thank you for taking the time over the holidays to look into this.\r\n\r\nUnsurprisingly I'd like to request keeping in support for the older CPU's, but it appears that I may be 1 out of only 10 or so people on the internet experiencing issues. However, I guess I'd be curious what the Tensorflow project's stance is on the CPU backend (assuming that might be what is driving AVX2).\r\nAs  a user, I've seen it as primarily the last resort or \"the thing that loads when I need TF as a dependency but not for training\". As such I appreciate small advances in speed but have always assumed it would prioritize maximum compatibility first due to its unique position. To that end, it's jarring to hear that my CPU rather than my GPU would potentially be the limiting factor; similarly odd that I would not be able to train small multi-layer dense neural nets due to my CPU being too old.\r\n\r\nThank you for your time and consideration in this matter; I'll be watching for the eventual outcome.", "Personally I also vote for avoiding requiring AVX2 -- models requiring a lot of computation should use an accelerator anyway, and I value the broad application of the PyPI tensorflow CPU backend more.", "The main issue is that TF is huge, having a lot of code and gets changes submitted from O(hundreds) people but only O(tens) people look at open source and only a team of O(1) sets up the CI infrastructure. So we'll have these accidental slips in the future too, though hopefully we can prevent them in CI.", "Any updates on that topic?", "People started coming up into the office, we're just getting started working on this. Apologies for the delay", "It seems it affects all release builds due to new CPU VMs used during release process. Trying to see if `-march=native` is the culprit.", "We found the culprit, working on a fix and then we will do a patch release on `r2.4`. Not certain on the timeline, but probably will be at least a few weeks as we're gathering other regressions on the 2.4 release.", "We should have the patched 2.4.1 release done by the end of the week", "And it has been released. Please test it to see that the issue has been fixed", "It is working now. Many thanks for your hard work, I really appreciate it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45744\">No</a>\n", "> And it has been released. Please test it to see that the issue has been fixed\r\n\r\nI'm using Tensorflow 2.4.1, Python 3.6 and I'm still getting\r\n`Illegal instruction (core dumpted)`\r\nwhen I run\r\n`import tensorflow`\r\nI'm using a Anaconda venv\r\n*edit:\r\nSame goes when I'm using Tensorflow 2.3.1", "@InnocentCow does your CPU support AVX? This issue is for AVX2.", "> > And it has been released. Please test it to see that the issue has been fixed\r\n> \r\n> I'm using Tensorflow 2.4.1, Python 3.6 and I'm still getting\r\n> `Illegal instruction (core dumpted)`\r\n> when I run\r\n> `import tensorflow`\r\n> I'm using a Anaconda venv\r\n> *edit:\r\n> Same goes when I'm using Tensorflow 2.3.1\r\n\r\nTry `import tensorflow as tf; print(tf.version.VERSION)`", "> > > And it has been released. Please test it to see that the issue has been fixed\r\n> > \r\n> > \r\n> > I'm using Tensorflow 2.4.1, Python 3.6 and I'm still getting\r\n> > `Illegal instruction (core dumpted)`\r\n> > when I run\r\n> > `import tensorflow`\r\n> > I'm using a Anaconda venv\r\n> > *edit:\r\n> > Same goes when I'm using Tensorflow 2.3.1\r\n> \r\n> Try `import tensorflow as tf; print(tf.version.VERSION)`\r\n\r\nTried this, still same error", "> @InnocentCow does your CPU support AVX? This issue is for AVX2.\r\n\r\nI have an Intel Celeron N3550, and from what I've found it does support AVX", "Nevermind, I get \r\n`Illegal instruction (core dumped)`\r\nas well when I only\r\n`import keras`\r\nSo I guess TensorFlow is not the problem here\r\n\r\nSeems like my CPU doesn't support AVX after all, I'll downgrade to TensorFlow 1.5\r\n\r\nTensorFlow 1.5 and keras 2.1.4 did the trick for me, sorry for bothering you guys", "Never mind, happy hacking!", "have same issue, installed 2.4.1 version, but it didn't help.\r\nCompiled 2.4.1 version from sources and it did the trick. ", "> have same issue, installed 2.4.1 version, but it didn't help.\r\n> Compiled 2.4.1 version from sources and it did the trick.\r\n\r\nI'm trying to install 2.4.1 onto my Jetson Nano, and I'm having the illegal instructions error, can you help me with compiling it from sources? I am a complete noob when it comes to Linux and python and stuff!", "> I'm trying to install 2.4.1 onto my Jetson Nano, and I'm having the illegal instructions error, can you help me with compiling it from sources? I am a complete noob when it comes to Linux and python and stuff!\r\n\r\nhttps://www.tensorflow.org/install/source", "CPUs with no AVX support are no longer supported by the default package. You have to compile from source or use a cloud version of TF (via Google Colab for example)."]}, {"number": 45743, "title": "Adding ROCm support for FftSupport::UpdatePlanWithScratchAllocator method", "body": "\r\nThis commit is essentially the ROCm version of the same change on the CUDA side : https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/commit/4c0adf2c26345dd63e2b883317f7efb464862532\r\n\r\n----------------------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work @hawkinsp @inailuig\r\n\r\n@hawkinsp @inailuig \r\n\r\nthis commit fixes a couple of the sub-tests within the jax unit test `tests/fft_test.py` that were failing with the following error\r\n\r\n```\r\n...\r\n----------------------------- Captured stderr call -----------------------------                                                                                                                                                                                   \r\n2020-12-15 16:08:28.770229: E external/org_tensorflow/tensorflow/stream_executor/rocm/rocm_fft.cc:510] update plan with scratch allocator not implemented                                                                                                          \r\n2020-12-15 16:08:28.771467: E external/org_tensorflow/tensorflow/stream_executor/rocm/rocm_fft.cc:510] update plan with scratch allocator not implemented                                                                                                          \r\n2020-12-15 16:08:28.898454: E external/org_tensorflow/tensorflow/stream_executor/rocm/rocm_fft.cc:510] update plan with scratch allocator not implemented                                                                                                          \r\n______________________________ tests/fft_test.py _______________________________                                                                                                                                                                                   \r\n...\r\n```", "comments": ["@cheshire @chsigg gentle ping"]}, {"number": 45742, "title": "Expose transform", "body": "See https://github.com/tensorflow/addons/pull/2293", "comments": ["Thanks for the PR! This is intended as a private utility. We should not expose it in the public API so as to keep our API surface small. Implementer-facing private APIs like this one should typically be forked in your own repo, or otherwise reimplemented.", "> Thanks for the PR! This is intended as a private utility. We should not expose it in the public API so as to keep our API surface small. Implementer-facing private APIs like this one should typically be forked in your own repo, or otherwise reimplemented.\r\n\r\nI understand the API surface point but this is really going to create an overhead for \"friends\" repositories in the SIG ecosystem. E.g. what Is the Keras CV approach? Is going to fork this or use the private API?\r\n\r\nProbably It Is better to handle private API changes and use private API than sync forks in our ecosystem. What do you think?"]}, {"number": 45741, "title": "Will tf.reshape() perform extra memcopy in tensorflow gpu backend?", "body": "```py\r\nmediate_op = tf.some_op(inputs)\r\noutput_op = tf.reshape(mediate_op, [128, -1])\r\n```\r\nIn terms of device memory occupation, is `tf.reshape` performed in place (reuse the data memory of `mediate_op`)?", "comments": ["@ghostplant \r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us i localizing the issue faster. Thanks!", "From the docs:\r\n> The `tf.reshape` does not change the order of or the total number of elements in the tensor, and so it can **reuse the underlying data buffer**. This makes it a fast operation independent of how big of a tensor it is operating on.", "Closing this issue since its resolved. Feel free to reopen if necessary. Thanks!", "Thanks! I also verified that.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45741\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45741\">No</a>\n"]}, {"number": 45740, "title": "tfLite armv7 build Issue", "body": "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make\r\n\r\nIs there any option to build for generic armv7 targets other than raspberry pi.\r\nIs this restricted for just raspberry pi targets ? Will the same makefile work for other armv7 targets other than raspi targets.", "comments": ["I don't know what the generic armv7 targets means.\r\nTo me, armv7 is very fragmented unlike aarch64 target.\r\nI'm seeing lots of armv7 compatibility issue since their armv7 capability are different each other.\r\n\r\nRPI is target for Armv7 which is Cortex-A hard-float little-endian neon-enabled which is compatibility with armhf of Debian.\r\nhttps://wiki.debian.org/ArmHardFloatPort\r\nIt's runnable on any ARMv7 target of Cortext-A hard-float little-endian neon-enabled machines.\r\n\r\nYou might be able to build other ARMv7 variant but it's your own responsibility.\r\n\r\nFYI, this is ABI of the generated ARM binary.\r\n```\r\n$ readelf -A benchmark_model\r\nAttribute Section: aeabi\r\nFile Attributes\r\n  Tag_CPU_name: \"7-A\"\r\n  Tag_CPU_arch: v7\r\n  Tag_CPU_arch_profile: Application\r\n  Tag_ARM_ISA_use: Yes\r\n  Tag_THUMB_ISA_use: Thumb-2\r\n  Tag_FP_arch: VFPv3\r\n  Tag_Advanced_SIMD_arch: NEONv1\r\n  Tag_ABI_PCS_wchar_t: 4\r\n  Tag_ABI_FP_rounding: Needed\r\n  Tag_ABI_FP_denormal: Needed\r\n  Tag_ABI_FP_exceptions: Needed\r\n  Tag_ABI_FP_number_model: IEEE 754\r\n  Tag_ABI_align_needed: 8-byte\r\n  Tag_ABI_enum_size: int\r\n  Tag_ABI_VFP_args: VFP registers\r\n  Tag_CPU_unaligned_access: v6\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45740\">No</a>\n"]}, {"number": 45739, "title": "GET returned 404 Not Found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.9.2009 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0 (release downloaded from github repository)\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: No all dependencies (GCC 6.3.0, Python 3.8.5, Bazel 3.7.1 and  NCCL 2.7.8-1) are installed from source and CUDA 11.0.3 and cuDNN 8.0.5 are manually installed using the installers provided by Nvidia\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 6.3.0\r\n- CUDA/cuDNN version: CUDA 11.0.3, cuDNN 8.0.5\r\n- GPU model and memory: Nvidia Tesla V100 (SXM2), 32 GB GPU memory\r\n\r\n**Describe the problem**\r\n\r\nSome of the sources that need to be downloaded for building tensorflow are not available:\r\n\r\n```\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n```\r\n\r\nWhen compiling tensorflow 2.3.0 (with GPU support for CUDA compute capabilities 6.1,7.0,7.5), then there were no issues like this and the total amount of build targets was around 70'000. Now with 2.4.0, some sources seem to be missing and there are only around 30'000 build targets, which seems a bit odd as I set optimization for more compute capabilities than for compinling 2.3.0  (we will soon get some nodes with A100 GPUs, therefore I want to optimize tensorflow already for compute capability 8.0).\r\n\r\nEven with those warnings tensorflow builds fine (in the log I attached below, I canceled the build process after seeing the warnings again and noticing again that there are only 30k build targets), but I wouldn't feel comfortable providing the 2.4.0 version to our cluster users before it is not clear if anything went wrong.\r\n\r\nThere are no network issues on my side. I don't experience any issue downloading any other software or source code.\r\n\r\nAny help is appreciated.\r\n\r\nBest regards\r\n\r\nSamuel Fux\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n\r\n[sfux@eu-a65-001 156113172.tmpdir]$ ls\r\ncache  temp  tensorflow-2.4.0\r\n[sfux@eu-a65-001 156113172.tmpdir]$ cd tensorflow-2.4.0/\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ module list\r\n\r\nCurrently Loaded Modules:\r\n  1) StdEnv   2) eth_proxy   3) gcc/6.3.0   4) jdk/8u141-b15   5) bazel/3.7.1   6) openblas/0.2.20   7) python/3.8.5   8) cuda/11.0.3   9) cudnn/8.0.5  10) nccl/2.7.8-1\r\n\r\n\r\n\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ pwd\r\n/scratch/156113172.tmpdir/tensorflow-2.4.0\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ ./configure\r\nYou have bazel 3.7.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python3]: /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python\r\n\r\n\r\nFound possible Python library paths:\r\n  /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\n        'local/cuda/extras/CUPTI/include'\r\nof:\r\n        '/lib64'\r\n        '/opt/ibutils/lib64'\r\n        '/opt/mellanox/hcoll/lib'\r\n        '/opt/mellanox/mxm/lib'\r\n        '/opt/mellanox/sharp/lib'\r\n        '/usr'\r\n        '/usr/lib64//bind9-export'\r\n        '/usr/lib64/mysql'\r\n        '/usr/lib64/qt-3.3/lib'\r\n        '/usr/lib64/xulrunner'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8.0\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 2.7\r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus,/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5,/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g\r\n\r\n\r\nFound CUDA 11.0 in:\r\n    /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/targets/x86_64-linux/lib\r\n    /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/lib64\r\n    /cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/include\r\nFound NCCL 2 in:\r\n    /cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/lib\r\n    /cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0]: 6.1,7.0,7.5,8.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/bin/gcc]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -ftree-vectorize -march=core-avx2 -mavx2 -Wno-sign-compare\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ bazel --output_base=$TMPDIR/cache build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=213\r\nINFO: Reading rc options for 'build' from /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /scratch/156113172.tmpdir/tensorflow-2.4.0/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python --action_env PYTHON_LIB_PATH=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages --python_path=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python --config=xla --action_env TF_CUDA_VERSION=11.0 --action_env TF_CUDNN_VERSION=8.0 --action_env TF_NCCL_VERSION=2.7 --action_env TF_CUDA_PATHS=/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus,/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5,/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g --action_env CUDA_TOOLKIT_PATH=/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5,8.0 --action_env LD_LIBRARY_PATH=/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/lib:/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/lib64:/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/lib64:/cluster/apps/gcc-6.3.0/openblas-0.2.20-cot3cawsqf4pkxjwzjexaykbwn2ch3ii/lib:/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64:/cluster/apps/gcc-6.3.0/jdk-8u141-b15-e47jxtcwbf7n6umt34mef2auq66ktj4k/lib:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib --action_env GCC_HOST_COMPILER_PATH=/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.tf_configure.bazelrc: --copt=-ftree-vectorize --copt=-march=core-avx2 --copt=-mavx2 --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /scratch/156113172.tmpdir/tensorflow-2.4.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  /scratch/156113172.tmpdir/tensorflow-2.4.0/WORKSPACE:37:30: in <toplevel>\r\n  /scratch/156113172.tmpdir/cache/external/bazel_toolchains/repositories/repositories.bzl:55:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /scratch/156113172.tmpdir/cache/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nDEBUG: /scratch/156113172.tmpdir/cache/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /scratch/156113172.tmpdir/tensorflow-2.4.0/WORKSPACE:37:30: in <toplevel>\r\n  /scratch/156113172.tmpdir/cache/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /scratch/156113172.tmpdir/cache/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (415 packages loaded, 32798 targets configured).\r\nINFO: Found 1 target...\r\n[6,201 / 33,224] 96 actions running\r\n```\r\n\r\n", "comments": ["I tried now to compile tensorflow 2.3.1, but there I get similar download issues.", "@samfux84,\r\n> Even with those warnings tensorflow builds fine\r\n\r\nYes that seems to be the case here, the build would complete as the compilation process continues from the other mirror.\r\n\r\nFor more information, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/42690#issuecomment-715661759) from a member of the TensorFlow team. Thanks!", "@amahendrakar Thank you for your reply. I am happy to read that the warning is just a warning and tensorflow downloads the sources from another mirror. What still puzzles me is the number of build targets. For my last 2.3.0 build with GPU support, there were like 70k build targets and now for 2.4.0 there are only around 30k build targets. Can this be correct or am I doing a mistake somewhere? I just started the build again and will post the complete logs, once it has finished.", "Full logs of successful build (with only 33k targets):\r\n\r\nDoes this build look fine?\r\n\r\n```\r\n[sfux@eu-a65-001 156408065.tmpdir]$ module list\r\nCurrently Loaded Modulefiles:\r\n  1) modules\r\n[sfux@eu-a65-001 156408065.tmpdir]$ env2lmod\r\n[sfux@eu-a65-001 156408065.tmpdir]$ module load eth_proxy gcc/6.3.0 bazel/3.7.1 python/3.8.5 cuda/11.0.3 cudnn/8.0.5 nccl/2.7.8-1\r\n\r\nThe following have been reloaded with a version change:\r\n  1) gcc/4.8.5 => gcc/6.3.0\r\n\r\n[sfux@eu-a65-001 156408065.tmpdir]$ export TEST_TMPDIR=$TMPDIR/cache\r\n[sfux@eu-a65-001 156408065.tmpdir]$ ls\r\ncache  temp  tensorflow-2.4.0\r\n[sfux@eu-a65-001 156408065.tmpdir]$ cd tensorflow-2.4.0/\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ ls\r\nACKNOWLEDGMENTS     AUTHORS  CODE_OF_CONDUCT.md  configure      configure.py     ISSUES.md          LICENSE       README.md   SECURITY.md  third_party  WORKSPACE\r\narm_compiler.BUILD  BUILD    CODEOWNERS          configure.cmd  CONTRIBUTING.md  ISSUE_TEMPLATE.md  models.BUILD  RELEASE.md  tensorflow   tools\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ ./configure\r\nYou have bazel 3.7.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python3]: /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python\r\n\r\n\r\nFound possible Python library paths:\r\n  /cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\n        'local/cuda/extras/CUPTI/include'\r\nof:\r\n        '/lib64'\r\n        '/opt/ibutils/lib64'\r\n        '/opt/mellanox/hcoll/lib'\r\n        '/opt/mellanox/mxm/lib'\r\n        '/opt/mellanox/sharp/lib'\r\n        '/usr'\r\n        '/usr/lib64//bind9-export'\r\n        '/usr/lib64/mysql'\r\n        '/usr/lib64/qt-3.3/lib'\r\n        '/usr/lib64/xulrunner'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8.0\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 2.7\r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus,/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5,/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g\r\n\r\n\r\nFound CUDA 11.0 in:\r\n    /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/targets/x86_64-linux/lib\r\n    /cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/lib64\r\n    /cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/include\r\nFound NCCL 2 in:\r\n    /cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/lib\r\n    /cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0]: 6.1,7.0,7.5,8.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/bin/gcc]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -ftree-vectorize -march=core-avx2 -mavx2 -Wno-sign-compare\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ bazel --output_base=$TMPDIR/cache build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n$TEST_TMPDIR defined: output root default is '/scratch/156408065.tmpdir/cache' and max_idle_secs default is '15'.\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=213\r\nINFO: Reading rc options for 'build' from /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /scratch/156408065.tmpdir/tensorflow-2.4.0/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python --action_env PYTHON_LIB_PATH=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64/python3.8/site-packages --python_path=/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/bin/python --config=xla --action_env TF_CUDA_VERSION=11.0 --action_env TF_CUDNN_VERSION=8.0 --action_env TF_NCCL_VERSION=2.7 --action_env TF_CUDA_PATHS=/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus,/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5,/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g --action_env CUDA_TOOLKIT_PATH=/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5,8.0 --action_env LD_LIBRARY_PATH=/cluster/apps/gcc-6.3.0/nccl-2.7.8-1-fyh5jnhi5a2g362ez65bl3lxr3fzsx6g/lib:/cluster/apps/gcc-6.3.0/cudnn-8.0.5-dgehjjr7e56oni2klhkrb7xuixyecbo5/lib64:/cluster/apps/gcc-6.3.0/cuda-11.0.3-qdlibd2luz2fy7izfefao4c5yitxwjus/lib64:/cluster/apps/gcc-6.3.0/openblas-0.2.20-cot3cawsqf4pkxjwzjexaykbwn2ch3ii/lib:/cluster/apps/nss/gcc-6.3.0/python/3.8.5/x86_64/lib64:/cluster/apps/gcc-6.3.0/jdk-8u141-b15-e47jxtcwbf7n6umt34mef2auq66ktj4k/lib:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib --action_env GCC_HOST_COMPILER_PATH=/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-6.3.0-sqhtfh32p5gerbkvi5hih7cfvcpmewvj/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.tf_configure.bazelrc: --copt=-ftree-vectorize --copt=-march=core-avx2 --copt=-mavx2 --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /scratch/156408065.tmpdir/tensorflow-2.4.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  /scratch/156408065.tmpdir/tensorflow-2.4.0/WORKSPACE:37:30: in <toplevel>\r\n  /scratch/156408065.tmpdir/cache/external/bazel_toolchains/repositories/repositories.bzl:55:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /scratch/156408065.tmpdir/cache/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nDEBUG: /scratch/156408065.tmpdir/cache/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /scratch/156408065.tmpdir/tensorflow-2.4.0/WORKSPACE:37:30: in <toplevel>\r\n  /scratch/156408065.tmpdir/cache/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /scratch/156408065.tmpdir/cache/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (415 packages loaded, 32798 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 2234.205s, Critical Path: 987.31s\r\nINFO: 33335 processes: 8991 internal, 24344 local.\r\nINFO: Build completed successfully, 33335 total actions\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$\r\n```", "When building the wheel afterwards, I again get many warnings, this time about missing files. Is this normal?:\r\n\r\n```\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package $TMPDIR/package/tensorflow_pkg\r\nThu Dec 17 08:45:01 CET 2020 : === Preparing sources in dir: /scratch/156408065.tmpdir/tmp.LMudJHLg3n\r\n/scratch/156408065.tmpdir/tensorflow-2.4.0 /scratch/156408065.tmpdir/tensorflow-2.4.0\r\n/scratch/156408065.tmpdir/tensorflow-2.4.0\r\n/scratch/156408065.tmpdir/tensorflow-2.4.0/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow /scratch/156408065.tmpdir/tensorflow-2.4.0\r\n/scratch/156408065.tmpdir/tensorflow-2.4.0\r\n/scratch/156408065.tmpdir/tmp.LMudJHLg3n/tensorflow/include /scratch/156408065.tmpdir/tensorflow-2.4.0\r\n/scratch/156408065.tmpdir/tensorflow-2.4.0\r\nThu Dec 17 08:45:29 CET 2020 : === Building wheel\r\nwarning: no files found matching 'README'\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pyi' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dylib' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nThu Dec 17 08:46:10 CET 2020 : === Output wheel file is in: /scratch/156408065.tmpdir/package/tensorflow_pkg\r\n[sfux@eu-a65-001 tensorflow-2.4.0]$\r\n```\r\n", "This should be normal.", "For the difference in targets, are you running `bazel clean --expunge` in between the builds? Otherwise, bazel might just ignore a few targets that were already built", "Thank you for your replies.\r\n\r\nIf I redo a build, I remove all directories and start a new interactive job in order to get a clean shell and environment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45739\">No</a>\n"]}, {"number": 45738, "title": "Why is the object_C library empty", "body": "\r\n", "comments": ["@KeepSmileEveryDay \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nCan you please elaborate about the issue & the context.Will it be possible to provide related code.Thanks!", "@ravikyram \r\nThis is my procedure, thank you!\r\n<img width=\"558\" alt=\"WechatIMG53\" src=\"https://user-images.githubusercontent.com/14270188/102335634-22d18700-3fcb-11eb-90e7-f3904dd8e2f5.png\">\r\n![WechatIMG54](https://user-images.githubusercontent.com/14270188/102335646-26650e00-3fcb-11eb-85d4-ae2b98b66bf9.jpeg)\r\n", "I saw that the Demo was written in SWIFT language. If I wanted to write in Object-C language, the above error would appear through normal Cocoapods mode", "@KeepSmileEveryDay \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "First, I searched TensorFlow through Pod Search and got the result of the first picture above. Later, I searched pod 'TensorFlow 'in podfile. Pod Install then gets the result of the second image above.", "TF 0.0.4 is obsolete, Please try to switch to recent TF version 2.3 or 2.4", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45737, "title": "@Tuxius and others which stuck until this bug is fixed:", "body": "@Tuxius and others which stuck until this bug is fixed: \r\nI had the same issue  and I found that my validation data set had less samples as the batch_size.  Because I'm working with TFRecords-data-sets which have no meta data about how many records, i.e. samples, are in the data set I check now upfront whether the data set  contains at least batch_size records (samples). \r\n\r\nFor that I use the following helper functions:\r\n\r\n```\r\ndef doesDataSetContainsEnoughDataForBatch(dataset, batch_size):\r\n    return len(list(dataset.take(batch_size).as_numpy_iterator())) == batch_size\r\n  \r\ndef doesDataSetFileContainsEnoughDataForBatch(sampleFileName=\"\", batch_size=100):\r\n    dataset = tf.data.TFRecordDataset(sampleFileName)\r\n    return doesDataSetContainsEnoughDataForBatch(dataset, batch_size=batch_size)\r\n\r\nif __name__ == '__main__':\r\n  dataSetFileName=\"./Samples/validation_data_123.tfrecord\"\r\n  if not doesDataSetFileContainsEnoughDataForBatch(dataSetFileName,batch_size=100):\r\n    raise Exception(f\"Data set file {dataSetFileName} doesn't contain enough data\")\r\n   \r\n  # Now open the data set a second time knowing you have enough data \r\n  # and use it ....\r\n  \"\"\"\r\n  trainDataset = tf.data.TFRecordDataset(dataSetFileName)\r\n  \r\n  model.fit ( trainDataset ... \r\n  \r\n  \"\"\"\r\n\r\n```\r\n\r\nPlease keep in mind that by using the first helper function directly , \r\ni.e. doesDataSetContainsEnoughDataForBatch,  you already read batch_size samples from the data set. So you should recreate the data set after the check.\r\n\r\nBy using the second helper function  you just lose some execution time upfront.\r\n\r\nIf you don't use TFRecord data sets you might have a similar  issue. But then it is also a good idea to check upfront whether you have enough data for at least one batch.\r\n\r\n_Originally posted by @PeterBrummer in https://github.com/tensorflow/tensorflow/issues/38064#issuecomment-614800998_", "comments": []}, {"number": 45736, "title": "model.predict(x,batch_size=500)  for around half an hour report WebSocketClosedError()", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.3.0\r\n- Python version:3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11\r\n- GPU model and memory: Tesla V100 \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nexecute model.predict as below for around half an hour report  StreamClosedError() and WebSocketClosedError()\r\n```python \r\nmodel.predict(x,batch_size=500) \r\n```\r\n \r\n\r\nbut when execute smaller part of the same data for prediction, no errors occur.\r\n\r\nby the way, as a beginner I wonder if the total number of samples for prediction needs to be integer multiple  of batch_size? what if that cannot be?\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1104, in wrapper\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-13497' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1102, in wrapper\r\n    await fut\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1104, in wrapper\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-13498' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1102, in wrapper\r\n    await fut\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1104, in wrapper\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-13499' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1102, in wrapper\r\n    await fut\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1104, in wrapper\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-13500' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1102, in wrapper\r\n    await fut\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1104, in wrapper\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-13501' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /opt/conda/lib/python3.8/site-packages/tornado/websocket.py:1100> exception=WebSocketClosedError()>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.8/site-packages/tornado/websocket.py\", line 1102, in wrapper\r\n    await fut\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\n\r\n```", "comments": ["Please provide a reproducible code example so that we can understand the issue better. Also note that your error is not raised by `tensorflow`.", "@learnpythontheew,\r\nAs @aaronmondal mentioned, could you please provide a minimal code snippet to reproduce the issue.\r\n\r\nLooking at the error log, it seems like the issue is caused due to the `tornado` library. You can verify this by running the `model.predict` function without the websocket/tornado implementation. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45736\">No</a>\n"]}, {"number": 45735, "title": "No tensorflow lite 2.4.0 pre-built libraries", "body": "**System information**\r\n- OS Platform and Distribution: Android\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.4.0\r\n\r\n**Describe the problem**\r\nCan not find pre-built libraries for tensorflow lite 2.4.0 at the below location. Latest available version is older 2.3.0\r\n\r\nhttps://bintray.com/google/tensorflow/tensorflow-lite", "comments": ["They should exist now:\r\n\r\nhttps://bintray.com/google/tensorflow/tensorflow-lite/2.4.0", "Note that, while the libraries have been uploaded, we're experiencing some issues syncing these libraries with JCenter. Hoping to resolve shortly.", "Yes. Works now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45735\">No</a>\n"]}, {"number": 45734, "title": "changed a sentence in readme to make it more easy to understand", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45734) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thank you, but importing this change has a cost and I don't think necessary here."]}, {"number": 45733, "title": "[Build] [ROCm] [Bazel] /opt/rocm/.info/version-dev file not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.9.1\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: N/A\r\n- ROCm version: 3.10.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building I am getting the following error:\r\n```\r\nINFO: Repository local_config_rocm instantiated at:\r\n  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/WORKSPACE:19:16: in <toplevel>\r\n  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/tensorflow/workspace.bzl:102:19: in tf_repositories\r\nRepository rule rocm_configure defined at:\r\n  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl:794:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_rocm':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl\", line 775, column 38, in _rocm_autoconf_impl\r\n\t\t_create_local_rocm_repository(repository_ctx)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl\", line 534, column 35, in _create_local_rocm_repository\r\n\t\trocm_config = _get_rocm_config(repository_ctx, bash_bin, find_rocm_config_script)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl\", line 388, column 30, in _get_rocm_config\r\n\t\tconfig = find_rocm_config(repository_ctx, find_rocm_config_script)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl\", line 366, column 41, in find_rocm_config\r\n\t\texec_result = _exec_find_rocm_config(repository_ctx, script_path)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/gpus/rocm_configure.bzl\", line 362, column 19, in _exec_find_rocm_config\r\n\t\treturn execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.4.0-rocm/third_party/remote_config/common.bzl\", line 217, column 13, in execute\r\n\t\tfail(\r\nError in fail: Repository command failed\r\nERROR: ROCm version file \"/opt/rocm/.info/version-dev\" not found\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_rocm//rocm': Repository command failed\r\nERROR: ROCm version file \"/opt/rocm/.info/version-dev\" not found\r\nERROR: no such package '@local_config_rocm//rocm': Repository command failed\r\nERROR: ROCm version file \"/opt/rocm/.info/version-dev\" not found\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. git clone\r\n2. `export TF_NEED_ROCM=1`\r\n3. `./configure`\r\n4. \r\n```\r\n  bazel \\\r\n        build \\\r\n          //tensorflow:libtensorflow.so \\\r\n          //tensorflow:libtensorflow_cc.so \\\r\n          //tensorflow:install_headers \\\r\n          //tensorflow/tools/pip_package:build_pip_package\r\n      bazel-bin/tensorflow/tools/pip_package/build_pip_package --gpu \"${srcdir}\"/tmpoptrocm\r\n```\r\n\r\nTo be exactly precise I am using the following build script (PKGBUILD):\r\nhttps://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=tensorflow-rocm\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDownstream issue: https://github.com/rocm-arch/tensorflow-rocm/issues/17\r\n\r\nIt could be that rocm 3.10 doesn't ship with `/opt/rocm/.info/version-dev` or that the Arch Linux package doesn't not ship with `/opt/rocm/.info/version-dev` files.\r\n\r\nCould someone check if `/opt/rocm/.info/version-dev` exists in a ubuntu installation of rocm-3.10 and if so which package it is installed under?\r\nIf not then we should remove this check in our bazel code and figure out someother way of obtaining the rocm version: https://github.com/tensorflow/tensorflow/blob/9e7a15398b6e16fdf9b684aaa1437d1d3c76a5ab/third_party/gpus/find_rocm_config.py#L72-L83", "comments": ["@acxz,\r\nTensorFlow v2.4 is compatible with Python 3.6 - 3.8. Please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#linux) for more information.\r\n\r\nCould you please check if you are facing the same issue with Python 3.8 as well? Thanks!", "Based on the error log, the issue does not have to do with the Python versions.\r\nI checked on Python 3.8 and am still getting the same issue.", "The version-dev file seems to be from the rocm-dev package, checked in CentOS 8:\r\n[version-dev.txt](https://github.com/tensorflow/tensorflow/files/5725044/version-dev.txt)\r\n\r\nRPMs are here:\r\nhttps://repo.radeon.com/rocm/centos8/4.0/\r\n\r\nSeems to be just a file with the version number and package version e.g. \"4.0.0-23\".\r\nSame with ```version-libs``` and ```version-utils``` in the same ```.info``` directory.\r\nGuess it's a good idea to add them to rocm-arch as well (in the rocm-libs and rocm-utils metapackages).", "ahh nice find!", "resolved with above pr", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45733\">No</a>\n"]}, {"number": 45732, "title": "Update tflite-converter-issue template", "body": "Updating TF Lite Converter Issue template to simplify issues debugging.", "comments": ["@ymodak Can you please resolve conflicts? Thanks!", "Already merged the PR internally."]}, {"number": 45731, "title": "ambiguity risk of crop_size", "body": "https://github.com/tensorflow/tensorflow/blob/14793703c9fde08b643892b1f9d2504488f2d5f4/tensorflow/python/ops/image_ops_impl.py#L4698\r\n\r\nBased on the introduction of this operator, crop_height, crop_width are supposed to stand for the size of output, which literally the middle step size of this op", "comments": ["I agree. This should probably be renamed to `output_size` or `out_size`, and the documentation updated accordingly. Would you mind filing a PR?", "@qazsweet,\r\nAs @aaronmondal suggested, could you please submit a PR for this change? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]