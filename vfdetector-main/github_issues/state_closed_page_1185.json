[{"number": 17637, "title": "make benchmark_model for TFLite build", "body": "It doesn't build for Android and Ubutu box. Make it build.", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "build failed!\r\n\r\nERROR: /home/apuser/deeplearning/tensorflow/tensorflow-master/tensorflow/core/BUILD:1243:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib_lite' failed (Exit 1)\r\ntensorflow/core/util/port.cc:19:10: fatal error: 'cuda/include/cuda.h' file not found\r\n         ^\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite/tools:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 97.980s, Critical Path: 6.79s\r\nFAILED: Build did NOT complete successfully\r\n", "@WenguoLi I think a quick solution is disabling CUDA. The benchmark_model uses some non-Lite functions. But I don't think CUDA will help anything.", "hi, @freedomtan ,\r\nThanks!\r\nHow to be disable CUDA ? \r\n", "re-run `./configure` or edit `.tf_configure.bazelrc`", "Nagging Reviewer @aselle: It has been 22 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "benchmark_model was updated and fixed. close this."]}, {"number": 17636, "title": "Fix broken link in programmers_guide/faq and some minor format", "body": "As we can see in [programmer_guide/faq](https://www.tensorflow.org/programmers_guide/faq), there are two broken links: \r\nThe first one:\r\n> See the how-to documentation for @{$reading_data#creating-threads-to-prefetch-using-queuerunner-objects$using QueueRunner objects to drive queues and readers} for more information on how to use them.\r\n\r\nThe seconde one:\r\n\r\n> If your data is not easily parsable with the built-in TensorFlow operations, consider converting it, offline, to a format that is easily parsable, such as ${tf.python_io.TFRecordWriter$TFRecord} format.\r\n\r\nThis PR is to fix the first broken link thru replace '-' with '_' and fix the second broken link thru replacing the begining '$' with '@'.\r\n\r\nBesides, I think there might be some format issue on the '*' which I assume they should be used to highlight the \"In particular\" and \"With one exception\" sentence. \r\n\r\n> The TensorFlow Python API adheres to the PEP8 conventions.* In particular, we use CamelCase names for classes, and snake_case names for functions, methods, and properties. We also adhere to the Google Python style guide.\r\n> The TensorFlow C++ code base adheres to the Google C++ style guide.\r\n> (* With one exception: we use 2-space indentation instead of 4-space indentation.)\r\n", "comments": []}, {"number": 17635, "title": "tensorflow lite build error ", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu16.04\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version (use command below): last master\r\nPython version: Python 2.7\r\nBazel version (if compiling from source): bazel 0.11.1\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: No GPU model\r\n**Describe the problem**\r\nI try to build tensorflow lite demo.\r\nHere is my WORKSPACE:\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 24,\r\n    # Ensure that you have the build_tools_version below installed in the\r\n    # SDK manager as it updates periodically.\r\n    build_tools_version = \"26.0.2\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/home/lucas/Android/Sdk\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/home/lucas/android/android-ndk-r14b\",\r\n    # This needs to be 14 or higher to compile TensorFlow.\r\n    # Please specify API level to >= 21 to build for 64-bit\r\n    # archtectures or the Android NDK will automatically select biggest\r\n    # API level that it supports without notice.\r\n    # Note that the NDK version is not the API level.\r\n    api_level=14)\r\n Build the demo app:\r\n bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo\r\n\r\nlucas@lucas:~/github/tensorflow$ bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo\r\nWARNING: /home/lucas/.cache/bazel/_bazel_lucas/41d00bd91f3cd94eb33af93ba59b3c39/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/lucas/.cache/bazel/_bazel_lucas/41d00bd91f3cd94eb33af93ba59b3c39/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nINFO: Analysed target //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/lucas/github/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:268:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:portable_tensor_utils' failed (Exit 1)\r\nsrc/main/tools/process-wrapper-legacy.cc:58: \"execvp(external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang, ...)\": No such file or directory\r\nTarget //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.644s, Critical Path: 0.04s\r\nFAILED: Build did NOT complete successfully\r\n\r\nHow to fix this error?\r\nthx.\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I think this has to do with a change in bazel. Could you try again?", "I am facing the problem with Bazel 0.12.0.\r\nAny idea which version works?\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I faced the same issue on Ubuntu 18.04, the reason was ndk14 does not have llvm folder some how, once I manually put the llvm from a different ndk version this issue was resolved."]}, {"number": 17634, "title": "error importing tensorflow after install", "body": "**System information**\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 64bit service pack 1\r\n**TensorFlow installed from (source or binary)**:source\r\n**TensorFlow version (use command below)**: 1.6\r\n**Python version**: Python 3.5.2\r\n**Bazel version (if compiling from source)**: N/A\r\n**GCC/Compiler version (if compiling from source)**: N/A\r\n**CUDA/cuDNN version**: N/A\r\n**GPU model and memory**: No GPU model\r\n**Exact command to reproduce**:import tensor flow\r\n\r\n**Describe the problem**\r\nI'm getting the following error when importing tensorflow after following the pip install directions here [](https://www.tensorflow.org/install/install_windows) . When I try to verify the installation, I receive the following error when I try to import tensorflow. Thanks for the help.\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n>     return importlib.import_module(mname)\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n> ImportError: DLL load failed with error code -1073741795\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n>     return importlib.import_module('_pywrap_tensorflow_internal')\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n> ImportError: No module named '_pywrap_tensorflow_internal'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<pyshell#0>\", line 1, in <module>\r\n>     import tensorflow\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n>     return importlib.import_module(mname)\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n> ImportError: DLL load failed with error code -1073741795\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n>     return importlib.import_module('_pywrap_tensorflow_internal')\r\n>   File \"C:\\Users\\Nhan\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n> ImportError: No module named '_pywrap_tensorflow_internal'\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n\r\nThanks for the help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Please tell us your compiler version since you built from source. I suspect that it's some mismatch there or a permissions issue. Sadly the DLL error doesn't tell us anything useful. \r\n\r\nCan you install and import the regular binary?", "When I type \r\n`print(sys.version)\r\n\r\nI get the following information \r\n\r\n3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)]\r\n\r\nWhat do you mean by the regular binary? \r\n", "You indicated you installed TF from source. Please let us know which compiler you used, and the configuration (your bazel.rc).\r\n\r\nIdeally, could you try to not install from source just to ensure that whatever problem this is is caused by the build?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is currently not an issue as I've been away from my machine and\nworking on other projects.\n-Nhan-\n\nOn Mon, May 28, 2018 at 12:38 PM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 44 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17634#issuecomment-392584015>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AjjPR39QWNAWWjidZupgZxVsn8Zt7GJnks5t3EQLgaJpZM4Sl5UZ>\n> .\n>\n\n\n\n-- \n-Nhan-\n"]}, {"number": 17633, "title": "Keras plot_model() giving error 'Model' object has no attribute '_container_nodes'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     TensorFlow Keras code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Windows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n     Binary\r\n- **TensorFlow version (use command below)**:\r\n     1.6, Keras version: 2.1.3-tf\r\n- **Python version**: \r\n     3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n     9/7\r\n- **GPU model and memory**:\r\n       GeForce 860M\r\n- **Exact command to reproduce**:\r\n     plot_model(model1, to_file='model1.png')\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n I am getting the following error with the  Keras plot_model()  command. \r\n'Model' object has no attribute '_container_nodes'\r\nThis error was first reported in #14542 and fixed in #14553. But it seems to have not carried over.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\ninput_X1 = Input(shape=(32,))\r\ny1 = Dense(units=64, activation='relu')(input_X1)\r\ny1 = Dense(units=32, activation='relu')(y1)\r\nprediction1 = Dense(units=1, activation='relu',name='prediction_2')(y1) #\r\nmodel1 = Model(inputs=input_X1,outputs=prediction1,name='model1')\r\nplot_model(model1, to_file='mobilenet1.png')\r\n```", "comments": ["Just checking, this is keras from tensorflow, right?", "Yes, I am using the Keras module within TensorFlow.", "It appears that this issue was re-introduced in [commit](https://github.com/tensorflow/tensorflow/commit/0bd0bf02aa15a3238b77053a2f0ad6fe373c7d1c#diff-2391fd575f61ea7e3cf589012305ad36)\r\n", "Could you contribute a fix?", "On my local machine, the error goes away and plot_model() works as expected if I replace \r\n`if node_key in model.container_nodes:`\r\nwith\r\n`if node_key in model._network_nodes:  # pylint: disable=protected-access`\r\nin vis_utils.py.\r\nI would love to help but I haven't done any pull requests before.", "Great!", "It is useful to me. That's great, thank you."]}, {"number": 17632, "title": "got Nan when powered float32 tensors", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **Windows 10 Pro**\r\n- **TensorFlow installed from (prebuild version here https://github.com/fo40225/tensorflow-windows-wheel  with avx2)**\r\n- **TensorFlow version 1.6.0**\r\n- **Python version 3.6.4**:\r\n- **Bazel version (if compiling from source)**\r\n- **GCC/Compiler version (if compiling from source)**\r\n- **CUDA 9.1.85/cuDNN 1.7.1 version**\r\n- **GPU Nvidia MX150 2GB**\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nOn my setup tensorflow get nan when I powered tensors with float32 type, but with float64 it's ok.\r\n\r\nBut if just product tensors by itself in float32 it's ok too.\r\n\r\n### Source code / logs\r\n\r\n## Scenario 1\r\n```\r\nX = tf.placeholder('float32')\r\n\r\nx = np.linspace(-3, 3)\r\n\r\ns = tf.Session()\r\n\r\nX_pow2 = tf.pow(X, 2)\r\nX_pow3 = tf.pow(X, 3)\r\nX_prod = X*X\r\n\r\nprint('X_pow2 =', X_pow2.eval({X:x}, session=s), '\\n')\r\nprint('X_pow3 =', X_pow3.eval({X:x},session=s),'\\n')\r\nprint('X_prod =', X_prod.eval({X:x},session=s),'\\n')\r\n```\r\n\r\nI get this \r\n\r\n```\r\nX_pow2 = [          nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n 3.7484397e-03 3.3735953e-02 9.3710966e-02 1.8367349e-01 3.0362350e-01\r\n 4.5356098e-01 6.3348603e-01 8.4339863e-01 1.0832988e+00 1.3531864e+00\r\n 1.6530614e+00 1.9829239e+00 2.3427739e+00 2.7326119e+00 3.1524367e+00\r\n 3.6022491e+00 4.0820494e+00 4.5918374e+00 5.1316123e+00 5.7013750e+00\r\n 6.3011241e+00 6.9308624e+00 7.5905871e+00 8.2803001e+00 9.0000000e+00] \r\n\r\nX_pow3 = [          nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n           nan           nan           nan           nan           nan\r\n 2.2949636e-04 6.1964002e-03 2.8687032e-02 7.8717217e-02 1.6730276e-01\r\n 3.0545944e-01 5.0420320e-01 7.7454978e-01 1.1275151e+00 1.5741148e+00\r\n 2.1253648e+00 2.7922804e+00 3.5858786e+00 4.5171747e+00 5.5971832e+00\r\n 6.8369217e+00 8.2474060e+00 9.8396511e+00 1.1624674e+01 1.3613489e+01\r\n 1.5817106e+01 1.8246557e+01 2.0912844e+01 2.3826992e+01 2.7000002e+01] \r\n\r\nX_prod = [9.0000000e+00 8.2803001e+00 7.5905881e+00 6.9308619e+00 6.3011246e+00\r\n 5.7013745e+00 5.1316123e+00 4.5918365e+00 4.0820489e+00 3.6022491e+00\r\n 3.1524365e+00 2.7326117e+00 2.3427739e+00 1.9829239e+00 1.6530612e+00\r\n 1.3531862e+00 1.0832986e+00 8.4339857e-01 6.3348603e-01 4.5356098e-01\r\n 3.0362347e-01 1.8367347e-01 9.3710959e-02 3.3735946e-02 3.7484383e-03\r\n 3.7484383e-03 3.3735946e-02 9.3710959e-02 1.8367347e-01 3.0362347e-01\r\n 4.5356098e-01 6.3348603e-01 8.4339857e-01 1.0832986e+00 1.3531862e+00\r\n 1.6530612e+00 1.9829239e+00 2.3427739e+00 2.7326117e+00 3.1524365e+00\r\n 3.6022491e+00 4.0820489e+00 4.5918365e+00 5.1316123e+00 5.7013745e+00\r\n 6.3011246e+00 6.9308619e+00 7.5905881e+00 8.2803001e+00 9.0000000e+00] \r\n\r\n```\r\n\r\n## Scenario 2\r\n```\r\nX = tf.placeholder('float64')\r\n\r\nx = np.linspace(-3, 3)\r\n\r\ns = tf.Session()\r\n\r\nX_pow2 = tf.pow(X, 2)\r\nX_pow3 = tf.pow(X, 3)\r\n\r\nprint('X_pow2 =', X_pow2.eval({X:x}, session=s), '\\n')\r\nprint('X_pow3 =', X_pow3.eval({X:x},session=s),'\\n')\r\n```\r\n\r\nget this \r\n\r\n```\r\nX_pow2 = [9.00000000e+00 8.28029988e+00 7.59058726e+00 6.93086214e+00\r\n 6.30112453e+00 5.70137443e+00 5.13161183e+00 4.59183673e+00\r\n 4.08204915e+00 3.60224906e+00 3.15243648e+00 2.73261141e+00\r\n 2.34277384e+00 1.98292378e+00 1.65306122e+00 1.35318617e+00\r\n 1.08329863e+00 8.43398584e-01 6.33486047e-01 4.53561016e-01\r\n 3.03623490e-01 1.83673469e-01 9.37109538e-02 3.37359434e-02\r\n 3.74843815e-03 3.74843815e-03 3.37359434e-02 9.37109538e-02\r\n 1.83673469e-01 3.03623490e-01 4.53561016e-01 6.33486047e-01\r\n 8.43398584e-01 1.08329863e+00 1.35318617e+00 1.65306122e+00\r\n 1.98292378e+00 2.34277384e+00 2.73261141e+00 3.15243648e+00\r\n 3.60224906e+00 4.08204915e+00 4.59183673e+00 5.13161183e+00\r\n 5.70137443e+00 6.30112453e+00 6.93086214e+00 7.59058726e+00\r\n 8.28029988e+00 9.00000000e+00] \r\n\r\nX_pow3 = [-2.70000000e+01 -2.38269854e+01 -2.09128424e+01 -1.82465554e+01\r\n -1.58171085e+01 -1.36134859e+01 -1.16246717e+01 -9.83965015e+00\r\n -8.24740542e+00 -6.83692169e+00 -5.59718315e+00 -4.51717397e+00\r\n -3.58587833e+00 -2.79228043e+00 -2.12536443e+00 -1.57411453e+00\r\n -1.12751490e+00 -7.74549720e-01 -5.04203181e-01 -3.05459460e-01\r\n -1.67302740e-01 -7.87172012e-02 -2.86870267e-02 -6.19639776e-03\r\n -2.29496213e-04  2.29496213e-04  6.19639776e-03  2.86870267e-02\r\n  7.87172012e-02  1.67302740e-01  3.05459460e-01  5.04203181e-01\r\n  7.74549720e-01  1.12751490e+00  1.57411453e+00  2.12536443e+00\r\n  2.79228043e+00  3.58587833e+00  4.51717397e+00  5.59718315e+00\r\n  6.83692169e+00  8.24740542e+00  9.83965015e+00  1.16246717e+01\r\n  1.36134859e+01  1.58171085e+01  1.82465554e+01  2.09128424e+01\r\n  2.38269854e+01  2.70000000e+01] \r\n```\r\n\r\nSo on more serious or complicated tasks this give unpredictable behavior and optimizers don't optimize, losses get NaN, everything going crazy or just NaN everywhere.\r\n\r\nSo I exactly had the same problem on tensorflow version 1.5.0 with CuDNN 7.0.5\r\n\r\nAnd I can't understand, is it my setup and maybe videocard just bad, or is it really bug?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nCUDA/cuDNN version\nGPU model and memory", "Hm. I think I filled this fields.\r\n\r\nBut it's ok to repeat)\r\n\r\n**OS Platform and Distribution:** Windows 10 Pro (last stable build)\r\n**CUDA/cuDNN version:** CUDA 9.1.85, cuDNN: 7.1.1\r\n**GPU model and memory:** nvidia GeForce MX150 2GB Memory", "So. I repeat it on another windows 10 machine with GTX 1060 3GB and core i7 with cuda 9.1 and cuDNN 7.1.1 and whl version if tensorflow from here https://github.com/fo40225/tensorflow-windows-wheel as before. \r\n(fresh build, 1.6.0, with gpu, SSE2 instructions) and got the same behavior. \r\n\r\nSo I think it is totally bag with this version of tensorflow and this setup of CUDA\\cuDNN", "I apologize but we do not support Windows builds. May be, @fo40225 or somebody else who uses TensorFlow on Windows can help you to troubleshoot the problem.", "It's my fault. I did not encounter this problem when using Keras.\r\n\r\nI've checked my whl, not all unit tests are passed.\r\n\r\n`--fast-math` cause this problem, it change `pow(x, y)` to `2^(y * log2(x))`\r\n\r\nIs it a common scenario to calculate the power of negative numbers?\r\n\r\nDue to the special function unit is limited resource in GPU, you should use multiply if your power is small integer.\r\n\r\nIf your power is not integer, it still return NaN when input is a negative."]}, {"number": 17631, "title": "Eager mode in multithreaded environment v1.6 generates : \"All graphs are building functions, and no eager context was previously active\"", "body": "### The problem\r\nScope initialization assumes that the  'context_stack' must be in the same stacktrace thread. while in some cases the user might use the eager context from a different thread. For example, the code below fails in version 1.6  , but not in 1.5.\r\nAlso, in version 1.6 it does not fail if \"foo\" is called from main thread, or if enable_eager_execution is called from the child thread.\r\n\r\n### Source code \r\n```\r\nfrom tensorflow.contrib.eager.python import tfe\r\nimport tensorflow as tf\r\nfrom threading import Thread\r\n\r\nclass MNISTModel(tfe.Network):\r\n    def __init__(self):\r\n        super(MNISTModel, self).__init__()\r\n        self.layer1 = self.track_layer(tf.layers.Dense(units=10))\r\n        self.layer2 = self.track_layer(tf.layers.Dense(units=10))\r\n    def call(self, input):\r\n        result = self.layer1(input)\r\n        result = self.layer2(result)\r\n        return result\r\n\r\ntfe.enable_eager_execution()\r\n\r\n\r\ndef foo():\r\n    model = MNISTModel()\r\n    batch = tf.zeros([1, 1, 784])\r\n    print(batch.shape)\r\n    result = model(batch)\r\n    print(result)\r\n\r\n\r\nt1 = Thread(target=foo)\r\nt1.start()\r\nt1.join()\r\n\r\n```\r\n### logs (exception) : \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/opt/anaconda3/lib/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/data/dopamine/dopapy/uriy_tests/dnn4/test_eager_1.py\", line 23, in foo\r\n    result = model(batch)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 696, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/data/dopamine/dopapy/uriy_tests/dnn4/test_eager_1.py\", line 11, in call\r\n    result = self.layer1(input)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 680, in __call__\r\n    self.build(input_shapes)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/layers/core.py\", line 134, in build\r\n    trainable=True)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 533, in add_variable\r\n    partitioner=partitioner)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\r\n    constraint=constraint)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\r\n    constraint=constraint)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\r\n    constraint=constraint)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 775, in _get_single_variable\r\n    with ops.init_scope():\r\n  File \"/opt/anaconda3/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5034, in init_scope\r\n    raise AssertionError(\"All graphs are building functions, and no \"\r\nAssertionError: All graphs are building functions, and no eager context was previously active.\r\n\r\n```\r\n### System information\r\n- Have I written custom code : no\r\n- OS Platform and Distribution :  Linux Ubuntu 16\r\n- TensorFlow installed from  : binary\r\n- TensorFlow version : v1.6.0-0-gd2e24b6039 1.6.0\r\n- Python version:  3.5\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- **Bazel version (if compiling from source): N/A (not compiling from source)**\r\n- **GCC/Compiler version (if compiling from source): N/A (not compiling from source)**\r\n- **CUDA/cuDNN version: N/A (not using GPU)**\r\n- **GPU model and memory: N/A (not using GPU)**\r\n- **Exact command to reproduce: python (and the file name of code above)**", "Thanks for the report, this shouldn't happen :), will take a look.\r\nCC @akshayka since this seems to be related to the change to use `init_scope`", "Yes, thanks for report --- I'll investigate further.", "@uyerushalmidop: We've fixed this bug at head, which is packaged on pip as tf-nightly. The fix will be included in version 1.7 of TensorFlow.\r\n\r\nDo you need a fix immediately? If so, I can provide you with one, with the caveat that the fix uses a private API which will be removed in a future release.", "Thanks for the quick response. \r\nNo need for an immediate fix. For now I have a workaround until 1.7 is out."]}, {"number": 17630, "title": "Fix broken link in kernel method tutorial", "body": "As we can see in [Kernel Methods tutorial](https://www.tensorflow.org/tutorials/kernel_methods), the reference links in Note section haven't generated valid links.\r\n> Note: This document uses a deprecated version of ${tf.estimator}, which has a ${tf.contrib.learn.estimator$different interface}. It also uses other contrib methods whose ${$version_compat#not_covered$API may not be stable}.\r\n\r\nThis PR is a simple fix just replace the begining \"$\" to \"@\" to fix above broken links.\r\n", "comments": []}, {"number": 17629, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "OS Platform and Distribution:\r\nLinux Ubuntu 17.10\r\n\r\nTensorFlow installed using pip\r\nTensorFlow version: 1.6, with GPU support\r\nPython Version: 3.6.4\r\nCUDA version: 9.1\r\nGPU model and memory: NVidia GEForce 940MX 2GB\r\ncommand to reproduce:\r\n~$ python3\r\n>>> import tensorflow as tf\r\n(basically run any tensorflow program to reproduce)\r\n\r\nProblem:\r\nWhenever you run a tensorflow program, you get a huge error log, but the main problem is this:\r\n`ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory`\r\nSo, the reason this is happening is because TensorFlow wants Cuda 9.0, but I have Cuda 9.1. This problem can be fixed by installing Cuda 9.0, but I have a few requests. Seeing that a couple of people have this problem (see https://github.com/tensorflow/tensorflow/issues/15604, https://github.com/tensorflow/tensorflow/issues/15817, https://github.com/tensorflow/tensorflow/issues/15817), I think that TensorFlow could be updated so that it works with Cuda 9.1 (but I think this issue is only with Ubuntu), or the following could be done:\r\nUpdate the TensorFlow documentation, saying that you specifically need Cuda 9.0 for TensorFlow 1.6, and Cuda 8.0 for TensorFlow 1.4, and so on\r\nAnd also, include this in the errors list at https://www.tensorflow.org/install/install_linux#common_installation_problems.\r\n\r\nEdit: If a Pull Request is required to update the documentation, I am fine with doing that.", "comments": ["I proposed a solution that worked for me in #15604 ", "@mldm4 I have also replied to you at 15604, the issue is not really that _I_ couldn't get it to work, I know what the problem is. I just need this to be fixed, or mentioned more specifically (it already is, just not that clearly) in the TensorFlow docs, and included in the common installation problems.", "I also face the same problem with same configuration. But when I install cuda-9.0 version the issue got solved. I feel tensorflow-gpu version is using the cuda-9.0 version specifically.", "this has been a persistent problem since i first installed tensorflow. it seems totally unable to reference the latest cuda library and instead insists on a specific x.version.\r\n\r\ni last managed to correct this with CUDA v8.0 by renaming than file libXXX.so.N to the version it was looking for. \r\n\r\n", "https://github.com/mikewlange/tensorflow-gpu-install-ubuntu-16.04 \r\n\r\nwas the only way could install TensorFlow gpu  and Cuda 9.1. Trust me - was about to throw my computer out the window. and that's what did it. \r\n\r\nI installed the full anaconda package.\r\n(tensorflow) mike@mike:~$ python\r\nPython 2.7.14 |Anaconda, Inc.| (default, Mar 27 2018, 17:29:31) \r\n[GCC 7.2.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('What a pain in the ARSE!')\r\n>>> print(sess.run(hello))\r\nWhat a pain in the ARSE!\r\n>>> \r\n\r\n\r\n(tensorflow) mike@mike:~$ nvidia-smi\r\nSun Apr 22 23:00:55 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:03:00.0  On |                  N/A |\r\n|  4%   53C    P8     9W / 180W |   7762MiB /  8116MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       975      G   /usr/lib/xorg/Xorg                           480MiB |\r\n|    0      1818      G   compiz                                       274MiB |\r\n|    0      2751      C   python                                      6995MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n(tensorflow) mike@mike:~$ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Nov__3_21:07:56_CDT_2017\r\nCuda compilation tools, release 9.1, V9.1.85\r\n\r\nI use Keras as it's so damn simple. But out of protest I may only use Theno and the backend and not TF. \r\n", "@mldm4 Hi, do you mean to downgrade coda to v9-0 ?\r\n\r\nCan you please be more specific and point your hyperlink to the comment with the proposed solution to gain people more time ?\r\n\r\nIt would be great !", "I'm on my phone so have to be brief.. However when installing cuda 9.1 the folder created is labeled cuda9.1. All the instructions say to set your $PATH to yadda/yadda/cuda/yadda/bin. Changing the path to $yadda/yadda/cuda9.1 shits the bed and you get this error. If you've installed 9.1, leave it alone and follow the installation instructions step 2 here https://github.com/mikewlange/tensorflow-gpu-install-ubuntu-16.04\n\nSent from my iPhone\n\n> On Apr 25, 2018, at 5:51 AM, sebma <notifications@github.com> wrote:\n> \n> @mldm4 Hi, do you mean to downgrade coda to v9-0 ?\n> \n> Can you please be more specific and point your hyperlink to the comment with the proposed solution to gain people more time ?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@mikewlange Thanks \ud83d\ude03 ", "My pleasure. Hope you got it going! \n\n\n> On Apr 26, 2018, at 10:57 AM, sebma <notifications@github.com> wrote:\n> \n> @mikewlange Thanks \ud83d\ude03\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "jamesredwards said: \"this has been a persistent problem since i first installed tensorflow. it seems totally unable to reference the latest cuda library and instead insists on a specific x.version.\r\n\r\ni last managed to correct this with CUDA v8.0 by renaming than file libXXX.so.N to the version it was looking for.\"\r\n\r\n@jamesredwards , or anyone else who is circumventing this error by renaming files, could you clarify how I'd do this for libcublas.so.9.0? I'm running cuda v8.0 on ubuntu 16.0.4, and tensorflow-gpu is erroring out with \"ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\" . \r\n\r\nIn the /usr/local/cuda/lib64/ directory I see libcublas.so , libcublas.so.8.0, libcublas.8.0.61 and libcublas.so.8.0.88. I can use sudo mv to rename these files. Just renaming libcublas.so.8.0 to libcublas.so.9.0 does not fix this error. Do I have to do something to the other files as well?\r\n\r\nIs tensorflow looking for licublas.so.9.0 in a different location than /usr/local/cuda/lib64/ ?\r\n\r\nThanks for any help!", "This issue is related to Google's protobuf-compiler due to which tensorflow fails to find the shared object file, in this instance, libcublas.so.9.0. Even switching from CUDA 9.1 to 9.0 didn't help as tensorflow was still unable to locate the file.\r\n\r\nBuilding the latest version of protobuf (3.5.0) from source didn't help either. What worked for me was to install the system-wide protobuf compiler through apt install protobuf-compiler on Ubuntu 16.04. And, install the python version through pip3 install protobuf. I am using CUDA 9.0 as 9.1 is not yet compatible with tensorflow's pre-built binary.\r\n\r\nYou can check the system-wide protobuf version using protoc --version which is 2.6.1 on 16.04. The protoc python version is 3.5.2.post1. Hope this helps. I had a similar issue using earlier versions of tensorflow and CUDA 8, and had documented this troubleshooting procedure. Using the same procedure, I am able to use tensorflow 1.8.0 too.", "What worked for me was the process described at https://medium.com/@taylordenouden/installing-tensorflow-gpu-on-ubuntu-18-04-89a142325138 plus the protobuf bit @dashsd provided above. $PATH and $LD_LIBRARY_PATH all use '/usr/local/cuda-9.0'. The latter contains two separate entries, as described in https://github.com/tensorflow/tensorflow/issues/16750: one for /usr/local/cuda-9.0/extras/CUPTI/lib64 and another for /usr/local/cuda-9.0/lib64.\r\n\r\nI use Ubuntu 18.04 on a Dell XPS 15\" with NVIDIA GeForce GTX 1050 (GP107M), driver version 390.48. Tensorflow now runs on CUDA 9.0 and CUDNN 7.0.5.", "Hi @shivaniag > it would be great if you can reassign this issue if you have other priorities at the moment. \r\nThis issue affects many users and is therefore critical. \r\nThank you for your caring consideration!", "I have the same bug cuda9.2 cuDNN7.14 ", "Same bug in cuda-9.2 for me as well", "Hi @shivaniag you would really help the community if you re-assign this issue...\r\nTo All:\r\nWho would be willing to solve this issue?", "TF 1.9 is also built against cuda 9.0. So, it's not going to work with cuda 9.2 unless it is built from source. I had a hard time trying to do it. With all unsuccessful attempts, I switched back to 9.0 for now. However, on Antergos, it's available pre-built with 9.2. It was more convenient to me rather than building it from source. Sorry, there's not much help I can provide regarding this. Some poor souls have successfully built  it. Here's one of the links: https://github.com/fo40225/tensorflow-windows-wheel\r\nHope this helps!!!", "I have the same problem. CUDA 9.2, cuDNN7.1", "Same problem here. CUDA 9.2, cudNN 7.1.4", "You have to re-compile tensorflow. The builds are not compatible with this combination yet.\r\n\r\nWe have compiled the latest master branch with \r\nNVIDIA Drivers 396.37\r\nCUDA 9.2, \r\ncuDNN 7.1, \r\nbazel-0.16.0\r\nNCCL v2.2.13,\r\n\r\nPackage available here (temporarily)\r\nhttps://drive.google.com/open?id=11E7hBBeAi79xPe7EYfYJZgOZDhIwmrBB", "re-compile tensorflow.\r\n@angeload @Parnia @agilebean @fay111101 @mesargent \r\ninstall bazel:\r\n```\r\nsudo apt-get install openjdk-8-jdk\r\necho \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\nsudo apt-get install curl\r\ncurl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\nsudo apt-get update && sudo apt-get install bazel\r\n```\r\nclone tf code:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit pull\r\ngit checkout r1.9 \r\n```\r\n\r\n`./configure`\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: enter\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: enter\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: enter\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: enter\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: enter\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: enter\r\nDo you wish to build TensorFlow with GDR support? [y/N]: enter\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: enter\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: enter\r\n**Do you wish to build TensorFlow with CUDA support? [y/N]: Y**\r\n**Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.2**\r\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-9.2\r\n**Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4**\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-9.2]: /usr/local/cuda-9.2\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: enter\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: enter\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: enter\r\nDo you want to use clang as CUDA compiler? [y/N]: enter\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: enter\r\nDo you wish to build TensorFlow with MPI support? [y/N]: enter\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -march=native\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:enter\r\n\r\nbuild:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package tensorflow_pkg\r\ncd tensorflow_pkg\r\nsudo pip install tensorflow*.whl\r\n```\r\ntesting:\r\n```\r\nimport tensorflow as tf   \r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\nprint  **Hello, TensorFlow!**", "Yeah but what a pain.. I mean cuda 9.2 and cudnn 7.1 have been there for quite a while already...\r\nWould be nice to have a ready package...", "Tensorflow 1.10 released and it still requires cuda 9.0!!!!!! Can someone confirm this?\r\n\r\n<b>ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive</b>", "Please refer [combinations of CUDA, CuDNN and Tensorflow](https://www.tensorflow.org/install/install_sources#tested_source_configurations).\r\n\r\nThis error happens majorly due to incorrect version combinations of Nvidia-driver, CUDA, CuDNN and Tensorflow-gpu\r\n![image](https://user-images.githubusercontent.com/15031475/44293189-4d9a5c00-a2a7-11e8-9e5f-13463cb4f60c.png)\r\n", "Yesterday I received this message.\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\nThis install is about 2 months old and had been under daily use. I had just completed a training run and ran the script again and received the message. Could someone point me in the right direction, to resolve this issue ? Also I am running this in a virtual environment.  \r\nUbuntu 18.04\r\nPython 3.6.5\r\ntensorflow-gpu==1.10.0\r\nprotobuf [required: >=3.4.0, installed: 3.6.0]\r\ngcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCuda compilation tools, release 9.0, V9.0.176\r\n NVIDIA-SMI 396.51    Driver Version: 396.51\r\n\r\n\r\n", "Try  this Victor \r\n\r\nSnagged most of these words from stack overflow. I\u2019ve run into similar. \r\n\r\n1. dir is wonky in your vm \r\nFirst check this:\r\nLD_LIBRARY_PATH and make sure it\u2019s  pointing to the correct location.\r\n\r\n2. Check the permission of the folder /usr/local/cuda-9.0 it\u2019s prob  wrong. Kinda common. \r\n\r\nThis how #2 above handled it upon discovery.. \r\n\r\nI could not even cd to the folder. I changed the owner of the cuda-9.0from root to my-user-name and after that python was able to find the missing library.\r\n\r\n3. If you still can\u2019t jump to hyperspace,  try these things. \r\n\r\nThis is almost always a missing path in your LD_LIBRARY_PATH. We know..   check it again and proceed. \r\n\r\nFind libcublas.so.9.0on your system (start looking under /usr/local). \r\n\r\n\r\nIf you don't find it, then install the CUDA 9.0 Toolkit https://developer.nvidia.com/cuda-90-download-archive after you read the rest.. \r\n\r\n NOTE:::::::  \r\n\r\nwith TF 1.5 you want 9.0 toolkit and not 9.1 as anyone who has installed this does that, don\u2019t.  \r\n\r\nIf you have it, then update your LD_LIBRARY_PATH to point to the appropriate lib directory.\r\n\r\nIf you've done either of those and are now getting a similar looking error for a cudnn related library, then repeat that process for the CUDNN library.\r\n\r\nhttps://developer.nvidia.com/cudnn\r\n\r\nI think the latest version works. Tensorflow depends on both CUDA toolkit and the CuDNN library extension.\r\n\r\nNote that you can install all of this in userspace too (sudo is typical, but not required).", "I got same error message when import tensorflow. And now, I solve this problem already.\r\nLet try \r\n`apt-get install cuda-cublas-[cuda-version]`\r\nwith cuda-version: 9-0, 9-1, etc...", "mikewlange, Thanks for pointing me in the right direction. Your resources worked. It seems that ''wonky'' is the right term to describe this problem with the pip virtual environment. ", "Thanks. Just a word to others trying to do all this. It\u2019s obvious by watching this thread.. If you don\u2019t know what you\u2019re doing from the getgo. I.e., you\u2019re not a AI engineer who know specifically knows why they need CUDA , taken heed. CUDA is mess with TensorFlow (and TF is a mess on its own). \r\n\r\nYou\u2019re wasting time here if you want to check out CUDA. think about the Ml/AI solution you\u2019re trying to solve and work accordingly. Don\u2019t just install this monster if your have no idea how to use it.  \r\n\r\nIf you want to just learn, use docker for the love of god. There are a million images that can work. Try Kaggle\u2019s AI image. Ready to go with no nonsense like this. Masochists you all are. Lol. ", "We'll, if you don't want me help. @elithrar please come with a better solution?", "And for the love of god. If you don't know why you need TF, stop using it!!! https://medium.com/@julsimon/apache-mxnet-support-in-keras-83de7dec46e5  convert you TF model to MXnet: https://github.com/Microsoft/MMdnn and them get to work!!!!", "ALL READ: -\r\n\r\nhttps://github.com/awslabs/keras-apache-mxnet is your TF replacement. you won't look back.... trust me. Been at this for 24 years (software engineering in general) and have built anything you've seen on the net. And guess what, without TF. Worked at hedge funds to build prop trading tools and algos, search and rescue 100% autonomous drones, ios slot machines, surgical tools, ect... \r\n\r\nAll without TF. you are correct no CUDA needed either.\r\n\r\nHonesty, anything you need to do, can do it PyTorch. Or, \r\n1. Python for data munging\r\n2. https://pytorch.org/docs/stable/index.html to build your model\r\n\r\nOR - easier...\r\n\r\n3. Convert your TF models to **MXNet** -  \r\n4. Find your converter: https://github.com/mechanicalAI/deep-learning-model-convertor https://github.com/Microsoft/MMdnn#conversion\r\n\r\nOR, here is what sane people do who have to do this for a living....\r\n\r\n1. Data pro? Munge that data like no-ones business?\r\n2. Keras!!! OR https://github.com/mechanicalAI/autokeras or toss the GPU and learn \r\n\r\n1.. FInd the model yourself - example:\r\n![image](https://user-images.githubusercontent.com/38701254/48317350-915ae400-e5be-11e8-847f-1a3fb708122a.png)\r\n\r\nOR -\r\n\r\nor https://github.com/mikewlange/KETTLE\r\nor https://github.com/mechanicalAI/tpot OR you're best bet\r\nhttps://github.com/mechanicalAI/h2o4gpu <- drop in replacement for ScikitLearn with with GPU support - https://github.com/mechanicalAI/h2o4gpu#requirements\r\n\r\nGood luck and don't let your self get stuck for more than 1:30 min. Draw a line..", "Unfortunately, Tensorflow and pytorch support CUDA 9.0 and CUDA 9.2 respectively until now: 18.11.20.\r\nand CUDA 9.0 support ubuntu 16 and 17. So, til now, if you wanna use Tensorflow framework or pytorch framework, you should install ubuntu 16.04 or 17.\r\nor you should compile tensorflow source on your machine.\r\n\r\nLet's try this.\r\nOS = ubuntu 16 or 17\r\nCUDA = 9.0\r\ncudnn >=7.2\r\ntensorflow, pytorch\r\netc.\r\n\r\nfollow images are tensorflow gpu support and pytorch gpu support respectively.\r\n\r\n\r\n![2018-11-20 10 13 31](https://user-images.githubusercontent.com/11930729/48776516-6df90d00-ed13-11e8-90c9-b139fc0c2cbb.png)\r\n\r\n![2018-11-20 10 14 16](https://user-images.githubusercontent.com/11930729/48776531-74878480-ed13-11e8-92f1-4476cb0f6b2a.png)\r\n", "Hi all, \r\nI use PyTorch as much as possible, but for a particular project where i need to export a (Keras) model to tensorflowjs, I'm forced to use tf. The only solution which has worked well for me has been to build from source, after installing CUDA from the Ubuntu multiverse, as described here:\r\n\r\nhttps://medium.com/@asmello/how-to-install-tensorflow-cuda-9-1-into-ubuntu-18-04-b645e769f01d\r\n\r\nBonne chance!", "February 2019, stil no concrete solution.", "This issue seems to not be given enough attention. Why is tensorflow searching for libcublas.so.9.0 when CUDA 10.0 is installed. I installed tensorflow and ran some programs but it seemed most of the GPU memory wasn't being allocated to tensorflow so I rebooted my pc and then I received this error. ", "@Emile0205 I agree!\r\nI tried using TF 1.12.0 with cuda 10.0, seemed to work fine. Then I change to TF-gpu 1.12.0 and I get this error. Like why is this still an issue almost a year later from the thread was created?\r\nI read in another thread that this problem has been around since TF 1.5!", "I had this problem trying to use tensor flow within a conda environment. tensorflow worked fine with the standard install in my base python 3.6, but not in my conda environment that I use for python. \r\n`import tensorflow as tf`\r\nI got the same error. \r\n\r\nHere is what worked for me. I checked that the file did actually exist at /usr/local/cuda-9.0/lib64 and found that I have libcublas.so.9.0\r\nThen I added the LD_LIBRARY_PATH to my ~/.bashrc:\r\n`echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/\" >> ~/.bashrc`\r\nRestarted my terminal, built conda environment:\r\n'conda create -n tf-gpu python=3.6 tensorflow-gpu'\r\n'source activate tf-gpu'\r\n'conda install ipykernel'\r\n'python -m ipykernel install --user --name --display-name \"tf-gpu\"'\r\n'source deactivate'\r\n'jupyter notebook'\r\n\r\nand then made a new notebook that uses the tf-gpu kernel, and:\r\n'import tensorflow as tf'\r\n'h = tf.constant('hellow')'\r\n'sess = tf.Session()'\r\n'print(sess.run(h))'\r\n\r\nWorked\r\n\r\n\r\n\r\n\r\n", "I had the same problem (with CUDA 9.2), and somehow got it solved ~magically using `conda install tensorflow-gpu` (after uninstalling it with pip), rather than `pip install tensorflow-gpu`. It does not sound like a read fix, but it may help :)", "**Hi, folks! Why all this fuss? Read below!\r\nIf you just downgrade from cuda 9.2 to cuda 9.0 (and the driver too), you will have other problems when installing new software packages. The right solution is just to add the cuda 9.0 toolkit leaving the cuda 9.2 intact.**\r\nAfter you have cuda 9.2 and the corresponding newest driver, you can just add cuda 9.0 toolkit using .run file install. While you additively install cuda 9.0 using .run file, say 'no' whey you are asked 'do you want to install the driver?' and the driver will remain intact and just cuda 9.0 toolkit will be installed. Cuda 9.0 works ok with the driver for cuda 9.2 (drivers are backward compatible). When you want to use tensorflow, just change the symbolic liink /usr/local/cuda to /usr/local/cuda-9.0 instead of /usr/local/cuda-9.2. \r\nIf you are in ubutu 16.04 or higher, you don't have to worry about the nouveau driver and nomodeset, nvidia-xconfig things during the .run file install. Just run the .run file and it is installed cleanly. (install all the patches too).\r\n\r\nSee https://devtalk.nvidia.com/default/topic/493290/multiple-cuda-versions-can-they-coexist-/\r\n", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 17628, "title": "iOS error: Running model failed:Invalid argument: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T ->", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**:none\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI was just trying to make a classifier iOS app by simply replacing the inception pb file to my pb file for my classifier.\r\nThe iOS app example I was using is camera. It worked well with the original inception.\r\nBut when I changed the pb file I got an error as listed below. The pb file was generated with [tensorflow-for-poest-2](https://github.com/googlecodelabs/tensorflow-for-poets-2). \r\nThen the model doesn't work. The camera is working but no classification message.\r\n\r\nI assume there is some mismatch between versions of TF?\r\n\r\n### Source code / logs\r\n2018-03-11 20:33:41.906421: E /Users/apple/OneDrive/P/tensorflow/tensorflow/examples/ios/camera/CameraExampleViewController.mm:327] Running model failed:Invalid argument: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; NodeDef: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Mul_0, conv/conv2d_params). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_Mul_0, conv/conv2d_params)]]\r\n", "comments": ["Hi, I do have the same problem actually. This seems to be a version mismatch between the generating Tensorflow version and the interpreting one. See [here](https://github.com/tensorflow/tensorflow/issues/15698)\r\n\r\nI recently built the tensorflow_core.a by myself with as stated [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios#building-the-tensorflow-ios-libraries-from-source). But still can't get it run.", "Hi, I've got the same issue. I retrained the model following the \"Tensorflow for Poets\" and the guide on how to prepare your model for mobile from the Tensorflow website. Once I launch the camera app it gives the same error.", "@andrehentz could you take a look?", "Ist seems like the version of tensorflow in the CocoaPods \u201etensorflow-experimental\u201c package is 1.1.0. I retrained a model with tf 1.1.0 and optimized it and it worked. Seems like using the CocoaPods version isn\u2018t a good idea.", "@sascha-greiner-adam I tried to specify the tensorflow dependency as `pod 'TensorFlow-experimental', '~> 1.1.0'` and it still doesn't work. Maybe you could share a repo with a working project that has the right tensorflow pod?", "You have to build the pb file with Version 1.1.0. Or if you check out tf version 1.1.0 from github, there is an iOS example without pod. You could copy that example, edit search paths and then check out latest. Build for iOS and then use your copied example app project. This worked for me. Monday I could give you more informations.", "Could `TensorFlow-experimental` please be updated to have the latest binary?", "I have the same problem, can you update the status of the issue or how the documentation of how it can be fixed please?\r\n\r\nThank you very much.", "I have the same issue as well... appreciate if someone can advice how it can be fixed please?", "same here!It will be great if someone can help fix it\r\n", "\r\nEdit:\r\n\r\nI fixed this issue by using the tools to freeze and transform from a matching version of TF that my device was running for inference.\r\n\r\n", "I think it's a version issue.I use the the libs generated by 'build_all_ios.sh',setting the header search path and lib search path,and the error disappeared when I load the retrained pb file", "@cartmanguo Can you give me some more details on what you did? I am kind of new to this environment.", "@shubchat  I download latest version of tensorflow,find the .sh file in /contrib/makefile.After compiling,this will give you four static libs,libtensorflow-core.a,libprotobuf-lite.a,libprotobuf.a,nsync.a\r\ninclude them all in your project,in the XCode build setting tab,set the header search path and library search path\r\nThis guide may save you some time:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios#building-the-tensorflow-ios-libraries-from-source", "I faced the same issue too but managed to fix it by generating the `.pb` file using TensorFlow 1.1.* (according to answers in StackOverflow).\r\n\r\nAs I understand it, the protobuf framework version in our desktop generating the `.pb` file must be of the same version as the one residing in iOS after Pod installation.\r\n\r\nhttps://www.tensorflow.org/mobile/linking_libs#protobuf_problems\r\n> ... [T]his generated code needs to be linked against shared libraries for the **exact same version** of the framework that was used for the generator.", "YC, do you maintain the cocoapod for tf mobile?", "@tensorflowbutler can we have an update on this? I am currently working on a project and get the same error. Would be wonderful to update the pod :)", "same issue here! appreciate if someone can help", "@hanspond We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. refer [link1](https://github.com/tensorflow/tensorflow/issues/20682) , [link2](https://stackoverflow.com/questions/49553155/tensorflow-servingexecutor-failed-to-create-kernel-invalid-argument-nodedef) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17628\">No</a>\n"]}, {"number": 17627, "title": "Unknown Issue in working Tensorflow", "body": "I use tensorflow as backend in keras. Implementation is done in R. Everything was working properly but today I got an error calling lstm model. Could you please help inrectifying the problem. Below is the message I get and then R stops working\r\n\r\n2018-03-11 11:12:48.620511: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-03-11 11:12:48.977900: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties: \r\nname: Quadro K2000\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.954\r\npciBusID 0000:03:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.64GiB\r\n2018-03-11 11:12:48.978254: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 \r\n2018-03-11 11:12:48.978386: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y \r\n2018-03-11 11:12:48.978612: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2000, pci bus id: 0000:03:00.0)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Sorry for late response. The issue has got resolved after re-intalling CUDA driver and cuDNN packages. Seems CUDA and cuDNN had been corrupted. This led to connection breach between tensorflow and GPU and hence, tensorflow was not able to find GPU."]}, {"number": 17626, "title": "'unsupported/Eigen/CXX11/Tensor' file not found on iOS", "body": "when I ran this two projects( https://github.com/yjmade/ios_camera_object_detection )( https://github.com/jeffxtang/yolov2_tf_ios ) on ios. I met this warning.\r\n\r\nStack Overflow didn't give any help, case #4680 is for Raspberry Pi. It didn't help me.\r\n\r\n### Describe the problem\r\n\r\nwarning like this!\r\n'unsupported/Eigen/CXX11/Tensor' file not found\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS10.13.4 Xcode9.2 iOS11.2\r\n- **TensorFlow installed from (source or binary)**:\r\ninstall from github source.\r\n- **TensorFlow version (use command below)**:\r\nversion 1.6.0\r\n\r\nwhy you close a different issue which isn't solved?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "of course it is still is an issue!", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "yes!", "Please keep in mind that the TensorFlow GitHub Issues page is for tracking bugs and feature requests. It is not a way to escalate a Stack Overflow support question that is not getting enough attention. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17625, "title": "tf.layers.batch_normalization shape bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04 / archlinux\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.6.0-0-gd2e24b6039 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```python\r\nx = tf.placeholder(tf.float32, [100,100,100,100])\r\ny = tf.layers.batch_normalization(x, axis=1, renorm=True, training=True, virtual_batch_size=2)\r\n```\r\nIt throws:\r\n```\r\nValueError: Dimension 1 in both shapes must be equal, but are 1 and 50. Shapes are [1,1,100,1,1] and [1,50,100,1,1]. for 'batch_normalization/AssignMovingAvg' (op: 'AssignSub') with input shapes:\r\n[1,1,100,1,1], [1,50,100,1,1].\r\n```\r\nHowever, \r\n```python\r\nx = tf.placeholder(tf.float32, [None,100,100,100])\r\ny = tf.layers.batch_normalization(x, axis=1, renorm=True, training=True, virtual_batch_size=2)\r\n```\r\nworks fine.", "comments": ["There's more going on in the example that you're citing than the code that you've included. For example, something else in your code must be giving rise to the order-5 tensors that are causing the error. Can you please build us a minimal representative example? That'll help us to diagnose. \r\n\r\nPlease provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "There is nothing more going on in the example code I posted. The code is self-contained and reproducible at least in my environment. I've already posted my environment information in the issue.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ppwwyyxx If I understand it correctly, `renorm` is designed for small batch size, while `virtual_batch_size` is for large batch size. So it seems not a good idea to use them together. ", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 49 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been >2 months. Despite how clear the bug report is, I didn't get a meaningful response from the team.\r\n@chrisying who seems to implement virtual_batch_size originally", "Hi @ppwwyyxx, originally `virtual_batch_size` was not intended to be used simultaneously with `renorm` (these two techniques are generally used for different use-cases) and thus not tested. However, I suppose there isn't anything stops the user from using the two together. I can see the potential use-case here where you have very small virtual_batches.\r\n\r\nI've reproduced your error and identified the bug in the code: https://github.com/tensorflow/tensorflow/blob/2716bfff551591297a4ba6e61299e8147ac27c05/tensorflow/python/keras/layers/normalization.py#L589 this code block needs to be moved before the renorm code block above so that the means are reduced `if virtual_batch_size is not None` (`new_mean`/`new_variance` should be renamed to `mean`/`variance`).\r\n\r\nI'll see if I can get this change in to the next TensorFlow release. In the meantime, if you need this exact functionality, please apply the change I described.\r\n\r\nSide question: I'm curious why you need to use `renorm` and `virtual_batch_size` at the same time? Generally `renorm` is used for small batches (or non-iid batches) and `virtual_batch_size` is used for large batches. If you use both, I can imagine might be able to get by with using a larger `virtual_batch_size` and `renorm=False`.", "Thanks a lot! \r\nI want to train stuff with small \"normalization-batchsize\" to see how bad it performs (just for research purposes. batch renorm is a baseline to compare with in our last paper), and at the same time I want to gain the GPU efficiency of larger batch for the rest of the network. That's why I want to mix the two.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this bug now that https://github.com/tensorflow/tensorflow/commit/2a484497062677f5cf0205ee3b9c28a64f03fe04 has been merged in.\r\n\r\nNote: I haven't extensively tested if combining renorm and ghost batch norm is numerically correct. The unit tests for each pass but there aren't any tests that test both of them used together."]}, {"number": 17624, "title": "Fix windows gpu cmake build", "body": "", "comments": ["This makes no sense, the failure I see is:\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/tools/graph_transforms/BUILD:89:1: Couldn't build file tensorflow/tools/graph_transforms/_objs/transforms_lib/tensorflow/tools/graph_transforms/sparsify_gather.pic.o: error while parsing .d file: /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/graph_transforms/_objs/transforms_lib/tensorflow/tools/graph_transforms/sparsify_gather.pic.d (No such file or directory)\r\n```\r\nWhich looks completely unrelated. @jhseu any ideas?\r\n@nlopezgi is it possible this is an infra issue?", "The error seems unlikely to be related, but trying again."]}, {"number": 17623, "title": " AttributeError: 'module' object has no attribute 'LookupTensor' ", "body": "", "comments": []}, {"number": 17622, "title": "Small markup documentation fixes around closing blocks", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Please sign the CLA :)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17621, "title": "Does TensorFlow 1.1 support CUDA 9.1?  ", "body": "Does TensorFlow 1.5 support CUDA 9.1? if not , shoud i degrade my cuda or upgrade tensorflow?", "comments": ["TensorFlow-gpu 1.1 ", "How are you installing tensorflow? The current prebuilt binaries 1.5 and 1.6 are built against CUDA 9.0 so you will need this version of CUDA installed. You can use CUDA 9.1 if you compile tensorflow from source and specify that version during the configuration step.", "No, tensorflow-gpu supports CUDA 9 since 1.5. Tensorflow < 1.5 is unable to be compiled with CUDA 9. I have tried to compile 1.4 with CUDA9, but I failed.\r\nSee the tested source configurations in the following site.\r\nhttps://www.tensorflow.org/install/install_sources#tested_source_configurations", "@xysmlx thanks a lot . here i met a new problem  . i've installed tensorflow-gpu with cuda 8.0 and i write a simple program, but it's freezed when running session = tf.session(). when use keras to train data. it's also freezed like this. \r\n![image](https://user-images.githubusercontent.com/16147268/37582552-883f12e2-2b88-11e8-816d-534b8cc018ee.png)\r\n", "@LANRRI These information say that tensorflow can get a higher performance when compiled with these instructions but tensorflow can also run normally without them. These information do not affect your programs. If there are problems, please check your codes.", "@xysmlx i mean when the code run to session = tf.session()  the thread is freezed with no error or exception in 1 hour . the program is no longer processing.\r\n\r\nsimply, just run one line code \"import tensorflow as tf ; session = tf.session()\"  the problem is still there.", "@LANRRI If you cannot run the validation code, there must be errors in your installation. I recommend you to reinstall the tensorflow in this case.\r\n\r\n[https://www.tensorflow.org/install/install_linux#validate_your_installation](https://www.tensorflow.org/install/install_linux#validate_your_installation)\r\n\r\n```\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\n", "@xysmlx  thank you "]}, {"number": 17620, "title": "tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n Mac OS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\n source\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\npython3\r\n- **Bazel version (if compiling from source)**:\r\nsorry, I don't know\r\n- **GCC/Compiler version (if compiling from source)**:\r\nsorry, I don't know\r\n- **CUDA/cuDNN version**:\r\nsorry, I don't know\r\n- **GPU model and memory**:\r\nradeon pro 560 4G\r\n\r\n\r\n### Describe the problem\r\nI try to use object detection using tensorflow API \r\nI follow this youtube\r\nhttps://www.youtube.com/watch?v=JR8CmWyh2E8\r\nbut there is a error\r\n\r\n### Source code / logs\r\nMacBook-Pro-de-Jongwun:~ jongwuni$ cd /Users/jongwuni/Documents/Jongwunibang/Neural_network/models\r\nMacBook-Pro-de-Jongwun:models jongwuni$ cd object_detection\r\nMacBook-Pro-de-Jongwun:object_detection jongwuni$ python3 train.py --logtostderr --train_dir=\"/Users/jongwuni/Documents/Jongwunibang/Neural_network/models/object_detection/training\" --pipline_config_path=\"/Users/jongwuni/Documents/Jongwunibang/Neural_network/models/object_detection/training/faster_rcnn_inception_resnet_v2_atrous_coco.config\"\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 106, in main\r\n    overwrite=True)\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 385, in copy\r\n    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@jongwun  did you fix the issue?\r\nThanks", "I met the same issue. Has anyone fixed this yet? Thanks", "@thanhtu19392  are you using external HDD as a path or storage?", "I fixed the error. It's an error from my side. Thanks anw", "@thanhtu19392 I'm having this error - how did you fix it?", "@ProjectDent You can post your screenshot here so that the others can see your prob and suggest some solutions to fix it. For me, it's an error in the path which leads to the file config."]}, {"number": 17619, "title": "undefined symbol error for _dataset_ops.so on rasp pi", "body": "\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\n```\r\n$ uname -a\r\nLinux pi_c 4.4.13-v7+ #894 SMP Mon Jun 13 13:13:27 BST 2016 armv7l GNU/Linux\r\n$ cat /etc/issue\r\nRaspbian GNU/Linux 8 \\n \\l\r\n```\r\n\r\n- **TensorFlow installed from (source or binary)**: \r\n\r\ninstalled from nightly wheel `http://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/`\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n```\r\n$ python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.6.0-rc1-1295-g851c289 1.6.0\r\n```\r\n\r\n- **Python version**: \r\n\r\n```\r\n$ python3 --version\r\nPython 3.4.2\r\n```\r\n\r\n- **CUDA/cuDNN version**:  N/A   running cpu on pi\r\n\r\nHave I written custom code NA\r\n\r\nBazel version NA\r\n\r\nGPU model and memory NA\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n$ python3 -c \"import tensorflow.contrib.slim as slim\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib import data\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/__init__.py\", line 63, in <module>\r\n    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/error_ops.py\", line 20, in <module>\r\n    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/contrib_op_loader.py\", line 24, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/../../_dataset_ops.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl15AllocateAlignedEj\r\n```\r\n\r\n### Describe the problem\r\n\r\nImport of slim fails with .so error (as described above). doubt it's actually slim, it was just the first import to reference `_dataset_ops.so`\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nGPU model and memory", "It looks like you are missing the protobuf library. Can you try to locate it?", "the following snippet of code runs in the same environment. i think this shows that, generally, proto buffers are ok ?  (this was the simplest tf related proto buff bit of code i could think of)\r\n\r\n```\r\n$ cat test.py\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nc = tf.ConfigProto()\r\nc.intra_op_parallelism_threads = 5\r\nprint(c)\r\n $ ./test.py\r\nintra_op_parallelism_threads: 5\r\n```\r\n\r\nthere might be something specific with `contrib` though; as mentioned here\r\nhttps://petewarden.com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi/#comment-108790\r\n", "The error you got is this:\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/../../_dataset_ops.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl15AllocateAlignedEj\r\n```\r\nwhich says that it can't load the protobuf runtime. \r\n\r\nI just saw what you are trying to do. You can't import directly. You have to use `import tensorflow as tf`, then  use `tf.contrib.slim`.", "that's true. i was just looking for a minimal reproduction case\r\n\r\n```\r\n$ cat test.py\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\n$ ./test.py\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 3, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib import data\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/__init__.py\", line 63, in <module>\r\n    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/error_ops.py\", line 20, in <module>\r\n    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/contrib_op_loader.py\", line 24, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/../../_dataset_ops.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl15AllocateAlignedEj\r\n```\r\n\r\nthe code i first saw this with was a much bigger piece https://github.com/matpalm/bnn/blob/master/predict.py\r\n\r\n(developed on my desktop, but fails on the pi)\r\n\r\n```\r\n$ ./predict.py\r\n$ ./predict.py \r\nTraceback (most recent call last):\r\n  File \"./predict.py\", line 9, in <module>\r\n    import model\r\n  File \"/home/pi/dev/bnn/model.py\", line 5, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib import data\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/__init__.py\", line 63, in <module>\r\n    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/error_ops.py\", line 20, in <module>\r\n    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/contrib_op_loader.py\", line 24, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/../../_dataset_ops.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl15AllocateAlignedEj\r\n```\r\n\r\n", "and, just for my sanity, regarding potential lazy loading / eval of the proto libraries or something...\r\n\r\n```\r\n$ cat test.py\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nc = tf.ConfigProto()\r\nc.intra_op_parallelism_threads = 5\r\nprint(c)\r\nimport tensorflow.contrib.slim as slim\r\n\r\n$ ./test.py\r\nintra_op_parallelism_threads: 5\r\n\r\nTraceback (most recent call last):\r\n..... AS BEFORE\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/../../_dataset_ops.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl15AllocateAlignedEj\r\n```", "OK. I'm not 100% sure what it is, but you're really not supposed to import directly from tensorflow package. Just don't do that and you should be fine. Also, we are deprecating high level APIs in favor of `tf.keras`.", "Fair enough. This might be the nail in the coffin for me & slim... I'll use this as an excuse to upgrade.", "(oh, i did forget to mention, thanks for the attempted workaround but referencing slim through tf import still didn't work)\r\n\r\n```\r\n$ cat test.py\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nprint(dir(tf.contrib.slim.conv2d))\r\n\r\n$ ./test.py\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 3, in <module>\r\n    print(dir(tf.contrib.slim.conv2d))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.4/importlib/__init__.py\", line 109, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 2254, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 2237, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 2226, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1200, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1129, in _exec\r\n  File \"<frozen importlib._bootstrap>\", line 1471, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 321, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib import data\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/__init__.py\", line 63, in <module>\r\n    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/error_ops.py\", line 20, in <module>\r\n    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/contrib_op_loader.py\", line 24, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/data/python/ops/../../_dataset_ops.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl15AllocateAlignedEj\r\n```\r\n", "Oh, right I forgot about the lazy loading of contrib. It is basically loaded on demand. There is something broken in your install. It's not to do with slim, but with `tf.contrib.data`. If you try to use it directly, it will likely break.", "agreed it's something broken with that nightly built wheel (from `http://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/`) when i built from source (awhile back) it was a pain to get working, but didn't have this problem. \r\n\r\nthought it might also be something weird on that pi (since i've built tensorflow on it multiple times) but i tried to install this wheel on a completely fresh install of jessie (on another pi) and got the same problem", "and agree it's not slim (that was just the first thing hitting this in my original code (see initial report)) . so i guess moving to whatever other framework for layers isn't going to help. (since they will all reference `data` i guess)\r\n\r\ni'm going to try to use a frozen graph (in the hope the frozen graph doesn't use `data`) and if that doesn't work i'll give up on the wheel for now & go back to building from source."]}, {"number": 17618, "title": "Raspberry Pi 3 C++ compiling issue", "body": "### System information\r\nRaspberry Pi 3 running a clean install of latest Raspbian (version November 2017)\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution : Raspbian (version November 2017)\r\nTensorFlow installed from : git cloned the latest version\r\nTensorFlow version: git cloned the latest version\r\nBazel version: N/A\r\nCUDA/cuDNN version : N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: [Tensorflow Makefile for Raspberry Pi](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi)\r\n\r\n### Problem\r\nI am having a problem building the C++ library of Tensorflow.\r\n\r\nFirst I use a USB as a swap by follwing the instructions here:\r\nhttps://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md#2-install-a-memory-drive-as-swap-for-compiling\r\n\r\nThen continued on building tensorflow with the instructions below:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi\r\n\r\nI followed all the steps and believed that everything went successfull, \r\nuntil I build the library and example using the command below:\r\n```\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\\r\n OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\r\n```\r\n\r\n### Logs\r\nIt will ran for quite few hours until it stops and outputs the error below:\r\n```\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:730: recipe for target '/home/pi/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed\r\nmake: *** [/home/pi/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\r\n```\r\n\r\nThe rest of the log can be seen here:\r\n[output_log.txt](https://github.com/tensorflow/tensorflow/files/1799928/output_log.txt)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have just completed my project on Github to run C++ inference on Ubuntu and Raspberry Pi. Feel free to grab the prebuilt Debian package for RaspPi and run your inference following my instructions:\r\n\r\nhttps://github.com/kecsap/tensorflow_cpp_packaging\r\n", "@kecsap Thank you. I will try your project. Its a little bit off-topic, but would I be able to run the [pi_example](https://github.com/tensorflow/tensorflow/tree/de1334da2c6e074c427d283898450de6e50a605d/tensorflow/contrib/pi_examples) using your project? ", "@01000001-086 Yes! I tested the label_image example and it works fine.\r\n\r\nI have just added the Raspberry Pi examples from the Tensorflow repo and adapted the README file:\r\n\r\nhttps://github.com/kecsap/tensorflow_cpp_packaging/tree/master/rpi_examples\r\n", "@kecsap I was able to build and make the [example](https://github.com/kecsap/tensorflow_cpp_packaging/tree/master/rpi_examples)\r\nHowever, I had to ran the command first because it could not the find the needed file. \r\n```\r\nsudo apt-get install libv4l-dev\r\n```\r\nAnyways, Thanks for your support.\r\n\r\nOne more thing, I was trying to run my test cpp file but every time I ran the command below to compile.\r\n```\r\ng++ -I/usr/include/tensorflow-cpp -L/usr/lib/tensorflow-cpp  tfCpp1.cpp\r\n```\r\n\r\nI get an error.\r\n```\r\n/usr/include/tensorflow-cpp/tensorflow/core/platform/default/mutex.h:25:22: fatal error: nsync_cv.h: No such file or directory\r\n #include \"nsync_cv.h\"\r\n```\r\n\r\nI believe I am missing something.", "@01000001-086: Yes because there are more include directories, you can check them in:\r\n\r\n/usr/lib/cmake/tensorflow-cpp/TensorFlowCppConfig.cmake\r\n\r\nor by building the RPi example with \"make VERBOSE=1\".", "@kecsap Thank you. I was able to compile my test cpp using the command below.\r\n```\r\n g++ -I/usr/include/tensorflow-cpp -I/usr/include/tensorflow-cpp/external/nsync/public  -L/usr/lib/tensorflow-cpp tfCpp1.cpp\r\n```\r\n\r\nNow, I am trying to compile it using cmake.\r\n```\r\nCMAKE_MINIMUM_REQUIRED(VERSION 3.0)\r\n\r\nPROJECT(TEST_TF)\r\n\r\nSET(CMAKE_CXX_STANDARD 11)\r\n\r\n# Find tensorflow-cpp\r\nFIND_PACKAGE(TensorFlowCpp REQUIRED PATHS /usr/lib/cmake/tensorflow-cpp)\r\n\r\n# Add the tensorflow-cpp paths to the include directories\r\nINCLUDE_DIRECTORIES(${TENSORFLOWCPP_INCLUDE_DIRS})\r\n\r\n# Add a target with your inference codes\r\nADD_EXECUTABLE(tfCpp1 tfCpp1.cpp)\r\n\r\n# Link your application against the shared Tensorflow C++ library\r\nTARGET_LINK_LIBRARIES(tfCpp1 ${TENSORFLOWCPP_LIBRARIES})\r\n```\r\n\r\nHowever, I keep getting an error.\r\n```\r\n/usr/lib/tensorflow-cpp/libprotobuf-tf.so: undefined reference to `deflate'\r\n/usr/lib/tensorflow-cpp/libprotobuf-tf.so: undefined reference to `deflateEnd'\r\n/usr/lib/tensorflow-cpp/libprotobuf-tf.so: undefined reference to `inflateInit2_'\r\n/usr/lib/tensorflow-cpp/libprotobuf-tf.so: undefined reference to `inflate'\r\n/usr/lib/tensorflow-cpp/libprotobuf-tf.so: undefined reference to `deflateInit2_'\r\n/usr/lib/tensorflow-cpp/libprotobuf-tf.so: undefined reference to `inflateEnd'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/tfCpp1.dir/build.make:95: recipe for target 'tfCpp1' failed\r\nmake[2]: *** [tfCpp1] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/tfCpp1.dir/all' failed\r\nmake[1]: *** [CMakeFiles/tfCpp1.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n\r\nCan you point out where I missed?", "@01000001-086: You can see in the RPi example CMake file the additional \"-lz\" to the TARGET_LINK_LIBRARIES() what is necessary on the Pi.", "@kecsap Thanks! I totally missed that. Your project really helped me.", "@kescap Thank you for your help!"]}, {"number": 17617, "title": "Fix mac installation documentation error", "body": "This fix tries to address #17614 where installation for python 2 was incorrectly pointing to python3.\r\n\r\nThe error was fixed by f4e70be, but later it has been overridden by 9dae88d.\r\n\r\nThis fix fixes #17614.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17616, "title": "Fix incorrect python version in installation doc for Mac", "body": "This fix fixes the incorrect python version (2 vs 3) in the installation documentation for Mac.\r\n\r\nThis fix fixes #17614.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the PR @yongtang !\r\n\r\nThough, I believe the docs were fixed by https://github.com/tensorflow/tensorflow/commit/f4e70be18b104fbb2efeefeb83bea190aec12727\r\nbut we didn't push an update.\r\n\r\nSo closing out this PR.\r\n", "Thanks @asimshankar. It seems https://github.com/tensorflow/tensorflow/commit/f4e70be18b104fbb2efeefeb83bea190aec12727 to have been overridden  by https://github.com/tensorflow/tensorflow/commit/9dae88dace281c0c5dc3b9e15521a6c46feb6d75 ?\r\n"]}, {"number": 17615, "title": "Tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory", "body": "\r\nHi I try to use tensorflow object_detection API\r\n\r\nMy computer is Mac book pro(version Sierra 10.12.6 )\r\n\r\nI follow this youtube :'https://www.youtube.com/watch?v=JR8CmWyh2E8'\r\n\r\nand final step, error show up\r\n\r\nMacBook-Pro-de-Jongwun:object_detection jongwuni$ python3 train.py --logtostderr --train_dir=\"/Users/jongwuni/Documents/Jongwunibang/Neural_network/models/object_detection/training\" --pipline_config_path=\"/Users/jongwuni/Documents/Jongwunibang/Neural_network/models/object_detection/training/faster_rcnn_inception_resnet_v2_atrous_coco.config\"\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 106, in main\r\n    overwrite=True)\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 385, in copy\r\n    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)\r\n  File \"/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 17614, "title": "Error command in installation guild", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nThere is an small error in the [installation guild](https://www.tensorflow.org/install/install_mac#determine_which_tensorflow_to_install)\r\nwhich is 7th step under the **Installing with Virtualenv** section.\r\n\r\nThe site give an example command of installing TensorFlow in the active Virtualenv for macOS, python which is actually for py3 with pip3 command. \r\n```\r\n $ pip3 install --upgrade \\\r\n https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Added a PR #17614 for the doc fix.", "I believe this was fixed by https://github.com/tensorflow/tensorflow/commit/f4e70be18b104fbb2efeefeb83bea190aec12727\r\nbut an update to the site hasn't been pushed since.\r\n\r\n@MarkDaoust : Can we push an update?", "@asimshankar @MarkDaoust It looks like the issue was fixed by f4e70be, but later it has been overridden by 9dae88d. I created #17617 to get the fix back."]}, {"number": 17613, "title": "Error with Installing tensorflow error using Virtualenv", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code was written steps followed from the installtensorflow webpage:\r\nhttps://www.tensorflow.org/install/install_mac \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX El Capitan version 10.11.6\r\n- **TensorFlow installed from (source or binary)**: error in installing tensorflow\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**: Python 3.6 \r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: not sure\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**: MacBook Pro 13 inch(early 2011), Processor: 2.3 GHz Intel Core I5, Memory: 16GB 1333 MHz DDR3, Graphics: Intel HD Graphics 3000 512 MB\r\n- **Exact command to reproduce**:  error when using the following command \r\n                                                     $ virtualenv --system-site-packages -p python3 ~/tensorflow\r\n                                          \r\n![screen shot 2018-03-10 at 12 01 49 am](https://user-images.githubusercontent.com/28227010/37238611-23339db0-23f8-11e8-8b4c-37d743b1474a.png)\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am trying to install tensorflow for mac os El Capitan, using Virtualenv as instructed in the tensorflow installation webpage: https://www.tensorflow.org/install/install_mac these are the steps i have taken and errors recieved:\r\n\r\n1.Start a terminal (a shell). You'll perform all subsequent steps in this shell.\r\n\r\n2. Installed pip and Virtualenv sucessfully by issuing the following commands:\r\n\r\n $ sudo easy_install pip\r\n $ pip install --upgrade virtualenv \r\n\r\n3. successfully Created a Virtualenv environment by issuing a command of one of the python 3 format:\r\n\r\n $ virtualenv --system-site-packages -p python3 ~/tensorflow\r\n\r\nthe following e4 errors were displayed in terminal after step 3:\r\n\r\nERROR: The executable /Users/User/tensorflow/bin/python3 is not functioning\r\nERROR: It thinks sys.prefix is '/Users/User' (should be '/Users/User/tensorflow')\r\nERROR: virtualenv is not compatible with this system or executable\r\n\r\n\r\n4. I get an error when doing step 4 :Activate the Virtualenv environment by issuing one of the    following commands:\r\n\r\n$ cd targetDirectory\r\n$ source ./bin/activate      # If using bash, sh, ksh, or zsh\r\n$ source ./bin/activate.csh  # If using csh or tcsh \r\n\r\nthe error i ger is: -bash: ./bin/activate: No such file or directory\r\n\r\nPlease help with this problem is greatly appreciated,  I have tried 1 other times using different methods to install tensorflow and it hasn't worked. Next ill try native pip method.\r\n\r\ni have attached a terminal screenshot of the whole process:\r\n\r\n![screen shot 2018-03-10 at 12 01 49 am](https://user-images.githubusercontent.com/28227010/37238654-fb964c0c-23f8-11e8-8da0-d290523c9287.png)\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I don't believe this is a TensorFlow problem. The virtualenv command failed, which suggests your computers python install is broken, before you have done anything TensorFlow specific. Closing for now."]}, {"number": 17612, "title": "Changing parallel_iterations of dynamic_rnn doesn't affect GPU memory consumption", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary (through pip)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 1.6\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Tesla P100\r\n- **Exact command to reproduce**: Download mnist dataset to dir \"input_data/\" and run the source code provided as below\r\n\r\n\r\n### Describe the problem\r\nI'm using dynamic_rnn to train an RNN on mnist dataset. I run a simple test (code as below) to find the best parallel_iterations number. However, checking with nvidia-smi, it seems that the GPU memory consumption remains the same, which infers dynamic_rnn doesn't run in parallel.\r\n\r\nLooking forward to your reply!\r\n\r\n### Source code / logs\r\nTest Code:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers import fully_connected\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.contrib import rnn\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\ndef train(batch_size, parallel_iterations, mnist):\r\n\tlr = 1e-3\r\n\tbatch_size_holder = tf.placeholder(tf.int32) \r\n\tinput_size = 28\r\n\ttimestep_size = 28\r\n\thidden_size = 28\r\n\tlayer_num = 50\r\n\tclass_num = 10\r\n\r\n\t_X = tf.placeholder(tf.float32, [None, 784])\r\n\ty = tf.placeholder(tf.float32, [None, class_num])\r\n\tkeep_prob = tf.placeholder(tf.float32)\r\n\tX = tf.reshape(_X, [-1, 28, 28])\r\n\r\n\trnn_cell = rnn.BasicRNNCell(num_units=hidden_size)\r\n\trnn_cell = rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\r\n\tmrnn_cell = rnn.MultiRNNCell([rnn_cell] * layer_num, state_is_tuple=True)\r\n\toutputs, state = tf.nn.dynamic_rnn(mrnn_cell, inputs=X, dtype=tf.float32, time_major=False, parallel_iterations=parallel_iterations)\r\n\th_state = outputs[:, -1, :]\r\n\r\n\tW = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\r\n\tbias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\r\n\ty_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\r\n\tcross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\r\n\ttrain_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\r\n\r\n\tcorrect_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\r\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n\r\n\tsession_config = tf.ConfigProto()\r\n\tsession_config.gpu_options.allow_growth = True\r\n\r\n\twith tf.Session(config=session_config) as sess:\r\n\t\tsess.run(tf.global_variables_initializer())\r\n\t\tfor i in range(50):\r\n\t\t\tbatch = mnist.train.next_batch(batch_size)\r\n\t\t\tonehot_labels = np.eye(class_num)[batch[1]]\r\n\t\t\tsess.run(train_op, feed_dict={_X: batch[0], y: onehot_labels, keep_prob: 0.5, batch_size_holder: batch_size}, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n\r\n\ttf.reset_default_graph()\r\n\r\n\r\n\r\ndef main(_):\r\n\tmnist = input_data.read_data_sets(\"input_data/\")\r\n\r\n\tfor num in range(1,10):\r\n\t\ttrain(128, num*10, mnist)\r\n\tfor num in range(10,-1,-1):\r\n\t\tif (num >0):\r\n\t\t\ttrain(128, num*10, mnist)\r\n\r\nif __name__ == '__main__':\r\n\ttf.app.run(main=main, argv=[sys.argv[0]])\r\n\r\n\r\n```\r\n\r\nnvidia-smi result:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:03:00.0 Off |                    0 |\r\n| N/A   25C    P0    37W / 250W |    517MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |\r\n| N/A   23C    P0    37W / 250W |    359MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2893      C   python                                       507MiB |\r\n|    1      2893      C   python                                       349MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "comments": ["@shivaniag Could you please look at it and make some comments? Thanks in advance!", "@WanCC1995 sorry for delayed response, assigning to @ebrevdo, could you please take a look.", "@ebrevdo Could you please look at it and make some comments? Thanks in advance!", "Nagging Assignees @ebrevdo, @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @ebrevdo, @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "That's a side effect but not a guarantee. Typically memory consumption is\nlower bounded by the frame count of you're doing backprop.  Have you tried\nusing recompute_grad?\n\nOn Sat, Apr 21, 2018, 11:27 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignees @ebrevdo <https://github.com/ebrevdo>, @shivaniag\n> <https://github.com/shivaniag>: It has been 14 days with no activity and\n> this issue has an assignee. Please update the label and/or status\n> accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17612#issuecomment-383318482>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3_PgoYMnZ3wGKly2Ll3Ypn-2fshks5tq3oHgaJpZM4SlIO2>\n> .\n>\n", "Thanks for your reply. So the parallel_iterations param is actually setting the upper bound?\r\n\r\nBy the way, I'm not familiar with recompute_grad. I couldn't find a way to apply it.", "Nagging Assignees @ebrevdo, @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @ebrevdo, @shivaniag: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @ebrevdo, @shivaniag: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @ebrevdo, @shivaniag: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @ebrevdo, @shivaniag: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think all the questions have been answered."]}, {"number": 17611, "title": "TypeError: __new__() got an unexpected keyword argument 'output_alternatives'", "body": "### System information\r\n- OS Distribution: Linux Ubuntu 16.04.3 LTS\r\n- TensorFlow is installed from binary\r\n- TensorFlow version: v1.6.0-0-gd2e24b6039 1.6.0\r\n- Python version: 3.6.1\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GeForce GTX 850m\r\n\r\nand\r\n\r\n- OS Distribution: CentOS Linux release 7.3.1611 (Core) \r\n- TensorFlow is installed from binary\r\n- TensorFlow version: v1.6.0-0-gd2e24b6039 1.6.0\r\n- Python version: 3.6.0\r\n\r\n### Describe the problem\r\n`TypeError: __new__() got an unexpected keyword argument 'output_alternatives'` is thrown on line 611 in `_scale_tower_loss` function in tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py\r\n\r\nIt can be resolved by adding `del estimator_spec['output_alternatives']`:\r\n```def _scale_tower_loss(tower_spec, loss_reduction, number_of_towers):\r\n  \"\"\"Produce an EstimatorSpec with approproriately scaled loss.\"\"\"\r\n  if tower_spec.loss is None:\r\n    return tower_spec\r\n\r\n  estimator_spec = _asdict(tower_spec)\r\n  estimator_spec['loss'] = _scale_loss(tower_spec.loss, loss_reduction,\r\n                                       number_of_towers)\r\n  del estimator_spec['output_alternatives'] # RESOLVES THE ISSUE\r\n  return model_fn_lib.EstimatorSpec(**estimator_spec)\r\n```\r\n\r\nAfter running `grep` command, I have noticed that `output_alternatives` variable is mostly used in tensorflow/contrib/learn. My guess is that `output_alternatives`  was forgotten to be removed from the new  tf.estimator.Estimator as it is only used by the old tf.contrib.learn.Estimator.\r\n\r\n\r\n### Source code / logs\r\nUnfortunately cannot be provided.\r\n", "comments": ["tf.estimator.EstimatorSpec should be used instead of tf.contrib.learn.ModelFnOps"]}, {"number": 17610, "title": "[CMake]Exclude duplicate tests from ${tf_test_src_simple}", "body": "This fixes errors caused by adding executable targets twice.\r\n\r\nTo reproduce:\r\n(tf-venv) tedchang@teds-mbp:~/tfws/tensorflow/tensorflow/contrib/cmake/build$ cmake -DCMAKE_BUILD_TYPE=Release -Dtensorflow_BUILD_CC_TESTS=ON ..\r\nCMake Error at tf_tests.cmake:73 (add_executable):\r\n  add_executable cannot create target\r\n  \"tensorflow_core_profiler_internal_tfprof_show_test\" because another target\r\n  with the same name already exists.  The existing target is an executable\r\n  created in source directory\r\n  \"/Users/tedchang/tfws/tensorflow/tensorflow/contrib/cmake\".  See\r\n  documentation for policy CMP0002 for more details.\r\nCall Stack (most recent call first):\r\n  tf_tests.cmake:46 (AddTest)\r\n  tf_tests.cmake:527 (AddTests)\r\n  CMakeLists.txt:469 (include)\r\n\r\n\r\nCMake Error at tf_tests.cmake:73 (add_executable):\r\n  add_executable cannot create target\r\n  \"tensorflow_core_profiler_internal_tfprof_stats_test\" because another\r\n  target with the same name already exists.  The existing target is an\r\n  executable created in source directory\r\n  \"/Users/tedchang/tfws/tensorflow/tensorflow/contrib/cmake\".  See\r\n  documentation for policy CMP0002 for more details.\r\nCall Stack (most recent call first):\r\n  tf_tests.cmake:46 (AddTest)\r\n  tf_tests.cmake:527 (AddTests)\r\n  CMakeLists.txt:469 (include)\r\n\r\n\r\nCMake Error at tf_tests.cmake:73 (add_executable):\r\n  add_executable cannot create target\r\n  \"tensorflow_core_profiler_internal_tfprof_tensor_test\" because another\r\n  target with the same name already exists.  The existing target is an\r\n  executable created in source directory\r\n  \"/Users/tedchang/tfws/tensorflow/tensorflow/contrib/cmake\".  See\r\n  documentation for policy CMP0002 for more details.\r\nCall Stack (most recent call first):\r\n  tf_tests.cmake:46 (AddTest)\r\n  tf_tests.cmake:527 (AddTests)\r\n  CMakeLists.txt:469 (include)\r\n\r\n\r\nCMake Error at tf_tests.cmake:73 (add_executable):\r\n  add_executable cannot create target\r\n  \"tensorflow_core_profiler_internal_tfprof_timeline_test\" because another\r\n  target with the same name already exists.  The existing target is an\r\n  executable created in source directory\r\n  \"/Users/tedchang/tfws/tensorflow/tensorflow/contrib/cmake\".  See\r\n  documentation for policy CMP0002 for more details.\r\nCall Stack (most recent call first):\r\n  tf_tests.cmake:46 (AddTest)\r\n  tf_tests.cmake:527 (AddTests)\r\n  CMakeLists.txt:469 (include)\r\n\r\n\r\nCMake Error at tf_tests.cmake:73 (add_executable):\r\n  add_executable cannot create target\r\n  \"tensorflow_core_profiler_internal_advisor_tfprof_advisor_test\" because\r\n  another target with the same name already exists.  The existing target is\r\n  an executable created in source directory\r\n  \"/Users/tedchang/tfws/tensorflow/tensorflow/contrib/cmake\".  See\r\n  documentation for policy CMP0002 for more details.\r\nCall Stack (most recent call first):\r\n  tf_tests.cmake:46 (AddTest)\r\n  tf_tests.cmake:527 (AddTests)\r\n  CMakeLists.txt:469 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\n", "comments": ["@mrry How many CI tests do you normally run for CMake related PR?  Why is a Windows CI test not executed since CMake is mainly for building TF on Windows.", "I believe we just run the Python tests for the Windows/CMake build, because it takes too long to build all the C++ tests."]}, {"number": 17609, "title": "The code in google_auth_provider.cc looks for application default credentials in an obsolete location.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 4.9.65\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.6.0-rc1-933-gf7acdf2ed5', '1.7.0-dev20180305')\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```sh\r\ngcloud auth activate-service-account --file ${CREDENTIALS_FILE}\r\nls ~/.config/gcloud/application_default_credentials.json\r\n```\r\n\r\n### Describe the problem\r\nThis is a bug where the code for reading Google credentials written by the `gcloud` tool no longer matches the behavior of the `gcloud` tool (the tool changed its behavior since this code was written).\r\n\r\nSpecifically, the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/google_auth_provider.cc#L97) tries to read credentials written by `gcloud` to a well-known location of \"~/.config/gcloud/application_default_credentials.json\".\r\n\r\nHowever, `gcloud` does not write credentials to that location anymore.\r\n\r\nMoreover, the Cloud SDK (of which `gcloud` is one component) provides no guarantees about the stability of where/how it stores credentials.\r\n\r\nInstead, tools using these credentials are expected to use the `gcloud` tool to produce them by running:\r\n\r\n```sh\r\ngcloud config config-helper --format \"value(credential)\"\r\n```", "comments": ["@chrisying  can you please take a look? Feel free to forward it any gcloud guy.\r\nThanks.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe the correct way to set up application_default_credentials is with https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login instead of activate-service-account.\r\n\r\n```\r\ngcloud auth application-default login\r\nls ~/.config/gcloud/application_default_credentials.json\r\n```\r\n\r\nClosing this issue for now."]}, {"number": 17608, "title": "Feature request: add easy way to install tensorflow as one package in win", "body": " adding easy way to install tensorflow as one package in windows .\r\nno one should stay 4 hours to do that!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@virtual-bug could you elaborate a little on your request, please? Do you want a .msi file for Windows that wraps TF? Or some other installer? ", "Closing for lack of response; please reopen if you've got more information about the request."]}]