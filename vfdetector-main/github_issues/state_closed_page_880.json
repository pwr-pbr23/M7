[{"number": 27089, "title": "Update tf.function doc: mention that `while` is captured, and explain\u2026", "body": "\u2026 how python numerical values are handled", "comments": []}, {"number": 27088, "title": "failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED", "body": "Hi, \r\n\r\nI am a on a Ubuntu system, installed tensorflow using conda install tensorflow-gpu, have cuda 9.0.176. I am getting the error: \r\n\r\n> failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n\r\nI tried the solutions:\r\n\r\n`rm -rf .nv/`\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config....)\r\n\r\n```\r\nBoth are not helping resolve the issue. What could be the error source? Any more possible solutions to try?\r\n\r\nThanks!\r\n\r\n```\r\n2019-03-24 20:18:47.549553: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA\r\n2019-03-24 20:18:47.718642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.91GiB freeMemory: 10.28GiB\r\n2019-03-24 20:18:47.718677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2019-03-24 20:18:48.009540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9950 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-03-24 20:19:03.125607: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-03-24 20:19:15.413499: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-03-24 20:19:27.701539: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-03-24 20:19:39.989549: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-03-24 20:19:52.277482: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_IThis template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\nNITIALIZED\r\n2019-03-24 20:20:04.565579: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-03-24 20:20:16.853469: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-03-24 20:20:29.141579: E tensorflow/stream_executor/cuda/cuda_blas.cc:443] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n\r\n```", "comments": ["rm -rf ~/.nv\r\nand try\r\ngpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\r\nconfig=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\ntf.keras.backend.set_session(sess)"]}, {"number": 27087, "title": "`tk.layers.CuDNNLSTM` is not compatible with `tk.layers.StackedRNNCells`", "body": "I follow the official document and write the following code to create a stack of RNN cells using Tensorflow 1.13 on GPU\r\n\r\n```python\r\nimport tensorflow.keras as tk\r\n\r\nlstm = [tk.layers.CuDNNLSTM(128) for _ in range(2)]\r\ncells = tk.layers.StackedRNNCells(lstm)\r\n```\r\nHowever, I receive the error message:\r\n\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-30-1b47a86eb8ab> in <module>\r\n>       2 # x = tf.placeholder(tf.float32, (4, 100, 128))\r\n>       3 lstm = [tk.layers.CuDNNLSTM(128) for _ in range(2)]\r\n> ----> 4 lstm = tk.layers.StackedRNNCells(lstm)\r\n> \r\n> ~/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py in __init__(self, cells, **kwargs)\r\n>      74         raise ValueError('All cells must have a '\r\n>      75                          '`state_size` attribute. '\r\n> ---> 76                          'received cells:', cells)\r\n>      77     self.cells = cells\r\n>      78     # reverse_state_order determines whether the state size will be in a reverse\r\n> \r\n> ValueError: ('All cells must have a `state_size` attribute. received cells:', [<tensorflow.python.keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x13aa1c940>])\r\n\r\nIs this a bug, or do I do it wrong?", "comments": ["One of the reasons is that the ```tf.keras.layers.CuDNNLSTM``` has been removed in 2.0 from ```tf.keras```. I suggest you must use ```tf.keras.layers.MultiRNNCell``` for now. But keep in mind that both **CuDNNLSTM** and **MultiRNNCell** has been removed in future versions, thats why the **StackedRNNCell** does not support it.\r\n ", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Sorry for responding late. I've posted it at [Stackoverflow](https://stackoverflow.com/questions/55324307/how-to-implement-a-stacked-rnns-in-tensorflow), and someone told me it is a bug and suggested me to ask it here...Which class is recommended to use then if I want to get the optimized performance on GPU?\r\n", "I've followed the [TF2.0 document](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM), and wrote the code\r\n```python\r\ntf.reset_default_graph()\r\nx = tf.placeholder(tf.float32, [None, 5, 10])\r\nlstm = tk.layers.LSTM(10, return_sequences=True, return_state=True)\r\n# lstm = tk.layers.StackedRNNCells([lstm])\r\ninitial_state = lstm.get_initial_state(x)\r\ny, h, c = lstm(x, initial_state=initial_state)\r\n```\r\nIf I uncomment line4, the same error occurs. I've also tried using `LSTMCell`, but it cannot return sequences. And it somehow requires a 2-D tensor as input? I'd be appreciated for any helps. \r\nPs. I'm using TF 1.13", "I think this is working as intended. Either StackedRNNCells or StackedRNNCells only works with Cell, not layer. The difference between the cell and layer in RNN is that cell will only process one time step within the whole sequence, whereas the layer will process the whole sequence. You can treat RNN layer as:\r\n```\r\nfor t in whole_time_steps:\r\n  output_t, state_t = cell(input_t, state_t-1)\r\n```\r\n\r\nIf you want to stack 2 LSTM layers to together with cudnn in 1.x, you can do:\r\n\r\n```\r\nl1 = tf.layers.CuDNNLSTM(128, return_sequence=True)\r\nl2 = tf.layers.CuDNNLSTM(128)\r\nl1_output = l1(input)\r\nl2_oupput = l2(l1_output) \r\n```\r\n\r\nIn tf 2.x, we unify the cudnn and normal implementation together, you can just change the example above with tf.layers.LSTM(128, return_sequence=True), which will use the cudnn impl if available.\r\n"]}, {"number": 27086, "title": "Tensorflow 2.0: Adding @tf.function decorator with categorical feature_column raises FailedPreconditionError: Table already initialized", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nJust want to preface this by saying while strictly speaking I am using 'custom' code, all I did was piece together code from the 'classify structured data' and the 'introduction for experts'.\r\n\r\nI am trying to add @tf.function for performance enhancement reasons to my custom training code in Tensorflow 2.0. However, running the code raises a FailedPreconditionError: Table already initialized. when using categorical feature columns.\r\n\r\nWhat I suspect is happening is that the lookup tables for the categorical encoding are being initialized more than once (in the model definition and in @tf.function).  How should I go about using feature_columns and @tf.function?\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n!pip install sklearn\r\n!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Model\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target').values.reshape(-1,1)\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_columns = []\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n\r\n# bucketized cols\r\nage = feature_column.numeric_column(\"age\")\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator cols\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\n\r\n# embedding cols\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nfeature_columns.append(thal_embedding)\r\n\r\n#crossed cols\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\nfeature_columns.append(crossed_feature)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns, trainable=False)\r\n\r\n\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.features = feature_layer\r\n    self.dense = layers.Dense(128, activation = 'relu')\r\n    self.dense2 = layers.Dense(128, activation = 'relu')\r\n    self.sigmoid = layers.Dense(1, activation = 'sigmoid')\r\n\r\n  def call(self, x):\r\n    x = self.features(x)\r\n    x = self.dense(x)\r\n    x = self.dense2(x)\r\n    return self.sigmoid(x)\r\n\r\nmodel = MyModel()\r\nloss_object = tf.keras.losses.BinaryCrossentropy()\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\r\n\r\n@tf.function\r\ndef train_step(features, label, counter):\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(features)\r\n    loss = loss_object(label, predictions)\r\n  gradients = tape.gradient(loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n  train_loss(loss)\r\n  train_accuracy(label, predictions)\r\n\r\n@tf.function\r\ndef test_step(image, label):\r\n  predictions = model(image)\r\n  t_loss = loss_object(label, predictions)\r\n\r\n  test_loss(t_loss)\r\n  test_accuracy(label, predictions)\r\n\r\nEPOCHS = 5\r\nfor epoch in range(EPOCHS):\r\n  counter = 0\r\n  for features, labels in train_ds:\r\n    train_step(features, labels, counter)\r\n    counter +=1\r\n\r\n  for features, labels in val_ds:\r\n      test_step(features, labels)\r\n\r\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n  print (template.format(epoch+1,\r\n                           train_loss.result(), \r\n                           train_accuracy.result()*100,\r\n                           test_loss.result(), \r\n                           test_accuracy.result()*100))\r\n```\r\n\r\n**Actual Output**\r\n```\r\nEpoch 1, Loss: 1.2603175640106201, Accuracy: 61.65803146362305, Test Loss: 1.3003877401351929, Test Accuracy: 67.34693908691406\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-3-165a6f89be48> in <module>()\r\n    110   counter = 0\r\n    111   for features, labels in train_ds:\r\n--> 112     train_step(features, labels, counter)\r\n    113     counter +=1\r\n    114 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    412       # In this case we have created variables on the first call, so we run the\r\n    413       # defunned version which is guaranteed to never create variables.\r\n--> 414       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    415     elif self._stateful_fn is not None:\r\n    416       # In this case we have not created variables on the first call. So we can\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1286     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1287     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1288     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1289 \r\n   1290   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    572     \"\"\"\r\n    573     return self._call_flat(\r\n--> 574         (t for t in nest.flatten((args, kwargs))\r\n    575          if isinstance(t, (ops.Tensor,\r\n    576                            resource_variable_ops.ResourceVariable))))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    625     # Only need to override the gradient in graph mode and when we have outputs.\r\n    626     if context.executing_eagerly() or not self.outputs:\r\n--> 627       outputs = self._inference_function.call(ctx, args)\r\n    628     else:\r\n    629       self._register_gradient()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    413             attrs=(\"executor_type\", executor_type,\r\n    414                    \"config_proto\", config),\r\n--> 415             ctx=ctx)\r\n    416       # Replace empty list with None\r\n    417       outputs = outputs or None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError: Table already initialized.\r\n\t [[{{node my_model_2/dense_features_2/age_bucketized_X_thal_indicator/thal_lookup/hash_table/table_init/LookupTableImportV2}}]] [Op:__inference_train_step_34833]\r\n```\r\n\r\n**Expected Output**\r\n```\r\nEpoch 1, Loss: 0.8113343119621277, Accuracy: 63.21243667602539, Test Loss: 0.5340840816497803, Test Accuracy: 71.42857360839844\r\nEpoch 2, Loss: 0.6469629406929016, Accuracy: 69.4300537109375, Test Loss: 0.5265070199966431, Test Accuracy: 72.44898223876953\r\nEpoch 3, Loss: 0.5749971270561218, Accuracy: 71.84800720214844, Test Loss: 0.5283268094062805, Test Accuracy: 72.10884094238281\r\nEpoch 4, Loss: 0.5360371470451355, Accuracy: 72.79792785644531, Test Loss: 0.5270806550979614, Test Accuracy: 72.44898223876953\r\nEpoch 5, Loss: 0.5122867226600647, Accuracy: 73.57512664794922, Test Loss: 0.5229357481002808, Test Accuracy: 72.65306091308594\r\n```", "comments": ["@alextp  - how to combine @tf.function with feature_columns ?", "@rohan100jain I thought you had already fixed this issue?", "Thank you for responding!  \r\nBtw interesting enough I just added a counter - and the model managed to run for one epoch before crashing ...  \r\n\r\nEDIT (Just updated the code in the issue to show the behavior with the counter added...)", "Ok! the code below solves the problem (adding a dummy counter for both training and testing steps ...)- but I am not sure why - thoughts @rohan100jain ?\r\n\r\n(Not incrementing the counter i.e. passing a fixed value - causes the error to be rasied again)\r\nseems to me like counter is acting like an incrementing global step which tells the model not to initialize the lookup tables again ... \r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n!pip install sklearn\r\n!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Model\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target').values.reshape(-1,1)\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_columns = []\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n\r\n# bucketized cols\r\nage = feature_column.numeric_column(\"age\")\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator cols\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\n\r\n# embedding cols\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nfeature_columns.append(thal_embedding)\r\n\r\n#crossed cols\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\nfeature_columns.append(crossed_feature)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns, trainable=False)\r\n\r\n\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.features = feature_layer\r\n    self.dense = layers.Dense(128, activation = 'relu')\r\n    self.dense2 = layers.Dense(128, activation = 'relu')\r\n    self.sigmoid = layers.Dense(1, activation = 'sigmoid')\r\n\r\n  def call(self, x):\r\n    x = self.features(x)\r\n    x = self.dense(x)\r\n    x = self.dense2(x)\r\n    return self.sigmoid(x)\r\n\r\nmodel = MyModel()\r\nloss_object = tf.keras.losses.BinaryCrossentropy()\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\r\n\r\n@tf.function\r\ndef train_step(features, label, counter):\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(features)\r\n    loss = loss_object(label, predictions)\r\n  gradients = tape.gradient(loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n  train_loss(loss)\r\n  train_accuracy(label, predictions)\r\n\r\n@tf.function\r\ndef test_step(image, label, counter):\r\n  predictions = model(image)\r\n  t_loss = loss_object(label, predictions)\r\n\r\n  test_loss(t_loss)\r\n  test_accuracy(label, predictions)\r\n\r\nEPOCHS = 5\r\ncounter = 0\r\nfor epoch in range(EPOCHS):\r\n  \r\n  for features, labels in train_ds:\r\n    train_step(features, labels, counter)\r\n    counter +=1\r\n    \r\n  for features, labels in val_ds:\r\n    test_step(features, labels, counter)\r\n    counter +=1\r\n\r\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n  print (template.format(epoch+1,\r\n                           train_loss.result(), \r\n                           train_accuracy.result()*100,\r\n                           test_loss.result(), \r\n                           test_accuracy.result()*100))\r\n  \r\n````\r\n\r\n", "@rohan100jain did you have time to take a look?", "https://github.com/tensorflow/tensorflow/commit/f6e0ec468a3dee7c2ae1fa3575bb46d884b4a319 should hopefully fix this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27086\">No</a>\n"]}, {"number": 27085, "title": "initialize_or_restore() executed on InitializationOnlyStatus returned by tf.train.Checkpoint.restore fails to initialize custom tf.keras.layers.Layer trainables", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, provided below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): various\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nIn the self-contained test below, I am expecting `status.initialize_or_restore(session)` to initialize `my_dense/dense/kernel` and `my_dense/dense/bias`, which should be checkpointable, but it does not, as can be seen from the error message.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe test should execute normally.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyDense(tf.keras.layers.Layer):\r\n    def __init__(self, units):\r\n        super(MyDense, self).__init__()\r\n        self.units = units\r\n\r\n    def build(self, input_shape):\r\n        self.dense = tf.keras.layers.Dense(units=self.units)\r\n\r\n    def call(self, inputs, mask=None):\r\n        return self.dense(inputs)\r\n\r\ninputs = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=MyDense(units=1)(inputs))\r\n\r\ncheckpoint = tf.train.Checkpoint(model=model)\r\nmanager = tf.train.CheckpointManager(checkpoint, directory='./', max_to_keep=None)  # no checkpoint present\r\nstatus = checkpoint.restore(manager.latest_checkpoint)\r\n\r\ninputs_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 1])\r\ncalled_model = model(inputs_placeholder)\r\n\r\nwith tf.Session() as session:\r\n    status.initialize_or_restore(session)\r\n    session.run(called_model, feed_dict={inputs_placeholder: np.array([[1.]])})\r\n```\r\n\r\n**Other info / logs**\r\n\r\nWARNING:tensorflow:From /venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-03-24 16:09:11.964524: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable my_dense/dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/my_dense/dense/kernel/N10tensorflow3VarE does not exist.\r\n\t [[{{node model/my_dense/dense/MatMul/ReadVariableOp}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/initialize_or_restore.py\", line 27, in <module>\r\n    session.run(called_model, feed_dict={inputs_placeholder: np.array([[1.]])})\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable my_dense/dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/my_dense/dense/kernel/N10tensorflow3VarE does not exist.\r\n\t [[node model/my_dense/dense/MatMul/ReadVariableOp (defined at /initialize_or_restore.py:13) ]]\r\n\r\nCaused by op 'model/my_dense/dense/MatMul/ReadVariableOp', defined at:\r\n  File \"/initialize_or_restore.py\", line 23, in <module>\r\n    called_model = model(inputs_placeholder)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 815, in call\r\n    mask=masks)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1002, in _run_internal_graph\r\n    output_tensors = layer.call(computed_tensor, **kwargs)\r\n  File \"/initialize_or_restore.py\", line 13, in call\r\n    return self.dense(inputs)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 975, in call\r\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5333, in mat_mul\r\n    name=name)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 511, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1175, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1222, in _dense_var_to_tensor\r\n    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1177, in _dense_var_to_tensor\r\n    return self.value()\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 644, in value\r\n    return self._read_variable_op()\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 728, in _read_variable_op\r\n    self._dtype)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 550, in read_variable_op\r\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): Error while reading resource variable my_dense/dense/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/my_dense/dense/kernel/N10tensorflow3VarE does not exist.\r\n\t [[node model/my_dense/dense/MatMul/ReadVariableOp (defined at /initialize_or_restore.py:13) ]]\r\n\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["Layer only started tracking its sub-Layers like Network/Model in https://github.com/tensorflow/tensorflow/commit/e0f8b4cea2486cae671c60cc7645913bea1abf60\r\n\r\nSo the issue is that the variables aren't tracked. Can you use tf.keras.Model instead? Or try a nightly and it should work.", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!"]}, {"number": 27084, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "Just cannot import from tensorflow.contrib in 1.13.xx, where do the packages be moved to? Thanks\r\n", "comments": ["Try to reinstall your tensorflow (prefer higher versions while installing it again).", "Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27084\">No</a>\n", "I have the same problem...tried reinstall but not working"]}, {"number": 27083, "title": "Build Failure: Cannot link XLA allocator with CUDA 10.1 and GCC 7.2.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLInux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: system install to `/usr`\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): GCC 7.2.1\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nCompiling Tensorflow results in failing the final linking step when linking the XLA GPU fusion instruction allocators.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\nexport PYTHON_BIN_PATH=/usr/bin/python\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_AWS=0\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_S3=0\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_TENSORRT=0\r\nexport TF_NEED_NGRAPH=0\r\nexport TF_NEED_IGNITE=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_DOWNLOAD_CLANG=0\r\nexport TF_NCCL_VERSION=2.3\r\nexport NCCL_INSTALL_PATH=/usr\r\nexport CC_OPT_FLAGS=\"-march=x86-64\"\r\nexport TF_NEED_CUDA=1\r\nexport GCC_HOST_COMPILER_PATH=/usr/bin/gcc-7\r\nexport TF_CUDA_CLANG=0\r\nexport CUDA_TOOLKIT_PATH=/opt/cuda\r\nexport TF_CUDA_VERSION=$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\r\nexport CUDNN_INSTALL_PATH=/usr/lib\r\nexport TF_CUDNN_VERSION=$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' /usr/include/cudnn.h)\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.0,5.2,5.3,6.0,6.1,6.2,7.0,7.2,7.5\r\n./configure\r\nbazel \\\r\n    build --config=opt \\\r\n      //tensorflow:libtensorflow.so \\\r\n      //tensorflow:libtensorflow_cc.so \\\r\n      //tensorflow:install_headers \\\r\n      //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package ${srcdir}/tmp\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```bash\r\nERROR: /build/tensorflow/src/tensorflow-1.13.1-cuda/tensorflow/BUILD:463:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1)\r\n/usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/gpu/libinstruction_fusion.pic.a(instruction_fusion.pic.o): in function `xla::gpu::GpuInstructionFusion::FusionWouldBeTooLarge(xla::HloInstruction const*, xla::HloInstruction const*)':\r\ninstruction_fusion.cc:(.text._ZN3xla3gpu20GpuInstructionFusion21FusionWouldBeTooLargeEPKNS_14HloInstructionES4_+0x7c): undefined reference to `std::allocator<xla::HloInstruction const*>::allocator()'\r\ncollect2: error: ld returned 1 exit status\r\nINFO: Elapsed time: 4415.531s, Critical Path: 2250.52s\r\nINFO: 15052 processes: 15052 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Potentially related: https://github.com/tensorflow/tensorflow/pull/26230", "Redirecting to Tatiana for triage.", "Hi Konstantin,\r\n\r\nAre you still interested in getting this fixed?  If yes, it might be best if you send a PR (and assign it to me for review) since that way I don't have to figure out how to reproduce your build environment.", "Newer versions solved it, so I don't particularly need a specific solution to this. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27083\">No</a>\n"]}, {"number": 27082, "title": "Win10: ImportError: DLL load failed: The specified module could not be found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 pro\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12 GPU\r\n- Python version: 3.6.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA 9.0 and cuDNN is  cudnn-9.0-windows10-x64-v7.1 (7.1.4)\r\n- GPU model and memory: NVIDIA GeForce 940MX /  2GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am facing following issue when I try to import tensorflow on python console \r\n\r\nimport tensorflow as tf\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I meet the same error.", "I also have this error", "I have this same error with Python 3.7", "You need to add sub folders: ```bin``` , ```include``` and ```lib``` to the path. Can you please try adding them?", "I have the same problem as @ajayshilwant \r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution: Windows 10 Pro\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1 GPU\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA 10.1 and cudnn-10.1-windows10-x64-v7.5.0.56\r\n- GPU model and memory: GeForce GTX 1050 / 2 GB\r\n\r\n**Describe the problem**\r\nI'm not been able to import Tensorflow on Python console\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\miguelangel\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\miguelangel\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\PythonEnv\\macv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\miguelangel\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\miguelangel\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "@Brezloft You can stick with Python 3.6 since you are using TF 1.12. Were you able to get it running?\r\n@miguelangelcv In your case you need to lower your cuda version to 10.0 (currently you are using 10.1) Find cuda 10.0 toolkit version [here](https://developer.nvidia.com/cuda-toolkit-archive). Thanks!", "@ymodak thanks for your reply. Now it's working! :D", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27082\">No</a>\n"]}, {"number": 27081, "title": "Can't create non-trainable keras variables", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n`tf.GIT_VERSION, tf.VERSION == v1.13.1-0-g6612da8951, 1.13.1`\r\n\r\n**Describe the current behavior**\r\n\r\nYou should be able to specify `K.variable(..., trainable=False)` to create a non-trainable variable. Currently, I'm having to do:\r\n\r\n```python\r\nimport tensorflow.keras.backend as K\r\n\r\nvariable = K.variable(5)\r\nvariable._trainable = False\r\n...\r\n```\r\n\r\nBut this doesn't seem like a proper/safe way to do this.\r\n\r\n**Describe the expected behavior**\r\n```python\r\nvariable = K.variable(5, trainable=False)\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 27080, "title": "Add soversion to libraries", "body": "Previously the libs were just \"libtensorflow.so\" without any version.\r\nThis adds the version to the library (eg libtensorflow.so.1.13.0) and\r\nadds the appropriate symlinks to the full name from the base and\r\nsoversion.\r\n\r\nThe soname is used by compilers to fill in the DT_NEEDED section in the\r\nheader of binaries that link to the library. Having the version means\r\nthat different versions of the library are able to co-exist and if the\r\nABI changes, programs linking to the lib do not break.\r\n\r\nFor more info see:\r\nhttps://www.debian.org/doc/debian-policy/ch-sharedlibs.html\r\nhttps://autotools.io/libtool/version.html\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nThis was previously merged in\r\nhttps://github.com/tensorflow/tensorflow/pull/22797\r\nbut then an issue with the pip wheel was found so reverted in\r\nhttps://github.com/tensorflow/tensorflow/commit/52bd1737373cb3e64fcbc0c64d7a5b1fe62ef6b6\r\n\r\nThe libtensorflow_framework.so libs were missing in the build_pip_package deps\r\nand in the MANIFEST.in file so they were not included in the wheel.\r\n\r\nThe single patch is also now split into separate patches for enabling the per-OS libs for tensorflow_framework and for the versioning.", "comments": ["The diff to the previous PR (22797) is:\r\n\r\n```\r\ndiff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex 992d1390f1..aea76acb3a 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -1912,6 +1912,7 @@ def tf_py_wrap_cc(\r\n         linkopts = extra_linkopts,\r\n         linkstatic = 1,\r\n         deps = deps + extra_deps,\r\n+        data = tf_binary_additional_srcs() + tf_binary_additional_srcs(fullversion=True),\r\n         **kwargs\r\n     )\r\n     native.genrule(\r\ndiff --git a/tensorflow/tools/pip_package/MANIFEST.in b/tensorflow/tools/pip_package/MANIFEST.in\r\nindex c304e8cf6e..a2f577c0b9 100644\r\n--- a/tensorflow/tools/pip_package/MANIFEST.in\r\n+++ b/tensorflow/tools/pip_package/MANIFEST.in\r\n@@ -3,6 +3,8 @@ recursive-include * *.py\r\n recursive-include * *.pyd\r\n recursive-include * *.pd\r\n recursive-include * *.so\r\n+recursive-include * *.so.*\r\n+recursive-include * *.dylib\r\n recursive-include * *.dll\r\n recursive-include * *.lib\r\n recursive-include * *.csv\r\n```", "This received wide approval in #22797, so it should be okay to hurry this PR along. I'll run tests to double-check it works for me internally, too.", "@perfinion please resolve conflicts", "@rthadur No problem, I've handled them internally. I'm talking with @perfinion about the rest of the conflicts."]}, {"number": 27079, "title": "Why this error on tensorflow 1.13.1 with python 2.7 : ImportError: No module named model_utils", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 2.7\r\n- GCC/Compiler version (if compiling from source): 7.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 2080Ti 11G\r\n\r\n\r\nYou can collect some of this information using our environment capture [script]\r\n**Describe the current behavior**\r\nJust got the import error\r\n**Describe the expected behavior**\r\nShould be able to run the code smoothly\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"official_tensorflow_phased_lstm.py\", line 6, in <module>\r\n    import tensorflow.contrib.slim as slim\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/__init__.py\", line 73, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 62, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu import tpu\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 24, in <module>\r\n    from tensorflow.contrib.compiler import xla\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/compiler/xla.py\", line 28, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/__init__.py\", line 26, in <module>\r\n    from tensorflow_estimator.python import estimator\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 22, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/canned/baseline.py\", line 64, in <module>\r\n    from tensorflow_estimator.python.estimator import estimator\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 66, in <module>\r\n    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/model_fn.py\", line 36, in <module>\r\n    from tensorflow_estimator.python.estimator.export import export_lib\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/export/export_lib.py\", line 25, in <module>\r\n    from tensorflow.python.saved_model.model_utils import build_all_signature_defs\r\nImportError: No module named model_utils\r\n\r\n", "comments": ["Same problem here..", "Same here with TF 1.13.1. Installed object_detection following https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md. Error shows when running model_builder_test.py.", "Same problem here with TF 1.13.1, Linux Ubuntu 18.04, Python 3.6 CUDA 10.0.", "Same problem. TF 1.13.1, SwagArch GNU/Linux 4.20.12\r\nComes from the tensorflow_probability import", "Same here, when running Apache Beam pipelines:\r\n\r\nTB:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"my_script.py\", line 18, in <module>\r\n    import tensorflow_transform as tft\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_transform/__init__.py\", line 19, in <module>\r\n    from tensorflow_transform.analyzers import *\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_transform/analyzers.py\", line 39, in <module>\r\n    from tensorflow_transform import tf_utils\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_transform/tf_utils.py\", line 24, in <module>\r\n    from tensorflow.contrib.proto.python.ops import encode_proto_op\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/distribute/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/tpu/__init__.py\", line 73, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 62, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu import tpu\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 24, in <module>\r\n    from tensorflow.contrib.compiler import xla\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/contrib/compiler/xla.py\", line 28, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow/python/estimator/__init__.py\", line 26, in <module>\r\n    from tensorflow_estimator.python import estimator\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py\", line 8, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 22, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py\", line 64, in <module>\r\n    from tensorflow_estimator.python.estimator import estimator\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 66, in <module>\r\n    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/model_fn.py\", line 36, in <module>\r\n    from tensorflow_estimator.python.estimator.export import export_lib\r\n  File \"/anaconda3/envs/beam/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/export/export_lib.py\", line 25, in <module>\r\n    from tensorflow.python.saved_model.model_utils import build_all_signature_defs\r\nImportError: No module named model_utils\r\n\r\n```", "The same with Python 3.7.1:\r\n\r\n```\r\nIn [1]: import tensorflow as tf                                                                                                          \r\n\r\nIn [2]: tf.__version__                                                                                                                   \r\nOut[2]: '1.13.1'\r\n\r\nIn [3]: tf.contrib.tensor_forest.python.tensor_forest.ForestHParams                                                                      \r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-3-ff9aa14a958c> in <module>\r\n----> 1 tf.contrib.tensor_forest.python.tensor_forest.ForestHParams\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py in __getattr__(self, item)\r\n     59 \r\n     60   def __getattr__(self, item):\r\n---> 61     module = self._load()\r\n     62     return getattr(module, item)\r\n     63 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py in _load(self)\r\n     42     \"\"\"Load the module and insert it into the parent's globals.\"\"\"\r\n     43     # Import the target module and insert it into the parent's namespace\r\n---> 44     module = importlib.import_module(self.__name__)\r\n     45     self._parent_module_globals[self._local_name] = module\r\n     46 \r\n\r\n/opt/anaconda3/lib/python3.7/importlib/__init__.py in import_module(name, package)\r\n    125                 break\r\n    126             level += 1\r\n--> 127     return _bootstrap._gcd_import(name[level:], package, level)\r\n    128 \r\n    129 \r\n\r\n/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py in _gcd_import(name, package, level)\r\n\r\n/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\n/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\n/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py in _load_unlocked(spec)\r\n\r\n/opt/anaconda3/lib/python3.7/importlib/_bootstrap_external.py in exec_module(self, module)\r\n\r\n/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/__init__.py in <module>\r\n     38 from tensorflow.contrib import data\r\n     39 from tensorflow.contrib import deprecated\r\n---> 40 from tensorflow.contrib import distribute\r\n     41 from tensorflow.contrib import distributions\r\n     42 from tensorflow.contrib import estimator\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/distribute/__init__.py in <module>\r\n     31 from tensorflow.contrib.distribute.python.parameter_server_strategy import ParameterServerStrategy\r\n     32 from tensorflow.contrib.distribute.python.step_fn import *\r\n---> 33 from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n     34 from tensorflow.python.distribute.cross_device_ops import *\r\n     35 from tensorflow.python.distribute.distribute_config import DistributeConfig\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py in <module>\r\n     25 import functools\r\n     26 \r\n---> 27 from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n     28 from tensorflow.contrib.tpu.python.tpu import tpu\r\n     29 from tensorflow.contrib.tpu.python.tpu import tpu_system_metadata as tpu_system_metadata_lib\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/tpu/__init__.py in <module>\r\n     71 from tensorflow.contrib.tpu.python.tpu.bfloat16 import *\r\n     72 from tensorflow.contrib.tpu.python.tpu.device_assignment import *\r\n---> 73 from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n     74 from tensorflow.contrib.tpu.python.tpu.keras_support import TPUDistributionStrategy\r\n     75 from tensorflow.contrib.tpu.python.tpu.topology import *\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in <module>\r\n     60 from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n     61 from tensorflow.contrib.tpu.python.tpu import keras_tpu_variables\r\n---> 62 from tensorflow.contrib.tpu.python.tpu import tpu\r\n     63 from tensorflow.contrib.tpu.python.tpu import tpu_function\r\n     64 from tensorflow.contrib.tpu.python.tpu import tpu_optimizer\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py in <module>\r\n     22 from six.moves import xrange  # pylint: disable=redefined-builtin\r\n     23 \r\n---> 24 from tensorflow.contrib.compiler import xla\r\n     25 from tensorflow.contrib.framework.python.framework import experimental\r\n     26 from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/contrib/compiler/xla.py in <module>\r\n     26 from tensorflow.compiler.jit.ops import xla_ops_grad  # pylint: disable=unused-import\r\n     27 from tensorflow.core.framework import attr_value_pb2\r\n---> 28 from tensorflow.python.estimator import model_fn as model_fn_lib\r\n     29 from tensorflow.python.framework import ops\r\n     30 from tensorflow.python.ops import array_ops\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/estimator/__init__.py in <module>\r\n     24 from __future__ import print_function\r\n     25 \r\n---> 26 from tensorflow_estimator.python import estimator\r\n     27 \r\n     28 # Include attrs that start with single underscore.\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/__init__.py in <module>\r\n     23 from __future__ import print_function\r\n     24 \r\n---> 25 import tensorflow_estimator.python.estimator.estimator_lib\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py in <module>\r\n     20 \r\n     21 # pylint: disable=unused-import,line-too-long,wildcard-import\r\n---> 22 from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier\r\n     23 from tensorflow_estimator.python.estimator.canned.baseline import BaselineEstimator\r\n     24 from tensorflow_estimator.python.estimator.canned.baseline import BaselineRegressor\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py in <module>\r\n     62 from tensorflow.python.training import training_util\r\n     63 from tensorflow.python.util.tf_export import estimator_export\r\n---> 64 from tensorflow_estimator.python.estimator import estimator\r\n     65 from tensorflow_estimator.python.estimator.canned import head as head_lib\r\n     66 from tensorflow_estimator.python.estimator.canned import optimizers\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in <module>\r\n     64 from tensorflow.python.util import nest\r\n     65 from tensorflow.python.util.tf_export import estimator_export\r\n---> 66 from tensorflow_estimator.python.estimator import model_fn as model_fn_lib\r\n     67 from tensorflow_estimator.python.estimator import run_config\r\n     68 from tensorflow_estimator.python.estimator import util as estimator_util\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/model_fn.py in <module>\r\n     34 from tensorflow.python.util import nest\r\n     35 from tensorflow.python.util.tf_export import estimator_export\r\n---> 36 from tensorflow_estimator.python.estimator.export import export_lib\r\n     37 from tensorflow_estimator.python.estimator.mode_keys import ModeKeys\r\n     38 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/export/export_lib.py in <module>\r\n     23 \r\n     24 # pylint: disable=unused-import,line-too-long, wildcard-import\r\n---> 25 from tensorflow.python.saved_model.model_utils import build_all_signature_defs\r\n     26 from tensorflow.python.saved_model.model_utils import export_outputs_for_mode\r\n     27 from tensorflow.python.saved_model.model_utils import EXPORT_TAG_MAP\r\n\r\nModuleNotFoundError: No module named 'tensorflow.python.saved_model.model_utils'\r\n```", "Same problem here with TF 1.13.1, Windows 10, Python 3.6 CUDA 10.0.\r\n\r\nThe problem appeared after I downgraded from the 2.0 alpha back to 1.13.", "This setup works for me (Ubuntu 16.04):\r\n\r\nPython 2.7.12\r\nCuda 10.0 (Driver 418.56)\r\ncuDNN 7.4.2\r\ntensorflow-gpu 1.13.0rc2\r\n\r\nobject_detection/builders/model_builder_test.py passes (with a warning about contrib being removed in TF 2.0), quick start tutorial also works.\r\n", "@ChaoYue0307  @mastedm I am using Python 3.6.5 and TF 1.13.1 (cpu) I was able to import following modules successfully.\r\n```python\r\nimport tensorflow.contrib.slim as slim\r\ntf.contrib.tensor_forest.python.tensor_forest.ForestHParams\r\n```\r\nCan you please try executing your code in virtual env?", "Same problem with TF 1.13.1 and master branch of \"tensorflow/models\"", "I was able to resolve the issue by blowing away all of my pip packages and reinstalling.", "@LucasSloan How did you do that? Here is my output of pip freeze --local\r\nKeras-Applications==1.0.6\r\nKeras-Preprocessing==1.0.5\r\nmock==2.0.0\r\nnumpy==1.16.2\r\nobject-detection==0.1\r\npbr==5.1.3\r\ntb-nightly==1.13.0a20190225\r\ntensorboard==1.13.1\r\ntensorflow==1.13.1\r\ntensorflow-estimator==1.13.0\r\ntf-estimator-nightly==1.14.0.dev2019033001\r\ntf-nightly==1.13.1", "@patrick-ucr \r\n\r\nI just got the list of all my installed packages `pip list`, and edited together a command `pip uninstall <ALL MY PACKAGES>`.", "@LucasSloan I just installed Tensorflow from source, so \"pip uninstall\" would remove it and prevent other functions to work? I don't think this is an option for me. ", "I was able to solve this under Python 3.6 on Linux with:\r\n\r\n```\r\npip uninstall tensorflow_estimator\r\npip install tensorflow_estimator\r\n```", "I was using pipenv as environment. I recreated the environment using virtualenv and pip and it works now.", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\n\r\nSo did I. This solves problem", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\n\r\nYes, it works in my environment. thank you.", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\n\r\nThanks, solved for python 2.7 also", "Closing this issue since it is resolved. Feel free to reopen if have additional problems. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27079\">No</a>\n", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\n\r\nThis solved too in python 3.7.3\r\nthanks", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\n\r\nDoesn't help:(\r\nWhen I run:\r\npython3 -c \"import tensorflow as tf; print(tf.contrib.eager.num_gpus())\"\r\n\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from : pip3\r\nTensorFlow version : 1.13.1\r\nPython version: 3.5.2\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: GTX 1080Ti 11G\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 665, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/__init__.py\", line 73, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 62, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu import tpu\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 24, in <module>\r\n    from tensorflow.contrib.compiler import xla\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/compiler/xla.py\", line 28, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/__init__.py\", line 26, in <module>\r\n    from tensorflow_estimator.python import estimator\r\n  File \"/home/punkproger/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/home/punkproger/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 22, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier\r\n  File \"/home/punkproger/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/canned/baseline.py\", line 64, in <module>\r\n    from tensorflow_estimator.python.estimator import estimator\r\n  File \"/home/punkproger/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 66, in <module>\r\n    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib\r\n  File \"/home/punkproger/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/model_fn.py\", line 36, in <module>\r\n    from tensorflow_estimator.python.estimator.export import export_lib\r\n  File \"/home/punkproger/.local/lib/python3.5/site-packages/tensorflow_estimator/python/estimator/export/export_lib.py\", line 25, in <module>\r\n    from tensorflow.python.saved_model.model_utils import build_all_signature_defs\r\nImportError: No module named 'tensorflow.python.saved_model.model_utils'", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\nworks also for me, dos anyone have any explanation to this?\r\n", "@ymodak why you called this issue \"resolved\"? I see only workaround for this bug.", "how to fix \r\n\r\n\r\nUse standard file APIs to check for files with this prefix.\r\nWARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\tools\\freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 156, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"export_inference_graph.py\", line 152, in main\r\n    write_inference_graph=FLAGS.write_inference_graph)\r\n  File \"C:\\Users\\HP\\Desktop\\RE\\models\\research\\object_detection\\exporter.py\", line 465, in export_inference_graph\r\n    write_inference_graph=write_inference_graph)\r\n  File \"C:\\Users\\HP\\Desktop\\RE\\models\\research\\object_detection\\exporter.py\", line 421, in _export_inference_graph\r\n    placeholder_tensor, outputs)\r\n  File \"C:\\Users\\HP\\Desktop\\RE\\models\\research\\object_detection\\exporter.py\", line 264, in write_saved_model\r\n    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)\r\n  File \"C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 425, in __init__\r\n    super(SavedModelBuilder, self).__init__(export_dir=export_dir)\r\n  File \"C:\\Users\\HP\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\builder_impl.py\", line 100, in __init__\r\n    \"directory: %s\" % export_dir)\r\nAssertionError: Export directory already exists. Please specify a different export directory: inference_graph\\saved_model", "solve with solution: \r\n\r\n```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n```\r\n\r\nthanks a lot.", "> \r\n> \r\n> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\nThank you so much. For those who missed this reply and scrolled past to last. \r\n", "doesn't work with conda", "> > I was able to solve this under Python 3.6 on Linux with:\r\n> > ```\r\n> > pip uninstall tensorflow_estimator\r\n> > pip install tensorflow_estimator\r\n> > ```\r\n> \r\n> Thanks, solved for python 2.7 also\r\n\r\ndo you know how to do this for windows ?\r\n", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\n\r\nI reinstalled, But again occur this error \"cannot import name 'monitoring'\r\n\" ubuntu 18.04.1\r\npython 3.6.8\r\ntensorflow 1.13.1\r\ntensorflow-estimator (1.14.0)", "If you are using TensorFlow 1.14.0 you should install using this command\r\n`pip install tensorflow-estimator==1.14.0`", "> I was able to solve this under Python 3.6 on Linux with:\r\n> \r\n> ```\r\n> pip uninstall tensorflow_estimator\r\n> pip install tensorflow_estimator\r\n> ```\r\nEven after doing that, I get the same error!! \r\nHave a look at it.\r\n\r\n```\r\nrsml@rsml-01:~/darkflow$ pip uninstall tensorflow_estimator\r\nUninstalling tensorflow-estimator-2.0.0:\r\n  Would remove:\r\n    /home/rsml/anaconda3/envs/darkflow-env/lib/python3.6/site-packages/tensorflow_estimator-2.0.0.dist-info/*\r\n    /home/rsml/anaconda3/envs/darkflow-env/lib/python3.6/site-packages/tensorflow_estimator/*\r\nProceed (y/n)? y\r\n  Successfully uninstalled tensorflow-estimator-2.0.0\r\n(darkflow-env) rsml@rsml-01:~/darkflow$ pip install tensorflow_estimator\r\nCollecting tensorflow_estimator\r\n  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 2.5MB/s \r\nInstalling collected packages: tensorflow-estimator\r\nSuccessfully installed tensorflow-estimator-2.0.1\r\n(darkflow-env) rsml@rsml-01:~/darkflow$ ./flow \u2013model cfg/yolov2.cfg \u2013load bin/yolov2.weights \u2013/home/rsml/tensorflow1/models/research/object_detection/images \u2013gpu 0.9\r\nTraceback (most recent call last):\r\n  File \"./flow\", line 4, in <module>\r\n    from darkflow.cli import cliHandler\r\n  File \"/home/rsml/darkflow/darkflow/cli.py\", line 3, in <module>\r\n    from .net.build import TFNet\r\n  File \"/home/rsml/darkflow/darkflow/net/build.py\", line 5, in <module>\r\n    from .ops import op_create, identity\r\n  File \"/home/rsml/darkflow/darkflow/net/ops/__init__.py\", line 1, in <module>\r\n    from .simple import *\r\n  File \"/home/rsml/darkflow/darkflow/net/ops/simple.py\", line 1, in <module>\r\n    import tensorflow.contrib.slim as slim\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n\r\n```", "Doesn't work for me! Python 3.6.6, tensorflow 2.0.0, tensorflow-estimator-2.0.1 in Ubuntu 16.04. ", "@solomonvimal  Did you get any solution? I have same problem\r\n", "Please open a new issue if you have a problem, filling in the template.\r\n\r\nFor example, @solomonvimal mentions python 3.6, tf 2.0 whereas issue is about python 2.7, tf 1.13. Please don't conflate the issues, especially after the original is closed.", "I have the same problem....\r\nreinstalling temsorflow-estimator not worked for me....\r\n\r\nOS Platform and Distribution: windows 10\r\nTensorFlow installed from : pip\r\nTensorFlow version : 1.13.2\r\nPython version: 3.7.4\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: GeForce 940MX\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abc\\Anaconda3\\Lib\\site-packages\\darkflow\\Image detection 4.py\", line 2, in <module>\r\n    from darkflow.net.build import TFNet\r\n  File \"C:\\Users\\abc\\Anaconda3\\Lib\\site-packages\\darkflow\\darkflow\\net\\build.py\", line 5, in <module>\r\n    from .ops import op_create, identity\r\n  File \"C:\\Users\\abc\\Anaconda3\\Lib\\site-packages\\darkflow\\darkflow\\net\\ops\\__init__.py\", line 1, in <module>\r\n    from .simple import *\r\n  File \"C:\\Users\\abc\\Anaconda3\\Lib\\site-packages\\darkflow\\darkflow\\net\\ops\\simple.py\", line 1, in <module>\r\n    import tensorflow.contrib.slim as slim\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n[Finished in 3.8s]"]}, {"number": 27078, "title": "tensorflow 1.13.1 on linux on python 3.7 (not osx) uses -D_GLIBCXX_USE_CXX11_ABI=1 -- this behavior is undocumented and/or unspecified", "body": "### System information\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 18.04.2`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: *N/A*\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version: `1.13.1`\r\n- Python version: `3.7`\r\n- Installed using virtualenv? pip? conda?: **pip** -- see excerpt in pantsbuild/pants#7424\r\n- Bazel version (if compiling from source): *N/A*\r\n- GCC/Compiler version (if compiling from source): `7.3.0`\r\n- CUDA/cuDNN version: *N/A*\r\n- GPU model and memory: *N/A*\r\n\r\n### Problem\r\n- `tensorflow==1.13.1`, specifically only on `Linux` for python `3.7`, uses `-D_GLIBCXX_USE_CXX11_ABI=1`, which contradicts the documentation at https://www.tensorflow.org/guide/extend/op#build_the_op_library, which specifically says that `-D_GLIBCXX_USE_CXX11_ABI=0` is used.\r\n  - This caused an opaque compile error when building a custom operator in C++ in pants (see pantsbuild/pants#7417).\r\n  - **Is this ABI change intentional?**\r\n- **This does not affect any user who is building from source nor any user/tool which relies on `tensorflow.sysconfig.get_compile_flags()` to get compile flags.**\r\n- *(tangential)* `self.test_session()` is deprecated, but is also used in the documentation at https://www.tensorflow.org/guide/extend/op.\r\n\r\n### Exact commands / steps\r\npantsbuild/pants#7424 has command line logs from linux and osx to show that `tensorflow==1.13.1` on python 3.7 uses `-D_GLIBCXX_USE_CXX11_ABI=1` on linux, but not osx.\r\n\r\n### Background\r\nIn pants, we are imitating the instructions in the tutorial, but we are not using the `tf.sysconfig` API right now (see pantsbuild/pants#7046). We had set `-D_GLIBCXX_USE_CXX11_ABI=0` for all tensorflow compiles, and found a failure when trying to run testing for our tensorflow example custom op project when trying to set up a python 3.7 Linux CI shard. This is not blocking us -- we have a well-functioning workaround in pantsbuild/pants#7424, but we would like to know if this change is intentional and whether we should expect new tensorflow releases to also use the newer C++ ABI.", "comments": ["Noting that https://github.com/tensorflow/tensorflow/issues/26826#issuecomment-474680451 is likely the cause of this. Setting `-D_GLIBCXX_USE_CXX11_ABI=0` in the ubuntu builds for the binary `1.13.1` artifact on python 3.7 seems like it would work to resolve this, if it is in fact unintentional. To be clear about severity, this only affects consumers of the tensorflow binary wheel which do not use `tensorflow.sysconfig.get_compile_flags()`, which is likely to be a small number of users.", "@yifeif Is this documented in github.com/tensorflow/custom-op ?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27078\">No</a>\n"]}, {"number": 27077, "title": "Workaround for MSVC not merging bit-fields of different size.", "body": "See issue #26958.", "comments": ["@sanjoy so will you merge this PR?", "@ZunDubCore We have a custom system that auto-merges PRs once the presubmits are successful.  I've kicked in a force run of the presubmits, if they come back successful the PR will be merged automatically.", "@sanjoy What do all that checks mean?", "@rthadur any idea what's up with the \"Internal CI build failed\" errors?", "@sanjoy changes are pushed to r1.13 branch , there is no internal merge for this. We only merge changes which are pushed to \"master\"", "I see.  @ZunDubCore do you mind rebasing this on master?  Or do you need this fix specifically on r1.13?", "I think it's already fixed on master. The error only appears on r1.13.", "@rthadur do we have a process for merging into release branches like this?", "> @rthadur do we have a process for merging into release branches like this?\r\n\r\n@gunan @yifeif can you please help merge this PR ", "@rthadur @sanjoy I think it is not necessary to fix this on `r1.13` as it is already fixed on `master`. And I am not using `r1.13` brach to build tensorflow anymore. Closing the pull request."]}, {"number": 27076, "title": "added default parameter for tf.image.ssim", "body": "#26929 added window size and sigma as default parameter in tensorflow.image.ssim . comments also changed", "comments": ["@gshashank84 sir, can you guide me in changing the  [docs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/ssim) ", "I am not the right reviewer for this change, sorry. No background on image ops.", "Sir, can you suggest someone.\ud83d\ude0a", "Can you run the test in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/api_compatibility_test.py with --update_goldens=True?", "Can you also add a unit test for the new parameters?", "@alextp changes done.", "#27138 ", "sir, it is giving me this error\r\n`ModuleNotFoundError: No module named 'tensorflow.tools.api`", "How are you running the test? You should use bazel test.\n\nOn Tue, Mar 26, 2019 at 9:21 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> sir, it is giving me this error\n> ModuleNotFoundError: No module named 'tensorflow.tools.api\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27076#issuecomment-476727643>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdixzM-g6qDp3u7YjTCzFyZyXExXks5vakkZgaJpZM4cFdCk>\n> .\n>\n\n\n-- \n - Alex\n", "\r\n`$ bazel test tensorflow/tools/api/tests/api_compatibility_test.py `\r\n`INFO: Analysed target //tensorflow/tools/api/tests:api_compatibility_test.py (0 packages loaded, 0 targets configured). `\r\n`INFO: Found 1 target and 0 test targets...`\r\n`INFO: Elapsed time: 0.447s, Critical Path: 0.02s`\r\n`INFO: 0 processes.`\r\n`INFO: Build completed successfully, 1 total action`\r\n`INFO: Build completed successfully, 1 total action`", "Sir, i am new to bazel. Can you guide me in adding update_goldens flag to .py file while running `bazel test`  as\r\n`bazel test tensorflow/tools/api/tests/api_compatibility_test.py --update_goldens=True` gives me invalid option update_goldens  error", "bazel test tensorflow/tools/api/tests:api_compatibility_test --\n--update_goldens=True\n\nOn Tue, Mar 26, 2019 at 7:19 PM Gurpreet singh <notifications@github.com>\nwrote:\n\n> Sir, i am new to bazel. Can you guide me in adding update_goldens flag to\n> .py file while running bazel test as\n> bazel test tensorflow/tools/api/tests/api_compatibility_test.py this\n> gives me invalid option update_goldens\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27076#issuecomment-476937155>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxb4KtBhW1JNdy1VAaL2Te9j9pyaWks5vatUZgaJpZM4cFdCk>\n> .\n>\n\n\n-- \n - Alex\n", "\r\namans@DESKTOP-58ST84L MINGW64 /d/tensorflow/shift/tensorflow (gurpreetsingh-patch-2)\r\n$ bazel test tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens=True\r\nERROR: Skipping '-update_goldens=True': no such target '//:-update_goldens=True': target '-update_goldens=True' not declared in package '' defined by D:/tensorflow/shift/tensorflow/BUILD\r\nERROR: no such target '//:-update_goldens=True': target '-update_goldens=True' not declared in package '' defined by D:/tensorflow/shift/tensorflow/BUILD\r\nINFO: Elapsed time: 2.267s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n", "Sorry!\n\n$  bazel run tensorflow/tools/api/tests:api_compatibility_test --\n--update_goldens=True\nworks for me\n\nOn Wed, Mar 27, 2019 at 8:58 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> amans@DESKTOP-58ST84L MINGW64 /d/tensorflow/shift/tensorflow\n> (gurpreetsingh-patch-2)\n> $ bazel test tensorflow/tools/api/tests:api_compatibility_test --\n> --update_goldens=True\n> ERROR: Skipping '-update_goldens=True': no such target\n> '//:-update_goldens=True': target '-update_goldens=True' not declared in\n> package '' defined by D:/tensorflow/shift/tensorflow/BUILD\n> ERROR: no such target '//:-update_goldens=True': target\n> '-update_goldens=True' not declared in package '' defined by\n> D:/tensorflow/shift/tensorflow/BUILD\n> INFO: Elapsed time: 2.267s\n> INFO: 0 processes.\n> FAILED: Build did NOT complete successfully (1 packages loaded)\n> FAILED: Build did NOT complete successfully (1 packages loaded)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27076#issuecomment-477221136>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxa1nlMQBAs5XsOjlHo4JVT2nq4Dpks5va5UngaJpZM4cFdCk>\n> .\n>\n\n\n-- \n - Alex\n", "sir, do i need to run compatibility test for each language? as it is giving me java error as it is not installed\r\n\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node '//tensorflow/cc:ops/audio_ops_gen_cc BuildConfigurationValue.Key[597f9842fba8dc6842879c7aa8391ec2] true' (requested by nodes '//tensorflow/cc:audio_ops_genrule BuildConfigurationValue.Key[597f9842fba8dc6842879c7aa8391ec2] true', '//tensorflow/cc:audio_ops_genrule BuildConfigurationValue.Key[40e5e27e1ce1b80860ca92af73efe56d] false')\r\n", "Just for python\n\nOn Wed, Mar 27, 2019 at 10:36 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> sir, do i need to run compatibility test for each language? as it is\n> giving me java error as it is not installed\n>\n> Internal error thrown during build. Printing stack trace:\n> java.lang.RuntimeException: Unrecoverable error while evaluating node\n> '//tensorflow/cc:ops/audio_ops_gen_cc\n> BuildConfigurationValue.Key[597f9842fba8dc6842879c7aa8391ec2] true'\n> (requested by nodes '//tensorflow/cc:audio_ops_genrule\n> BuildConfigurationValue.Key[597f9842fba8dc6842879c7aa8391ec2] true',\n> '//tensorflow/cc:audio_ops_genrule\n> BuildConfigurationValue.Key[40e5e27e1ce1b80860ca92af73efe56d] false')\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27076#issuecomment-477268642>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfhiw-EK80JD5l_i5ikc5Qil7Bt2ks5va6wlgaJpZM4cFdCk>\n> .\n>\n\n\n-- \n - Alex\n", "Sir, How to specify that in bazel run script", "Just bazel run should work\n\nOn Wed, Mar 27, 2019 at 10:50 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> Sir, How to specify that in bazel run script\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27076#issuecomment-477275475>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTHoQ4PYmRulchy2fF6wrg8UYACIks5va694gaJpZM4cFdCk>\n> .\n>\n\n\n-- \n - Alex\n", "But it is giving be java error. \r\nOs : windows", "sir, how can i run `image_ops_test` file", "bazel test tensorflow/python/kernel_tests:image_ops_test", "sir, api_compatability_test and image_ops_test [/tensorflow/python/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L3621-L3645) worked in my virtual machine, os used ubuntu 16.04, btw how much time is take to build from source?", "I t depends on your machine\n\nOn Thu, Mar 28, 2019 at 8:20 PM Gurpreet singh <notifications@github.com>\nwrote:\n\n> sir, api_compatability_test and image_ops_test /tensorflow/python/BUILD\n> <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L3621-L3645>\n> worked in my virtual machine, os used ubuntu 16.04, btw how much time is\n> take to build from source?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27076#issuecomment-477850612>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeQR_NDiia8d6KA0bAYN_iwD0hRzks5vbYaLgaJpZM4cFdCk>\n> .\n>\n\n\n-- \n - Alex\n", "Sir, any changes required?", "Lots of lint errors:\r\n\r\n```\r\nFAIL: Found 38 non-whitelited pylint errors:\r\ntensorflow/python/ops/image_ops_impl.py:2821: [C0301(line-too-long), ] Line too long (85/80)\r\ntensorflow/python/ops/image_ops_impl.py:2897: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2898: [C0330(bad-continuation), ] Wrong continued indentation (add 18 spaces).\r\ntensorflow/python/ops/image_ops_impl.py:2898: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2898: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2898: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2961: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\ntensorflow/python/ops/image_ops_impl.py:2971: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2972: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2972: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:2972: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3036: [C0326(bad-whitespace), ] No space allowed before bracket\r\ntensorflow/python/ops/image_ops_impl.py:3047: [C0330(bad-continuation), ] Wrong continued indentation (add 16 spaces).\r\ntensorflow/python/ops/image_ops_impl.py:3047: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3047: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3048: [C0330(bad-continuation), ] Wrong continued indentation (add 16 spaces).\r\ntensorflow/python/ops/image_ops_impl.py:3048: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3048: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3147: [C0330(bad-continuation), ] Wrong continued indentation (add 18 spaces).\r\ntensorflow/python/ops/image_ops_impl.py:3147: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3148: [C0330(bad-continuation), ] Wrong continued indentation (add 18 spaces).\r\ntensorflow/python/ops/image_ops_impl.py:3148: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3148: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_impl.py:3148: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_test.py:4722: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/python/ops/image_ops_test.py:4723: [C0330(bad-continuation), ] Wrong continued indentation (add 1 space).\r\ntensorflow/python/ops/image_ops_test.py:4723: [C0301(line-too-long), ] Line too long (81/80)\r\ntensorflow/python/ops/image_ops_test.py:4733: [C0301(line-too-long), ] Line too long (82/80)\r\ntensorflow/python/ops/image_ops_test.py:4734: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_test.py:4734: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_test.py:4737: [C0301(line-too-long), ] Line too long (84/80)\r\ntensorflow/python/ops/image_ops_test.py:4738: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).\r\ntensorflow/python/ops/image_ops_test.py:4738: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_test.py:4738: [C0326(bad-whitespace), ] No space allowed around keyword argument assignment\r\ntensorflow/python/ops/image_ops_test.py:4781: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/python/ops/image_ops_test.py:4816: [C0326(bad-whitespace), ] No space allowed before keyword argument assignment\r\ntensorflow/python/ops/image_ops_test.py:4866: [C0330(bad-continuation), ] Wrong continued indentation (add 1 space).\r\ntensorflow/python/ops/image_ops_test.py:4867: [C0330(bad-continuation), ] Wrong continued indentation (add 1 space).\r\n```", "sir, how to reproduce pylint error `pylint tensorflow/python/ops/image_ops_impl.py` gives me extra indentation errors but the one listed in the above build. thank you for you help", "@Gurpreetsingh9465 I've rerun our CI. You'll see the lint errors in the log of the sanity one when it terminates.", "lint issue solved :+1: ", "@Gurpreetsingh9465 please resolve conflicts", "@rthadur done :+1: ", "@alextp :+1: "]}, {"number": 27075, "title": "added ifftshift and fftshift solving issue #26989", "body": " this PR implements ifftshift and fftshift in the signal module as it was missing in [signal](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal) #26989", "comments": ["@Gurpreetsingh9465 can you please check failed build errors", "![patch-1](https://user-images.githubusercontent.com/32022215/55274383-b21d9f00-52fd-11e9-8795-adf00e013e0b.png)\r\nsir, any idea on how much time will it take? \r\nused following commands\r\n`!git clone https://github.com/Gurpreetsingh9465/tensorflow`\r\n`!sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python`\r\n`!wget https://github.com/bazelbuild/bazel/releases/download/0.24.0/bazel-0.24.0-installer-linux-x86_64.sh`\r\n`!chmod +x bazel-0.24.0-installer-linux-x86_64.sh`\r\n`!sudo bash ./bazel-0.24.0-installer-linux-x86_64.sh`\r\n`cd tensorflow`\r\n`!git checkout gurpreetsingh-patch-1`\r\n`bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens=True`", "` kernal died while running `", "@Gurpreetsingh9465 can you please check build errors.", "@rthadur sir i am bit busy in writing gsoc proposal once that  is done i will continue working on this.(most probably by tomorrow)", "![Screenshot from 2019-04-03 21-55-49](https://user-images.githubusercontent.com/32022215/55496324-6b60d980-565c-11e9-8913-77b9b2dce27e.png)\r\nsir, API compatibility test giving me no error", "Right, so you should have modified files in your clone of tensorflow. Add\nthem to the PR.\n\nOn Wed, Apr 3, 2019 at 9:38 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> [image: Screenshot from 2019-04-03 21-55-49]\n> <https://user-images.githubusercontent.com/32022215/55496324-6b60d980-565c-11e9-8913-77b9b2dce27e.png>\n> sir, API compatibility test giving me no error\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27075#issuecomment-479565415>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxamuWnvdxlTIQ9cMJ40MH6CU9IS9ks5vdNkYgaJpZM4cFaiv>\n> .\n>\n\n\n-- \n - Alex\n", "done :+1: ", "sir, `manip_ops.roll` does not takes `name=None` as a argument so by using this method name will be appended with `/roll_n`", "Sir, can you guide me in replacing `ifftshift/roll` with only `ifftshift` ", "@Gurpreetsingh9465 I don't think the current API allows that. Please file an issue", "Added issue #27665", "Sir, not able to see the error in the copybara check.", "======================================================================\r\nERROR: testDefinition (__main__.FFTShiftTest)\r\ntestDefinition (__main__.FFTShiftTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/kernel_tests/signal/fft_ops_test.py\", line 564, in testDefinition\r\n    self.assertAllEqual(fft_ops.fftshift(x).numpy(), y)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/ops/signal/fft_ops.py\", line 368, in fftshift\r\n    return manip_ops.roll(x, shift, axes)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/ops/manip_ops.py\", line 30, in roll\r\n    return _gen_manip_ops.roll(input, shift, axis)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/ops/gen_manip_ops.py\", line 93, in roll\r\n    \"Roll\", input=input, shift=shift, axis=axis, name=name)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/framework/op_def_library.py\", line 531, in _apply_op_helper\r\n    raise err\r\nTypeError: Failed to convert object of type <type 'list'> to Tensor. Contents: [Dimension(4)]. Consider casting elements to a supported type.\r\n", "======================================================================\r\nERROR: testDefinition (__main__.FFTShiftTest)\r\ntestDefinition (__main__.FFTShiftTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/kernel_tests/signal/fft_ops_test.py\", line 564, in testDefinition\r\n    self.assertAllEqual(fft_ops.fftshift(x).numpy(), y)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/ops/signal/fft_ops.py\", line 368, in fftshift\r\n    return manip_ops.roll(x, shift, axes)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/ops/manip_ops.py\", line 30, in roll\r\n    return _gen_manip_ops.roll(input, shift, axis)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/ops/gen_manip_ops.py\", line 93, in roll\r\n    \"Roll\", input=input, shift=shift, axis=axis, name=name)\r\n  File \"/build/work/e94c2349cdfe73aa2a35b0124f5c93fc82b5/google3/runfiles/google3/third_party/tensorflow/python/framework/op_def_library.py\", line 531, in _apply_op_helper\r\n    raise err\r\nTypeError: Failed to convert object of type <type 'list'> to Tensor. Contents: [Dimension(4)]. Consider casting elements to a supported type.\r\n", "![Screenshot from 2019-04-12 16-52-44](https://user-images.githubusercontent.com/32022215/56034008-a7262e00-5d43-11e9-8bb5-ab81404e1cd6.png)\r\n![Capture](https://user-images.githubusercontent.com/32022215/56034039-bad19480-5d43-11e9-8532-883d3490a049.PNG)\r\ntested on windows and linux. sir, linux doesn't not support `tensor.numpy()` while windows have this functionality. ", "@Gurpreetsingh9465 please don't send me screenshots; I don't think this helps the discussion. You should know how to copy-paste the relevant code snippets.\r\n\r\nMoreover, both test runs you show me show the tests passing, so I don't understand what you're trying to claim here.", "@alextp will take care of that next time. the latest commit will solve pylint error. :smile: \r\n`pylint fft_ops.py --indent-string=\"  \"`\r\nno whitespace error found\r\n\r\nfft_ops_test passed\r\n`\r\nINFO: Build completed successfully, 196 total actions\r\n//tensorflow/python/kernel_tests/signal:fft_ops_test                     PASSED in 10.5s\r\n`", "@rthadur @alextp @rryan any changes required? I didn't find error in the files which are modified.", "@Gurpreetsingh9465  changes are breaking internal tests , can you test with `--test_env=TF2_BEHAVIOR=1 ` and try to fix the issues", "@rthadur \r\n`bazel run tensorflow/tools/api/tests:api_compatibility_test -- --test_env=TF2_BEHAVIOR=1`\r\nerror:\r\n`Unknown command line flag 'test_env'`\r\nthe flag might be something else", "> @rthadur\r\n> `bazel run tensorflow/tools/api/tests:api_compatibility_test -- --test_env=TF2_BEHAVIOR=1`\r\n> error:\r\n> `Unknown command line flag 'test_env'`\r\n> the flag might be something else\r\n\r\n@alextp can you please assist", "--test_env only applies to bazel test, not bazel run.\r\n\r\nUse bazel build followed by `TF2_BEHAVIOR=1 bazel-bin/path/to/test`", "> --test_env only applies to bazel test, not bazel run.\r\n> \r\n> Use bazel build followed by `TF2_BEHAVIOR=1 bazel-bin/path/to/test`\r\n\r\n`bazel build TF2_BEHAVIOR=1 bazel-bin/tools/api/tests:api_compatibility_test`\r\n`target 'TF2_BEHAVIOR=1' not declared in package 'tensorflow' defined by`\r\n`tensorflow/BUILD`\r\n\r\nalso tried\r\n`bazel test tools/api/tests:api_compatibility_test -- --test_env=TF2_BEHAVIOR=1`\r\nno such target '//tensorflow:-test_env=TF2_BEHAVIOR=1': target '-test_env=TF2_BEHAVIOR=1'\r\n\r\n`bazel run tensorflow/tools/api/tests:api_compatibility_test` // without any flag\r\n\r\nFAIL: testAPIBackwardsCompatibilityV2 (__main__.ApiCompatibilityTest)\r\ntestAPIBackwardsCompatibilityV2 (__main__.ApiCompatibilityTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 394, in testAPIBackwardsCompatibilityV2\r\n    omit_golden_symbols_map=omit_golden_symbols_map)\r\n  File \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 349, in _checkBackwardsCompatibility\r\n    api_version=api_version)\r\n  File \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 270, in _AssertProtoDictEquals\r\n    self.fail('%d differences found between API and golden.' % diff_count)\r\n  File \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/absl_py/absl/testing/absltest.py\", line 1609, in fail\r\n    return super(TestCase, self).fail(self._formatMessage(prefix, msg))\r\nAssertionError: 1 differences found between API and golden.\r\n\r\n\r\n", "Just do\n\nbazel build tensorflow/tools/api/tests:api_compatibility_test\nbazel-bin/tensorflow/tools/api/tests:api_compatibility_test\n--update_goldens=True\n\njust as it says in the test error message.\n\nOn Thu, Apr 25, 2019 at 5:29 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> --test_env only applies to bazel test, not bazel run.\n>\n> Use bazel build followed by TF2_BEHAVIOR=1 bazel-bin/path/to/test\n>\n> bazel build TF2_BEHAVIOR=1 bazel-bin/tools/api/tests:api_compatibility_test\n> target 'TF2_BEHAVIOR=1' not declared in package 'tensorflow' defined by\n> tensorflow/BUILD\n>\n> also tried\n> bazel test tools/api/tests:api_compatibility_test --\n> --test_env=TF2_BEHAVIOR=1\n> no such target '//tensorflow:-test_env=TF2_BEHAVIOR=1': target\n> '-test_env=TF2_BEHAVIOR=1'\n>\n> bazel run tensorflow/tools/api/tests:api_compatibility_test // without\n> any flag\n> FAIL: testAPIBackwardsCompatibilityV2 (*main*.ApiCompatibilityTest)\n> testAPIBackwardsCompatibilityV2 (*main*.ApiCompatibilityTest)\n>\n> Traceback (most recent call last):\n> File\n> \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\",\n> line 394, in testAPIBackwardsCompatibilityV2\n> omit_golden_symbols_map=omit_golden_symbols_map)\n> File\n> \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\",\n> line 349, in _checkBackwardsCompatibility\n> api_version=api_version)\n> File\n> \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\",\n> line 270, in _AssertProtoDictEquals\n> self.fail('%d differences found between API and golden.' % diff_count)\n> File\n> \"/home/jarvis/.cache/bazel/_bazel_jarvis/7f3ec7e1988613a7d6a01dd8eab38aa0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/absl_py/absl/testing/absltest.py\",\n> line 1609, in fail\n> return super(TestCase, self).fail(self._formatMessage(prefix, msg))\n> AssertionError: 1 differences found between API and golden.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27075#issuecomment-486649695>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJEQTFJDNUUV3X2TYDPSGPZBANCNFSM4HAVVCXQ>\n> .\n>\n\n\n-- \n - Alex\n", "    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[ RUN      ] ApiCompatibilityTest.test_session\r\n[  SKIPPED ] ApiCompatibilityTest.test_session\r\n\r\nRan 7 tests in 6.052s\r\n\r\nOK (skipped=1)\r\n\r\nthis changes\r\n`\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.summary.pbtxt`\r\n\r\ndo i need to commit changes of this file?", "Yes\n\nOn Thu, Apr 25, 2019 at 9:01 AM Gurpreet singh <notifications@github.com>\nwrote:\n\n> $ bazel build tensorflow/tools/api/tests:api_compatibility_test\n> $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\n>       --update_goldens True\n>\n> [ RUN ] ApiCompatibilityTest.testAPIBackwardsCompatibility\n> [ OK ] ApiCompatibilityTest.testAPIBackwardsCompatibility\n> [ RUN ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\n> [ OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\n> [ RUN ] ApiCompatibilityTest.testNoSubclassOfMessage\n> [ OK ] ApiCompatibilityTest.testNoSubclassOfMessage\n> [ RUN ] ApiCompatibilityTest.testNoSubclassOfMessageV1\n> [ OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\n> [ RUN ] ApiCompatibilityTest.testNoSubclassOfMessageV2\n> [ OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\n> [ RUN ] ApiCompatibilityTest.test_session\n> [ SKIPPED ] ApiCompatibilityTest.test_session\n>\n> Ran 7 tests in 6.052s\n>\n> OK (skipped=1)\n>\n> this changes\n> modified: tensorflow/tools/api/golden/v2/tensorflow.summary.pbtxt\n>\n> do i need to commit changes of this file?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27075#issuecomment-486731946>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROPLTDPZ7M2PW643YTPSHIVJANCNFSM4HAVVCXQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp done :+1: ", "Just a quick note, the fft2d is broken in tensorflow when you feed it complex data.  This should impact the checks for *fftshift.\r\n\r\n#28192", "> Just a quick note, the fft2d is broken in tensorflow when you feed it complex data. This should impact the checks for *fftshift.\r\n> \r\n> #28192\r\n\r\nis anyone working on this?", "Not thatI know of.\n\nOn Fri, Apr 26, 2019 at 9:39 PM Gurpreet singh <notifications@github.com>\nwrote:\n\n> Just a quick note, the fft2d is broken in tensorflow when you feed it\n> complex data. This should impact the checks for *fftshift.\n>\n> #28192 <https://github.com/tensorflow/tensorflow/issues/28192>\n>\n> is anyone working on this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27075#issuecomment-487253947>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPAOCAZZLRC5CSETULPSPKHXANCNFSM4HAVVCXQ>\n> .\n>\n\n\n-- \n - Alex\n", "The tests are still failing with --test_env=TF2_BEHAVIOR=1", "[summary.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/golden/v2/tensorflow.summary.pbtxt) copied the content of this file"]}, {"number": 27074, "title": "Celery hangs with Tensorflow shared model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on Docker (18.03)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: No gpu\r\n- GPU model and memory: No gpu\r\n- Celery 4.2.2, Keras 2.2.4\r\n\r\n**Scenario**\r\nI'm running a Celery worker which handless ml requests for prediction using LSTM NN. I'm loading the model in a main worker which passes it to the worker process as reference so that the model is not loaded all the time in a concurrent set-up. \r\n\r\n**Describe the current behavior**\r\nWhen the request comes the execution hangs indefinitely when it hits the access to the passed model reference.  I've searched for a solution and found that the sessions cannot be shared across process which is the cause of the hang. I've tried several solutions which are mentioned lower. However only changing back end to Theano solved the issue. Is there any other solution for this issue or I have to keep loading model in worker who uses it for prediction? Or I am doing something wrong?  \r\n\r\n**Describe the expected behavior**\r\nI would like the set-up to work with Tensorflow as is with Theano that is to share a reference with workers to save memory. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nmain.py\r\n\r\nfrom keras.models import load_model\r\nfrom celery import Celery, states\r\nfrom celery import Task\r\n\r\nclass DomainAnalyzer(Task):\r\n    ignore_result = True\r\n    def __init__(self):\r\n        self.model = load_model(\"lstm_model.h5\")\r\n\r\n    def run(self, data):\r\n        try:\r\n            preprocessor = Preprocessor(data, self.model)\r\n            data = preprocessor.predict_data()\r\n            return data \r\n        except Exception as e:\r\n            print(e)\r\n\r\napp = Celery('worker', broker=\"broker\")\r\napp.register_task(DomainAnalyzer())\r\n\r\nif __name__ == '__main__':\r\n    app.start()\r\n```\r\n```\r\nworker.py\r\n\r\nclass Preprocessor:\r\n\r\n    def __init__(self, data: str, model):\r\n        self.data= data\r\n        self.model = model\r\n\r\n    def data_analysis(self):\r\n        return self.model.predict(self.data)\r\n```\r\n\r\n### Tried solution 1.\r\nI've specified the multiprocessing start method. \r\n`multiprocessing.set_start_method('spawn', force=True)`\r\n\r\n### Tried solution 2.\r\nPassing graph object in worker\r\n```\r\nimport tensorflow as tf\r\nself.model = load_model(\"lstm_model.h5\")\r\nself.model._make_predict_function()\r\nself.graph = tf.get_default_graph()\r\npreprocessor = Preprocessor(data, self.model, self.graph)\r\n```\r\n\r\nIn worker:\r\n```\r\ndef data_analysis(self):\r\n     with self.graph.as_default():\r\n         return self.model.predict(self.data)\r\n\r\n```\r\n### Tried solution 3. (currently only one which works)\r\nUse Theano :(\r\n\r\n", "comments": ["Can you clarify what your high-level goal is? To run a server for inference? If so, we would recommend you check out TensorFlow Serving: https://www.tensorflow.org/tfx/guide/serving . We do not currently support Celery or have existing examples, so it's hard to help you debug here.", "Yes, I'm building something of that sort, that is an extensible rest API service which provides some analysis. I've checked TensorFlow Serving during analysis but I'm not sure about the possibility of tweaking and further extensibility. Are you planning on making Tensorflow Celery friendly or rather multiprocess friendlier? Do you know any other workarounds for the general scenario (parallel model use) which could solve the issue apart from those mentioned?", "There are no plans for integrating with Celery currently AFAIK. You could load multiple copies of the model into multiple processes, but other than that, I would encourage the use of TF Serving, which is what we use internally for this.", "Ok, I'll try that", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27074\">No</a>\n", "celery workers serving with billiard, not multiprocessing"]}, {"number": 27073, "title": "Removed trival warnings", "body": "Removed some warnings from the files", "comments": ["@jdduke , thanks for pointing this out, i updated the code as per the review comments, kindly check and approve.\r\n\r\nRegards\r\nAmit", "@jdduke , i have updated the code as per comments, kindly check and update.\r\n\r\nRegards\r\nAmit", "@jdduke , i found one problem in the function CalculateDeallocationOfInternalTensors and CalculateAllocationOfInternalTensors, this is when the node_index becomes negative(in one of the TC) then dues to the implicit conversion with the unsigned int, negative values were becoming positive and hence the test case was passing, not sure this was intentional or oversight, anyway i changed the code as per your suggestion and added the small error check code to ensure sanity. \r\nif (node_index < 0) {\r\nreturn kTfLiteOk;\r\n}\r\nHope this is ok, Kindly check and approve.\r\n\r\nRegards\r\nAmit", "@jdduke , i found one problem in the function CalculateDeallocationOfInternalTensors and CalculateAllocationOfInternalTensors, this is when the node_index becomes negative(in one of the TC) then dues to the implicit conversion with the unsigned int, negative values were becoming positive and hence the test case was passing, not sure this was intentional or oversight, anyway i changed the code as per your suggestion and added the small error check code to ensure sanity.\r\nif (node_index < 0) {\r\nreturn kTfLiteOk;\r\n}\r\nHope this is ok, Kindly check and approve.\r\n\r\nRegards\r\nAmit", "> @jdduke , i found one problem in the function CalculateDeallocationOfInternalTensors and CalculateAllocationOfInternalTensors, this is when the node_index becomes negative(in one of the TC) \r\n\r\nInteresting, can we expand the test to catch this? Can you tell if this would happen for a legit model/interpreter use-case? \r\n", "@jdduke this happens when the graph is empty , the TC which is triggering this condition is (ArenaPlannerTest, EmptyGraph)  , which ultimately calls CalculateAllocations which subtracts one from the active node hence causing the problem.\r\n\r\nRegards\r\nAmit", "> @jdduke this happens when the graph is empty , the TC which is triggering this condition is (ArenaPlannerTest, EmptyGraph) , which ultimately calls CalculateAllocations which subtracts one from the active node hence causing the problem.\r\n\r\nCan you add a (unit) test for this case?", "@jdduke , thanks for the review i have added one case to cover this scenario, kindly check and approve.\r\n\r\nRegards\r\nAmit", "@jdduke , sorry for the miss,i have updated the code with the comments, explaining why this TC is added and what problem previously was seen. Hope this is ok.\r\n\r\nRegards\r\nAmit", "@jdduke , yes you are right, sorry for the miss, i have updated the code as per your suggestion.\r\nKindly check.\r\n\r\nRegards\r\nAmit", "@jdduke , thanks for approving the PR, can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur is working to land this.", "> @rthadur is working to land this.\r\n\r\n@rthadur any update as to when this can be merged.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "Its failing because of internal error .  Here is the error: \r\n`[==========] Running 14 tests from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 14 tests from ArenaPlannerTest\r\n[ RUN      ] ArenaPlannerTest.EmptyGraph\r\n[       OK ] ArenaPlannerTest.EmptyGraph (3 ms)\r\n[ RUN      ] ArenaPlannerTest.DeallocationOfInputTensor\r\nUnhandled exception:\r\n    @     0x7feea2d26ce2  GoogleTerminateHandler()\r\n    @     0x7feea2814266  __cxxabiv1::__terminate()\r\n    @     0x7feea2814293  std::terminate()\r\n    @     0x7feea28143cf  __cxa_throw\r\n    @     0x7feea2758f60  std::__g::__throw_out_of_range_fmt()\r\n    @     0x7feea2e33599  tflite::ArenaPlanner::PlanAllocations()\r\n    @     0x7feea47cab82  tflite::(anonymous namespace)::ArenaPlannerTest::SetGraph()\r\n    @     0x7feea47cba8d  tflite::(anonymous namespace)::ArenaPlannerTest_DeallocationOfInputTensor_Test::TestBody()\r\n    @     0x7feea35f960a  testing::Test::Run()\r\n    @     0x7feea35fa973  testing::TestInfo::Run()\r\n    @     0x7feea35fb50b  testing::TestSuite::Run()\r\n    @     0x7feea360ba77  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x7feea360b446  testing::UnitTest::Run()\r\n    @     0x7feea47c9701  main\r\n    @     0x7feea23aabbd  __libc_start_main\r\n    @     0x55574cb9e4c9  ../sysdeps/x86_64/start.S:108 _start\r\nterminate called after throwing an instance of 'std::__g::out_of_range'`\r\n", "@rthadur , i have fixed the problem.\r\n@jdduke , i know this is old PR, can you please re approve.\r\n\r\nRegards\r\nAmit", "Changes are merged internally , waiting for auto merge to happen."]}, {"number": 27072, "title": "preserve collections after converting graph to a tensorrt-optimized one", "body": "Fix https://github.com/tensorflow/tensorflow/issues/26800", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27072) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27072) for more info**.\n\n<!-- ok -->"]}, {"number": 27071, "title": "fixed Comments for CategorcialCrossEntropy", "body": "Typo in doc: `tf.keras.losses.CategoricalCrossentropy` #27070\r\n\r\nThe probabilities must be add up to 1.00, therefore ```[.5, .89, .6]``` -> ```[.05, .89, .06]```", "comments": ["@gbaned @rthadur  can you start the process of Merging this PR?", "@gshashank84 thank you, currently we don't have internal for branches other than \"master\" , @dynamicwebpaige can you please help merge this to r1.13 branch", "hey @dynamicwebpaige, could you have a look at this PR?", "@yifeif @gunan can you please help merge this PR", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 27070, "title": "Typo in doc: `tf.keras.losses.CategoricalCrossentropy`", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.13\r\n- Doc Link: [tf.keras.losses.CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy#class_categoricalcrossentropy)\r\n\r\n\r\n**Describe the documentation issue**\r\nIn the example code \r\n\r\n```\r\nloss = cce(\r\n    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\r\n    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\n```\r\n\r\nI think the second array in the second line should be `[.05, .89, .06]` instead of `[.5, .89, .6]`, as they're supposed to be summed to one.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Thank you for noticing this and submitting the PR! Approving now. \ud83d\ude42 ", "Thank you so much. Fixed in nightly: https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy?version=nightly#class_categoricalcrossentropy"]}, {"number": 27069, "title": "Deprecate tf.floor_div, tf.floormod, tf.mod, adds tf.math.floormod, tf.math.mod", "body": "This fix is based on the RFC: https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md\r\n\r\nAccording to RFC, `floor` and related functions should be exposed in `tf.math` in TF2.0:\r\n- `tf.floor_div`, `tf.floormod`, `tf.mod`, should be deprecated,\r\n- `tf.math.floormod`, `tf.math.mod` should be added.\r\n\r\nThis PR made the changes to conform to RFCs (move floor/floormod/floordiv/etc to tf.math).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@martinwicke can you check if this looks ok? If yes, then we also need to cherrypick it into 1.14 since tf.floor_div, tf.floormod and tf.mod are getting removed in 2.0 and instead replaced by tf.math.* endpoints.", "Yes, and let's cherry-pick it. We may have kept the aliases because they are common? I think that argument is pretty weak, so removing them sounds good to me. @aselle FYI.", "Yeah I don't remember why we kept them. I must have missed them somehow while making changes. I will cherrypick it then once the change gets submitted."]}, {"number": 27068, "title": "Remove tf.floor endpoint from TF 2.0", "body": "This fix tries to address the issue raised in #27003 where `ceil` is only exposed as `tf.math.ceil` for TF 2.0 while `floor` is exposed in both `tf.floor` and `tf.math.floor` for TF 2.0.\r\n\r\nIt makes sense to keep consistency for `ceil` and `floor`. This PR deprecates tf.floor and exposes only tf.math.floor in TF 2.0.\r\n\r\nThis fix fixes #27003.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang can you please resolve merge conflicts.", "TF2 API frozen, so we can't remove it anymore."]}, {"number": 27067, "title": "TensorFlow stopped working with custom ops built with GCC5", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly >= 20190321\r\n- Python version: Python 2, Python 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.11\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nTensorFlow fails with segmentation fault when using custom ops built with gcc5.  The segmentation fault originates from usage of `std::function` on the interface boundary, introduced in https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee5.\r\n\r\n**Describe the expected behavior**\r\nCustom ops built with gcc5 should continue working with TF built with gcc4.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n$ docker run -it tensorflow/tensorflow:nightly\r\n# apt install -y mpich\r\n# pip install horovod\r\n# cat > test.py\r\nimport tensorflow as tf\r\nimport horovod.tensorflow as hvd\r\nhvd.init()\r\nsess = tf.Session()\r\nsess.run(hvd.allreduce(tf.constant(1.0)))\r\n^D\r\n# python test.py\r\n```\r\n\r\nOutputs:\r\n```\r\nroot@011ee61f092e:/# python test.py\r\n2019-03-24 00:25:51.898835: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-24 00:25:51.910390: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-03-24 00:25:51.914924: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4269380 executing computations on platform Host. Devices:\r\n2019-03-24 00:25:51.914981: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nSegmentation fault (core dumped)\r\nroot@011ee61f092e:/#\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe reason for this issue is the fact that definition of `std::function` has changed between gcc4 and gcc5.\r\n\r\ngcc4:` _M_invoke(const _Any_data& __functor, _ArgTypes... __args)`\r\ngcc5: `_M_invoke(const _Any_data& __functor, _ArgTypes&&... __args)`\r\n\r\nWhile this change is ABI-compatible, it produced segfault in situation where gcc4-compiled code is calling function defined in gcc5-compiled plugin.\r\n\r\nShort repro:\r\n\r\nPrepare files:\r\n```\r\n$ cat > std_function_fw.h\r\n#include <functional>\r\nint call_me(std::function<int(int)> f);\r\n^D\r\n$ cat > std_function_fw.cc\r\n#include \"std_function_fw.h\"\r\nint call_me(std::function<int(int)> f) {\r\n\treturn f(42);\r\n}\r\n^D\r\n$ cat > std_function_client.cc\r\n#include <iostream>\r\n#include \"std_function_fw.h\"\r\nint main(int argc, char **argv) {\r\n\tstd::cout << call_me([](int val) { return val + 201808; }) << std::endl;\r\n}\r\n^D\r\n```\r\n\r\nMount to gcc4 docker (e.g. `debian:jessie`):\r\n```\r\n# g++ --std=c++11 -fPIC std_function_fw.cc -shared -o libstd_function_fw.so\r\n# g++ --std=c++11 -fPIC std_function_client.cc -o std_function_client -lstd_function_fw -L.\r\n# LD_LIBRARY_PATH=. ./std_function_client\r\n<will work>\r\n```\r\n\r\nMount to gcc5 docker (e.g. `ubuntu:16.04`), keep `libstd_function_fw.so`:\r\n```\r\n# g++ --std=c++11 -fPIC std_function_client.cc -o std_function_client -lstd_function_fw -L.\r\n# LD_LIBRARY_PATH=. ./std_function_client\r\n<will crash>\r\n```\r\n\r\nProposed solution is to revert https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee5 and keep using function pointers on the plugin interface boundary w/o using `std::function`.", "comments": ["cc @martinwicke, plugins built with gcc5 are broken", "@gunan @sjamesr FYI", "@yifeif \r\nI think for everything except python 3.7 pip packages, this is WAI.\r\nWe build using GCC 4.8. Sometime in April, hopefully our binaries should become more compatible. Until then, you need to build with gcc 4.8", "@gunan, it used to work correctly until https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee\r\n\r\nWhat is the proposed change that will improve this in April?", "Interesting, I would have expected to see ABI issues.\r\nUbuntu 14.04 is going EOL, so we have to upgrade our build environment. We are exploring how we can make things backwards compatible with a new build environment.", "Back in the day we've added `tf.sysconfig.get_compile_flags()` to get around ABI incompatibility, and gcc4/gcc5 interop was working fine until three days ago.\r\n\r\nSince Horovod and many other custom ops are compiled on the user machine, and most users have gcc5+, its not a good user experience to ask them to install gcc4.\r\n\r\nCan we revert https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee until you upgrade the build system?", "We can consider reverting, but if you use `std::string` you just wont be able to compile an add on to TF using GCC 5 or more recent. Our [custom-op guide](https://github.com/tensorflow/custom-op) provides a docker container which guarantees that if you use that to compile your add ons, the compiled objects will work.\r\n\r\nConsidering @sjamesr's work is to get us to a fully ABI compatible state, I will check with him before rolling back, to see how important this change in his greater task.", "`std::string` actually works fine with `-D_GLIBCXX_USE_CXX11_ABI=0`, which is returned by `tf.sysconfig.get_compile_flags()` if TF is compiled with gcc4.\r\n\r\nI\u2019m only aware of the issue with `std::function` which is not fixed by _GLIBCXX_USE_CXX11_ABI.", "I understand your frustration, however this is just how C++ ABI is. There are no standards governing how C++ ABI is generated, so an arbitrary change may break you at any given time. That is why we are right now pushing for https://github.com/tensorflow/community/pull/77. @sjamesr is working on creating a C ABI we can use that can remove  all ABI restrictions. Unfortunately, until that is ready, for custom ops all we can guarantee is what is documented in our [custom-op guide](https://github.com/tensorflow/custom-op)\r\n\r\nI think we have a prototype of a kernel that uses a C API ready, and we have a few changes preparing in the pipeline to help with the situation. But until then I unfortunately have to ask you to use our docker container to build any C++ add-ons to TF.", "C API sounds great!  I'd love not to have to worry about C++ ABI compatibility anymore.  Happy to be one of the early adopters.\r\n\r\nIs there any update on whether reverting https://github.com/tensorflow/tensorflow/commit/41e7b3ca0abfd7e40a82ae38f96a9fbfeecb0ee would impede the progress of that work?  While I understand that C++ ABI has little theoretical guarantees across compiler versions, in practice this compatibility has been working fine for the last two years for a vast majority of users.  `_GLIBCXX_USE_CXX11_ABI` does a pretty good job with allowing backward compatibility, and `std::function` is one of the outliers, rather than a common pattern.\r\n\r\nAsking users to compile plugins like Horovod in a custom-op container is a pretty bad user experience because Horovod requires environment such as CUDA, NCCL, and MPI to have the same version as what'd be used on the host.\r\n\r\nBecause of that, it'd be great if we could keep using existing C++ API with the revert until we can cut over to C API.", "Unfortunately, All I can offer is creating a docker container based on our custom op containers to provide an environment to build.\r\nWe have created these containers, because such issues has been hitting our users continuously. So for your simple example, you are only now hitting the issue. However, we have seen with tensorflow/compression or tensorflow/lingvo that withour the referenced change, they are hitting similar ABI issues.", "OK, fair enough.  What's the timeline for C API and/or moving your build system to gcc5+?", "For moving TF builds to gcc5+, we are  aiming end of april, the latest.\r\nFor C API, I would defer to @sjamesr ", "Got it, TF 1.14.0 still planned on GCC 4.8 then?", "90% yes. If by some miracle, we manage to pull it off and migrate builds would it be difficult to migrate your workflows?\r\n\r\nBTW, the toolchains we are testing right now are centos6 + devtoolset7, to achieve manylinux 2010 compatibility.", "I expect it to be easy, could you share a WIP wheel built with centos6 + devtoolset7 file for me to try out early?", "I have a centos7+devtoolset7 image, and am working on a centos6 one - centos6 is unfortunately hard to target from debian, so I have to jump through some hoops.", "TensorFlow 1.15 RC has been built with a devtoolset7. You can try with that.\r\n\r\n@yifeif have we updated our custom-op docker/repo?", "Yes, https://github.com/tensorflow/custom-op supports two sets of dockerfiles, one with devtoolset7(all pip packages released after Aug 1, 2019), and one for everything before that. Please give it a try.", "Is devtoolset7 gcc-7? I tried to compile custom ops with gcc-7, but when loading it I catch a `undefined symbol` error in both TF 2.0 and TF 1.15-rc2.\r\n```\r\n  File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/deepmd/_prod_force_grad.py\", line 15, in <module>\r\n    op_grads_module = tf.load_op_library(module_file)\r\n  File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/deepmd/libop_grads.so: undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n```", "> Is devtoolset7 gcc-7? I tried to compile custom ops with gcc-7, but when loading it I catch a `undefined symbol` error in both TF 2.0 and TF 1.15-rc2.\r\n> \r\n> ```\r\n>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/deepmd/_prod_force_grad.py\", line 15, in <module>\r\n>     op_grads_module = tf.load_op_library(module_file)\r\n>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n>     lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n> tensorflow.python.framework.errors_impl.NotFoundError: /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/deepmd/libop_grads.so: undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n> ```\r\n\r\nFound the reason: I need to add `-D_GLIBCXX_USE_CXX11_ABI=0`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27067\">No</a>\n"]}, {"number": 27066, "title": "Merge pull request #1 from tensorflow/master", "body": "pull from tensorflow", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27066) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 27065, "title": "In tensorflow, why does a 6B GPU show with memory limit than 5GB of VRAM?", "body": "I have a 6GB 1060 GPU. Running:\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n```\r\ngives\r\n\r\n```\r\nname: \"/device:GPU:0\"\r\n device_type: \"GPU\"\r\n memory_limit: 4951913267\r\n locality {\r\n   bus_id: 1\r\n   links {\r\n   }\r\n }\r\n incarnation: 15506209764385210283\r\n physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, \r\npci bus id: 0000:02:00.0, compute capability: 6.1\"\r\n```\r\nNotice it is showing memory_limit of 4951913267, which is 1.1GB short of 6GB.  Where did my 1.1GB go?", "comments": ["I think there is a limit(set by system) to what extent you can utilize your virtual memory of GPU, therefore the default size is set to 5GB. However it can be clocked further upto the actual size.", "> I think there is a limit(set by system) to what extent you can utilize your virtual memory of GPU, therefore the default size is set to 5GB. However it can be clocked further upto the actual size.\r\n\r\nThe above explanation is correct. Closing, since the issue is not at TensorFlow's end. Thanks!"]}, {"number": 27064, "title": "Support for Kotlin Native?", "body": "As I see there is a full support for swift language now and I believe it will surely replace python as the primary language for building deep learning models by software developers. Will I see the same for Kotlin Native? ", "comments": ["We have no plans for this currently. However, there is a proposal for a SIG-JVM (\r\nhttps://github.com/tensorflow/community/pull/86), that would likely be the right place to discuss Kotlin bindings/integration."]}, {"number": 27063, "title": "How Can I set input shape of Subclass Model ?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow 1.13\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nin subclass model,\r\n\r\n def call(self,x):\r\n\r\nWhen I call my mode(subclass) at first time, I can't check batch_size(None).\r\n\r\n\r\n\r\nIf I want to concatenate multiple layers, the model check input shape.\r\n\r\nbut it can't\r\n\r\nHow can I set input shape of subclass model?\r\n\r\n    def call(self,x):\r\n        \"\"\" Forward \"\"\"\r\n        # [x,y]=Lambda(lambda x:[x[0],x[1]], output_shape=[-1,self.opt.ncond,self.opt.nc,self.opt.height,self.opt.width])(x)\r\n        inputs = Reshape((self.opt.ncond * self.opt.nc, self.opt.height, self.opt.width))(x)  # batch_size, shape\r\n        target = Reshape((self.opt.npred * self.opt.nc, self.opt.height, self.opt.width))(self.y)\r\n        g_pred_v = K.variable(self.deterministic(inputs), name=\"g_pred_v\")\r\n        r=K.abs(g_pred_v-target) # residual\r\n        z=self.phi_network_conv(r)\r\n        s = self.deterministic.get_layer()[0](inputs)\r\n        h=concatenate([s,z], axis=1) # concatenate\r\n        # shape=K.int_shape(s)\r\n        # h=Reshape((shape[1]*2, shape[2],shape[3]))(h)\r\n        pred_f = self.f_network_decoder(h)\r\n        return pred_f\r\n\r\n    def get_target(self,y):\r\n        # input layer\r\n        self.y=y\r\n\r\n\r\n \r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "thanks!"]}, {"number": 27062, "title": "Relationship between batch size and CUDA load in deep learning task", "body": "I am learning about machine translation by running this TensorFlow example on my GPU in my home PC: https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\r\n\r\nMy GPU has 6GB of VRAM. I have to reduce the batch size to get the example code not to run out of memory on the GPU. With trial and error, I find a batch size (minimum is 1) where the code runs and uses the most memory. I notice that, as I reduce the batch size, the CUDA core load as reported by Windows Task Manager GPU view goes down.\r\n\r\nThe application described in the link above creates a complex TensorFlow network. I don't know whether Tensorflow creates one copy of the network or multiple copies to load the GPU.\r\n\r\nIf it can create multiple copies, is there a TensorFlow switch for that? I don't think memory speed should be a bottleneck in feeding the GPU. That is, I should be able to optimize between batch size or number of jobs resident in the GPU, and number of compute networks in the GPU.\r\n\r\nIs there an easy way in TensorFlow to assess the size in CUDA cores of a compute network?\r\n\r\nQuestion: What are the factors I can use to optimize CUDA load in a small GPU with a large deep learning task?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I did that.  Here are some unanswered threads:\r\n- https://stackoverflow.com/questions/55313399/relationship-between-batch-size-and-cuda-load-in-deep-learning-task\r\n- https://datascience.stackexchange.com/questions/47918/how-can-i-increase-cuda-load-on-a-tensorflow-deep-learning-task-after-reducing-b?noredirect=1#comment54931_47918\r\n\r\nIt's safe to say that\r\n- You can count the number of people in the world on one or two hands that truly understand how Tensorflow consumes memory and cores in the GPU\r\n- I will be getting a lot more familiar with this: https://github.com/tensorflow/profiler-ui and this; https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler\r\n- I will be getting more familiar with nvidia-smi and re-acquainting myself with NVidia Parallel NSight if I can run that on Python.exe and get anything out of it.\r\n\r\nMemory and core usage is an area that is truly opaque and confusing in TensorFlow, and there is no good venue to learn about it, I just have to grind it out."]}, {"number": 27061, "title": "Added support for 8-bit Quantization for LeakyRelu", "body": "This is also part of issue #26755", "comments": ["@suharshs , thanks for the review, i have updated the code as per your suggestion, kindly check and approve.\r\n\r\nRegards\r\nAmit", "@rthadur , resolved the merged conflicts.\r\n\r\n@suharshs , sorry for the trouble , i resolved the merged conflicts can you please approve again.\r\n\r\nRegards\r\nAmit", "@jianlijianli , thanks ,i really appreciate your suggestions, however since this issue seems to be faced\r\nby many can you please recheck and approve.\r\n\r\nRegards\r\nAmit", "@suharshs & @jianlijianli , I have rebased the code again and pushed, kindly approve the same.\r\n\r\nRegards\r\nAmit", "@suharshs and @jianlijianli , really appreciate you spending time on this PR< this is kind of important PR,and i would like to see it though, can you please help to review this PR.\r\n\r\nRegards\r\nAmit"]}, {"number": 27060, "title": "Where is the api of PhasedLSTM in tensorflow 2.0.0 alpha? ", "body": "Just like the title suggests, where can I find the PhasedLSTM api, or there is still lacking of that? Thanks\r\n", "comments": ["`tf.contrib.rnn.PhasedLSTM` was deprecated, as outlined [here](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md#list-of-projects); however, it is in the process of being replaced with the new RNN API and [`tf.keras.layers.*`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers). In the mean time, you should be able to recreate the `PhasedLSTM` functionality you need using the latter.", "Closing this issue since it was resolved. Feel free to reopen if have any additional questions. Thanks!", "I'd like to make use of Phased LSTMs for a project I'm working on; do you have any information about when this is likely to be integrated into the `tf.keras.layers` API? The Phased LSTM is currently a little unwieldy to use as I'm having to mix the `tensorflow.contrib.rnn` API with the Keras `RNN` API together to get it to work."]}]