[{"number": 9397, "title": "hi  tensorflow", "body": "Please go to Stack Overflow for help and support.\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues.\r\nWe want to focus on work that benefits the whole community, e.g., fixing\r\nbugs and adding features. Support only helps individuals. GitHub also notifies\r\nthousands of people when issues are filed. We want them to see you communicating\r\nan interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can collect the TensorFlow version with\r\n```sh\r\npython -c \"import tensorflow as tf; print (tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached. Try to provide a reproducible test case that is the minimum\r\nnecessary to generate the problem.\r\n", "comments": []}, {"number": 9396, "title": "Delete unnecessary forward declarations", "body": "These are made in tensorflow/stream_executor/stream_executor_internal.h\r\nAdditionally, RngSupport is within perftools::gputools::rng rather than\r\nperftools::gputools::internal.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 9395, "title": "Use AppendShape() instead of looping over and adding the dims of output_matrix_shape individually", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9394, "title": "Max_pool with dynamic ksize", "body": "I have the following code for a convolutional layer. This layer is part a larger computational graph.\r\n\r\n```\r\n# Define the shape of the filter\r\nfilter_shape = [1,\r\n                config.char_filter_size,\r\n                config.dim_char,\r\n                config.dim_char]\r\n\r\n# Define the convolutional layer weights and biases\r\nW_conv = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1),\r\n                     name=\"W_conv\")\r\nb_conv = tf.Variable(tf.constant(0.1, shape=[config.dim_char]),\r\n                     name=\"b_conv\")\r\n# Do 2d convolution\r\nconv = tf.nn.conv2d(char_embeddings,\r\n                    W_conv,\r\n                    strides=[1, 1, 1, 1],\r\n                    padding=\"VALID\",\r\n                    name=\"conv\")\r\n# Apply nonlinearity\r\n# h_conv has the same shape as conv\r\nh_conv = tf.nn.relu(tf.nn.bias_add(conv, b_conv),\r\n                    name=\"conv_relu\")\r\n# Maxpooling h_conv over dim 2 (char dim)\r\n\r\n# ERROR HERE\r\nconv_pooled = tf.nn.max_pool(h_conv,\r\n                             ksize=[1, 1, tf.shape(h_conv)[-2], 1],\r\n                             strides=[1, 1, 1, 1],\r\n                             padding='VALID',\r\n                             name=\"conv_max_pool\")\r\n```\r\n\r\nWhen trying to run, I get the error:\r\n\r\n> TypeError: Expected int for argument 'ksize' not tf.Tensor shape=() dtype=int32.\r\n\r\nis tf.nn.max_pool unable to handle dynamic ksize?", "comments": ["This looks like it should work via Derek's auto-stack machinery.  I'll briefly try to reproduce...", "Ah, correct: `tf.nn.max_pool` does not support dynamic size.  `ksize` is an attr, therefore must be a constant.  This would be reasonable to fix, but would require a `MaxPoolV2` op kernel similar to `TopKV2`.  I'll leave it open as contributions welcome.", "Duplicate of https://github.com/tensorflow/tensorflow/issues/4746.", "tf.reduce_max(activation, axis=1, keep_dims=True)", "see https://stackoverflow.com/questions/46760927/using-the-shape-of-a-tensor-with-dynamic-shape-in-tensorflow-operations"]}, {"number": 9393, "title": "Quantize Graph - check whether the key exists before using it", "body": "It is a simple fix for a hidden bug in `tensorflow/tools/quantize_graph`, the existence of the node key wasn't checked before using it.\r\n\r\nTests in `//tensorflow/tools/quantization:quantize_graph_test` are all passed.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9392, "title": "Build of tensor flow r1.1 w/ Google Cloud option enabled fails: incompatible C++ header and code", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:   OS X 10.10.3 (14D2134)\r\n- **TensorFlow installed from (source or binary)**:  github clone / source (r1.1)\r\n- **TensorFlow version (use command below)**: r1.1\r\n- **Bazel version (if compiling from source)**: 0.4.5-homebrew\r\n- **CUDA/cuDNN version**:  N/A (not built with CUDA)\r\n- **GPU model and memory**: N/A (not built with CUDA)\r\n- **Exact command to reproduce**:  \r\n1. configure build for CPU and with google cloud enabled,  \r\n2. bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nBuild of tensor flow for CPU / google cloud enabled fails for r1.1 (and also master),\r\n\r\n1. configure build for CPU and with google cloud enabled,  \r\n2. bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Source code / logs\r\nERROR: /Users/jshore/src/tensorflow/tensorflow/core/platform/cloud/BUILD:115:1: C++ compilation of rule '//tensorflow/core/platform/cloud:retrying_utils' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 93 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/core/platform/cloud/retrying_utils.cc:97:9: error: return type 'const tensorflow::Status' must match previous return type 'tensorflow::Status' when lambda expression has unspecified explicit return type\r\n        return status;\r\n", "comments": ["Right.\r\n\r\n```\r\nconst auto& status = delete_func();\r\n```\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cloud/retrying_utils.cc#L92\r\nshould be:\r\n```\r\nconst Status status = delete_func();\r\n```\r\n\r\nWould you mind sending a PR?"]}, {"number": 9391, "title": "Is it possible to remove the \"third_party directory\"?! [An attempt to package for Ubuntu]", "body": "Hi guys,\r\n\r\nWe have successfully used C/C++ API as shared library in our applications, but there are some issues [like this one](https://github.com/cjweeks/tensorflow-cmake/issues/13) which my colleages ran into.\r\nSimultaneously, I was working to distribute the whole work as a debian package for Ubuntu 16.04, and I found that it is very annoying that you have used many external dependencies under the \"third_party\" directory. I'm trying to remove these dependencies one by one, which is a little bit troublesome but I think is possible. I am packaging anything else needed on my way.\r\n\r\nThe question is, do you have any advise on this? What should have been taken care of? And why in first place you put these in your repo instead of leading the users to install specific versions on their system as build dependency? I think that having external header files under the directory after build does not make sense at all! Please lighten me up.\r\n\r\nThanks for your great work, Google and The Community!\r\n", "comments": ["The tensorflow executable relies on a hermetic build, to ensure that we build the dependent libraries at the right version that we test, and using the right compilation flags. For instance, LLVM libraries typically requires `-fno-rtti`, which requires careful compilation. We link statically to these dependencies, making your executable effectively unstable if you link against these libraries outside of tensorflow.\r\n\r\nTechnically the right thing is to seal up tensorflow with `-fvisibility=hidden` to avoid polluting the linkage namespace. It is not a difficult thing to do but we have been postponing the fix.\r\n\r\nCC: @keveman @skye ", "I have been looking into your binary distribution (`libtensorflow-cpu-linux-x86_64-1.1.0.tar.gz`) and one of the first thing for integration I looked into symbol visibility. Please use `-fvisibility=hidden` and only make globally visible symbols from API. There is a good fraction of externals that we already link to (jemalloc, Eigen, etc) thus it would be nice if tensorflow library would be sealed up.", "I tried to hide \\*google::protobuf\\* in version script, but it didn't work, I don't know exactly what to hide! I tried to keep only _TF* symbols like in the c_api, but the resulting binary was useless. Please let me know what approximately I have to put in my version script so I will figure it out. ", "@nkhdiscovery, @girving is going to do this as part of #9525. It's not a trivial change and it's probably better if we did this on our end.\r\n\r\nI'm going to close this in favor of #9525 so that every can keep track of progress there.\r\n\r\nThanks."]}, {"number": 9390, "title": "Full set of changes to ISSUE_TEMPLATE.md", "body": "A few changes got missed in https://github.com/tensorflow/tensorflow/pull/9385, so I'm adding them back here.", "comments": ["Thanks!"]}, {"number": 9389, "title": "Removed legacy rnn constructors", "body": "These are deprecated. But someone still needs to check whether they are being used internally.", "comments": ["@tensorflow-jenkins test this please", "@caisq FYI It's actually usually auto-triggered for me since I am in TF Github org. Thanks though!", "Note: this can go in after 154336469.", "Done! @martinwicke "]}, {"number": 9388, "title": "Added StateSavingRnnEstimator in __init__ of contrib.learn", "body": "", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins Test this please", "@zheng-xq this can be merged. (not merging myself only because I don't want to mess with your sync if you're doing one -- conflicts etc.)", "Unlikely to cause conflicts -- I'll merge."]}, {"number": 9387, "title": "Wrong implementation of ResNet in tensorflow/tensorflow/contrib/slim/python/slim/nets/ ?", "body": "I'm a researcher working on models related to ResNet_v1. As I was trying to use the code in this reporsitory: tensorflow/tensorflow/contrib/slim/python/slim/nets/\r\nI realized that the ResNet_v1 model is different from Kaiming He's implementation. Specifically, in Kaiming's implementation, he applied stride 2 at the START of each block and the stride is applied to the 1*1 conv layer. For example, in Kaiming's original caffe prototxt (https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-101-deploy.prototxt), this is the first layer of 'conv3' (named in his ResNet paper) or block 2 (named by your implementation):\r\n\r\n```\r\nlayer {\r\n\tbottom: \"res2c\"\r\n\ttop: \"res3a_branch1\"\r\n\tname: \"res3a_branch1\"\r\n\ttype: \"Convolution\"\r\n\tconvolution_param {\r\n\t\tnum_output: 512\r\n\t\tkernel_size: 1\r\n\t\tpad: 0\r\n\t\tstride: 2\r\n\t\tbias_term: false\r\n\t}\r\n}\r\n```\r\n\r\nThis layer is a 1*1 conv layer that mimics the 'skip-connection' because of dimension mismatch at the bottleneck between blocks. And so does 'res3a_branch2a', which is the start of the first residual module in 'conv3'.\r\n\r\nInstead, in the tensorflow implementation, the stride 2 is applied to the END of each block(see function **resnet_v1_block()** in resnet_v1.py) and the stride is applied to the 3*3 conv layer of the last Residual module in each block(see function **bottleneck()** in resnet_v1.py). \r\n\r\nIt's obvious that these two are different. Can anybody explain if it is wrong or implemented as such for some reasons?", "comments": ["@sguada what do you think?", "Both implementation are equivalent, but the current one is more efficient.\r\n\r\nApplying \r\n`conv([3, 3], stride=1, scope='conv_3x3a') followed by conv([1,1], stride=2, scope='conv_1x1a')`\r\nis the same as\r\n`conv([3, 3], stride=2, scope='conv_3x3b') followed by conv([1,1], stride=1, scope='conv_1x1b')`\r\n\r\nHowever, the first approach the conv_3x3 computes a lot of activations that the conv_1x1 is going to skip.\r\n\r\nLet's assume that the output of 'conv_3x3a' is `[a, x, b, x, c]` then 'conv_1x1a' is going to skip all the 'x' due to stride=2 and going to produce `[a2, b2, c2]`. So all the memory and computation used to generate the 'x's is wasted.\r\n\r\nWhile the output of 'conv_3x3b' is `[a, b, c]` due to stride=2, then 'conv_1x1a' is going to still produce produce `[a2, b2, c2]`\r\n", "@sguada Hi. Could you please explain the equivalence and why it's more efficient?", "see my updated comment", "@sguada Thank you for the great explanation. Another doubt about the ResNet implementation I have is that in the tf slim resnet_v1_50, resnet_v1_101 and resnet_v1_152 etc. implementations, downsampling is performed by block**1**, block**2**, and block**3**; while in the original ResNet paper, downsampling is performed by conv3_1(block**2** in your notation), conv4_1(block**3**), and conv5_1(block**4**) with a stride of 2. With the skip connections between the blocks involved, the equivalence of two implementations is not clear to me, could you please explain more? Thanks!"]}, {"number": 9386, "title": "Fix typo in encoders.py documentation", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Single test failure is unrelated. Merging PR."]}, {"number": 9385, "title": "Improve ISSUE_TEMPLATE.md", "body": "Largely based on https://github.com/tensorflow/tensorflow/pull/9199, which may have get lost as a result of a bad merge.", "comments": ["Jenkins, test this please."]}, {"number": 9384, "title": "Branch 153925676", "body": "", "comments": ["queue_runner_test failure on Mac was pre-existing. Merging PR now."]}, {"number": 9383, "title": "Issue with latest Cuda Install for Windows", "body": "Took a little digging but the new DLL from Nvidia is called cudnn64_6.dll; the latest build of TensorFlow requires that to be cudnn64_5.dll. Crashes importing TensorFlow until this is renamed.", "comments": ["Thanks for reporting. Maybe we should document this better.\r\n\r\n/CC: @yifeif @wolffg ", "Thank you for this. I've been having issues trying to get cudnn64_6.dll to work.\r\n\r\nI'll try cudnn64_5.dll", "If you know where to get it and the lib/h files that go with it - I would appreciate it.", "You have to sign up. \r\nhttps://developer.nvidia.com/cudnn\r\n\r\nDownload cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0"]}, {"number": 9382, "title": "Tensorboard not (correctly) displaying histograms; unreliably displaying graphs (V1.1rc2,win7,chrome57)", "body": "When I went from tensorflow V1.0 to V1.1rc2, histograms stopped working correctly: the data isn't drawn unless that trace is highlighted via mouseover, and even then it's not using the right plot boundaries:\r\n![image](https://cloud.githubusercontent.com/assets/8495647/25305145/f3be2926-2743-11e7-8ed1-589d4b17452e.png)\r\nis an example (cursor is not displayed but is at the dot in top left of graph; otherwise graph is blank) generated from this code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nxx = tf.Variable(tf.random_normal([4,4], dtype=np.float32),dtype=tf.float32)\r\nyy = tf.Variable(np.eye(4, dtype=np.float32), dtype=tf.float32)\r\nzz = tf.matmul(xx, yy)\r\nsave_location = 'g:\\\\tmp\\\\lstm3\\\\dbg\\\\dbg'\r\ntf.summary.histogram('tensorboard_no_like', zz)\r\nmerged = tf.summary.merge_all()\r\nsess = tf.InteractiveSession()\r\ntrain_writer = tf.summary.FileWriter(save_location, sess.graph)\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\nqwert = zz.eval()\r\nm = sess.run(merged)\r\ntrain_writer.add_summary(m)\r\n\r\n```\r\nI haven't tried this toy repro with v1.0, but other models that I've done in 1.0 and 1.1rc2 exhibit the same behavior. The distributions look fine, and scalars also display fine.\r\n\r\nAnother problem that's new to me between 1.0 and 1.1rc2 is that graphs sometimes display fine, sometimes are blank until I reload browser page a few times. (Apologies if this should have been a separate issue; wanted to keep spam volume down). I don't have a strong idea of what triggers this, but the frequency is pretty high, roughly half the time.\r\n\r\nthe console running tensorboard emits the following warning periodically, the timing of which I haven't correlated with either problem mentioned above:\r\n\r\n> WARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\n\r\n### System Information\r\n- using custom code, copied above\r\n- Windows 7 64 bit, fully patched\r\n- tensorflow 1.1rc2 downloaded from https://pypi.python.org/packages/0d/cb/25f2cdd8905070373945c1f57edbe9d5f51a4482aa7097e5613cbdc4a41f/tensorflow_gpu-1.1.0rc2-cp35-cp35m-win_amd64.whl#md5=80ccd71614b438ffe8af3cd0dd572d3b and installed via pip\r\n- CUDA 8.0 CUDNN 5.1\r\n- GTX 1080 ti, 11gb ram\r\n- generate histogram's data (eg, with code above), start tensorboard, (fail to) view histogram; view graph, experience intermittent success\r\n", "comments": ["@dandelionmane any insight on this?", "both these issues appear to be fixed with v1.1.0 so I'm going to go ahead and close this issue.\r\n\r\neta: it's still emitting the warning but that doesn't seem to have anything to do with the issues I raised.\r\n\r\n> WARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404"]}, {"number": 9381, "title": "Mac Gpu Link not working?", "body": "Hello,\r\n\r\nI was trying to install Tensorflow for Mac GPU Python 3 one from github and I found that it is a broken URL Giving me an HTTP 404 Error.\r\n\r\nThe URL is - `https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.1.0rc2-py3-none-any.whl`\r\n\r\nPlease Fix the Url.\r\n\r\n\r\nBest,\r\nDaksh", "comments": ["Where did you copy that URL from?", "From the readme of Tensorflow's Github.", "I was wondering about this as well. \r\n\r\nLooks like the build for nightly Mac GPU Python 3 is currently broken, and for this reason link to the last successful build of nightly Mac GPU Python 3 version _1.1.0_ is broken. Same issue with nightly Mac GPU Python 2 1.1.0 link and build."]}, {"number": 9380, "title": "decode the read bytes before passing to text_format.Merge", "body": "When using `freeze_graph.py` in Python 3, there will be error output as follows:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"../tensorflow/tensorflow/python/tools/freeze_graph.py\", line 220, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"$HOME/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"../tensorflow/tensorflow/python/tools/freeze_graph.py\", line 152, in main\r\n    FLAGS.variable_names_blacklist)\r\n  File \"../tensorflow/tensorflow/python/tools/freeze_graph.py\", line 98, in freeze_graph\r\n    text_format.Merge(text, input_graph_def)\r\n  File \"$HOME/anaconda3/lib/python3.6/site-packages/google/protobuf/text_format.py\", line 472, in Merge\r\n    text.split('\\n'),\r\nTypeError: a bytes-like object is required, not 'str'\r\n```\r\n\r\nAfter performing the fix in this patch, there will be no more `TypeError` like this.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "It seems that the failure was not caused by this revision", "@kumasento Two of the three test failures are unrelated. But this one is:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/4194/consoleFull\r\n\r\n(See //tensorflow/python/tools:freeze_graph_test)\r\n\r\nPlease note that the code needs to work on both python 2 and python 3.", "@tensorflow-jenkins test this please", "@caisq Thanks for your comments. I have updated the code. However, I tried to call the CI, but I don't have the privilege to do so. Could you please help me schedule a test for this PR? Thanks!", "@tensorflow-jenkins test this please", "@caisq Thanks, could you please let me know how could I look into the error message in the failed test?", "Oh I found them, no worries", "Ah, I get the point of the test error: The latest `FastGFile` API in the master branch has been updated and will return type `str` rather than `bytes` if the mode is `r`. So there is no point to add `decode`! I think it is OK to close this PR. Thank you again for the help recently!"]}, {"number": 9379, "title": "[C++] xthread:  0xC0000005: Access violation reading location 0xFFFFFFFFFFFFFFFF", "body": "I want to read frozen graph from file.\r\nSo I have class A:\r\n\r\n```\r\nclass A\r\n{\r\n     std::shared_ptr<B> b_ptr;\r\npublic:\r\n     A()\r\n     {\r\n         b_ptr.reset(new B());\r\n     }\r\n};\r\n```\r\n\r\nclass B:\r\n\r\n```//include tensorflow\r\nusing namespace tensorflow;\r\nclass B\r\n{\r\n     SessionOptions _sessionOptions;\r\n     std::unique_ptr<Session> _session;\r\n     GraphDef _graph;\r\n     std::shared_ptr<std::thread> _thread;\r\npublic:\r\n     B()\r\n     {\r\n         graph::SetDefaultDevice(\"/cpu:0\", &_graph);\r\n         ConfigProto& config = _sessionOptions.config;\r\n\t config.set_intra_op_parallelism_threads(1);\r\n\t _session.reset(NewSession(_sessionOptions));\r\n         ReadBinaryProto(Env::Default(), \"path/to/graph\", &_graph);\r\n\t _session->Create(_graph);\r\n         _thread.reset(new std::thread(&B::Task, this));\r\n     }\r\n     void Task();\r\n}\r\n```\r\n\r\nAnd I have the problem mentioned above when I read frozen graph with ReadBinaryProto  ~~I create session~~. If I comment a line _thread.reset(new std::thread(&B::Task, this)), this code will return successfully. If I comment all the code refered with tensorflow it will return successfully too.", "comments": ["Are you using the C++ api (not loading a shared library)? Did you try building with asan/msan?\r\n\r\n@asimshankar might have some insight.", "I load tensorflow as shared library. No, I didn't", "How are you linking your library? Against which proto library?\r\n\r\nCC @keveman ", "I apologize for not mentioning this earlier. I use windows. I run cmake and build it via MSBUILD. Then I include tensorflow lib and other dependencies in my linker options. So how I mentioned earlier If I do not create another thread, it works good.", "Are you running MSVCRTD ? (debug version) Are you building in debug? What does `Task` do?\r\nCould you also check the status codes with `CHECK_OK(..)` around `ReadBinaryProto` and `SetDefaultDevice` and `Session::Create`?", "As I know tensowflow cannot be built in debug ?!\r\nI am running Release version.\r\nTask has infinite loop that run tensorflow session and postprocess its results.\r\n\r\nI check the status codes in my code. I didn't add them in order to reduce code size.", "Any chance you could get the stack traces? Does it crash during program exit, or before?", "I was wrong. It fails when session resets: \r\n`_session.reset(NewSession(_sessionOptions));`\r\nIf I comment this line:\r\n`_thread.reset(new std::thread(&B::Task, this));`\r\nwhich is actually located after the previous one, exception is thrown.", "OK, what does the status return?", "It does not return any status. It throws exception.", "Any chance to get the stack trace?", "This is everything that I managed to get:\r\n```\r\n>\tTestTensorflow.exe!std::_LaunchPad<std::unique_ptr<std::tuple<void (__cdecl B::*)(void) __ptr64,B * __ptr64>,std::default_delete<std::tuple<void (__cdecl B::*)(void) __ptr64,B * __ptr64> > > >::_Run(std::_LaunchPad<std::unique_ptr<std::tuple<void (__cdecl B::*)(void),B *>,std::default_delete<std::tuple<void (__cdecl B::*)(void),B *> > > > * _Ln) Line 247\tC++\r\n\r\n \tTestTensorflow.exe!std::_Pad::_Call_func(void * _Data) Line 210\tC++\r\n\r\n \tucrtbase.dll!_o__realloc_base\u001e()\tUnknown\r\n\r\n \tkernel32.dll!BaseThreadInitThunk\u001e()\tUnknown\r\n\r\n \tntdll.dll!RtlUserThreadStart\u001e()\tUnknown\r\n\r\n", "I solved this problem using tensorflow::thread::ThreadPool instread of std::thread", "If you have solved it please close it. On the other hand, this problem is difficult to help you solve since you didn't give us the code in B::Task. You also didn't show us the calling code. I'd guess that B was getting destroyed because A was getting destroyed after your main thread died but the thread you started survived and accessed this now deleted memory. It's also unclear why you made b_ptr shared and the thread ptr shared. You should prefer unique_ptr's unless you absolutely need shared ownership.", "Ok. I delete all my code from B::Task(). Now this method doing nothing. And I still have the error.\r\n\r\nCalling code something like this:\r\n```\r\nA a();\r\n```", "Where does it crash? During deletion of `a` or at program exit?", "Neither.\r\nIt crashes in this line:\r\n```\r\nReadBinaryProto(Env::Default(), \"path/to/graph\", &_graph);\r\n```\r\n", "BTW, `SetDefaultDevice` won't do anything since the graph is empty when you start.\r\n\r\nDid you call\r\n```\r\n  tensorflow::port::InitMain(argv[0], &argc, &argv);\r\n```", "No, I didn't.\r\nWhat should I do if I don't want to read params from command line arguments?", "Just construct a simple argv which has only one element.", "Closing due to lack of recent activity. Please reopen if this is still an issue.", "I have the same issue, @mrlzla were you able to solve this?\r\nFor me it fails on line:\r\n`tensorflow::ReadBinaryProto(tensorflow::Env::Default(), pathToGraph, &graph_def);`\r\nand the error message is \r\n> Unhandled exception at 0x00007FFC81116988 (ntdll.dll) in main.exe: 0xC0000005: Access violation reading location 0xFFFFFFFFFFFFFFFF.\r\n\r\nI compiled both tensorflow + protobuf as a shared library in Release mode using MSVC compiler", "@sergeyshilin, I don\u2019t exactly know if it helps but you can try to change compiler flag O3(full optimization) to O2 or try to use tensorflow::ThreadPool instead of std::thread.", "@mrlzla The problem to me is that I am trying to link tensorflow library built in Release mode to my solution in Debug. When linking Release -> Release, everything works properly. Since I still do need Debug environment in our application, I will try to figure out if it's possible to get rid of this error message shown above.\r\n  ", "@sergeyshilin try to set all compiler flags in debug mode the same as in release", "@sergeyshilin Did you ever figure out a solution to this? I have the same need of debug with a tensorflow dependency ", "@jtavrisov, it does not seem to have a solution for now, because tensorflow simply does not support a Debug mode. I decided to switch to a Release mode with no optimization properties, therefore I still can debug my own application and use tensorflow library without debugging it. To do so in VS, Microsoft suggests to follow the instruction [https://msdn.microsoft.com/en-us/library/fsk896zz.aspx](https://msdn.microsoft.com/en-us/library/fsk896zz.aspx)"]}, {"number": 9378, "title": "gcc failed: error executing command", "body": "`$ bazel build tensorflow/tools/graph_transforms:transform_graph:`\r\n\r\n```\r\nERROR: /home/osboxes/tensorflow/tensorflow/core/kernels/BUILD:2083:1: C++ compilation of rule '//tensorflow/core/kernels:svd_op' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 115 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.\r\nSlow read: a 607144-byte read from /home/osboxes/.cache/bazel/_bazel_osboxes/a2c72928f8ca1d272e54d2254eb1b173/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/core/kernels/_objs/strided_slice_op/tensorflow/core/kernels/strided_slice_op.o took 96724ms.\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n```\r\n\r\n[gccFailed.txt](https://github.com/tensorflow/tensorflow/files/948805/gccFailed.txt)\r\n\r\nUbuntu 17.04", "comments": ["We don't support ubutun 17 quite yet. What version of gcc are you using?\r\n\r\nI think it's likely to have been an out of memory error. Please check `dmesg` to verify. It should say that `cc1plus` has been sacrificed.", "`gcc (Ubuntu 6.3.0-12ubuntu2) 6.3.0 20170406`\r\n\r\nI changed vmware memory for this ubuntu from 4 to 6 GB, and seems it has been built ok\r\nand yeah during building PC used 80-90% of all 8 GB "]}, {"number": 9377, "title": "grpc error in distributed tensorlfow ", "body": "pciBusID 0000:04:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.10GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0)\r\n**# E0422 12:13:21.315971987   26528 tcp_server_posix.c:148]     check for SO_REUSEPORT: {\"created\":\"@1492834401.315943300\",\"description\":\"OS Error\",\"errno\":92,\"file\":\"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.c\",\"file_line\":181,\"os_error\":\"Protocol not available\",\"syscall\":\"setsockopt(SO_REUSEPORT)\"}**\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:8865}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8866}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:8865\r\n\r\nI try distributed tensorflow and start only one server and one worker, the model does not converge as the local version.", "comments": ["Please report OS and version.", "Linux version 2.6.32_1-13-0-0", "(gcc version 4.4.4 20100726 (Red Hat 4.4.4-13) (GCC) )            CentOS release 6.3 (Final)", "What distribution?\n\nOn Apr 22, 2017 6:56 PM, \"TomorrowIsAnOtherDay\" <notifications@github.com>\nwrote:\n\n> Linux version 2.6.32_1-13-0-0\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9377#issuecomment-296413898>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbYo6ebCtiGYsZ_FE0ALb-KXUuZEtks5ryq_lgaJpZM4NE-DL>\n> .\n>\n", "2.6.32 is a very old kernel... I don't think it supports `SO_REUSEPORT`, which as far as I can tell was [merged in 2013](https://lwn.net/Articles/542629/) (to Linux 3.9).\r\n\r\nAs far as I remember, we don't actually use this feature but gRPC enables it by default on some sockets. We actually try to [disable it on server sockets](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L58), because it wreaks havoc with our unit tests.", "@TomorrowIsAnOtherDay I am closing since this is not a supported platform. Your options, at this juncture, are to either disable that feature in your private copy of tensorflow and rebuild, or upgrade your kernel. The comment on [stackoverflow](http://stackoverflow.com/questions/3261965/so-reuseport-on-linux) indicates that there was a backport for `2.6.32`, so this might be the most viable option.", "I will try to update my kernel, thanks for your reply."]}, {"number": 9376, "title": "PeriodicResample Op: Fixes #9369", "body": "This implements sub-pixel shuffling in the module `contrib`. https://github.com/tensorflow/tensorflow/issues/9369", "comments": ["Can one of the admins verify this patch?", "I jumped the gun and made the pull request before getting feedback on issue #9369. Thus, I'll CC @martinwicke here.", "Can you rebase against master? There are some changes in your branch that are not in master.", "Before we go into details, I am wondering whether shuffle is the best name. I associate shuffling with some sort of random permutation being applied, which clearly isn't the case here.\r\n\r\nThe example in the docstring didn't really make it clear to me, it just looks like reshape (and so does the test), can you explain what this actually does?", "@martinwicke I think your concern regarding the name of the operation is valid. I had difficulty coming up with a generic enough operation name and \"shuffle\" was the best I could come up with. Maybe \"periodic_arrange\" or \"periodic_intersperse\" may be better, but I'm open to suggestions.\r\n\r\nRegarding what the operation actually does, it performs a generic version of the sub-pixel convolution found [here](https://arxiv.org/pdf/1609.05158.pdf) (Figure 1). My version allows for analogous operations for tensors of arbitrary rank and and relaxes a few of the other restrictions. For instance, the operation can handle rearranging a tensor of shape [2,3,6] into a tensor of shape [6,6,1]. I do need to add more examples and tests. I'll devise more tests and examples.\r\n\r\nIn the mean time, I currently only perform `assert`s rather than actually throwing errors. Can you recommend a format for raising exceptions, etc. in TF?", "Errors should be reported using the OP_REQUIRES macros. You can look at other kernels for examples. They will cleanly return with an appropriate Status.", "ping for @jhetherly ! \r\n\r\nTrying to get our active PR count down, so a response would be appreciated!", "I'm in the process of correctly converting the `assert`s to using the `OP_REQUIRES ` macro. I'll try to have something ready for further review by tomorrow.", "Just to update: I've had a bit of trouble with hardware failure which has slowed my progress. I will still try to have something by the end of the day.", "I've committed an updated name (`periodic_intersperse`), proper error handling, and more mathematical detail regarding what the transformation does located in the documentation string (tensorflow/contrib/periodic_intersperse/core/ops/array_ops.cc). The math is written in Latex, so I was curious if TF includes MathJax in it's documentation pages. If not, is it possible to specify this in the documentation string?", "Can one of the admins verify this patch?", "@suharshs would you have time to review this?", "I pushed the changes that @suharshs recommended.", "We do support MathJax. There's a bunch of math in the tutorials, and the code docs go through the same pipeline.", "Hey @suharshs, I just uploaded the files after running clang-format", "Thanks, I am gonna ask Rasmus to take a closer look (or find someone more knowledgeable than me) at the op implementation, which i am not too familiar with.", "I think \"periodic_intersperse\" is a very confusing name. This feel more like a (nearest neighbor) resampling or slicing operation. Is there an equivalent numpy method from which we can get inspiration for a better name?", "How does periodic_resample sound, @rmlarsen?", "@rmlarsen, if you think my proposed name of the op is suitable (periodic_resample) I can upload the change rather quickly", "@jhetherly periodic_resample sounds good.", "@jhetherly @martinwicke @shlens Before we proceed, I'd like to question if this is a general enough op to eventually belong in core TensorFlow?", "@rmlarsen,\r\nI'm currently battling an issue similar to #10436 in getting the current master to compile. Regardless, once I resolve this issue I'll implement the desired changes and comment on each issue in turn. Regarding your comment on making this a core op, this is how I envisioned it eventually being used as it's quite generic.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@jhetherly Just to be clear, did you address all reviewer comments in your last push?", "@drpngx no, not yet. I'll likely have addressed all of them by the end of today", "drpngx@ @jhetherly I will be on vacation until July 26, so I am unassigning myself.", "Thanks @rmlarsen !", "@drpngx and @rmlarsen: I've implemented the requested changes including more Python tests.", "Jenkins, test this please.", "BTW, the CLA switched to no. Did you add another author?", "@drpngx: I had to use another computer to make these recent changes and I'm not sure how to resolve this CLA issue. I'm not sure what email was used to commit the changes", "Could you fix the lint [errors](https://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4997/consoleFull)?", "@jhetherly any luck with this?", "@drpngx just uploaded the fixes (just indentation issues)", "Jenkins, test this please.\n\nOn Jun 30, 2017 6:44 PM, \"Jeff\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> just uploaded the fixes (just\n> indentation issues)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-312403150>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbcoKaMvTwoJfOI8i2mjADWsHDnWjks5sJaSPgaJpZM4NE8mk>\n> .\n>\n", "@jhetherly could you rebase and address build failures?", "ping for @jhetherly ! ", "Jenkins, test this please.", "I've committed a change that resolves the conflicts in the two offending files. I'm recompiling now, but I think this fixes the issue.", "@vrv: just pushed after removing those lines\r\nit compiles and runs fine for me, so I think we're good to go", "@tensorflow-jenkins test this please", "I see there are several failures, but I can't seem to track down why in any of the logs and many of them say they've simply been 'aborted by unknown.' Is this something I should be concerned about?", "Maybe server reboot\n\nJenkins, test this please\n\nOn Jul 25, 2017 9:02 PM, \"Jeff\" <notifications@github.com> wrote:\n\n> I see there are several failures, but I can't seem to track down why in\n> any of the logs and many of them say they've simply been 'aborted by\n> unknown.' Is this something I should be concerned about?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-317838220>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbSd32VowTHA7nCTKaMtZfLO6gjlxks5sRjumgaJpZM4NE8mk>\n> .\n>\n", "I think it's because the sanity check failed (which aborts everything else).  \r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/5397/console\r\n\r\nSuggests that the problem is that the BUILD files are failing 'buildifier'.  If you see our CONTRIBUTING.md file, there will be instructions for how to automatically format BUILD files (kind of like clang-format).  That should probably solve the problem!", "@vrv: I've searched for this formatting specification and haven't found it. I located the sanity-check script that requires Docker, however I'm not at a machine that can run Docker. I've modified the BUILD script, but I'm not sure it'll pass the sanity-check.", "Okay, let's try it out and see!  If not I'll try to help.  @tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Ok sanity check passes now, but now we see the following error:\r\n\r\n[11,471 / 16,873] Compiling tensorflow/contrib/periodic_resample/core/ops/array_ops.cc\r\nERROR: /workspace/tensorflow/contrib/periodic_resample/BUILD:15:1: C++ compilation of rule '//tensorflow/contrib/periodic_resample:all_ops' failed: gcc failed: error executing command \r\n  (cd /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/contrib/periodic_resample/_objs/all_ops/tensorflow/contrib/periodic_resample/core/ops/array_ops.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/contrib/periodic_resample/_objs/all_ops/tensorflow/contrib/periodic_resample/core/ops/array_ops.o' -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/contrib/periodic_resample/core/ops/array_ops.cc -o bazel-out/local-opt/bin/tensorflow/contrib/periodic_resample/_objs/all_ops/tensorflow/contrib/periodic_resample/core/ops/array_ops.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/core/framework/op.h:23:0,\r\n                 from tensorflow/contrib/periodic_resample/core/ops/array_ops.cc:17:\r\nbazel-out/local-opt/genfiles/tensorflow/core/framework/op_def.pb.h:9:42: fatal error: google/protobuf/stubs/common.h: No such file or directory\r\n #include <google/protobuf/stubs/common.h>\r\n                                          ^\r\n\r\nso probably some protobuf dependency is missing, I suspect?", "Ok i've updated the build rule similar to another custom op library BUILD rule I saw, not sure how this works though..\r\n\r\n@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please (eigen dep)", "Almost good, just one error it seems, related to exposing symbols that we shouldn't be.\r\n\r\n======================================================================\r\nERROR: testBuildDocs (__main__.BuildDocsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/tools/docs/build_docs_test.runfiles/org_tensorflow/tensorflow/tools/docs/build_docs_test.py\", line 67, in testBuildDocs\r\n    raise RuntimeError(msg)\r\nRuntimeError:           Modules nested too deep:\r\ntf.contrib.periodic_resample.python.ops.periodic_resample_op.loader.load_library.errors_impl.c_api.ctypes.util\r\n\r\nThis is likely a problem with an accidental public import.\r\n\r\n          ****************************************************************\r\n          If this test fails here, you have most likely introduced an\r\n          unsealed module. Make sure to use `remove_undocumented` or similar\r\n          utilities to avoid leaking symbols. See above for more information\r\n          on the exact point of failure.\r\n          ****************************", "Perhaps look at some of the other modules that use 'remove_undocumented' to see how we 'seal' the APIs?  I think that's hopefully the last thing before this could go in!", "@vrv: I think I've corrected the issue. I'm recompiling right now and will run the test script afterwards. I'll let you know the results of the test.", "Jenkins, test this please\n\nOn Jul 27, 2017 7:01 PM, \"Jeff\" <notifications@github.com> wrote:\n\n> @vrv <https://github.com/vrv>: I think I've corrected the issue. I'm\n> recompiling right now and will run the test script afterwards. I'll let you\n> know the results of the test.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-318423422>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTJF-oRrlZGOwjV6rruvNe-4_nCbks5sSMKDgaJpZM4NE8mk>\n> .\n>\n", "I merged the GRPC change where the name of the protobuf was updated, so I've gone ahead and changed it here too.\r\n\r\n@tensorflow-jenkins test this please", "@vrv: I just completed the compilation and testing of the recent changes and everything seems to work okay. However, there also seems to be a few failures during testing that prevent merging and I can't seem to trace these failures down to my contribution (except for the Windows CMAKE test exclaiming that it can't import 'periodic_resample'). ", "I think all of the other failures are existing failures, so you don't have to worry about them -- the windows cmake one suggests that some changes to CMake files are needed -- can you take a look at that one?  Most likely by looking at how a similar PR handled the CMake change.", "@vrv: I'm not sure where these CMAKE files are that need modification. Additionally, after merging my repo with the current master it doesn't compile on my system and complains of missing instructions for protobuf.", "My guess is that the Cmake files you need to edit are somewhere here: https://github.com/tensorflow/tensorflow/tree/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/contrib/cmake, but I actually don't know how our CMake build works :(.  Maybe you can pattern match among one of the directories or entries there.\r\n\r\nAs for current master: you can try `bazel clean` before rebuilding -- we had to rename the protobuf module cause it was conflicting with external defintions.", "@vrv: Thanks a lot for the pointer. I will try to make the necessary changes tomorrow. It looks like quite a lot to modify.", "I've modified the CMake build files to include my periodic_resample op. Can we test this to see if it builds?", "@tensorflow-jenkins test this please", "Dang, it doesn't build :( . I don't know enough about the CMake build to help at the moment.  If you feel comfortable digging, let us know, otherwise we can try to find an expert next week to help you.", "It looks like the copy failed. The double trailing slash looks suspicious, maybe some undefined variable.  It might be some automatically-derived name that fails. It looks like you're using `array_ops.cc` and not `resample_ops`. Why is that?\r\n\r\n```\r\n23:48:47          Error copying file \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/Release/_periodic_resample_op.dll\" to \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tf_python/tensorflow/contrib/periodic_resample/python/ops//\".\r\n23:48:47     15>C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: The command \"setlocal [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\_periodic_resample_op.vcxproj]\r\n23:48:47 C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: \"C:\\Program Files\\cmake\\bin\\cmake.exe\" -E copy C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/Release/_periodic_resample_op.dll C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tf_python/tensorflow/contrib/periodic_resample/python/ops// [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\_periodic_resample_op.vcxproj]\r\n```", "@drpngx: I used `array_ops` as to mimic the core tensorflow file name. My thinking is that this could be a core op in the future and this naming scheme reflects this. Overall, I'm quite unfamiliar with tensorflow's CMake build structure. I'll try again to get the CMake build working, but I may need an expert on this point.", "Jenkins, test this please.", "@tensorflow-jenkins test this please", "@jhetherly It look like you need to add your new op to the Cmake build:\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/3767/consoleFull\r\n\r\n15:33:07 CMake Error at tf_python.cmake:612 (add_executable):\r\n15:33:07   Error evaluating generator expression:\r\n15:33:07 \r\n15:33:07     $<TARGET_OBJECTS:tf_contrib_periodic_resample_op>\r\n15:33:07 \r\n15:33:07   Objects of target \"tf_contrib_periodic_resample_op\" referenced but no such\r\n15:33:07   target exists.\r\n15:33:07 Call Stack (most recent call first):\r\n15:33:07   tf_python.cmake:703 (GENERATE_PYTHON_OP_LIB)\r\n15:33:07   CMakeLists.txt:288 (include)\r\n15:33:07 \r\n15:33:07 \r\n15:33:45 -- Generating done\r\n15:33:48 -- Build files have been written to: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build\r\n15:33:48 ", "@rmlarsen: I've tried writing the CMake build myself a few times myself and I can't get it to work. I've no access to a Windows machine, so it make iterating on this tedious. Is there an expert on Tensorflow's side that could help with this?", "I've resolved the conflicts. I guess I should've asked this earlier, but can this contribution be skipped in the CMAKE build so that the Window's Build can pass?", "@tensorflow-jenkins test this please", "@jhetherly I don't know enough about CMake to help. @gunan or @drpngx could perhaps  take a look?", "@mrry to recommend on cmake. What I see as the error is this:\r\n\r\n```\r\nCMake Error at tf_python.cmake:612 (add_executable):\r\n00:01:01.735   Error evaluating generator expression:\r\n00:01:01.735 \r\n00:01:01.735     $<TARGET_OBJECTS:tf_contrib_periodic_resample_op>\r\n00:01:01.735 \r\n00:01:01.735   Objects of target \"tf_contrib_periodic_resample_op\" referenced but no such\r\n00:01:01.735   target exists.\r\n00:01:01.735 Call Stack (most recent call first):\r\n00:01:01.735   tf_python.cmake:703 (GENERATE_PYTHON_OP_LIB)\r\n00:01:01.735   CMakeLists.txt:288 (include)\r\n```\r\n\r\nIf you exclude it your contribution from cmake, it will be excluded from windows. This may make future attempts to move this into core difficult. But if this is just meant to be excluded in windows builds you can revert all changes to the current cmake files, and add the exclusion of your test files to tf_tests.cmake.", "I think the CMake problems stem from some confusion about the idiomiatic way to include custom op libraries in `tf.contrib`. (This doesn't appear to be well documented, so it's not your fault!)\r\n\r\nTo outline the steps:\r\n1. Since b2ac11c0797447eb36d427a0907fb2f05af57fcc, we've been statically generating the Python wrappers for all contrib op kernels (on all platforms and build systems). Have a look at that commit to see how to do that; it's quite possible that you're already generating them with the existing Bazel rules. The important thing to note is that the imported symbol(s) for the new ops should come from the `gen_*_ops` module rather than the result of `load_op_library()`.\r\n\r\n2. To make the build easier on Windows, **delete** the `AddUserOps` from `tf_python.cmake`. Including the source files in the list of `tf_contrib_kernels_srcs` in `tf_core_kernels.cmake` should be enough to get the code statically linked into the Python extension (which is a somewhat easier route to take on Windows).\r\n\r\n3. Keep the `GENERATE_PYTHON_OP_LIB()` and `add_python_module()` invocations in `tf_python.cmake`. Those look right to me.", "@gunan @mrry thanks for looking.", "Sorry for the delay, but I implemented the changes @mrry suggested and it seems to have worked. I think I'll still have an issue with the CLA as I made a commit on a temporary machine without changing my email address on that machine. I don't know of a way to resolve this issue.", "@jhetherly it sound to me like we can merge this, assuming the tests pass. @vrv do you agree that the statement in this thread about the the reason for the failing CLA check is sufficient? \r\n@tensorflow-jenkins test this please", "Yeah, it seems fine.  If you want to be on the safe side, once this is perfectly working, you could squash the entire PR into a single commit and the CLA check would pass.", "```\r\n11:14:48 CMake Error at tf_python.cmake:677 (add_executable):\r\n11:14:48   Error evaluating generator expression:\r\n11:14:48 \r\n11:14:48     $<TARGET_OBJECTS:tf_contrib_periodic_resample_op>\r\n11:14:48 \r\n11:14:48   Objects of target \"tf_contrib_periodic_resample_op\" referenced but no such\r\n11:14:48   target exists.\r\n11:14:48 Call Stack (most recent call first):\r\n11:14:48   tf_python.cmake:767 (GENERATE_PYTHON_OP_LIB)\r\n11:14:48   CMakeLists.txt:304 (include)\r\n```", "Can you amend the PR as @mrry suggests?", "@drpngx: I thought I did, but I suppose I forgot to include a necessary directory. I just uploaded a commit that may fix this.", "Jenkins, test this please.\n\nOn Sep 13, 2017 8:30 PM, \"Jeff\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx>: I thought I did, but I suppose I\n> forgot to include a necessary directory. I just uploaded a commit that may\n> fix this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-329359413>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sba91tf3XjUOmhCSoLvJCMDnq8bsUks5siJ3LgaJpZM4NE8mk>\n> .\n>\n", "Jenkins, test this please.", "This is still missing,\r\n\r\n```\r\n17:47:33 CMake Error at tf_python.cmake:687 (add_executable):\r\n17:47:33   Error evaluating generator expression:\r\n17:47:33 \r\n17:47:33     $<TARGET_OBJECTS:tf_contrib_periodic_resample_ops>\r\n17:47:33 \r\n17:47:33   Objects of target \"tf_contrib_periodic_resample_ops\" referenced but no such\r\n17:47:33   target exists.\r\n```\r\nwhich might be causing:\r\n```\r\n7:55:03   'Release\\contrib_periodic_resample_ops_gen_python.exe' is not recognized as an internal or external command,\r\n17:55:03   operable program or batch file.\r\n17:55:03 C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 9009. [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_python_ops.vcxproj]\r\n```", "Jenkins, test this please.", "@mrry if you have some cycles, could you take a look? Thanks.", "This looks like the error:\r\n\r\n```\r\n17:33:33   Error copying file \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/Release/_periodic_resample_op.dll\" to \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tf_python/tensorflow/contrib/periodic_resample/python/ops//\".\r\n17:33:33 C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: The command \"setlocal [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\_periodic_resample_op.vcxproj]\r\n17:33:33 C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: \"C:\\Program Files\\cmake\\bin\\cmake.exe\" -E copy C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/Release/_periodic_resample_op.dll C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tf_python/tensorflow/contrib/periodic_resample/python/ops// [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\_periodic_resample_op.vcxproj]\r\n```\r\n\r\nThe path looks broken, although I'm not sure how CMake ended up with it. I'll note that the paths named in [the `add_python_module()` calls](https://github.com/tensorflow/tensorflow/pull/9376/files#diff-10f12827f7259a4fd7dba7abbd8307b0R526) (`tensorflow/contrib/periodic_resample` and `tensorflow/contrib/periodic_resample/python`) don't match the apparent destination for the generated module in [`AddUserOps()`](https://github.com/tensorflow/tensorflow/pull/9376/files#diff-10f12827f7259a4fd7dba7abbd8307b0R972) (`tensorflow/contrib/periodic_resample/python/ops`) or the [generated bindings](https://github.com/tensorflow/tensorflow/pull/9376/files#diff-10f12827f7259a4fd7dba7abbd8307b0R768)(`tensorflow/contrib/periodic_resample/ops/gen_periodic_resample_ops.py`).\r\n\r\nIt's imperative that you call `add_python_module()` for all subdirectories containing a Python file, and it looks like there's some confusion between similarly named paths like `tensorflow/contrib/periodic_resample/ops` and `tensorflow/contrib/periodic_resample/python/ops`.", "Thank you Derek!\n\nJeff, can we rename one of them to see if that fixes it?\n\n\nOn Sep 19, 2017 10:51 PM, \"Derek Murray\" <notifications@github.com> wrote:\n\nThis looks like the error:\n\n17:33:33   Error copying file\n\"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/Release/_periodic_resample_op.dll\"\nto \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tf_python/tensorflow/contrib/periodic_resample/python/ops//\".\n17:33:33 C:\\Program Files\n(x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5):\nerror MSB3073: The command \"setlocal\n[C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\_periodic_resample_op.vcxproj]\n17:33:33 C:\\Program Files\n(x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5):\nerror MSB3073: \"C:\\Program Files\\cmake\\bin\\cmake.exe\" -E copy\nC:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/Release/_periodic_resample_op.dll\nC:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/tf_python/tensorflow/contrib/periodic_resample/python/ops//\n[C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\_periodic_resample_op.vcxproj]\n\nThe path looks broken, although I'm not sure how CMake ended up with it.\nI'll note that the paths named in the add_python_module() calls\n<https://github.com/tensorflow/tensorflow/pull/9376/files#diff-10f12827f7259a4fd7dba7abbd8307b0R526>\n(tensorflow/contrib/periodic_resample and tensorflow/contrib/periodic_\nresample/python) don't match the apparent destination for the generated\nmodule in AddUserOps()\n<https://github.com/tensorflow/tensorflow/pull/9376/files#diff-10f12827f7259a4fd7dba7abbd8307b0R972>\n(tensorflow/contrib/periodic_resample/python/ops) or the generated bindings\n<https://github.com/tensorflow/tensorflow/pull/9376/files#diff-10f12827f7259a4fd7dba7abbd8307b0R768>\n(tensorflow/contrib/periodic_resample/ops/gen_periodic_resample_ops.py).\n\nIt's imperative that you call add_python_module() for all subdirectories\ncontaining a Python file, and it looks like there's some confusion between\nsimilarly named paths like tensorflow/contrib/periodic_resample/ops and\ntensorflow/contrib/periodic_resample/python/ops.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-330751405>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbcryEOJ2-yU2wwHhALl2XhDDfBMaks5skKf6gaJpZM4NE8mk>\n.\n", "@mrry Thanks for digging out the root cause!", "Ah, I was unaware that within the CMAKE build I needed to include all directories that contain Python files. I fixed this as well as removed the `GENERATE_PYTHON_OP_LIB` command as I don't think I need this (and I don't include a `gen_periodic_resample_op` in any of my python files anyway).", "Jenkins, test this please.\n\nOn Oct 2, 2017 10:50 PM, \"Jeff\" <notifications@github.com> wrote:\n\n> Ah, I was unaware that within the CMAKE build I needed to include all\n> directories that contain Python files. I fixed this as well as removed the\n> GENERATE_PYTHON_OP_LIB command as I don't think I need this (and I don't\n> include a gen_periodic_resample_op in any of my python files anyway).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-333745772>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbWPq6eaDUMpMhqNSjYaHBAojezgMks5socsngaJpZM4NE8mk>\n> .\n>\n", "This seems to be an issue with how I\u2019m importing my module. I\u2019ll debug this later today. ", "Just pushed a potential fix for the import statement.", "@tensorflow-jenkins test this please", "The failures seem like they aren't related to my contribution. Could someone rerun the tests?", "Jenkins, test this please.\n\nOn Wed, Oct 4, 2017 at 1:57 PM, Jeff <notifications@github.com> wrote:\n\n> The failures seem like they aren't related to my contribution. Could\n> someone rerun the tests?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-334285805>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbW6Q3v9d9XZuCo3uQs49XsN7r1M5ks5so_EagaJpZM4NE8mk>\n> .\n>\n", "I'm a bit confused now. I can compile the module, but `tensorflow/contrib/__init__.py` fails to import the module during the CMAKE build. I don't know why this is whenever this works for the `bazel` build and would be open to suggestions.", "You have to add a tf_module to the tf_python.cmake file, otherwise it won't be copied to the right place. ", "thanks for the tip, @martinwicke. I can't seem to locate another `tf_module` command in any of the CMAKE files. Could you point to an example?", "Sorry, got the names all wrong, you want to add your module here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake\r\n\r\nAdd another add_python_module into the alphabetically sorted list.", "Hey @martinwicke,\r\nI think I already have that covered here  https://github.com/jhetherly/tensorflow/blob/bfa29c3a63e746e7f622d22ee0ef293c88c2bc13/tensorflow/contrib/cmake/tf_python.cmake#L538\r\nHowever, if you are referring to something else just let me know. Any other suggestions?", "@mrry do you have an idea?", "Jenkins, test this please.", "I'm not sure what the cause of these errors are. In the log of the Window's CMake build I can see that 'periodic_resample' seems to build without error.", "Jenkins, test this please.\r\nJust in case, let me rerun the tests.", "The error for the Windows' CMake build is still quite obtuse and seems unrelated to my contribution. The only other issue I see is the CLA failure which is present simply because I had to use a temporary computer to commit changes for a brief period in time. Is there any possibility of overriding the CLA issue?", "We can only accept commits with email addresses which have a CLA registered. If all emails have a CLA, then we would be ok, but the failure would look different. The simplest solution is to squash your commits into one with the proper email. Or you can amend the offending commits with the proper email.\r\n\r\nNow, regarding the test failure, it does look like there is still a problem with the cmake setup:\r\n\r\n```\r\n23:09:07 CMake Error at tf_core_kernels.cmake:208 (add_library):\r\n23:09:07   Cannot find source file:\r\n23:09:07 \r\n23:09:07     C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/contrib/periodic_resample/core/kernels/periodic_resample_op.cc\r\n23:09:07 \r\n23:09:07   Tried extensions .c .C .c++ .cc .cpp .cxx .m .M .mm .h .hh .h++ .hm .hpp\r\n23:09:07   .hxx .in .txx\r\n```\r\n\r\n@mrry, can you help with this?", "@tensorflow-jenkins  test this please.", "@mrry: I seem to did not commit all the changes I made regarding the 'core' directory stuff. It should be resolved now.", "Weird... I thought I saw them!\r\n\r\n@tensorflow-jenkins test this please.", "After doing some quick searches, I think I may have resolved the (new) errors in the CMake build.", "@tensorflow-jenkins test this please.", "I will trigger the tests, but as @wicke mentioned, we still need the CLA updated. It can be through creting another PR from your own computer, or amending commits and force pushing to the current branch we are trying to merge.", "@gunan I'll work on squashing the previous commits.\r\nI now see a new error in the CMake build related to `InvalidArgumentError (see above for traceback): Multiple OpKernel registrations match NodeDef`. I searched and found this comment on a previous issue: https://github.com/tensorflow/tensorflow/issues/11277#issuecomment-312985983. I'm not sure how to proceed from this point as the links in that comment no longer point to the relevant lines. I'll try a few things, but I'm more or less fumbling in the dark.", "@gunan Forgive my ignorance, but what exactly do you mean by \"creating another PR\"? Are you suggesting that I create another branch and submit a PR from that new branch (seems like the easiest option from my end)? I've not had much luck with squashing the previous commits, so something like opening another PR sounds more attractive to me at the moment (although, I do see that as being more of a headache on your end). Also, would you mind rerunning the tests (I may have fixed the current issue)?", "@jhetherly yes, that's what I meant. You can create a new commit in the repository you are normally using (and the github username which has the CLA taken care of) with an email address which has the CLA resolved, and you may create a new PR from that branch. In terms of headache, it won't be too bad at all for us.\r\n\r\nJenkins, test this please.", "@gunan I just created #14339 that has a clean history  that is ready for testing", "Thanks! Will close this.", "I cannot import\r\n `from tensorflow.contrib import periodic_resample`\r\n with CPU version of tensorflow==1.4.1 installed with pip.\r\n\r\n", "Hello, not sure if this is correct place for bug reports... Tried to better understand this operation, found few issues:\r\n1) Mathematic expressions in documentation look weird https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/periodic_resample/periodic_resample  , seems, something wrong in markup\r\n2) Resampling operation produce strange results, showing some garbage when adjustable dimension is not the last dimension, even in case when resulting tensor shapes are same. Not sure if this is expected behavior, just for your information. Here is test script to reproduce:\r\nhttps://pastebin.com/W14xwpb5\r\nand it output\r\nhttps://pastebin.com/uE5UiAvW\r\n", "Thanks! Are you in a position to send a PR it two to fix?\n\nOn Sat, Dec 23, 2017, 5:12 AM vchigrin <notifications@github.com> wrote:\n\n> Hello, not sure if this is correct place for bug reports... Tried to\n> better understand this operation, found few issues:\n>\n>    1. Mathematic expressions in documentation look weird\n>    https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/periodic_resample/periodic_resample\n>    , seems, something wrong in markup\n>    2. Resampling operation produce strange results, showing some garbage\n>    when adjustable dimension is not the last dimension, even in case when\n>    resulting tensor shapes are same. Not sure if this is expected behavior,\n>    just for your information. Here is test script to reproduce:\n>    https://pastebin.com/W14xwpb5\n>    and it output\n>    https://pastebin.com/uE5UiAvW\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9376#issuecomment-353725432>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbdbr1TX3ARkxTyYFcfcSkympIPLRks5tDPw9gaJpZM4NE8mk>\n> .\n>\n", "> Thanks! Are you in a position to send a PR it two to fix?\r\n\r\nJust made\r\nhttps://github.com/tensorflow/tensorflow/pull/15617\r\nAbout 2 item - that my mistake, did not read documentation precisely enough. It says \"The specified sizes of the non-adjustable dimensions must by at least as large as in the values tensor\", so my test example is not correct since it violates this requirement. I've added check in my PR for that case, since getting python exception is better then get unpredictable results in runtime."]}, {"number": 9375, "title": "Bug in the situation of multiple ps servers, NotFoundError", "body": "I have 2 ps servers and 3 worker servers (Ubuntu14.04, Python 3.5.3 and Tensorflow 1.0.1 with virtualenv). I am trying the demo based on https://www.tensorflow.org/deploy/distributed.\r\n\r\nI got NotFoundError errors when I use 2 ps servers. (It can run well when I use 1 ps server. And It can run well with 2 ps server, when I use \"save_checkpoint_secs=None\" in MonitoredTrainingSession().)\r\n\r\n-----------------------------------------------------------------------------------------\r\nTHE CODE:\r\n..........\r\n      # Build model...\r\n      x_data = np.random.rand(100).astype(np.float32)\r\n      y_data = x_data*0.1 + 0.3\r\n      Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\r\n      biases = tf.Variable(tf.zeros([1]))\r\n      y = Weights*x_data + biases\r\n      loss = tf.reduce_mean(tf.square(y-y_data))\r\n      global_step = tf.contrib.framework.get_or_create_global_step()\r\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\r\n          loss, global_step=global_step)\r\n    # The StopAtStepHook handles stopping after running given steps.\r\n    hooks=[tf.train.StopAtStepHook(last_step=50000)]\r\n\r\n    # The MonitoredTrainingSession takes care of session initialization,\r\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\r\n    # or an error occurs.\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(FLAGS.task_index == 0),\r\n                                           checkpoint_dir=\"./train_logs\",\r\n                                           hooks=hooks) as mon_sess:\r\n      while not mon_sess.should_stop():\r\n..........\r\n\r\n-----------------------------------------------------------------------------------------------\r\nTHE ERROR:\r\nError in worker server 0 (master): \r\n........\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server                                                                                                              with target: grpc://localhost:22222\r\nI tensorflow/core/distributed_runtime/master_session.cc:1012] Start master sessi                                                                                                             on f8c75e38e6e9419a with config:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.                                                                                                             py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.                                                                                                             py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/error                                                                                                             s_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./train_logs/model.ckpt-0                                                                                                             _temp_0205a1571bdb4f9db000d0caf3139d5a/part-00000-of-00002.index\r\n         [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=tr                                                                                                             ue, _device=\"/job:ps/replica:0/task:1/cpu:0\"](save/MergeV2Checkpoints/checkpoint                                                                                                             _prefixes, _recv_save/Const_0_S63)]]\r\n         [[Node: save/Identity_S65 = _Recv[client_terminated=false, recv_device=                                                                                                             \"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:1/cpu:                                                                                                             0\", send_device_incarnation=7747326126893392586, tensor_name=\"edge_33_save/Ident                                                                                                             ity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"0.1.py\", line 103, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py    \r\n\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"0.1.py\", line 68, in main\r\n    _, step, loss_v, weight, biase = mon_sess.run([train_op, global_step, loss,                                                                                                              Weights, biases])\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 462, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 786, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 744, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 899, in run\r\n    run_metadata=run_metadata))\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/basic_                                                                                                             session_run_hooks.py\", line 355, in after_run\r\n    self._save(global_step, run_context.session)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/basic_                                                                                                             session_run_hooks.py\", line 371, in _save\r\n    self._get_saver().save(session, self._save_path, global_step=step)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.                                                                                                             py\", line 1363, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.                                                                                                             py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.                                                                                                             py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.                                                                                                             py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.                                                                                                             py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./train_logs/model.ckpt-0                                                                                                             _temp_0205a1571bdb4f9db000d0caf3139d5a/part-00000-of-00002.index\r\n         [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=tr                                                                                                             ue, _device=\"/job:ps/replica:0/task:1/cpu:0\"](save/MergeV2Checkpoints/checkpoint                                                                                                             _prefixes, _recv_save/Const_0_S63)]]\r\n         [[Node: save/Identity_S65 = _Recv[client_terminated=false, recv_device=                                                                                                             \"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:1/cpu:                                                                                                             0\", send_device_incarnation=7747326126893392586, tensor_name=\"edge_33_save/Ident                                                                                                             ity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'save/MergeV2Checkpoints', defined at:\r\n  File \"0.1.py\", line 103, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py                                                                                                             \", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"0.1.py\", line 62, in main\r\n    hooks=hooks) as mon_sess:\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 315, in MonitoredTrainingSession\r\n    return MonitoredSession(session_creator=session_creator, hooks=all_hooks)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 601, in __init__\r\n    session_creator, hooks, should_recover=True)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 434, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 767, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 772, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 494, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             \r\nred_session.py\", line 366, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 180, in finalize\r\n    lambda: training_saver.Saver(sharded=True, allow_empty=True,\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 232, in get_or_default\r\n    op = default_constructor()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/monito                                                                                                             red_session.py\", line 181, in <lambda>\r\n    write_version=saver_pb2.SaverDef.V2))\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.                                                                                                             py\", line 1040, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.                                                                                                             py\", line 1070, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.                                                                                                             py\", line 669, in build\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.                                                                                                             py\", line 356, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.                                                                                                             py\", line 338, in _AddShardedSaveOpsForV2\r\n    sharded_prefixes, checkpoint_prefix, delete_old_dirs=True)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.                                                                                                             py\", line 185, in merge_v2_checkpoints\r\n    delete_old_dirs=delete_old_dirs, name=name)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_de                                                                                                             f_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.p                                                                                                             y\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.p                                                                                                             y\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): ./train_logs/model.ckpt-0_temp_0205a157                                                                                                             1bdb4f9db000d0caf3139d5a/part-00000-of-00002.index\r\n         [[Node: save/MergeV2Checkpoints = MergeV2Checkpoints[delete_old_dirs=tr                                                                                                             ue, _device=\"/job:ps/replica:0/task:1/cpu:0\"](save/MergeV2Checkpoints/checkpoint                                                                                                             _prefixes, _recv_save/Const_0_S63)]]\r\n         [[Node: save/Identity_S65 = _Recv[client_terminated=false, recv_device=                                                                                                             \"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:1/cpu:                                                                                                             0\", send_device_incarnation=7747326126893392586, tensor_name=\"edge_33_save/Ident                                                                                                             ity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nError in ps server 1:\r\n........\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server                                                                                                              with target: grpc://localhost:22222\r\nW tensorflow/core/framework/op_kernel.cc:993] Not found: ./train_logs/model.ckpt                                                                                                             -0_temp_0205a1571bdb4f9db000d0caf3139d5a/part-00000-of-00002.index\r\n\r\n", "comments": ["OK, I use nfs fix this problem. As below, node0 is my worker server 0 (master).\r\n \r\n   with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                           is_chief=(FLAGS.task_index == 0),\r\n                                           checkpoint_dir=\"/home/node0_nfs_share\",\r\n                                           hooks=hooks) as mon_sess:\r\n\r\nBut I also think this is a little bug should be fixed.\r\n", "This is not really a bug. You are expected to have a file that exists."]}, {"number": 9374, "title": "`tensorflow.python.client.device_lib.list_local_devices()` Bug", "body": "I am trying to set up GPU configuration for Tensorflow. The step is very simple - Call `tensorflow.python.client.device_lib.list_local_devices()` to detect the number of gpu devices on the machine, and then set `config` for Tensorflow.  The following is the code for reproducing:\r\n\r\n```\r\nfrom logging import getLogger\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n\r\n\r\nlog = getLogger(__name__)\r\n\r\n\r\ndef get_available_gpus():\r\n    \"\"\" Get available GPU devices info. \"\"\"\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\n\r\n\r\ndef test_gpu_memory_usage():\r\n    # Detect available GPU devices info.\r\n    log.info(\"On this machine, GPU devices: \", get_available_gpus())\r\n\r\n    # Set Tensorflow GPU configuration.\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\r\n    tf_config=tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        device_count={'GPU': len(get_available_gpus())},\r\n        gpu_options=gpu_options,\r\n        log_device_placement=True)\r\n    session = tf.Session(config=tf_config)\r\n\r\n    # Mimick training process.\r\n    while True:\r\n        pass\r\n        \r\n\r\ntest_gpu_memory_usage()\r\n```\r\nIf you run the above code, you could notice that even though you set GPU memory fraction per process to 0.1, it still allocates the whole GPU memory by looking at command `nvidia-smi`. However, if you don't call `get_available_gpus()`, the memory allocation works fine. That means, there might be a bug in `device_lib.list_local_devices()` to prevent setting up Tensorflow GPU memory usage.\r\n\r\nPS. My code runs on machine with GPU `GeForce GTX 1080`, CUDA 8.0, OS Ubuntu 16.04 and Python 3.5, and the above issue could be reproduced using either Tensorflow v.0.12, v.1.0 or v.1.1.", "comments": ["Interesting. Thanks for reporting! If memory serves, getting the list does have side effects (registering the devices).\r\n\r\n@mrry might know on the top of his head. I'll take a look later.", "Yes, I believe calling `device_lib.list_local_devices()` [invokes the (morally static) `DeviceFactory::AddDevices()` code](https://github.com/tensorflow/tensorflow/blob/48d9915ebca770b40c4497a13c3edb87b6b042d0/tensorflow/python/client/device_lib.i#L32) for the GPU devices, but doesn't take a `ConfigProto`  and so passes [an empty `SessionOptions`](https://github.com/tensorflow/tensorflow/blob/48d9915ebca770b40c4497a13c3edb87b6b042d0/tensorflow/python/client/device_lib.i#L30) to the factory.\r\n\r\nThere's no good reason for that... IIRC we added `device_lib.py` in order to be able to print diagnostic information about the available devices when running benchmarks, and none of the benchmarks depended on configuring the device initialization. I don't know if there's a technical limitation in the GPU device code that prevents multiple instantiations, or separating the enumeration from the instantiation. #8136 contains a feature request for rationalizing the runtime initialization code, so it might be worth chiming in there.", "Thanks @mrry! Closing in favor of #8136. I believe that #8021 has the temporary workaround you need."]}, {"number": 9373, "title": "Add Feature: Sparse matrix multiplications for Tensors with rank > 2", "body": "This is according to Issue #9210 \r\n\r\nThe cpu implementation now support rank 3, 4, 5.\r\n\r\nI have a trouble in the gpu implementation. On current line 113 (old 102):\r\n`To32Bit(out).device(d) = To32Bit(out).constant(T(0));`\r\nnvcc can compile it without error. But at runtime, it produces\r\n`Assertion failed: (cudaGetLastError() == cudaSuccess), function run, file /Library/Python/2.7/site-packages/tensorflow/include/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h, line 262.\r\nAbort trap: 6`\r\nWhen I try to change it to\r\n`To32Bit(out).setZero();`\r\nIt gives\r\n`Bus error: 10`\r\nI got the same failure on r1.1 source files.\r\n\r\nAny help will be appreciated.", "comments": ["Can one of the admins verify this patch?", "@zycdragonball , please add some unit tests", "@caisq I've put on several tests. Please let me know if need more.", "@zycdragonball there are conflicts due to recent commits. Can you resolve them please? Thanks.", "@caisq Done!", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "This looks out of date, the GPU kernel no longer uses an eigen generator.\n\nOn May 1, 2017 4:29 PM, \"Rasmus Munk Larsen\" <notifications@github.com>\nwrote:\n\n> @tensorflow-jenkins <https://github.com/tensorflow-jenkins> test this\n> please\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9373#issuecomment-298459546>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4sOfq6cu5Ij1lSjxHy0lrE-zLXDks5r1mr2gaJpZM4NEogR>\n> .\n>\n", "@ebrevdo I was making minimal changes, so didn't touch the generator. It can be easily replaced by two for-loops as in the cpu implementation. Do you want me to change it?", "ping for @ebrevdo \r\n\r\n(also some merge conflicts, sadly :(", "Yeah see if you can fix the merge conflicts and restore your intended\nbehavior.\n\nOn May 4, 2017 10:36 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> ping for @ebrevdo <https://github.com/ebrevdo>\n>\n> (also some merge conflicts, sadly :(\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9373#issuecomment-299382875>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0liHY7II83_fcy8mIyFoYMUGB4fks5r2rVpgaJpZM4NEogR>\n> .\n>\n", "@b11094 if you are going to leave a review, please properly the github review tools -- I'm going to delete your comments since they're making it hard to read the thread.  Thanks!", "I've solved the conflicts. Since the gpu implementation has been completely rewritten and I don't have an environment to test the gpu code recently, I took the master branch and only changed the interface. So the cpu supports to dimension 5, gpu is still only dimension 2. I hope I can write the gpu version soon.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "@zycdragonball are you planning update the GPU implementation to handle up to rank 5 in this PR?", "@rmlarsen I will update the GPU implementation by 5/24. I still don't have an environment to test my codes this week.", "Marking this stalled to notify the TF oncall that this will be picked back up after 5/24.", "@rmlarsen I've added the gpu implementation.", "@zycdragonball any update on this?", "@rmlarsen See if it looks good now.", "@tensorflow-jenkins test this please", "@zycdragonball It looks like the unit tests for shape inference are failing, can you take a look please?\r\n\r\n[ RUN      ] SparseOpsTest.SparseTensorDenseMatMul_ShapeFn\r\ntensorflow/core/ops/sparse_ops_test.cc:187: Failure\r\nValue of: StringPiece(error_message).contains(substring)\r\n  Actual: false\r\nExpected: true\r\nExpected to see 'must be equal' in 'Invalid input spec (] not found in dim shape): [?'\r\ntensorflow/core/ops/sparse_ops_test.cc:194: Failure\r\n      Expected: \"\"\r\nTo be equal to: ::tensorflow::shape_inference::ShapeInferenceTestutil::InferShapes( op, \"?;?;?;[?,?,?,?]\", \"[?,d3_3]\") .error_message()\r\n      Which is: \"Output 0 expected rank 2 but was 4. Output shape was [?,?,?,?]\"\r\ntensorflow/core/ops/sparse_ops_test.cc:199: Failure\r\n      Expected: \"\"\r\nTo be equal to: ::tensorflow::shape_inference::ShapeInferenceTestutil::InferShapes( op, \"?;?;?;[?,?,?,?]\", \"[?,d3_2]\") .error_message()\r\n      Which is: \"Output 0 expected rank 2 but was 4. Output shape was [?,?,?,?]\"\r\ntensorflow/core/ops/sparse_ops_test.cc:219: Failure\r\n      Expected: \"\"\r\nTo be equal to: ::tensorflow::shape_inference::ShapeInferenceTestutil::InferShapes( op, \"?;?;[3];[3,2,4]\", \"[3,1,d3_2]\") .error_message()\r\n      Which is: \"Dimensions must be equal, but are 1 and 2 for 'test' (op: 'SparseTensorDenseMatMul') with input shapes: ?, ?, [3], [3,2,4] and with computed input tensors: input[2] = <3 1 2>.\"\r\n[  FAILED  ] SparseOpsTest.SparseTensorDenseMatMul_ShapeFn (0 ms)\r\n", "@rmlarsen It should be good now.", "@zycdragonball Thanks! \r\n@tensorflow-jenkins test this please", "@zycdragonball you still have a bug in your shape inference test - you are always passing the output dims through, not creating a new unknown dimension. I think your actual shape inference code is correct.", "@rmlarsen I'm not sure about the c++ shape inference test, although I have tested the shape inference code through python. Is there a quicker way to run the c++ tests on a local machine? I tried `bazel test`, but it cost 2 hrs every single time.", "@zycdragonball you can run \"bazel test tensorflow/core:ops_sparse_ops_test\"", "@zycdragonball any progress on fixing the test?", "@rmlarsen Thanks for the command line! It runs much faster now. The shape inference test should work now.", "@ebrevdo -- does this look good to you?", "Jenkins, test this please", "@zycdragonball mind checking the test failures?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "What is the status here? This would be really nice to get integrated.", "@rmlarsen  I can't reproduce the failure locally.", "I'm back from vacation. Let's try to get this in.\r\n\r\n@tensorflow-jenkins test this please", "It looks like there is a bug in the GPU code since you get an illegal address cuda error. If you cannot spot the problem, I'll try to look at it tomorrow.\r\n\r\n2017-08-08 23:47:13.194194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:967] Device peer to peer matrix\r\n2017-08-08 23:47:13.194200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:973] DMA: 0 \r\n2017-08-08 23:47:13.194203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   Y \r\n2017-08-08 23:47:13.194211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1042] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:07.0)\r\n.2017-08-08 23:47:15.108739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1042] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:07.0)\r\n.2017-08-08 23:47:29.211769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1042] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:07.0)\r\n**2017-08-08 23:47:29.364981: E tensorflow/stream_executor/cuda/cuda_driver.cc:1098] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n2017-08-08 23:47:29.365012: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS**\r\n2017-08-08 23:47:29.365040: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n2017-08-08 23:47:29.365059: F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\n/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/sparse_tensor_dense_matmul_op_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 40: 21114 Aborted                 (core dumped) $@", "@zycdragonball didn't get to this yet. Will try later this week.", "@rmlarsen @zycdragonball any update on the GPU test failure?", "@zycdragonball @jhseu sorry, didn't have time to look at ti yet.", "@rmlarsen any cycles to take a look?", "@drpngx I will take a look.", "@zycdragonball can you look the test failure whenever you have a chance", "When will this be included into the stable build?", "I'm starting to test this now. Will hopefully found the issue today.", "@zycdragonball sorry for the long delay. I think the best way forward is to refactor this code according to ebrevdo@'s recommendation.  ", "Seems need quite a bit change. But I don't have time to work on this anymore. Should I close it?", "@zycdragonball too bad :-( I think we can close it, but it is a good template for somebody wanting to continue this work. ", "Thanks for solving this problem. This feature helps a lot in my work. But it seems the binary files are unaccessable any longer by the links on the main webpage https://github.com/zycdragonball/tensorflow. I appreciate and expect that they work again. @zycdragonball"]}, {"number": 9372, "title": "TensorBoard does not load in Internet Explorer 11", "body": "### System Information\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: Server is running on Ubuntu 14.04\r\n- *TensorFlow installed from (source or binary)?*: Docker image\r\n- *TensorFlow version* (use command below): 1.0.1-gpu\r\n- *CUDA/cuDNN version*: CUDA 8 / cuDNN 6\r\n- *GPU Model and Memory*: K520 (AWS g2.2xlarge)\r\n- *Exact command to reproduce*: Launch TensorBoard and load in IE11\r\n\r\n### Describe the problem clearly\r\nGet a blank page when loading TensorBoard in IE11.  Works fine in Firefox/Chrome/Safari.\r\nGet three errors from the IE console:\r\n```\r\nSCRIPT1002: Syntax error\r\nFile: javascript;charset=utf-8,%0A%20%20Polymer(%7B%0A%20%20%20%20is%3A%20%22tf-run-selector%22%2C%0A%20%20%20%20properties%3A%20%7B%0A%20%20%20%20%20%20backend%3A%20Object%2C%0A%20%20%20%20%20%20outSelected%3A%20%7Btype%3A%20Array%2C%20notify%3A%20true%7D%2C%0A%20%20%20%20%20%20%2F%2F%20runs%3A%20an%20array%20of%20strings%2C%20representing%20the%20run%20names%20that%20may%20be%20chosen%0A%20%20%20%20%20%20runs%3A%20Array%2C%0A%20%20%20%20%20%20colorScale%3A%20Object%2C%20%2F%2F%20TF.ColorScale%0A%20%20%20%20%20%20logdir%3A%20%7B%0A%20%20%20%20%20%20%20%20type%3A%20String%2C%0A%20%20%20%20%20%20%20%20notify%3A%20true%2C%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%7D%2C%0A%20%20%20%20ready%3A%20function()%20%7B%0A%20%20%20%20%20%20%2F%2F%20Populate%20the%20logdir.%0A%20%20%20%20%20%20this.backend.logdir().then(logdirObject%20%3D%3E%20%7B%0A%20%20%20%20%20%20%20%20this.set(&apos;logdir&apos;%2C%20logdirObject.logdir)%3B%0A%20%20%20%20%20%20%7D).catch(e%20%3D%3E%20%7B%0A%20%20%20%20%20%20%20%20%2F%2F%20Fetching%20the%20logdir%20failed.%20Prevent%20the%20exception%20from%20logging%20to%0A%20%20%20%20%20%20%20%20%2F%2F%20console.%20The%20console%20already%20logs%20a%20404%20network%20event.%0A%20%20%20%20%20%20%7D)%3B%0A%20%20%20%20%7D%2C%0A%20%20%20%20_toggleAll%3A%20function()%20%7B%0A%20%20%20%20%20%20this.%24.multiCheckbox.toggleAll()%3B%0A%20%20%20%20%7D%2C%0A%20%20%20%20%2F%2F%20Break%20the%20string%20at%20natural%20points%2C%20including%20commas%2C%20equals%2C%20and%20slashes%0A%20%20%20%20_breakString%3A%20function(originalString)%20%7B%0A%20%20%20%20%20%20return%20originalString.replace(%2F(%5B%5C%2F%3D-_%2C%5D)%2Fg%2C%20%22%241%3Cwbr%3E%22)%0A%20%20%20%20%7D%2C%0A%20%20%7D)%3B%0A%20%20%0A%2F%2F%23%20sourceURL%3Dhttp%3A%2F%2Fec2-54-221-2-83.compute-1.amazonaws.com%3A6006%2Fdist%2Ftf-tensorboard.html-24.js%0A, Line: 17, Column: 48\r\n```\r\n\r\n```\r\nSCRIPT1003: Expected ':'\r\nFile: javascript;charset=utf-8,%0A(function()%20%7B%0APolymer(%7B%0A%20%20is%3A%20&apos;vz-projector-dashboard&apos;%2C%0A%20%20properties%3A%20%7B%0A%20%20%20%20dataNotFound%3A%20Boolean%2C%0A%20%20%20%20routePrefix%3A%20String%0A%20%20%7D%2C%0A%20%20ready()%20%7B%0A%20%20%20%20var%20self%20%3D%20this%3B%0A%20%20%20%20d3.json(this.routePrefix%20%2B%20&apos;%2Fruns&apos;%2C%20function(err%2C%20runs)%20%7B%0A%20%20%20%20%20%20self.dataNotFound%20%3D%20(runs.length%20%3D%3D%3D%200)%3B%0A%20%20%20%20%7D)%3B%0A%20%20%7D%0A%7D)%3B%0A%7D)()%3B%0A%0A%2F%2F%23%20sourceURL%3Dhttp%3A%2F%2Fec2-54-221-2-83.compute-1.amazonaws.com%3A6006%2Fdist%2Ftf-tensorboard.html-70.js%0A, Line: 9, Column: 8\r\n```\r\n\r\n```\r\nSCRIPT1014: Invalid character\r\nFile: javascript;charset=utf-8,%0A%20%20%20%20Polymer(%7B%0A%20%20%20%20%20%20is%3A%20%22tf-tensorboard%22%2C%0A%20%20%20%20%20%20behaviors%3A%20%5BTF.TensorBoard.AutoReloadBehavior%5D%2C%0A%20%20%20%20%20%20properties%3A%20%7B%0A%20%20%20%20%20%20%20%20router%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20type%3A%20Object%2C%0A%20%20%20%20%20%20%20%20%20%20value%3A%20function()%20%7B%0A%20%20%20%20%20%20%20%20%20%20%20%20return%20TF.Backend.router()%3B%0A%20%20%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20_backend%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20type%3A%20Object%2C%0A%20%20%20%20%20%20%20%20%20%20computed%3A%20%22_makeBackend(router%2C%20demoDir)%22%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%2F%2F%20Which%20tab%20is%20selected%20(scalars%2C%20graph%2C%20images%20etc).%0A%20%20%20%20%20%20%20%20mode%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20type%3A%20String%2C%0A%20%20%20%20%20%20%20%20%20%20computed%3A%20&apos;_getModeFromIndex(modeIndex)&apos;%2C%0A%20%20%20%20%20%20%20%20%20%20notify%3A%20true%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20tabs%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20type%3A%20Array%2C%0A%20%20%20%20%20%20%20%20%20%20readOnly%3A%20true%2C%0A%20%20%20%20%20%20%20%20%20%20value%3A%20TF.Globals.TABS%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%2F%2F%20If%20this%20is%20set%20to%20a%20string%2C%20TensorBoard%20will%20switch%20to%20%22demo%20mode%22%0A%20%20%20%20%20%20%20%20%2F%2F%20and%20attempt%20to%20load%20serialized%20json%20data%20from%20that%20directory.%20You%20can%0A%20%20%20%20%20%20%20%20%2F%2F%20generate%20conformant%20json%20using%0A%20%20%20%20%20%20%20%20%2F%2F%20tensorboard%2Fscripts%2Fserialize_tensorboard.py%0A%20%20%20%20%20%20%20%20demoDir%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20type%3A%20String%2C%0A%20%20%20%20%20%20%20%20%20%20value%3A%20null%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%20%20%2F%2F%20Set%20this%20to%20true%20to%20store%20state%20in%20URI%20hash.%20Should%20be%20true%20for%20all%20non-test%20purposes.%0A%20%20%20%20%20%20%20%20useHash%3A%20%7B%0A%20%20%20%20%20%20%20%20%20%20type%3A%20Boolean%2C%0A%20%20%20%20%20%20%20%20%20%20value%3A%20false%2C%0A%20%20%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_getModeFromIndex%3A%20function(modeIndex)%20%7B%0A%20%20%20%20%20%20%20%20var%20mode%20%3D%20this.tabs%5BmodeIndex%5D%3B%0A%20%20%20%20%20%20%20%20TF.URIStorage.setString(TF.URIStorage.TAB%2C%20mode)%3B%0A%20%20%20%20%20%20%20%20return%20mode%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_makeBackend%3A%20function(router%2C%20demoDir)%20%7B%0A%20%20%20%20%20%20%20%20%2F%2F%20use%20the%20demoDir%20if%20it%20is%20set%2C%20otherwise%20use%20the%20provided%20router%0A%20%20%20%20%20%20%20%20if%20(demoDir%20!%3D%20null)%20%7B%0A%20%20%20%20%20%20%20%20%20%20router%20%3D%20TF.Backend.router(demoDir%2C%20true)%3B%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%20%20return%20new%20TF.Backend.Backend(router)%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsScalars%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22scalars%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsImages%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22images%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsAudio%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22audio%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsGraphs%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22graphs%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsEmbeddings%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22embeddings%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsDistributions%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22distributions%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_modeIsHistograms%3A%20function(mode)%20%7B%0A%20%20%20%20%20%20%20%20return%20mode%20%3D%3D%3D%20%22histograms%22%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20selectedDashboard%3A%20function()%20%7B%0A%20%20%20%20%20%20%20%20var%20dashboard%20%3D%20this.%24%24(%22%23%22%20%2B%20this.mode)%3B%0A%20%20%20%20%20%20%20%20if%20(dashboard%20%3D%3D%20null)%20%7B%0A%20%20%20%20%20%20%20%20%20%20throw%20new%20Error(%60Unable%20to%20find%20dashboard%20for%20mode%3A%20%24%7Bthis.mode%7D%60)%3B%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%20%20return%20dashboard%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20ready%3A%20function()%20%7B%0A%20%20%20%20%20%20%20%20TF.Globals.USE_HASH%20%3D%20this.useHash%3B%0A%0A%20%20%20%20%20%20%20%20this._getModeFromHash()%3B%0A%20%20%20%20%20%20%20%20window.addEventListener(&apos;hashchange&apos;%2C%20function()%20%7B%0A%20%20%20%20%20%20%20%20%20%20this._getModeFromHash()%3B%0A%20%20%20%20%20%20%20%20%7D.bind(this))%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20_getModeFromHash%3A%20function()%20%7B%0A%20%20%20%20%20%20%20%20var%20tabName%20%3D%20TF.URIStorage.getString(TF.URIStorage.TAB)%3B%0A%20%20%20%20%20%20%20%20var%20modeIndex%20%3D%20this.tabs.indexOf(tabName)%3B%0A%20%20%20%20%20%20%20%20if%20(modeIndex%20%3D%3D%20-1%20%26%26%20this.modeIndex%20%3D%3D%20null)%20%7B%0A%20%20%20%20%20%20%20%20%20%20%2F%2F%20Select%20the%20first%20tab%20as%20default.%0A%20%20%20%20%20%20%20%20%20%20this.set(&apos;modeIndex&apos;%2C%200)%3B%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%20%20if%20(modeIndex%20!%3D%20-1%20%26%26%20modeIndex%20!%3D%20this.modeIndex)%20%7B%0A%20%20%20%20%20%20%20%20%20%20this.set(&apos;modeIndex&apos;%2C%20modeIndex)%3B%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20reload%3A%20function()%20%7B%0A%20%20%20%20%20%20%20%20if%20(this.mode%20%3D%3D%3D%20%22graphs%22%20%7C%7C%20this.mode%20%3D%3D%3D%20%22embeddings%22)%20%7B%0A%20%20%20%20%20%20%20%20%20%20return%3B%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%20%20this.selectedDashboard().reload()%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%20%20openSettings%3A%20function()%20%7B%0A%20%20%20%20%20%20%20%20this.%24.settings.open()%3B%0A%20%20%20%20%20%20%7D%2C%0A%20%20%20%20%7D)%3B%0A%20%20%0A%2F%2F%23%20sourceURL%3Dhttp%3A%2F%2Fec2-54-221-2-83.compute-1.amazonaws.com%3A6006%2Fdist%2Ftf-tensorboard.html-72.js%0A, Line: 77, Column: 27\r\n```\r\n", "comments": ["Thanks for reporting! It will be hard for us to test since we don't have easy access to these machines.", "I can set you up an instance if you tell me when you\u2019re ready to work on it.\r\n\r\nFrom: drpngx [mailto:notifications@github.com]\r\nSent: Saturday, April 22, 2017 11:20 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Mark Ebersole <mebersole@nvidia.com>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] TensorBoard does not load in Internet Explorer 11 (#9372)\r\n\r\n\r\nThanks for reporting! It will be hard for us to test since we don't have easy access to these machines.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/9372#issuecomment-296388123>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ADGOcPcgkdWNXNZ3JVP0FG7YFkP8zJQwks5ryja5gaJpZM4NEnSJ>.\r\n\r\n-----------------------------------------------------------------------------------\r\nThis email message is for the sole use of the intended recipient(s) and may contain\r\nconfidential information.  Any unauthorized review, use, disclosure or distribution\r\nis prohibited.  If you are not the intended recipient, please contact the sender by\r\nreply email and destroy all copies of the original message.\r\n-----------------------------------------------------------------------------------\r\n", "Unfortunately, the TensorBoard team is not committing to supporting IE. If someone in the community wants to step up and support it themselves, we'll be happy to see that. Otherwise, please download Chrome or Firefox. "]}, {"number": 9371, "title": "load python saved model in java", "body": "Hello,\r\n\r\nI have created, trained and saved a tensorflow model using python \r\n\r\n`classifier = learn.DNNClassifier(hidden_units=[10, 20, 5], n_classes=5\r\n                                 ,feature_columns=feature_columns\r\n                                 ,model_dir= model_dir\r\n                                 )\r\n`\r\n\r\nThe model files are in :\r\n\r\n`D:\\java\\workspace\\APIJavaSampleCode\\tfModels\\dnn\\ModelSave`\r\n\r\nfiles :\r\n\r\ncheckpoint\r\nevents.out.tfevents.1492792287.BIRINHOS-PC\r\ngraph.pbtxt\r\nmodel.ckpt-1.data-00000-of-00001\r\nmodel.ckpt-1.index\r\nmodel.ckpt-1.meta\r\nmodel.ckpt-100.data-00000-of-00001\r\nmodel.ckpt-100.index\r\nmodel.ckpt-100.meta\r\n \r\nin Java I have the line code :\r\n\r\n`SavedModelBundle.load(\"D:/java/workspace/APIJavaSampleCode/tfModels/dnn/ModelSave\");`\r\n\r\nThe result error is :\r\n\r\n`Exception in thread \"main\" org.tensorflow.TensorFlowException: SavedModel not found in export directory: D:/java/workspace/APIJavaSampleCode/tfModels/dnn/ModelSave\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n\tat org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:38)\r\n\tat tensorflow.HelloTF.main(HelloTF.java:32)\r\n`\r\n\r\nCan anyone help me loading a tf model in Java ? \r\n\r\nThanks ", "comments": ["in java the tf version is 1.1.0-rc1\r\n\r\nin python the tf version is (print(tf.VERSION) ) :\r\n1.1.0 \r\n", "I have download the git example model file \"saved_model.pb\" and it work (load in my java program).\r\n\r\nmaybe it is because is a binary format ? \r\n\r\nI have to see how to set DNNClassifier() to save binary format.. and test..   \r\n\r\nhowever the example .pb file given in https://www.tensorflow.org/extend/tool_developers/ (https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip)\r\n\r\ndoes not work ... ", "well this is like mission impossible using the files created by .DNNClassifier(hidden_units=[10, 20, 5], n_classes=5 ,feature_columns=feature_columns ,model_dir= model_dir ) setup ... it seams like it is impossible to have a model loaded in java after trained in python using tensorflow  \r\n\r\nI will turn to deeplearning4J ... \r\n", "@rjpg see https://github.com/loretoparisi/tensorflow-java, if it can help!", "hello,\r\n\r\nthanks for the help. I found a solution and you can get the example code there :\r\n\r\nhttp://stackoverflow.com/questions/43598953/loading-sklearn-model-in-java-model-created-with-dnnclassifier-in-python\r\n\r\n", "hello! I  meet the same problem!  I  use tf 1.4 and train a model with python 3.6,now I want to load the model in java ,but it can not work! throws the exception SavedModel not found in export directory: d:/pythonworkspace/alerm/model/predicted........\r\ncan you share you point about how to solution the problem? or  the model train in python can not load in java ?", "same with @moroseking .  I use tf 1.4 and train a model , when I load in java throws the exception SavedModel not found.", "Same problem with TF 1.5 for Windows when trying to load [this model](http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_4_layer.zip). Code:\r\n```\r\ntry (SavedModelBundle modelBundle = SavedModelBundle.load(\"D:/model/ende_gnmt_model_4_layer\")) {\r\n...\r\n}\r\n```\r\nException:\r\n```\r\norg.tensorflow.TensorFlowException: SavedModel not found in export directory: D:/model/ende_gnmt_model_4_layer\r\n\tat org.tensorflow.SavedModelBundle.load(Native Method)\r\n\tat org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:39)\r\n```\r\nDirectory content:\r\n```\r\ntranslate.ckpt.data-00000-of-00001\r\ntranslate.ckpt.index\r\ntranslate.ckpt.meta\r\n```\r\nI am not sure if `SavedModelBundle.load()` can load `ckpt` model at all, because `tensorflow/cc/saved_model/loader.cc#MaybeSavedModelDirectory(const string& export_dir):297` checks that either `saved_model.pb` or `saved_model.pbtxt` is present in the given directory:\r\n```\r\nStatus LoadSavedModelInternal(const SessionOptions& session_options,\r\n                              const RunOptions& run_options,\r\n                              const string& export_dir,\r\n                              const std::unordered_set<string>& tags,\r\n                              SavedModelBundle* const bundle) {\r\n  if (!MaybeSavedModelDirectory(export_dir)) {\r\n    return Status(error::Code::NOT_FOUND,\r\n                  \"SavedModel not found in export directory: \" + export_dir);\r\n  }\r\n...\r\nbool MaybeSavedModelDirectory(const string& export_dir) {\r\n  const string saved_model_pb_path =\r\n      io::JoinPath(export_dir, kSavedModelFilenamePb);\r\n  const string saved_model_pbtxt_path =\r\n      io::JoinPath(export_dir, kSavedModelFilenamePbTxt);\r\n  return Env::Default()->FileExists(saved_model_pb_path).ok() ||\r\n         Env::Default()->FileExists(saved_model_pbtxt_path).ok();\r\n}\r\n```", "@dmak What if you use: `\"D:/model/ende_gnmt_model_4_layer.ckpt`?", "I had the same issue as @dmak. When looking at the `SavedModelBundle.load` Javadocs I noticed that The model that is being loaded should be created using the Saved Model API. I was at first using the `Saver` API. After switching to the `Saved Model` API for writing the saved model, it worked.", "@timmolter It would be fantastic if you could add links (e.g. to Python API) to your above post to tell the difference between `Saver API` and `aver Model API`. Thanks!\r\n\r\nThe argument for `SavedModelBundle.load()` should be a directory containing a `.pb` or `.pbtxt` file. Just for case I've tried `SavedModelBundle.load(\"D:/model/ende_gnmt_model_4_layer/translate.ckpt\")` and it didn't work either. I had to convert `.ckpt` to `.pb` using the \"model freezing\" approach described in [How to convert .meta, .data and .index model files into one graph.pb file](https://stackoverflow.com/a/45868106/267197). After that the graph could be loaded like that:\r\n```\r\nimport org.apache.commons.io.FileUtils;\r\nimport org.tensorflow.Graph;\r\n\r\ntry (Graph graph = new Graph()) {\r\n    graph.importGraphDef(FileUtils.readFileToByteArray(new File(\"D:/translate.pb\")));\r\n}\r\n```", "I think what he means by `Saver Api` is: \r\n```\r\nsaver = tf.train.Saver()\r\n...\r\nsaver.save(sess, CHECKPOINT_PATH) \r\n```\r\nwhich saves out .chkpt, .pbtxt and variables files.\r\n\r\nUsing the `SaverModel Api`:\r\n```\r\nbuilders = saved_model_builder.SavedModelBuilder(export_path)\r\nbuilders.add_meta_graph_and_variables(sess, [\"myTag\"], signature_def_map= {\r\n        \"predict\": tf.saved_model.signature_def_utils.predict_signature_def(\r\n            inputs= {\"images\": x_placeholder},\r\n            outputs= {\"scores\": output})\r\n        })\r\nbuilders.save()\r\n```\r\nWhich outputs:\r\n```\r\n.\r\n\u251c\u2500\u2500 saved_model.pb\r\n\u2514\u2500\u2500 variables\r\n    \u251c\u2500\u2500 variables.data-00000-of-00001\r\n    \u2514\u2500\u2500 variables.index\r\n```\r\nwhich then can be loaded with:\r\n```\r\ntf.saved_model.loader.load(sess, [\"myTag\"], OUTPUT_DIR)\r\ngraph = tf.get_default_graph()\r\n```"]}, {"number": 9370, "title": "Possibly serious bug in cuDNN RNNParamsSaveable", "body": "`RNNParamsSaveable` appears to only save half of the weights when the RNN is bidirectional. See below.\r\n\r\nWhen the RNN is unidirectional, `model.params_size()` matches the total size of weights + biases returned by `model.params_to_canonical(params)`\r\n\r\n```\r\nmodel = cudnn_rnn_ops.CudnnLSTM(num_layers=1, num_units=100, input_size=20, direction='unidirectional')\r\nparams = tf.get_variable('cudnn_rnn_params', initializer=tf.random_uniform([model.params_size()]), validate_shape=False)\r\nmodel.params_size().eval(session=sess) # returns 48800\r\nsum([wts.eval(session=sess).shape[0] for wtss in model.params_to_canonical(params) for wts in wtss]) # returns 48800\r\n```\r\n\r\nOn the other hand, when the RNN is bidirectional, `model.params_size()` returns twice the size of the unidirectional case, which makes sense, but the size of `model.params_to_canonical(params)` is unchanged.\r\n\r\n```\r\nmodel = cudnn_rnn_ops.CudnnLSTM(num_layers=1, num_units=100, input_size=20, direction='bidirectional')\r\nparams = tf.get_variable('cudnn_rnn_params', initializer=tf.random_uniform([model.params_size()]), validate_shape=False)\r\nmodel.params_size().eval(session=sess) # returns 97600\r\nsum([wts.eval(session=sess).shape[0] for wtss in model.params_to_canonical(params) for wts in wtss]) # returns 48800\r\n```\r\n\r\nI believe this may have been missed by tests because, as this [TODO](https://github.com/tensorflow/tensorflow/blob/f0f7a1ef63c47075e3eb7eaeeb3588d057f3171d/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L37) suggests, both the canonical and non-canonical versions are being saved and restored, and so even if only half the weights are being restored from the canonical version, the non-canonical weights can compensate and hide the problem.\r\n\r\nAm I missing something?", "comments": ["@ebrevdo any comment on this? Is this a documentation or usage issue?", "And updates on this?", "@alquraishi, thanks for reporting the issue. This does look like a bug/feature request. @boche also reported a related (potentially the same) issue here: https://github.com/tensorflow/tensorflow/issues/6072\r\n\r\nI looked into the code and currently the code only stores params for the forward direction, as you have observed. So I don't think checkpointing for the bidirectional case is currently working/supported.\r\n\r\nThe unknown param order (in the bidirectional case) problem @boche raised is a valid question. I looked into the cuDNN documentation. The order (in the bidirectional case) is not documented for function cudnnGetRNNLinLayerMatrixParams and \u00a0cudnnGetRNNLinLayerBiasParams. Another possibility is that the backward params are accessed by the \"layer\" parameter in these two functions. However, it is also not clear if this is the case in the cuDNN documentation.\r\n\r\nI marked this as contribution welcome for interested people to take a deeper look from here and possibly propose and work on a fix.\r\n\r\n\r\n", "We should consider also contacting nvidia for clarification in their\ndocumentation.\n\nOn Sat, May 6, 2017 at 2:43 PM, zhangyaobit <notifications@github.com>\nwrote:\n\n> @alquraishi <https://github.com/alquraishi>, thanks for reporting the\n> issue. This does look like a bug/feature request. @boche\n> <https://github.com/boche> also reported a related (potentially the same)\n> issue here: #6072 <https://github.com/tensorflow/tensorflow/issues/6072>\n>\n> I looked into the code and currently the code only stores params for\n> forward direction, as you have observed. So I don't think checkpointing for\n> the bidirectional the case is currently working/supported.\n>\n> The unknown param order (in the bidirectional case) problem @boche\n> <https://github.com/boche> raised is a valid question. I looked in the\n> cuDNN documentation. The order (in the bidirectional case) is not\n> documented for function cudnnGetRNNLinLayerMatrixParams and  cudnnGetRNNLinLayerBiasParams.\n> Another possibility is that the backward params are accessed by the \"layer\"\n> parameter in these two functions. However, it is also not clear if this is\n> the case in the cuDNN documentation.\n>\n> I marked this as contribution welcome for interested people to take a\n> deeper look from here and possibly propose and work on a fix.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9370#issuecomment-299667653>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7L2tls7yl9HK82TWyLdUmS3PP1Uks5r3Ol5gaJpZM4NEkX1>\n> .\n>\n", "Good point. Assigning @zheng-xq", "May I suggest that this gets documented very clearly as a limitation, perhaps even preventing the user from accidentally using RNNParamsSaver with a bidirectional RNN? I understand the temptation to mark this as contributions welcome and set it aside, but I really do think it's a very serious bug. What makes it particularly problematic is that it's completely silent, so a user may suddenly see their model behaving inexplicably without any idea why. The only reason I stumbled onto this is because I noticed the size difference by accident, otherwise I would've merrily used it without realizing there's a hidden bug, and I routinely checkpoint models on one GPU type and resume on another, so it's not an unlikely scenario. Just my two cents. Of course ideally the problem would get solved.", "Any updates on this?", "@alquraishi, we haven't had time to look into this. We provided this checkpointing feature for cuDNN and I'm glad you find it useful and seem to be a heavy user of it. If you think you benefit from this feature, please consider registering as a contributor and contributing back to improve it. We could together make TensorFlow better. I like your idea of better documentation. And if you could help dive deeper (from my previous suggestions) into the bidirectional RNN issue you found, that would be even better. Please consider registering as NVIDIA developer as well to ask them about the bidirectional RNN layout. \r\n\r\nWe just haven't found a time to look into this, ebrevdo@ and zheng-xq@ will follow up when they get a chance. ", "This is not a bug in the checkpointing nor in `RNNParamsSaveable`.\r\n\r\nThis is a bug in the `CudnnRNNParamsToCanonical` op or in `CudnnSupport::createRnnDescriptor` or even in cuDNN itself.\r\n\r\nNote that the test also seems to be not correct. @alquraishi, you are only summing `shape[0]`. But you should sum `prod(shape)`. I wonder why that worked for you in the unidirectional case. It does not for me.\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "This class is deprecated. The new CudnnRNN layers manages vars and saveables for users."]}, {"number": 9369, "title": "Sub-pixel shuffling tensor operation", "body": "Is there interest in integrating a [subpixel shuffling](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf) operation as a core operation to Tensorflow? I've implemented such an operation on my [fork](https://github.com/jhetherly/tensorflow/tree/shuffle_op) and I think this could find widespread use. My implementation is a bit more generic than the one in the paper and is implemented using the C++ Tensorflow API. The operation I've defined is dimension agnostic and is found in the `contrib` portion of the source tree. Let me know if there is anything I need to do before making a pull request (i.e. better documentation, etc.).", "comments": ["Feel free to send the PR and we can review it there. Please CC @martinwicke and make sure you put `Fixes #9369` in the commit message.", "I'm interested in this operation, but I don't know how to integrate it into my model with variable batch size since only one dimension can be ``None`` in this implementation. Can you help me out with it?", "Since, I think, tf 2.x has removed contrib, how do we now access the `PeriodicResample` operator? It could be useful for subpixel convolution (the original goal of this issue). I am guessing it is now [`depth_to_space`](https://www.tensorflow.org/api_docs/python/tf/nn/depth_to_space) but I am not sure, can anyone confirm?", "I think you can use depth_to_space or a combination of flatten + reshape to\nachieve the same result, although I'm not sure whether there are behavior\ndifferences in cases where things don't divide neatly.\n", "Ok thanks ! Yes in my case everything divides nicely so I should be fine using `depth_to_space`."]}, {"number": 9367, "title": "[XLA] Ptxas Error when TF_CPP_MIN_VLOG_LEVEL=2 ", "body": "### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: Linux Ubuntu 14.04\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version* (use command below): `('v1.1.0-rc2-219-g623dd83', '1.1.0-rc2')`\r\n- *Bazel version (if compiling from source)*: `0.4.5-jdk7`\r\n- *CUDA/cuDNN version*: 7.5/5\r\n- *GPU Model and Memory*: GeForce GTX TitanX \r\n- *Exact command to reproduce*: `python test.py --batch_size 16 --step 20`\r\n\r\n### Describe the problem clearly\r\nTo make tensorflow print the logs in VLOG(2), I set the `TF_CPP_MIN_VLOG_LEVEL=2`. After doing that, the program throws a fatal error. It seems that there's something wrong when compiling xla hlo_instruction to ptx.\r\n\r\n>2017-04-21 14:35:23.158362: I tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:219] ptxas fatal   : SM version specified by .target is higher than default SM version assumed\r\n2017-04-21 14:35:23.158423: F tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:221] Invalid PTX. See the error message above for reasons.\r\n\r\n\r\n### Source Code / Logs\r\n\r\nFull log can be found [here](https://gist.github.com/pgplus1628/b257901de5af4bdd88fd78adab084177)\r\nReproduce with command `python test.py --batch_size 16 --step 20`  \r\n\r\nCode:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '2' # enable logging debug info\r\nimport inspect\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nflags = tf.flags\r\nlogging = tf.logging\r\nflags.DEFINE_integer(\"batch_size\", 1, \"inference batch size\")\r\nflags.DEFINE_integer(\"step\", 1, \"step size for infernece\")\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef data_type():\r\n    return tf.float32\r\n\r\n\r\nclass InputData(object):\r\n    \r\n    def __init__(self, config):\r\n        self.batch_size = batch_size = config.batch_size\r\n        self.num_steps = num_steps = config.num_steps\r\n        self.hidden_size = hidden_size = config.hidden_size\r\n        self.input_data = tf.placeholder(data_type(), [batch_size, num_steps, hidden_size], name = 'input_data')\r\n\r\n\r\nclass Config(object):\r\n    num_layers = 1\r\n    num_steps = 20\r\n    hidden_size = 256\r\n    batch_size = 20\r\n    vocab_size = 10000\r\n    init_scale = 0.1\r\n    num_iter = 50\r\n    warm_iter = 2\r\n\r\n\r\nclass LSTMModel(object):\r\n    \"\"\" Only forward, No Embedding\r\n    \"\"\"\r\n\r\n    def __init__(self, config, input_):\r\n        self._input = input_\r\n        self._input_data = self._input.input_data\r\n        \r\n        batch_size = input_.batch_size\r\n        num_steps =  input_.num_steps\r\n        size = config.hidden_size\r\n        vocab_size = config.vocab_size\r\n\r\n        def lstm_cell():\r\n            if 'reuse' in inspect.getargspec(\r\n              tf.contrib.rnn.BasicLSTMCell.__init__).args:\r\n                print(\"reuse\")\r\n                return tf.contrib.rnn.BasicLSTMCell(\r\n                    size, forget_bias=0., state_is_tuple=True,\r\n                    reuse = tf.get_variable_scope().reuse)\r\n            else:\r\n                print(\"not reuse\")\r\n                return tf.contrib.rnn.BasicLSTMCell(\r\n                    size, forget_bias=0.0, state_is_tuple=True)\r\n\r\n        attn_cell = lstm_cell\r\n        cell = tf.contrib.rnn.MultiRNNCell(\r\n            [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\r\n\r\n        self._initial_state = cell.zero_state(batch_size, data_type())\r\n\r\n        outputs = []\r\n        state = self._initial_state\r\n        with tf.variable_scope(\"RNN\"):\r\n            for time_step in range(num_steps):\r\n                if time_step > 0 : tf.get_variable_scope().reuse_variables()\r\n                (cell_output, state) = cell(self._input.input_data[:, time_step, :], state)\r\n                outputs.append(cell_output)\r\n        \r\n        self._output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])\r\n        self._final_state = state\r\n        softmax_w = tf.get_variable(\r\n            \"softmax_w\", [size, vocab_size], dtype=data_type())\r\n        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\r\n        self._logits = tf.matmul(self._output, softmax_w) + softmax_b\r\n\r\n        return\r\n\r\n    @property\r\n    def initial_state(self):\r\n        return self._initial_state\r\n\r\n    @property\r\n    def logits(self):\r\n        return self._logits\r\n  \r\n    @property\r\n    def input_data(self):\r\n        return self._input_data\r\n\r\n\r\ndef run_inference(session, model, input_data, sv) :\r\n    # initialize with a clean state\r\n    state = session.run(model.initial_state)\r\n\r\n    fetches = {}\r\n    fetches['logit'] = model.logits\r\n\r\n    feed_dict = {}\r\n    feed_dict[model.input_data] = input_data\r\n    for i, (c, h) in enumerate(model.initial_state):\r\n        feed_dict[c] = state[i].c\r\n        feed_dict[h] = state[i].h\r\n\r\n    session.run(fetches, feed_dict)\r\n\r\n\r\ndef main(_):\r\n    # config\r\n    eval_config = Config()\r\n    eval_config.num_steps = FLAGS.step\r\n    eval_config.batch_size = FLAGS.batch_size\r\n\r\n    # generate random data\r\n    input_data = np.random.rand(eval_config.batch_size, eval_config.num_steps, eval_config.hidden_size).astype(np.float32)\r\n\r\n    with tf.Graph().as_default():\r\n        initializer = tf.random_uniform_initializer(-eval_config.init_scale, \r\n                                                    eval_config.init_scale)\r\n        with tf.name_scope('Inference'):\r\n            _input = InputData(eval_config)\r\n            with tf.variable_scope('Model', reuse=None, initializer=initializer):\r\n                model = LSTMModel(config=eval_config, input_=_input)\r\n\r\n        sv = tf.train.Supervisor()\r\n        sess_config = tf.ConfigProto(allow_soft_placement=True,\r\n                                     log_device_placement=False)\r\n        # enable xla\r\n        sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\n        # run inference\r\n        with sv.managed_session(config=sess_config) as session:\r\n            run_inference(session, model, input_data, sv)\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n```", "comments": ["I guess this error is caused by the code in `tensorflow/tensorflow/compiler/service/gpu/gpu_compier.cc`\r\n\r\n```\r\n  ptxas_info_dumper.SetProgram(ptxas_path, {ptxas_path, ptx_path, \"-o\",\r\n                                            \"/dev/null\", \"-v\", \"-arch=sm_35\"});\r\n```\r\n\r\nbecause the ptx generated by xla has the target  of `sm_52`.\r\n\r\n```\r\n//\r\n// Generated by LLVM NVPTX Back-End\r\n//\r\n\r\n.version 4.2\r\n.target sm_52\r\n.address_size 64\r\n```", "@tatatodd could you suggest a fix?", "Nice debugging @pgplus1628 \r\n\r\nAs a short-term workaround, which you've probably already done, you can change the -arch=sm_35 to the appropriate value (e.g. in your case -arch=sm_52), and recompile and run.\r\n\r\nWe'll figure out a better fix moving forward."]}]