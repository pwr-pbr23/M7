[{"number": 10650, "title": "tf.reshape does not take tf.Dimension as argument ", "body": "Minimal code to reproduce\r\n\r\n```\r\nimport tensorflow as tf\r\nt = tf.constant([1, 2, 3])\r\nd = tf.Dimension(3)\r\nt = tf.reshape(t, [-1, d])\r\n```\r\n\r\nGives stack trace:\r\n\r\n```\r\nTypeError: Expected binary or unicode string, got -1\r\n```\r\n\r\n\r\nThe reason this would be useful to allow is because when you access tensor shapes (with, e.g. ```t.shape```) you get it in tf.Dimension so if you want to assign relative to the current shape then you don't need to convert to int\r\n\r\n\r\nIf there is some reason why tensorflow doesn't allow this behaviour than I think at least the stack trace should be more verbose (e.g. in the example above it shouldn't be complaining about -1, it should be complaining about the value in tf.Dimension class)", "comments": ["Yes, I can reproduce this. @lukaszkaiser can comment further, but reuse in variable scope is one of those things in the API that is somewhat confusing and we have been working on making more consistent.", "Sorry guys, is this the right github id? I don't see how this has anything to do with variables, what am I missing?", "Yes I don't think this has anything to do with variable scope, possible confusion between a different GitHub issue\r\n\r\n\r\n", "I'm facing this exact issue, is there a workaround for this?\r\n\r\nps. for me the error is:\r\n```\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [-1, Dimension(3)]. Consider casting elements to a supported type.\r\n```\r\non tensorflow 1.2.1", "Yeah there is a workaround, cast the element to an int (that message telling you cast to a supported type is new since this issue was created so that is good.)", "In fact I think the new error message is satisfactory enough that I will close the issue", "@artcg that'd work in this specific use case, but in my actual case the dimension was unknown at that point. Casting it to int wouldn't help.", "I think you should be able to just wrap an int() over wherever you are passing the tf.Dimension object?\r\n\r\nCan you post your code to re-create?", "I still find this error in `tf.reshape` whenever the list of indicating the new shape includes a `None`. This might be useful in the case where we don't know the exact shape or the shape might change dynamically according to the data input."]}, {"number": 10649, "title": "Implement architecture-independent fallback in tensorflow/workspace.bzl", "body": "Archictecture-dependent binaries such as `nodejs` limits portability to different targets (namely, I'm interested in Tensorflow ppc64le builds). I tried replacing x64 binaries with ppc64 in the cached dirs and managed to compile Tensorflow (see https://github.com/tensorflow/tensorflow/issues/10306).\r\n\r\nThis issue is similar to https://github.com/bazelbuild/rules_closure/issues/207.\r\n", "comments": ["Thanks for experimenting. It would be great if you could contribute a pull-request of this!", "@aselle But what was the motivation for using a binary on bazel? I tried to get this [answer on StackOverflow](https://stackoverflow.com/questions/44413137/why-does-bazels-rules-closure-downloads-platform-specific-binaries-instead-of-s) but I'm still waiting.\r\n\r\nFor now, IMHO, the best solution is to ask bazel to compile these sources instead of downloading a binary.", "Thk", "nodejs binary is used by tensorboard, I think.\r\nIn TF we should not be downloading and using any binaries, except if you enable MKL support.\r\n\r\nShould this be filed against tensorboard repository?", "Closing under TF as we should not have nodejs dependency anymore in TF."]}, {"number": 10648, "title": "Segmentation Fault (core dumped) on exit from unit test that imports `tf.contrib.rnn`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.0.0\r\n- **Bazel version (if compiling from source)**: 0.4.4\r\n- **CUDA/cuDNN version**: 8.0/5110\r\n- **GPU model and memory**: GeForce GTX 980 Ti\r\n- **Exact command to reproduce**: `python -m unittest discover -s tests -p \"example_test.py\"`(see below for details)\r\n\r\n### Describe the problem\r\nI am running Tensorflow code from within the Python `unittest` module. The individual unit-tests, each of which train a specific architecture, run successfully with an OK. However, I observed that given the above described system configuration, I receive a `Segmentation Fault (core dumped)` just before the program exits. And this leads to the overall test being marked as `FAIL` (despite the individual tests passing).\r\n\r\nOn further investigation, I observed that the segmentation fault can be prevented from occurring by preventing the `tensorflow/contrib/rnn/python/ops/_gru_ops.so` file from being loaded by the `load_op_library` function inside `tensorflow.contrib.rnn.python.ops.gru_ops`\r\n\r\n### Source code / logs\r\nSay I have a folder `my_code` that contains a `tests` folder where my unit-tests are all located. The error can be reproduced by running an `example_test.py` (given below) within the `unittest` module using the call `python -m unittest discover -s tests -p \"example_test.py\" from within the `my_code` directory. The file `my_code/tests/example_tests.py` contains the following:\r\n\r\n```\r\nimport unittest                               \r\nimport tensorflow as tf                       \r\n                                              \r\nclass SomeTestClass(unittest.TestCase): \r\n    \"\"\"Some docstring.\"\"\"              \r\n                                              \r\n    def test_something(self):          \r\n        print(\"Testing something...\\n\")\r\n        print tf.contrib.rnn.LSTMCell         \r\n        session = tf.Session()                \r\n        session.close()                       \r\n```\r\n\r\nNote that the error occurs only when both `session` is created and there is a reference to `tf.contrib.rnn`.", "comments": ["I'm having the same problem - my python code that uses TensorFlow will segfault when the python process terminates. I'm also using TF 1.0.0, but haven't touched any of the source code. \r\n\r\nI am also seeing the problem ONLY when I use both tf.Session() AND anything in tf.contrib.rnn. ", "@gunan, do you have any ideas on this? I am able to reproduce this.\r\n", "I was able to reproduce this in 1.0.1, but I found that in the nightly build it works fine, so I'd recommend upgrading to at least 1.2.", "I am using `tensorflow-gpu` binary and could not reproduce this with v1.1.0 (current PyPI version).\r\n\r\nLog is [here](https://gist.github.com/byronyi/54d30dd39f5420d883a1a26919e6ad7c).", "Yes - I don't see the problem in v1.1 or higher. However, our team is heavily using v1.0 and we won't be able to upgrade everything anytime soon....\r\n\r\nFor what it's worth, I found a temporary fix:\r\n\r\nI have a train() method, in which i have something like\r\n\r\n```\r\ndef train(self, ...):\r\n    self.get_model(...)\r\n    self.fit(...)\r\n```\r\n\r\nin which get_model() builds the model:\r\n\r\n```\r\ndef get_model(self, ...):\r\n    tf.reset_default_graph()\r\n    self.tf_graph = tf.Graph()\r\n    with self.tf_graph.as_default():\r\n        build graph...(this is where I use tf.contrib.rnn)\r\n```\r\n\r\nand fit() trains the model:\r\n\r\n```\r\ndef fit(self, ...):\r\n    with self.tf_graph.as_default():\r\n        self.sess = tf.Session()\r\n        self.sess.run(...)\r\n```\r\n\r\n\r\n\r\nThe temporary fix I added was in train():\r\n\r\n```\r\ndef train(self, ...):\r\n    _ = tf.Session()   # adding this makes the segfault go away\r\n    self.get_model(...)\r\n    self.fit(...)\r\n```\r\n\r\n\r\nWhat's interesting is the session returned from _ = tf.Session() is never used anywhere, because in fit() I make a new session and use that session. \r\n\r\nHopefully this helps anyone who's also stuck in v1.0 for the moment...\r\n\r\n\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10647, "title": "SyntaxError: invalid syntax when import tensorflow-gpu as tf  ", "body": "hiiiii,,,,\r\niam newbie in tensorflow, i have followed tutorial from https://www.tensorflow.org/versions/r0.12/get_started/os_setup#using_conda. Every step that i take,  no error,\r\n\r\nbut when import tensorflow, it's appears error, like this\r\n\r\n> import tensorflow-gpu as tf\r\n  File \"<stdin>\", line 1\r\n    import tensorflow-gpu as tf\r\n                     ^\r\nSyntaxError: invalid syntax\r\n\r\n\r\nplease help me ", "comments": ["The package on pypi is called tensorflow-gpu but you just import it with \"tensorflow\"\r\n\r\nso...\r\n\r\n```import tensorflow as tf```", "thank u somemuch @artcg :) \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d thanks for everything \ud83d\udc4d ", "I feel that this issue may be open\r\n\r\n```\r\n(Python 3.7) MacBook-xxxe:python xxx$ python3 tensorflow1.py\r\n  File \"tensorflow1.py\", line 3\r\n    import tensorflow-gpu as tf\r\n                     ^\r\nSyntaxError: invalid syntax\r\n```", "@GitHubMurt it's a non-issue, as artcg has said just do \r\n`import tensorflow as tf`", "hmm currently i'm not able to run it with python3 ,but I can with python 2", "then you need to install it with pip3. instead of just with pip\r\nso:\r\npip3 install tensorflow-gpu", "Suppose I have run both pip install tensorflow-gpu and pip3 install tensorflow-gpu, So, I have two different instances of tensor flow. I am I correct?\r\n\r\nNow I assume two instances of tf is installed, does it mean when I run my script with python3 tfsample.py, it goes and use tensorflow-gpu installed by pip3 and when I  use python2.7 tfsample.py, it goes and use tensorflow-gpu installed by pip?", "\"Suppose I have run both pip install tensorflow-gpu and pip3 install tensorflow-gpu, So, I have two different instances of tensor flow. I am I correct?\" -> yes, for python 2.x and 3.x each\r\n\r\n"]}, {"number": 10646, "title": "Clarify Installation Docs for Windows (32-bit)", "body": "Quick feature/docs request. I've noticed a few people on Stack Overflow asking about running Tensorflow on 32-bit machines. I'm aware that some people have had some success as described by @mrry 's post here:\r\n\r\n[S/O: \"TensorFlow on 32-bit Linux?\"](https://stackoverflow.com/a/33635450/7604321)\r\n\r\nOne example of a confused user trying to use Windows: \r\n\r\n[S/O: \"how to install tensorflow for windows 7 32bit system?i installed python 3.5(32 bit) into my system and also installed anaconda 3.4.4(32 bit)\"](https://stackoverflow.com/questions/44449972/how-to-install-tensorflow-for-windows-7-32bit-systemi-installed-python-3-532-b)\r\n\r\nI was wondering whether it was worth putting a little system requirement that reminded users that Tensorflow is only supported on 64 bit machines on the install page? Unless it is not then perhaps some little clarification? Cheers", "comments": ["@wolffg, could you triage this?", "Ping!\r\nI think this is something important, as we receive issues about installing on 32 bit OS every now and then.\r\nCC @dr4b @jugglerix", "Hi, this has been fixed and is visible in master here: https://www.tensorflow.org/versions/master/install/install_windows and will become root when r1.5 becomes the installable binary.  Thanks!"]}, {"number": 10645, "title": "The official code about:Reading data from Tfrecord file, I do not this it is a good way.Because it do not real random shuffle data, any one have a better way, and how to sample data with tfrecord file. ", "body": "tensorflow official code about read data from tfrecord like this:\r\n1. creat a shuffle file queue\r\n2. several thread read data from file that dequeue from file queue(The reader will read data from the file until the file is empty, and then the file queue dequeue a new file)\r\n3. put the data read by the reader to a randomshufflequeue, to make a batch data for training or testing.(this way likes one people drinking sweet water problem:drinking a mouthful of sweet water, and adding equal water.)\r\nEven though you shuffle data before convert it to tfrecord, it is not real random.\r\nIf you convert each data to a tfrecord file, I think this is real random, but the efficiency is reduced.\r\nAny one has a better way?\r\nImbalance data is common, sample is a good way to solve this problem, how to sample data with tfrecord file.\r\nThanks\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10644, "title": "TFRecordWriter.flush() for Python bundle?", "body": "TFRecords.Writer seems to take up too much memory when dealing with large dateset and may cause out-of-memory issue. \r\n\r\nAlso, the `.TFRecords` file seems not appendable right now, so it's pretty hard to write large dataset to a single `.TFRecords` file at one run.\r\n\r\n`Flush()` is available on the C++ side in tensorflow::core::lib::io::RecordWriter but is not exposed in the Python bindings. Is it possible to add a python interface to make it easier?", "comments": ["What do you mean by using too much memory? TfRecords doesn't consume any memory itself (especially if not using compression)\r\n\r\nYou could use strategies like writing shards of data to handle the appendability issues (this is how the protos for summary events are handled for tensorboard).\r\n\r\nFlush() would be reasonable to expose at the python level, we'd welcome a CL that did this.\r\n\r\n\r\n", "Created a PR #10686 to expose `Flush()` in Python.\r\n\r\nNote that in https://github.com/tensorflow/tensorflow/blob/abae4305a6208bde6072d257a6ce734a8d369089/tensorflow/core/lib/io/record_writer.cc#L125-L130\r\n```\r\nStatus RecordWriter::Flush() {\r\n  if (IsZlibCompressed(options_)) {\r\n    return dest_->Flush();\r\n  }\r\n  return Status::OK();\r\n}\r\n```\r\n\r\nSo the `Flush()` is only effective in case of compressed files. ", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10643, "title": "Implements tf.arg for complex numbers (Closes #483)", "body": "The following code runs as expected so I believe that the gradients are working fine\r\n```python\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\nx0 = tf.constant(1.0)\r\ny0 = tf.constant(2.0)\r\nc0 = tf.complex(x0, y0)\r\n\r\ny = tf.Variable(0.4)\r\nc = tf.complex(x0, y)\r\n\r\nerr = tf.abs(c - c0)\r\noptimize = tf.train.AdagradOptimizer(1e-1).minimize(err)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nfor i in range(10000):\r\n    sess.run(optimize)\r\n    if i % 10 == 0:\r\n        print(sess.run(c)) # should eventually converge to (1 + 2j)\r\n\r\n```", "comments": ["Can one of the admins verify this patch?", "@lakshayg maybe i'm missing something, but i don't see the new `tf.arg` used anywhere in the example code you provided above.", "@caisq Thanks for pointing that out. All this time I was testing on the incorrect code. I just tested on that actual code I was supposed to test on and it seems I have stumbled upon a bug. I will be revising the commits and resubmitting shortly. Here is the correct code\r\n\r\n```python\r\nsess = tf.InteractiveSession()\r\n\r\ntheta0 = tf.atan2(1.0, 1.0) # pi/4\r\nx0 = tf.constant(1.0)\r\ny  = tf.Variable(0.4)\r\nc  = tf.complex(x0, y)\r\ntheta = tf.arg(c)\r\nerr = (theta - theta0)**2\r\noptimize = tf.train.GradientDescentOptimizer(1e-1).minimize(err)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nfor i in range(10000):\r\n    sess.run(optimize)\r\n    if i % 10 == 0:\r\n        print(sess.run(c)) # should go to 1 + 1j\r\n```", "@caisq I have just amended the commit. The posted code works on my machine. Please review. ", "@lakshayg sorry, it looks like there are merge conflicts now. Feel free to merge and rebase.\r\nCC @asimshankar for go", "@drpngx Conflicts resolved!", "Jenkins, test this please.\n\nOn Jun 26, 2017 11:33 PM, \"Lakshay Garg\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> Conflicts resolved!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10643#issuecomment-311265866>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sba1aYUMxTB9TDbs-fhcC7q-gOHnTks5sIKJIgaJpZM4N2oY9>\n> .\n>\n", "Linux GPU test shows NO STATUS for all the tests. Seems to be a problem with the build infrastructure\r\n```\r\nExecuted 0 out of 319 tests: 1 test passes and 318 were skipped.\r\n\r\nParameterized build ends with FAILURE at: Tue Jun 27 15:14:37 UTC 2017 (Elapsed time: 70 s)\r\nBuild step 'Execute shell' marked build as failure\r\n[Set GitHub commit status (universal)] ERROR on repos [] (sha:5dcc47c) with context:tensorflow-pull-requests-gpu\r\nUnable to get pull request builder trigger!!\r\nSetting status of 65e5a2cfe96dba87ebbf26e8e264acaa6c009067 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/5506/ and message: 'FAILURE\r\n```\r\n\r\nLinux XLA, not sure why the tests fail to build\r\n```\r\nExecuted 241 out of 306 tests: 248 tests pass and 58 fail to build.\r\nBuild step 'Execute shell' marked build as failure\r\nUnable to get pull request builder trigger!!\r\nSetting status of 65e5a2cfe96dba87ebbf26e8e264acaa6c009067 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-xla/1514/ and message: 'FAILURE\r\n```", "So, the XLA is more of a problem. This is a new one, so it could be unrelated to this change.\r\n\r\nWe need to re-trigger the Linux GPU test to see what's going on.\r\n\r\nJenkins, test this please.", "@drpngx Linux GPU does the same again\r\n```\r\nExecuted 0 out of 319 tests: 1 test passes and 318 were skipped.\r\n\r\nParameterized build ends with FAILURE at: Tue Jun 27 16:59:38 UTC 2017 (Elapsed time: 104 s)\r\nBuild step 'Execute shell' marked build as failure\r\n[Set GitHub commit status (universal)] ERROR on repos [] (sha:5dcc47c) with context:tensorflow-pull-requests-gpu\r\nUnable to get pull request builder trigger!!\r\nSetting status of 65e5a2cfe96dba87ebbf26e8e264acaa6c009067 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/5513/ and message: 'FAILURE\r\n```", "@gunan Linux GPU and XLA are both new, right?", "@gunan never mind, it seems to succeed on other builds, so it's probably specific to this one.\r\n", "BTW, `tf.arg` is probably not a good name. I would suggest `angle`, like `numpy`.", "@drpngx There is some discussion about the name for this operation here: https://github.com/tensorflow/tensorflow/issues/483. `tf.arg` seems to be the one that was finally chosen.", "Jenkins, test this please.", "@drpngx these tests (XLA & GPU) are failing again. Do you know of a reason which might be causing this failure? ", "The code fails to build:\r\n\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: calling a __host__ function(\"__builtin_cargf\") from a __device__ function(\"Eigen::internal::EigenMetaKernelEval< ::Eigen::TensorEvaluator<const  ::Eigen::TensorAssignOp< ::Eigen::TensorMap< ::Eigen::Tensor<float, (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_arg_op< ::std::complex<float> > , const  ::Eigen::TensorMap< ::Eigen::Tensor<const  ::std::complex<float> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> > > ,  ::Eigen::GpuDevice> , int, (bool)0> ::run\") is not allowed\r\n```\r\n", "@drpngx Could you please run the tests again. I don't have a GPU machine to run them locally.", "Jenkins, test this please.", "Here's the error:\r\n\r\n```\r\nNFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_arg.cu.cc:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: calling a __host__ function(\"__builtin_cargf\") from a __device__ function(\"Eigen::internal::EigenMetaKernelEval< ::Eigen::TensorEvaluator<const  ::Eigen::TensorAssignOp< ::Eigen::TensorMap< ::Eigen::Tensor<float, (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_arg_op< ::std::complex<float> > , const  ::Eigen::TensorMap< ::Eigen::Tensor<const  ::std::complex<float> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> > > ,  ::Eigen::GpuDevice> , int, (bool)0> ::run\") is not allowed\r\n\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: calling a __host__ function(\"__builtin_carg\") from a __device__ function(\"Eigen::internal::EigenMetaKernelEval< ::Eigen::TensorEvaluator<const  ::Eigen::TensorAssignOp< ::Eigen::TensorMap< ::Eigen::Tensor<double, (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> , const  ::Eigen::TensorCwiseUnaryOp< ::Eigen::internal::scalar_arg_op< ::std::complex<double> > , const  ::Eigen::TensorMap< ::Eigen::Tensor<const  ::std::complex<double> , (int)1, (int)1, int> , (int)16,  ::Eigen::MakePointer> > > ,  ::Eigen::GpuDevice> , int, (bool)0> ::run\") is not allowed\r\n\r\n2 errors detected in the compilation of \"/tmp/tmpxft_00006593_00000000-7_cwise_op_gpu_arg.cu.cpp1.ii\".\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:2358:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/tensorflow/core/kernels/cwise_op_gpu_arg.cu.o' was not created.\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:2358:1: not all outputs were created or valid.\r\n```", "Another attempt at solving this. Please test again.", "Jenkins, test this please.", "I have reimplemented the op from scratch just to make sure that I am not missing something. Please test this.", "Jenkins, test this please.", "Can't seem to get my head aroud the problem. Should I just remove the GPU kernels at the moment, open an issue for writing tf.arg gpu kernels, mark it as \"contributions-welcome\" and send in a PR for that later?", "I have just removed the GPU support for arg. Please test again.", "@tensorflow-jenkins test this please", "```\r\nERROR: No test targets were found, yet testing was requested.\r\n```\r\n\r\nThis is the error message in the log. Any hints on how to proceed? @vrv ", "We identified the culprit for that problem, and created a rollback for it.\r\nOnce that is pushed, the test should be back to normal.\r\nLater today we should be able to test your PR again.", "@tensorflow-jenkins test this please", "This is ready to be merged.", "(Adding final API review tag before we can merge).", "(For API review): A few notes:\r\n\r\n- Would it make sense for this to be a composite op in Python instead of defining a TensorFlow op? Something like:\r\n```python\r\ndef arg(value, name=None):\r\n  return tf.atan2(tf.imag(value), tf.real(value))\r\n```\r\nOr am I missing something?\r\n\r\n- You don't need to update the `.go` files, it will be updated by a cron job that keeps the generated wrappers for Go in sync.\r\n\r\n- And what's the story about `numpy.angle` vs. `tf.arg`. If this is a new op, should we call it `angle`? @aselle ?", "@asimshankar It would be vastly slower to implement this as a composite op, and might not even handle corner cases correctly. Since Eigen now has an arg functor, we should use it. I'm OK with deferring the GPU versions until the issues have been resolved.", "@martinwicke can you put this in the API review queue, please?", "@lakshayg  As @asimshankar  suggests, we should call the op angle for Numpy compatibility. \r\n@aselle do you agree?", "@rmlarsen Just renamed arg to angle. Please review.", "(Looks good for API change)", "@asimshankar Thanks!"]}, {"number": 10642, "title": "[1.1.0-gpu image] Can't open shared object file libcuda.so.1", "body": "#### Version info\r\nGPU: Nvidia K40 and K80\r\nDocker : 1.12.6\r\nImage tag: 1.1.0-gpu and latest-gpu\r\n#### Reproduce\r\nI pulled the [tensorflow/tensorflow:1.1.0-gpu](https://hub.docker.com/r/tensorflow/tensorflow/tags/) docker image and I got the error as below when I started to run it:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```", "comments": ["Are you using nvidia-docker? e.g. https://hub.docker.com/r/tensorflow/tensorflow/\r\n", "No, I'm running on Kubernetes 1.6.4, and followed this [guide](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/) to schedule GPUs. I installed CUDA 8.0 and copied all shared object files about Nvidia GPU driver into `/usr/local/nvidia/lib64`. Then, I got new error and the reason is seems like the container needs `/dev/nvidia-uvm`. I'm not sure whether it's an issue on TensorFlow image or Kubernetes.\r\n```\r\n tf.global_variables_initializer().run()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1552, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3776, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no devi\r\nces matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n```", "Is there a libcuda.so.1 in your /usr/lib/nvidia-367 directory? You might have to mess with ldconfig.\r\nI'm not sure how you reached the conclusion that you need nvidia-uvm, but that should be taken care of by the kubelet if you use the gpu resource.", "libcuda.so.1 is in my `/usr/local/nvidia/lib64`, I attached the ldconfig of `_pywrap_tensorflow_internal.so` as below:\r\n```\r\n        linux-vdso.so.1 =>  (0x00007fffdd3fa000)\r\n        libcublas.so.8.0 => /usr/local/nvidia/lib64/libcublas.so.8.0 (0x00007fca1a1d9000)\r\n        libcuda.so.1 => /usr/local/nvidia/lib64/libcuda.so.1 (0x00007fca197f5000)\r\n        libcudnn.so.5 => /usr/local/nvidia/lib64/libcudnn.so.5 (0x00007fca14a20000)\r\n        libcufft.so.8.0 => /usr/local/nvidia/lib64/libcufft.so.8.0 (0x00007fca0bbd2000)\r\n        libcurand.so.8.0 => /usr/local/nvidia/lib64/libcurand.so.8.0 (0x00007fca07c69000)\r\n        libcudart.so.8.0 => /usr/local/nvidia/lib64/libcudart.so.8.0 (0x00007fca07a02000)\r\n        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fca077f4000)\r\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fca075d7000)\r\n        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fca073d2000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fca070c9000)\r\n        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fca06d47000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fca06b30000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fca06767000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007fca292fa000)\r\n        libnvidia-fatbinaryloader.so.367.48 => /usr/local/nvidia/lib64/libnvidia-fatbinaryloader.so.367.48 (0x00007fca06519000)\r\n```", "Also, the usual gpu thing on docker or raw metal is to run nvidia-smi to see if the gpu is visible. I'd try that too (if it applies to kubernetes).\r\n", "We are facing the same issue of\r\n`ImportError: libcuda.so.1: cannot open shared object file: No such file or directory`\r\nwhen upgrading from TF 1.0.0 to 1.1.0 seems others have the issue as well -\r\n https://github.com/tensorflow/tensorflow/issues/9969  \r\nSeems this was failing silently in TF1.0.0 - https://github.com/tensorflow/tensorflow/issues/10073\r\nwe are using https://gitlab.com/nvidia/cuda/blob/ubuntu16.04/8.0/runtime/Dockerfile no issues while using TF1.0.0 , Please advise ", "Is this a version mismatch? Do the libraries on your disk match those within the containers?", "@aselle Since nvidia-smi is in `/usr/bin`, I don't think it's a good way to mount `/usr/bin` into container. Do you have some advice to map nvidia-smi into container? \r\n@cmluciano My host OS is redhat7 with 3.10.0l-514.21.1.e17.x86_64 kernel, and TensorFlow 1.1.0 image is based on Ubuntu 16.04. Do you think there is a version mismatch of Nvidia drivier of them?", "It is possible. Can you check the versions of the cuda and or nvidia-driver packages?", "Is there a libcuda.so in the Docker image? I have seen cases where the image has a libcuda.so from a CUDA installation, but the Kubernetes node had a mix of libraries from the Linux driver installation (e.g. 367.48 vs 367.57 or 375.66).", "Actually, I see that there's a libnvidia-fatbinaryloader.so.367.48 in the ldd output. What does the libcuda.so.1 symlink point to? Perhaps _pywrap_tensorflow_internal.so has been linked against 367.48 and you need libcuda.so.1 to really point to libcuda.so.367.48. (Why does it have to link against libnvidia-fatbinaryloader.so in the first place?)", "This looks like the same as https://github.com/tensorflow/tensorflow/issues/9071", "I'd install the cuda toolkit on the image as #9071 recommends. that would give you access to nvidia-smi. Anyways, my point is that you should verify CUDA GPU is working independent of TensorFlow first.", "That sounds like asking for trouble. You want the nvidia-smi binary and kernel driver to match. So that means that the binary should live outside of the image entirely and should live in a directory on the Kubernetes node that is kept in sync with the driver by the cluster administrator. Then the pod mounts the host directory as e.g. /usr/local/bin or /opt/bin. That's what we've been doing for the past year, going through 4-5 versions of the Nvidia drivers and never had issues (save one with nvprof, where we had erroneously included a few libraries in the image). It sucks that you have to add a couple of volumes and mounts to all your GPU YAMLs, but I have filed a feature request for Kubernetes to extend PodPreset so that you can say, e.g. \"when someone asks for GPU resources, always inject these volumes and mountpoints in the pod\".", "Fair enough. The real question is whether or not the GPU install is working or not or if it is a Tensorflow bug. Using some other GPU program like nvidia-smi that is simpler is a common way to separate the problem. LD_DEBUG is a great debugging tool for seeing why shared libraries aren't loading\r\n```sh\r\nLD_DEBUG=libs python -c \"import tensorflow\"\r\n```\r\npipe it to a file and find where it is trying to load libcuda and it will tell you exactly what locations its trying for which libraries.\r\n\r\nMaybe google for others that have gotten tenosrflow on kubernetes. \r\nhttps://medium.com/jim-fleming/running-tensorflow-on-kubernetes-ca00d0e67539", "@cmluciano Cuda version is 8.0.44, and the nvidia driver version is 375.66.  I uninstalled the nvidia driver When I realize CUDA includes Nvidia driver actually.\r\n@therc Both `libcuda.so.1` and `libnvidia-fatbinaryloader.so.367.48` are copied from `/usr/lib64` to `/usr/local/nvidia/lib64` in host, and mounted into container later. The same as other shared object files about Nvidia driver.\r\nAfter uninstalled the nvidia driver, the nvidia-smi shows that the nvidia driver, a part of CUDA, version is 367.48. What's more, I can locate the `nvidia-uvm` and `nvidia0` in the container. \r\n```\r\n$ nvidia-smi\r\nMon Jun 19 10:04:35 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K40m          Off  | 0000:04:00.0     Off |                    0 |\r\n| N/A   45C    P0    71W / 235W |      0MiB / 11439MiB |     97%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nWhen I run the `deviceQuery` in the container, a cuda sample I built in host, I got the `Permission denied` error. \r\n```\r\n# ./deviceQuery\r\n/bin/sh: 8: ./deviceQuery: Permission denied\r\n```\r\n But if run it outside the container, everything goes well. (The container run as root user too)\r\n```\r\n# ./deviceQuery\r\n./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"Tesla K40m\"\r\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\r\n  CUDA Capability Major/Minor version number:    3.5\r\n  Total amount of global memory:                 11440 MBytes (11995578368 bytes)\r\n  (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores\r\n  GPU Max Clock rate:                            745 MHz (0.75 GHz)\r\n  Memory Clock rate:                             3004 Mhz\r\n  Memory Bus Width:                              384-bit\r\n  L2 Cache Size:                                 1572864 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     No\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Enabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = Tesla K40m\r\nResult = PASS\r\n```\r\nIt seems like the GPU is running well in host, but I'm not sure the `permission denied` error is caused by TensorFlow image or kubernetes. What I can ensure is that kubelet detected the GPU. When I run `kubectl describe node`, I got the GPU resource.\r\n```\r\n$kubectl describe node\r\n...\r\nCapacity:\r\n alpha.kubernetes.io/nvidia-gpu:        1\r\n cpu:                                   48\r\n memory:                                395679780Ki\r\n pods:                                  110\r\nAllocatable:\r\n alpha.kubernetes.io/nvidia-gpu:        1\r\n cpu:                                   48\r\n memory:                                395577380Ki\r\n pods:                                  110\r\n...\r\n```\r\n\r\n@aselle I agreed with @therc , if install the cuda toolkit into container, it would cause a mismatch problem between nvidia-smi binary and kernel driver.", "Hi guys, although I can't run `deviceQuery` in container, but it doesn't matter. I reallocate the device to GPU in mnist_softmax, and it runs well. I think the key point is copying shared object files about nvidia, installed by CUDA, into `/usr/local/nvidia/lib64` and mount them into container. **Don't install nvidia driver individually, just install CUDA which includes a matched nvidia driver**.\r\nClose the issue~", "PS: If the `nvidia-uvm` is not found in `/dev/`, you need to run the NVIDIA_SAMPLE such as `deviceQuery` to load the `nvidia-uvm`. I think it's a problem of Nvidia.", "@DjangoPeng so have you solved with kubernetes? Have you found the minimum set of nvidia files to share from node to pod?", "Make sure to run with `nvidia-docker`", "may be nvidia-docker-plugin is not running, try:\r\nsudo -b nohup nvidia-docker-plugin > /tmp/nvidia-docker.log", "> Make sure to run with `nvidia-docker`\r\n\r\nsolved it for me\r\n", "> https://hub.docker.com/r/tensorflow/tensorflow/\r\n\r\n\r\n\r\n> Are you using nvidia-docker? e.g. https://hub.docker.com/r/tensorflow/tensorflow/\r\n\r\n```\r\ndocker run --runtime=nvidia -p xxxxxxx -t tensorflow/serving:1.12.0-gpu \r\n```", "Yes, just append --runtime=nvidia to your docker command. The reason is nvidia-docker v1 uses the nvidia-docker alias, where v2 uses docker --runtime=nvidia."]}, {"number": 10641, "title": "bug: BeamSearchDecoder should not assume that  when time > 0 beam will be full", "body": "```\r\n  scores_flat = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: array_ops.reshape(scores, [batch_size, -1]),\r\n      lambda: scores[:, 0])\r\n  num_available_beam = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: math_ops.reduce_prod(scores_shape[1:]),\r\n      lambda: math_ops.reduce_prod(scores_shape[2:]))\r\n\r\n  # Pick the next beams according to the specified successors function\r\n  next_beam_size = math_ops.minimum(\r\n      ops.convert_to_tensor(\r\n          beam_width, dtype=dtypes.int32, name=\"beam_width\"),\r\n      num_available_beam)\r\n  next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size)\r\n  next_beam_scores.set_shape([static_batch_size, beam_width])\r\n  word_indices.set_shape([static_batch_size, beam_width])\r\n``` \r\ncode start from\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L510\r\n\r\nCorrect me if I am wrong, but I think this code is assuming that, when time > 0 the beam will be full. It is true when the vocabulary is big such as is the case in machine translation. but if the vocabulary is small, the beam might won't be full when time > 0 and might pose a problem.  the value of `next_beam_size ` in the code seems must be `beam_width` or it will raise an error since `next_beam_scores.set_shape([static_batch_size, beam_width])`, which make ` next_beam_size = math_ops.minimum` useless.\r\n\r\nI am trying to write a Pointer Network BeamSearch Decoder by modifying this source file. And the vocabulary is usually small, so there is a possibility that when time == 1 the beam won't be fully filled. \r\n\r\nI appreciate finally some one wrote a general BeamSeach decoder, that will make my life easier.\r\n\r\n", "comments": ["@ebrevdo, could you please take a look, it's beyond my expertise.", "This is indeed the assumption.  Unfortunately it's unlikely to be fixed\nuntil after the 1.2 release and I'm away this week.  If you would like to\nsend a PR to fix this by using the tf.shape() on the input Tensor instead\nof assuming beam_width, and adding a unit test, I can review when I return.\n\nOn Jun 13, 2017 10:28 AM, \"Andrew Selle\" <notifications@github.com> wrote:\n\n@ebrevdo <https://github.com/ebrevdo>, could you please take a look, it's\nbeyond my expertise.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/10641#issuecomment-308189704>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABtim6kdt62nhk_I7Ie5cDr45ccAy5MRks5sDsbEgaJpZM4N2l0D>\n.\n", "```\r\ndef initialize(self, name=None):\r\n    \"\"\"Initialize the decoder.\r\n\r\n    Args:\r\n      name: Name scope for any created operations.\r\n\r\n    Returns:\r\n      `(finished, start_inputs, initial_state)`.\r\n    \"\"\"\r\n    finished, start_inputs = self._finished, self._start_inputs\r\n\r\n    log_prob_mask = array_ops.one_hot(                          # shape(batch_sz, beam_sz)\r\n        array_ops.ones([self._batch_size], dtype=dtypes.int32),\r\n        depth=self._beam_width, dtype=dtypes.bool)\r\n\r\n    log_prob_zeros = array_ops.zeros([self._batch_size, self._beam_width],  # shape(batch_sz, beam_sz)\r\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype)\r\n    log_prob_neg_inf = array_ops.ones([self._batch_size, self._beam_width],  #shape(batch_sz, beam_sz)\r\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype) * -float('inf')\r\n\r\n    log_probs = array_ops.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)\r\n\r\n    initial_state = BeamSearchDecoderState(\r\n        cell_state=self._initial_cell_state,\r\n        log_probs=log_probs,\r\n        finished=finished,\r\n        lengths=array_ops.zeros(\r\n            [self._batch_size, self._beam_width], dtype=dtypes.int32))\r\n\r\n    return (finished, start_inputs, initial_state)\r\n```\r\n@ebrevdo \r\nIt probably is not  a good idea to push tensors with variant shape to TensorArray.\r\nActually I think it's a good idea to just set `log_probs[:, 1:weight_width]` to negative infinity in initialize function. \r\nand of course set\r\n```\r\n  scores_flat = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: array_ops.reshape(scores, [batch_size, -1]),\r\n      lambda: scores[:, 0])\r\n  num_available_beam = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: math_ops.reduce_prod(scores_shape[1:]),\r\n      lambda: math_ops.reduce_prod(scores_shape[2:]))\r\n\r\n  # Pick the next beams according to the specified successors function\r\n  next_beam_size = math_ops.minimum(\r\n      ops.convert_to_tensor(\r\n          beam_width, dtype=dtypes.int32, name=\"beam_width\"),\r\n      num_available_beam)\r\n```\r\nto a  simple reshape\r\n```\r\n  scores_flat = array_ops.reshape(scores, [batch_size, -1])\r\n```\r\n\r\n I will add a test unit and test it if you think it's ok.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10640, "title": "Estimator missing `tf.convert_to_tensor`", "body": "#10623 shows, there is `tf.convert_to_tensor` missing somewhere.\r\n@magixsno Could you provide the whole stacktrace, please?", "comments": ["I no longer have access to the machine I was developing on, but I'll get you the stack trace as soon as I get my home machine set up.", "@magixsno Any updates on this?", "When I set up on my local machine, I have been unable to reproduce, unfortunately. ", "@magixsno \r\nDid you checkout the old commit?", "Just noticed, they renamed the file and changed the lines of the matter: [estimator.md](https://github.com/tensorflow/tensorflow/commit/5479240a00e904b0f05fa4ae3760cb4deedb864c)", "Community resolved."]}, {"number": 10639, "title": "Use correct sort of dtype", "body": "This completes #10623", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10638, "title": "Might be a bug for contrib.legacy_seq2seq", "body": "Hi, \r\n\r\nI am using the embedding_attention_seq2seq with output_projection. The document from https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_seq2seq/embedding_rnn_decoder says,\r\n> outputs: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x num_decoder_symbols] containing the generated outputs.\r\n\r\nBut the output seems to be the output before projection when I used it. So I go through the source code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py , \r\n\r\nembedding_attention_seq2seq is base on the embedding_attention_decoder and attention_decoder. It has to give output_size to attention_decoder, but output_size is set to None when output_projection is not None.\r\n```python\r\ndef embedding_attention_seq2seq(encoder_inputs,\r\n                                decoder_inputs,\r\n                                cell,\r\n                                num_encoder_symbols,\r\n                                num_decoder_symbols,\r\n                                embedding_size,\r\n                                num_heads=1,\r\n                                output_projection=None,\r\n                                feed_previous=False,\r\n                                dtype=None,\r\n                                scope=None,\r\n                                initial_state_attention=False):\r\n    ... # skip\r\n    output_size = None\r\n    if output_projection is None:\r\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n    if isinstance(feed_previous, bool):\r\n      return embedding_attention_decoder(\r\n          decoder_inputs,\r\n          encoder_state,\r\n          attention_states,\r\n          cell,\r\n          num_decoder_symbols,\r\n          embedding_size,\r\n          num_heads=num_heads,\r\n          output_size=output_size,\r\n          output_projection=output_projection,\r\n          feed_previous=feed_previous,\r\n          initial_state_attention=initial_state_attention)\r\n```\r\n\r\nWhen output_size is None, the output_size is simply the cell's output_size. And so the shape of output for embedding_attention_seq2seq will be [batch_size x cell's output_size]  rather than [batch_size x num_decoder_symbols]\r\n```python\r\ndef attention_decoder(decoder_inputs,\r\n                      initial_state,\r\n                      attention_states,\r\n                      cell,\r\n                      output_size=None,\r\n                      num_heads=1,\r\n                      loop_function=None,\r\n                      dtype=None,\r\n                      scope=None,\r\n                      initial_state_attention=False):\r\n  ... # skip\r\n  if output_size is None:\r\n    output_size = cell.output_size\r\n  ... # skip\r\n      with variable_scope.variable_scope(\"AttnOutputProjection\"):\r\n        output = linear([cell_output] + attns, output_size, True)\r\n      if loop_function is not None:\r\n        prev = output\r\n      outputs.append(output)\r\n\r\n  return outputs, state\r\n```\r\n\r\n\r\nThanks.\r\n\r\n\r\n", "comments": ["Please take a look @ebrevdo. Thanks!", "+1 I am observing this same behavior in embedding_rnn_decoder -- are there any updates or workarounds here? Is there a way to get the output_projection weights and multiple them?\r\n\r\nMy current workaround is to create a dense later that takes in outputs and converts to the num_decoder_size, but this feels inaccurate.", "I'm not sure what you want to achieve: when `output_projection` is given, you are passing the variables to the dense layer yourself, right? In that case, `num_decoder__symbols` is ignored, as I think we specify. Could you explain the problem?", "So I am actually trying to point out the opposite.  When output_projection = None, then num_decoder_symbols should not be ignored and the outputs should be a list of outputs with shape [batch_size x num_decoder_symbols], but what is actually happening is that with output_projection = None the outputs are coming out as [batch_size x cell.output_size] so I am actually being forced to pass the variables to the dense layer myself.  I would like to not have to use output_projection if it can be avoided, but it seems there is no way to avoid it at the moment. ", "I can hardly believe this is ignored, it's in the code above:\r\n\r\n```\r\nif output_projection is None:\r\n      cell = core_rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n```\r\n\r\nSo if `output_projection` is None, then we wrap the decoder cell and make sure it outputs `num_decoder_symbols`. How comes it doesn't work for you?", "I am actually using [embedding_rnn_decoder] (https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L233) (sorry if that caused confusion) so I don't see where num_symbols is passed into the cell?  The line you mentioned is in the embedding_rnn_seq2seq, but the functionality is not replicated in the embedding_rnn_decoder. I think that if the code snippet you mentioned was added before cell was passed then the issue I am seeing would be solved?", "At the very least, the documentation is misleading for embedding_rnn_decoder as it suggests that if output_projection is None that the output projection layer will be wrapped automatically.", "Yes - the functionality isn't replicated in `embedding_rnn_decoder` because you can just wrap the cell before passing it there. It's also not claimed anywhere that `embedding_rnn_decoder` does this, it doesn't even have a parameter `num_decoder_symbols`. So this is solved, I'm still unclear about the original issue, which was clearly with output_projection.", "Maybe we are reading different documentation, because it is claimed that embedding_rnn_decoder does this.\r\n\r\nembedding_rnn_decoder has a parameter [num_symbols](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L236) which is the same as num_decoder_symbols and hte [documentation](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L273) says \"It is of shape [batch_size x num_decoder_symbols] when output_projection is None.\" so at the very least it seems that this documentation is in fact unclear.  Would you prefer if I open up a separate issue for this?"]}, {"number": 10637, "title": "Add link that helps explain feed_dict parameter", "body": "The current tutorial mentions using feed_dict parameter, but there is not further mention.  The example code does not explain where feed_dict is used.  By linking to the run documentation, the reader can see it is the second parameter to the run command.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10636, "title": "Non-determinism Docs (#2732)", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/2732. Wrote a short tutorial on non-determinism in TensorFlow due to GPU reductions.", "comments": ["Can one of the admins verify this patch?", "This looks fine to me, but I will pass to @zheng-xq in case there is a different best practice to ensure determinism when it is required.", "Erich is working on a design to enable better deterministic support. Adding him to make sure the language is consistent with the direction we are taking.", "@ekelsen any updates?", "Reductions will soon be deterministic on the GPU without performance loss.  For some types (float16, complex64, complex128, bool) the performance will increase by 30-1000x.", "This is an awesome doc and thank you for taking the time to write it.  The non-deterministic nature of the GPU reductions has been confusing people for a long time.\r\n\r\nHowever, I'm not sure that it makes sense to pull this in for the brief period of time before the new reductions go in.", "Sounds awesome, deterministic reductions with no performance loss. If determinism is guaranteed from then on, then perhaps it doesn't make sense to pull this in for the brief period.\r\n\r\nIt might make sense to still pull this in and add more explanation on how your side has made it deterministic with no performance loss, for those who intend to understand it.\r\n\r\nFeel free to close this PR if it's the best option at this point.", "@ekelsen I am closing this PR since it's going to be obsolete pretty soon. Feel free to re-open if you think it's worth having in the meantime.\r\n@tfboyd just FYI\r\n@jkschin nice doc! I wish we had earlier :-)", "@drpngx cheers! Shall look for the next PR to do :)", "@ekelsen Hi, do you maybe have an update for when the deterministic functionality will be available? Is it something implemented in TF itself or a feature of CUDNN? Thank you", "Reductions are deterministic now. l2loss is deterministic.  softmax will be shortly.  cross entropy sometime soon.", "The backward pass of the convolutions is not deterministic, but this basically NVIDIA's fault.  There is possibly to make it deterministic, but it is 3x slower.", "For up-to-date status of work on making TensorFlow operate deterministically on GPUs, please see the following repo: https://github.com/NVIDIA/tensorflow-determinism."]}, {"number": 10635, "title": "fix typos", "body": "some spelling mistakes", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10634, "title": "Cannot find Eigen source code", "body": "I got confused by the directory structure after I compiled the code.\r\nThere are some folders starting with `bazel-`, so what are they?\r\nI want to find where is the source code for Eigen library (e.g., implementation for `extract_image_patches` function) and I want to change some lines inside.\r\nCurrently I only found them under `bazel-tensorflow/external/eigen_archive/unsupported/Eigen/CXX11`, but `bazel-tensorflow` is a soft link and it points to a tmp dir on my mac where I found there's a README file says \r\n\r\n> This directory was generated by Blaze.\r\nDo not attempt to modify or delete any files in this directory.\r\n\r\nSo, where is the original source code for Eigen so I can modify it?\r\n\r\nI guess this question is better to be asked on stackoverflow but I tried there is no reply.\r\nI hope I won't waste too much of your time. It's a very quick question I suppose :)", "comments": ["I guess the source code for eigen is automatically downloaded by bazel and not kept in the tensorflow repository. Here is the line you might want to see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L149. You can try by modifying the URL and pointing it to your fork of eigen.", "@PKUEcho, does @lakshayg's answer solve your problem?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 10633, "title": "How can I code this android imagery app from scratch? ", "body": "How can I code this android imagery app from scratch? ", "comments": ["The Android camera demo in tensorflow/examples/android may be relevant as an example implementation for you.\r\n\r\nFor further assistance please refer your questions to Stack Overflow as this is neither a bug nor a feature request."]}, {"number": 10632, "title": "InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_9' with dtype float", "body": "Hi, I'm wondering what is causing this problem seems to be persistent. The data I'm feeding into the graph is indeed float\r\n\r\n```\r\nWARNING:tensorflow:From <ipython-input-98-ea92c0cb6f5e>:18: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\n[2017-06-11 20:48:41,474] From <ipython-input-98-ea92c0cb6f5e>:18: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\n0\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-99-2436fc2ab63a> in <module>()\r\n      1 if __name__ == '__main__':\r\n----> 2     main()\r\n\r\n<ipython-input-92-503a02b9fa94> in main()\r\n     19                 state, action, next_state, reward, done, portfolio, portfolio_value = env_stage_data(agent, step, episode_data, portfolio, portfolio_value, True)\r\n     20                 total_reward += reward\r\n---> 21                 agent.perceive(state,action,reward,next_state,done)\r\n     22                 if done:\r\n     23                     break\r\n\r\n<ipython-input-98-ea92c0cb6f5e> in perceive(self, state, action, reward, next_state, done)\r\n     76 \r\n     77         if len(self.replay_buffer) > 2000:\r\n---> 78             self.train_Q_network()\r\n     79 \r\n     80     def train_Q_network(self):\r\n\r\n<ipython-input-98-ea92c0cb6f5e> in train_Q_network(self)\r\n    105         summary_str = self.session.run(merged_summary_op,feed_dict={\r\n    106                 self.y_input : y_batch,\r\n--> 107                 self.action_input : action_batch,\r\n    108                 #self.state_input : state_batch\r\n    109                 })    \r\n\r\n/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    776     try:\r\n    777       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 778                          run_metadata_ptr)\r\n    779       if run_metadata:\r\n    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    980     if final_fetches or final_targets:\r\n    981       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 982                              feed_dict_string, options, run_metadata)\r\n    983     else:\r\n    984       results = []\r\n\r\n/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1030     if handle is None:\r\n   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1032                            target_list, options, run_metadata)\r\n   1033     else:\r\n   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1050         except KeyError:\r\n   1051           pass\r\n-> 1052       raise type(e)(node_def, op, message)\r\n   1053 \r\n   1054   def _extend_graph(self):\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_9' with dtype float\r\n\t [[Node: Placeholder_9 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'Placeholder_9', defined at:\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-75-2436fc2ab63a>\", line 2, in <module>\r\n    main()\r\n  File \"<ipython-input-74-503a02b9fa94>\", line 5, in main\r\n    agent = DQN(data_dictionary)\r\n  File \"<ipython-input-73-76942abbcce5>\", line 13, in __init__\r\n    self.create_Q_network(data_dictionary)\r\n  File \"<ipython-input-73-76942abbcce5>\", line 45, in create_Q_network\r\n    self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1507, in placeholder\r\n    name=name)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1997, in _placeholder\r\n    name=name)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/jiewwantan/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_9' with dtype float\r\n\t [[Node: Placeholder_9 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```", "comments": ["Are you sure you are feeding a value into the placeholder? I get this error when I don't feed a value into the placeholder at all. Here is an example which generates an error message similar to yours.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.placeholder(tf.float32)\r\nb = tf.placeholder(tf.float32)\r\nc = a + b\r\n\r\nwith tf.Session() as sess:\r\n    print(c.eval(feed_dict={a:1.0})) # I did not feed any value to\r\n                                     # `b` which is required for\r\n                                     # evaluating `c`\r\n```", "Thanks, lakshayg for sharing an example.  I recheck and think I do feed the relevant values into the placeholders, I'm not very sure though. Below is my graph and 2 feed_dict. For the same placeholders, _self.optimizer.run_ feed_dict works well. But _not _summary_str = self.session.run(merged_summary_op__\r\n\r\nHere is a snapshot of my codes: \r\n\r\n```\r\ndef get_initial_data():\r\n    data_dictionary = {}\r\n    data_dictionary[\"input\"] = len(x_train[0][0]) + 1 \r\n    data_dictionary[\"action\"] = 3 \r\n    data_dictionary[\"hidden_layer_1_size\"] = 40\r\n    data_dictionary[\"hidden_layer_2_size\"] = 20 \r\n    data_dictionary[\"x_train\"] = x_train\r\n    data_dictionary[\"x_test\"] = x_test\r\n    data_dictionary[\"y_test\"] = y_test\r\n    data_dictionary[\"y_train\"] = y_train\r\n    return data_dictionary\r\n\r\nclass DQN():\r\n    # DQN Agent\r\n    def __init__(self, data_dictionary):\r\n        self.replay_buffer = deque()\r\n        self.time_step = 0\r\n        self.epsilon = INITIAL_EPSILON #1\r\n        self.state_dim = data_dictionary[\"input\"] #21\r\n        self.action_dim = data_dictionary[\"action\"] #3\r\n        self.create_Q_network(data_dictionary)\r\n        self.create_training_method()\r\n\r\n        # Init session\r\n        self.session = tf.InteractiveSession()\r\n        self.session.run(tf.initialize_all_variables())\r\n        global summary_writer\r\n        summary_writer = tf.summary.FileWriter('logs',graph=self.session.graph)\r\n\r\n  def create_Q_network(self, data_dictionary):       \r\n        # network weights \r\n        #[21,40]\r\n        W1 = self.weight_variable([self.state_dim,data_dictionary[\"hidden_layer_1_size\"]])\r\n        variable_summaries(W1, \"layer1/weights\")\r\n        # [40]\r\n        b1 = self.bias_variable([data_dictionary[\"hidden_layer_1_size\"]])\r\n        variable_summaries(b1, \"layer1/bias\")\r\n        # [21,3]\r\n        W2 = self.weight_variable([data_dictionary[\"hidden_layer_1_size\"],self.action_dim])\r\n        variable_summaries(W2, \"layer2/weights\")\r\n        # [3]\r\n        b2 = self.bias_variable([self.action_dim])\r\n        variable_summaries(b2, \"layer2/bias\")\r\n        #tf.summary.scalar(\"second_layer_bias_scaler\", b2)\r\n        self.b2 = b2\r\n        \r\n        # input layer\r\n        # [None,21]\r\n        self.state_input = tf.placeholder(tf.float32,[None,self.state_dim])\r\n        # hidden layers\r\n        h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\r\n        # Q Value layer\r\n        self.Q_value = tf.matmul(h_layer,W2) + b2\r\n\r\n  def create_training_method(self):\r\n        # [none,3]\r\n        self.action_input = tf.placeholder(tf.float32,[None,self.action_dim]) # one hot presentation\r\n        self.y_input = tf.placeholder(tf.float32,[None])\r\n        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\r\n        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\r\n        tf.summary.scalar(\"loss\",self.cost)\r\n        global merged_summary_op\r\n        merged_summary_op = tf.summary.merge_all()\r\n        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\r\n\r\n  def train_Q_network(self):\r\n        # Step 1: obtain random minibatch from replay memory\r\n        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\r\n        #print(\"minibatch: \",minibatch)\r\n        state_batch = [data[0] for data in minibatch]\r\n        action_batch = [data[1] for data in minibatch]\r\n        reward_batch = [data[2] for data in minibatch]\r\n        next_state_batch = [data[3] for data in minibatch]\r\n\r\n        # Step 2: calculate y\r\n        y_batch = []\r\n        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\r\n        for i in range(0,BATCH_SIZE):\r\n            done = minibatch[i][4]\r\n            if done:\r\n                y_batch.append(reward_batch[i])\r\n            else :\r\n                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\r\n\r\n        self.optimizer.run(feed_dict={\r\n            self.y_input:y_batch,\r\n            self.action_input:action_batch,\r\n            self.state_input:state_batch\r\n            })\r\n\r\n       summary_str = self.session.run(merged_summary_op,feed_dict={\r\n       self.y_input : y_batch,\r\n       self.action_input : action_batch,\r\n       self.state_input : state_batch })    \r\n       summary_writer.add_summary(summary_str,self.time_step)\r\n```", "I am not very familiar with summary writer but what I suggest is that you try to identify which of the placeholders is \"Placeholder_9\" and try feeding in some value for it.", "1. Name your placeholders... then it is clear where they are defined\r\n\r\ni.e. tf.placeholder(name=\"something I know about\")\r\n\r\n2. It tells you in the 2nd traceback where the placeholder that is unspecified:\r\n\r\n```\r\nCaused by op u'Placeholder_9', defined at:\r\n...\r\n  File \"<ipython-input-73-76942abbcce5>\", line 45, in create_Q_network\r\n    self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\r\n```\r\n\r\n3. It tells you what sess.run() call failed to have all the placeholders in the traceback too.\r\n\r\n```\r\n<ipython-input-98-ea92c0cb6f5e> in train_Q_network(self)\r\n    105         summary_str = self.session.run(merged_summary_op,feed_dict={\r\n    106                 self.y_input : y_batch,\r\n--> 107                 self.action_input : action_batch,\r\n    108                 **#self.state_input : state_batch**\r\n    109                 })    \r\n```\r\nNotice how self.state_input feed is commented out.\r\n\r\n4. This question is not a bug with TensorFlow so is more appropriate for StackOverflow. So I am closing for now.\r\n\r\n", "I have a similar problem. I want to feed a value(batch_size) and use it as shape parameter. I think I am feeding value into the placeholder.\r\n\r\n```\r\nimport tensorflow as tf\r\nclass Conn(object):\r\n    def __init__(self):\r\n      self.batch_size = tf.placeholder(tf.int32,[], name=\"batch_size\")  # `batch_size` is a scalar (0-D tensor).\r\n      with  tf.name_scope(\"Conn_func\"):\r\n        random_t = tf.Variable(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0),validate_shape=False, name=\"random_t\")\r\n        random_y = tf.Variable(tf.random_uniform([6, 4], -0.1, 0.1),name=\"random_y\")\r\n        self.k = tf.matmul(random_t, random_y)\r\nwith tf.Session() as sess:\r\n    deep = Conn()\r\n    tf.set_random_seed(1234)\r\n    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n    sess.run([deep.k], feed_dict={deep.batch_size: 3})\r\n    sess.close()\r\n```\r\n\r\n```\r\nInvalidArgumentErrorTraceback (most recent call last)\r\n<ipython-input-4-d5c3455c644d> in <module>()\r\n     10     deep = Conn()\r\n     11     tf.set_random_seed(1234)\r\n---> 12     sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n     13     sess.run([deep.k], feed_dict={deep.batch_size: 3})\r\n     14     sess.close()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    903     try:\r\n    904       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 905                          run_metadata_ptr)\r\n    906       if run_metadata:\r\n    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1138     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1139       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1140                              feed_dict_tensor, options, run_metadata)\r\n   1141     else:\r\n   1142       results = []\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1319     if handle is None:\r\n   1320       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1321                            run_metadata)\r\n   1322     else:\r\n   1323       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1338         except KeyError:\r\n   1339           pass\r\n-> 1340       raise type(e)(node_def, op, message)\r\n   1341 \r\n   1342   def _extend_graph(self):\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'batch_size' with dtype int32\r\n\t [[Node: batch_size = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op u'batch_size', defined at:\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-97b6a51f4b49>\", line 11, in <module>\r\n    deep = Conn()\r\n  File \"<ipython-input-1-97b6a51f4b49>\", line 5, in __init__\r\n    self.batch_size = tf.placeholder(tf.int32,[], name=\"batch_size\")  # `batch_size` is a scalar (0-D tensor).\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1777, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 4521, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_size' with dtype int32\r\n\t [[Node: batch_size = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```", "I solved the problem incidently using tf.stack instead of tf.Variable since I didn't understand where the error is from the traceback.\r\n\r\n```\r\nimport tensorflow as tf\r\nclass Conn(object):\r\n    def __init__(self):\r\n      self.batch_size = tf.placeholder(tf.int32,[], name=\"batch_size\")  # `batch_size` is a scalar (0-D tensor).\r\n      with  tf.name_scope(\"Conn_func\"):\r\n        #random_t = tf.Variable(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0),validate_shape=False, name=\"random_t\")\r\n        random_t = tf.stack(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0), name=\"random_t\")\r\n        random_y = tf.Variable(tf.random_uniform([6, 4], -0.1, 0.1),name=\"random_y\")\r\n        self.k = tf.matmul(random_t, random_y)\r\nwith tf.Session() as sess:\r\n    deep = Conn()\r\n    tf.set_random_seed(1234)\r\n    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n    sess.run([deep.k], feed_dict={deep.batch_size: 3})\r\n    sess.close()\r\n```", "#today,I met the same problem.it seems like a little BUG.one simple way I consider to solve this:\r\n#%% firstly,reset the graph to make the name of placeholder  same\r\ntf.reset_default_graph()\r\n#%% secondly,in the line where you define the placeholder(assume the placeholder is named as x)\r\n#by doing this you can get the name of placeholder in the graph\r\nprint(x.name)\r\n#then,before session.run(assume Placeholder:0 is the name of placeholder)\r\nx=tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\r\n#good luck!!!\r\n\r\n", "> \r\n> \r\n> I solved the problem incidently using tf.stack instead of tf.Variable since I didn't understand where the error is from the traceback.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> class Conn(object):\r\n>     def __init__(self):\r\n>       self.batch_size = tf.placeholder(tf.int32,[], name=\"batch_size\")  # `batch_size` is a scalar (0-D tensor).\r\n>       with  tf.name_scope(\"Conn_func\"):\r\n>         #random_t = tf.Variable(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0),validate_shape=False, name=\"random_t\")\r\n>         random_t = tf.stack(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0), name=\"random_t\")\r\n>         random_y = tf.Variable(tf.random_uniform([6, 4], -0.1, 0.1),name=\"random_y\")\r\n>         self.k = tf.matmul(random_t, random_y)\r\n> with tf.Session() as sess:\r\n>     deep = Conn()\r\n>     tf.set_random_seed(1234)\r\n>     sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n>     sess.run([deep.k], feed_dict={deep.batch_size: 3})\r\n>     sess.close()\r\n> ```\r\n\r\nThe `stack` solution worked for me too and indicates that this is beyond doubt a bug.", "Please restart your kernel and try again.  ", "Most likely you are creating nested graph so newly added placed holders won't get fed during evaluation, hence the error **You must feed a value for placeholder tensor 'xxx'**. In fact if you keep running the same code, the placeholder name could change, that means more placeholders are being added to the graph. The solution is simple, use `tf.reset_default_graph()`, add it before you start to define the computation graph.\r\n", "> Most likely you are creating nested graph so newly added placed holders won't get fed during evaluation, hence the error **You must feed a value for placeholder tensor 'xxx'**. In fact if you keep running the same code, the placeholder name could change, that means more placeholders are being added to the graph. The solution is simple, use `tf.reset_default_graph()`, add it before you start to define the computation graph.\r\n\r\nHi, I add `tf.reset_default_graph()' before creating the graph, but it said\r\n'Tensor(\"ConvNet/conv2d/kernel:0\", shape=(5, 5, 3, 32), dtype=float32_ref) must be from the same graph as Tensor(\"x:0\", shape=(?, 128, 128, 3), dtype=float32).'\r\n", "I met the same problem,and I tried all the above methods but did not solve them.the next_batch extracts 50 samples of the data set. The data set is 800*256*256*3 image data. I have changed the data type of the data set.\r\n\r\nx = tf.placeholder(tf.float32,[None,256,256,3],name = 'X')\r\nbatch_xs, batch_ys = next_batch(50)\r\nsession.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\r\n\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_6' with dtype float\r\n[[node Placeholder_6 (defined at <ipython-input-22-2f4d8ee04e25>:4) ]]", "> I solved the problem incidently using tf.stack instead of tf.Variable since I didn't understand where the error is from the traceback.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> class Conn(object):\r\n>     def __init__(self):\r\n>       self.batch_size = tf.placeholder(tf.int32,[], name=\"batch_size\")  # `batch_size` is a scalar (0-D tensor).\r\n>       with  tf.name_scope(\"Conn_func\"):\r\n>         #random_t = tf.Variable(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0),validate_shape=False, name=\"random_t\")\r\n>         random_t = tf.stack(tf.random_uniform([10,2*self.batch_size], -1.0, 1.0), name=\"random_t\")\r\n>         random_y = tf.Variable(tf.random_uniform([6, 4], -0.1, 0.1),name=\"random_y\")\r\n>         self.k = tf.matmul(random_t, random_y)\r\n> with tf.Session() as sess:\r\n>     deep = Conn()\r\n>     tf.set_random_seed(1234)\r\n>     sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\r\n>     sess.run([deep.k], feed_dict={deep.batch_size: 3})\r\n>     sess.close()\r\n> ```\r\n\r\nThe stack solution removes the error. However, it's not useful when you need a trainable variable which is the issue in my case. Would love to hear how others have solved this.. "]}, {"number": 10631, "title": "i get an error when i build tensorflow by bazel on windows", "body": "PS C:\\WINDOWS\\system32> cd tensorflow\r\nPS C:\\WINDOWS\\system32\\tensorflow> bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --cop\r\nt=-msse4.2 --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-msse4.1 //tensorflow/tools/pip_package:build_pip_package\r\n>>\r\n.....................\r\nERROR: C:/windows/system32/tensorflow/tensorflow/tools/pip_package/BUILD:27:1: error loading package 'tensorflow/core':\r\nEncountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent cal\r\nl last):\r\n        File \"C:/windows/system32/tensorflow/tensorflow/workspace.bzl\", line 117\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"C:/windows/system32/tensorflow/tensorflow/workspace.bzl\", line 108, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"C:/windows/system32/tensorflow/tensorflow/workspace.bzl\", line 92, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(127) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/godw/appdata/local/t\r\nemp/_bazel_godw/nseddbsr/external/protobuf -i C:/windows/system32/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: command not found\r\n and referenced by '//tensorflow/tools/pip_package:included_headers_gather'.", "comments": ["i realy want to know ho to solve this problem", "i installed the patch tool on my meachine \uff0cbut then\uff0cget this error\r\n----------------------------------\r\nERROR: C:/windows/system32/tensorflow/tensorflow/tools/pip_package/BUILD:27:1: error loading package 'tensorflow/core':\r\nEncountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent cal\r\nl last):\r\n        File \"C:/windows/system32/tensorflow/tensorflow/workspace.bzl\", line 117\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"C:/windows/system32/tensorflow/tensorflow/workspace.bzl\", line 108, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"C:/windows/system32/tensorflow/tensorflow/workspace.bzl\", line 92, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(2) when executing 'C:\\tools\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/godw/appdata/local/tem\r\np/_bazel_godw/nseddbsr/external/protobuf -i C:/windows/system32/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: C:\\Program Files (x86)\\GnuWin32\\bin\\patch.exe: **** Can't open patch file C:/windows/system32/tensorflow/third_p\r\narty/protobuf/add_noinlines.patch : No such file or directory\r\n and referenced by '//tensorflow/tools/pip_package:included_headers_gather'.\r\n\r\n-------------------\r\nbut the file is in that directory........\r\n", "Perhaps try cmake? Be sure to read everything here.\r\nhttps://www.tensorflow.org/install/install_sources\r\nUnfortunately building from source (particularly on Windows) is quite difficult and not recommended unless you are comfortable diagnosing the kind of errors that come from these tools.\r\n", "I think i had a similar issue. Mysys is missing the patch command. You need to install it.\r\nI think i did this with the command `pacman -Syuu patch` in the command line.\r\n\r\nMaybe it also helps to follow some of these steps: [Bazel build with Windows](https://bazel.build/versions/master/docs/windows.html)\r\n\r\nSadly i can't build tensorflow on windows too... i just can't get it to work in anyway. :( I'm totally stuck but i really would need this to work.", "I just had the same error compiling on Windows 10 with Cuda 10.0, the latest release of msys64, and the latest release of bazel. Just as @Spenhouet said, `patch` can't be found because msys doesn't have it by default for some reason.", "Hi joserotiz3 how did you solved it?", "patch must be added after msys2 is installed.  In the directions for installing MSYS2 here: https://www.tensorflow.org/install/source_windows  You can see the instructions to run the following command: pacman -S git patch unzip.  Here is a screen grab from my attempt to update msys2:  https://s3.amazonaws.com/chris-pub-share/msys2_patch.png   HOWEVER, I ran this command nearly 20 minutes ago and I am getting no confirmation that the process is running properly or complete.  ", "So... where it says to run pacman in Cmd.exe - it means to run the pacman command in Cmd.exe, not powershell (like I did above).  Running in powershell you never see a prompt to \"continue(Y/N)\" you do see it in Cmd.exe", "yea trying to get this stuff to work on windows is absolute crap. ", "I imstalled it, and spent a good whole day finding why it didnt work in the pycharm CMD, turn out, you have to restart pycharm entirly for env vars to chnge! also mods close this one", "Closing as requested, also because it's old and patch setup has since changed"]}, {"number": 10630, "title": "fix javadoc issues", "body": "", "comments": ["Can one of the admins verify this patch?", "@asimshankar please review this", "@tensorflow-jenkins test this please", "The test failures are unrelated. Merging PR."]}, {"number": 10629, "title": "[WIP] Multi-bus support with NUMA-aware allocator", "body": "As the title suggests, this is a WIP to partially address issues brought by @poxvoculi in #5986. \r\n\r\nIn our testbed, we have a multi-bus topology where 4 K40m GPUs are connected by different PCI-e root, as shown with the following command:\r\n\r\n```\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tGPU2\tGPU3\tmlx4_0\tCPU Affinity\r\nGPU0\t X \tPHB\tSOC\tSOC\tSOC\t0-5\r\nGPU1\tPHB\t X \tSOC\tSOC\tSOC\t0-5\r\nGPU2\tSOC\tSOC\t X \tPHB\tPHB\t6-11\r\nGPU3\tSOC\tSOC\tPHB\t X \tPHB\t6-11\r\nmlx4_0\tSOC\tSOC\tPHB\tPHB\t X\r\n\r\nLegend:\r\n\r\n  X   = Self\r\n  SOC  = Connection traversing PCIe as well as the SMP link between CPU sockets(e.g. QPI)\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nCurrent TF always allocates the host memory used to transfer from/to GPU on NUMA node 0, which is sub-optimal for GPUs with other CPU affinity. This could be further validated by adjusting `CUDA_VISIBLE_DEVICES` and `numactl -m <numa_node> -N <numa_node>` prepended to TF process.\r\n\r\nThe basic idea is simple: use ``numa_alloc_onnode`` and other related functions available in [libnuma](https://linux.die.net/man/3/numa) to allocate memory for CUDA host allocator. There are some pieces of code, e.g. [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L669), [there](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L951), and [there](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc#L855) to identify bus id and numa node for each device, and I plan to reuse those as much as possible.\r\n\r\nI am wondering if anyone in the TF team could give me some advice so I can start to implement it.", "comments": ["Can one of the admins verify this patch?", "I'm out of the office this week with poor internet connectivity.  I hope\nthis can wait till next week, or someone else can review it.\n\nOn Jun 13, 2017 4:14 PM, \"Martin Wicke\" <notifications@github.com> wrote:\n\n> Assigned #10629 <https://github.com/tensorflow/tensorflow/pull/10629> to\n> @poxvoculi <https://github.com/poxvoculi>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10629#event-1122108566>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818fAsk9ItMm7VwaOAvt8Ui1j2435Kks5sDu2NgaJpZM4N2Ue2>\n> .\n>\n", "Sure. I can start researching myself in the mean time. Hope that I could make some progress before your first review.", "@poxvoculi can this be merged? I wonder if we should try this internally first to see if there are perf regressions.", "@byronyi: do you see any significant performance improvements with this change?  We experimented with per-numa-node buffer allocation a long time ago, and didn't see any significant effect on our platforms.  I suppose we should revisit the question.", "@poxvoculi This is a primer patch to a larger one that tries to implement NIC-compatible host allocator and device memory allocator for RDMA (with GPU direct). We had a similar design to current contrib/verbs implementation but we would like to avoid memory copies at all. Our initial prototype shows that the overhead of ad-hoc memory registration is noticeable, and the best solution seems to be allocating relevant tensor buffers from a RDMA capable memory pool (with memory window registration).", "I am wondering if #4185 and 699d1e8f50e5d27d9a791a44f0384fcfaf51e3af are related, as I am not familiar with code in the stream executor part.\r\n\r\n@reedwm I saw you are working (6448f41d0b0ee45fea482ee14952f006245feb56) with BFC managed CPUAllocator. Any comments on this PR?", "@byronyi \r\n\r\nIf you look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.h#L43\r\nyou'll see some indication of how these issues are handled in the Google internal version, which is specific to our hardware platform.  (Handling of networking related issues is nearly the only difference between internal and open-source versions.)   Memory is either GPU RAM or CPU RAM.  CPU RAM may optionally be gpu_registered which means that it was allocated via CUDAHostAllocator and both page locked and known to the CUDA driver as such.  gpu_registered memory is used for CPU resident tensors which are going to be transferred to/from GPU RAM via a copy operation.  nic_registered memory has been preregistered for RDMA.  GPU RAM is pre-allocated in large regions by the allocator (BFCAllocator), and when we're planning to do RDMA and GPUDirect is available, we go ahead and preregister all of those GPU memory regions with the bus-adjacent NIC.  If we plan on doing RDMA with CPU RAM as well, we preregister all memory in the pool obtained from CUDAHostAllocator with all NICs bus-adjacent to CPU RAM.\r\n\r\nThe numa-related bits that you're touching in this change were written into the code base a long time ago on the assumption that someday we'd want do numa-specific memory allocation and registration.  In fact, we don't.  The page locking and NIC registration issues are important, with significant performance gain.  But we have not been able to get any gain out of maintaining multiple numa-specific memory pools, and having multiple pools will probably incur greater memory overhead. \r\n\r\nThis lack of benefit probably has something to do with characteristics of our platforms and execution environment, but I would encourage you to address the other issues first, and only try to change the numa-0 default here when doing so realizes demonstrable benefit.  \r\n", "@poxvoculi Thanks a lot to that bit of information. It clearly points out the direction for my next step, so I won't waste too much time on NUMA-specific issues. In fact, I think your feedback confirmed many observations I had, which is worth my effort digging into the internals of memory management. At least I know I am on the right track.\r\n\r\nPersonally I found it quite interesting that GPU DMA does share a lot of similarities with RDMA, and there is plenty of GPU DMA code in the open sourced TF that could be reused. So I will just go ahead and implement it.\r\n\r\nIf you don't mind, I have two more questions: \r\n\r\n1. As we do have verbs support now, would you mind to accept another PR doing RDMA? It is designed to handle many issues in a better way, i.e. an interface that utilises out-of-band ``transport_options`` in current gRPC, connection management using `librdmacm` (I already implemented these), zero-copy of tensor buffer and no extra memory allocation (as we discussed above), and GPU direct when available. However, the performance gain may not be substantial than what we have now, which, after several patches since it's merged, is close to linear scaling on a small testbed. Additionally, it will probably touch a lot different places outside `core/distributed_runtime` and adds more code complexity.\r\n2. You mentioned there are multiple NICs bus-adjacent to different PCI-e devices in Google's internal platform, and we are also working on hardware spec that fits TF running on RoCE. I understand it might be confidential, but would it be a performance boost good enough to justify the extra NICs? I will probably consider that in my patch as well, just to deal with different pairings of GPU and NIC, or to decide if we should do GPU direct at all.\r\n\r\nThanks again for your comments!", "@byronyi 6448f41d0b0ee45fea482ee14952f006245feb56 allows a BFC allocator to be used instead of a pool allocator for the CPUAllocator, because pool allocators can run out of memory quickly if many allocs and frees are made with many different sizes. I know almost nothing about NUMA and RDMA, but since 6448f41d0b0ee45fea482ee14952f006245feb56 did not change the host allocator, I don't think it will have an affect on RDMA."]}, {"number": 10628, "title": "Merge pull request #1 from tensorflow/master", "body": "from origin", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 10627, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Test failures are unrelated. Merging PR.", "Thanks, @taehoonlee "]}, {"number": 10626, "title": "R1.1", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@williamwsf333 can you briefly explain what this PR is about? Thanks.", "I assume this PR was created in error -- I will close it. "]}, {"number": 10625, "title": "Fix typo, \"as far as *is*\" in doc", "body": "The word, \"is\" looks unnecessary in these sentences.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10624, "title": "Support multi-processing on tf.py_func", "body": "This would be a great feature to have. The tf.py_func op is very handy for certain custom operations, but when training on multiple GPUs it ends up becoming a bottleneck that slows everything down. One way to get around that is to allow tf.py_fun to run on a different process each time, and by doing so avoid the GIL bottleneck. This could be enabled only when stateful = False, to avoid unintended side effects.\r\n\r\n\r\n", "comments": ["I cann't agree more! I also try to use multi gpu in my project, but I failed because of py_func.", "Unfortunately, multiprocessing's `fork` does not play nicely with tensorflow's threads (https://github.com/tensorflow/tensorflow/issues/5448) or threads in general (http://www.linuxprogrammingblog.com/threads-and-fork-think-twice-before-using-them)", "py_func is definitely super useful for quick tests, but it is difficult to make it work well in multithreading contexts. A lot of the problem is due to Python's extremely limiting notion of threading. Nobody is actively working on this right now. @alextp may have more comments. ", "@sjperkins Thanks for the links. If I understand the discussion there correctly, in Python 3.4+ using ```multiprocessing.set_start_method('spawn') ``` gets around the ```fork``` issues. I don't know if there is a way to get the same effect on 2.7, but even if not, having it working on Python 3 is still a big win. ", "I believe you can write your own wrapper around py_func to do this, a forking_py_func decorator which calls the py_func decorator on a function wrapped by a safe fork.", "@alextp Could you explain in more detail how to wrap a forking_py_func around py_func to do multiprocessing? Thanks", "Actually on second thought the tensorflow runtime uses threads and it's\nunlikely that liberal use of multiprocessing will work here.\n\nOn Tue, Aug 8, 2017 at 10:54 PM, yj8907 <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Could you explain in more detail how\n> to wrap a forking_py_func around py_func to do multiprocessing? Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10624#issuecomment-321159590>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXud4ed051lwJN75WS6DspqSjozXks5sWUmXgaJpZM4N2R8f>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Then if I write a new op kernel in C++ and register it, will it avoid the bottleneck problem of py_func in multi gpu? I have this same problem of py_func slowing down multi-gpu speed. ", "Yes\n\nOn Wed, Aug 9, 2017 at 12:30 PM, yj8907 <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Then if I write a new op kernel in\n> C++ and register it, will it avoid the bottleneck problem of py_func in\n> multi gpu? I have this same problem of py_func slowing down multi-gpu speed.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10624#issuecomment-321357536>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYgBjfWXnxFNvrtjgQMAVdUKOLTGks5sWgjYgaJpZM4N2R8f>\n> .\n>\n\n\n\n-- \n - Alex\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "will this still move forward? Or the py_function thing should be totally ignored when pursuing performance? ", "py_function is very hard to do performantly"]}, {"number": 10623, "title": "Change np.array to tf.constant", "body": "np.array fails with no 'module' get_shape().", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks, @magixsno !"]}, {"number": 10622, "title": "RecordInput Documentation in API doc site", "body": "[RecordInput is in the tf 1.2 code](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/data_flow_ops.py#L1602) but I don't see it in the [tf 1.2 API docs](https://www.tensorflow.org/versions/r1.2/api_docs/python/). Should it be present?\r\n", "comments": ["This does seem like a documentation bug.\r\n@ekelsen  (it seems you authored it) and @wolffg could you take a look?", "Mark, is this an issue with an unsealed library?", "The opposite problem actually. It's over-sealed and the objects are not exposed.\r\n\r\nIt is removed by the `remove_undocumented` in [standard_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/standard_ops.py).\r\n\r\nIts neighbor, `RandomShuffleQueue`, is made visible by the `@@RandomShuffleQueue` in [io_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/io_ops.py).\r\n\r\nSo if we want it exposed as `tf.RecordInput` we would just need to add `@@RecordInput` there. It's probably even better to move those `@@`s across to `data_flow_ops` where these things are defined. \r\n\r\nThe same situation seems to apply to `Barrier` and `StagingArea`.", "@martinwicke can you comment on the intended API here? It look like these are meant to be public. Are they meant to go in the main `tf` namespace of is there another place for them.", "I'm pretty sure that Barrier and StagingArea are intentionally not exposed, so the same ma apply here. Can you ask the authors to make sure these are not intentionally private?", "The current state is intentional.", "Why would it be intentional? Because the C++ backend _is_ open ([source](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/record-input)), and it's been described and recommended in the high performance models guide ([source](https://www.tensorflow.org/performance/performance_models)).", "@TimZaman has a valid point", "If they are used, we should expose them through a public endpoint in contrib. We don't want to expose them directly since that would subject them to API stability guarantees, and they're not ready for that yet.", "@martinwicke There are some other TF APIs that have been both documented and clearly marked as experimental and subject to change, which I think would work well here. I agree regarding relocating to contrib as well.  How does that sounds as an approach?", "Considering recent discussion can this be reopened?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I think this is still valid for the python api version", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@tfboyd, you seem to own the relevant docs. Is the recommendation to use `RecordInput` still up to date? I have the feeling that this been replaced by `tf.data`.", "These can definitely be exposed as symbols in contrib. Please send a PR. Sorry for the endless delays. \r\n\r\nExposing in contrib is (for now) our way of making as experimental.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Who should provide this PR?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Added a PR #18223 to expose `RecordInput` symbol and docs in `tf.contrib.framework.RecordInput`."]}, {"number": 10621, "title": "when i use bazel to build i got error that  no such package '@protobuf", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["**Please try upgrading bazel**, but I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. \r\n\r\nThank you.\r\n\r\n"]}]