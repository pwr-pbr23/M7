[{"number": 1840, "title": "Fix two typos in API docs for ExponentialMovingAverage.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "(we can't look at this until the CLA comes back as signed, can you double check)\n", "Can one of the admins verify this patch?\n", "I think I know what is wrong with the CLA. I use lukaszbk+github@ as my GitHub login, while I signed the CLA as lukaszbk@.\n\nIs there a way to sign the CLA as lukaszbk+github@? Unfortunately, I cannot use lukaszbk+github@ as an alternate Gmail address in my Google Account settings.\n", "Not sure, you can give it a try!  I think you can add multiple emails to your github account (as non-primary) as well, if you felt comfortable doing that.  Basically:\n- email of commit must match CLA email\n- CLA email must be associated with your github account (somehow)\n", "OK. I verified that lukaszbk@ which I signed the CLA with is added to my GitHub account.\n", "google cla bot doesn't seem to have picked it up, sadly.  what's the commit email you used on the commit?  (why github makes this very hard to find is beyond me)\n", "I am guessing that the email used for commit was lukaszbk@users.noreply.github.com, since I created the commit in the web UI I had \"keep my email private\" setting checked. I think I'll close this pull request and try another one with a different email.\n"]}, {"number": 1839, "title": "Got \"ValueError: array must not contain infs or NaNs\" error in word2vec_basic.py.", "body": "I ran https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py, and got \"ValueError: array must not contain infs or NaNs\" error from this line \"low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\".\nThere wasn't any inf or Nan in final_embeddings[:plot_only,:].\nI think it is scipy or numpy's issue. \nIs there any suggestion for me ?\n### Environment info\n\nOperating System:OS X EI Capitan 10.11.4\nunder anaconda python 3.5\nnumpy (1.11.0)\nscipy (0.17.0)\n\nInstalled version of CUDA and cuDNN: \nDidn't install CUDA.\n\nIf installed from sources, provide the commit hash:\nde5101da4638ac469041575dadb4921ebb33eb6a\n### Steps to reproduce\n\npython3 word2vec_basic.py\n### Logs or other output that would be helpful\n\nTraceback (most recent call last):\n  File \"word2vec_basic.py\", line 237, in <module>\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n  File \"/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 866, in fit_transform\n    embedding = self._fit(X)\n  File \"/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 777, in _fit\n    skip_num_points=skip_num_points)\n  File \"/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 832, in _tsne\n    params, error, it = _gradient_descent(obj_func, params, **opt_args)\n  File \"/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\", line 387, in _gradient_descent\n    grad_norm = linalg.norm(grad)\n  File \"/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/scipy/linalg/misc.py\", line 129, in norm\n    a = np.asarray_chkfinite(a)\n  File \"/Users/zhongzyd/anaconda/envs/TensorFlow-master/lib/python3.5/site-packages/numpy-1.11.0-py3.5-macosx-10.5-x86_64.egg/numpy/lib/function_base.py\", line 1022, in asarray_chkfinite\n    \"array must not contain infs or NaNs\")\nValueError: array must not contain infs or NaNs\n", "comments": ["TensorFlow does not support 3.5 as of today. Try using 3.4 which is supported.\n", "I am facing the same issue - did you find a solution yet?\n", "No, I haven't found a solution yet. @kavasseri \n", "I was able to solve this same problem (under anaconda python 2.7) by updating numpy to 1.11.  I see that the OP already has 1.11, but is using python 3.  Maybe try either updating all of numpy/scipy/sklearn to latest or reverting to older versions to find the combination that works with python 3?\n", "just wanted to reference the related thread on the scikit-learn tracker here: https://github.com/scikit-learn/scikit-learn/issues/6665\n"]}, {"number": 1838, "title": "Let reshape guess the dim.", "body": "Let 'tf.reshape' guess the dim and use it. \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n\n(nice!)\n"]}, {"number": 1837, "title": "Changed the dim computation in a compact form. It is also very easy t\u2026", "body": "Changed the dim computation in a compact form. \n\nI think this is much easy to read and understand. I am not sure about the performance, but it's only a 3-element production, so it won't matter much.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Done!\n\nOn Sun, Apr 10, 2016 at 8:05 PM, googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> [image: :memo:] _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1837#issuecomment-207971312\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I made a better patch.\n"]}, {"number": 1836, "title": "Tensorboard can't work.", "body": "### Environment info\n\nOperating System: Mac OS X 10.11.4\n\nDidn't install CUDA.\n\nUnder anaconda.\n\nInstalled from sources, provide the commit hash:\n### Steps to reproduce\n\n1.$ git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n2.$ brew install bazel swig\n3.$ cd tensorflow/\n4.$ ./configure\nPlease specify the location of python. [Default is /anaconda/lib/python2.7]:\nDo you wish to build TensorFlow with GPU support? [y/N] n\n5.$ bazel build -c opt --define=use_fast_cpp_protos=true //tensorflow/tools/pip_package:build_pip_package\n6.$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tenso\nrflow_pkg\n7.$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\n### Try\n\n1.$ python tensorflow/tensorflow/tensorboard/tensorboard.py --logdir=path/to/log-directory\ndidn't work\n2.$ tensorboard --logdir=/path/to/log-directory\ndidn't work \n### Logs or other output that would be helpful\n\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '//anaconda/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path //anaconda/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://0.0.0.0:6006)\n", "comments": ["\"didn't work\" - What do you mean? What does TensorBoard display?\nI think this may be a duplicate of https://github.com/tensorflow/tensorflow/issues/1421 so I'm closing it. Please re-open it if it's not the same issue.\n\nAlso, can you try with the 0.8 release instead of 0.7 and let me know if you still have the issue?\n"]}, {"number": 1835, "title": "pdf version of the TF document", "body": "Recently we often can't access https://www.tensorflow.org, could you provide a pdf version of the TF document, It will be very convenient for us\n\n. \n", "comments": ["Or What is the way to lanuch doc\n", "Almost all of the content on tensorflow.org is also on github. Can you see it here? Also, if you check our the source, it is in the g3doc folder locally. \n", "g3doc folder is enough for me, thanks @martinwicke \n", "Quick-and-dirty solution. Convert all Markdown files to PDF, then join. Order maybe broken, but you get a single PDF (2856 pages, some documents missing, without images).\n\n```\n$ cd tensorflow/g3doc\n$ for i in $(find . -name \"*md\"); do pandoc -s -o $i.zzzz.pdf $i; done\n$ pdfjoin -o tfdocs.pdf $(find . -name \"*zzzz.pdf\" -printf \"%p \")\n```\n", "@miku:  If you're still listening, your command line works for some files (thanks). But other files give an error message: \"! You can't use `macro parameter character #' in math mode\"\r\nAny idea what's up?", "I have a python script to do this https://github.com/O-KAYgorodov/TF2pdf", "i think it\u2018s best to provide a pdf version of manual\uff0cweb version is  hard to use\uff0cplease\uff01", "For those of you who is in mainland China and has frequently met connection problems accessing [the TF main site](https://tensorflow.com), you could try [the mirror in China](https://tensorflow.google.cn)."]}, {"number": 1834, "title": "Add polygamma and zeta function to tensorflow", "body": "This PR adds the polygamma and zeta functions as discussed in #1741. It also implements the derivative of the digamma function. Tests are provided and can be run provided `scipy` is installed.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Overall looks good, though I am worried about the integer first argument.  It is mainly an issue because of the proto serialization and working across multiple machines; it's not always gonna be the case that if i create a float 2.0, and then it undergoes proto serialization, then gets passed to the op on a different architecture, that floor(n) == n anymore.\n\nWould you consider making a functor that adds a small epsilon (e.g. 0.01) , then floors the integer input, just before calling the Eigen tensor method?\n", "Can one of the admins verify this patch?\n", "What would be your opinion on demanding `abs(round(n) - n) < epsilon` in the eigen implementation or do you think this is TensorFlow specific because of the serialisation?\n", "i think the abs(round(n) - n) < epsilon, where epsilon is e.g. 10*machine\nepsilon is a good solution.\n\nOn Thu, Apr 14, 2016 at 4:14 PM, Till Hoffmann notifications@github.com\nwrote:\n\n> What would be your opinion on demanding abs(round(n) - n) < epsilon in\n> the eigen implementation or do you think this is TensorFlow specific\n> because of the serialisation?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1834#issuecomment-210199478\n", "I'm having second thoughts about the suggested implementation. I'm sure it works fine but I'm a bit worried about consistency: a number of functions in `SpecialFunctions.h` in eigen use equality tests. For example, `igamma` tests for equality with zero and I also test for equality with one in the implementation of the zeta function. Is there something special about the representation of one and zero in memory that we don't have to worry about serialisation issues? Would it be worthwhile adding a helper function `integer_equal_impl` for all such comparisons?\n", "Perhaps I jumped the gun.  Looks like float and double can represent\nintegers up to and including 2^{mantissa bits + 1}.  So there seems to be\nno need to worry in this case.  Even with the vagaries of\nserializing/deserializing, I don't believe that a  number with exact FP\nrepresentation will be modified.\n\nOn Fri, Apr 15, 2016 at 2:50 AM, Till Hoffmann notifications@github.com\nwrote:\n\n> I'm having second thoughts about the suggested implementation. I'm sure it\n> works fine but I'm a bit worried about consistency: a number of functions\n> in SpecialFunctions.h in eigen use equality tests. For example, igamma\n> tests for equality with zero and I also test for equality with one in the\n> implementation of the zeta function. Is there something special about the\n> representation of one and zero in memory that we don't have to worry about\n> serialisation issues? Would it be worthwhile adding a helper function\n> integer_equal_impl for all such comparisons?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1834#issuecomment-210393620\n", "Great, it seems we can just leave the implementations as they are. I have fixed the inline math. The problem with unescaped characters appears to be a [new feature for raw strings](https://solarianprogrammer.com/2011/10/16/cpp-11-raw-strings-literals-tutorial/) in C++11. E.g. the following will compile and print the expected string:\n\n```\nconst char* test = R\"doc(This is the \\zeta function.)doc\";\nstd::cout << test;\n```\n", "I think we're ready to import this, but I'll be away for a while.  Assigning to @benoitsteiner for final merge.\n", "Just wanted to follow up on this PR.\n", "@tensorflow-jenkins test this please.\n", "I was looking at the eigen implementation and saw that there may be a missing registration for the CUDA (float4 or double2) packet ops.  Can you revisit and make sure you added all the correct items?\n", "@tensorflow-jenkins : test this please.\n", "@martinwicke jenkins doesn't like me? :(\n", "@tillahoffmann ignore my comment about Eigen.  I re-checked and it looks good.  Waiting on jenkins to review.\n", "Sorry, they sneaked past my .gitignore. Should be all cleaned up now.\n", "I'll try one running tests more time.  It's possible you have to remerge and fix any merge conflicts\n", "@tensorflow-jenkins : test this please.\n", "Jenkins, test this please. And stop ignoring @ebrevdo.\n"]}, {"number": 1833, "title": "[Skflow] Move skflow to learn and make it available by default.", "body": "Builds on top of #1832 \n", "comments": ["@ilblackdragon README seems to be broken in `.md` format.\n", "Can you rebase?\n", "I merged #1829 and #1832, but conflicts.\n", "Done.\n"]}, {"number": 1832, "title": "[Skflow] Fix imports to use specific tf modules", "body": "... and added skflow to default import of contrib.\n\nBuilds on top of #1829 \n", "comments": []}, {"number": 1831, "title": "[skflow] Change batch_norm default", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1830, "title": "A way to get the last non-zero output of a variable-length RNN", "body": "Feature request: The _sequence_length_ parameter in rnn.rnn() is a convenient way to support variable length sequences. The great thing about it is that it propagates the state at an example's sequence length to the final state output. However, it doesn't similarly propagate the output at an example's sequence length. This is the feature being requested here.\n\nIf one is interested in the last output only, reading outputs[-1] returns zeros for all the examples that are shorter than the max sequence length. It would simplify things considerably if there was a way to get the output value at the example's sequence length. \n", "comments": ["you can take a look at reverse_sequence to impl a kernel op yourself -- should be pretty easy. an alternative hack for this is after you run your RNN, u call reverse_sequence and then grab the first output[0].\n", "@ebrevdo: Should we mark this contributions welcome, or are you working on it? \n", "This requires advanced slicing.  I started work on extending `gather_nd` to support this type of usecase this week.  No ETA, but expect 1-2 weeks at the absolute minimum.\n", "@ebrevdo: Thanks!  It'd be good to coordinate further slicing work with @aselle to make sure there's no duplication. \n", "After discussing w/ @girving and @aselle, looks like gather_nd extensions will be coming in soon.  **Keep in mind** as of now, gather_nd has no gradient equivalent, so you still won't be able to use this to train :(  We'll need a scatter_nd op for that.\n", "This has been delayed as I work on performance improvements to the implementation.\n", "Any update on this issue?\n", "a new version of gather_nd op is being pushed.  it provides functionality\nto do what you need but currently no gradient method exists.\n\nOn Tue, Jul 26, 2016 at 7:23 PM, Zhuang Ma notifications@github.com wrote:\n\n> Any update on this issue?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235463215,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimxXKUx-niPQRERZOSS5N7RFHCVyzks5qZsEwgaJpZM4IDhJS\n> .\n", "@ebrevdo THANKS! Correct me if I miss something, the value of the state in currently rnn function will inherit the value from previous state if the the time step t is larger than the sentence length. Conceptually, the same trick can be applied to output without any further difficulty.  It seems that we can simply replace zero_output=zero_output in the following code with zero_output = output to achieve this and set the initial value of output to be zero_output. \n`for time, input_ in enumerate(inputs):\n\n```\n  if time > 0: varscope.reuse_variables()\n  # pylint: disable=cell-var-from-loop\n  call_cell = lambda: cell(input_, state)\n  # pylint: enable=cell-var-from-loop\n  if sequence_length is not None:\n    (output, state) = _rnn_step(\n        time=time,\n        sequence_length=sequence_length,\n        min_sequence_length=min_sequence_length,\n        max_sequence_length=max_sequence_length,\n        zero_output=zero_output,\n        state=state,\n        call_cell=call_cell,\n        state_size=cell.state_size)\n  else:\n    (output, state) = call_cell()\n  outputs.append(output)\n```\n\n`\n", "Yes, but that would not be backwards compatible and would result in\nincorrect gradients if you apply losses on every output.\n\nOn Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\n> something, the value of the state in currently rnn function will inherit\n> the value from previous state if the the time step t is larger than the\n> sentence length. Conceptually, the same trick can be applied to output\n> without any further difficulty. It seems that we can simply replace\n> zero_output=zero_output in the following code with zero_output = output to\n> achieve this and set the initial value of output to be zero_output.\n> ` for time, input_ in enumerate(inputs):\n> if time > 0: varscope.reuse_variables()\n> \n> # pylint: disable=cell-var-from-loop\n> \n> call_cell = lambda: cell(input_, state)\n> \n> # pylint: enable=cell-var-from-loop\n> \n> if sequence_length is not None:\n> (output, state) = _rnn_step(\n> time=time,\n> sequence_length=sequence_length,\n> min_sequence_length=min_sequence_length,\n> max_sequence_length=max_sequence_length,\n> zero_output=zero_output,\n> state=state,\n> call_cell=call_cell,\n> state_size=cell.state_size)\n> else:\n> (output, state) = call_cell()\n> \n>   outputs.append(output)`\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n> .\n", "Why will there be an issue with the backward step since the function seems\nto be completely based on current tensorflow ops?  Will the issue still\nexist if I only use the final output once instead of using all of them from\nthe sentence length to the maximum length?\n\nOn Wed, Jul 27, 2016 at 10:29 AM, ebrevdo notifications@github.com wrote:\n\n> Yes, but that would not be backwards compatible and would result in\n> incorrect gradients if you apply losses on every output.\n> \n> On Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n> \n> > @ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\n> > something, the value of the state in currently rnn function will inherit\n> > the value from previous state if the the time step t is larger than the\n> > sentence length. Conceptually, the same trick can be applied to output\n> > without any further difficulty. It seems that we can simply replace\n> > zero_output=zero_output in the following code with zero_output = output\n> > to\n> > achieve this and set the initial value of output to be zero_output.\n> > ` for time, input_ in enumerate(inputs):\n> > if time > 0: varscope.reuse_variables()\n> > \n> > # pylint: disable=cell-var-from-loop\n> > \n> > call_cell = lambda: cell(input_, state)\n> > \n> > # pylint: enable=cell-var-from-loop\n> > \n> > if sequence_length is not None:\n> > (output, state) = _rnn_step(\n> > time=time,\n> > sequence_length=sequence_length,\n> > min_sequence_length=min_sequence_length,\n> > max_sequence_length=max_sequence_length,\n> > zero_output=zero_output,\n> > state=state,\n> > call_cell=call_cell,\n> > state_size=cell.state_size)\n> > else:\n> > (output, state) = call_cell()\n> > \n> > outputs.append(output)`\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341\n> > ,\n> > or mute the thread\n> > <\n> > https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n> > \n> > .\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235602163,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AF2mdb86wpS8zseD5E9nlrLwTghh36_Hks5qZ2s_gaJpZM4IDhJS\n> .\n\n## \n\nZhuang Ma\nDepartment of Statistics\nThe Wharton School\nUniversity of Pennsylvania\n", "Suppose the sequence length of an item is 3, but the total computation time\n(max_time) is 10.  Then the last 7 outputs are copies of output @ time 2.\nNow you pass gradients back for all of these, and as a result you get\ngradient updates for all 10 time steps.  The gradients coming in from times\n2-9 all get accumulated into the bprop of step 2.\n\nEmitting the last output is not useful for most LSTM applications: people\nusually care about all outputs.\n\nOn Wed, Jul 27, 2016 at 7:52 AM, Zhuang Ma notifications@github.com wrote:\n\n> Why will there be an issue with the backward step since the function seems\n> to be completely based on current tensorflow ops? Will the issue still\n> exist if I only use the final output once instead of using all of them from\n> the sentence length to the maximum length?\n> \n> On Wed, Jul 27, 2016 at 10:29 AM, ebrevdo notifications@github.com\n> wrote:\n> \n> > Yes, but that would not be backwards compatible and would result in\n> > incorrect gradients if you apply losses on every output.\n> > \n> > On Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n> > \n> > > @ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\n> > > \n> > > something, the value of the state in currently rnn function will\n> > > inherit\n> > > the value from previous state if the the time step t is larger than the\n> > > sentence length. Conceptually, the same trick can be applied to output\n> > > without any further difficulty. It seems that we can simply replace\n> > > zero_output=zero_output in the following code with zero_output = output\n> > > to\n> > > achieve this and set the initial value of output to be zero_output.\n> > > ` for time, input_ in enumerate(inputs):\n> > > if time > 0: varscope.reuse_variables()\n> > > \n> > > # pylint: disable=cell-var-from-loop\n> > > \n> > > call_cell = lambda: cell(input_, state)\n> > > \n> > > # pylint: enable=cell-var-from-loop\n> > > \n> > > if sequence_length is not None:\n> > > (output, state) = _rnn_step(\n> > > time=time,\n> > > sequence_length=sequence_length,\n> > > min_sequence_length=min_sequence_length,\n> > > max_sequence_length=max_sequence_length,\n> > > zero_output=zero_output,\n> > > state=state,\n> > > call_cell=call_cell,\n> > > state_size=cell.state_size)\n> > > else:\n> > > (output, state) = call_cell()\n> > > \n> > > outputs.append(output)`\n> > > \n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly, view it on GitHub\n> > > <\n> > \n> > https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341\n> > \n> > > ,\n> > > or mute the thread\n> > > <\n> > \n> > https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n> > \n> > > .\n> > \n> > \u2014\n> > You are receiving this because you commented.\n> > Reply to this email directly, view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235602163\n> > ,\n> > or mute the thread\n> > <\n> > https://github.com/notifications/unsubscribe-auth/AF2mdb86wpS8zseD5E9nlrLwTghh36_Hks5qZ2s_gaJpZM4IDhJS\n> > \n> > .\n> \n> ## \n> \n> Zhuang Ma\n> Department of Statistics\n> The Wharton School\n> University of Pennsylvania\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235610283,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimxiJ59Zu_XUIjBGpUu2c4PREQoJpks5qZ3C8gaJpZM4IDhJS\n> .\n", "Right now there are two solutions:  the slower `tf.reverse_sequence` followed by pulling out the 0th index, or using `tf.gather_nd`.  Note that `gather_nd` does not currently have a gradient defined.  We're working on that now, but it's not blocking.  So I'm marking this as resolved.\n"]}, {"number": 1829, "title": "[Skflow] Make sklearn not mandatory dependency", "body": "To include skflow by default in the tensorflow, making sklearn non mandatory dependency.\n\nAdditionally started internal datasets package, similar to sklearn. Will be extended in subsequent work with dbpedia and other datasets from examples.\n", "comments": []}, {"number": 1828, "title": "SparseTensor: common unary ops", "body": "Currently, we have a slew of common unary ops that work on Tensors, but not SparseTensors ([ref](https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#basic-math-functions)):\n\n```\ntf.pow()\ntf.exp()\ntf.log()\n\n# lower priority?\ntf.abs()\ntf.neg()\ntf.sign()\ntf.inv()\ntf.square()\ntf.round()\ntf.sqrt()\ntf.ceil()\ntf.floor()\n```\n\nand so on.\n\nWe'd like these ops to work on SparseTensor. These do not change the indices nor shape of `SparseTensor`s, so all that's needed is transform the `.values` field on Python side in O(1) line.\n", "comments": ["Are `log()` and `inv()` well-defined for sparse tensors? Also, doesn't `exp()` make the whole tensor dense? (Or do other frameworks deal with the implicitly-zero elements differently in those cases?)\n", "As a first cut, I think having these ops transform only the non-empty entries makes sense and is simple to start with.  (This is what Theano calls \"structured\", as opposed to \"regular\" where the implicitly-zero elements are worked on as well.)  \n\nSo, `log(), inv(), exp()` should all work on the provided elements and thus will not make the tensor dense.\n", "@concretevitamin @ebrevdo Added PR for `tf.log()`, let me know whether I'm on the right track.\n", "Hi everyone, I am new to tensorflow but have been working on it recently and was hoping to contribute. This task might help me understand the basic functionalities like adding ops. Would it be fine if I try implementing these. I suppose @siddharth-agrawal  has already worked on some of these. Please let me know whether I can work on this. Thanks. \n", "@maniteja123 I would suggest looking at how to implement `tf.pow()` as it is a binary operator. I have already figured out how the unitary operator code and tests need to look like, so I will be putting them out one by one.\n", "pow(0, x) is undefined for x == 0, so that requires subtle work.\n\nexp(0) == 1, so that densifies the SparseTensor (therefore it's not very useful)\nlog(0) = -inf, and i see you just pushed that; we will have to revert it because the correct version densifies the sparse tensor (and is not very useful) while the pushed version assumes log(0) == 0.\ninv(0) is not defined, and so is not implementable.\n", "@vrv have to assign you for now, to revert the log() change.  feel free to unassign yourself after.\n", "@ebrevdo The implementations were based on @concretevitamin's comment [here](https://github.com/tensorflow/tensorflow/issues/1828#issuecomment-225325574), which was a reply to @mrry's comment just before that (which raised the same issues you raised). I think an option should be added later for the operators to make the result tensor dense, if and when applicable.\n", "No sparse matrix library I know of supports these operations, and SparseTensor was definitely not designed to support structured unary operations like this. It's meant to represent tensors with lots of zeros. Any functions that modify the sparsity structure of a dense version of the matrix have to be implemented similarly. One can always call tf.exp(st.values) themselves. Then it's very clear what's going on.\n", "Binary operations like tf.pow are even more tricky. See the various implementations of sparse_add.\n", "Could you point me towards some library where I can see what the expected outputs of each of these operations should be? Also, could you mention which of these operations should be implemented? (since you mentioned exp won't be useful and inv is not implementable)\n", "There are no sparse libraries that implement log or exp or inv.  Unary operations that preserve the sparsity pattern are reasonable and easy to implement... Think sign, neg, square, round, floor.\n", "Basically, any injective function for which f(0)=0 is OK.  floor and ceil require care because they move values near 0 to 0, so those are probably also not great candidates here.\n", "After more thoughts, I agree with @ebrevdo about leaving out the tricky ones for now.  Biggest reason being the purposed semantics is indeed weird, and if users do want to do something like that, it'd be easy for them to make a new SparseTensor with transformed values.  \n\nApologies for my earlier comment in this thread.  This issue probably still should exist for other unary ops that are well-behaved.\n", "Hi, sorry for the noise but in scipy sparse matrices, functions like log(1+X) [log1p](http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.sparse.csr_matrix.log1p.html) and exp(X)-1 [expm1](http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.sparse.csr_matrix.expm1.html) are implemented which satisfy f(0)=0. Though I am not sure if these operations are too specific to be implemented here. Cheers.\n", "@concretevitamin @ebrevdo Making a list of all the ops that cannot be made to work for `SparseTensor`s, let me know if there are any mistakes.\n\n```\ntf.inv()\ntf.rsqrt()\ntf.exp()\ntf.log()\ntf.sigmoid()\ntf.sin()\ntf.cos()\ntf.lgamma()\ntf.erfc()\ntf.tan()\ntf.asin()\ntf.acos()\ntf.atan()\n```\n", "Assuming the above list is correct, only `tf.erf()` remains to be done.\n", "@siddharth-agrawal I noticed that many of the ops are now in the same following form:\n\n``` python\n +  with ops.op_scope([x], name, \"Erf\") as name:\n +    if isinstance(x, ops.SparseTensor):\n +      x_erf = gen_math_ops.erf(x.values, name=name)\n +      return ops.SparseTensor(indices=x.indices, values=x_erf, shape=x.shape)\n +    else:\n +      return gen_math_ops.erf(x, name=name)\n```\n\nIs it possible that we use some registration wrapper / decorator to factor out this form? \n", "@concretevitamin Yes, this should be possible. But there are exceptions, for example `tf.abs()`. I'm not very familiar with registration wrappers / decorators. Could you provide some example in the TF code where they are used?\n", "Not directly applicable, but search for `ops.Tensor._override_operator` in `math_ops.py`.\n\nNow that I think about it more - what we wanted was more like a macro, not registration.  I don't see a good solution immediately, so fine to drop it for now.\n", "@concretevitamin If we are dropping this, then this issue has been resolved. Unless, of course, there is some problem with the list I provided above.\n", "@siddharth-agrawal I am closing this issue.  Thanks again for your awesome series of work on this!\n", "@concretevitamin Thanks! If there are any other `SparseTensor` ops I can help with, do let me know.\n"]}, {"number": 1827, "title": "Upstream changes from past few days", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins: test this please\n", "well, cmake is failing for unknown reasons, everything looks right.  Merging anyway since we're way behind on upstreaming, and everything else works.\n"]}, {"number": 1826, "title": "add a reference for TensorBoard in tutorial", "body": "add a internal reference for Tensorboard where it first comes out in cnn tutorial\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1825, "title": "Various matrix triangular functions. ", "body": "A feature request.\n\nAt the moment as far as I'm aware TensorFlow doesn't have equivalents of the following numpy functionality:\n\n1) np.tril: http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.tril.html and similarly\nnp.triu: http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.triu.html\n\n2) np.tril_indeces http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.tril_indices.html and the upper triangular equivalent and the ability to assign to a tensor using them.\n\n3) The ability to convert a vector to a lower or upper triangular square matrix which blas has. \n\nThis sort of thing is useful for instance in linear algebra usages of TensorFlow. \n\nI am unaware of an easy work around for these but I would be happy to be wrong. \n", "comments": ["Hi Alex,\n\nI'm actually working on adding the equivalent of np.tril and np.triu, or rather an op that can copy a specified set of diagonals from a given matrix or batch of matrices. I expect it to be done sometime next week. \n\nThe other pack/unpack of triangular to/from vector are probably lower priority. Feel free to submit a PR :-)\n\nRasmus\n", "Actually, regarding the packing/unpacking, you might want to look at the code in tensorflow/core/kernels/batch_matrix_diag_op.h which should get pushed to OSS shortly. It could be extended to extract multiple diagonals or you could at least use it as a template for a packing/unpacking op.\n", "Hi Rasmus, Thanks for your reply. I think what you are working on sounds good and will have a look when it gets merged. \n", "Any progress on this? Looking to be able to extract the upper (or lower) triags!\n", "1) is done . have a look at batch_matrix_band_part\n", "Some of the functionality of 2) and 3) is covered by the SparseTensor class and the functions\ntf.sparse_merge, and tf.sparse_to_dense. \n\nThe op pair tf.diag / tf.diag_part  (and batch_ extensions) do perform the unpacking/packing similar to what you suggest in 3), but here the potential space savings are much larger. I suppose batch_matrix_band_part could have been written that way (with a companion unpacking op), but I thought the current form, which simply zeros out the remaining part of the matrix, more convenient and much faster than having to write a packing followed by an unpacking op to achieve the same.\n\nAs always, feel free to send a PR if you think it would be useful.\n", "@alexggmatthews Great, thanks so much alexggmatthews! `tf.batch_matrix_band_part` is not a part of the official 0.8 release correct? I see documentation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/math_ops.md) but not [here in the official documentation](https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html). Nevertheless this looks like exactly what I need. Wonderful!\n", "You're most welcome @eriophora but it was @rmlarsen who did the hard bit - I guess he will know more about the release plans too. You're correct that it hasn't been in a release yet. \n", "@rmlarsen What's left to do in this issue? \n", "tf.batch_matrix_band_part is in the 0.9 release, and as discussed above there are reasonable equivalents for the other use cases. Feel free to open specific issue for them if you disagree. I'll close this issue. \n", "@rmlarsen Is it correct that the issue of unpacking a vector to a triangular matrix (similar to BLAS (d|s|c)tpttf) is not addressed yet? I have some code that does this, which I'm happy to contribute.\n", "@markvdw I think it would be useful if such code were in tensorflow; various projects, especially ones that do Gaussian process inference, really need to work with vectors of triangular matrices, e.g. [GPflow](https://github.com/GPflow/GPflow/blob/master/gpflow/tf_wraps.py) and [AutoGP](https://github.com/ebonilla/AutoGP/blob/master/autogp/util/tf_ops/tri_to_vec.cc)", "Sorry for just jumping in here. But the `tf.contrib.distributions.fill_triangular` utility function might be useful.  It reshapes a length-`d*(d+1)/2` vector into a lower (or upper) triangular matrix (with zeros on the flip-side).\r\n\r\n(Note: Im just now making this function public in tf.contrib.distributions. This should be live by tomorrow.)"]}, {"number": 1824, "title": "Profiling tools for open source TensorFlow ", "body": "It would be very useful to understand which parts of the computational graph are causing computational bottlenecks.\n\nThis question has been asked on Stack Overflow here:\n\nhttp://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations\n\nand here:\n\nhttp://stackoverflow.com/questions/34629613/how-to-profile-tensorflow-networks\n\nbut it seems more appropriate as a feature request as this would allow systematic discussion. The TensorFlow white paper talks about very advanced internal tools that Google have along these lines. Something like that would make open source TensorFlow even better. I did it look at the road map but I'm not sure it explicitly mentioned this topic. \n\nMy apologies if I have missed a post about this. \n", "comments": ["Could you take a look at the [recently added timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) and its associated test to see if it's helpful?\n", "This looks good to me. I'm really impressed by the rate of progress on tensorflow in general. I guess a HowTo is on the way as well?\n", "@concretevitamin: A how-to for TensorFlow profiling is probably a good idea.  If you wrote the timeline, would you want to write a tutorial highlighting it?\n", "@girving Paul is probably working on something along this line (profiling tools for OSS land).  (Although I wrote the RunOptions/RunMetadata support, I'm not the original author of the timeline.)\n", "@concretevitamin Which Paul?\n", "@geoffreyi pbar.\n\nOn Tue, Jun 14, 2016 at 12:36 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @concretevitamin https://github.com/concretevitamin Which Paul?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225636669,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAkLHlA4FU1FAmpGsoL4brz0m7WKKB-Uks5qLYcjgaJpZM4IDGnH\n> .\n", "@prb12 What's the story for OSS profiling?  If we have tools in place, could we make a tutorial explaining them?  Assigning to you for now, but feel free to triage further.\n", "I'm unlikely to have much time to write a tutorial in the near future, but the current status of the open source tools are as follows:\n\nThere is now a basic CUPTI GPU tracer integrated in the runtime.  You can run a step with tracing enabled and it records both the ops which are executed and the GPU kernels which are launched.  Here is an example:\n\n```\nrun_metadata = tf.RunMetadata()\n_, l, lr, predictions = sess.run(\n            [optimizer, loss, learning_rate, train_prediction],\n            feed_dict=feed_dict,\n            options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n            run_metadata=run_metadata)\n```\n\nAfter the step completes, the run_metadata should contain a StepStats protobuf with lots of timing information, grouped by tensorflow device.  The CUPTI GPU tracing appears as some additional devices with names like  /gpu:0/stream:56 and /gpu:0/memcpy\n\nNote: to get GPU tracing you will need to ensure that libcupti.so is on you LD_LIBRARY_PATH.  It is usually found in /usr/local/cuda/extras/lib64.\n\nThe simplest way to use this information is to load the stats into a 'Timeline' as follows:\n\n```\nfrom tensorflow.python.client import timeline\ntrace = timeline.Timeline(step_stats=run_metadata.step_stats)\n```\n\nThe Timeline class can then be used to emit a JSON trace file in the Chrome Tracing Format, as follows:\n\n```\ntrace_file = open('timeline.ctf.json', 'w')\ntrace_file.write(trace.generate_chrome_trace_format())\n```\n\nTo view this trace, navigate to the URL  'chrome://tracing' in a Chrome web browser, click the 'Load' button and locate the timeline file.\n\nIt would be fairly simple to write a small python web server which served up these traces from a running TensorFlow program like [this](https://github.com/catapult-project/catapult/blob/master/tracing/docs/embedding-trace-viewer.md)\n", "We have been using these tracing features for tensorflow computations on a single machine and they are very helpful. Is there a way to collect the Timeline trace as mentioned above for distributed tensorflow? Thank you!\n", "@ericox  There are a bunch of hooks for distributed trace collection, but they are currently unimplemented in the open source gRPC code, see [here](https://github.com/tensorflow/tensorflow/blob/a7b7b26051989e1af40e657adef84c5550b838fe/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc#L445) and [here](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/protobuf/worker.proto#L180)\n\nIt might not be _too_ hard to plumb these through...  @mrry do you know of any showstoppers? \n", "@prb12 Thanks... Sounds good to me. I am interested to know if there are any showstoppers here. I wouldn't mind submitting a PR if its straightforward.\n", "It wouldn't be too hard to get a basic version of this up and running:\n1. Plumb the [`ExecutorOpts.record_timeline`](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/protobuf/worker.proto#L147) through the `RunGraphRequest` sent from the master to the worker.\n2. Detect when this is set at the worker and create a `StepStatsCollector` to get the profiling information for the step.\n3. Pass the stats back in the `RunGraphResponse`.\n4. Aggregate the stats at the master and pass the aggregated stats back in the `RunStepResponse`.\n", "@mrry Thanks that is helpful! I will try and see if I can get it working.\n", "It would also be necessary to pass in a worker-specific device prefix to the Collect() method of the GPUTracer for use [here](https://github.com/tensorflow/tensorflow/blob/545df470168f52a369b5f1510f26ad001f48c650/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L550).\n(At the moment this code assumes a single worker.  When running with multiple workers we wouldn't want to merge all of the gpu activity together)\n", "@prb12 I am wondering if it would it be more useful to use a worker-specific device prefix such as `/job:worker/task:7` from the code snippet from the distributed tensorflow tutorial?\n\n``` python\nwith tf.device(\"/job:worker/task:7\"):\n  input, labels = ...\n  layer_1 = tf.nn.relu(tf.matmul(input, weights_1) + biases_1)\n  logits = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)\n  # ...\n  train_op = ...\n```\n\nOr to include the worker hostname in the prefix as well, say the worker_hosts string(s) here. \n\n```\n# On ps0.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=ps --task_index=0\n```\n", "@ericox  The internal google equivalent of this tracing uses the `/job:worker/task:7` style prefixes so that the GPU hardware tracing appears next to the 'software tracing' for devices on the same worker in the timeline visualizations.   (The TensorFlow worker's environment probably has the correct name in it somewhere)\n\nI'm not sure yet if the chrome trace viewer code will correctly preserve the sort-order, but if not it's an easy fix... \n\nPlease let me know if any of this end up being more difficult than expected (this has been on my TODO list for a while!)\n", "@prb12 Thanks! That's good to know. I will definitely mention if this becomes more difficult than expected. I've read through some of the code @mrry outlined and I am planning on giving this an effort over the next few days.\n", "@prb12 thanks, that tracing tool is pretty useful. For macos CUPTI is in /usr/local/cuda/extras/CUPTI/lib, also needed to add `trace_file.close()`\n", "I tried the code change suggested by @prb12 but I just get the trace file blank like\n\n{\n    \"traceEvents\": [\n        {\n            \"ph\": \"M\",\n            \"args\": {\n                \"name\": \"Allocators\"\n            },\n            \"pid\": 0,\n            \"name\": \"process_name\"\n        }\n    ]\n\nThere is no data in there. Am I missing something?\n", "Before the training loop,\n\n```\nrun_metadata = tf.RunMetadata()\ntrace_file = open('timeline.ctf.json', 'w')\n```\n\nIn the training loop, I have done:\n          sess.run(model.learning_rate_decay_op,\n                        options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\n                        run_metadata=run_metadata)\n......\n\nand at the end of the loop:\n\n```\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n    trace_file.write(trace.generate_chrome_trace_format())\n```\n\nFinally:\n    trace_file.close()\n\nThe file gets created but without any data in the file.\n Anything missing?\n", "@srp1970 Without seeing your entire program it's hard to know, but one thing you should bear in mind is that this tracing mechanism is designed to capture just a single step (i.e. a single call to session.run()).  The `run_metadata` protobuf is an _out_ parameter of the `session.run` call, and will be overwritten each time - i.e. you shouldn't really use it for multiple calls.   \n\nI'm not sure what the python swig wrapper will do if you reuse a non-empty proto.  It _may_ attempt to merge in the new results - but it's very unlikely that the `Timeline` code with know how to deal with this. \n\nPerhaps you could try printing out the contents of step_stats?  Then try tracing just a single run call?\n", "Sorry, my mistake. I had put my trace collection at the wrong place. Corrected and it does write non-zero JSON file. It gives tons of warning such as:\n\nWARNING:tensorflow:Can't find tensor ^GradientDescent_3/update_embedding_attention_seq2seq/embedding_attention_decoder/attention_decoder/AttnOutputProjection/Linear/Bias/ApplyGradientDescent\nWARNING:tensorflow:Can't find tensor GradientDescent_3/update/NoOp_1\nWARNING:tensorflow:Can't find tensor ^GradientDescent_3/update/NoOp\nWARNING:tensorflow:Can't find tensor ^GradientDescent_3/update/NoOp_1\nWARNING:tensorflow:Can't find tensor ^GradientDescent_3/update\n\nAlso, the trace file is too big, and browser crawls to load/display anything. It is about 10MB for just few seconds run. Should I use smaller traces (instead of FULL_TRACE)? I need only GPU traces/activities. Is there something light weight for me?\n", "Ok, got the reason for WARNINGS above. I think that is because mismatch of the number of layers in the original model and reruns. \n\nThe only issue I have now is that the trace file is very large 100+MB and so chrome is unable to load it. It just crawls and hangs.\n", "@srp1970 What version of TensorFlow are you using?  This bug (i.e. lots of warnings) was fixed a few weeks ago IIRC, and the defaults for the optional args to `generate_chrome_trace_format` were also changed to remove a lot of the more detailed information (e.g. memory histograms and tensor snapshots). \n\nLooks like the changes are in 0.9 [here](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/python/client/timeline.py#L556) and [here](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/python/client/timeline.py#L615).\n\nGPU tracing is only present with FULL_TRACE - there is no cheaper option which will give you the information you want.  However this flag affects what is in the `RunMetadata.StepStats` protobuf you get back from `session.run`.  The size of the chrome trace file is determined by which information from the `StepStats` you ask to visualize.  e.g. adding full shape/size information on all intermediate tensors (`show_memory=True`) can make the JSON output much larger.   \n", "@prb12 I've started work on the plumbing for the distributed trace as discussed earlier. I got a bit delayed but I did some code injection to see what relevant code gets executed during a distributed run. FWIW my group mates and I were hoping that we could try to aggregate trace files by merging the json data from individual worker nodes. But it seems that Timeline trace files are empty for all but a few of the metadata events for the distributed runs. Is this behavior consistent with the suggestion that we need to plumb the `StepStatsCollectors` through the protobuffers to get the trace data?\n", "@ericox I believe it is exactly due to that.  The whole stack of `RunOptions/RunMetadata -> StepStats -> Chrome timeline` is currently supported in `DirectSession` only.  \n", "@concretevitamin Ok thanks. I thought I may have been missing something.\n", "@ericox  Merging the JSON data is unlikely to be a good idea... it is _much_ simpler for the master to just collect the `StepStats` protobufs from each worker and merge the protos (which is a single line of code!)  The Timeline code ought to just deal with the merged `StepStats` and create a single JSON file.\n", "@prb12 OK that sounds simpler. Thank you.\n", "@ericfox  FYI - I just started hacking this up...  I think I can probably have something basic working by the end of the day.  \n", "@prb12 Sounds good. I wouldn't mind still helping out on this if you want to split the work.\n", "@ericox Great!    \n\nThere's some basically working code [here](https://github.com/prb12/tensorflow/tree/distributed-tracing).   I've only tried it with a single Grpc worker, and I haven't plumbed through the GPUTracer yet, but you ought to be able to patch and try it out.\n\nObviously this will take some work to clean up and write tests etc. before it could be submitted! ;-)\n", "@prb12 Excellent! Thanks for the code. I will try it out and do the plumbing through the GPUTracer + tests ASAP. This is a big help! \n", "I just pushed some code to start/stop the GPUTracer in the right places - haven't tested in anger.\n\nTo produce a decent visualization the GPUTracer::Collect method will need changing to be able to pass in a worker-specific device prefix.  Since it's used elsewhere this will probably require a bunch of other code to be fixed up and tested. \n\nOther than that, it should be mostly there.\n", "I started hacking on adding the worker-specific device prefix to the `GPUTracer::Collect` method. To get something going I am trying to test and get things running using the interface `GPUTracer::Collect(StepStatsCollector *collector, string device_prefix)`.\n", "LGTM +1  that's good utils to trace flow states,hope someone write tutorial to description this \ud83d\udc4d \n", "Is this issue still worked on? With a recent build of TF ( `57ff9d9be2f1faaba9598a3d99ef6c3af02342a4`), I still get an \"empty\" step_stats object in a distributed environment. When written to JSON it looks like this: \n{\n    \"traceEvents\": [\n        {\n            \"ph\": \"M\",\n            \"args\": {\n                \"name\": \"Allocators\"\n            },\n            \"pid\": 0,\n            \"name\": \"process_name\"\n        }\n    ]\n}\n\nIt would be great if it were possible to profile in the distributed environment. Thx!\n", "I am running the tracing as described above on a single application on a single node. Is there any documentation anywhere on what the various row names mean? Specifically, I am seeing the bulk of time spent in rows with no label in the left column but grouped under rows like:\n\n/job:localhost/replica:0/task:0/gpu:0 Compute (pid 3)\n\nand very little time in\n/gpu:0/stream:all Compute \n/gpu:0/memcpy Compute\n\nAre the localhost/replica times some kind of GPU setup/scheduling tasks?\nI'm also confused as to why there are so many pids. There are 2 pids each for\n/job:localhost/replica:0/task:0/cpu:0 Compute\n/job:localhost/replica:0/task:0/gpu:0 Compute\n\nI would only expect one of each for a single application on a single node.  \n", "@kbrems This is not really meant to be a discussion forum, ... . but...\n\n>  I am seeing the bulk of time spent in rows with no label in the left column but grouped under rows like /job:localhost/replica:0/task:0/gpu:0\n\nAll of the 'rows' under this heading are ops being **dispatched** on the same Tensorflow gpu device. Because multiple ops can be fired off in parallel on the host, their execution may overlap in time.  A simple bin-packing algorithm is used to assign them to multiple rows so that they don't overlap in the UI.  (Note- this doesn't correspond to 1:1 with host threads)\n\n> ...and very little time in /gpu:0/stream:all Compute\n\nThis usually means that your ops are running very small GPU kernels which take a long time to enqueue compared to the time they take to execute.  Or that the critical path of your computation includes a bunch of ops executing on a different device.\n\n> Are the localhost/replica times some kind of GPU setup/scheduling tasks?\n\nTechnically, they show the time spend in the TensorFlow `OpKernel::Compute` method of each op executed on that device.  \n\nFor CPU devices the `Compute` method actually performs the work required by the op, and the timings show the **elapsed** time for the op to complete.  Note that the work may actually be parallelized across many threads/cores in a shared threadpool.  So, if more than one parallel CPU op is running at the same time they contend for processor resources and most likely the elapsed time will appear longer than the actual amount of processor cycles consumed.  (If you care about measuring this you can either run without op-parallelism, or use a standard sampling profiler). \n\nFor GPU, the `OpKernel::Compute` method usually just enqueues a CUDA kernel launch or DMA transfer on a GPU stream.  This typically only takes a few microseconds (in fact, the overheads of tracing are often in this range!)  For GPU devices, the 'real' kernel timings come from the NVidia GPU profiler and are shown in the `/stream:*` lines.  When perf debugging it is often useful to see both.\n\n> I'm also confused as to why there are so many pids. There are 2 pids each for ...\n\nThis is just an unfortunate property of the builtin Chrome Trace Visualizer GUI - it isn't very flexible in terms of labels/hierarchy and which parts of the trace you can expand/collapse.  But it's installed in every browser and saved us having to write a separate visualizer tool.\n\nWe chose to map TensorFlow devices to 'PID's in CTV so that all the activity on a device was grouped nicely (and could be selectively expanded/collapsed).   All of the numeric 'PID's and 'TID's in the UI should be ignored- they were just invented to get CTV to lay out the trace better but there's no way to stop the GUI displaying them!\n", "@matthiasreisser No - I haven't seen any progress on this since the code I posted in my private fork.  I'll see if I can convince somebody on the team to pick up the ball.   There's not really much left to do - it's mainly a matter of tidying up the code quality and writing some tests.\n", "@prb12 That would be great - It seems to be quite important to know how distributing an algorithm influences the computation time in a particular implementation. I am looking forward to this! \n", "@prb12 Thanks for the information - that is helpful. Sorry about the forum, but this thread was the only place I could find any sort of documentation on the profile traces!\n", "@kbrems  No problem.  Please bear in mind that much of this functionality is still being developed/refactored and will probably need to change a little before we are happy with it.\n\n@mrry has just submitted some support for distributed trace collection  to our internal tree and it should hopefully go into the next nightly sync.  \n", "My code runs 10x slower (10 seconds vs 1 second) with `trace_level=tf.RunOptions.FULL_TRACE`. The code includes rather heavy computation such as large matrix multiplications, and I am very surprised that the overhead of profiling is so huge. Can anybody comment on this?", "@rizar That much overhead sounds very implausible, especially if you are running expensive large ops.  Please can you file a proper issue report with sufficient information and code to reproduce the problem.", "Closing this issue since there is now distributed tracing support.", "@prb12 the timeline gives cumulative time taken by each op type (e.g. total time taken by matmul).. Is there a way to see the time taken not just by op type, but by each graph node ? e.g. if i want to measure the time taken by the matmus inside a lstm cell vs. the matmul in the sampled softmax, how do I get that split ? ", "@rakdl I don't know of any tool that has this kind of customization for aggregation, but you can get the raw timing information from the run_metadata.step_stats object, and aggregate stats yourself in a custom way", "@rakdl the timeline gives cumulative time taken by each op type (e.g. total time taken by matmul).. \r\n\r\nNo, it doesn't.  It shows the time taken by each graph node?", "Hi, is there any documentation of what different fields in `NodeExecStats` mean? All I see in the [respective file](https://github.com/tensorflow/tensorflow/blob/714f9b74b53e3644c7be4d967feb4750fcc26fe6/tensorflow/core/framework/step_stats.proto) is \r\n\r\n```\r\n// Time/size stats recorded for a single execution of a graph node.\r\nmessage NodeExecStats {\r\n  // TODO(tucker): Use some more compact form of node identity than\r\n  // the full string name.  Either all processes should agree on a\r\n  // global id (cost_id?) for each node, or we should use a hash of\r\n  // the name.\r\n  string node_name = 1;\r\n  int64 all_start_micros = 2;\r\n  int64 op_start_rel_micros = 3;\r\n  int64 op_end_rel_micros = 4;\r\n  int64 all_end_rel_micros = 5;\r\n  repeated AllocatorMemoryUsed memory = 6;\r\n  repeated NodeOutput output = 7;\r\n  string timeline_label = 8;\r\n  int64 scheduled_micros = 9;\r\n  uint32 thread_id = 10;\r\n  repeated AllocationDescription referenced_tensor = 11;\r\n};\r\n```\r\n\r\nwhich is not very informative.\r\n\r\nIn particular I am trying to understand if these statistics allow me to see which output tensors are kept in memory until the backward pass and which are quickly garbage collected.", "/cc @poxvoculi, who defined this proto originally.", "NodeExecStats was intended to collect measurements related to the execution time and memory requirements of a graph node so as to better inform an intelligent placement and/or scheduling algorithm.  None of its fields record the lifespan of any output tensors, since this data is recorded soon after the termination of node execution, and the lifespan may be unknown.\r\n\r\nWhat you are asking for sounds useful, and may be available some other way that I don't know of, but I'm doubtful.  If we tagged memory allocations with the name of the Op that caused them, it would be possible to annotate the timeline with tensor allocation lifespans.  An Op's output persists until the backward pass whenever its output is also an input to a node in that backward pass, so it should be possible to write a program that analyses that graph and identifies such cases.", "@rizar for keeping track of live tensors and please see my package https://github.com/yaroslavvb/memory_util . I've gotten it debugged to a reasonable state while optimizing memory usage of our models, so let me know if you see anything that seems off", "@poxvoculi , thank you for your explanations. The reason why I thought that this information could be there, is because when I generate a timeline in the Chrome trace format and load it in the browsers, for some of the tensors I can see a span in it (see a screenshot below). I tried to understand what this span means and made a wrong hypothesis that it may correspond to the period when the tensor was kept in memory. But from what you are saying it does not seem to be the case. That said, I am still curious: what do the spans in the timeline stand for, why do some tensors have a span associated with them, while other just a creation moment?\r\n\r\n![screenshot from 2017-02-24 09 02 43](https://cloud.githubusercontent.com/assets/654434/23306144/16cbd0ae-fa70-11e6-9e9c-d602983cde26.png)\r\n\r\nThank you @yaroslavvb , I will take a look at your tool!\r\n", "@rizar  The tensor (and memory) views generated by tf.Timeline for Chrome Trace Viewer do not typically represent what's going on very well, hence those options are turned off by default.\r\n\r\nAs you pointed out, the NodeExecStats proto doesn't really provide enough information to track tensor dependencies accurately.  To do this you really need to know the complete dataflow graph which was executed - and this is **not** the same as the input GraphDef since the placer and optimizer can make quite substantial changes. \r\n\r\nWhen tf.Timeline was written, it was possible to get a reasonable approximation of the graph by parsing the `timeline_label` field of NodeExecStats to reconstruct some of the dataflow dependencies.  This has become less and less robust over time.\r\n\r\nHowever - it is now possible to programmatically retrieve the optimized graphs via the `partition_graphs` field of the `RunMetadata` proto, and these ought to provide an accurate idea of tensor lifetime.  The tensor lifetime and memory views of tf.Timeline should probably be rewritten to use these GraphDefs if available.   (however, I don't have any time to do this!)", "I find it's hard to determine the run time of Recv Op in distributed tensorflow since two machines may show different time.", "@prb12 not sure if it's what you were discussing above (it was a bit over my head), but the timeline that I generated for the CIFAR10 tutorial has relatively unmeaningful labels for each time-block. I managed to make it a bit better by wrapping chunks of code with: \r\n`with tf.name_scope(<<label>>):`\r\n\r\nHowever, this sometimes makes some of the labels longer (could it be grouping together processes in the same name_scope?), and also doesn't make it much clearer anyway. Is there a way to set up the timeline such that each time-block will represent a function or an individual specific process?\r\n\r\nI put the timeline code mentioned above here (marked with asterisks):\r\n\r\n```\r\n# Excerpt from CIFAR10_train.py\r\n\r\nwith tf.train.MonitoredTrainingSession(\r\n        checkpoint_dir=FLAGS.train_dir,\r\n        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\r\n               tf.train.NanTensorHook(loss),\r\n               _LoggerHook()],\r\n        config=tf.ConfigProto(\r\n            log_device_placement=FLAGS.log_device_placement)) as mon_sess:\r\n  \r\n    while not mon_sess.should_stop():\r\n*       run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n*       run_metadata = tf.RunMetadata()\r\n*       mon_sess.run(train_op, options=run_options, run_metadata=run_metadata)\r\n\r\n*       # Create the Timeline object, and write it to a .json file\r\n*       tl = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n*       with open('./json/timeline.ctf.json', 'w') as f:\r\n*           f.write(tl.generate_chrome_trace_format())```", "> @rizar The tensor (and memory) views generated by tf.Timeline for Chrome Trace Viewer do not typically represent what's going on very well, hence those options are turned off by default.\r\n> \r\n> As you pointed out, the NodeExecStats proto doesn't really provide enough information to track tensor dependencies accurately. To do this you really need to know the complete dataflow graph which was executed - and this is not the same as the input GraphDef since the placer and optimizer can make quite substantial changes.\r\n> \r\n> When tf.Timeline was written, it was possible to get a reasonable approximation of the graph by parsing the timeline_label field of NodeExecStats to reconstruct some of the dataflow dependencies. This has become less and less robust over time.\r\n> \r\n> However - it is now possible to programmatically retrieve the optimized graphs via the partition_graphs field of the RunMetadata proto, and these ought to provide an accurate idea of tensor lifetime. The tensor lifetime and memory views of tf.Timeline should probably be rewritten to use these GraphDefs if available. (however, I don't have any time to do this!)\r\n\r\n@prb12  Is this still true? Did anyone start to rewrite tf.Timeline as you suggested? Or ist there another way to debug memory usage?", "@georgh you can use mem_util as shown [here](https://github.com/yaroslavvb/chain_constant_memory/blob/master/mem_util_test.py) to get detailed view of tensor allocations/deallocations", "@yaroslavvb Thank you nice tool! Something like this should definitely part of TF-core.\r\nDo you have any experience with missing allocations or a wrong reported timeline?\r\nAs I reported in #17092 , I got the problem that an operation executed twice fails on its second execution, but all the memory prints look totally fine.\r\nIn addition, the mem_util tool reports no memory usage of the input tensor (Placeholder) - is this expected?\r\nThank you for your help :+1:   ", "How can I view tensor shapes in the timeline? Currently I only see a list of operations. Also it is very hard to zoom into the timeline.", "Is there any documentation how to use it with Tensorflow C++ from `StepStats` ?  #21312", "@prb12  Could you please have a look at my problem?\r\nI have two GPUs and want to try some distributed training(model-parallelism) in TensorFlow.\r\nMy plan is to divide LeNet into two parts, assign each part to one GPU.\r\nLeNet has 5 layers, I use `with tf.device('/gpu:0'):` to assign layer 1 to GPU 0, `with tf.device('/gpu:1'):`to assign layer2-layer5 to GPU 1.\r\nThe device mapping log shows that, all the ops have been assigned to the device as I wish:\r\n````\r\nlayer5/fc3_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer5/fc3_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer4/fc2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer3/fc1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer2/conv2_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:1\r\ninit/NoOp_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:1\r\nlayer1/conv1_b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nlayer1/conv1_w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\r\n````\r\n\r\nI use Timeline to profile this model, but it gives the unexpected result.\r\n\r\n![inlvu](https://user-images.githubusercontent.com/25604857/46917875-4caf4d80-cffe-11e8-8cfa-64da3c23bec0.jpg)\r\n\r\nIt seems that ops of layer2-layer5 are launched in GPU1 but run on GPU0. I don't think this is what I want by using with tf.device('/gpu:1'):.\r\n\r\nIs this expected in TensorFlow?\r\n\r\nI've posted this [question](https://stackoverflow.com/questions/52791296/with-tf-device-does-not-work-in-tensorflow) on StackOverflow", "How to see threads of a CPU process in the Timeline?"]}, {"number": 1823, "title": "Saver optimistic restore", "body": "Hello,\n\nI have a feature request about `Saver`. Is it possible to add an option to stop it from complaining about absent variables in a checkpoint? Quite often when I experiment with different architectures, I change layers between runs, but want to reuse pretrained earlier checkpoints. An option \"just restore whatever you can from this checkpoint without complaining\" would be very useful. Now I play with `var_list`, but find it tiring and error-prone. Another idea is to retrieve variable names from a given checkpoint, but I couldn't find API for that.\n", "comments": ["The `tf.train.NewCheckpointReader()` API lets you inspect a checkpoint to find out what variables it contains (it's in the nightlies at the moment, will be in 0.8).\n", "I'm closing this because the `tf.train.NewCheckpointReader()` API is now in the latest version, and enables users to build this functionality in code that uses the existing Saver API.\n", "It does not seem to be documented in v0.9, where can I check this API?\n", "It looks like there's no documentation for this API (indeed, it's in generated code, so there's no source code for it either). Here's a quick guide (though note that this is subject to change between versions):\n- `tf.train.NewCheckpointReader(filename)`: returns a checkpoint reader for the given file. (I believe this has an auto-generated type, though it implements the same interface as [`tensorflow::checkpoint::CheckpointReader`](https://github.com/tensorflow/tensorflow/blob/b0166b8b07a0f134cea3bfbe9a55a7c9f887ba3a/tensorflow/core/util/checkpoint_reader.h), so we'll call it `CheckpointReader`.\n- `CheckpointReader.has_tensor(tensor_name)`: returns `True` if a tensor with the given name is in the checkpoint, otherwise `False`.\n- `CheckpointReader.get_tensor(tensor_name)`: returns a NumPy array containing the containing the value of the tensor with the given name from the checkpoint.\n- `CheckpointReader.get_variable_to_shape_map()`: returns a dict mapping tensor names to lists of ints, representing the shape of the corresponding tensor in the checkpoint.\n", "I think it's way more user-friendly to allow new names, new shapes, and removed variables (Caffe style). This code seems to get that job done. I call it after running the global init operation.\r\n\r\n```python\r\ndef optimistic_restore(session, save_file):\r\n    reader = tf.train.NewCheckpointReader(save_file)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n            if var.name.split(':')[0] in saved_shapes])\r\n    restore_vars = []\r\n    with tf.variable_scope('', reuse=True):\r\n        for var_name, saved_var_name in var_names:\r\n            curr_var = tf.get_variable(saved_var_name)\r\n            var_shape = curr_var.get_shape().as_list()\r\n            if var_shape == saved_shapes[saved_var_name]:\r\n                restore_vars.append(curr_var)\r\n    saver = tf.train.Saver(restore_vars)\r\n    saver.restore(session, save_file)\r\n```", "Thanks for @danielgordon10 's Caffe-style restore implementation. It is very useful when loading a pretrained model with network change. \r\n\r\nI tried the function in my task, but it doesn't work correctly when running this code line\r\n\r\n   ```curr_var = tf.get_variable(saved_var_name)```\r\n\r\nI guess it's because the variable_scope of my networks are nested, and makes the variable names prepended with scope names (the variable names are therefore something like scope1_scope2_scope3_op:0). \r\nIn this case using an empty variable_scope might be inappropriate. (Not sure whether this is the reason)\r\n\r\n```with tf.variable_scope('', reuse=True):```\r\n\r\nAnyway, I modified the function to get the variables by their full names without the empty variable_scope:\r\n```python\r\ndef optimistic_restore(session, save_file, graph=tf.get_default_graph()):\r\n    reader = tf.train.NewCheckpointReader(save_file)\r\n    saved_shapes = reader.get_variable_to_shape_map()\r\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n            if var.name.split(':')[0] in saved_shapes])    \r\n    restore_vars = []    \r\n    for var_name, saved_var_name in var_names:            \r\n        curr_var = graph.get_tensor_by_name(var_name)\r\n        var_shape = curr_var.get_shape().as_list()\r\n        if var_shape == saved_shapes[saved_var_name]:\r\n            restore_vars.append(curr_var)\r\n    opt_saver = tf.train.Saver(restore_vars)\r\n    opt_saver.restore(session, save_file)\r\n```\r\nThe new function works well in my case. But I'm not quite confident of the implementation. "]}, {"number": 1822, "title": "Clarify attention_decoder documentation", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins , test this please.\n"]}, {"number": 1821, "title": "Basic Lstm cell giving NAN loss and 0 acuraccy.", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 \n\nInstalled version of CUDA and cuDNN:  7.5 cudnnv4.0\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 M\u00e4r 29 12:20 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 M\u00e4r 29 12:20 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 M\u00e4r 29 12:20 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 M\u00e4r 29 12:20 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 M\u00e4r 29 12:20 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1\n\n```\n\nHi,\n\nI had modified [this](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py) code to accept an input 4096 vector with 16 time steps.\nand changed very little.\nThe loss is always nan and acuraccy 0.\nI have tried several combinations of Learningrate/batchsize/optimizers with no change.\n\n[Here Is my code.](http://pastebin.com/S1WcVNmk)\n# Output\n\n```\nIter 640, Minibatch Loss= nan, Training Accuracy= 0.75000\nTesting Accuracy: 0.75\nIter 1280, Minibatch Loss= nan, Training Accuracy= 0.75000\nTesting Accuracy: 0.0\nIter 1920, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.0\nIter 2560, Minibatch Loss= nan, Training Accuracy= 0.01562\nTesting Accuracy: 0.0\nIter 3200, Minibatch Loss= nan, Training Accuracy= 0.01562\nTesting Accuracy: 0.0\nIter 3840, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.0\nIter 4480, Minibatch Loss= nan, Training Accuracy= 0.03125\nTesting Accuracy: 0.0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 4696 get requests, put_count=4701 evicted_count=1000 eviction_rate=0.212721 and unsatisfied allocation rate=0.21678\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281\nIter 5120, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.0\nIter 5760, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.0\nIter 6400, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.015625\nIter 7040, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.0\nIter 7680, Minibatch Loss= nan, Training Accuracy= 0.00000\nTesting Accuracy: 0.015625\nIter 8320, Minibatch Loss= nan, Training Accuracy= 0.00000\n```\n\nAny ideas on y this is happening? \n", "comments": ["not a real issue.. Fixed the problem \nclosing \n", "How did you end up fixing this? I'm having be same issue\n", "The issue was caused because I initialized the one hot classes as np.array() and didnt fill a few of them with 0 or 1. And as they are randomly initialized. A few of them turned out to be nan.\nJust debug your input data to the N/W. \n"]}, {"number": 1820, "title": "Tensorflow version of word2vec issue with huge corpora", "body": " Hi All,\n\nI have the pip wheel distribution of Tensorflow r7.1 in Red Hat 6 in our cluster.\n\nMy issue is that I wanted to extract the embedding from a huge corpora (5*10\u2079 tokens), but  the 32 bit \"corpus_size_\" counter in \"word2vec_kernel.cc\" is not enough. It seems that it is not so straightforward to fix it (changing the counter to int64). \n\nI have seen that this very error was reported last  17th of Dec 2015 in issue #531.\n\nI also receive the following error:\n\nW tensorflow/models/embedding/word2vec_kernels.cc:59] Invalid argument: The text file wbu.txt contains too little data: -522037214 words\nE tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: The text file wbu.txt contains too little data: -522037214 words\nE tensorflow/core/common_runtime/executor.cc:275] Executor failed to create kernel. Invalid argument: The text file wbu.txt contains too little data: -522037214 words\n     [[Node: Skipgram = Skipgram[batch_size=16, filename=\"wbu.txt\", min_count=5, subsample=0.001, window_size=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nTraceback (most recent call last):\n  File \"word2vec_minibatch.py\", line 425, in <module>\n    tf.app.run()\n  File \"/opt/python-2.7/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"word2vec_minibatch.py\", line 411, in main\n    model = Word2Vec(opts, session)\n  File \"word2vec_minibatch.py\", line 175, in __init__\n    self.build_graph()\n  File \"word2vec_minibatch.py\", line 312, in build_graph\n    opts.words_per_epoch) = self._session.run([words, counts, words_per_epoch])\n  File \"/opt/python-2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 315, in run\n    return self._run(None, fetches, feed_dict)\n  File \"/opt/python-2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 511, in _run\n    feed_dict_string)\n  File \"/opt/python-2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _do_run\n    target_list)\n  File \"/opt/python-2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 586, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: The text file wbu.txt contains too little data: -522037214 words\n     [[Node: Skipgram = Skipgram[batch_size=16, filename=\"wbu.txt\", min_count=5, subsample=0.001, window_size=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Skipgram', defined at:\n  File \"word2vec_minibatch.py\", line 425, in <module>\n    tf.app.run()\n  File \"/opt/python-2.7/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"word2vec_minibatch.py\", line 411, in main\n    model = Word2Vec(opts, session)\n\nThanx\n", "comments": ["The issue seems to be that `corpus_size_` is an int rather than long and so overflows...\n", "To deal w/ a large text corpus while still using a single process, mainly we need to change the code to avoid loading the text file in memory in whole and instead use LineReader to maintain a sliding window over the corpus.\n", "Got same issue when dealing with wikipedia corpus.\nBut error message differs a bit:\n\n> W tensorflow/core/framework/op_kernel.cc:890] Invalid argument: wiki_combined.txt\n> E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: wiki_combined.txt\n> E tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Invalid argument: wiki_combined.txt\n\nTensorFlow v0.8.0rc0\n", "any update? the link [https://tensorflow-review.googlesource.com/#/c/1252/](https://tensorflow-review.googlesource.com/#/c/1252/) in #531 is invalid\n", "Maybe this helps someone: \nFor me, It still does not work on OS X using clang (7.3.0) but it works on CentOS 6 using GCC (4.8.2 and 4.9.2).\n", "I have to processes around 30 million tweets text with Tensorflow based word2vec implementation. I have data in json files saved on bases of daily streaming. Is there a way I can batch (queue) feed data from multiple files to word2vec? Please help!", "I am closing this issue due to inactivity.\r\nAlso, we moved word2vec to our models repo. \r\nIf this is still a problem, please file an issue in the models repository."]}, {"number": 1819, "title": "python cifar10_eval.py hang forever sometimes", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nLinux ubuntu 4.2.0-27-generic #32-Ubuntu SMP Fri Jan 22 04:49:08 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\njames@ubuntu:~/practice$ ls -l /usr/local/cuda*\nlrwxrwxrwx 1 root root    8 Mar 29 13:21 /usr/local/cuda -> cuda-7.5\n\n/usr/local/cuda-7.5:\ntotal 32\ndrwxr-xr-x  3 root root 4096 Mar 29 14:48 bin\ndrwxr-xr-x  2 root root 4096 Mar 29 12:16 doc\nlrwxrwxrwx  1 root root   28 Aug 16  2015 include -> targets/x86_64-linux/include\nlrwxrwxrwx  1 root root   24 Aug 16  2015 lib64 -> targets/x86_64-linux/lib\n-rw-r--r--  1 root root  365 Aug 16  2015 LICENSE\ndrwxr-xr-x  7 root root 4096 Mar 29 12:16 nvvm\n-rw-r--r--  1 root root  365 Aug 16  2015 README\ndrwxr-xr-x 11 root root 4096 Mar 29 14:48 samples\ndrwxr-xr-x  3 root root 4096 Feb  3 18:04 targets\n-rw-r--r--  1 root root   20 Aug 15  2015 version.txt\n\ninstalled from docker.\ntag   \"b.gcr.io/tensorflow/tensorflow:latest-devel-gpu\"\n(id:7f61540f94b2951574fd313b5980b850c7326933fc5a857779a36a94443c64cb)\n### Steps to reproduce\n1. pull images from docker\n   2.docker run -it -v /lib/modules/4.2.0-22-generic:/lib/modules/4.2.0-22-generic -v /lib/modules/4.2.0-23-generic:/lib/modules/4.2.0-23-generic -v /lib/modules/4.2.0-25-generic:/lib/modules/4.2.0-25-generic -v /lib/modules/4.2.0-27-generic:/lib/modules/4.2.0-27-generic -v /usr/lib/x86_64-linux-gnu/libcuda.so:/usr/lib/x86_64-linux-gnu/libcuda.so -v /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1 -v /usr/lib/x86_64-linux-gnu/libcuda.so.352.79:/usr/lib/x86_64-linux-gnu/libcuda.so.352.79 --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm -v /home/james:/home/james -P tensor-james\n2. python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_train.py \n3. stop train after some checkpoint file been written to disk\n4. python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_eval.py  --run_once true\n5. the last step hanging forever some times. \n### What have you tried?\n\n1.I add some logs to the cifar10_eval.py and it comes out that all threads of queue runner is active, and  it is hanging at the first call to   `predictions = sess.run([top_k_op])`\n\nprint([t.isAlive() for t in threads]);\n      while step < num_iter and not coord.should_stop():\n        print(\"begin step %d;\" %(step))\n        predictions = sess.run([top_k_op])\n        print(\"step %d: %d;\" %(step, true_count))\n\n[train_checkpoints.zip](https://github.com/tensorflow/tensorflow/files/209762/train_checkpoints.zip)\n### Logs or other output that would be helpful\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n...\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning N\nUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.291\npciBusID 0000:01:00.0\nTotal memory: 5.99GiB\nFree memory: 5.43GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\n... \nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 5.14GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x706400000 extends to 0x84ef38000\n", "comments": ["Did you pass in --checkpoint_dir=your-checkpoint-dir?\n\nSherry\n", "No user response. Closing as not reproducible. Please open a new bug if the issue persists.\n"]}, {"number": 1818, "title": "add reference for several concepts in tutorial", "body": "add a wiki or quora reference for several concepts in cnn tutorial\n", "comments": ["Can one of the admins verify this patch?\n", "@vrv , updated as (Chapter 3.3 in [AlexNet paper](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf))\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@caisq: we probably don't need to test doc changes :)  (awaiting for one more URL update and then I'll merge).\n", "@vrv , updated for the link, \ud83d\ude04 \n"]}, {"number": 1817, "title": "400 Bad Request caused by https://github.com/google/boringssl.git:", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nUbuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nyunlong@dl-y9:~/github/tensorflow$ ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Mar 19 15:48 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Mar 19 15:48 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Mar 19 15:48 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Mar 19 15:48 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Mar 19 15:48 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\ncommit 51f5f6be3ede7242b01c4bdf136de938fd00839f\n### Steps to reproduce\n\nyunlong@dl-y9:~/github/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR: /home/yunlong/github/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:207:1: no such package '@grpc//': Error cloning repository: https://github.com/google/boringssl.git: 400 Bad Request caused by https://github.com/google/boringssl.git: 400 Bad Request and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_server_lib'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.189s\n### What have you tried?\n\nyunlong@dl-y9:~/tmp$ git clone https://github.com/google/boringssl.git\nCloning into 'boringssl'...\nremote: Counting objects: 24195, done.\nremote: Compressing objects: 100% (117/117), done.\nReceiving objects:  12% (3019/24195), 2.64 MiB | 256.00 KiB/s     \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Note that v0.7.1 can be built successfully.\n", "Unfortunately it looks like this fell through the cracks.  @Yunlong-He: Do you know if it's still an issue?  @martinwicke: Is this a known problem? \n", "If the connectivity to github is patchy, these things happen. Those are\nusually transient issues though, sadly, bazel does not retry anything.\n\nOn Mon, Jun 6, 2016 at 5:21 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> Unfortunately it looks like this fell through the cracks. @Yunlong-He\n> https://github.com/Yunlong-He: Do you know if it's still an issue?\n> @martinwicke https://github.com/martinwicke: Is this a known problem?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1817#issuecomment-224127572,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_cp12zLYLXhVar3BSorIP-5Pobupks5qJLmMgaJpZM4ICoAr\n> .\n", "FWIW I'm getting the same error, and doesn't seem to be intermittent. I can repeatedly clone https://github.com/google/boringssl.git but always get the error:\n\n```\nError cloning repository: https://github.com/google/boringssl.git: 400 Bad Request caused by https://github.com/google/boringssl.git: 400 Bad Request and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_server_lib'.\n```\n\nwhen running\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n"]}, {"number": 1816, "title": "`tf.reverse_sequence()` doesn't work with arguments of totally unknown shape.", "body": "From [Stack Overflow](http://stackoverflow.com/questions/36480456/dynamic-rnn-and-array-ops-reverse-sequence-problems):\n\n> I am trying to reverse my inputs with array_ops.reverse_sequence() before sending it to dynamic_rnn(), the inference graph can be build with no problem, but when building the training graph, I got the following error:\n> \n> ```\n>   Traceback (most recent call last):\n>   File \"bin/trainer.py\", line 158, in <module>\n>     kmer_len=args.kmer_len)\n>   File \"/home/ubuntu/GIT/IvyMike/ivymike/base_model.py\", line 193, in run_training\n>     train_op = model.training(loss, learning_rate)\n>   File \"/home/ubuntu/GIT/IvyMike/ivymike/base_model.py\", line 100, in training\n>     train_op = optimizer.minimize(loss, global_step=global_step)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 190, in minimize\n>     colocate_gradients_with_ops=colocate_gradients_with_ops)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n>     colocate_gradients_with_ops=colocate_gradients_with_ops)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n>     in_grads = _AsList(grad_fn(op, *out_grads))\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_grad.py\", line 307, in _ReverseSequenceGrad\n>     seq_lengths=seq_lengths),\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1143, in reverse_sequence\n>     batch_dim=batch_dim, name=name)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n>     op_def=op_def)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2119, in create_op\n>     set_shapes_for_outputs(ret)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1586, in set_shapes_for_outputs\n>     shapes = shape_func(op)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1257, in _ReverseSequenceShape\n>     (batch_dim, input_shape.ndims))\n> TypeError: %d format: a number is required, not NoneType\n> ```\n\nThe problem appears to arise when `input_shape.ndims` is `None` (which is a valid possibility).\n", "comments": ["For some reasons I am not allowed to post my code here, but the bug occurs when I try to stack multiple layers of BLSTM using dynamic_rnn() (see this related request: https://github.com/tensorflow/tensorflow/issues/1779). For the backward pass, I first reversed the input using array_ops.reverse_sequence(), then reversed the output using the same function. I also used feed forward layers in between the BLSTMs. \n\nHowever, with one BLSTM layer, everything is fine.  \n\nThanks for all the help!\n", "I'm not certain that fixing this issue will solve #1779, but I have a simple fix to this issue in review so hopefully you'll be able to check soon!\n", "Great! Works like magic, solved all my problems! Thanks a lot!!\n"]}, {"number": 1815, "title": "feature request: \"a trou\"  (with hole algorithm)", "body": "To generate dense feature maps (e.g. semantic segmentation) the convolution and the maxpooling operators should have the option to define \"holes\" in the kernel.\nThe concept is used  in the paper:\nSemantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs\n(Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille)\n\nand it is implemented in the excellent  deeplab library based on Caffe.\n", "comments": ["I believe this is a duplicate of #889, which contains a workaround. I'll close this one, but we still have to fix it.\n", "It's not a duplicate, atrous convolution is adding holes in the input patch, whereas #889 is about dense patches but large strides.\n", "Hello Martin,\n\nThanks for the super quick reply.\nI attached a short PPT to illustrate the a-trou Algorithm.\n\nLet me know if you need additional details.\nYour tutorial on deep dream was great !!\n\nOn Thu, Apr 7, 2016 at 5:12 PM, Martin Wicke notifications@github.com\nwrote:\n\n> I believe this is a duplicate of #889\n> https://github.com/tensorflow/tensorflow/issues/889, which contains a\n> workaround. I'll close this one, but we still have to fix it.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1815#issuecomment-207145737\n", "For future reference, this is supported in Caffe master branch now and it is one of the features in Caffe rc3 https://github.com/BVLC/caffe/releases/tag/rc3. It is called dilation in convolution parameter and added by pull request https://github.com/BVLC/caffe/pull/3487.\n\nThere was detailed discussion about whether this feature should be named dilation, hole or atrous in this pull request https://github.com/BVLC/caffe/pull/3452. Relevant discussion started on Dec 18, 2015 in the pull request. This feature is eventually called dilation in convolution or dilated convolution following the term in Holschneider et al. (1987). The Caffe developers concluded that \"dilated convolution\" is a historically more accurate name than \"\u00e0 trous convolution\".\n", "@gpapan I assigned you based on your email to @yaroslavvb. Thanks!\n", "Thanks. This will be ready in the next few days. \n", "The change is in the process of being committed to github's Tensorflow.\n", "Thank @gpapan for adding this to TensorFlow!\n", "You are very welcome @fyu! Hope that people find this feature useful.\n", "Hi,\nThank you for having implemented the new feature. I'm doing some test and so far it looks great. What is the reason the regular stride parameter has been removed from the API ? \nDoes it make sense to have dilated input map processed with different overlapped receptive field (stride parameter) ?\n\nall the best\nLaurent\nSent from my iPhone\n\n> On Apr 27, 2016, at 8:16 PM, George Papandreou notifications@github.com wrote:\n> \n> You are very welcome @fyu! Hope that people find this feature useful.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n", "@laurentk67 \nI have not yet come across a use case that needs both stride > 1 and rate > 1, so I decided to omit the stride parameter from the API:\nYou typically use atrous_conv2d() in order to preserve the spatial resolution of your input feature map, whereas conv2d() with stride greater than one lowers the resolution. Not exposing conv2d's stride parameter thus prevents the user from using atrous convolution incorrectly. If some legitimate use case of atrous convolution comes up where both stride and rate may be greater than one, we can easily add a stride parameter and pass it along to conv2d().\n", "Having atrous convolution in TensorFlow is great - thank you @gpapan. I'd like access to strides for learning multi-scale raw audio filters, where using a stride of 1 is computationally infeasible.\n", "You should file a new issue with that feature request, I doubt it will be seen hidden in this thread.\n"]}, {"number": 1814, "title": "Implementing a Siamese Network", "body": "I want to implement a Siamese Convolutional Neural Network, where two images share weights in the convolutional layers, and are then concatenated before being passed through the fully-connected layers. I have tried an implementation, but it seems rather a \"hacked\" solution. In particular, I have defined an operation on tensors as simply a Python function, and I'm not sure whether this is allowed.\n\nHere is the code I have tried:\n\n```\nimages = tf.placeholder(tf.float32, shape=[None, 64 * 64])\n# Convolutional layers\n# ...\n# ...\n# Results in pool3_flat, which is the flattened output of the third convolutional layer\npool3_flat = tf.reshape(pool3, [-1, 8 * 8 * 128])\n\n# Now, merge the image pairs, where each pair is composed of adjacent images in the batch, with a stride of 2\ndef merge_pairs():\n  # Create a tensor to store the merged image pairs\n  # The batch size is 128, therefore there will be 64 pairs (64 in the first dimension of this tensor)\n  merged_pairs = tf.Variable(tf.zeros([64, 8 * 8 * 128]))\n  # Split the images into 64 pairs\n  pairs = tf.split(0, 64, pool3_flat)\n  # For each pair, concatenate the two images across dimension 1, and set this tensor in the appropriate row of merged_pairs\n  for pair_num, pair in enumerate(pairs):\n    merged_pair = tf.concat(1, pair)\n    merged_pairs[pair_num] = merged_pair\n\n# Proceed with operations on the merged_pair tensor, as if the batch size is 64\nfc4 = tf.matmul(merge_pairs, weights4)\n# ...\n# ...\n```\n\nWhen running this, I get the following error message:\n\n`TypeError: Expected binary or unicode string, got <function merge_pairs at 0x7f006edbed70>`\n\nSo it seems that I cannot simple replace a TensorFlow operation with a Python function. Is there a way to implement a Siamese network using built-in operations in TensorFlow?\n", "comments": ["You should make merge_pairs return its result, and call it in the call to matmul.\n\nThis type of question is generally better answered on StackOverflow. I will close this issue, we're trying to keep issues for bug reports.\n"]}, {"number": 1813, "title": "[feature request] port benchmarks to google benchmark", "body": "you have a simple benchmark framework in tensorflow/tensorflow/core/platform/test_benchmark.h but i was curious if you'd be interested in integrating with github.com/google/benchmark instead.\n\ni'm happy to help with the integration if you are.\n", "comments": ["We're working on a benchmarking framework and some infrastructure. I'll let @ebrevdo comment.\n", "Gchat me and we can discuss.\n", "@dominichamon, @ebrevdo: What was the result of the offline discussion?  \n", "No concrete plans to merge code, but backward compatible commits are welcome.\n", "Since no action has been taking in a while and it appears that there isn't compelling motivation to use the google framework, closing for now. @annarev, reopen if you feel this is likely to be a priority."]}, {"number": 1812, "title": "model parameters contains a lot of 'nan' at certain stage of training using AdamOptimizer", "body": "I am training a deep neural network using a simple and well defined case. The training goes very well and reach a high accuracy (~95%), and then, suddenly, just one step iteration, the accuracy is drop to almost zero. I then printed out all the model parameters, and there are lot of 'nan', something like this \n....0.076455489, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.08646702....\n\nHere is the information about my system.\n\n1) Installation. I installed tensor flow from source code on AWS ubuntu g2 instance (GPU instance). During the installation, I use customized configuration so that the tensor flow can run on g2 instance with computing capacity 3.0. \n\n2) DNN structure: I have a 30 hidden fully connected layers. Each layer has 32 nodes. To make the DNN more stable, I have added short-cuts for every three layers (similar to a recent publication from Microsoft Research \"Deep Residual Learning for Image Recognition\", by Kaiming He et al.) \n\n3) Optimizer: AdamOptimizer. \n  I first found the problem with AdamOptimzer. However, I just realized that the same problem also happens to GradientDescentOptimizer.\n\n4) Procedure to produce the bug. \n\nI have a saved model, which has good accuracy, and I restart the training by reading the saved model, and the problem shows up for the first iteration. \n\nI wonder if this is a known issue or not. I would be happy to provide my code and input data if this is necessary. I need to create a simple case with simplified code as I am working on a production code that I cannot post it here. \n", "comments": ["I'm not hugely experienced with either neural networks or Tensor Flow, but it sounds like it could be the \"vanishing gradients\" problem. When you have a very deep network (and30 layers is _very_ deep), the gradients in the loss function become very small, and I have seen elsewhere that this can cause NaN values. It sounds like in the single step you mention, the gradients reduce and cross a precision threshold, and therefore become undefined.\n", "As for a solution, you should just try decreasing the number of layers in your network. (Possibly) increasing the precision of your data might help, e.g. to ft.float64.\n", "This type of question is better handled at StackOverflow, please ask it there. We're trying to keep github issues for bug reports. Thanks.\n", "I report it as a bug. I will post a simple code for reproducing the bug.\n", "Yes, please post some code to reproduce. Getting NaNs during training is quite common and not usually a sign of a bug. A small code snippets that shows an actual bug will help a lot. I'll reopen this once you post code to reproduce.\n", "Hi Martin,\n\nHere is the code for reproducing the bug. I have made the code much simpler, and only have two hidden layers, a very moderate NN. \n\nTo run the code, type\n\npython TFDebug.py\n\nAfter about 5000 iterations, the bad model parameters will be printed out. \n\nI have tested it on all platforms that I have (ubuntu CPU, ubuntu GPU and Mac OS).\n\nAll show the same type of error. \n\nMany thanks.\n\nBWT, since I have the source code, if you can point out which codes are related to the problem, I can also try to debug it.\n\nJiabo\n\n[TFDebug.txt](https://github.com/tensorflow/tensorflow/files/209258/TFDebug.txt)\n", "Please use the check_numerics op to find out where the NaNs are first introduced. In particular, split the .optimize() call on the optizer into compute_gradients and apply_gradients, so you can look at the computed gradients. If you see any variable get really large or very small, that's a sign of trouble. \n\nThe distance function you wrote may be the cause, you could try making sure the value inside the square root is greater then 0 before you take the square root. \n", "I think it's a bug in your code rather than in TensorFlow. Your confidences converge to 0's and 1's, then you take a log of 0 which gives -Inf, and multiply it by 0, which gives NaN\n", "Thanks! I got too hypnotized by the sqrt to see the log. :)\n", "Thanks, I will take a closer look at the code.\n\nYaroslavb, could you explain your idea in a little more detail? I am not sure what you mean \"confidence converge to 0's and 1's. \n", "You are taking a tf.log(y), and you get a 0 as an element of y as you train\n", "Good point. Thanks.  However, y comes from a softmax function, which should always return a positive value, right?  I will check if some elements in y is actually rounded to zeros. Any simple trick to avoid log(0)? Add a very small positive number?\n\nI also check the distance function, and I don't think the problem comes from it. To make sure that is not a problem, I even put a check before taking sqrt. \n\nI will dig a little more deeper into the code, following Martin's suggestion. \n", "Mathematically, softmax can never be zero, but because we use floating point numbers, a number close enough to zero is represented as zero. Anyway, this is a common issue with neural networks, search for NaN and softmax\n", "Many thanks. I appreciate very much your comments!\n", "I have verified that the problem is indeed from log(y). I added a small constant to y, and the problem is solved. Thanks to all.\n"]}, {"number": 1811, "title": "Python 3 not installing in virtual environment", "body": "Using Tensorflow 0.7.1\n\nI have tried to install the Python 3 version of Tensorflow in a Virtual environment but I think it didn't work since the command\n\n$ python3 -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))' \n\nreturned\n\n$ /usr/local/lib/python3.4/dist-packages/tensorflow\n\ninstead of\n\n/home/dlm/tensorflow/local/lib/python3.4/site-packages/tensorflow\n\nI was sure I had activated the tensorflow virtual env before insstaling tensorflow with suport to python 3.\n", "comments": []}]