[{"number": 44240, "title": "Why is calling model.build() required to load  weights for a subclassed tf.keras.Model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): tf-nightly \r\n- TensorFlow version (use command below):2.4\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A \r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\nThis issue is related to models that subclass `tf.keras.Model`. When calling `model.load_weights(\"path.h5\")` why is it required to have called `model.build()` beforehand? This [line](https://github.com/tensorflow/tensorflow/blob/8bfaaccad9594e215765cf927bc51c4bf8c0aad8/tensorflow/python/keras/engine/training.py#L2221) checks whether `model.built == True` and fails to load the model if it is not. `model.built` gets set to `True` if `model.build()` is called. However the doc-string for `model.build()` makes it seem as if it is an optional call:\r\n\r\n```\r\nThis is to be used for subclassed models, which do not know at instantiation\r\ntime what their inputs look like.\r\nThis method only exists for users who want to call `model.build()` in a\r\nstandalone way (as a substitute for calling the model on real data to\r\nbuild it). It will never be called by the framework (and thus it will\r\nnever throw unexpected errors in an unrelated workflow).\r\n```\r\n\r\nIs there a reason that `model.built` is checked when loading model weights?\r\n\r\nI believe this issue appeared in TF2.3.", "comments": ["@aaron276h \r\nPlease share simple stand alone code to replicate issue faced or if possible share a colab gist with the issue.", "@Saduf2019 Added a script the reproduces the error in TF 2.3+\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.keras.engine import data_adapter\r\nfrom tensorflow.python.eager import backprop\r\n\r\n\r\n\r\ndef get_dataset():\r\n    (mnist_images, mnist_labels), _ = \\\r\n        tf.keras.datasets.mnist.load_data(path='mnist.npz')\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(\r\n        (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\r\n         tf.cast(mnist_labels, tf.int64))\r\n    )\r\n    dataset = dataset.repeat().shuffle(10000).batch(128)\r\n    return dataset\r\n\r\n\r\n\r\nclass MNIST(keras.Model):\r\n    def __init__(self):\r\n        super(MNIST, self).__init__()\r\n        self.mnist_model = keras.Sequential([\r\n            tf.keras.Input((28, 28, 1,)),\r\n            tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\r\n            tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\r\n            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n            tf.keras.layers.Dropout(0.25),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(128, activation='relu'),\r\n            tf.keras.layers.Dropout(0.5),\r\n            tf.keras.layers.Dense(10, activation='softmax')\r\n        ])\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        pass\r\n\r\n    def compile(self, my_opt, loss):\r\n        super(MNIST, self).compile()\r\n        self.my_opt = my_opt\r\n        self.loss_fn = loss\r\n\r\n    def train_step(self, data):\r\n        data = data_adapter.expand_1d(data)\r\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n\r\n        with backprop.GradientTape() as tape:\r\n            y_pred = self.mnist_model(x, training=True)\r\n            loss = self.loss_fn(y, y_pred, sample_weight)\r\n\r\n        grads = tape.gradient(loss, self.trainable_variables)\r\n        self.my_opt.apply_gradients(\r\n            zip(grads, self.trainable_variables)\r\n        )\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n        metrics = {m.name: m.result() for m in self.metrics}\r\n        metrics[\"loss\"] = loss\r\n        return metrics\r\n\r\n\r\ndef train():\r\n    model = MNIST()\r\n    optimizer = tf.keras.optimizers.Adam(0.001)\r\n\r\n    model.compile(\r\n        optimizer,\r\n        loss=tf.losses.SparseCategoricalCrossentropy()\r\n    )\r\n    dataset = get_dataset()\r\n\r\n    model.built = True\r\n    model.fit(\r\n        x=dataset.take(5),\r\n        epochs=1\r\n    )\r\n\r\n    model.save_weights(\"model-weights.h5\", save_format=\"h5\")\r\n\r\n\r\ndef load():\r\n    model = MNIST()\r\n    optimizer = tf.keras.optimizers.Adam(0.001)\r\n    model.compile(\r\n        optimizer,\r\n        loss=tf.losses.SparseCategoricalCrossentropy()\r\n    )\r\n\r\n    # If this line is commented out in TF 2.3+ the model fails to load.\r\n    # model.built = True\r\n\r\n    model.load_weights(\"model-weights.h5\")\r\n    print(\"Model loaded successfully.\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n    load()\r\n```", "@aaron276h \r\nCould you please refer to [this link](https://stackoverflow.com/questions/63658086/tensorflow-2-0-valueerror-while-loading-weights-from-h5-file) and let us know if it helps.", "@Saduf2019 thanks for the link. It seems to indicate that saving subclassed model weights with `h5` format is no longer supported which is a bit strange as that use case worked perfectly fine if users just set `model.built = True`. ", "The computation graph of custom objects such as subclassed models, custom metrics, custom layers, custom loss is not included in the saved file using `h5` format. \r\nSee https://www.tensorflow.org/guide/keras/save_and_serialize#limitations\r\nSavedModel formats saves the execution graph therefore we need not reinstantiate custom objects.\r\n", "Thanks for the explanation @ymodak "]}, {"number": 44239, "title": "Reintroduce TF to TF lowering patterns to legalize_tf pass for TFLite ", "body": "The TF lowering patterns are useful across multiple different backends. This PR adds the lowering patterns from TF to TFLite so that TFLite specific code can focus on lowerings unique to it. We can also automatically benefit from any TF lowerings that get added without having to manually sync it with TFLite if TFLite didn't use the lowering patterns of TF directly.\r\n\r\nThe PR was originally reverted for an unrelated issue which was fixed in https://github.com/tensorflow/tensorflow/pull/44097. It can be reintroduced now.", "comments": ["This seems to block nightly builds again. See following error message in CI:\r\n\r\n```\r\nINFO: From Testing //tensorflow/compiler/tests:stateless_random_ops_test_cpu_mlir_bridge_test:\r\n==================== Test output for //tensorflow/compiler/tests:stateless_random_ops_test_cpu_mlir_bridge_test:\r\nRunning tests under Python 3.6.12: /usr/local/bin/python3.6\r\nWARNING:tensorflow:From /buildfarm/default/operations/4d2cb36e-fb6e-43ce-a05c-32e283615e12/bazel-out/k8-opt/bin/tensorflow/compiler/tests/stateless_random_ops_test_cpu_mlir_bridge_test.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nXLA:CPU and XLA:GPU devices are deprecated\r\nW1026 18:11:56.800348 139974732013696 deprecation.py:339] From /buildfarm/default/operations/4d2cb36e-fb6e-43ce-a05c-32e283615e12/bazel-out/k8-opt/bin/tensorflow/compiler/tests/stateless_random_ops_test_cpu_mlir_bridge_test.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nXLA:CPU and XLA:GPU devices are deprecated\r\n[ RUN      ] StatelessRandomOpsTest.testDeterminism\r\nINFO:tensorflow:Start test case: StatelessRandomOpsTest.testDeterminism\r\nI1026 18:11:56.802196 139974732013696 xla_test.py:199] Start test case: StatelessRandomOpsTest.testDeterminism\r\n2020-10-26 18:11:56.817781: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-26 18:11:56.846378: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\r\n2020-10-26 18:11:56.851322: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55eb4f6412a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-26 18:11:56.851354: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-26 18:11:56.862759: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2020-10-26 18:11:56.874682: I tensorflow/compiler/jit/xla_device.cc:399] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(experimental_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.\r\n2020-10-26 18:11:56.880781: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_compile_on_demand_op.cc:171 : Internal: MLIR TF to XLA legalization failed<unknown>:0: error: The following operations cannot be legalized: tf.StatelessRandomGetKeyCounterAlg (count: 1). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.\r\n<unknown>:0: error: loc(\"stateless_random_uniform/StatelessRandomGetKeyCounterAlg\"): 'tf.StatelessRandomGetKeyCounterAlg' op is not legalizable\r\n<unknown>:0: note: loc(\"stateless_random_uniform/StatelessRandomGetKeyCounterAlg\"): see current operation: %0:3 = \"tf.StatelessRandomGetKeyCounterAlg\"(%arg0) {Tseed = i32, _XlaHasReferenceVars = false, device = \"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"} : (tensor<2xi32>) -> (tensor<1xui64>, tensor<2xui64>, tensor<i32>)\r\n<unknown>:0: error: A failure has been detected while processing the MLIR module, a reproducer has been generated in '/buildfarm/default/operations/4d2cb36e-fb6e-43ce-a05c-32e283615e12/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/stateless_random_ops_test_cpu_mlir_bridge_test/test.outputs/mlir_reproducer_n148-102-199-5a7642e6-83266-5b29029067726.mlir'\r\n<unknown>:0: note: see current operation: \"module\"() ( {\r\n  \"func\"() ( {\r\n  ^bb0(%arg0: tensor<2xi32>):  // no predecessors\r\n    %0:3 = \"tf.StatelessRandomGetKeyCounterAlg\"(%arg0) {Tseed = i32, _XlaHasReferenceVars = false, device = \"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"} : (tensor<2xi32>) -> (tensor<1xui64>, tensor<2xui64>, tensor<i32>)\r\n    \"std.return\"(%0#0, %0#1, %0#2) : (tensor<1xui64>, tensor<2xui64>, tensor<i32>) -> ()\r\n  }) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"_arg0\", outputs = \"_retval0,_retval1,_retval2\"}, type = (tensor<2xi32>) -> (tensor<1xui64>, tensor<2xui64>, tensor<i32>)} : () -> ()\r\n  \"module_terminator\"() : () -> ()\r\n}) {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 565 : i32}} : () -> ()\r\n\r\nINFO:tensorflow:time(__main__.StatelessRandomOpsTest.testDeterminism): 0.09s\r\nI1026 18:11:56.888627 139974732013696 test_util.py:2076] time(__main__.StatelessRandomOpsTest.testDeterminism): 0.09s\r\nINFO:tensorflow:End test case: testDeterminism\r\nI1026 18:11:56.888831 139974732013696 xla_test.py:206] End test case: testDeterminism\r\n[  FAILED  ] StatelessRandomOpsTest.testDeterminism\r\n[ RUN      ] StatelessRandomOpsTest.testDistributionOfStatelessRandomNormal\r\nINFO:tensorflow:Start test case: StatelessRandomOpsTest.testDistributionOfStatelessRandomNormal\r\nI1026 18:11:56.891267 139974732013696 xla_test.py:199] Start test case: StatelessRandomOpsTest.testDistributionOfStatelessRandomNormal\r\n2020-10-26 18:11:56.905228: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_compile_on_demand_op.cc:171 : Internal: MLIR TF to XLA legalization failed<unknown>:0: error: The following operations cannot be legalized: tf.StatelessRandomGetKeyCounterAlg (count: 1). These legalization failure(s) may be due to missing TF to HLO lowerings and/or unsupported attributes, etc.\r\n<unknown>:0: error: loc(\"stateless_random_normal/StatelessRandomGetKeyCounterAlg\"): 'tf.StatelessRandomGetKeyCounterAlg' op is not legalizable\r\n<unknown>:0: note: loc(\"stateless_random_normal/StatelessRandomGetKeyCounterAlg\"): see current operation: %0:3 = \"tf.StatelessRandomGetKeyCounterAlg\"(%arg0) {Tseed = i32, _XlaHasReferenceVars = false, device = \"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"} : (tensor<2xi32>) -> (tensor<1xui64>, tensor<2xui64>, tensor<i32>)\r\n<unknown>:0: error: A failure has been detected while processing the MLIR module, a reproducer has been generated in '/buildfarm/default/operations/4d2cb36e-fb6e-43ce-a05c-32e283615e12/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/stateless_random_ops_test_cpu_mlir_bridge_test/test.outputs/mlir_reproducer_n148-102-199-5a7642e6-83266-5b2902906d80a.mlir'\r\n<unknown>:0: note: see current operation: \"module\"() ( {\r\n  \"func\"() ( {\r\n  ^bb0(%arg0: tensor<2xi32>):  // no predecessors\r\n    %0:3 = \"tf.StatelessRandomGetKeyCounterAlg\"(%arg0) {Tseed = i32, _XlaHasReferenceVars = false, device = \"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"} : (tensor<2xi32>) -> (tensor<1xui64>, tensor<2xui64>, tensor<i32>)\r\n    \"std.return\"(%0#0, %0#1, %0#2) : (tensor<1xui64>, tensor<2xui64>, tensor<i32>) -> ()\r\n  }) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"_arg0\", outputs = \"_retval0,_retval1,_retval2\"}, type = (tensor<2xi32>) -> (tensor<1xui64>, tensor<2xui64>, tensor<i32>)} : () -> ()\r\n  \"module_terminator\"() : () -> ()\r\n}) {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 565 : i32}} : () -> ()\r\n\r\n```", "Thanks for reporting this. Let me look into this.", "It seems to have broken by the following commit. Let me fix it. https://github.com/tensorflow/tensorflow/commit/9745e24f30cc54b35af85936eab277c1dc62643f", "You are correct. Simply reverting this PR doesn't fix the issue. Compatibility horizon seems to bite again..", "cc @wangpengmit for 6d605dde", "Ah, the newly added V2 stateless ops kicked in when compatibility horizon expired. There is a lowering in the old tf2xla bridge: https://github.com/tensorflow/tensorflow/blob/547bd9c88b1a86f0543fff3460e2d4d1c8009cb4/tensorflow/compiler/tf2xla/kernels/stateless_random_ops_v2.cc#L482 . Can you do a similar thing in the MLIR bridge?", "This should have got fixed with https://github.com/tensorflow/tensorflow/commit/337d20c7f07139d8e00e67a58b4f5d771e1f313a.", "Thanks!", "cc @ahmedsabie it seems to be reverted again in da6eb9c93d023de5e9613efeb42c7d8e723f26e5."]}, {"number": 44238, "title": "Iterative Horizontal Fusion", "body": "\r\nIn this PR, we\r\n1. Extend Horizontal Fusion to support standalone elementwise and reduce instructions.\r\n2. Iteratively run the horizontal fusion, so that it can perform horizontal fusion in a layer-by-layer manner.\r\n\r\nNote that because the current algorithm performs horizontal fusion for candidates who share a common consumer, each iteration of horizontal fusion will naturally expose more horizontal fusion opportunities for future iterations as the consumers might be fused into a fusion! As such , it works in a layer-by-layer fusion manner.\r\n\r\nAn example is constructed as in `IterativeHorizontalFusion` in horizontal_loop_fusion_test.cc.\r\n\r\nWe observe this layer-by-layer pattern in some training optimizers and iterative fusion works fine for the cases we see.\r\n", "comments": ["@thomasjoerg could you help to review this PR? See the PR description for general info. Thanks!", "> @thomasjoerg could you help to review this PR? See the PR description for general info. Thanks!\r\n\r\nOr recommend someone? :-)\r\n", "Let's reopen this later."]}, {"number": 44236, "title": "[Cherrypick] Force document private methods/functions", "body": "PiperOrigin-RevId: 338486134\r\nChange-Id: I62efc520652e6db7d5f84cbfc55a6def48f5be4f", "comments": []}, {"number": 44234, "title": "TensorFlow Lite Crashed when calling void tflite::reference_ops::Gather<long, int>", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04 and Android10\r\n- TensorFlow installed from (source or binary):\r\nbinary tf_nightly\r\n- TensorFlow version (or github SHA if from source):\r\n2.4.0-dev20201005\r\n- TensorFlow Lite version:\r\ntensorflow-lite:0.0.0-nightly and also a debug build jni so for the backtrack in C.\r\n\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n[inference.tflite.zip](https://github.com/tensorflow/tensorflow/files/5423912/inference.tflite.zip)\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- The conversion was successful and I was able to invoke the generated tflite model on my desktop with Python API. However, it crashed when invoking with Java code in the Android App. I have compiled a debug JNI so and attached the native  backtrace below. It seems something wrong with the `Gather ` function.\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nsignal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------\r\n    x0  0000000000000000  x1  00000000000078df  x2  0000000000000006  x3  0000007d53ac2410\r\n    x4  0000007d446feb40  x5  0000007d446feb40  x6  0000007d446feb40  x7  000000010000012c\r\n    x8  00000000000000f0  x9  17de649dc883d4e6  x10 0000000000000001  x11 0000000000000000\r\n    x12 fffffff0fffffbdf  x13 0000000000000070  x14 0000000000000088  x15 0000007d53ac2528\r\n    x16 0000007e4192f738  x17 0000007e4190dd20  x18 000000000000000a  x19 00000000000078af\r\n    x20 00000000000078df  x21 00000000ffffffff  x22 0000007d53ac3350  x23 0000007db1589c22\r\n    x24 0000000000000010  x25 0000007d53ac5020  x26 0000007da4c39cb0  x27 0000000000000004\r\n    x28 0000007d53ac30d0  x29 0000007d53ac24b0\r\n    sp  0000007d53ac23f0  lr  0000007e418bf404  pc  0000007e418bf430\r\n\r\nbacktrace:\r\n      #00 pc 0000000000073430  /apex/com.android.runtime/lib64/bionic/libc.so (abort+160) (BuildId: 084c8a81b8c78e19cd9a1ff6208e77cf)\r\n      #01 pc 0000000000238b54  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (void tflite::reference_ops::Gather<long, int>(tflite::GatherParams const&, tflite::RuntimeShape const&, long const*, tflite::RuntimeShape const, int const*, tflite::RuntimeShape const, tflite::RuntimeShape const&*)+536)\r\n      #02 pc 0000000000236ec0  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (TfLiteStatus tflite::ops::builtin::gather::Gather<long, int>(TfLiteGatherParams const&, TfLiteTensor const*, TfLiteTensor const, TfLiteGatherParams const&*)+208)\r\n      #03 pc 00000000002365e0  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::ops::builtin::gather::Eval(TfLiteContext*, TfLiteNode*)+448)\r\n      #04 pc 00000000003af5b4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*)+96)\r\n      #05 pc 00000000003aecd4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::Invoke()+1396)\r\n      #06 pc 0000000000335a48  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::ops::builtin::while_kernel::Eval(TfLiteContext*, TfLiteNode*)+1048)\r\n      #07 pc 00000000003af5b4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*)+96)\r\n      #08 pc 00000000003aecd4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Subgraph::Invoke()+1396)\r\n      #09 pc 00000000003ba72c  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (tflite::Interpreter::Invoke()+88)\r\n      #10 pc 00000000000d22f4  /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+152)\r\n      #11 pc 000000000013f350  /apex/com.android.runtime/lib64/libart.so (art_quick_generic_jni_trampoline+144) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #12 pc 00000000001365b8  /apex/com.android.runtime/lib64/libart.so (art_quick_invoke_static_stub+568) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #13 pc 000000000014500c  /apex/com.android.runtime/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+276) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #14 pc 00000000002e27cc  /apex/com.android.runtime/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+384) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #15 pc 00000000002dda2c  /apex/com.android.runtime/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+892) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #16 pc 00000000005a2824  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+372) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #17 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #18 pc 000000000018c28e  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+166)\r\n      #19 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #20 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #21 pc 000000000018b812  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\n      #22 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #23 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #24 pc 00000000001896ae  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.ImageClassifierInference.runInference+394)\r\n      #25 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #26 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #27 pc 0000000000189be6  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.ImageClassifier.classifyFrame+70)\r\n      #28 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #29 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #30 pc 00000000001880c2  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame+150)\r\n      #31 pc 00000000005a231c  /apex/com.android.runtime/lib64/libart.so (MterpInvokeDirect+1100) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #32 pc 0000000000130914  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #33 pc 0000000000187fcc  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.Camera2BasicFragment.access$1000)\r\n      #34 pc 00000000005a2ac0  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+1040) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #35 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #36 pc 00000000001878ae  [anon:dalvik-classes.dex extracted in memory from /data/app/android.example.com.tflitepillardemo-2m8RGRJS-L6Uzg6DwrPrxA==/base.apk] (com.example.android.tflitecamerademo.Camera2BasicFragment$8.run+34)\r\n      #37 pc 00000000005a1830  /apex/com.android.runtime/lib64/libart.so (MterpInvokeInterface+1788) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #38 pc 0000000000130a14  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #39 pc 0000000000320014  /system/framework/framework.jar (android.os.Handler.handleCallback+4)\r\n      #40 pc 00000000005a2ac0  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+1040) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #41 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #42 pc 000000000031fe80  /system/framework/framework.jar (android.os.Handler.dispatchMessage+8)\r\n      #43 pc 00000000005a0010  /apex/com.android.runtime/lib64/libart.so (MterpInvokeVirtual+1352) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #44 pc 0000000000130814  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #45 pc 0000000000344e14  /system/framework/framework.jar (android.os.Looper.loop+484)\r\n      #46 pc 00000000005a2ac0  /apex/com.android.runtime/lib64/libart.so (MterpInvokeStatic+1040) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #47 pc 0000000000130994  /apex/com.android.runtime/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #48 pc 000000000031f59c  /system/framework/framework.jar (android.os.HandlerThread.run+56)\r\n      #49 pc 00000000002b3ae0  /apex/com.android.runtime/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.17460956533834400288+240) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #50 pc 00000000005912b8  /apex/com.android.runtime/lib64/libart.so (artQuickToInterpreterBridge+1032) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #51 pc 000000000013f468  /apex/com.android.runtime/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #52 pc 0000000000136334  /apex/com.android.runtime/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #53 pc 0000000000144fec  /apex/com.android.runtime/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+244) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #54 pc 00000000004afcbc  /apex/com.android.runtime/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #55 pc 00000000004b0dd0  /apex/com.android.runtime/lib64/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue const*)+416) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #56 pc 00000000004f178c  /apex/com.android.runtime/lib64/libart.so (art::Thread::CreateCallback(void*)+1176) (BuildId: d409c858261cc9eed63528a22714bb15)\r\n      #57 pc 00000000000d6cb0  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+36) (BuildId: 084c8a81b8c78e19cd9a1ff6208e77cf)\r\n      #58 pc 0000000000074eac  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (BuildId: 084c8a81b8c78e19cd9a1ff6208e77cf)\r\n```\r\n", "comments": ["@objectc \r\nPlease share simple stand alone code such that we can replicate the issue faced or is possible share a  colab gist with  the error reported.", "\r\n> @objectc\r\n> Please share simple stand alone code such that we can replicate the issue faced or is possible share a colab gist with the error reported.\r\n\r\nIt was in an Android project written with Java code, I have attached an Android project [tflite_crash.zip](https://github.com/tensorflow/tensorflow/files/5430293/tflite_crash.zip) for you to replicate the issue. The following code snippet is the core part of the project for you to review conveniently.  Just create the interpreter and invoke it, then it will crash. \r\n\r\nLet me know if you need any other information. Thanks\r\n\r\n``` Java\r\n    float[][][] inferenceOutput1 = new float[1][300][4];\r\n    float[][] inferenceOutput2 = new float[1][300];\r\n    int[][] inferenceOutput3 = new int[1][300];\r\n    Map<Integer, Object> inferenceOutputs = new HashMap<>();\r\n    inferenceOutputs.put(0, inferenceOutput1);\r\n    inferenceOutputs.put(1, inferenceOutput2);\r\n    inferenceOutputs.put(2, inferenceOutput3);\r\n    \r\n    Object[] inferenceInputs = {new float[1][360045][4], new float[1][360045][2]};\r\n    interpreter.runForMultipleInputsOutputs(inferenceInputs,\r\n            inferenceOutputs);\r\n```", "@thaink could you take a look?", "Sure. The indices in gather should not be random data.\r\nI'll check if the cause is invalid indices here.", "I can run this model successfully in C++. So the issue seem to be the input data.\r\nCan you try to set the same input as you used in python side? In your snipet, the input data is all zeros.\r\n\r\nIf you believe the error is in Gather kernel, could you provide a simpler model/example to reproduce it.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44234\">No</a>\n"]}, {"number": 44233, "title": "[ROCm] Fix for ROCm CSB Breakage - 201022", "body": "The following commit introduces a failure on in the `//tensorflow/python/kernel_tests:linalg_grad_test_gpu` test on the ROCm Platform\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/3ce466a482f3ef247882e069e810d79913cfe940\r\n\r\nThe failure is in the `MatrixExponentialGradient` subtest, and the errors we get are of the following form\r\n\r\n```\r\n======================================================================\r\nERROR: test_MatrixExponentialGradient_float64_5_5 (__main__.MatrixUnaryFunctorGradientTest)\r\ntest_MatrixExponentialGradient_float64_5_5 (__main__.MatrixUnaryFunctorGradientTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/linalg_grad_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/linalg_grad_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1360, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/linalg_grad_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found: No registered 'MatrixSolve' OpKernel for 'GPU' devices compatible with node {{node MatrixSolve}}\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[MatrixSolve]]\r\n\t [[matrix_exponential_1/cond/PartitionedCall/matrix_exponential_1/cond]]\r\n\t [[PartitionedCall/gradients/matrix_exponential_1/cond_grad/StatelessIf/then/_22/gradients/Neg_grad/Neg/_73]]\r\n  (1) Not found: No registered 'MatrixSolve' OpKernel for 'GPU' devices compatible with node {{node MatrixSolve}}\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[MatrixSolve]]\r\n\t [[matrix_exponential_1/cond/PartitionedCall/matrix_exponential_1/cond]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\n```\r\n\r\nThe regression was fixed by the subsequent commit, https://github.com/tensorflow/tensorflow/commit/7d57263720c7d2c88861f2f325e8ca1bd02deb38\r\n\r\nand then re-introduced when parts of the above commits were rolled back by the following commit\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/fb22dff31744a9fdc571f6c2bf8743eb2844eb95\r\n\r\nThe regression seems to occur on the ROCm platform because the \"MatrixSolve\" operator is currently only enabled for the CUDA platform for GPUs, and not on the ROCm platform.\r\n\r\nThis commit is to temporarily disable the subtest on the ROCm platform, to get the ROCm CSB to pass. It can be reverted once the reverted changes are put back in.\r\n\r\n\r\n--------------------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": []}, {"number": 44232, "title": "TFlite interpreter fails to load a saved tflite model when Dropout is used!", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (or github SHA if from source): v 2.3.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-74-3ae2f98ec211> in <module>\r\n----> 1 interpreter = tf.lite.Interpreter(model_path=tflite_file)\r\n      2 translate_tflite(\"este \u00e9 o primeiro livro que eu fiz\",tokenizer_pt,tokenizer_en,interpreter,MAX_LENGTH)\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates, num_threads)\r\n    196       self._interpreter = (\r\n    197           _interpreter_wrapper.CreateWrapperFromFile(\r\n--> 198               model_path, self._custom_op_registerers))\r\n    199       if not self._interpreter:\r\n    200         raise ValueError('Failed to open {}'.format(model_path))\r\n\r\nValueError: Did not get operators, tensors, or buffers in subgraph 1.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**It is a transformer-based machine translation. The code is the one implemented in Tensorflow tutorials page:**\r\nhttps://www.tensorflow.org/tutorials/text/transformer\r\n\r\n**The code for saving and loading the tflite model is as follows:**\r\n```\r\n# Load TFLite model and allocate tensors.\r\ndef evaluate_tflite(inp_sentence,tokenizer_pt,tokenizer_en,interpreter,max_length):\r\n  start_token = [tokenizer_pt.vocab_size]\r\n  end_token = [tokenizer_pt.vocab_size + 1]\r\n  \r\n  # TODO: languague change [check for erros]\r\n  # inp sentence is lev, hence adding the start and end token\r\n  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\r\n  encoder_input = tf.expand_dims(inp_sentence, 0)\r\n  \r\n  # as the target is english, the first word to the transformer should be the\r\n  # english start token.\r\n  decoder_input = [tokenizer_en.vocab_size]\r\n  output = tf.expand_dims(decoder_input, 0)\r\n  \r\n  for i in range(max_length):\r\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\r\n        encoder_input, output)\r\n    \r\n    # Use interpreter for inference\r\n    # print (input_details)\r\n#     interpreter = tf.lite.Interpreter(model_path=tflite_file)\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    interpreter.resize_tensor_input(input_details[0]['index'], encoder_input.shape)\r\n    interpreter.resize_tensor_input(input_details[1]['index'], output.shape)\r\n    interpreter.resize_tensor_input(input_details[3]['index'], enc_padding_mask.shape)\r\n    interpreter.resize_tensor_input(input_details[4]['index'], combined_mask.shape)\r\n    interpreter.resize_tensor_input(input_details[5]['index'], dec_padding_mask.shape)\r\n    interpreter.allocate_tensors()\r\n\r\n    # input_details = interpreter.get_input_details()\r\n    # output_details = interpreter.get_output_details()\r\n    interpreter.set_tensor(input_details[0]['index'], tf.cast(encoder_input, tf.float32))\r\n    interpreter.set_tensor(input_details[1]['index'], tf.cast(output, tf.float32))\r\n    interpreter.set_tensor(input_details[2]['index'], [False])\r\n    interpreter.set_tensor(input_details[3]['index'], enc_padding_mask)\r\n    interpreter.set_tensor(input_details[4]['index'], combined_mask)\r\n    interpreter.set_tensor(input_details[5]['index'], dec_padding_mask)\r\n    interpreter.invoke()\r\n\r\n    # print(\"Inference worked!\")\r\n    \r\n\r\n    # [print(d) for d in output_details]\r\n\r\n    # The function `get_tensor()` returns a copy of the tensor data.\r\n    # Use `tensor()` in order to get a pointer to the tensor.\r\n    predictions = interpreter.get_tensor(output_details[0]['index'])\r\n    attention_weights = interpreter.get_tensor(output_details[1]['index'])\r\n    \r\n\r\n    \r\n    # select the last word from the seq_len dimension\r\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\r\n\r\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n    \r\n    # return the result if the predicted_id is equal to the end token\r\n    if predicted_id == tokenizer_en.vocab_size+1:\r\n      return tf.squeeze(output, axis=0), attention_weights\r\n    \r\n    # concatentate the predicted_id to the output which is given to the decoder\r\n    # as its input.\r\n    output = tf.concat([output, predicted_id], axis=-1)\r\n\r\n  return tf.squeeze(output, axis=0), attention_weights\r\n\r\n\r\ndef translate_tflite(sentence, tokenizer_pt,tokenizer_en, interpreter, max_length, plot=False):\r\n  result, attention_weights = evaluate_tflite(sentence, tokenizer_pt,tokenizer_en, interpreter, max_length)\r\n  \r\n  predicted_sentence = tokenizer_en.decode([i for i in result \r\n                                            if i < tokenizer_en.vocab_size])  \r\n\r\n  print('Input: {}'.format(sentence))\r\n  print('Predicted translation: {}'.format(predicted_sentence))\r\n  \r\n  if plot:\r\n    plot_attention_weights(attention_weights, sentence, result, plot)\r\n  \r\n  return predicted_sentence\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=tflite_file)\r\ntranslate_tflite(\"este \u00e9 o primeiro livro que eu fiz\",tokenizer_pt,tokenizer_en,interpreter,MAX_LENGTH)\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nWhen I remove Dropout layers from, the interpreter works well and loads and interprets the tflite model well.\r\nDo you have any idea how I can incorporate Dropout?", "comments": ["Dropout is not supported by TFLite because usually it's of use during inference. Why you want to use Dropout during inference?", "@freedomtan \r\nI do not want to use it during inference. I just want to use it for training. But the point is that when I add Dropout layer to the model architecture, and train the model, and save it using tflite compressor, It will not be loaded anymore for inference and raise the above exception.", "when you save a model for inference, it should not contain Dropout :-)", "It does not seem to be a straightforward step!\r\nDo you have an easier solution other than the ones suggested here : (removing the node from the graph)\r\nhttps://github.com/tensorflow/tensorflow/issues/5867\r\n\r\nthanks,", "I solved it. In the call functions, put a condition: if it is on inference mode, do not call Dropout at all. (The training option that is passed as an argument to the Dropout class was not enough for me.)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44232\">No</a>\n", "Hi @Arman-IMRSV, I have the exact same issue. Thanks for pointing out a workaround!\r\n\r\nI do have a question, how did you save you model after training? Because you now have this condition in the call function, you have to set it to true for training. If you just save the model after training, the model will still have dropout in it right?", "@HongtaoYang This is how I implemented the inference function:\r\n```\r\n    @tf.function(\r\n        experimental_relax_shapes=True,\r\n        input_signature=[\r\n            tf.TensorSpec(shape=[1, None], dtype=tf.float32),\r\n            tf.TensorSpec(shape=[1, None], dtype=tf.float32),\r\n            tf.TensorSpec(shape=(1), dtype=tf.bool),\r\n            tf.TensorSpec(shape=(1,1,1,None), dtype=tf.float32),\r\n            tf.TensorSpec(shape=(1,1,None,None), dtype=tf.float32),\r\n            tf.TensorSpec(shape=(1,1,1,None), dtype=tf.float32)\r\n        ],\r\n    )\r\n    def tflite_infer(self, inp, tar, training, enc_padding_mask, \r\n                     look_ahead_mask, dec_padding_mask):\r\n\r\n        ##IMPORTANT! DON'T REMOVE...\r\n        training = False\r\n\r\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\r\n\r\n        dec_output, attention_weights = self.decoder(\r\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\r\n\r\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\r\n\r\n        return final_output, attention_weights\r\n```\r\nWhen calling the trainer, you will set it to true. I saved it to TFLite and dropout is not put into it's graph as it is in inference time."]}, {"number": 44231, "title": "Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 20.04 Focal\r\n- TensorFlow installed from (source or binary): Source as also used in nightly build from 20201021; also tested the official tf-nightly-gpu.dev20201021 build installed with pip\r\n- TensorFlow version (use command below): 2.4.0 (pre-release due to source build)\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11 / 8\r\n- GPU model and memory: RTX 2080, 8 GB VRAM\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhenever `RaggedTensor` objects are returned from a nested `tf.map_fn` call, Tensorflow is unable to find a GPU implementation or some \"unary variant unary_op function\" by given the following `InternalError`:\r\n\r\n```\r\n2020-10-22 12:00:57.819438: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at constant_op.cc:243 : Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU\r\nTraceback (most recent call last):\r\n  File \"/home/julilien/Programming/tf_gpu_issue.py\", line 23, in <module>\r\n    optimizer.minimize(f, [x])\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 496, in minimize\r\n    grads_and_vars = self._compute_gradients(\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 539, in _compute_gradients\r\n    loss = loss()\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 894, in _call\r\n    return self._concrete_stateful_fn._call_flat(\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1933, in _call_flat\r\n    forward_backward.record(flat_outputs)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1458, in record\r\n    self._functions.record(\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1302, in record\r\n    backward_function, to_record = self._wrap_backward_function(\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1250, in _wrap_backward_function\r\n    variant_zeros_like[output_index] = default_gradient.zeros_like(output)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/default_gradient.py\", line 57, in zeros_like\r\n    return array_ops.zeros_like(t)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2921, in zeros_like\r\n    return zeros_like_impl(tensor, dtype, name, optimize)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2982, in zeros_like_impl\r\n    return gen_array_ops.zeros_like(tensor, name=name)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 12442, in zeros_like\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/julilien/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU [Op:ZerosLike]\r\n```\r\n\r\nOn CPU, however, it works without any issue, e.g., when I explicitly disable the GPU support (by setting the `CUDA_VISIBLE_DEVICES` to `-1`). I've also observed the same behavior in a \"conventional\" training setting within Keras training code. According to [this](https://github.com/tensorflow/tensorflow/issues/42047#issuecomment-691208579) comment, I would have expected this error not being an issue due to the CUDA version that I've used.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect to be able to run this code on both CPU and GPU providing the same results.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI was able to (successfully) run this code on my CPU but not on a GPU (Nvidia RTX 2080, Driver Version: 455.23.05, CUDA Version: 11 with cuDNN 8).\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    x = tf.Variable([1., 2., 3.], name='x', trainable=True, dtype=tf.float32)\r\n\r\n\r\n    @tf.function\r\n    def f():\r\n        seg_data = tf.constant([[0, 0, 1], [0, 1, 2]])\r\n\r\n        def seg_batch_fn(loc_segs):\r\n            return tf.math.unsorted_segment_prod(x, loc_segs, tf.reduce_max(loc_segs) + 1)\r\n\r\n        return tf.reduce_sum(tf.map_fn(seg_batch_fn, seg_data,\r\n                                       fn_output_signature=tf.RaggedTensorSpec(shape=[None],\r\n                                                                               dtype=tf.float32), name=\"seg_batch_fn\"))\r\n\r\n\r\n    optimizer = tf.optimizers.SGD()\r\n    optimizer.minimize(f, [x])\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2020-10-22 11:29:44.986842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-22 11:29:46.802265: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-22 11:29:46.802836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-10-22 11:29:46.858113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 11:29:46.858476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2020-10-22 11:29:46.858494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-22 11:29:46.860514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-10-22 11:29:46.860564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-10-22 11:29:46.861269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-22 11:29:46.861446: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-22 11:29:46.863415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-10-22 11:29:46.863890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-22 11:29:46.863996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-22 11:29:46.864087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 11:29:46.864466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 11:29:46.865040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-10-22 11:29:46.865065: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-22 11:29:47.286906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-22 11:29:47.286929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-10-22 11:29:47.286934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-10-22 11:29:47.287113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 11:29:47.287681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 11:29:47.287996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 11:29:47.288286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6942 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-22 11:29:50.715204: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-10-22 11:29:50.737811: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz\r\nEpoch 1/40\r\n2020-10-22 11:30:03.130840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-22 11:30:04.518337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-10-22 11:30:04.822802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-10-22 11:30:17.122236: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at constant_op.cc:243 : Internal: No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU\r\n```\r\n", "comments": ["Was able to reproduce your issue in Tensorflow GPU 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2e6636659aaedfef6550905d9fbea798/44231.ipynb). Thanks!", "Closing as I believe this is a duplicate of https://github.com/tensorflow/tensorflow/issues/46635.  Please reopen if you disagree.\r\n\r\nSeparately @julilien, are you willing to contribute a PR fixing this issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44231\">No</a>\n"]}, {"number": 44230, "title": "AttributeError: 'Tensor' object has no attribute 'numpy'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0\r\n- Python version:  3.7.x\r\n\r\n\r\n**Describe the current behavior**\r\n got errors\r\n\r\n**Describe the expected behavior**\r\nprint out the tensor value\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef f(x, y):\r\n  output = 1.0\r\n  for i in range(y):\r\n    if i > 1 and i < 5:\r\n      output = tf.multiply(output, x)\r\n  return output\r\n\r\ndef grad(x, y):\r\n  with tf.GradientTape() as t:\r\n    t.watch(x)\r\n    out = f(x, y)\r\n  return t.gradient(out, x)\r\n\r\nx = tf.convert_to_tensor(2.0)\r\nprint(\"result: \", grad(x, 6).numpy())\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tf.py\", line 17, in <module>\r\n    print(\"result: \", grad(x, 6).numpy())\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "comments": ["The bug you report doesn't exist, please check this gist :https://colab.research.google.com/drive/1kezUav5qkO3uY6AuvqUWLEx0iKrXYN3Q?usp=sharing", "@liangzelang \r\nI ran the code shared on tf 2.3 and on tf 2.0 as reported and do not face any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/c30abd199b4e580ae5891b88c00e88b7/untitled450.ipynb).\r\n\r\nAs per the error reported: please refer to : [link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy), [link1](https://github.com/tensorflow/tensorflow/issues/27519).\r\n\r\nIf the issue still exist share a colab gist with the error reported, else please move the issue to closed status. Thanks!", "thanks,  it works by adding  `tf.enable_eager_execution()` . \r\nAnd I found my tensorflow version in conda is 1.14.0, so maybe this issue does not exist in tf 2.0\r\nthank you  @Saduf2019  @fuhailin ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44230\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44230\">No</a>\n", "I am facing a similar issue. Attentions of Graph Convolution Transformer are in form of tensors, I am getting this error while converting them to numpy"]}, {"number": 44229, "title": "Inlcude Ubuntu 20.04 in GPU Support Linux Setup", "body": "## URL(s) with the issue:\r\n[GPU Setup -- Linux setup](https://www.tensorflow.org/install/gpu#linux_setup)\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe documentation for adding GPU support has descriptions for ubuntu 18.04 and 16.04 **but not ubuntu 20.04**. It's been a while since ubuntu 20.04 has been released and I think adding the instructions for ubuntu 20.04 will be helpful. I followed the article [Installing TensorFlow GPU in Ubuntu 20.04](https://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d) to set up GPU support on my system. Adding something along the lines would be great.\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue?\r\n\r\nI'm willing to work on this.", "comments": ["@Harsh188 Are you planning to submit a PR to update the docs? Thanks!", "@jvishnuvardhan Yep. I'll do this shortly. I'm not sure how this will change with TF 2.4 since CUDA 11 will be supported. I've been having trouble myself trying to get TF to use my RTX 3080. ", "On the official Nvidia CUDA toolkit 10.1 archive, there is no support for ubuntu 20.04 which makes me think that it's not recommended to install CUDA 10.1 for Ubuntu 20.04. However, certain articles exist that show you how to do it. I followed one of these articles hoping that it would work but when I ran `tf.config.list_physical_devices('GPU')` it resulted in an error. \r\n\r\nI was recommended to use tf-nightly, which works with CUDA 11.1 and is supported on ubuntu 20.04. I'm not too sure when TF 2.4 will be released and if we should bother including a GPU installation doc support for ubuntu 20.04 since this issue will be resolved with TF 2.4. \r\n\r\n@mihaimaruseac  could you comment on this topic?", "TF 2.4 is in the release phase now, we already cut the branch", "In that case, I think it's best to wait."]}, {"number": 44228, "title": "build tensorflow lite failed ", "body": "Hi tensorflow lite,\r\n    I use arm-a7 to run TFlite , but first build TFlite failed ,and log as attachment . I have build libtensorflow-lite.a ,but I did confirm it OK, because there are some error , so pls check  whether it is ok? thank you very much!\r\n\r\n\r\nmike@ubuntu:~/working/tensorflow$ ./tensorflow/lite/tools/make/build_bbb_lib.sh\r\n+ set -e\r\n+++ dirname ./tensorflow/lite/tools/make/build_bbb_lib.sh\r\n++ cd ./tensorflow/lite/tools/make\r\n++ pwd\r\n+ SCRIPT_DIR=/home/mike/working/tensorflow/tensorflow/lite/tools/make\r\n+ cd /home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../..\r\n+ CC_PREFIX=/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-\r\n+ make -j 4 -f tensorflow/lite/tools/make/Makefile TARGET=bbb TARGET_ARCH=armv7l\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \\\r\n-o /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/obj/tensorflow/lite/tools/benchmark/benchmark_main.o \\\r\n   -Wl,--whole-archive /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a -Wl,--no-whole-archive -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt \r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \\\r\n-o /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/obj/tensorflow/lite/tools/benchmark/benchmark_tflite_performance_options_main.o \\\r\n /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt \r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/arm-buildroot-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/mike/working/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include \\\r\n-o /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/obj/tensorflow/lite/examples/minimal/minimal.o \\\r\n /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl -lrt \r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter_builder.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter_builder.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(allocation.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(allocation.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(common.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(common.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(error_reporter.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(error_reporter.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(flatbuffer_conversions.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(flatbuffer_conversions.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(op_resolver.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(op_resolver.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(subgraph.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(subgraph.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(external_cpu_backend_context.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(external_cpu_backend_context.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(graph_info.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(graph_info.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(add_n.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(add_n.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(arena_planner.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(arena_planner.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(tensor_utils.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(tensor_utils.o)\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:359: recipe for target '/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal' failed\r\nmake: *** [/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o)\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:394: recipe for target '/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options' failed\r\nmake: *** [/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model_performance_options] Error 1\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(allocation.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(arena_planner.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api_experimental.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(c_api_experimental.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(common.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(error_reporter.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(flatbuffer_conversions.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(op_resolver.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(tensor_utils.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(subgraph.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(resource_variable.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(resource_variable.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(static_hashtable.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(static_hashtable.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(external_cpu_backend_context.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(graph_info.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(interpreter_builder.o)\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o) does not\r\n/home/mike/working/100ask_imx6ull-sdk/ToolChain/arm-buildroot-linux-gnueabihf_sdk-buildroot/bin/../lib/gcc/arm-buildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: failed to merge target specific data of file /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/benchmark-lib.a(add_n.o)\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:388: recipe for target '/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model' failed\r\nmake: *** [/home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/benchmark_model] Error 1\r\nmike@ubuntu:~/working/tensorflow$ ", "comments": ["@mike2ning,\r\nIn order to expedite the trouble-shooting process, could you please provide the following details \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand also provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "Hi ,\r\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): it is embedded linux and kernel is 4.9.88\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: it is NXP -imx6ull\r\nTensorFlow installed from (source or binary):  source\r\nTensorFlow version: R2.4\r\nPython version: 3.5\r\nInstalled using virtualenv? pip? conda?: no\r\nBazel version (if compiling from source): no \r\nGCC/Compiler version (if compiling from source): arm-buildroot-linux-gnueabihf-gcc.br_real (Buildroot 2020.02-gee85cab) 7.5.0\r\nCUDA/cuDNN version: no \r\nGPU model and memory: no\r\n\r\nthere are steps to compile TFlite as below:\r\n\r\n1,sudo apt-get install build-essential\r\n\r\n2,./tensorflow/lite/tools/make/download_dependencies.sh\r\n\r\n3,./tensorflow/lite/tools/make/build_bbb_lib.sh\r\n\r\n", "It works well for me but I used stock ARM toolchain.\r\n\r\nAccording to the link error,\r\n\r\nbuildroot-linux-gnueabihf/7.5.0/../../../../arm-buildroot-linux-gnueabihf/bin/ld: error: /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/bin/minimal uses VFP register arguments, /home/mike/working/tensorflow/tensorflow/lite/tools/make/gen/bbb_armv7l/lib/libtensorflow-lite.a(interpreter.o) does not\r\n\r\nI think there is a mismatch on compile flags.\r\nPlease check if you're using the same compile flags for both libtensorflow-lite.a and minimal", "FYI, https://www.tensorflow.org/lite/guide/build_cmake_arm#check_your_target_environment", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44228\">No</a>\n"]}, {"number": 44227, "title": "Removed the no longer necessary if statements in ARC makefiles. Minor fixes in generation flow.", "body": "This pull request removes unnecessary TARGET checks in ARC makefiles as far as target-specific makefiles now used automaticly.\r\nAlso, TARGET_ARCH is no longer required for custom ARC build and corresponding changes have been made in code and README.\r\n\r\n@bigcat-himax Hi, can you please check out changes I applied in the himax_we1_evb_makefile.inc file? I added/removed some code that was preventing build for himax platform target. Thank you.\r\n\r\nFixes #43898", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44226, "title": "linux go1.12.2 tensorflow 2.3.1 installation error", "body": "I am facing this issue with TensorFlow 2.3.1 installation in golang go1.12.2 in ubuntu 20.04 with go mod.\r\n\r\nError:\r\n```\r\ngo: downloading google.golang.org/protobuf v1.23.0\r\ngo: extracting google.golang.org/protobuf v1.23.0\r\ngo build github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: no Go files in \r\n```\r\n\r\nFull error:\r\n```\r\n$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\n\r\ngo: finding github.com/tensorflow/tensorflow/tensorflow/go latest\r\ngo: finding github.com/tensorflow/tensorflow/tensorflow latest\r\ngo: finding github.com/tensorflow/tensorflow v2.3.1+incompatible\r\ngo: downloading github.com/tensorflow/tensorflow v2.3.1+incompatible\r\ngo: extracting github.com/tensorflow/tensorflow v2.3.1+incompatible\r\ngo: finding github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto latest\r\ngo: finding github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf latest\r\ngo: finding github.com/tensorflow/tensorflow/tensorflow/go/core latest\r\ngo: finding github.com/golang/protobuf/proto latest\r\ngo: finding github.com/golang/protobuf v1.4.3\r\ngo: downloading github.com/golang/protobuf v1.4.3\r\ngo: extracting github.com/golang/protobuf v1.4.3\r\ngo: finding github.com/google/go-cmp v0.4.0\r\ngo: finding google.golang.org/protobuf v1.23.0\r\ngo: finding github.com/golang/protobuf v1.4.0\r\ngo: finding golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543\r\ngo: finding google.golang.org/protobuf v1.21.0\r\ngo: finding github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0\r\ngo: finding google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967\r\ngo: finding github.com/golang/protobuf v1.4.0-rc.2\r\ngo: finding google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60\r\ngo: finding github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208\r\ngo: finding google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64\r\ngo: finding github.com/google/go-cmp v0.3.1\r\ngo: finding github.com/golang/protobuf v1.4.0-rc.1\r\ngo: finding google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd\r\ngo: finding github.com/google/go-cmp v0.3.0\r\ngo: downloading google.golang.org/protobuf v1.23.0\r\ngo: extracting google.golang.org/protobuf v1.23.0\r\ngo build github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto: no Go files in \r\n```", "comments": ["@sayan1999 \r\nPlease refer to this resolved issue with same error: # 39744 and let us know.\r\n\r\n [similar issues [link](https://github.com/tensorflow/tensorflow/issues/34580#issuecomment-646753289), #36495 #39307]", "This is absolutely right. It helped.\r\n\r\nJust one question before I close the issue.\r\nMy tensorflow c  has version 2.3.1\r\nBut the go library wrapper is of version v1.15.0\r\n\r\nWhy isn't that being an issue??", "@sayan1999 \r\nSorry for the delayed response, Could you please create a new issue for the information requested as i did look it up in [this link](https://www.tensorflow.org/install/lang_go) and dint find anything.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44226\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44226\">No</a>\n"]}, {"number": 44225, "title": "[API] Tensorflow.Tensor.Tensor(byte[ ][ ], long[ ]) constructor not exist any more", "body": "We used to use version \"1.14.0\" of package \"SciSharp.TensorFlow.Redist\" and recently we upgrade to version \"2.3.1\" and seems the constructor for Tensor take byte[][] not exists any more, so I'm wondering is there any replacement for this API?\r\n", "comments": ["@mihaimaruseac Any idea of this package \"SciSharp.TensorFlow.Redist\" with TF2.3? Thanks!", "Best is to identify the commit that introduced the change and see if there is reasoning for it listed there.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44224, "title": "does \"shared_embedding_columns_v2\" api need check executing_eagerly? ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nDoes \"shared_embedding_columns_v2\" api need check executing_eagerly? \r\n\r\n```\r\n@tf_export('feature_column.shared_embeddings', v1=[])\r\ndef shared_embedding_columns_v2(categorical_columns,\r\n                                dimension,\r\n                                combiner='mean',\r\n                                initializer=None,\r\n                                shared_embedding_collection_name=None,\r\n                                ckpt_to_load_from=None,\r\n                                tensor_name_in_ckpt=None,\r\n                                max_norm=None,\r\n                                trainable=True,\r\n                                use_safe_embedding_lookup=True):\r\n  **if context.executing_eagerly():\r\n    raise RuntimeError('shared_embedding_columns are not supported when eager '\r\n                       'execution is enabled.')**\r\n```\r\n\r\nerror:\r\n```\r\n  File \"~/models.py\", line 92, in build_feature_columns\r\n    group_embedding = tf.feature_column.shared_embeddings(group_encode, dimension=32)\r\n  File \"~/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 902, in shared_embedding_columns_v2\r\n    raise RuntimeError('shared_embedding_columns are not supported when eager '\r\nRuntimeError: shared_embedding_columns are not supported when eager execution is enabled.\r\n```\r\n**Describe the expected behavior**\r\n\r\nJust comment the check code, seems all things normal. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@osljw,\r\nIn order to expedite the trouble-shooting process, could you please provide following details \r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand also the complete code with the dataset, to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary):\r\n> * TensorFlow version (use command below):\r\n> * Python version:\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory:\r\n> \r\n> You can collect some of this information using our environment capture\r\n> [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n> You can also obtain the TensorFlow version with:\r\n> \r\n> 1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n> 2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n> \r\n> **Describe the current behavior**\r\n> Does \"shared_embedding_columns_v2\" api need check executing_eagerly?\r\n> \r\n> ```\r\n> @tf_export('feature_column.shared_embeddings', v1=[])\r\n> def shared_embedding_columns_v2(categorical_columns,\r\n>                                 dimension,\r\n>                                 combiner='mean',\r\n>                                 initializer=None,\r\n>                                 shared_embedding_collection_name=None,\r\n>                                 ckpt_to_load_from=None,\r\n>                                 tensor_name_in_ckpt=None,\r\n>                                 max_norm=None,\r\n>                                 trainable=True,\r\n>                                 use_safe_embedding_lookup=True):\r\n>   **if context.executing_eagerly():\r\n>     raise RuntimeError('shared_embedding_columns are not supported when eager '\r\n>                        'execution is enabled.')**\r\n> ```\r\n> \r\n> error:\r\n> \r\n> ```\r\n>   File \"~/models.py\", line 92, in build_feature_columns\r\n>     group_embedding = tf.feature_column.shared_embeddings(group_encode, dimension=32)\r\n>   File \"~/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 902, in shared_embedding_columns_v2\r\n>     raise RuntimeError('shared_embedding_columns are not supported when eager '\r\n> RuntimeError: shared_embedding_columns are not supported when eager execution is enabled.\r\n> ```\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> Just comment the check code, seems all things normal.\r\n> \r\n> **Standalone code to reproduce the issue**\r\n> Provide a reproducible test case that is the bare minimum necessary to generate\r\n> the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n> \r\n> **Other info / logs** Include any logs or source code that would be helpful to\r\n> diagnose the problem. If including tracebacks, please include the full\r\n> traceback. Large logs and files should be attached.\r\n\r\nhello, i have the same problem\uff0cdo you have a solution\uff1fwhen i need eager execution enabled and must used tf.feature_column.shared_embeddings, i am very confused and I don't know how to do it.", "> hello, i have the same problem\uff0cdo you have a solution\uff1fwhen i need eager execution enabled and must used tf.feature_column.shared_embeddings, i am very confused and I don't know how to do it.\r\n\r\n@chenyanyin,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Has this problem been solved\uff1f I have the same problem"]}, {"number": 44223, "title": "jjUpdate BUILD", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44223) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 44222, "title": "syslibs updates", "body": "* dill_archive and tblib_archive are newly unbundled following the same pattern as existing python-only deps\r\n\r\n* The protobuf syslib update adds some missing headers to the list and re-structures the file using the well_known_proto_map from the protobuf repo\r\n\r\n* the grpc syslibs updates are the bulk of this changeset, here is more info from that commit message:\r\n\r\nsystemlibs: Update to build against system GRPC\r\n    \r\n- Add libgpr\r\n  - Newer grpc-1.28 has a libgpr.so that is also needed during link time so add it to the linkopts\r\n\r\n- Add starlark files\r\n  - Several starlark files are load()'d from the GRPC repo, vendor them or add stubs as appropriate when using the system version of grpc.\r\n\r\n- grpc WORKSPACE deps\r\n  - Several deps were loaded in WORKSPACE that were needed by grpc, they are not needed when building against the system but are difficult to stub out causing the build to fail. grpc_extra_deps.bzl is provided to load all the requirements, so use that from WORKSPACE instead of directly loading each individually. This is also more maintainable going forward since there is less to keep in sync in TF's WORKSPACE file.\r\n\r\n/CC @mihaimaruseac @angerson \r\n", "comments": ["Will handle the conflict internally, no need to try and solve it for now.", "@mihaimaruseac  Any update on this PR? Please. Thanks!", "Builds are still failing and didn't have much time to look into why."]}, {"number": 44221, "title": "Update version numbers for TensorFlow 2.4.0-rc0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.4.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/c/experimental/saved_model/internal/testdata/UninitializedVariable/sa\nved_model.pb matches\nBinary file tensorflow/cc/saved_model/testdata/AssetModule/saved_model.pb \nmatches\nBinary file \ntensorflow/cc/saved_model/testdata/StaticHashTableModule/saved_model.pb matches\ntensorflow/tools/pip_package/setup.py:53:2.4.0\ntensorflow/tools/pip_package/setup.py:112:2.4.0\ntensorflow/tools/pip_package/setup.py:114:2.4.0\ntensorflow/tools/ci_build/release/common.sh:143:2.4.0\ntensorflow/tools/ci_build/release/common.sh:199:2.4.0\ntensorflow/tools/ci_build/release/common_win.bat:49:2.4.0\ntensorflow/security/advisory/tfsa-2020-028.md:16:2.4.0\ntensorflow/security/advisory/tfsa-2020-027.md:41:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:188:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:211:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:212:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:267:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:268:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:269:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:270:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:271:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:272:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:296:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:297:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:298:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:301:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:302:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:128:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:176:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:177:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:181:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:219:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:220:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:227:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:228:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:229:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:230:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:129:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:178:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:179:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:184:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:215:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:216:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:222:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:223:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:224:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:225:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/tensorflow.bzl:59:2.4.0\ntensorflow/python/keras/__init__.py:35:2.4.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.4.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\nBinary file \ntensorflow/c/experimental/saved_model/internal/testdata/UninitializedVariable/sa\nved_model.pb matches\nBinary file tensorflow/cc/saved_model/testdata/AssetModule/saved_model.pb \nmatches\nBinary file \ntensorflow/cc/saved_model/testdata/StaticHashTableModule/saved_model.pb matches\ntensorflow/tools/pip_package/setup.py:53:2.4.0\ntensorflow/tools/pip_package/setup.py:112:2.4.0\ntensorflow/tools/pip_package/setup.py:114:2.4.0\ntensorflow/tools/ci_build/release/common.sh:143:2.4.0\ntensorflow/tools/ci_build/release/common.sh:199:2.4.0\ntensorflow/tools/ci_build/release/common_win.bat:49:2.4.0\ntensorflow/security/advisory/tfsa-2020-028.md:16:2.4.0\ntensorflow/security/advisory/tfsa-2020-027.md:41:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:188:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:211:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:212:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:267:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:268:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:269:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:270:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:271:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:272:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:296:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:297:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:298:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:301:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:302:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb:321:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:128:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:176:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:177:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:181:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:219:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:220:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:227:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:228:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:229:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:230:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_text_classification.ipynb:249:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:129:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:178:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:179:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:184:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:215:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:216:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:217:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:218:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:222:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:223:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:224:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:225:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb:244:2.4.0\ntensorflow/tensorflow.bzl:59:2.4.0\ntensorflow/python/keras/__init__.py:35:2.4.0\n```", "comments": []}, {"number": 44220, "title": "Update release notes for TensorFlow 2.4.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.4.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": ["I added a note referencing breakages for TF-related builds (like SIG Addons), ref. https://groups.google.com/a/tensorflow.org/g/build/c/LbAw8RILvTg/m/ttnuhYU2BgAJ\r\n\r\n@seanpmorgan @perfinion This should cover it, right?\r\n\r\n@mihaimaruseac @goldiegadde FYI", "> I added a note referencing breakages for TF-related builds (like SIG Addons), ref. https://groups.google.com/a/tensorflow.org/g/build/c/LbAw8RILvTg/m/ttnuhYU2BgAJ\r\n> \r\n> @seanpmorgan @perfinion This should cover it, right?\r\n> \r\n> @mihaimaruseac @goldiegadde FYI\r\n\r\nLooks good to me. The non-experimental flag is `/Zc:preprocessor` which works in MSVC 2019 but the `/experimental` version works on older MSVC so fine to stay with that in the notes", "I'll make a PR to fix these. Sorry for missing them."]}, {"number": 44219, "title": "How to calculate mixed partial derivatives w.r.t inputs?", "body": "Assume I have the following neural network with two inputs:\r\n\r\n`(x, t) ---> [neural network] ---> u(x,t)`\r\n\r\nIs it possible to calculate the mixed partials i.e.: `d/dx[du(x,t)/dt]`?", "comments": ["@uninstallit \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nAnd move this to closed status.", "I figured it out myself: https://stackoverflow.com/questions/64474239/how-to-calculate-d-dxdu-dt-in-nn-w-r-t-inputs-x-t-in-keras-tensorflow", "@uninstallit \r\nThank you for your update,glad your issue is resolved."]}, {"number": 44218, "title": "ModelCheckpoint() verbose changed?", "body": "- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nWhen using tf.keras.callbacks.ModelCheckpoint() with verbose = 1 the callback behaves weirdly.\r\nIf I monitor the validation metric, during epochs where the checkpoint is triggered, the validation metric is not shown in the training output. The callback output reads fewer information than it previously did, making it harder to understand if it's working as intended. For example, it currently says something like:\r\n```\r\n14/14 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9929\r\nEpoch 00001: saving model to model.h5\r\n```\r\n\r\n**Describe the expected behavior**\r\nChecking some older code, the output used to be something like:\r\n\r\n```\r\n14/14 [==============================] - ETA: 0s - loss: 0.0253 - acc: 0.9929 - val_loss: 0.1335 - val_acc: 0.93469\r\nEpoch 00001: val_acc improved from -inf to 0.93469, saving model to model.h5\r\n```\r\nThis output makes it clearer the checkpoint is monitoring the correct quantity and when the changes are happening.\r\n\r\nI just want to be double sure that the current ModelCheckpoint() is doing the expected thing, even if the output is more minimal?\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe following code should be copy-paste-able and should reproduce the problem:\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport tensorflow as tf\r\n\r\n# Model / data parameters\r\nnum_classes = 10\r\ninput_shape = (28, 28, 1)\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\n# Scale images to the [0, 1] range\r\nx_train = x_train.astype(\"float32\") / 255\r\nx_test = x_test.astype(\"float32\") / 255\r\n# Make sure images have shape (28, 28, 1)\r\nx_train = np.expand_dims(x_train, -1)\r\nx_test = np.expand_dims(x_test, -1)\r\nprint(\"x_train shape:\", x_train.shape)\r\nprint(x_train.shape[0], \"train samples\")\r\nprint(x_test.shape[0], \"test samples\")\r\n\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = keras.Sequential(\r\n    [\r\n        keras.Input(shape=input_shape),\r\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\r\n        layers.MaxPooling2D(pool_size=(2, 2)),\r\n        layers.Flatten(),\r\n        layers.Dropout(0.5),\r\n        layers.Dense(num_classes, activation=\"softmax\"),\r\n    ]\r\n)\r\n\r\nmodel.summary()\r\n\r\nmy_callbacks = [\r\n    tf.keras.callbacks.ModelCheckpoint(filepath='model.h5', monitor = 'val_categorical_accuracy', verbose =2),\r\n]\r\n\r\nbatch_size = 128\r\nepochs = 5\r\n\r\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"])\r\n\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks = my_callbacks)\r\n```\r\n", "comments": ["@palatos \r\n\r\nI have tried in colab with TF version 2.3 , nightly version (`2.4.0-dev20201021`) and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f2845463028b1167dd44d4d776d14105/untitled477.ipynb).Please, verify once and close the issue, if you feel it is working as expected. Thanks!", "Thank you for creating that collab notebook. I ran the code there for a slightly larger number of epochs on the tensorflow 2.3.0 part of the code, and notice how the model is checkpointing every epoch even when \"val_categorical_accuracy\" doesn't improve. This is not how the checkpoint is supposed to behave. Looking at the tf-nightly code further down on your gist I see this incorrect behavior also happened on your own run.\r\nHere is the output for some epochs where val_categorical_accuracy did not improve but the model still checkpointed anyway:\r\n```\r\nEpoch 00007: saving model to model.h5\r\n422/422 [==============================] - 2s 5ms/step - loss: 0.0637 - categorical_accuracy: 0.9801 - val_loss: 0.0554 - val_categorical_accuracy: 0.9850\r\nEpoch 8/55\r\n422/422 [==============================] - ETA: 0s - loss: 0.0585 - categorical_accuracy: 0.9820\r\nEpoch 00008: saving model to model.h5\r\n422/422 [==============================] - 2s 5ms/step - loss: 0.0585 - categorical_accuracy: 0.9820 - val_loss: 0.0542 - val_categorical_accuracy: 0.9847\r\nEpoch 9/55\r\n421/422 [============================>.] - ETA: 0s - loss: 0.0561 - categorical_accuracy: 0.9824\r\nEpoch 00009: saving model to model.h5\r\n422/422 [==============================] - 2s 5ms/step - loss: 0.0561 - categorical_accuracy: 0.9824 - val_loss: 0.0537 - val_categorical_accuracy: 0.9837\r\n```\r\n\r\nCan you confirm if this is the case? The way the print-outs are arranged also makes it confusing as to whether the Model Checkpoint is really doing what it's supposed to do.\r\n", "@palatos I think we are missing `save_best_only=True`. Please check the [TF page](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) of ModelCheckpoint. When I add `save_best_only=True` as shown below, everything worked as you are expecting.\r\n\r\n```\r\nmy_callbacks = [\r\n    tf.keras.callbacks.ModelCheckpoint(filepath='Epoch_{epoch:04d}_model.h5', monitor = 'val_categorical_accuracy', verbose =2,save_best_only=True),\r\n]\r\n```\r\n\r\nIn the arguments section, it clearly mentions \r\n\r\n> if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. If `filepath` doesn't contain formatting options like {epoch} then `filepath` will be overwritten by each new better model. \r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/f0d5f342820c216c5654aa369cc948b6/untitled477.ipynb).  In the gist, you can see starting epoch=12, validation accuracy was not improved and the model was not saved. Just to show model saving, i added epoch number in to to the filename. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "> Please verify once and close the issue if this was resolved for you. Thanks!\r\n\r\nYou are right, it was that argument that was missing. Thank you!\r\nI'll close the issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44218\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44218\">No</a>\n"]}, {"number": 44217, "title": "inference time improvement using pb model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu==2.3.0rc0\r\n- TensorFlow version (use command below): tensorflow-gpu==2.3.0rc0\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: nvidia 2080ti\r\n\r\n**Describe the current behavior**\r\n\r\n**method 1) inference with model define and load weights**\r\n_model = tf.keras.Model(input_layer, bbox_tensors)\r\nutils.load_weights(model, FLAGS.weights)\r\npred_bbox = model.predict(image_data)_\r\n\r\n**method 2) inference with pb model**\r\n_saved_model_loaded = tf.saved_model.load(path_to_weight, tags=[tag_constants.SERVING])\r\ninfer = saved_model_loaded.signatures['serving_default']\r\nbatch_data = tf.constant(image_data)\r\npred_bbox = infer(batch_data)_\r\n\r\n**behavior**\r\nInference with method2 is much faster than that of method1. The inference time was reduced from 100 ms to 50 ms when an image was analyzed. And I observed that the metric, GPU-Util, is increased from 35% to 47%.\r\n\r\nIn both inference methods, all settings including batch size is identical except the model loading part - Image is analyzed one by one. And model is uploaded or defined only once at the beginning before reading images. \r\n\r\n**Describe the expected behavior**\r\nI want to know why inference with pb model increased the inference time performance.", "comments": ["@kusashim,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\nAlso, please update TensorFlow to v2.3 (i.e. the stable release) from TensorFlow v2.3.0rc0 and check if you are facing the same issue. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44216, "title": "A description problem in tf.keras.layers.MaxPooling2D", "body": "## URL(s) with the issue:\r\nhttps://keras.io/api/layers/pooling_layers/max_pooling2d/\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAt the end of second paragraph, you give a fomula to compute the size of output without padding: 'output_shape = (input_shape - pool_size + 1) / strides)'. However, there is something wrong with it. The correct one should be 'output_shape = (input_shape - pool_size) / strides + 1'. \r\nThank you!", "comments": ["I believe this was fixed in 31969210c748b85e8f7ff5c0dd64ced988bdb5f6, but this change didn't make it into [2.3](https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/python/keras/layers/pooling.py#L337) branch and somehow missed [2.4](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/python/keras/layers/pooling.py#L341) branch too. I guess keras.io site gets updated with latest stable TF version so it won't get fixed with 2.4 release unless changes get cherry picked (it's already Release Candidate 3). Maybe in 2.5?", "@Molkree This is already updated in [Keras.io](https://keras.io/api/layers/pooling_layers/max_pooling2d/). The [`master`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/pooling.py#L333) branch has the updated code as mentioned above but I think this will be updated soon in TF website. Thanks!\r\n", "This is fixed in latest tf nightly. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D?version=nightly\r\nhttps://github.com/tensorflow/tensorflow/blob/d7fc63e468546c4bcbaaeabb1f3b3a09deb4e894/tensorflow/python/keras/layers/pooling.py#L341", "Closing this issue as this was resolved already. Thanks!"]}, {"number": 44215, "title": "TFLite model weight shape is different from original model", "body": "I use TF2.3.0 and try two different way. One is python API and the other is command line `tflite_convert`.\r\n**Python API**\r\n```\r\nimport os\r\nimport sys\r\nimport absl.logging as logging\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\n\r\nnp.set_printoptions(threshold=sys.maxsize)\r\ntf.reset_default_graph()\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\ntf.keras.backend.set_session(sess)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('best_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops = True\r\nquant_best_model = converter.convert()\r\nwith open('quant_best_model.tflite', 'wb') as w:\r\n  w.write(quant_best_model)\r\n```\r\n\r\n**Command line**\r\n```\r\ntflite_convert \\\r\n--output_file 'tflite_convert.tflite' \\\r\n--saved_model_dir 'best_model'\r\n```\r\n\r\n[best_weights.zip](https://github.com/tensorflow/tensorflow/files/5418846/best_weights.zip)\r\n[best_model.zip](https://github.com/tensorflow/tensorflow/files/5418507/best_model.zip)\r\n\r\nOpen `best_weights` and `quant_best_model.tflite` or `tflite_convert.tflite` in [netron](https://lutzroeder.github.io/netron/) and inspect the weight shape. Actually `quant_best_model.tflite` and `tflite_convert.tflite` should be same.\r\n\r\nlast conv2D layer in tflite model (the dtype is float32 because the tensor is too small to be quantized, no problem here)\r\n![image](https://user-images.githubusercontent.com/59950011/96791211-9c655280-142a-11eb-9113-a1baa0e9ea3d.png)\r\n\r\nlast conv2D layer in original model\r\n![image](https://user-images.githubusercontent.com/59950011/96791262-b2731300-142a-11eb-93da-fa315d56eb3c.png)\r\n\r\nActually it's the transpose version if you look into how weights distributed.\r\n\r\nThe tflite model just fail like the model that generate random output in my case.\r\nAny advice? Thanks! ", "comments": ["@ravikyram @ymodak Sorry for pushing you guys, but the weekend is coming and I know that there won't be any response for a few days. Please comment if there is any advice.", "@tu1258 Is your question about why the shape is transposed ? That's because TFLite expects the weights to be OHWI, and we transpose the values during conversion from TF to TFLite, so we don't have to do this during inference.\r\n\r\nAs for \"The tflite model just fail like the model that generate random output in my case.\"\r\nI am not sure i understand this statement.\r\nDo you mean you tested a model and the resulting values is different between TF and TFLite ?\r\nIf yes, then please include the model and a reproduce code snippet.\r\nNote that, if you are quantizing the model, then loss in accuracy is expected.\r\n\r\nThanks\r\n", "@karimnosseir thanks for the reply.\r\n\r\n> tested a model and the resulting values is different between TF and TFLite\r\n\r\nyes. And the accuracy is between 8~9 for a 12 class classification task, which seems like just random output. The original model accuracy is around 95.\r\n", "\"please include the model and a reproduce code snippet.\r\nNote that, if you are quantizing the model, then loss in accuracy is expected.\"", "@karimnosseir  The code above should be enough for generate tflite model. For reproducing inference, I will add later.", "@karimnosseir \r\nremember to replace `train_dir + '/best_model'` with model I attach and also replace `train_dir + '/quant_best_model.tflite'` with the directory you save.\r\n\r\n### Code (include both contain model and generating final output)\r\n```\r\nimport os\r\nimport sys\r\nimport absl.logging as logging\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\n\r\nnp.set_printoptions(threshold=sys.maxsize)\r\ntf.reset_default_graph()\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\ntf.keras.backend.set_session(sess)\r\n\r\nread_fingerprints = np.loadtxt('./testbench/data.csv', delimiter=',')\r\n\r\ntest_fingerprints = read_fingerprints.reshape(-1,16384).astype(np.float32)\r\nprint('type(test_fingerprints):', type(test_fingerprints))\r\nprint('shape(test_fingerprints):', test_fingerprints.shape)\r\n\r\ntrain_dir = 'tcn/kws_7x36_1'\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(train_dir + '/best_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops = True\r\nquant_best_model = converter.convert()\r\nwith open(train_dir + '/quant_best_model.tflite', 'wb') as w:\r\n  w.write(quant_best_model)\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=os.path.join(train_dir + '/quant_best_model.tflite'))\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninputs = []\r\nfor s in range(len(input_details)):\r\n  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))\r\n  \r\nmodel = tf.keras.models.load_model(train_dir + '/best_model', custom_objects={'tf': tf})\r\n\r\nread_fingerprints = np.loadtxt('./testbench/data.csv', delimiter=',')\r\ntest_fingerprints = read_fingerprints.reshape(1,16384)\r\nmodel.run_eagerly = True\r\nprint('tf.executing_eagerly:',tf.executing_eagerly())\r\nprint('model.run_eagerly:',model.run_eagerly)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], test_fingerprints.astype(np.float32))\r\ninterpreter.invoke()\r\nout_tflite = interpreter.get_tensor(output_details[0]['index'])\r\nprint('out_tflite:',out_tflite)\r\n\r\n\r\nintermediate_layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=len(model.layers)-1).output)\r\nintermediate_output = intermediate_layer_model(test_fingerprints)\r\nprint('intermediate_output:',intermediate_output)\r\n```\r\n\r\nThe final output I generate\r\n```\r\nout_tflite: [[12.354025   -1.9960475  -0.01582483 -0.13261858 -2.7005167  -1.4998152\r\n  -3.1499515  -3.5536532  -3.20407    -0.65802205 -4.0220547   2.5022275 ]]\r\nintermediate_output: tf.Tensor(\r\n[[12.102602    0.08953991 -3.884885   -0.07776461 -4.7590914  -0.7605376\r\n  -3.9976068  -4.774014   -3.9251728  -1.4000554  -5.652582    2.8998525 ]], shape=(1, 12), dtype=float32)\r\n```", "@srjoglekar246 Can you have a look on the accuracy issue ?\r\n\r\nThanks", "@tu1258 To help me debug this faster, could you clarify: If you convert to TFLite without quantization/optimization, does the output look okay? (i.e. without `converter.optimizations = [tf.lite.Optimize.DEFAULT]`) If yes, this might be a bug with our quantization tooling and I can point this to them. If the float model is also wrong, then the issue is likely with the converter.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44215\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44215\">No</a>\n"]}, {"number": 44213, "title": "tf.contrib.layers.Conv2D and MaxPool2D in tf.keras.layers", "body": "I am trying to convert the following lines of code to TensorFlow 2.0. These are written in tf.contrib.layers and now I need to put them in tf.keras.layers. \r\n\r\n```python\r\nnet1_h = layers.conv2d(input, 32, 3, stride=1, scope=\"Enc_conv_1\") \r\nnet2 = layers.max_pool2d(input, kernel_size=2, stride=2, scope=\"Enc_maxpool_1\")\r\n```\r\nThe documentation for tf.keras.layers.Conv2D under TF2 states that input is not listed for Conv2D and MaxPool2D does not have input or kernel_size.\r\n\r\nAny suggestions?\r\n", "comments": ["@teenazral \r\nPlease refer to this [migration link](https://www.tensorflow.org/guide/migrate) for the mentioned issue, you may try \"pip --forceinstall tensorflow==1.15\", also have you imported conv2f and other parameters with tf else you could run into error, please share error log if any and stand alone code if you face any errors.\r\n\r\nyou could also try:\r\nspecifying your input in a separate input layer -\r\nmodel=tf.keras.models.sequential()\r\nmodel.add(Input(shape==(28,28,1)))\r\n\r\nthen for conv layer specify the filters,kernel_size,strides and other parameters you need.\r\nmodel.add(Conv2d(num_of_fileters;kernel_size,activation,padding)\r\n\r\nand for max pooling you only specify the pool size\r\nodel.add(Maxpooling2D(pool_size)\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44212, "title": "Compiling tensorflow-lib for jetson nano not working.", "body": "I am trying to build tensorflow-lib.so for jetson nano\r\nhttps://developer.nvidia.com/embedded/jetson-nano-developer-kit\r\n\r\nThis issue is a continuation on https://github.com/tensorflow/tensorflow/issues/44198\r\n\r\nSo I am trying to build the tensorflow library to use with the c api on jetson nano, I have a project that uses tensorflow r1.14 and I assume that I will need a r1.14 library with that, I can migrate to r1.15 if that would be easier since the r1.15 build gets further and from the issue #34429 it seems as if the compilation issue I am getting on r1.14 will not get fixed. \r\n\r\nJetson nano comes with aarch64, ubuntu 18.04, cuda 10.2 and cudnn 8.\r\n\r\n\r\n**Describe the problem**\r\n\r\nbazel build fails on tensorflow r1.15 with:\r\n\r\n```bash\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nERROR: /home/jonasrsv/.cache/bazel/_bazel_jonasrsv/71718566c3609207e5a44599481f772c/external/nccl_archive/BUILD.bazel:72:1: undeclared inclusion(s) in rule '@nccl_archive//:nccl':\r\nthis rule is missing dependency declarations for the following files included by 'external/nccl_archive/src/channel.cc':\r\n  'bazel-out/aarch64-opt/bin/external/nccl_archive/nccl.h'\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1299.618s, Critical Path: 130.78s\r\nINFO: 1525 processes: 1525 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nbazel build fails on r1.14 with: \r\n\r\n```bash\r\nINFO: From Compiling external/highwayhash/highwayhash/sip_hash.cc:\r\nIn file included from external/highwayhash/highwayhash/arch_specific.h:39:0,\r\n                 from external/highwayhash/highwayhash/sip_hash.h:23,\r\n                 from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nexternal/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, State*)':\r\nexternal/highwayhash/highwayhash/compiler_specific.h:52:46: warning: requested alignment 32 is larger than 16 [-Wattributes]\r\n #define HH_ALIGNAS(multiple) alignas(multiple)  // C++11\r\n                                              ^\r\nexternal/highwayhash/highwayhash/state_helpers.h:49:41: note: in expansion of macro 'HH_ALIGNAS'\r\n   char final_packet[State::kPacketSize] HH_ALIGNAS(32) = {0};\r\n                                         ^~~~~~~~~~\r\nERROR: /home/jonasrsv/.cache/bazel/_bazel_jonasrsv/71718566c3609207e5a44599481f772c/external/nccl_archive/BUILD.bazel:67:1: fatbinary external/nccl_archive/device_dlink_hdrs.fatbin failed (Exit 1)\r\nfatbinary fatal   : Unknown option '-bin2c-path'\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1732.677s, Critical Path: 143.12s\r\nINFO: 2101 processes: 2101 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nThis error seems to be related to #34429\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.14\r\npython3 configure.py\r\n\r\n# Click yes on tensorRT and cudaNN\r\n\r\nbazel build --config opt //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n", "comments": ["I noticed that saved_models produced by r1.14 can be loaded by libtensorflow-2.3.1 so I guess the saved_model format is backwards compatible, I will try to compile tf-2.3.1 on the jetson nano, and if that works we can ignore the older versions. Will update later today, just need to build a newer bazel version on the nano first.", "Build fails on r2.3 with\r\n\r\n```bash\r\nINFO: From Compiling tensorflow/core/util/dump_graph.cc:\r\ntensorflow/core/util/dump_graph.cc:169:8: warning: 'tensorflow::Status tensorflow::{anonymous}::WriteTextProtoToUniqueFile(const google::protobuf::MessageLite&, tensorflow::WritableFile*)' defined but not used [-Wunused-function]\r\n Status WriteTextProtoToUniqueFile(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nERROR: /home/jonasrsv/friday-voice-assistant/local_repositories/tensorflow/tensorflow/cc/BUILD:629:22: Linking of rule '//tensorflow/cc:ops/functional_ops_gen_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/cc/ops/functional_ops_gen_cc-2.params\r\nbazel-out/aarch64-opt-exec-50AE0418/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_checksums_do_cpu_id'\r\nbazel-out/aarch64-opt-exec-50AE0418/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_checksums_crc32c_hw'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1145.178s, Critical Path: 136.91s\r\nINFO: 94 processes: 5 internal, 89 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "The error above was probably from issues caused by the fact that I restarted the compilation after the computer crashed because too much resources was used and I did not clear cache inbetween.\r\n\r\nI re ran the build job over night with a cleaned cache using the bazel flag '--jobs=1' such that it didnt crash from exhausting resources. Now it instead crashes with this\r\n\r\n```bash\r\nERROR: /home/jonasrsv/friday-voice-assistant/local_repositories/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:660:11: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_matmul' failed (Exit 4): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/aarch64-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/runtime_matmul.pic.d ... (remaining 60 argument(s) skipped)\r\naarch64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 14063.490s, Critical Path: 2100.73s\r\nINFO: 8095 processes: 6355 internal, 1740 local.\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Hi, as you can see from my comment on Oct 22 I also tried with Tensorflow r2.3 which failed too. I haven't tried with 2.4 and probably won't in the near future, feel free to close this though. I'll open this one again whenever I get time to try the latest TF and if the problem persists :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44212\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44212\">No</a>\n"]}, {"number": 44211, "title": "Fix tests that fail with tf32", "body": "I ran the following command with both tf32 enabled and disabled, and fixed all the tests that failed only when tf32 was enabled:\r\n\r\n```bash\r\nbazel test --keep_going --test_tag_filters=gpu,-no_gpu,-nogpu,-benchmark-test,-no_oss,-oss_serial,-v1only,-no_gpu_presubmit --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute --local_test_jobs=100  --config=opt --config=cuda -- //tensorflow/... -//tensorflow/python/integration_testing/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... //tensorflow/compiler/mlir/lite/... -//tensorflow/lite/micro/examples/... -//tensorflow/core/tpu/...  -//tensorflow/lite/... -//tensorflow/java/...\r\n```\r\n\r\nNote Complex64 now sometimes uses tf32, so I updated `matmul_without_tf32` to handle complex64.", "comments": []}, {"number": 44210, "title": "Bug in loading keras model with custom losses (metrics)", "body": "**System information**\r\n- Have I written custom code: yes, class for custom loss `tf.keras` \r\n- OS Platform and Distribution Linux Ubuntu 20.04:\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.1.0 --> 2.3.0 (it seems that the bug is present until master branch):\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\n\r\n(see code below)\r\n1. Define a class for custom loss in keras named `BatchMeanSquaredError`\r\n2. Create a model using `tf.keras.Sequential`\r\n3. Train the model on random data\r\n4. Save the model with `tf.keras.models.save_model`\r\n5. Load the model using `tf.keras.models.load_model('model.h5', custom_objects={'BatchMeanSquaredError': BatchMeanSquaredError})`\r\n\r\nThe loading function raises the following error \r\n\r\n    ValueError: Unknown loss function: BatchMeanSquaredError\r\n\r\n\r\n**Describe the expected behavior**\r\nI think that the bug is caused by two little issues:\r\n\r\n***1***\r\nIn `tensorflow_core/python/keras/losses.py` the function `get(identifier)` misses the parameter `custom_objects`. The same parameter should be added when calling `deserialize`. \r\nNow it is:\r\n\r\n    def get(identifier):\r\n      [...]\r\n      if isinstance(identifier, six.string_types):\r\n        identifier = str(identifier)\r\n        return deserialize(identifier)\r\n      if isinstance(identifier, dict):\r\n        return deserialize(identifier)\r\n      [...]\r\n\r\n\r\nI think it should be:\r\n\r\n    def get(identifier, custom_objects=None):\r\n      [...]\r\n      if isinstance(identifier, six.string_types):\r\n        identifier = str(identifier)\r\n        return deserialize(identifier, custom_objects=custom_objects)\r\n      if isinstance(identifier, dict):\r\n        return deserialize(identifier, custom_objects=custom_objects)\r\n      [...]\r\n\r\n\r\n***2***\r\nIn `tensorflow_core/python/keras/saving/saving_utils.py` the function `compile_args_from_training_config(training_config, custom_objects=None)` has the parameter `custom_objects` but the same parameter is not present when calling `losses.get`:\r\n\r\n    if isinstance(loss_config, dict) and 'class_name' in loss_config:\r\n        loss_config = losses.get(loss_config)\r\n\r\nI think it should be:\r\n\r\n    if isinstance(loss_config, dict) and 'class_name' in loss_config:\r\n        loss_config = losses.get(loss_config, custom_objects=custom_objects)\r\n\r\n***Overall***\r\nThe problem may be caused by the coexistence of the possibility to define `tf.keras` custom losses, or custom metrics, both using functions or classes. I think it should be allowed to use only classes to have a more general behavior.\r\n \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n    from tensorflow.keras.losses import Loss\r\n    import tensorflow as tf\r\n    import tensorflow.keras.backend as K\r\n    import numpy as np\r\n    \r\n    \r\n    class BatchMeanSquaredError(Loss):\r\n    \r\n        def __init__(self, reduction='auto', name='batch_mean_squared_error'):\r\n            super().__init__(reduction=reduction, name=name)\r\n    \r\n        def call(self, y_true, y_pred):\r\n            y_pred = tf.convert_to_tensor(y_pred)\r\n            y_true = tf.cast(y_true, y_pred.dtype)\r\n            L = K.mean((y_pred - y_true) ** 2, axis=0)\r\n            return L\r\n    \r\n    X = np.random.random((1000, 3))\r\n    y = np.ones(shape=(1000, 3))\r\n\r\n    model = tf.keras.Sequential(\r\n        [\r\n            tf.keras.layers.Dense(3, activation='relu'),\r\n            tf.keras.layers.Dense(3, activation='relu'),\r\n            tf.keras.layers.Dense(3)\r\n        ]\r\n    )\r\n    \r\n    bmse = BatchMeanSquaredError()\r\n    model.compile(loss=bmse, optimizer='sgd')\r\n\r\n    model.fit(X, y, batch_size=10, epochs=5)\r\n\r\n    tf.keras.models.save_model(model=model, filepath='model.h5')\r\n\r\n    custom_objects = {'BatchMeanSquaredError': BatchMeanSquaredError}\r\n    tf.keras.models.load_model('model.h5', custom_objects=custom_objects)", "comments": ["Can I reproduce with `Standalone code to reproduce the issue`?", "@bhack Is it a question? I've posted it just to reproduce the issue", "Yes I meant can I reproduce your issue with your code?\r\n```\r\nfrom tensorflow.keras.losses import Loss\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\n\r\nclass BatchMeanSquaredError(Loss):\r\n\r\n    def __init__(self, reduction='auto', name='batch_mean_squared_error'):\r\n        super().__init__(reduction=reduction, name=name)\r\n\r\n    def call(self, y_true, y_pred):\r\n        y_pred = tf.convert_to_tensor(y_pred)\r\n        y_true = tf.cast(y_true, y_pred.dtype)\r\n        L = K.mean((y_pred - y_true) ** 2, axis=0)\r\n        return L\r\n\r\nX = np.random.random((1000, 3))\r\ny = np.ones(shape=(1000, 3))\r\n\r\nmodel = tf.keras.Sequential(\r\n    [\r\n        tf.keras.layers.Dense(3, activation='relu'),\r\n        tf.keras.layers.Dense(3, activation='relu'),\r\n        tf.keras.layers.Dense(3)\r\n    ]\r\n)\r\n\r\nbmse = BatchMeanSquaredError()\r\nmodel.compile(loss=bmse, optimizer='sgd')\r\n\r\nmodel.fit(X, y, batch_size=10, epochs=5)\r\n\r\ntf.keras.models.save_model(model=model, filepath='model.h5')\r\n\r\ncustom_objects = {'BatchMeanSquaredError': BatchMeanSquaredError}\r\ntf.keras.models.load_model('model.h5', custom_objects=custom_objects)\r\n```", "Cause this one is running correctly on colab.", "@bhack thanks for your reply.\r\nI wrongly executed the script in tf 2.1 while i thought I was using tf 2.3. \r\nThe problem affects only tf 2.1 and not tf 2.2 nor tf 2.3", "@mantonios107 Ok but I am quite sure that nobody will backport this on 2.1", "@mantonios107,\r\nI was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/334d94a5141bb724ec29784bcc84e19d/44210-2-1.ipynb). However, the issue seems to be fixed with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a5ae9299aba523f9a90793bbb1927025/44210.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/918e433fb6cab78f0e4aff2e0d7ee667/44210-tf-nightly.ipynb). \r\n\r\n> The problem affects only tf 2.1 and not tf 2.2 nor tf 2.3\r\n\r\nIn this case, could you please update TensorFlow to the latest version i.e. v2.3. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44209, "title": "Cannot access a resource variable from a custom op on Mac and Windows", "body": "I am facing an issue with accessing a resource variable from a custom op on Mac and on Windows (it works fine on Linux though). When running the attached reproducer on Mac, I get the following error:\r\n\r\n```\r\nTrying to access resource using the wrong type. Expected N10tensorflow3VarE got N10tensorflow3VarE [Op:CustomOp]\r\n```\r\n\r\nOn Windows the error is similar:\r\n\r\n```\r\nTrying to access resource using the wrong type. Expected class tensorflow::Var got class tensorflow::Var [Op:CustomOp]\r\n```\r\n\r\nOn Linux there is no error. The op works as expected and the attached test prints this output:\r\n\r\n```\r\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=([1., 2., 3.], dtype=float32)>\r\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=([2.22, 2.22, 2.22], dtype=float32)>\r\n```\r\n\r\nI think this issue is due to the use of an address of a static variable ```hash_bit``` in ```TypeIndex::Make<T>``` as a hash. When the shared library with the custom op is compiled, it will have its own copy of ```TypeIndex::Make<Var>::hash_bit```, so the hash codes for the same type (e.g. Var) will be different. I suspect this still works on Linux because a STB_GNU_UNIQUE symbol is generated for ```hash_bit```, so that there is only one definition of it (per type) in the whole process. Mac and Windows don't have this feature though.\r\n\r\nIs it possible to rewrite TypeIndex in a way that will make it work across shared libraries on all platforms? Or perhaps various instantiations of ```TypeIndex::Make<T>``` could be exported from tensorflow framework library?\r\n\r\nA standalone reproducer along with build scripts is attached.\r\n[custom_op.zip](https://github.com/tensorflow/tensorflow/files/5417289/custom_op.zip)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pypi)\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "comments": ["@zhezherun It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44209\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44209\">No</a>\n"]}, {"number": 44208, "title": "Docker build tensorflow cannot detect gpu", "body": "Hello:\r\n\r\nI used docker to install tensorflow but GPU cannot be detected\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 20.04.1 LTS\r\n- TensorFlow installed from (source or binary): docker\r\n- Docker Image: 2.2.1-gpu-py3-jupyter\r\n- Python version: Python 3.8.5\r\n- GPU: GTX1650\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAfter running the command: **sudo docker run --gpus all -it --rm -p 8888:8888 tensorflow/tensorflow:2.2.1-gpu-py3-jupyter**, I use the jupyter on the browser to write python codes. But I find that gpu cannot be detected by tensorflow.\r\n\r\n```import tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.client import device_lib\r\n\r\nprint(tf.__version__)\r\nprint(device_lib.list_local_devices())\r\n```\r\n\r\nresult:\r\n\r\n```\r\n2.2.1\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 16222889058350606218\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 1739177089072959980\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n]\r\n```\r\n\r\nThere are no GPU.\r\n\r\n**Any other info / logs**\r\nHere is results of **sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi**:\r\n\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1650    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   44C    P8     3W /  N/A |    281MiB /  3911MiB |      1%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n", "comments": ["@YuhL6 \r\n\r\nPlease, refer this [SO link](https://stackoverflow.com/questions/63122486/the-tensorflow-docker-gpu-image-doesnt-detect-my-gpu) and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44208\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44208\">No</a>\n"]}]