[{"number": 9982, "title": "Updating the release notes.", "body": "", "comments": []}, {"number": 9981, "title": "Open up visibility of tf_imports", "body": "This is so other people can build web apps with the new build rules for\r\nTensorBoard, based on Closure Rules web_library.", "comments": ["The failing test appears to be flaky. We can merge this as soon as Dandelion approves."]}, {"number": 9980, "title": "Fixing the api compatibility test.", "body": "", "comments": ["Thanks for the fix, Amit.", "We should also cherry-pick this commit back to the master branch to fix nightly"]}, {"number": 9979, "title": "Building with config MKL with current master (git version: v1.1.0-rc2-1163-gcbe5eb4) fails with a build rule error", "body": "Building with config MKL with the current master fails with build rule error.\r\n\r\n### System information: \r\n(contents from tf_env.txt)\r\n== cat /etc/issue ===============================================\r\nLinux desktop 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== uname -a =====================================================\r\nLinux desktop 4.4.0-75-generic #96-Ubuntu SMP Thu Apr 20 09:56:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.1.0-rc2\r\ntf.GIT_VERSION = v1.1.0-rc2-1163-gcbe5eb4\r\ntf.COMPILER_VERSION = v1.1.0-rc2-1163-gcbe5eb4\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n\r\n== cuda libs  ===================================================\r\n\r\n**Exact command to reproduce**:\r\n\r\nbazel --output_user_root=/home/desktop/gtt/tfbuild_opt/ build --copt=\"-DEIGEN_USE_VML\" --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem: Build with the above setup and following configure options fails with a build rule error.\r\n\r\n$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] \r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] \r\nNo CUDA support will be enabled for TensorFlow\r\nWarning: ignoring http_proxy in environment.\r\n...\r\n.....................\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n\r\nBuild failed with following error:\r\n\r\n`\r\nERROR: /home/desktop/gtt/tensorflow/core/BUILD:1544:1: undeclared inclusion(s) in rule '//tensorflow/core:core_cpu_base':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/graph/mkl_tfconversion_pass.cc':\r\n  '/home/desktop/gtt/tensorflow/core/common_runtime/function.h'\r\n  '/home/desktop/gtt/tensorflow/core/common_runtime/device_mgr.h'\r\n  '/home/desktop/gtt/tensorflow/core/common_runtime/optimization_registry.h'\r\n  '/home/desktop/gtt/tensorflow/core/common_runtime/device_set.h'.\r\n____Building complete.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n____Elapsed time: 615.488s, Critical Path: 227.18s\r\n`\r\n", "comments": ["@jrvb, any thoughts about this?"]}, {"number": 9977, "title": "Updating version to 1.2.0-rc0", "body": "", "comments": []}, {"number": 9976, "title": "Tensorboard 404 Errors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:5.1\r\n- **GPU model and memory**:k40 12gb\r\n- **Exact command to reproduce**: launch tensorboard\r\n\r\n\r\n### Describe the problem\r\nTensorboard is 404'ing on a lot of resources. See the error messages below. I think I saw a similar issue somewhere so this may be a duplicate. You're probably aware but thought i'd file just in case.\r\n\r\n### Source code / logs\r\n```\r\nWARNING:tensorflow:path ../external\\weblas_weblas_js/file/weblas.map.json not found, sending 404\r\nWARNING:tensorflow:path ../external\\web_animations_js/web-animations-next-lite.min.js.map not found, sending 404\r\nWARNING:tensorflow:path ../external\\weblas_weblas_js/file/weblas.map.json not found, sending 404\r\nWARNING:tensorflow:path ../external\\web_animations_js/web-animations-next-lite.min.js.map not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\n```", "comments": ["@dandelionmane @jart ", "We're working on the text/plugin 404. The other ones are for js.map files and shouldn't cause problems if they're not available.", "Ok. I guess i'll leave this open until it's sorted then. Thanks.", "Get the same problem with this system information:\r\nUbuntu 16.04\r\npython 3.5.2\r\ntensorflow 1.1.0", "I have the same problem! how can I fix it?\r\n\r\nFirst everything is OK. \r\n\r\n> Starting TensorBoard b'47' at http://0.0.0.0:6006\r\n> (Press CTRL+C to quit)\r\n\r\nbut when I open the tensorboard in chome, it shows\r\n\r\n> WARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\n> WARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\n> WARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\n> WARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404", "@XeMinZa I'm assuming you're on Windows?", "@jart Yes, I'm using win10. Someone said that this bug was fixed in the later release. But I still have it...", "@XeMinZa what exact Tensorflow version? I'm on Windows and I think this bug has gone. I can double check though.", "I use v1.1.0 and meet the similar issue.\r\n\r\nStarting TensorBoard b'47' at http://0.0.0.0:6007\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404", "I have the same problem!!!\r\nMac     python2.7    running in virtualenv", "Same problem.\r\ntf 1.1.0 + Ubuntu 16.04 + python 3.5 + cuda 8.0\r\n", "Same issue with : tf 1.1.0 + Ubuntu 16.04 + python 3.5 + cuda 8.0. I can not access it from internet", "I've confirmed that all these 404 warning messages are now gone at HEAD. They should be relatively harmless in the old releases.", "Cheers @jart ", "I have the same problem when i use \r\nc:> tensorboard --logdir='DIR'\r\nbut when i use\r\nc:> tensorboard --logdir= \"DIR\"\r\nits ok\r\nwin10 ", "Still facing this issue.\r\nSystem Info:\r\nOS : Windows 10\r\nAnaconda3\r\nTensorflow 1.1.0 (installed via Anconda navigator)\r\nStarting TensorBoard b'47' at http://0.0.0.0:6006\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404\r\nWARNING:tensorflow:path ../external\\data/plugin/text/runs not found, sending 404", "@sivaram10 Can you please try upgrading Tensorflow. As you are on an old version then you won't have the fixes released in newer version. I believe you may have to install via pip instead of conda to receive the latest version. ", "Thanks @jubjamie  I updated Tensorflow to 1.2.1 version. I'm not getting those warnings anymore. But, I'm still not able to see graphs and scalar data. I can see the log files. ", "I would suggest refiling over at the **tensorboard** issue tracker as this 404 issue is now closed.", "I once opened a file successfully, but after a while, when I opened it again, it would show this error.Why?please give me some help", "I would suggest refiling over at the **tensorboard** issue tracker.", "> I have the same problem when i use\r\nc:> tensorboard --logdir='DIR'\r\nbut when i use\r\nc:> tensorboard --logdir= \"DIR\"\r\nits ok\r\nwin10\r\n\r\nit helps!", "Do you have a  directory named \"data\" beside the log-dir? \r\nWhen I rename the \"data\" dir, my problem fixed.", "I updated chrome and its work", "It doesn't work on chromium nor firefox for me.", "I have the same error:\r\nWARNING: tensorflow: path ../external/data/plugin/text/runs not found, sending 404\r\n\r\nI have tensorflow 1.1.0 and tensorboard 0.1.8.\r\nI read: https://github.com/tensorflow/tensorboard/issues/743\r\n\r\nI reinstalled 1.1.0 and it worked, but it showed me in:\r\n- tensorboard --logdir .....\r\n- Starting TensorBoard 47 at http://0.0.0.0:6006\r\nincompatibility with numpy as warnings\r\n\r\nI have updated tensorboard to 1.11.0\r\nI have updated numpy with sudo pip install --upgrade numpy\r\nit works again but with this 404 message.\r\n\r\nShould I download to a smaller version of Tensorboard?\r\n\r\nUbuntu: 16.04\r\nPython 2.7\r\nnumpy 1.15.2\r\ntensorflow 1.1.0\r\ntensorboard 1.11.0\r\n\r\nI appreciate your help\r\n\r\n\r\nI have another Pc:\r\nAlso with Ubuntu: 16.04\r\nPython 2.7 from Anaconda\r\ntensorflow-gpu 1.1.0\r\ntensorboard 1.11.0\r\nThe same message:\r\n\r\nterranostra @ terranostra: ~ $ tensorboard --logdir = / tmp / tensorflow / mnist / logs / mnist_with_summaries\r\nStarting TensorBoard 47 at http://0.0.0.0:6006\r\n(Press CTRL + C to quit)\r\nWARNING: tensorflow: path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING: tensorflow: path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING: tensorflow: path ../external/data/plugin/text/runs not found, sending 404\r\nWARNING: tensorflow: path ../external/data/plugin/text/runs not found, sending 404\r\n\r\nPd. same with\r\ntensorboard --logdir / tmp / tensorflow / mnist / logs / mnist_with_summaries\r\nthe graphs are displayed, I installed (downloaded to) tensorboard 1.8.0 and (then a) 0.1.x nothing is shown, I stayed with 1.11.0"]}, {"number": 9975, "title": "Fix a typo in export_output.py", "body": "", "comments": ["Can one of the admins verify this patch?", "We should merge this into master as well -- it was only merged into r1.1."]}, {"number": 9974, "title": "tf.Estimator vs contrib. Very unclear differences and lots of deprecations.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows/Ubuntu\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: /\r\n- **CUDA/cuDNN version**: 5\r\n- **GPU model and memory**: K40\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nIs it just me or is there quite a large disconnect between the two versions of Estimator. The tutorial in the docs guides you through the contrib version but I understand that it has also been moved to tf.Estimator? However in the docs it appears that almost all functionality, e.g. canned estimators or .fit() appears to be missing or altered and there is little documentation to explain this new API.\r\n\r\nHave I misunderstood something here? I imagine that we are preferred to use tf.estimator because using the contrib version kicks up all sorts of warning about how it will be deprecated last year! Although the estimator tutorial still uses it and also .fit() which I can't clearly see the replacement for in the new API. Is there going to be any example code or tutorial for the new tf.Estimator API as I feel it desperately needs it. The contrib version was difficult enough to understand that I gave up but want to try again! Thanks\r\n\r\n", "comments": ["@wolffg @ispirmustafa (please redirect as appropriate, thanks)", "Sorry for confusion. \r\nRight there is a difference between tf.Estimator and contrib one. Most of those changes are removal of deprecated behaviors. There are a few name changes. Currently we're moving some of the canned contrib Estimators under  tf.estimator. We'll update all our examples and tutorials after that."]}, {"number": 9973, "title": "Branch 155393864", "body": "", "comments": ["http://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/8684/"]}, {"number": 9972, "title": "#issue: broken or outdated link link for NLP tutorial", "body": "The second link, in the highlights session of the webpage\r\n\r\n[Vector Representations](https://www.tensorflow.org/tutorials/word2vec)\r\n\r\nis broken or outdated.\r\n\r\nThis is the link that's not working (404 from github)\r\n\r\n[https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py](https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py)\r\n\r\n\r\n", "comments": ["@wolffg @dr4b ", "Another broken link in the same page: \r\n\r\n[https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec_optimized.py](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec_optimized.py)", "Actually this is already fixed in the latest 1.2 docs: https://www.tensorflow.org/versions/r1.2/tutorials/word2vec\r\n\r\nThis will be fixed in the main docs when we switch them over to the 1.2 docs.", "Wait, but I still see a broken link to https://www.github.com/tensorflow/tensorflow/blob/r1.2/tensorflow_models/tutorials/embedding/word2vec.py in the 1.2 doc; that 404's for me, so are you sure this is fixed?", "Ah you're right, still looks broken (although in a different way!).", "Also this bug is a duplicate of #9905 if I'm not mistaken.", "Thanks, closing this as a duplicate of #9905 please reopen if there's still a problem when that one is closed."]}, {"number": 9971, "title": "Unable to build tensorflow for Java in Windows", "body": "As many may have this problem, I am unable to build tensorflow for Java in Windows. I need to build tensorflow for Java with GPU support in Windows. I followed all the directions specified, and I get the following error message:\r\n\r\nThank you!\r\n\r\n$ bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\nERROR: C:/development/projects/tensorflow/tensorflow/java/BUILD:142:1: error loading package 'tensorflow/java/src/main/native': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 958\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 846, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 656, in _get_cuda_config\r\n                _cudnn_install_basedir(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 211, in _cudnn_install_basedir\r\n                auto_configure_fail(\"Cannot find cudnn install path....)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 128, in auto_configure_fail\r\n                fail(\"\r\n%sAuto-Configuration Error:%s ...))\r\n\r\nAuto-Configuration Error: Cannot find cudnn install path.\r\n and referenced by '//tensorflow/java:libtensorflow_jni.so'.\r\nERROR: Analysis of target '//tensorflow/java:libtensorflow_jni' failed; build aborted.\r\nINFO: Elapsed time: 2.365s\r\n\r\n\r\n\r\n\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, msys2 64bit\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Quadro K5200\r\n- **Exact command to reproduce**: bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I use cmake , it works!", "I tried but cmake also failed for me... Can you give me instructions??", "CMAKE gives me the following errors:\r\n\r\nThe C compiler identification is MSVC 19.10.25019.0\r\nThe CXX compiler identification is MSVC 19.10.25019.0\r\nCheck for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.10.25017/bin/Hostx86/x64/cl.exe\r\nCheck for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.10.25017/bin/Hostx86/x64/cl.exe -- works\r\nDetecting C compiler ABI info\r\nDetecting C compiler ABI info - done\r\nCheck for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.10.25017/bin/Hostx86/x64/cl.exe\r\nCheck for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.10.25017/bin/Hostx86/x64/cl.exe -- works\r\nDetecting CXX compiler ABI info\r\nDetecting CXX compiler ABI info - done\r\nDetecting CXX compile features\r\nDetecting CXX compile features - done\r\nPerforming Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\nPerforming Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\nFound PythonInterp: C:/ProgramData/Anaconda3/python.exe (found version \"3.6\") \r\nFound PythonLibs: C:/ProgramData/Anaconda3/libs/python36.lib (found version \"3.6.0\") \r\nCMake Error at C:/Program Files/CMake/share/cmake-3.8/Modules/FindPackageHandleStandardArgs.cmake:137 (message):\r\n  Could NOT find SWIG (missing: SWIG_EXECUTABLE SWIG_DIR)\r\nCall Stack (most recent call first):\r\n  C:/Program Files/CMake/share/cmake-3.8/Modules/FindPackageHandleStandardArgs.cmake:377 (_FPHSA_FAILURE_MESSAGE)\r\n  C:/Program Files/CMake/share/cmake-3.8/Modules/FindSWIG.cmake:63 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)\r\n  tf_python.cmake:656 (find_package)\r\n  CMakeLists.txt:268 (include)\r\n\r\n\r\nConfiguring incomplete, errors occurred!\r\nSee also \"C:/Temp/CMakeFiles/CMakeOutput.log\".\r\nSee also \"C:/Temp/CMakeFiles/CMakeError.log\".", "```\r\nCould NOT find SWIG (missing: SWIG_EXECUTABLE SWIG_DIR)\r\n```\r\n\r\nyou need swig", "Thanks. Let me install that.", "Now I get this: \r\n(even though I have installed CUDNN). \r\n\r\nCMake Error at CMakeLists.txt:221 (FILE):\r\n  file COPY cannot find \"/include/cudnn.h\".\r\n\r\nConfiguring incomplete, errors occurred!\r\nSee also \"C:/Temp/CMakeFiles/CMakeOutput.log\".", "Finally build it with CMAKE. So what do I do next to create the JNI libraries for Java?", "When I build it on Visual Studio I get:\r\n\r\n187>Building Custom Rule C:/Development/Projects/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n185>Building Custom Rule C:/Development/Projects/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n186>Building Custom Rule C:/Development/Projects/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n185>CMake does not need to re-run because C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/generate.stamp is up-to-date.\r\n185>Building NVCC (Device) object CMakeFiles/cuda_compile_2.dir/__/rnn/kernels/Debug/cuda_compile_2_generated_lstm_ops_gpu.cu.cc.obj\r\n187>CMake does not need to re-run because C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/generate.stamp is up-to-date.\r\n187>Building NVCC (Device) object CMakeFiles/cuda_compile_3.dir/__/seq2seq/kernels/Debug/cuda_compile_3_generated_beam_search_ops_gpu.cu.cc.obj\r\n186>CMake does not need to re-run because C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/generate.stamp is up-to-date.\r\n186>Building NVCC (Device) object CMakeFiles/cuda_compile_1.dir/__/rnn/kernels/Debug/cuda_compile_1_generated_gru_ops_gpu.cu.cc.obj\r\n185>Failed to run C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin (The system cannot find the file specified.\r\n185>\r\n185>).\r\n185>CMake Error at cuda_compile_2_generated_lstm_ops_gpu.cu.cc.obj.Debug.cmake:222 (message):\r\n185>  Error generating\r\n185>  C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/cuda_compile_2.dir/__/rnn/kernels/Debug/cuda_compile_2_generated_lstm_ops_gpu.cu.cc.obj\r\n185>\r\n185>\r\n186>Failed to run C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin (The system cannot find the file specified.\r\n186>\r\n186>).\r\n186>CMake Error at cuda_compile_1_generated_gru_ops_gpu.cu.cc.obj.Debug.cmake:222 (message):\r\n186>  Error generating\r\n186>  C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/cuda_compile_1.dir/__/rnn/kernels/Debug/cuda_compile_1_generated_gru_ops_gpu.cu.cc.obj\r\n186>\r\n186>\r\n187>Failed to run C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin (The system cannot find the file specified.\r\n187>\r\n187>).\r\n187>CMake Error at cuda_compile_3_generated_beam_search_ops_gpu.cu.cc.obj.Debug.cmake:222 (message):\r\n187>  Error generating\r\n187>  C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/cuda_compile_3.dir/__/seq2seq/kernels/Debug/cuda_compile_3_generated_beam_search_ops_gpu.cu.cc.obj\r\n187>\r\n187>\r\n186>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1.\r\n186>Done building project \"_gru_ops.vcxproj\" -- FAILED.\r\n187>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1.\r\n187>Done building project \"_beam_search_ops.vcxproj\" -- FAILED.\r\n185>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1.\r\n185>Done building project \"_lstm_ops.vcxproj\" -- FAILED.\r\n188>------ Build started: Project: tf_extension_ops, Configuration: Debug x64 ------\r\n189>------ Skipped Build: Project: ALL_BUILD, Configuration: Debug x64 ------\r\n189>Project not selected to build for this solution configuration \r\n188>Building Custom Rule C:/Development/Projects/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n188>CMake does not need to re-run because C:/Development/Projects/tensorflow/cmake_build/CMakeFiles/generate.stamp is up-to-date.\r\n190>------ Skipped Build: Project: tf_python_build_pip_package, Configuration: Debug x64 ------\r\n190>Project not selected to build for this solution configuration \r\n========== Build: 164 succeeded, 24 failed, 57 up-to-date, 2 skipped ==========", "I really need to build tensorflow for Java with GPU support in Windows, and unfortunately, it is an extremely frustrating experience. :-( :-( Is there any place I can download the gpu JNI lib that has been pre-built? ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thank you, will do.", "Now I get this:\r\n(even though I have installed CUDNN).\r\n\r\nCMake Error at CMakeLists.txt:221 (FILE):\r\nfile COPY cannot find \"/include/cudnn.h\".\r\n\r\nConfiguring incomplete, errors occurred!\r\nSee also \"C:/Temp/CMakeFiles/CMakeOutput.log\".\r\n\r\n@nectario \r\nHow can you solve it?", "I am having a similar issue building but without cmake. Very frustrating experience. Nobody from stackoverflow responded. The build script seems extremely buggy. \r\n", "@nectario The CI is using cmake too.", "This is a valid issue that should not be closed. Stackoverflow did not respond not even once. Please open it. When I run this command:\r\n`bazel build --config opt //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni`\r\nI get:\r\n\r\n```\r\nERROR: C:/development/projects/tensorflow/tensorflow/java/BUILD:142:1: error loading package 'tensorflow/java/src/main/native': Encountered error while reading extension file cuda/build_defs.bzl: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 958\r\n                _create_cuda_repository(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 846, in _create_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 656, in _get_cuda_config\r\n                _cudnn_install_basedir(repository_ctx)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 211, in _cudnn_install_basedir\r\n                auto_configure_fail(\"Cannot find cudnn install path....)\r\n        File \"C:/development/projects/tensorflow/third_party/gpus/cuda_configure.bzl\", line 128, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s...))\r\n\r\n**Auto-Configuration Error: Cannot find cudnn install path.**\r\n and referenced by //tensorflow/java:libtensorflow_jni.so.\r\nERROR: Analysis of target //tensorflow/java:libtensorflow_jni failed; build aborted.\r\nINFO: Elapsed time: [0.672s]\r\n```\r\n", "@nectario how did you end up solving CudNN.h problem? Is there a hidden property I need to set?", "I believe I had to set the property in cmake: CUDNN_HOME. I also set it as an environment variable. For example, my entry is:\r\n\r\nCUDNN_HOME = C:/Program Files/NVIDIA GPU Computing Toolkit/CUDNN/5.1/cuda "]}, {"number": 9970, "title": "Add batch_size and ValidationMonitor", "body": "Add batch_size and ValidationMonitor", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@ispirmustafa Can you review this or delegate?", "@rmlarsen Can anybody review this? I had successfully run it myself, and some questions on stackoverflow  are about this example. it will be helpful.", "@GuohongLi ping?", "Closing due to lack of response for 4 weeks, please re-open a new PR with the addressed changes if you can!  Thank you for your contribution."]}, {"number": 9969, "title": "tensorflow-gpu crashes without libcuda.so", "body": "I'm on a multi-machine cluster where not all the machines have gpus. Previously with 0.11 I could use one tensorflow-gpu installation (either from source or from the provided wheel) on all the machines. I've now upgraded to 1.1, and tensorflow crashes at import on the non-gpu machines:\r\n\r\n    ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nMy current workaround is to have different tensorflows (cpu or gpu) in different conda environments and load the conda environment based on whether I need to use a gpu or not.", "comments": ["TensorFlow GPU is statically linked against cuda, you need cuda drivers. Having CUDA does not require having a GPU, so a workaround is to install cuda on your non-GPU machines", "So this is expected behavior and I was just lucky somehow with 0.11?", "I think it was always this behavior. There was a bit of refactoring in how cuda paths are found, perhaps 0.11 managed to find the libcuda.so on the machine", "According to `locate` the non-gpu machines (at least the ones I checked) don't have libcuda.so. Mystery unsolved I guess :)", "I encountered a similar issue:\r\nI am using nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 docker image.\r\n\r\nWhile trying to upgrade tensorflow from 0.12.1 to 1.1.0 , I get this error:\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\n\r\n", "I think it is a bug in tensorflow 1.1.0.\r\nI downgraded to tensorflow 1.0.0, and it works.\r\n\r\nHere are the lines from my Dockerfile:\r\n\r\n\r\n```\r\nFROM nvidia/cuda:8.0-devel-ubuntu16.04\r\n\r\nRUN pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl\r\n\r\n# RUN pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.1.0-cp27-none-linux_x86_64.whl\r\n\r\n```", "I'm not sure I understand why this issue was marked as Closed. \r\nWe have exactly the same problem: we're on a multi-node slurm cluster where the head node does not have a GPU. In order to run tensorboard on the head node without having to use a separate tensorflow-cpu environment (which is fine too, but that's one more thing to keep track of), we need to have the NVIDIA driver libraries on the head node. \r\n\r\nFortunately, copying the relevant libraries from one of the compute nodes to the head node does the trick. Unfortunately, there's a bunch of them, together with a host of symbolic links. \r\nHere's the simplest way I found to get everything in place (replace 384.90 by whatever driver version you have):\r\n#### on a compute node\r\n```\r\ntar cvf /path/to/folder/accessible/from/head/node/libs_nvidia384-90.tar --files-from /dev/null  # create an empty tar file so that we can add files to it\r\npushd /usr/lib/x86_64-linux-gnu/\r\nfor i in `ls *384.90`; do j=`echo $i | sed 's|.384.90||'`; tar rvf /path/to/folder/accessible/from/head/node/libs_nvidia384-90.tar ${j}*; done  # add all relevant files to the .tar\r\n```\r\n#### on head node\r\n```\r\npushd /usr/lib/x86_64-linux-gnu/\r\nsudo tar xvf /path/to/folder/accessible/from/head/node/libs_nvidia384-90.tar\r\n```\r\nWith this, we're now able to run tensorboard on the head node. \r\n", "It was marked as closed because it was working as intended -- there are two versions of TensorFlow, one that requires cuda libraries, and the one that doesn't, so it's expected that the former will crash without cuda libraries present.\r\n\r\nI agree see that it adds to management pain, if someone has an idea of a solution (ie, how to restructure tensorflow to remove static linking requirement) it could make to make a feature request with an overview of how to solve this.", "I see, thanks. My comment is just a hack to work around the issue, hopefully it will be useful to someone in a similar situation.", "The proper solution is to use official instructions to install CUDA. You do not need a GPU in order to install CUDA", "We have the CUDA toolkit installed on the head node, but not the CUDA driver. I didn't know it was possible to install the driver without a GPU, but if that's the case, then indeed it sounds like the proper solution.", "I had a similar issue, and it was solved using nvidia-docker instead of just docker.  It is just the difference between the forced runtime, and regular in my case.  The error message doesn't give information about the shared library failing to load during its initialization.  It seems to respond as if it is missing.."]}, {"number": 9968, "title": "ValueError: Refusing to perform an overparameterized separable convolution", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1rc\r\n- **Bazel version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: No CUDA\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI am using tensorflow function tf.nn.separable_conv2d. I want to understand why  channel_multiplier * in_channels > out_channels is not allowed. It was not clear anywhere from the documentation.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I believe it is a documentation request. Reason for throwing this error should be documented because I haven't been able to find its answer anywhere.", "For example, the only way currently to get around this bug if in_channels*channel_multiplier > out_channels is to use a depthwise separable convolution followed by a pointwise convolution which causes the network to train twice as (most likely) because different kernels are used for the latter.", "Can you share a very minimal standalone example or Colab for TF 2.x to reproduce this ValueError?", "Hi @singlasahil14 ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Attaching relevant [thread](https://stackoverflow.com/questions/45193238/why-out-channels-must-be-greater-then-channel-multiplier-in-channels-in-pointw?noredirect=1&lq=1) for reference though. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 9967, "title": "e libcupti", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 9966, "title": "gcc failed and fatal error: infiniband/verbs.h: No such file or directory", "body": "When building TensorFlow from source\r\n\r\nERROR: /home/rock/tensorflow/tensorflow/core/distributed_runtime/rdma/BUILD:81:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rdma:rdma' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 118 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/core/distributed_runtime/rdma/rdma.cc:10:0:\r\n./tensorflow/core/distributed_runtime/rdma/rdma.h:8:30: fatal error: infiniband/verbs.h: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request (most install issues are configuration/environment issues). There is also a larger community that reads questions there. Thanks!", "@ChinaMobileNG  - you complied with VERBS support (this is what you wanted right ?) \r\nand in this case you need to install IB verbs SW stack (libibverbs and all of it kernel dependencies == OFED or MLNX_OFED if using Mellanox devices)", "Posted a similar question on Stackoverflow:\r\nhttps://stackoverflow.com/questions/44267616/bazel-build-tensorflow-python-toolsstrip-unused-compilation-failure\r\n\r\nThanks.\r\n", "I have the same question. Can you give me some suggestions?", "Try using `local_resources` for your bagel build commands:\r\n`bazel build tensorflow/python/tools:strip_unused --local_resources 2048,.5,1.0`", "I followed your advice, but there were still the following error\r\nERROR: /home/fhkj/tensorflow/tensorflow/contrib/verbs/BUILD:104:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 167 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/contrib/verbs/rdma_mgr.h:24:0,\r\n                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/contrib/verbs/rdma.h:21:30: fatal error: infiniband/verbs.h: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/python/tools:strip_unused failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n@achainan ", "I solved the problem by following the link below\r\n[](https://github.com/tensorflow/tensorflow/issues/9818)\r\nbut there's another error\r\nERROR: /home/fhkj/tensorflow/tensorflow/python/BUILD:2687:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so -Wl,--version-script ... (remaining 10 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/usr/bin/ld.gold: error: cannot find -libverbs\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n@achainan ", "https://github.com/tensorflow/tensorflow/issues/9818", "Thanks  very much! I have solved the problem by installing libibverbs-dev.\r\n    sudo apt-get update\r\n    sudo apt-get install libibverbs-dev"]}, {"number": 9965, "title": "fix a bug in using record_host_persistent_memory_allocation", "body": "", "comments": ["Can one of the admins verify this patch?", "Indeed, this bug only affects 32 bits builds. Is there any one using 32-bits tensorflow? ", "@samjabrahams are Raspberry Pi TF builds 32 bit by any chance?", "@yaroslavvb yes- though unfortunately I won't be able to play around with the effect of this code for a week or two (traveling).\r\n\r\nIs this the gist of the problem: casting between `size_t` and `int64` on 32-bit systems causes memory to not be freed/tracked properly? Just trying to make sure I'm following.", "@samjabrahams  Yes. As you say.\r\n\r\nThe following code:\r\n\r\n\t#include <iostream>\r\n\t#include <stdint.h>\r\n\r\n\tvoid record_host_persistent_memory_allocation(int64_t size) {\r\n\t\tstd::cout << size << std::endl;\r\n\t}\r\n\tint main() {\r\n\t\tsize_t a = 3;\r\n\t\trecord_host_persistent_memory_allocation(-a);\r\n\t\treturn 0;\r\n\t}\r\n\r\noutput is \"4294967293\" on 32 bits build and \"-3\" on 64 bits build.", "Can one of the admins verify this patch?", "I'll take a looksy when I am near some hardware. I also need to double check exactly which flags are set when TensorFlow compiles.", "@tensorflow-jenkins test this please"]}, {"number": 9964, "title": "Check the return value of fopen", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Thanks for the fix!"]}, {"number": 9963, "title": "configure script overrides user's bazelrc", "body": "The `./configure` script will write to a `.bazelrc` file in the cwd to save some of it's options (in particular, jemalloc). Unfortunately this has the effect of overriding `~/.bazelrc`. This is undocumented, and probably should not happen. For now, the solution is copy your own bazelrc into the tensorflow root before configuring - fortunately `./configure` will not overwrite it and only append to it.", "comments": ["@jart, any thoughts about this?", "I'm aware of this behavior, consider it suboptimal, and I believe we should escalate this to Bazel. I've filed https://github.com/bazelbuild/bazel/issues/3022. Please follow there for progress.\r\n\r\nThe workaround is to copy the stuff in your `~/.bazelrc` into `tensorflow/.bazelrc`. Our configure script will not delete the file.", "Re-opening because I'm thinking of modifying our configure to work around this automatically.", "Im getting the following error while trying to build `so` file\r\n\r\n> Unexpected error reading .blazerc file '/Users/Johnny/Downloads/tensorflow-master/.tf_configure.bazelrc'\r\n\r\nFollowing is the command that I tried,\r\n`\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`\r\n"]}, {"number": 9962, "title": "tf.contrib.seq2seq attention_wrapper.py memory_sequence_length can not set None", "body": "tf version '1.1.0-rc2'\r\nThe problem is in attention_wrapper.py  77 \r\n  memory_sequence_length = ops.convert_to_tensor(\r\n      memory_sequence_length, name=\"memory_sequence_length\")\r\n\r\nDon't check if memory_sequence_length is None", "comments": ["@ebrevdo ", "I believe this has been fixed in TF 1.2rc2 ([here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L76))."]}, {"number": 9961, "title": "[CMAKE] Unresolved external symbol rdft", "body": "\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (Visual studio 2015)\r\n- **TensorFlow installed from (source or binary)**: via cmake and includes into c++ project as static lib.(tensorflor_static.lib file).\r\n\r\n### Describe the problem\r\nAfter successful building Release version I tried to add tensorflow_static.lib and dependencies into c++ project. Then linker threw an error:\r\n\r\n> Severity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\n> Error\tLNK2001\tunresolved external symbol rdft\tMyLib\tC:\\path\\to\\release\\tensorflow_static.lib(spectrogram.obj)\t1\t\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I build tensorflow from sources and use the latest version of master branch.\r\n Windows 10 (Visual studio 2015). Build in Release, x64.\r\n", "Hi @mrlzla, thank you for reaching out. Could you please provide any additional information that might help us reproduce this problem? Exact command lines or precise sequences of steps would be very helpful. Thanks!", "That's how I compile tensorflow:\r\n\r\n```\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=%CONFIGIRATION% -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -DPYTHON_EXECUTABLE=%PYTHONPATH% -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF\r\nMSBuild /p:Configuration=%CONFIGIRATION% tensorflow_static.vcxproj\r\n```", "@mrry could you reassign to whomever supports Windows configuration?", "AFAICT, @petewarden added the spectrogram code, which appears to be causing the failure, in 7c9d2a458ee6cb925a0b3d23793d0e356a6eac12.", "What's the best way to reproduce this, since I believe the standard Windows cmake builds are working? Is there a script I can run to kick off a test that shows this error?", "I'd guess the issue is `-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX`, which AFAIK isn't part of the standard build. Probably easiest to run the build on a GCE VM (@gunan or @yifeif might have a canonical configuration script) using the following script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/cpu/cmake/run_build.bat\r\n\r\n...although you'll need to modify the CMake invocation with the additional AVX flag.", "Correct, windows build uses that script.\r\nWe do not currently build on windows using AVX on CI, because not all machines seem to have AVX, but we can revisit this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We haven't had a chance to work on this one for the last few months, and I'm not sure when we will get to it, so closing it as infeasible. Contributions are very welcome, and please reopen if this is still an issue for you."]}, {"number": 9960, "title": "Placeholder modification behavior", "body": "I am using Tensorflow 1.0.1, on Mac OS Sierra 10.12.4, with CPU in a jupyter notebook (but the behavior I am describing appeared in a regular python script too).\r\n\r\nThe problem I am having is with placeholders. I have been using tensorflow for only 1 month so I was not really comfortable with using them and I made a mistake that took me a very long time to debug.\r\nHere is the type of code I wrote:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nx_ = tf.placeholder(tf.float64, [5])\r\nx_ = tf.where(tf.is_nan(x_), tf.zeros_like(x_), x_)\r\na = tf.constant(1, shape=[5])\r\na = tf.cast(a, tf.float64)\r\nc = a + x_\r\ninit_op = tf.group(\r\n    tf.global_variables_initializer(),\r\n    tf.local_variables_initializer())\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nx = np.arange(5, dtype=float)\r\nx[3] = np.nan\r\nsess.run(c, feed_dict={x_: x})\r\n```\r\nTo this, the output was: `array([  1.,   2.,   3.,  nan,   5.])`, which is not what I expected since I had replaced all the `nan` with the `tf.where`. I understood only later that it was because tensorflow was feeding the placeholder in the latest position it appeared (maybe not exactly how it's implemented, but empirically, that is what happens).\r\nTherefore a good (in the sense of what I intended to do) code is:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nx_ = tf.placeholder(tf.float64, [5])\r\nr = x_\r\nr = tf.where(tf.is_nan(r), tf.zeros_like(r), r)\r\na = tf.constant(1, shape=[5])\r\na = tf.cast(a, tf.float64)\r\nc = a + r\r\ninit_op = tf.group(\r\n    tf.global_variables_initializer(),\r\n    tf.local_variables_initializer())\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nx = np.arange(5, dtype=float)\r\nx[3] = np.nan\r\nsess.run(c, feed_dict={x_: x})\r\n```\r\nThis indeed outputs `array([ 1.,  2.,  3.,  1.,  5.])`. However, to me it's a bit weird that you would need to throw in an extra variable to achieve this purpose and even weirder that I didn't receive any error when modifying my placeholder.\r\nMy suggestions are:\r\n- Throw an error or a warning when reallocating a placeholder.\r\n- State clearly that placeholders are not meant to be modified (I looked for it in the docs and didn't find it, maybe I didn't look well enough).\r\n- Allow placeholders to be modified (might be offputting for someone who understands it well).\r\n", "comments": ["Hi @zaccharieramzi, I would argue this is just how Python works. In your code, you're not actually reallocating or modifying your original placeholder, but reassigning the x_ variable, and then feeding the new x_ value (which is the where op):\r\n```python\r\nx_ = tf.placeholder(tf.float64, [5])\r\nx_ = tf.where(tf.is_nan(x_), tf.zeros_like(x_), x_)\r\n...\r\nsess.run(c, feed_dict=x_: x)\r\n```\r\n\r\nThis is analogous to doing something like:\r\n```python\r\nx_ = 1\r\nx_ = x_ * 2\r\nprint(x_) # prints 2\r\n```\r\nIn general, I think it's not a bad idea to use a new variable for each TensorFlow op to avoid bugs like these. Note you can write it in the same number of lines:\r\n```python\r\nx_ = tf.placeholder(tf.float64, [5])\r\nr = tf.where(tf.is_nan(x_), tf.zeros_like(x_), x_)\r\n```\r\nI'm gonna close this for now as I don't think it's a TensorFlow bug, but feel free to comment further.", "I agree with your explanation of what I am doing (wrong choice of words on my end). \r\nI also agree with your analogy. However, it's because it's analogous that I made this mistake.\r\nI am not saying that it's a bug, but I think it can be a bit puzzling the first time you look at it, and that is why I was suggesting to throw a warning when reassigning a placeholder because it is going to lead to an unwanted behavior.\r\n\r\nI also agree with your general comment on naming ops.", "True, I agree it's not really a bug per-se. Even so, I don't think we should change the default assignment operator to print a warning (in fact, I don't think it's even possible to do so in Python). Thank you for reporting though, hopefully if others run into the same problem they will find this thread."]}, {"number": 9959, "title": "Tensorboard Error Problem 'Can not convert a AdamOptimizer into a Tensor or Operation.'", "body": "I'm using tensorflow through anaconda3(64bit).\r\nI have a problem when I try to make tensorboard.\r\ncause I'm beginner, my ground is too low. I can't understand what is problem.\r\nplz help me. \r\n------------------------\r\nI'm not sure here is right or not to write this problem. \r\nplz tell me If I'm wrong.\r\n\r\n### System information\r\nServer Information:\r\n\r\nYou are using Jupyter notebook.\r\n\r\nThe version of the notebook server is 4.2.3 and is running on:\r\nPython 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\r\n\r\nCurrent Kernel Information:\r\n\r\nPython 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 5.1.0 -- An enhanced Interactive Python.\r\n?         -> Introduction and overview of IPython's features.\r\n%quickref -> Quick reference.\r\nhelp      -> Python's own help system.\r\nobject?   -> Details about 'object', use 'object??' for extra details.\r\n\r\nWhen I command this line to check tf.version,\r\n \r\n tf.__version__\r\nOut[63]:\r\n'1.0.0-alpha'\r\n\r\n### Describe the problem\r\n\r\nTypeError: Can not convert a AdamOptimizer into a Tensor or Operation.\r\nTypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x000001E08E7E1CF8> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)\r\n\r\nwhat should I do? plz help me\r\n\r\n### Source code / logs\r\nw2_hist=tf.summary.histogram(\"weight2\",W2)\r\ncost_summ=tf.summary.scalar(\"cost\",cost)\r\nsummary=tf.summary.merge_all()\r\n#Create Summary writer\r\nwriter=tf.summary.FileWriter('C:\\\\Users\\\\jh902\\\\Documents\\\\.logs')  #I'm using window 10\r\nwriter.add_graph(sess.graph)\r\ns,_= sess.run([summary, optimizer], feed_dict={X: x_data, Y: y_data})\r\nwriter.add_summary(s, global_step=2001)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __init__(self, fetches, contraction_fn)\r\n    266         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\r\n--> 267             fetch, allow_tensor=True, allow_operation=True))\r\n    268       except TypeError as e:\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   2469     with self._lock:\r\n-> 2470       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   2471 \r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   2558       raise TypeError(\"Can not convert a %s into a %s.\"\r\n-> 2559                       % (type(obj).__name__, types_str))\r\n   2560 \r\n\r\nTypeError: Can not convert a AdamOptimizer into a Tensor or Operation.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-20-b8394996caf6> in <module>()\r\n----> 1 s,_= sess.run([summary, optimizer], feed_dict={X: x_data, Y: y_data})\r\n      2 writer.add_summary(s, global_step=2001)\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    950 \r\n    951     # Create a fetch handler to take care of the structure of fetches.\r\n--> 952     fetch_handler = _FetchHandler(self._graph, fetches, feed_dict_string)\r\n    953 \r\n    954     # Run request and get response.\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __init__(self, graph, fetches, feeds)\r\n    406     \"\"\"\r\n    407     with graph.as_default():\r\n--> 408       self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n    409     self._fetches = []\r\n    410     self._targets = []\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in for_fetch(fetch)\r\n    228     elif isinstance(fetch, (list, tuple)):\r\n    229       # NOTE(touts): This is also the code path for namedtuples.\r\n--> 230       return _ListFetchMapper(fetch)\r\n    231     elif isinstance(fetch, dict):\r\n    232       return _DictFetchMapper(fetch)\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __init__(self, fetches)\r\n    335     \"\"\"\r\n    336     self._fetch_type = type(fetches)\r\n--> 337     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n    338     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)\r\n    339 \r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in <listcomp>(.0)\r\n    335     \"\"\"\r\n    336     self._fetch_type = type(fetches)\r\n--> 337     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n    338     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)\r\n    339 \r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in for_fetch(fetch)\r\n    236         if isinstance(fetch, tensor_type):\r\n    237           fetches, contraction_fn = fetch_fn(fetch)\r\n--> 238           return _ElementFetchMapper(fetches, contraction_fn)\r\n    239     # Did not find anything.\r\n    240     raise TypeError('Fetch argument %r has invalid type %r' %\r\n\r\nC:\\Users\\jh902\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py in __init__(self, fetches, contraction_fn)\r\n    269         raise TypeError('Fetch argument %r has invalid type %r, '\r\n    270                         'must be a string or Tensor. (%s)'\r\n--> 271                         % (fetch, type(fetch), str(e)))\r\n    272       except ValueError as e:\r\n    273         raise ValueError('Fetch argument %r cannot be interpreted as a '\r\n\r\nTypeError: Fetch argument <tensorflow.python.training.adam.AdamOptimizer object at 0x000001E08E7E1CF8> has invalid type <class 'tensorflow.python.training.adam.AdamOptimizer'>, must be a string or Tensor. (Can not convert a AdamOptimizer into a Tensor or Operation.)", "comments": ["I will help you quickly but I think you'll need to **move this to [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)** as it will be closed as it's not a bug. \r\nI think you're confused by the output of your sess.run call. It does not return multiple variables in the form s,_ but instead it will return an array in the form [summary,optimizer]. In your writer you are passing the whole array and hence you are passing the optimizer. Please start a [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) thread, link it here and close the issue. I will come and explain more in the [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) Thread.\r\n\r\nEDIT: My comment on what sess.run return may not be correct in hindsight for those stumbling upon this.", "I've found your thread and will help you there. Thanks"]}, {"number": 9958, "title": "Android demo app crashes when using quantized model obtained from graph_transform tool?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nOnly edited `ClassifierActivity.java` to suit a custom model\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid smartphone / host machine: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX 860M\r\n\r\n### Describe the problem\r\nI am currently following the guide in: http://nilhcem.com/android/custom-tensorflow-classifier\r\n\r\nto train a custom classifier. However, I am using my own frozen graph that I obtained from my own training. What I noticed was when I used the quantized graph obtained through the `graph_transform` method, the app simply crashes without even running. In the guide, it is recommended to run this command on the inference graph:\r\n\r\n```\r\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\\r\n  --input=/tf_files/retrained_graph.pb \\\r\n  --output=/tf_files/retrained_graph_optimized.pb \\\r\n  --input_names=Mul \\\r\n  --output_names=final_result\r\n```\r\n\r\nWhile I think it may be in conflict with the quantized graph transformations, I ran this command to test, and the app did crash. Here are the observations for all 4 permutations I tested:\r\n\r\n1. quantization + optimize_for_inference = **app crash**\r\n2. quantization only = **app crash**\r\n3. optimize_for_inference on frozen graph = **app works**\r\n4. frozen_graph only = **app works**\r\n\r\nSo my conclusion is the quantization operations within the quantized graph caused the app to fail. \r\n\r\nHere is the quantization command I ran:\r\n\r\n```\r\n/home/kwotsin/tensorflow-android/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model_mobilenet.pb \\\r\n--out_graph=./quantized_model_mobilenet.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='MobileNet/Predictions/Softmax' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\nAnd the java file I edited to build my APK:\r\n\r\n```\r\n  private static final int INPUT_SIZE = 224;\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128;\r\n  private static final String INPUT_NAME = \"Placeholder_only\";\r\n  private static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\r\n\r\n  private static final String MODEL_FILE = \"file:///android_asset/quantized_model_mobilenet.pb\";\r\n  private static final String LABEL_FILE =\r\n      \"file:///android_asset/mobilenet_labels.txt\";\r\n```\r\n\r\nOther than these edit, every other file is the same. I then imported this project in Android Studio, and ran the `build apk` option located on the top toolbar of Android Studio.\r\n\r\nSo far, existing tutorials I've seen (e.g. those from Pete Warden) use the `quantize_graph` method to run on mobile devices. Is `graph_transform` compatible for quantizing models for mobile devices yet?", "comments": ["@asimshankar can you comment or redirect to someone who can? Thanks!", "@kwotsin : Do you have any logs/stracktraces you can share from the crash? That will help in diagnosing the problem. Also, is it possible to share the graph that you are quantizing? Basically, I'm looking for detailed information on the error and/or the ability to reproduce the problem to help us diagnose.\r\n\r\nFYI @suharshs @petewarden ", "@asimshankar Is there a way to see the error log if an app crashes? So far the only idea I have that it's not working is just that the app crashes. Because I can't find the error log, I can't seem to find the source of the problem as well.\r\n\r\nI have attached the PB files for both the frozen and quantized model here. https://www.dropbox.com/sh/grp7xlxu4r59qss/AADV9imhKpY9auiNAkaSsjyMa?dl=0\r\n\r\nI have noticed another problem as well: when I tried to predict custom labels with another model trained on the flowers dataset, my app crashes even if I'm not quantizing it. Could this problem be related to why quantization fails?", "The errors should appear like any android app, see https://developer.android.com/studio/debug/am-logcat.html and https://developer.android.com/studio/command-line/logcat.html\r\n", "Unfortunately I am currently unable to get my android device mounted on my computer, and I've tried asking on StackOverflow in this link some time ago: http://stackoverflow.com/questions/43996994/android-unable-to-mount-android-phone-although-lsusb-shows-device-and-after-ins\r\n\r\nI am currently building the APK before transferring it onto dropbox to let my android phone download the app as a workaround.\r\n\r\nAs the labels and pb file names needs to be changed in this file as well, before building the APK. To keep things simple, I simply renamed the pb file name and the labels file name to be the same as what the demo app provided, and the frozen graph APK works fine. Once again, when I tried quantizing the graph, my app crashes. Is there a way to get the error log files without using ADB?", "It will be hard to debug without access to debugging information from the android logs.\r\n\r\nI don't _think_ there is a way to get access to logs without connecting it to your computer, but maybe there is. That would be a question better suited to Android support forums.", "@asimshankar I have finally solved the ADB issue by doing a factory reset of the android phone and then enabling PTP instead of MTP as the file transfer method. \r\n\r\nFrom the Android Studio's android monitor, I have gotten these log when my app crashes using the quantized graph.\r\n\r\n```\r\n05-19 16:48:38.403 23564-23564/? I/art: Late-enabling -Xcheck:jni\r\n05-19 16:48:38.509 23564-23564/? W/ActivityThread: Application org.tensorflow.demo can be debugged on port 8100...\r\n05-19 16:48:38.609 23564-23564/? D/ContextHelper: convertTheme. context->name=org.tensorflow.demo themeResourceId=2131034118\r\n05-19 16:48:38.611 23564-23564/? D/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@7b0b069\r\n05-19 16:48:38.614 23564-23564/? I/PhoneWindow: [generateLayout] setColorNavigationBar => color=0x ff000001\r\n05-19 16:48:38.617 23564-23564/? D/PhoneWindowEx: [LMJ][PWEx][generateLayout] setNavigationBarColor2 : colors=0xff000000\r\n05-19 16:48:38.617 23564-23564/? I/PhoneWindow: [setNavigationBarColor2] color=0x ff000000\r\n05-19 16:48:38.704 23564-23564/? D/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@7b0b069\r\n05-19 16:48:38.704 23564-23564/? D/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@7b0b069\r\n05-19 16:48:38.722 23564-23597/? D/OpenGLRenderer: Render dirty regions requested: true\r\n05-19 16:48:38.725 23564-23597/? I/Adreno-EGL: <qeglDrvAPI_eglInitialize:410>: EGL 1.4 QUALCOMM build:  ()\r\n                                               OpenGL ES Shader Compiler Version: E031.25.03.00\r\n                                               Build Date: 12/12/14 \uae08\r\n                                               Local Branch: LA.BF.1.1_RB1.05.00.00.002.031_20141212_01821442\r\n                                               Remote Branch: \r\n                                               Local Patches: \r\n                                               Reconstruct Branch: \r\n05-19 16:48:38.729 23564-23597/? I/OpenGLRenderer: Initialized EGL, version 1.4\r\n05-19 16:48:38.763 23564-23597/? D/OpenGLRenderer: Enabling debug mode 0\r\n05-19 16:48:38.777 23564-23564/? D/Atlas: Validating map...\r\n05-19 16:48:38.890 23564-23564/? W/ArrayUtils: Ignoring invalid value mw_continuous-picture\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value emboss\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value sketch\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value neon\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value asd\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value backlight\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value flowers\r\n05-19 16:48:38.891 23564-23564/? W/ArrayUtils: Ignoring invalid value AR\r\n05-19 16:48:38.907 23564-23564/? I/tensorflow: CameraConnectionFragment: Valid preview sizes: [1920x1440, 1920x1080, 1776x1080, 1440x1080, 1280x960, 1280x720, 960x720, 880x720, 864x480, 848x480, 800x480, 720x480, 768x432, 640x480, 576x432, 480x320]\r\n05-19 16:48:38.908 23564-23564/? I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [384x288, 352x288, 320x240, 240x160, 176x144, 160x120]\r\n05-19 16:48:38.908 23564-23564/? I/tensorflow: CameraConnectionFragment: Chosen size: 480x320\r\n05-19 16:48:38.914 23564-23564/? W/ArrayUtils: Ignoring invalid value emboss\r\n05-19 16:48:38.914 23564-23564/? W/ArrayUtils: Ignoring invalid value sketch\r\n05-19 16:48:38.914 23564-23564/? W/ArrayUtils: Ignoring invalid value neon\r\n05-19 16:48:38.915 23564-23564/? W/ArrayUtils: Ignoring invalid value asd\r\n05-19 16:48:38.915 23564-23564/? W/ArrayUtils: Ignoring invalid value backlight\r\n05-19 16:48:38.915 23564-23564/? W/ArrayUtils: Ignoring invalid value flowers\r\n05-19 16:48:38.915 23564-23564/? W/ArrayUtils: Ignoring invalid value AR\r\n05-19 16:48:38.936 23564-23564/? I/TensorFlowImageClassifier: Reading labels from: imagenet_comp_graph_label_strings.txt\r\n05-19 16:48:38.944 23564-23564/? I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\n05-19 16:48:38.949 23564-23564/? E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n05-19 16:48:38.950 23564-23564/? I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n05-19 16:48:39.065 23564-23564/? I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n05-19 16:48:39.879 23564-23564/? A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 23564 (tensorflow.demo\r\n```\r\n\r\nI've not done any android development before, so I would really appreciate it if you could share what this error log means. \r\n\r\nAlso, does the image input size (299 * 299) that my model requires does cause any problem with the camera app activity? I'm not sure if the image will be distorted to fit into 299*299 to make it work.", "@kwotsin your logcat cuts off in the middle of the relevant error, can you paste the rest please?", "@andrewharp After several days of testing, I have managed to locate the source of the error, which is actually coming from `quantize_nodes`. As the ClassifierActivity.java file requires an input and output nodes of:\r\n\r\n```\r\nprivate static final String INPUT_NAME = \"Placeholder_only\";\r\nprivate static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\r\n```\r\nI realized the quantized file should have such an input output node as well. After confirming that the frozen model has these input and output nodes, I checked whether the quantized version has these nodes as well, and I realized it doesn't. Instead, it has these nodes after quantization:\r\n\r\n```\r\n(<tf.Tensor 'import/MobileNet/conv_ds_6/dw_batch_norm/batchnorm/sub/_7__cf__7_quantized_const:0' shape=(256,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_1/batch_norm/batchnorm/sub/_25__cf__25_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_1/weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_10/pw_batch_norm/batchnorm/sub/_29__cf__29_quantized_const:0' shape=(512,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_12/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_1/convolution_eightbit/Placeholder_only/reduction_dims:0' shape=(1,) dtype=int32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_9/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_5/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_9/dw_batch_norm/batchnorm/Rsqrt/_34__cf__34_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_quantized_const:0' shape=(3, 3, 32, 1) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/biases_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_const:0' shape=(1, 1, 1024, 1024) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_14/depthwise_conv/biases_quantized_const:0' shape=(1024,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/fc_16/weights_quantized_min:0' shape=() dtype=float32>,)\r\n```\r\n\r\nAfter removing each graph transform one by one, I hoped to see if there is a specific transformation which after I removed, will give me the correct input node called 'Placeholder_only', which I included in the graph when I first froze the model. After checking all transformations, I found `quantize_nodes` is the transformation that removed the input_node. After removing `quantize_nodes` from the transformation, I have these nodes instead:\r\n\r\n```\r\n(<tf.Tensor 'import/Placeholder_only:0' shape=<unknown> dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_const:0' shape=(512,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53:0' shape=(512,) dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_const:0' shape=(512,) dtype=quint8>,)\r\n```\r\n\r\nAnd now my quantized model works on the app. Is there a reason why `quantize_nodes` would cause the model to not work? **However**, my accuracy drops dramatically after removing this operation, till an extent where quantization won't bring any benefit to the application. \r\n\r\nOne possible reason I could think of is that the input node is missing or at least changed to a Tensor which I can't identify. As I scanned through the tensors available after importing the quantized pb file, there seem to be only weight tensors, but not any input tensor. Is this supposed to be the case?\r\n\r\nAlso, even after I've read the documentation here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#quantize_nodes \r\nI don't quite understand what `quantize_nodes` does and the benefits that may be gained. What are the purpose and benefits of this tool?\r\n\r\nAlso, strangely for the logcat, the error log is all I could see when I run the app and after it crashes immediately. There is nothing produced after the 'fatal signal' output. My current logcat status is at 'Verbose' - is this the correct status to use?\r\n\r\n\r\n", "Seems to be issue #8698\r\nThis is probably a bug in `merge_duplicate_nodes` (called by `quantize_nodes`)\r\nSee #8897 for reference", "I think the underlying issue here is that you need to specify the right input and output names when you call `transform_graph` too. In the code above, you're using the default of `Mul` and `final_result` when you invoke the tool, but they should be `input` and `MobileNet/Predictions/Softmax`, like your application code.", "@petewarden `Mul` and `final_result` are not the input and output names I used, but rather, the example usage given in the documentation. I included them just for the sake of illustration, but the actual command I used had the input and output node names I used in the java code as well. In fact, I think it would not be possible to run the graph transform tool successfully if the node names are invalid or do not exist within the graph. I think, at least for my use case, quantize_nodes somehow gave me the error when running.", "@kwotsin @petewarden  @andrewharp Were you able to solve this issue? I've hit the same error.. ", "@anandcu3 Did you try removing `quantize_nodes` from your transformation?", "@kwotsin But isn't `quantize_nodes ` necessary for improving performance ?", "YMMV but the last time I tried it, this gave me worse results instead. I think it shouldn't take too long for you \r\nto verify whether it gives better accuracy or not - perhaps you can try it and see if your results does improve.", "@kwotsin  I tried it and you are right the ` Fatal signal 11 (SIGSEGV), code 1` error is resolved, but overall there is no change in either accuracy or performance. However the interesting thing is that the model size inreased from 2.3MB to around 6MB.Though the output is the same. AFAIK `quantize_nodes` is the differentiator in terms of performance..", "@anandcu3 To verify with you, 'no change in either accuracy or performance' - does it mean the accuracy is retained as your original model, or is the poor performance retained? I think `quantize_nodes` is still largely experimental now, but so far I've seen quite good results with just `quantize_weights` alone.", "@kwotsin Accuracy is retained and the performance is also the same as original model.. ", "I fine-tuned a mobilenet model for binary classification. The problem is when I use Quantised model on Android, it produces exactly same prediction values for all images after retraining. The same mobilenet model when non-quantised produces reasonable values. \r\nThis problem is only on Android. On iOS it works nicely for both non-quantised and quantised model. What could be reason for this and solution for Android? I tried \r\n1. pre-compiled .so of tensorflow inference\r\n2. compiling .a file with selective registration. \r\n\r\n", "I solved this problem by updating libtensorflow_inference.so from nightly build. Leave this message just in case someone would have same problem as we had.\r\nTry this.\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/native/", "https://github.com/tensorflow/tensorflow/issues/11182\r\nI solved it by compiling the tensorflow .a lib file for android with flag `-DTENSORFLOW_DISABLE_META`", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We're now focusing on using TF Lite for quantization, and the demo app shows how to run quantized MobileNet, so closing this one as obsolete.", "If I use the frozen graph the app works but if I use optimize_for_inference_lib.optimize_for_inference, I get loading error in android. Is there a solution for this?\r\n", "The classification is work with frozengraph, is there something I am missing?\r\n"]}, {"number": 9957, "title": "Branch 155393864", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "cc @mrry. There seems to be a test failure in Windows CMAKE related to dataset_construct_op:\r\nhttps://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/2372/console", "@caisq That's mysterious, because there's no platform-specific code in that test (as far as I can tell). To unblock the process while I investigate, can you add the following line:\r\n\r\n```\r\n      \"${tensorflow_source_dir}/tensorflow/contrib/data/python/kernel_tests/dataset_constructor_op_test.py\"\r\n```\r\n\r\n...to the list of tests to disable on Windows, here:\r\n\r\nhttps://github.com/av8ramit/tensorflow/blob/5abe2a643c591f91d73abd53502ab2ac344298e6/tensorflow/contrib/cmake/tf_tests.cmake#L217", "@mrry @caisq done, rerunning tests now.", "Hi @mrry  I have a question: why these new ops (like RangeDatasetOp) are not inherited from ResourceOpKernel ?", "@snnn AFAIK `ResourceOpKernel<T>` doesn't give you enough control over how the resource is created. In particular, the Dataset constructor ops need to create an ephemeral resource in the step-local container, whereas\u2014if I'm reading the code correctly\u2014`ResourceOpKernel<T>` only creates session-scoped (or more widely shared) resources.", "@mrry  Thanks for the detailed explanation", "Hi @mrry , can you use MakePerStepResourceHandle<T> instead? \r\nHow about add a template arg to ResourceOpKernel class, like\r\n\r\n\ttemplate <typename T, bool per_step=false>\r\n\tclass ResourceOpKernel : public OpKernel {\r\n\t ...\r\n\t \r\n\t if (context->expected_output_dtype(0) == DT_RESOURCE) {    \r\n\t\t   if(per_step){\r\n\t\t\t   ...\r\n\t\t\t} else {\r\n\t\t\t....\r\n\t\t   }\r\n\t\t} \r\n\t}\r\n\r\nI want to create a new Dataset OP for a proprietary file format, now it is not easy to do so. \r\n", "It looks like `MakePerStepResourceHandle` would work, yes. As for adding a template parameter to `ResourceOpKernel<T>`, I don't have a strong feeling either way... but it wouldn't be useful for creating per-step `DatasetBase` resources, because the `CreateResource()` method does not have access to the `OpKernelContext*`, and this is usually needed for getting the arguments to the `Dataset` constructor."]}, {"number": 9956, "title": "how to switch between cpu and gpu mode?", "body": "Hi,\r\nI have installed tensorflow-gpu.  I want to employ the cpu mode, whether i need to install tensorflow for cpu?\r\nand if the two are all installed, how to switch between them?", "comments": ["you can write code like this\r\n```\r\nwith device(\"/cpu:0\")\r\n```", "You don't need to install for CPU. CPU installation is by default. Using `with device(\"/cpu:0\")` will make all your operations run in CPU explicitly. To check the device placement i.e. where your operation is being executed you can take a look at the _LOG_DEVICE_PLACEMENT_ flag.", "@utkarsh39 , you mean  just installing  tensorflow-gpu is ok?  where is  LOG_DEVICE_PLACEMENT flag?\r\nI am a newer, thanks your help.", "@dugu9sword , thanks , i will have a try. ", "Closing this issue since it does not require attention of tensorflow developers"]}, {"number": 9955, "title": "MatchPath Implementation", "body": "Hi,\r\n\r\nI can't seem to find the implementation of the \"MatchPath\" function. I understand this depends on the environment being used, but is there a default implementation somewhere? I want to find out the details of how the path patterns are treated in TensorFlow so I can write a converter from a TF pattern to a regex and use it within another language that has no access to the TF file IO library.\r\n\r\nThanks,\r\nAnthony", "comments": ["Hi @eaplatanios, the posix implementation of MatchPath simply calls fnmatch (defined in <fnmatch.h>. See [posix/env.cc](https://github.com/tensorflow/tensorflow/blob/4c192f060cf9ff897911d240c140299d6db257b6/tensorflow/core/platform/posix/env.cc).", "Closing this issue. \r\n\r\nIf the above doesn't answer your question, please ask it on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9954, "title": "Fixing documentation for tf.abs to include missing details about complex types.", "body": "Adding lost documentation to tf.abs from the old tf.complex_abs when it learned how to work on complex data.", "comments": ["Can one of the admins verify this patch?", "I don't see a way to formally associate it, but this PR goes with https://github.com/tensorflow/tensorflow/issues/9827", "Thanks for fixing that!", "We should merge this into master as well -- it was only merged into r1.1."]}, {"number": 9953, "title": "Tensorboard parsing graph.pbtxt failed after uploaded by by web UI", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12.4\r\n- **TensorFlow installed from (source or binary)**: install binary with cpu version by pip based on Python 2.7.13\r\n- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: only use cpu version\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: just upload event file by web UI on 'GRAPH'\r\n\r\n### Describe the problem\r\nAfter uploaded the event file by web UI('GRAPH' table), it failed at the point of parsing graph.pbtxt with the error 'Cannot read property '' of undefined' as below. But it works fine for the cmd of 'tensorboard --logdir=xxx'.\r\n\r\nError info from chrome dev tool(console):\r\ntf-tensorboard.html:9827 Uncaught TypeError: Cannot read property '' of undefined\r\n    at addAttribute (tf-tensorboard.html:9827)\r\n    at tf-tensorboard.html:9859\r\n    at readHandler (tf-tensorboard.html:9729)\r\n    at FileReader.file.onload (tf-tensorboard.html:9744)", "comments": ["Hi @luchensk, does this happen consistently with any graph.pbtxt file that you upload, or did it just happen that one time? If the former, are you able to paste the graph.pbtxt file you were using in a [gist](http://gist.github.com) and send over the link?", "Thanks Ali for the quick response. The issue happens consistently and the graph.phtxt is as below as the attachment. \r\n[events.out.tfevents.1494834183.ali-186590d30ecd.local.zip](https://github.com/tensorflow/tensorflow/files/1009903/events.out.tfevents.1494834183.ali-186590d30ecd.local.zip)\r\n\r\nI also attached the graph from the zip file by the cmd of 'tensorboard --logdir=xxx', which you can refer to if needed.\r\n![graph-run](https://cloud.githubusercontent.com/assets/28526467/26188879/4d544976-3bd4-11e7-8be4-6d6f638a6c93.png)\r\n\r\nJust let me know if need any info, thanks again.", "Hi @ali01 and @aselle , is there some update? Thanks.", "Hi @luchensk, the file you attached does not seem to be a graph.pbtxt file, which might explain why the Web UI is having trouble parsing it. One way of getting a graph.pbtxt file from your graph is to do the following within your TensorFlow program:\r\n\r\n    graph_def = g.as_graph_def()\r\n    f.write(text_format.MessageToString(graph_def))\r\n\r\nLet me know if this works.", "As Ali correctly diagnosed, the upload button only accepts graph pbtxts, not event files. Closing this issue. ", "Yes, please close this issue, thanks."]}, {"number": 9952, "title": "Error loading saved_model.pb from C++", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes (minor changes), see below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n\r\n### Describe the problem\r\nI use the high level API to train an estimator (specifically, `tf.contrib.learn.DNNRegressor`) in Python. I then use `export_savedmodel` to save it to protobuf. When I try to load it from C++ I get:\r\n`Data loss: Can't parse saved_model.pb as binary proto`.\r\n\r\n### Source code / logs\r\n- Python file\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport sys\r\nimport pickle\r\nimport tensorflow as tf\r\nimport random, os, shutil\r\nfrom tensorflow.contrib.layers import create_feature_spec_for_parsing\r\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntrain_filename = sys.argv[1]\r\ntest_filename = sys.argv[2]\r\nsaved_model_directory = sys.argv[3]\r\n\r\nCOLUMNS = [\"X1\", \"Y1\", \"X2\", \"Y2\", \"SP\"]\r\nFEATURES = [\"X1\", \"Y1\", \"X2\", \"Y2\"]\r\nTARGET = \"SP\"\r\ntraining_set = pd.read_csv(train_filename, skipinitialspace=True, names=COLUMNS)\r\ntest_set = pd.read_csv(test_filename, skipinitialspace=True, names=COLUMNS)\r\n\r\ndef input_fn(data_set):\r\n    feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\r\n    targets = tf.constant(data_set[TARGET].values)\r\n    return feature_cols, targets\r\n\r\nfeature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\r\nfeature_spec = create_feature_spec_for_parsing(feature_cols)\r\nserving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\r\n\r\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\r\n    input_fn=lambda: input_fn(test_set),\r\n    eval_steps=1,\r\n    every_n_steps=100)\r\n\r\nestimator = tf.contrib.learn.DNNRegressor(\r\n    feature_columns=feature_cols,\r\n    hidden_units=[50, 25, 5],\r\n    model_dir=saved_model_directory,\r\n    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\r\n\r\nestimator.fit(input_fn=lambda: input_fn(training_set), steps=100, monitors=[validation_monitor])\r\n\r\nestimator.export_savedmodel(export_dir_base=saved_model_directory,\r\n    serving_input_fn = serving_input_fn)\r\n\r\n```\r\n\r\n- CPP file\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n\r\nusing namespace tensorflow;\r\nusing std::cout;\r\nusing std::vector;\r\nusing std::pair;\r\n\r\nint main(int argc, char* argv[]) {\r\n  // Initialize a tensorflow session\r\n  Session* session;\r\n  Status status = NewSession(SessionOptions(), &session);\r\n  if (!status.ok()) {\r\n    cout << status.ToString() << \"\\n\";\r\n    return 1;\r\n  }\r\n\r\n  // Read in the protobuf graph we exported\r\n  // (The path seems to be relative to the cwd. Keep this in mind\r\n  // when using `bazel run` since the cwd isn't where you call\r\n  // `bazel run` but from inside a temp folder.)\r\n  cout << \"READING MODEL.\\n\";\r\n  GraphDef graph_def;\r\n  status = ReadBinaryProto(Env::Default(), \"saved_model.pb\", &graph_def);\r\n  if (!status.ok()) {\r\n    cout << status.ToString() << \"\\n\";\r\n    return 1;\r\n  }\r\n  cout << \"DONE reading model.\\n\";\r\n}\r\n```", "comments": ["`estimator.export_savedmodel` exports the model to a directory, and the proto file there isn't a `GraphDef`, which is why you're getting the error.\r\n\r\nUnfortunately, this isn't documented well enough yet, but to load an exported saved model, use [`LoadSavedModel`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.h#L50) in C++\r\n\r\n@sukritiramesh @nfiedel @skye @josh11b : Shouldn't the saved model loader be included in the C++ API documentation at https://www.tensorflow.org/api_docs/cc/ ?", "@lirchi : Closing this out since this isn't a bug and this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\nFeel free to let us know if I misunderstood\r\n\r\nAnd we'll follow up on the documentation separately. Thanks!", "Yes, I can work on getting the saved model headers included in the C++ docs.\r\n\r\n@lirchi In the meantime https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#c may also help (that's the C++ section but whole README should be useful).", "This link was very helpful and I can successfully load models now - thanks a lot @skye!\r\n\r\nI've been doing a lot of digging, and it seems like the correct way to define input tensor for a high-level estimator is to construct a tensorflow::Example and then serialize it as string. Are there more efficient ways to do it? (That is, assuming the inputs are a bunch of floats, then building an object, defining feature map, serializing to string and then parsing it seems wasteful when this is done millions+ times. For example in real-time robotics domain.)", "@lirchi @skye \r\nWere you able to load the graph and run the session in c++? \r\nOnce you serialize a tensorflow::Example as a string, how do you pass it to session->Run(..)\r\nI can't find any documentation on this, or any answered stack overflow posts.\r\n\r\nI've posted on stack overflow as well:\r\nhttps://stackoverflow.com/questions/44790456/how-to-feed-inputs-into-a-loaded-tensorflow-model-using-c\r\nhttps://stackoverflow.com/questions/44848733/how-to-convert-a-serialized-tensorflowexample-into-a-single-tensor", "Hey, sorry for the delay. I suggest asking on StackOverflow or discuss@tensorflow.org. Github issues are for bugs and feature requests, and also I don't know the answer to your question :) (In general not many people look at individual github issues)", "It may be helpful\uff0c In my code, I find that tensorflow/tensorflow/cc/saved_model/loader.cc, ReadBinaryProto load saved_model.pb \u3002\r\nThe model is loaded in the SavedModel  way, but My model is format MetaGraphDef .\r\nyou can see [SavedModel ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/saved_model.proto#L20)\r\n", "if I pass GraphDef as an argument, is there a default argument value I can use? I tried \"=0,\" \"=nullptr,\" and \"=NULL,\" but non work, and I get errors saying GraphDef cannot be set to any of those values", "> \r\n@lirchi How did you solved that error in loading model?\r\n"]}]