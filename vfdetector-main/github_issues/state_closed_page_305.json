[{"number": 45115, "title": "[INTEL MKL] MKL DNN 0.x code clean -- common_runtime and ops modules", "body": "DNN 0.x cleanup of common_runtime and ops modules\r\n\r\n(1) Remove all DNN 0.x related code;\r\n\r\n(2) low-hanging-fruit - remove some non-production code (commented-out statements or unnecessary comments)", "comments": []}, {"number": 45113, "title": "Documentation for using a private CocoaPod spec for TensorFlow Lite is not correct", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_ios#using_local_tensorflow_lite_core\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe documentation linked above says to change the following line in the TensorFlowLiteC.podspec file:\r\n\r\n```\r\n  s.source       = { :http => \"file://<path_to_TensorFlowLiteC_framework.zip>\" }\r\n```\r\n\r\nIt then asks you to follow the instructions on [creating a private CocoaPod repo](https://guides.cocoapods.org/making/private-cocoapods.html) to use this pod in your project.\r\n\r\nThat guide asks you to run the following:\r\n\r\n```\r\npod repo push tfliteswift TensorFlowLiteC.podspec\r\n```\r\n\r\nBut when you do this, you get an error message about the podspec line:\r\n\r\n```\r\n[!] Error installing TensorFlowLiteC\r\n -> TensorFlowLiteC (2.3.0)\r\n    - ERROR | [iOS] unknown: Encountered an unknown error ([!] /usr/local/anaconda3/bin/curl -f -L -o /var/folders/5j/wwtlv8lx0m5fmhc3hy5w5_000000gp/T/d20201123-17126-1ko7ol4/file.zip file://<redacted>/tensorflow/bazel-bin/tensorflow/lite/experimental/ios/TensorFlowLiteC_framework.zip --create-dirs --netrc-optional --retry 2 -A 'CocoaPods/1.10.0 cocoapods-downloader/1.4.0'\r\n\r\ncurl: (3) URL using bad/illegal format or missing URL\r\n```\r\nI don't believe the `:http => \"file://<path_to_TensorFlowLiteC_framework.zip>\"` line makes sense, because you're specifying a local file path for the `http` key. It's clearly not an http URL. Therefore I believe this documentation is incorrect / incomplete.\r\n\r\nI have yet to get my app compiling with a custom build of TensorFlowLite, so I don't know the solution. Any help appreciated.", "comments": ["Everyone here says this works: https://github.com/CocoaPods/cocoapods-packager/issues/216#issuecomment-513514364\r\n\r\nYou just need a triple `/` like `file:///path`. `file://` is the protocol,  `/path` is the path\r\n\r\n```\r\n$ curl file://path\r\n\r\ncurl: (3) URL using bad/illegal format or missing URL\r\n```\r\n\r\n", "I'm sending a little fix to make this clearer."]}, {"number": 45112, "title": "Tensorflow 2.x can't feed data with non-uniform shapes to a multi-input multi-output model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere are currently no documentations on feeding data with non-uniform shape to a multi-input multi-output model with a tf.data.dataset.generator. When I tried to do so, I will receive such an error: `Data is expected to be in format x, (x,), (x, y), or (x, y, sample_weight)`.\r\n\r\nBecause each piece of data has the shape of (None,5), it is not possible to convert it to numpy arrays, which can be easily feed into the model. Moreover, tf.ragged_tensor can't be fed into the model, so I couldn't figure out a way to feed my data. I understand I can probably solve this with paddings, but that's going to be my last choice. Is there a way to feed data with non-uniform shape to a multi-input multi-output model regardless of what method being used?\r\n\r\nThanks to all in advance!\r\n\r\nHere's the code to reproduce my issues:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ndummy_1 = [[[1.1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]],\r\n           [[1.2,2,3,4,5],[2,3,4,5,6.8]],\r\n           [[1.3,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[4,5,6,7,8.9]]]\r\n\r\ndummy_2 = [[[1.1,2,3,4,5],[2,3,4,5,6]],\r\n           [[1.1,2,3,4,5],[2,3,4,5,6]],[3,4,5,6,7],\r\n           [[1.3,2,3,4,5],[2,3,4,5,6]]]\r\n\r\ndummy_3 = [[[1.5,2,3,4,5],[2,3,4,5,6]],\r\n           [[1.6,2,3,4,5],[2,3,4,5,6]],[3,4,5,6,7],\r\n           [[1.7,2,3,4,5],[2,3,4,5,6]]]\r\n\r\ndef gen():\r\n    for i in range(len(dummy_1)):\r\n        yield(dummy_1[i],dummy_2[i],dummy_2[i],dummy_3[i])\r\n\r\ndef custom_loss(y_true, y_pred):\r\n    return tf.reduce_mean(tf.abs(y_pred - y_true))\r\n\r\nclass network():\r\n    def __init__(self):\r\n        input_1 = keras.Input(shape=(None,5))\r\n        input_2 = keras.Input(shape=(None,5))\r\n        output_1 = layers.Conv1DTranspose(16, 3, padding='same', activation='relu')(input_1)\r\n        output_2 = layers.Conv1DTranspose(16, 3, padding='same', activation='relu')(input_2)\r\n\r\n        self.model = keras.Model(inputs=[input_1, input_2],\r\n                                 outputs=[output_1, output_2])\r\n        \r\n        # compile model\r\n        self.model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001),\r\n                           loss={\"mel_loss\":custom_loss, \"mag_loss\":custom_loss})\r\n    \r\n    def train(self):\r\n        self.dataset = tf.data.Dataset.from_generator(gen, \r\n                                                      (tf.float32, tf.float32, tf.float32, tf.float32))\r\n        self.dataset.batch(32).repeat()\r\n        self.model.fit(self.dataset,epochs=3)\r\n        #self.model.fit([dummy_1, dummy_2],\r\n        #               [dummy_2, dummy_3],\r\n        #               epochs=3)\r\n\r\nnet = network()\r\nnet.train()\r\n```\r\n\r\n**Will this change the current api? How?**\r\nYes, if there are currently no support for feeding data with non-uniform shape to a multi-input multi-output model.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who are working with time-series data that has an inconsistent dimension that represent the timesteps.\r\n\r\n**Any Other info.**\r\n", "comments": ["@CXYCarson,\r\nCan you try building the `Dataset` using `tf.data.Dataset.from_tensor_slices` as shown in [Building Datasets with ragged tensors](https://www.tensorflow.org/guide/ragged_tensor#building_datasets_with_ragged_tensors) and see if it works. Also, please refer [Documentation of Ragged Tensors](https://www.tensorflow.org/guide/ragged_tensor) for more info and let us know if the functionality is still not available. Thanks!", "I tried your method and I can create a `Dataset` with ragged tensors. But the model could not be fed with the ragged tensors. The error message is: `TypeError: object of type 'RaggedTensor' has no len()`\r\nThe code to reproduce the error is here:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nclass network():\r\n    def __init__(self):\r\n        input_1 = keras.Input(shape=(None,5))\r\n        input_2 = keras.Input(shape=(None,5))\r\n        output_1 = layers.Conv1DTranspose(16, 3, padding='same', activation='relu')(input_1)\r\n        output_2 = layers.Conv1DTranspose(16, 3, padding='same', activation='relu')(input_2)\r\n\r\n        self.model = keras.Model(inputs=[input_1, input_2],\r\n                                 outputs=[output_1, output_2])\r\n    \r\n    def train(self):\r\n        dummy_1 = tf.ragged.constant([[[1.1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]],\r\n                   [[1.2,2,3,4,5],[2,3,4,5,6.8]],\r\n                   [[1.3,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[4,5,6,7,8.9]]])\r\n\r\n        dummy_2 = tf.ragged.constant([[[1.1,2,3,4,5],[2,3,4,5,6]],\r\n                   [[1.1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]],\r\n                   [[1.3,2,3,4,5],[2,3,4,5,6]]])\r\n\r\n        dummy_3 = tf.ragged.constant([[[1.5,2,3,4,5],[2,3,4,5,6]],\r\n                   [[1.6,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]],\r\n                   [[1.7,2,3,4,5],[2,3,4,5,6]]])\r\n        self.dataset = tf.data.Dataset.from_tensor_slices((dummy_1, dummy_2, dummy_3))\r\n        for dummy_1, dummy_2, dummy_3 in self.dataset.batch(2):\r\n            x, y = self.model((dummy_1, dummy_2), training=True)\r\n\r\nnet = network()\r\nnet.train()\r\n```", "I took a look at the code for `Conv1DTranspose`, and it doesn't currently support RaggedTensors -- i.e., the current implementation assumes that the inputs are dense.  So I guess the feature request here is to have Conv1DTranspose (and possibly some other keras layers) be updated to support ragged tensors.\r\n\r\nAlso, in the second code example you posted, you need to specify `ragged=True` when creating the `keras.Input`s.  I.e.:\r\n`input_1 = keras.Input(shape=(None,5), ragged=True)`.  But that just gives you an error further along (which indicates that Conv1DTranspose doesn't support RaggedTensors).\r\n", "> I took a look at the code for `Conv1DTranspose`, and it doesn't currently support RaggedTensors -- i.e., the current implementation assumes that the inputs are dense. So I guess the feature request here is to have Conv1DTranspose (and possibly some other keras layers) be updated to support ragged tensors.\r\n> \r\n> Also, in the second code example you posted, you need to specify `ragged=True` when creating the `keras.Input`s. I.e.:\r\n> `input_1 = keras.Input(shape=(None,5), ragged=True)`. But that just gives you an error further along (which indicates that Conv1DTranspose doesn't support RaggedTensors).\r\n\r\nThank you for pointing out the error in my code!\r\nI'm not sure if TensorFlow has any other ways of handling time-series data with non-uniform shape, but if `RaggedTensor` is the only way, then you are correct that my feature request would be to have more layers be updated to support `RaggedTensor`\r\n\r\nAlso, as a way around, if I replace my `Conv1DTranspose` with other layers that support `RaggedTensor`, then is it fine to apply `Conv1DTranspose` afterwards? Or is it just not possible to use `Conv1DTranspose` anywhere in my model as long as the shape of my data is still non-uniform?\r\n\r\nLastly, do you happen to know what are the layer APIs that support `RaggedTensor` as input?", "> I replace my Conv1DTranspose with other layers that support RaggedTensor, then is it fine to apply Conv1DTranspose afterwards\r\n\r\nAs long as the input to Conv1DTranspose is a dense tensor, Conv1DTranspose should work fine.  I.e., if you have an earlier layer that has a RaggedTensor input and a DenseTensor output, then you could use Conv1DTranspose.\r\n\r\nI'm not an expert in Keras (I do most of my work at lower levels of the TensorFlow stack), but another thing that you might want to look into is Keras [masking](https://keras.io/guides/understanding_masking_and_padding/).  Though from a quick look at that guide plus the code for Conv1DTranspose, I'm *guessing* that this layer it won't support masking?  (In particular, I don't see any `compute_mask` method being defined.)\r\n\r\n> Lastly, do you happen to know what are the layer APIs that support RaggedTensor as input?\r\n\r\nI don't, but if anyone else does, please chime in.", "When you call `model.fit(list)`, you should make sure that each element in `list` is a NumPy array (or eager tensor). As explained by other folks in this thread, only arrays with uniform shapes are supported at this time. The way to encode tensors with non-uniform shapes in TF is RaggedTensor, but we don't have very good support for RaggedTensor at this time, so you won't be able to use that. Support will expand over time.\r\n\r\nClosing as won't fix."]}, {"number": 45111, "title": "[r2.4 cherry-pick] Include C logging API into part of the libtensorflow_framework.so", "body": "**Note:** This PR is to **r2.4** branch and is a cherry-pick of e8c4a4d  (PR #44993) \r\n\r\n/cc @mihaimaruseac I think this cherry-pick makes sense to r2.4 if there is a rc3. Though I understand TF 2.4.0 release might be imminent. Feel free to close if there is no rc3.\r\n\r\nThis PR includes C logging API into part of the libtensorflow_framework.so\r\nas otherwise the file system plugins will not be able to link to TF_Log\r\nto send logs to tensorflow logging system.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 45108, "title": "[Intel MKL] Enabling Quantized Conv ops in native format mode", "body": "", "comments": ["Thank you for the valuable comments. I have addressed them.", "I have addressed the remaining comments. Please have a look. Thank you!"]}, {"number": 45107, "title": "[Intel MKL] Changes in common files to enable MKL Quantized ops with native format", "body": "This PR adds the common changes to enable MKL Quantized ops in native format mode. It removes the dependency on the duplicate number of inputs/outputs for quantized ops.\r\n\r\nAdded by @penpornk for API review:\r\n* The ops that are changed in API Golden files all have hidden visibility.\r\n* The attribute `data_format` is added because MKL ops support both TF's native NHWC format and its own blocked format.\r\n* The attributes `is_weight_const` and `is_filter_const` are added for caching purposes.\r\n* The changes should be backward compatible.", "comments": ["Thank you for reviewing the PR. I have made the requested changed.\r\nFor adding an attribute to the ops, I thought these ops are not visible in the API as we mark them as hidden, for e.g. [here](https://github.com/tensorflow/tensorflow/blob/5892c5bbe5edc62b1f30c6cf005a5f2e37a293b4/tensorflow/core/api_def/base_api/api_def_QuantizedDepthwiseConv2DWithBias.pbtxt#L3). These ops usually get used by frozen graphs generated by our offline tool.", "@mahmoud-abuzaina Can you please check @penpornk's comments and keep us posted ? Thanks!", "@penpornk no worries. This PR and other related pending PRs don't add any new implementation to use the newly added attributes. Rather, the implementation that uses these attributes has been there for long time. \r\nHere is the background: all Quantized* ops are hidden API ops and get generated/added to frozen pb file via offline tool. They were considered dummy ops. At runtime those ops will be rewritten to corresponding _MklQuantized*. The purpose of _MklQuantized* is to handle the extra number of inputs and outputs that we used to require. With native format support, we are getting rid of all extra inputs and outputs. So I thought we can reuse Quantized* ops (that are usable only via oneDNN integration code) instead of creating new ops. While doing that I noticed that these ops are missing few required attributes. These attributes have been used for a while with existing implementation, but the missing attributes were not causing a failure before because Quantized* ops were dummy ops and will be always rewritten to _Mkl ones which have the those attributes. Now since we are using Quantized* named ops, we are adding the missing attributes but they will use existing implementation. As an example of existing attribute usage see [here](https://github.com/tensorflow/tensorflow/blob/697f117f36176fc5d07c0b37d1b17b9d7e3a3bc9/tensorflow/core/kernels/mkl/mkl_conv_ops.cc#L697). Sorry for my verbosity and please let me if I am missing something.", "Closing for now. I will do some changes and reopen once ready.", "@mahmoud-abuzaina So sorry for my delayed reply and thank you very much for the thorough explanation! I didn't remember that these attributes are already used in the already checked-in kernels. \r\n\r\nRe: Hidden ops: The API review committee reminded me that op names not prefixed by `_` are still callable. Marking them hidden just puts them in [tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops), so they are still part of the API golden.  \r\n\r\nEdited to add: We synced offline about closing the PR for now. Looking forward to the changes!"]}, {"number": 45106, "title": "Error during building examples for ARC target in TFLM.", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): master\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARC\r\n\r\nDuring building generated for ARC target project, user gets the following error:\r\n```\r\nIn file included from tensorflow/lite/micro/kernels/arc_mli/fully_connected.cc:27:\r\n.\\tensorflow/lite/micro/kernels/arc_mli/mli_tf_utils.h:99:16: error: static_cast from 'const int *' to 'void *' is not allowed\r\n  mliT->data = static_cast<void*>(tflite::micro::GetTensorData<datatype>(tfT));\r\n               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\nNeed to fix mli_tf_utils.h file.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45106\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45106\">No</a>\n"]}, {"number": 45105, "title": "ValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata).", "body": "from tensorflow.keras import backend as K\r\nget_activations = K.function([model.layers[0].input,model.layers[1].input,model.layers[2].input,model.layers[3].input, K.learning_phase()],[model.layers[layer_idx].output,])\r\n\r\nTF 2.3.1\r\n\r\nWindows 10 in anaconda python 3.8 environment\r\n\r\nGot error while executing(four stream architecture)\r\nValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata).", "comments": ["@MTeja1991 could you provide us more context so we can reproduce the error? I am curious about about this `model` object.", "@MTeja1991 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45105\">No</a>\n"]}, {"number": 45104, "title": "Missing dependency on PowerPCCodeGen leads to undefined references in tf_to_kernel", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.4, PPC\r\n\r\n**Describe the problem**\r\n\r\nRunning the tests with `bazel test ...` leads to undefined reference errors, see below log extract.\r\nThe reason is that `InitializeNativeTarget` is called which seemingly is a generated function and while the x86 dependency lib is linked via `@llvm-project//llvm:X86CodeGen`, the PPC equivalent is not.\r\n\r\nThe same problem may also affect ARM and arch64. Compare https://github.com/tensorflow/tensorflow/blob/15f4bda049539dd41c6dd9d0737d33da86cc32cf/tensorflow/compiler/aot/BUILD#L73-L83\r\nwith https://github.com/tensorflow/tensorflow/blob/15f4bda049539dd41c6dd9d0737d33da86cc32cf/tensorflow/compiler/mlir/tools/kernel_gen/BUILD#L122-L123\r\n\r\nI think those lists should be pretty much identical.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel test //tensorflow/...`\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nSUBCOMMAND: # //tensorflow/core/kernels/mlir_generated:invert_i32_kernel_cubin [action 'compile tensorflow/core/kernels/mlir_generated/invert_i32_kernel_cubin.sm_37.bin', configuration: 18b45a5a4afc1ae33c4d05\r\n7c86644439e6ecd019473065d664167c1910aff23b, execution platform: @local_execution_config_platform//:platform]\r\n...\r\nbazel-out/ppc-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/_objs/tf_to_kernel/tf_to_kernel.o:tf_to_kernel.cc:function main: error: undefined reference to 'LLVMInitializePowerPCTargetInfo'\r\nbazel-out/ppc-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/_objs/tf_to_kernel/tf_to_kernel.o:tf_to_kernel.cc:function main: error: undefined reference to 'LLVMInitializePowerPCTarget'\r\nbazel-out/ppc-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/_objs/tf_to_kernel/tf_to_kernel.o:tf_to_kernel.cc:function main: error: undefined reference to 'LLVMInitializePowerPCTargetMC'\r\nbazel-out/ppc-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/_objs/tf_to_kernel/tf_to_kernel.o:tf_to_kernel.cc:function main: error: undefined reference to 'LLVMInitializePowerPCAsmPrinter'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n", "comments": ["I think so too. Can you send a PR please? Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45104\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45104\">No</a>\n"]}, {"number": 45103, "title": "TFLiteConverter.from_saved_model runs on an error", "body": "**System information**\r\n- using colab\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nTrained model:\r\nhttps://colab.research.google.com/drive/1PRuBCtkp8WIQn66IbGjgvfr-kLMpA3P5?usp=sharing (just in case you want to see, that the model actually works)\r\n\r\nColab with the tfLite-Converter:\r\nhttps://colab.research.google.com/drive/1up5goKZLY6rfTTt98mg7KRqZoUAeEaIO?usp=sharing\r\n\r\n```\r\nmodel_dir=\"./saved_model\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\r\ntflite_model = converter.convert()\r\nopen('detect.tflite','wb').write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://github.com/SerQuicky/shoe-dataset/tree/master/models/stock_shoe_model\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nHello guys, I actually trained a Mobilenet SSD v1 model and it works fine in Colab (test it through the first Colab link).\r\nNow I am trying to convert the model to a tflite model so I can use it on my android phone.\r\n\r\nI am using tensorflow version 1.15.0. The [documentation](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter) has an example for the conversion from a saved model to tflite, sadly it does not work.   \r\n\r\nComplete stacktrace\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-40156df8e12f> in <module>()\r\n      8 #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n      9 \r\n---> 10 tflite_model = converter.convert()\r\n     11 open('detect.tflite','wb').write(tflite_model)\r\n\r\n/tensorflow-1.15.2/python3.6/tensorflow_core/lite/python/lite.py in convert(self)\r\n    894               \"None is only supported in the 1st dimension. Tensor '{0}' has \"\r\n    895               \"invalid shape '{1}'.\".format(\r\n--> 896                   _get_tensor_name(tensor), shape_list))\r\n    897         elif shape_list and shape_list[0] is None:\r\n    898           self._set_batch_size(batch_size=1)\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\n", "comments": ["@SerQuicky,\r\nI do not have access to the links you have provided. Could you please provide the required permissions to view the files. \r\n\r\nAlso, TensorFlow 1.x is not actively supported. Please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "@amahendrakar sorry for the mistake, I have updated the links. Can I convert a model that I trained with tf v1.x  with tf v2.3+ ? Or should I train it again, but with tf v2.3?", "I will close the issue, because I tried the process under tf 1.15.0. I am currently trying to run it under tf 2.3. Thanks for the information so far.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45103\">No</a>\n"]}, {"number": 45102, "title": "Added useful C++ symbols missing from the tensorflow DLL on windows", "body": "On Windows, there is tool for selecting which symbols get exported to the tensorflow DLL.\r\nHowever, this filter does not include the symbols needed to use SessionOptions and SavedModel. It is also missing symbols for accessing shapes of a given node in the graph and creating a default instance of GraphDef.\r\n\r\nThis fix adds these symbols manually, thus not an elegant fix, and there are most likely several other symbols that should be included, but these are symbols which I find essential to: load a SavedModel, determine shapes and run inference with specific SessionOptions in C++.", "comments": ["This should be opened against master branch. We only backport changes to releases if they are related to security."]}, {"number": 45101, "title": "TFRecords writing and reading", "body": "Hi,\r\ni have a very huge data and which I want to convert to tfrecords before sending it to neural network. My writing code is \r\n`for n, pat in tqdm(enumerate(pats)):\r\n    cache_file = cache_dir+f'{pat}.tfrecords'\r\n    writer = tf.io.TFRecordWriter(cache_file)\r\n    files = os.listdir(pat_dir+pat+'/')\r\n    files.sort()\r\n    X_train = []\r\n    for i, file in enumerate(files):\r\n        byte_image = tf.io.read_file(pat_dir+pat+'/'+file)\r\n        image = tfio.image.decode_dicom_image(byte_image, dtype=tf.uint16)\r\n        image = image.numpy()\r\n        image = image/(image.max())\r\n        X_train.append(image)\r\n    X_train = np.array(X_train).reshape((180, 256, 256, 1))\r\n    Y_train = imread(lab_dir+lab_files[n], as_gray=True)\r\n    Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=3)\r\n    # plt.imshow(Y_train)\r\n    # plt.show()\r\n    feature = { 'train/label'  : _bytes_feature(tf.compat.as_bytes(Y_train.tostring())),\r\n                 'train/images' : _bytes_feature(tf.compat.as_bytes(X_train.tostring()))\r\n               }\r\n    example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    writer.write(example.SerializeToString())\r\n    writer.close()\r\n    del writer\r\n    gc.collect()`\r\nwhich i think is working perfectly fine but when i follow the example given on keras page to read tfrecords [https://keras.io/examples/keras_recipes/tfrecord/](url) it reads all the files fine but for last two files it gives a dimension error. My reading code is \r\n`def read_tfrecord(example):\r\n    tfrecord_format ={'train/label'  : tf.io.FixedLenFeature([], tf.string),\r\n            'train/images' : tf.io.FixedLenFeature([], tf.string)}\r\n    example = tf.io.parse_single_example(example, tfrecord_format)\r\n    tr_images = tf.io.decode_raw(example['train/images'], tf.float32)\r\n    tr_label = tf.io.decode_raw(example['train/label'], tf.float32)\r\n    tr_images = tf.reshape(tr_images, [180, 256, 256, 1])\r\n    tr_label = tf.reshape(tr_label, [1, 256, 256, 3])\r\n    return tr_images, tr_label`\r\nthe problem is that it reads all the fines just fine but last two files are merged together and when gives this error\r\n`Cannot reshape a tensor with 2949120 elements to shape [180,256,256,1] (11796480 elements) for '{{node Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](DecodePaddedRaw, Reshape/shape)' with input shapes: [2949120], [4] and with input tensors computed as partial shapes: input[1] = [180,256,256,1].`\r\nplease help!\r\nsorry for my English it's not good but i hope that someone gets my point and will reply me soon. and also this is my 1st time with any issue so forgive me if i have not follow the pattern.", "comments": ["@NaveedMazhar-eng could you please provide your system information? Also, it would be easier for us to help if you show your code snippets nicely. Thank you!", "@NaveedMazhar-eng \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45097, "title": "installation of any version on python 3.9 and installation of v1.x on python 3.8", "body": "**System information**\r\n- OS Platform and Distribution: Fedora 32 and Fedora 33\r\n- TensorFlow installed from (source or binary): pip/wheel\r\n- TensorFlow version: any\r\n- Python version: python 3.9 on Fedora 33 and python 3.8 on Fedora 32\r\n\r\n**Describe the problem**\r\n\r\nNo version at all available for python 3.9\r\n\r\n```\r\n$ python3 --version\r\nPython 3.9.0\r\n$ pip3 install --user tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\nno 1.x available on python 3.8\r\n\r\n```\r\n$ python3 --version\r\nPython 3.8.3\r\n$ pip3 install --user tensorflow==1.15\r\nWARNING: Running pip install with root privileges is generally not a good idea. Try `pip3 install --user` instead.\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2)\r\nERROR: No matching distribution found for tensorflow==1.15\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nabove\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nabove\r\n", "comments": ["@muayyad-alsadi,\r\n> No version at all available for python 3.9\r\n\r\nThe request for TensorFlow support on Python 3.9 is already being tracked in [#44485](https://github.com/tensorflow/tensorflow/issues/44485).\r\n\r\n> no 1.x available on python 3.8\r\n\r\nAnd yes, TensorFlow 1.x is not supported on Python 3.8. The lastest Python version to support TensorFlow 1.x is Python v3.7. \r\n\r\n\r\nVersion | Python version | Compiler | Build tools\r\n-- | -- | -- | --\r\ntensorflow-1.15.0 | 2.7, 3.3-3.7 | GCC 7.3.1 | Bazel 0.26.1\r\ntensorflow-1.14.0 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.24.1\r\n\r\nFor more information, please take a look at the [tested build configuration](https://www.tensorflow.org/install/source#tested_build_configurations). Thanks!\r\n\r\n\r\n\r\n\r\n\r\n", "As mentioned in https://github.com/tensorflow/tensorflow/issues/44485#issuecomment-720675069, we cannot release an older version of TF with support for newer Python. The build system is too complex and any addition to the matrix is prone to create hard to debug bugs.\r\n\r\nPy3.9 is tracked in #44485", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45097\">No</a>\n"]}, {"number": 45096, "title": "How to save a keras.Model with an embedded hub.KerasLayer ?", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: [Google Colab](https://colab.research.google.com/drive/18VmYP7Q5GmdToZCX3mx5Msmby0WfEMRE?usp=sharing)\r\n- TensorFlow installed from (source or binary): [Google Colab](https://colab.research.google.com/drive/18VmYP7Q5GmdToZCX3mx5Msmby0WfEMRE?usp=sharing)\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nMethod 1\r\n```python\r\nmodel = tf.keras.Sequential([\r\n    hub.KerasLayer(\"https://tfhub.dev/google/bit/s-r50x1/1\"),\r\n])\r\n\r\nmodel.build(input_shape=(None, 224, 224, 3))\r\nmodel.save(\"/tmp\")\r\n# Success\r\n```\r\n\r\nMethod 2\r\n```python\r\nclass DummyModel(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(DummyModel, self).__init__()\r\n    self.model = hub.KerasLayer(\"https://tfhub.dev/google/bit/s-r50x1/1\")\r\n\r\n  def call(self, inputs):\r\n    return self.model(inputs)\r\n\r\n\r\nmodel = DummyModel()\r\nmodel.build((None, 224, 224, 3))\r\nmodel.save(\"/tmp\")\r\n# Error\r\n```\r\n\r\n**Describe the expected behavior**\r\nHow can I properly build a Model with an embedded KerasLayer? \r\n\r\nMethod 1 succeeds, but method 2 fails with `ValueError: Model cannot be saved because the input shapes have not been set.`\r\n\r\nThis is a minimized version. I explicitly need a design like method #2 for my more complicated problem.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Google Colab](https://colab.research.google.com/drive/18VmYP7Q5GmdToZCX3mx5Msmby0WfEMRE?usp=sharing)\r\n", "comments": ["@jvishnuvardhan \r\nI am able to replicate the issue on nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b90bf04f9528bb76bf78946a3982d8df/untitled468.ipynb).", "Update from my side:\r\n\r\n`model._set_inputs(tf.keras.Input(shape=[None, None, 3]))` solves the issue for me.\r\n\r\nThis fix feels weird though. Python recommends using _method only for private purposes, and this issue feels like a common situation. So I wonder if there exists a proper solution?", "Hi--\r\n\r\nThe message that you get \"ValueError:DummyModel cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling `.fit()` or `.predict()`. To manually set the shapes, call `model.build(input_shape)`.\" is confusing since `build` does not work.\r\n\r\nIf you follow the recommendation to `fit` or `predict`, then you can save successfully. Here I have \"called\" it before saving, which works:\r\n```\r\nmodel = DummyModel()\r\nz = tf.zeros([1, 224, 224, 3])\r\nmodel(z)\r\nmodel.save(\"/tmp\")\r\n```", "@RobRomijnders Agree with you @monicadsong. \r\nWe need to call the model on some data to determine the shapes of different layers which is required for initializing model weights and biases before saving the model. This approach (calling the model) is specific to subclass model. Please check the example given in one of [the tutorial](https://www.tensorflow.org/guide/keras/save_and_serialize) on TF website.\r\n\r\n```\r\nclass CustomModel(keras.Model):\r\n    def __init__(self, hidden_units):\r\n        super(CustomModel, self).__init__()\r\n        self.dense_layers = [keras.layers.Dense(u) for u in hidden_units]\r\n\r\n    def call(self, inputs):\r\n        x = inputs\r\n        for layer in self.dense_layers:\r\n            x = layer(x)\r\n        return x\r\n\r\n\r\nmodel = CustomModel([16, 16, 10])\r\n# Build the model by calling it\r\ninput_arr = tf.random.uniform((1, 5))\r\noutputs = model(input_arr)\r\nmodel.save(\"my_model\") \r\n```\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if I am mistaken. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45096\">No</a>\n", "Thanks for answering!"]}, {"number": 45095, "title": "Failed to build wheel on MacOS Big Sur", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.0.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NONE\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest from tensorflow:master\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: NONE\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 12.0.0 (clang-1200.0.32.27)\r\n- CUDA/cuDNN version: NONE\r\n- GPU model and memory: NONE\r\n\r\nHello, \r\n\r\nafter successfuly compiled tensorflow by the Bazel, I try to make a wheel file. I used './bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg', but the result folder /tmp/tensorflow_pkg didn't exist. I send screenshot with output from building wheel.\r\n\r\n<img width=\"1074\" alt=\"Sni\u0301mka obrazovky 2020-11-23 o 10 04 34\" src=\"https://user-images.githubusercontent.com/74611856/99944598-71488880-2d73-11eb-9d04-550bce91f38c.png\">\r\n<img width=\"1074\" alt=\"Sni\u0301mka obrazovky 2020-11-23 o 10 04 49\" src=\"https://user-images.githubusercontent.com/74611856/99944607-74437900-2d73-11eb-81fe-48cc9d9f87dc.png\">\r\n\r\n", "comments": ["Did you try Apple released forked version of [TensorFlow optimized](https://github.com/apple/tensorflow_macos) for macOS Big Sur?", "No, I used your instructions to build it for MacOS: https://www.tensorflow.org/install/source", "Since issue title says building pip package failed and you are using instructions for building from source.\r\nWere able to use `pip install` to build TF on your system?.\r\nAlso you may want to give it a try to build from apple released TF version, optimized for macOS Big Sur at the moment.", "I have a trouble with building pip's wheel from source. And, I tried `pip3 install tensorflow` too. The result was: \r\n\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\n> ERROR: No matching distribution found for tensorflow\r\n", "#45120", "Also, please don't post screnshoots, post the actual error/code. Easier to search, easier to reference, easier for accessibility reasons too.", "I sent you only part of my screen that contains the error and nothing else. Okay, but when I didn't found a wheel, I intuitively a try to build my own wheel by instructions posted here: https://www.tensorflow.org/install/source .... Thanks.", "I am having the same issue, here is the text version of logs:\r\n```\r\nFri Nov 27 17:59:17 MSK 2020 : === Preparing sources in dir: /var/folders/j1/x3dxynqd187gm7w_lccw0h880000gn/T/tmp.XXXXXXXXXX.yybbrD8N\r\n~/tensorflow ~/tensorflow\r\n~/tensorflow\r\n~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/tensorflow\r\n~/tensorflow\r\n/var/folders/j1/x3dxynqd187gm7w_lccw0h880000gn/T/tmp.XXXXXXXXXX.yybbrD8N/tensorflow/include ~/tensorflow\r\n~/tensorflow\r\nsed: /var/folders/j1/x3dxynqd187gm7w_lccw0h880000gn/T/tmp.XXXXXXXXXX.yybbrD8N/tensorflow/__init__.py: in-place editing only works for regular files\r\n```\r\nThe file, mentioned in logs, is a symlink into `bazel-out`; forcing `cp` to follow symbolic links solves the problem for me.\r\n\r\nHere is a patch: [build_pip_package.txt](https://github.com/tensorflow/tensorflow/files/5608469/build_pip_package.txt).\r\n\r\n", "I have been trying Apple released forked version of [TensorFlow optimized](https://github.com/apple/tensorflow_macos) for macOS Big Sur with result:\r\n\r\n`Successfully installed appnope-0.1.0 backcall-0.2.0 decorator-4.4.2 ipython-7.19.0 ipython-genutils-0.2.0 jedi-0.17.2 parso-0.7.1 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.8 ptyprocess-0.6.0 pygments-2.7.2 traitlets-5.0.5 wcwidth-0.2.5`\r\n`Processing /var/folders/sl/sgly1pz10_517l3vv7x98l3w0000gn/T/tmp.kE3Qlunc/tensorflow_macos/x86_64/tensorflow_macos-0.1a0-cp38-cp38-macosx_11_0_x86_64.whl`\r\n`Installing collected packages: tensorflow-macos`\r\n`Successfully installed tensorflow-macos-0.1a0`\r\n", "We cannot support the private forks.", "#45404 will make this repo compile on the M1 chips. Until that lands, the support has to come from the Apple fork.", "I don't have MacBook with M1 chip, I'm using only MacBook Air Retina 2018 with Intel Core i5.", "But then you don't need to use the Apple fork. That one is for the M1/ARM chips.", "Finally managed to see the images (you should have really posted the error message as text, not as image).\r\n\r\nThe error is clear in the first screenshot:\r\n\r\n```\r\nin-place editing only works for regular files\r\n```\r\n\r\nThis results in a crash during the pip package creation as `sed` crashes. This is why you don't have the wheel.\r\n\r\nDo you have a symlink at that path?", "Sorry, I don't know I do nothing with `sed`. I only follow these steps:\r\n\r\n0. Install newer Xcode CLI (gcc, git, and so on)\r\n1. pip3 install -U --user pip numpy==1.18.5 wheel\r\n    pip3 install --user keras_applications --no-deps\r\n    pip3 install --user keras_preprocessing --no-deps\r\n2. Install Bazel\r\n3. Download https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0\r\n4. cd tensorflow\r\n5. ./configure\r\n6. bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package\r\n7. ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg (only in this step was the trouble ....)", "The last step does a `sed` during build. `sed` on MacOS does not support in-place editing on symbolic links. Can you try passing a different argument, instead of `/tmp/tensorflow_pkg`? For example `./tf_pkg`?", "The issue remains:\r\n\r\n\r\n```\r\ntensorflow-2.4.0 % ./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./tf_pkg                                                        \r\n\u0161t 31. december 2020 14:01:37 CET : === Preparing sources in dir: /var/folders/hn/5hk6gn6s6gv4n54vh80rrzkw0000gn/T/tmp.XXXXXXXXXX.4nbX7Iod\r\n~/Projects/tensorflow-2.4.0 ~/Projects/tensorflow-2.4.0\r\n~/Projects/tensorflow-2.4.0\r\n~/Projects/tensorflow-2.4.0/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/Projects/tensorflow-2.4.0\r\n~/Projects/tensorflow-2.4.0\r\n/var/folders/hn/5hk6gn6s6gv4n54vh80rrzkw0000gn/T/tmp.XXXXXXXXXX.4nbX7Iod/tensorflow/include ~/Projects/tensorflow-2.4.0\r\n~/Projects/tensorflow-2.4.0\r\nsed: /var/folders/hn/5hk6gn6s6gv4n54vh80rrzkw0000gn/T/tmp.XXXXXXXXXX.4nbX7Iod/tensorflow/__init__.py: in-place editing only works for regular files`\r\n```", "It's weird that it's still looking for a path in `.../T/tmp...`.", "Yes I think too, but the file `/var/folders/hn/5hk6gn6s6gv4n54vh80rrzkw0000gn/T/tmp.XXXXXXXXXX.4nbX7Iod/tensorflow/__init__.py` exists ...\r\n\r\n```\r\nmartin@MacBook-Air-uzivatela-martin ~ % ls -all  /var/folders/hn/5hk6gn6s6gv4n54vh80rrzkw0000gn/T/tmp.XXXXXXXXXX.4nbX7Iod/tensorflow/__init__.py\r\nlrwxr-xr-x  1 martin  staff  135 31 dec 14:01 /var/folders/hn/5hk6gn6s6gv4n54vh80rrzkw0000gn/T/tmp.XXXXXXXXXX.4nbX7Iod/tensorflow/__init__.py -> /private/var/tmp/_bazel_martin/303923b4557f94aa900fd66f39ae4258/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/__init__.py\r\n```", "Exists, but is a symbolic link. On macos, `sed` does not work on symbolic links.", "Oh, yes of course, but what with it can I do?\r\n\r\nI compile my own variant of TF with enabled instruction sets for my CPU.\r\n\r\n> 2021-01-03 22:05:28.862745: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n", "[These are the `sed` commands that get executed](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/pip_package/build_pip_package.sh;l=214-223;drc=71313bcb180f2220adee93d964bc4721ad6bf575).\r\n\r\nYou could remove them, it seems it is only for some autocomplete. Alternatively, you could change them to not do inplace replacement.\r\n\r\nThe person that wrote that code no longer works on TF, so it will take a while until we can get a proper fix.", "I have a proper solution .... Can I create PR?\r\n\r\nIn cp must be used '-L' option (always follow symbolic links in SOURCE).\r\n\r\n[build_pip_package.sh](https://github.com/tensorflow/tensorflow/files/5762630/build_pip_package.sh.txt)\r\n\r\n\r\n\r\n", "That's awsome. Yes, please make a PR. Please tag me in the PR.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45095\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45095\">No</a>\n"]}, {"number": 45094, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib.keras.python'", "body": "When I run the following code, I met the error as written above\r\n`from tensorflow.contrib.keras.python.keras.initializers import TruncatedNormal`  \r\nIs this bug caused by the tensorflow version?\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow -gpu1.14.0\r\n- Python version: 3.6", "comments": ["@chens72,\r\nCould you please share the output of the below code with us?\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n```\r\n\r\nAlso `tensorflow.contrib` exists only in TensorFlow 1.x, which is not actively supported anymore. We'd recommend you to update TensorFlow to the latest stable version v2.3. Thanks!", "Thank you. But after I  update TensorFlow to the  version v2.3, I met the error\uff1a\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n", "Hello @chens72, try to import it this way `from tensorflow.keras.initializers import TruncatedNormal`. As @amahendrakar  said, tf v2.+ remove `tensorflow.contrib` . Hope it can help!", "Yes\uff0cit works. Thank you very much!"]}, {"number": 45092, "title": "manylinux2010 compatible libtensorflow for TF 2.x", "body": "The official TF 2.x libtensorflow binaries for linux are built in a manylinux2010 sysroot only when using CUDA.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/tools/ci_build/builds/libtensorflow.sh#L54-L59\r\n\r\nIdeally CPU libtensorflow builds would be built in a manylinux2010 compatible way as well.\r\n\r\nHappy to put up a PR if this makes sense.\r\n", "comments": ["This is likely an oversight from our side. Happy to review the PR, thank you for the issue report.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45092\">No</a>\n"]}, {"number": 45090, "title": "tf keras upsample2d operator produces wrong results on tflite conversion", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source and binary\r\n- TensorFlow version (or github SHA if from source): 2.3.0, tf-nightly, 2.5.0(latest source)\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\nColab Link: https://colab.research.google.com/drive/1QZo8QZubmiiXIxc0O8SZEfEsfaxN5kxR?usp=sharing\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"mnv3_test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**Failure details**\r\nThe conversion is successful, but the generated model is wrong,\r\n\r\nThe tf keras model with **upsample2d layers(bilinear interpolation)** correctly produces a h5 file; but the converted tflite model has **extra layers** and there is **shape mismatch**.The **output shape becomes 1x1** instead of original upsampled size. The error occurs in tf 2.3.0, tf nightly and latesr source (dev 2.5.0). The conversion produces **additonal shape, mul and strided slice** layer(not supported by hardware accelerators).\r\n\r\nThe problem persists even if we use  **ResizeNearestNeighbo**r interpolation.\r\n\r\n**Minimal Example**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Conv2D, UpSampling2D\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ndef get_model():\r\n    \r\n    ip = Input(shape=(256,256,3))\r\n    x= Conv2D(kernel_size=3, filters=32, padding ='same')(ip)\r\n    x = UpSampling2D( size=(2, 2), interpolation='bilinear')(x)\r\n    x= Conv2D(kernel_size=3, filters=32, padding ='same')(x)\r\n    \r\n    \r\n    model = Model(inputs=ip, outputs=x)\r\n    \r\n    model.compile(optimizer=Adam(lr=1e-4),\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy']) # Ensure you have sparse labels\r\n    return model\r\n\r\nmodel=get_model()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"bilinear.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**Models**\r\n[mnv3_resize_bilinear_test.zip](https://github.com/tensorflow/tensorflow/files/5581389/mnv3_resize_bilinear_test.zip)", "comments": ["@ymodak \r\nI am able to replicate the issue on nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a9660e19ea17e5e216bcdd6ff5147745/untitled468.ipynb).Thanks!", "I met the same problem, additonal shape, mul and strided slice layer(not supported by hardware accelerators).", "@Saduf2019 when I set my batch dim with 1 these layers was disappear;", "@sunzhe09 How  did you set batch dim to 1 ? The tfllite already has batch size 1 right?\r\nAlso, is the upsampled size correct this time? ", "@Saduf2019 If we set the batch dimesion to input layer during model defintion of tf.keras model, all the problems seems to be resolved. However can we set the batch dimension of all layers for the **already trained model** to 1, before tflite conversion?", "The minimal example generated tflite file viewed in netron shows additonal shape, mul and strided slice layer.\r\n<img width=\"381\" alt=\"Screen Shot 2020-11-23 at 11 43 26 AM\" src=\"https://user-images.githubusercontent.com/42785357/100007914-9e039c80-2d81-11eb-9460-524fe19835c3.png\">\r\n", "This is similar to issue #43882. See my explanation on why this is correct and an example on how to set your model shape to be static before conversion\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/43882#issuecomment-731636562", "For anyone wondering. For single input keras model the fix above can be implemented like this.\r\n```\r\nmodel.input.set_shape(1 + model.input.shape[1:])\r\n```"]}, {"number": 45089, "title": "Tensorflow 2. x not recognizing RTX 3090 GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 2.2 / 2.3/ 2.5.0.dev20201028\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: Pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1/8.0.4\r\n- GPU model and memory: RTX 3090\r\n\r\n\r\nI have installed T.F. 2.2 using conda install -C anaconda TensorFlow-gpu, it is installed properly but it is not able to recognize the GPU, I have checked using:-\r\n\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n\r\nIt gives output CPU only.\r\n\r\nI have also checked by installing T.F. 2.3 and 2.5 nightly build using pip install but all of them are failed to recognize GPU. Does anyone know the cause and solution?\r\n", "comments": ["try cuda10.1 and cudnn7.6.5 or compiling from source", "@houcze please answer something of which you are sure or else give idea having logic, don't spam. 3090 has Ampere architecture, CUDA 10.1 is incompatible. I have seen on some GitHub issues that there is a problem with compiling from source as well.", "@Thunder003 \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#gpu).\r\n\r\nCurrent tf-nightly version supports cuda 11.0\r\nIf you are okay with unstable version (tf-nightly) you can give it a try or wait for upcoming stable TF 2.4 release.Thanks!", "@ravikyram I have tried CUDA 11 on RTX 3090 but it throws sm_86 error with CUDA 11. I think only 11.1 works on it. \r\nAnd I tried 11.1 + TF 2.5.0.dev20201028, it hasn't detected the GPU", "Have you added the correct CUDA And CuDnn path in the console. I recommend you check that. \r\n`\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\extras\\CUPTI\\lib64;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\include;%PATH%\r\nSET PATH=C:\\tools\\cuda\\bin;%PATH%`\r\nthis folder can be changes according to where you have installed cuda and CuDNN. Just make sure to include all the sub folders such as extras and bin. Also if you want a bit of stability. You can install Tf 2.4. it works with my RTX 3070. \r\n`pip install tensorflow-gpu==2.4.0rc2` \r\n", "@king398 in your case does \r\n\r\n**from tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()**\r\n\r\ngives the output a list having GPU also?", "Yes it does and also trains my own network.\r\nIt gives the output\r\n`[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 9648146130202569739\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 6910041152\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 8457418497758750663\r\nphysical_device_desc: \"device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\"\r\n]`\r\nIs the fix working. Please also give me a screen shot of - Type `PATH` in your command prompt window", "@king398 , Thanks for sharing the code output, I was just confirming if it was a code issue. \r\nI'll try to add a path and give an update here. \r\n\r\nI'm using Linux and will share the screenshot of PATH.", "It should be somewhat like this:\r\n![image](https://user-images.githubusercontent.com/58468909/99967221-8cd88100-2dbd-11eb-8509-47b67ac5466c.png)\r\n", "Also see https://github.com/tensorflow/tensorflow/issues/43718#issuecomment-711057497", "@king398 @ymodak thanks, ....my issue is resolved. It was a docker image problem, I was using docker which had drivers compatible with CUDA 10.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45089\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45089\">No</a>\n", "`pip install tensorflow==2.5.0-rc1` worked for me"]}, {"number": 45088, "title": "worker go to endless loop when ps send a UnavailableError to worker.", "body": "TF version: tf1.15 & master\r\nI want to implement a small function that is worker blacklist in ps.\r\nPS check worker-blacklist when worker create session on ps.\r\nIf PS find an unexpected worker, ps return a errors.UnavailableError to this worker, then refused worker go to endless loop(bug).\r\n\r\nI check codes and logs, I find a while(True) when worker recv errors.UnavailableError.\r\n**I think worker should have max retry time and sleep when worker recv errors.UnavailableError.(pr)**\r\nof course, I will add retrun permission error when ps frequently recv new connection from refused worker .\r\npseudo code:https://github.com/zhaozheng09/tensorflow/pull/1/files\r\n\r\nShould we change endless-loop to max_retry-loop when worker create session ?", "comments": ["@zhaozheng09,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using. \r\n\r\nAlso, TensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "@amahendrakar Ok.", "It is not too easy for me to run v2.3, and I think that is a simple question, so I just want to ask what should the worker do when the worker keeps receiving retry requests(UnavailableError) from Ps?\r\n1. Infinite retry.\r\n2. max times retry.", "I reproduced in v2.3\r\nPS changed:\r\nI add a line that `s = errors::Unavailable(\"Fake a UnavailableError when create session .\");` for simulating Ps return a UnavailableError when worker try to create session in ps.\r\n\r\nI launch worker & ps, then I get endless-loop logs like:\r\n\r\nI1125 09:47:04.864392 140107344144192 monitored_session.py:246] Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from ./log_dir/202011101457/model.ckpt-0\r\nI1125 09:47:04.865339 140107344144192 saver.py:1293] Restoring parameters from ./log_dir/202011101457/model.ckpt-0\r\nINFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: **Fake a UnavailableError when create session .**\r\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\r\n:{\"created\":\"@1606268824.888877822\",\"description\":\"Error received from peer ipv4:127.0.0.1:9282\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Fake a UnavailableError when create session .\",\"grpc_status\":14}\r\nI1125 09:47:04.896030 140107344144192 monitored_session.py:1246] An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: **Fake a UnavailableError when create session .**\r\nAdditional GRPC error information from remote target /job:ps/replica:0/task:0:\r\n:{\"created\":\"@1606268824.888877822\",\"description\":\"Error received from peer ipv4:127.0.0.1:9282\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Fake a UnavailableError when create session .\",\"grpc_status\":14}\r\nINFO:tensorflow:Graph was finalized.\r\nI1125 09:47:04.896190 140107344144192 monitored_session.py:246] Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from ./log_dir/202011101457/model.ckpt-0\r\nI1125 09:47:04.897225 140107344144192 saver.py:1293] Restoring parameters from ./log_dir/202011101457/model.ckpt-0\r\nINFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: **Fake a UnavailableError when create session .**\r\n\r\nI find CallWithRetry in grpc_remote_master.cc::155 retry max times when grpc recv Unavailable, but _create_session in training/monitored_session.py::1234 retry infinitely, so I think we should change endless-loop to max_retry_loop to solve endless-loop.", "Happy Thanksgiving~ @all", "> I add a line that `s = errors::Unavailable(\"Fake a UnavailableError when create session .\");` for simulating Ps return a UnavailableError when worker try to create session in ps.\r\n\r\n@zhaozheng09,\r\nCould you please provide a minimal code snippet so that we can reproduce this error on our end? Thanks!", "I try, thanks, please close this quest until I directly find this question.", "@zhaozheng09,\r\nThank you for the update. Marking the issue as closed, please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45088\">No</a>\n"]}, {"number": 45087, "title": "Create makefile targets to expose file lists to enable separate scripting", "body": "As per bug #45086 and the RFC at https://docs.google.com/document/d/1gfbpBtiivYMenCDIvJb-m5RIV4rKAR5dvdAJC5LLv8o/edit?usp=sharing we want to break out the project generation process from the standard make file dependency logic. By creating these targets, external scripts can retrieve the source files needed for a particular platform.\r\n\r\nThere are no tests for this change, because it's tricky to integrate GitHub-only tests into our standard Bazel setup. The scripts that will rely on this change will have their own tests that will check this API is working, and we don't have direct tests for other makefile targets, so I hope this is reasonable.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> There are no tests for this change, because it's tricky to integrate GitHub-only tests into our standard Bazel setup. The scripts that will rely on this change will have their own tests that will check this API is working, and we don't have direct tests for other makefile targets, so I hope this is reasonable.\r\n\r\nI don't see why we can't add a test script that does (for example) a cmake build in /tmp and add that to `test_all.sh`. We don't need to touch bazel for this at all.\r\n\r\nIt can certainly be a separate PR but would definitely be worth having.", "@petewarden Any update on this PR? Please. Thanks!", "@petewarden  Can you please resolve conflicts? Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@petewarden Can you please resolve conflicts? Thanks!", "closing this PR in favor of the work being done for https://github.com/tensorflow/tensorflow/issues/47413"]}, {"number": 45086, "title": "Create makefile targets to expose file lists to enable separate scripting", "body": "As the first step in the project generation process described in https://docs.google.com/document/d/1gfbpBtiivYMenCDIvJb-m5RIV4rKAR5dvdAJC5LLv8o/edit?usp=sharing we need to expose the file lists held within the makefile through an API. This bug is to track the work involved in that process.\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/44909 is a similar bug that we have closed in favor of the current bug.", "closing the current bug in favor of https://github.com/tensorflow/tensorflow/issues/47413", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45086\">No</a>\n"]}, {"number": 45085, "title": "Support SparseSegmentSum and SparseSegmentSumWithNumSegments on GPU", "body": "This is a PR from JIZHI Team & TaiJi AI platform in Tencent.\r\n\r\nCurrently, there is no GPU support for `sparse_segment_reduction` ops in Tensorflow. This commit offers GPU supports on `SparseSegmentSum` and `SparseSegmentSumWithNumSegments`. And we are looking forward to supporting more sparse reduction ops for training and inference in sparse scenarios.", "comments": ["@Lifann  Can you please resolve conflicts? Thanks!", "> @Lifann Can you please resolve conflicts? Thanks!\r\n\r\nThanks for your reminder! I have resolved the conflicts.", "@Lifann  Can you please check @sanjoy's comments and keep us posted ? Thanks!\r\n", "@Lifann  Any update on this PR? Please. Thanks!", "> @Lifann Can you please check @sanjoy's comments and keep us posted ? Thanks!\r\n\r\nSorry for the delay. And thanks for @sanjoy 's review.\r\n\r\nBesides, I also found and fixed an [asynchronous problem in D2H copy](https://github.com/tensorflow/tensorflow/pull/45085/commits/6ac8ef4dc24bf0ad720c0d126b652a80cf5c1a8c#diff-d29812caca45335066ddacf8fb06afc416f1f557bc446068ebaa163495e0b03bR1023), which may lead to the host gets `output_rows` before the `last_segment_id_on_device` copied to host.", "@Lifann Is this ready for review or are you still working on it?", "> @Lifann Is this ready for review or are you still working on it?\r\n\r\nThis PR is ready for review.", "@gbaned @sanjoy  Sorry for bothering you. Is there any further problem with this PR? If there is any need for modification, please let me know. Thanks.", "Hi @Lifann,\r\n\r\nSorry for the delayed response, this PR somehow fell through the cracks.\r\n\r\nIt seems like @benbarsdell has also implemented these ops in https://github.com/tensorflow/tensorflow/pull/47974.  Would it be OK if we review & merged that PR (please feel free to leave a review there)?  On first glance that implementation looks like it is vectorized so I'm expecting that to be faster.", "> Hi @Lifann,\r\n> \r\n> Sorry for the delayed response, this PR somehow fell through the cracks.\r\n> \r\n> It seems like @benbarsdell has also implemented these ops in #47974. Would it be OK if we review & merged that PR (please feel free to leave a review there)? On first glance that implementation looks like it is vectorized so I'm expecting that to be faster.\r\n\r\nThanks for the reply.\r\n\r\nI go check #47974. It seems to be much better than this PR.\r\n\r\nGood to know there is more professional developer aiming at the same goal. And also it's a nice chance for me to learn from a better experience.", "> Hi @Lifann,\r\n> \r\n> Sorry for the delayed response, this PR somehow fell through the cracks.\r\n> \r\n> It seems like @benbarsdell has also implemented these ops in #47974. Would it be OK if we review & merged that PR (please feel free to leave a review there)? On first glance that implementation looks like it is vectorized so I'm expecting that to be faster.\r\n\r\nThen I close this PR."]}, {"number": 45084, "title": "Support SparseSegmentSum and SparseSegmentSumWithNumSegments on CUDA.", "body": "Currently, there is no GPU support for `sparse_segment_reduction` ops. This commit offers GPU support on `SparseSegmentSum` and `SparseSegmentSumWithNumSegments`. And I'm looking forward to supporting other sparse reduction operations.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45084) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 45083, "title": "Which version of Protobuf do I need to compile the TensorFlow2.0 source code \uff1f", "body": "Which version of Protobuf do I need to compile the TensorFlow2.0 source code \uff1f", "comments": ["@tensorflowt \r\nProtobuf = 3.6.0 will be good to go.\r\n\r\nYou can install by below commands  and then import tensorflow.\r\n\r\n```\r\npip uninstall protobuf\r\npip install protobuf==3.6.0\r\n```\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "HI\uff01If I compile c++ source code is also this version \uff1f", "C++ code has versions specified in `workspace.bzl` file and automatically pulled for you when you do `bazel build`.", "Closing this issue since its answered. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45083\">No</a>\n"]}, {"number": 45082, "title": "mnasnet with squeeze and excite can not be quantized", "body": " I wrote a Colab document with Mnasnet in keras and added a squeeze and excite block\r\nTensorFlow was 2.3.0\r\nI have been able to generate the .h5 file correctly \r\nHowever when I apply the post training quantization procedure, I have this error\r\nRuntimeError: Unsupported output type INT8 for output tensor 'Identity' of type FLOAT32.\r\nWithout the squeeze and excite block it works nicely generating int8 version\r\nAttached the notebook\r\n[Quantize_procedure_mnasnet_se.zip](https://github.com/tensorflow/tensorflow/files/5580189/Quantize_procedure_mnasnet_se.zip)\r\nThanks and kind regards\r\nDanilo\r\n", "comments": ["@danilopau,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: 'ILSVRC2012_img_val'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7a2bc45ee8dd09b316f2232c58bdeea9/45082.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide all the supporting files necessary to reproduce the issue. Thanks!", "> However when I apply the post training quantization procedure, I have this error\r\n> RuntimeError: Unsupported output type INT8 for output tensor 'Identity' of type FLOAT32.\r\n\r\n@danilopau,\r\nAlso, please take a look at issues [#42082](https://github.com/tensorflow/tensorflow/issues/42082) and [#38285](https://github.com/tensorflow/tensorflow/issues/38285) with a similar error and let us know if it helps. Thanks!", "\r\n\r\nI used pip install tensorflow==2.4.0-rc2\r\nwith code\r\nmodel = tf.keras.models.load_model(selected_model_path_h5)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\n\r\nbut got error on converter.convert()\r\nRuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor input_1\r\nEmpty min/max for tensor input_1\r\n\r\nwhere representative_data_gen is\r\nnum_calibration_steps = 10000\r\nnum_classes = 1000\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(X_temp).batch(1).take(num_calibration_steps):#from_tensor_slices(x_train)\r\n    # Model has only one input so each data point has one element.\r\n    yield [input_value]\r\n\r\nCalibration data file is @ https://drive.google.com/file/d/153rsbE_37d5trxxWMLQClZH7mf4htdHe/view?usp=sharing", "and network is a simple squeeze and excite\r\ndef seblock(input_shape=None, nb_classes=10):\r\n    img_input = layers.Input(shape=input_shape)\r\n    \r\n    x = layers.Conv2D(filters=320, kernel_size=(3,3))(img_input)\r\n    x = layers.GlobalAveragePooling2D()(x)\r\n    # x = layers.GlobalMaxPooling2D()(x)\r\n    # start squeeze and excite block\r\n    ratio=16\r\n    filters=320 #depends on previous x shape\r\n    se_shape = (1, 1, 320)\r\n    se = x\r\n    se = layers.Reshape(se_shape)(se)\r\n    se = layers.Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\r\n    se = layers.Dense(640, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\r\n    y = layers.Dense(640, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(x)\r\n    w = layers.Multiply()([y, se])\r\n    # end squeeze and excite block\r\n\r\n    z = layers.Dense(nb_classes, activation='softmax', use_bias=True, name='proba')(w)\r\n    \r\n    inputs = img_input\r\n    \r\n    model = Model(inputs, z, name='seblock')\r\n    return model\r\n\r\ncan you try to quantize it using ILVRC 1000 classes dataset please ?", "Guys, I need a step by step tutorial not pointing me to half million tickets to surf between trying and loosing time. If you want tensorflow used by ordinary programming guys in MCU space you need to be humble and simple teaching to people how to do this quantization job \r\n", "@danilopau,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a75a39c0587d8b0915ef0c51201f1e36/45082.ipynb). With [TF v2.4.0rc4](https://colab.research.google.com/gist/amahendrakar/907a1e4b56f3246c8b2f2a848b024196/45082-2-4.ipynb) the error is `ValueError: Model output is not dequantized.`\r\n\r\n\r\n\r\nHowever, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8e7b269ac8f42da2422cce0423aa1f55/45082-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "Greatly appreciated Abhilash!\r\n\r\nWhen do you expect tf 2.5.0 stable and released ?\r\n\r\nKindest regards\r\nDanilo", "@danilopau,\r\nSorry for the delayed response. I was waiting to check if the issue would be resolved with TF v2.4 stable version, but it did not. \r\n\r\n> When do you expect tf 2.5.0 stable and released ?\r\n\r\nI do not have an exact date but it should be released by Q1 2021. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45082\">No</a>\n"]}, {"number": 45081, "title": "Management of Gpu memory by fit method to avoid memory errors", "body": "\r\n**System information**\r\n- Colab GPU kernel (Tf : 2.3.0 , GPUs Available : 1)\r\n\r\n**Describe the current behavior**\r\nThere are memory errors when:\r\n- I fit models with huge value for batch size ;\r\n- I fit CNN models with large images ;\r\n\r\n**Describe the expected behavior**\r\nUpdate the fit method so that the training of models can according to the memory available on the GPU and automatically define the best mini-batch size (split the batch into several parts) to avoid the memory errors so that users no longer have to reduce batch size or image size.\r\nFor large images, the fit method should be able to split the image into several parts to try to reduce the risk of memory errors.", "comments": ["@eaedk \r\nPlease share simple stand alone code for us to replicate the issue faced, or if possible share a colab gist with the error reported.", "https://colab.research.google.com/drive/13m4NSY5f5UUwIsvyTie_UejqNNmMXVrU?usp=sharing", "![Screenshot from 2020-11-27 16-40-25](https://user-images.githubusercontent.com/28601730/100470032-58233d00-30cf-11eb-98d6-5621f38d0432.png)\r\n above you can see the error when I try to apply transfer learning on Resnet50V2 with batch_size = 32 and input_size = (720, 720, 3) .", "when I reduce the batch size, there is no more this error. so I want an update of the fit method that will be able to find automatically the best mini-batch to split my inference by its . Then we'll no more have RessourceExhaustedError and we'll have more simplicity to train big architectures, architectures with large batch size, or architectures with large inputs. It will help us to have better models", "Hi @eaedk, trying to better understand the request here. Are you proposing a feature request to automatically determine the largest batch size that will fit in memory?", "Hi @nikitamaia, what I want is that whatever the bigger batch_size the Gpu could automatically compute the batch by many min-batch of a well-determined size to avoid memory error . \r\n\r\nI'm a french-speaking and sometimes I have some difficulties to explain idea... so I repeat, if it isn't clear yet.\r\n\r\nwhat I want is that whatever the size of the batch that we set, the GPU can split it into several small batches to process so as to avoid memory errors due to the size of the batch being too large.\r\n", "## \ud83d\ude80 Feature\r\nEnsure that models can according to the memory available on the GPU during a feed-forward, automatically split the sent batch into several small mini_batches to avoid the memory errors. \r\n\r\n## Motivation\r\nSo that during the training, users no longer have to reduce batch_size (or image size in case of image). The training will just take time but any users could train models any batch_size without check manually the good size for mini_batch.\r\nDuring inference also, user could just pass a big batch_size and the framework will automatically define the best number of sample to pass at the time through the GPU to don't have memory errors anymore.\r\n\r\n## Pitch\r\n\r\nwhat I want is that whatever the size of the batch that we set, the GPU can split it into several small batches to process so as to avoid memory errors due to the size of the batch being too large.\r\n", "@eaedk Sorry for the late response. Can you please share a simple standalone code to reproduce the issue or share the data? \r\n\r\nWhen you run a model in a `for` loop, keras will take more memory to keep those models in the memory. Did you try releasing the memory https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session. Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45080, "title": "Fix typo in gradients.cc", "body": "faciliate -> facilitate", "comments": []}, {"number": 45079, "title": "Custom loss function always throws: TypeError: __init__() takes 1 positional argument but 3 were given", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution : Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GTX 1060 (6gb)\r\n\r\n**Describe the current behavior**\r\nI've created a custom loss function by subclassing from `tf.keras.losses.Loss` and a model using the functional API and specified its outputs to be that of two layers, I need one of them to passed to the loss function and another is the actual output that should print out to the user. Since there are two outputs I pass a list of the same loss when compiling the model and specify the loss weights as `[1.0, 0.0]` because I only care about the loss of one output. When I try the model using on of the basic losses like `categorical_crossentropy` it runs fine. But when I use the loss function I wrote it throws me `TypeError: __init__() takes 1 positional argument but 3 were given`. Furthermore, when building the model, I tried specifying the output to be only one value instead of two, and the same issue arises.\r\n\r\n**Describe the expected behavior**\r\nThe loss function should work normally since I wrote it according to the docs.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab link](https://colab.research.google.com/drive/1nB2kVk_e_Mj3dy8mJ0kka2gHHLUSwiHn?usp=sharing)\r\n", "comments": ["I have tried in colab with TF version 2.2, 2.3, nightly versions(`2.5.0-dev20201122`) and was able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/271dca3a80c757bd335ae20b05f633b6/untitled529.ipynb) Thanks!", "Someone suggested passing `EndToEndLoss()` to the `compile` method instead of `EndToEndLoss`, this resulted in the error `    OperatorNotAllowedInGraphError: iterating over tf.Tensor is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.`", "@rmothukuru will someone provide insight into this?", "that loss function indeed should get only one input, cause it's a simple logistic regression over the cosine similarity, which is a scalar in the range [-1,1]\r\nDid you forget maybe to do the average of the enrollment utterances and then take the cosine similarity with the evaluation utterance before adding the logistic loss layer?\r\n\r\ncosine_distance should return a scalar... It's the scalar product of the average enrollment embedding and the evaluation embedding, after L2 normalization to force them to have unit length\r\nI think the error is in `cosine_distance`\r\n`return -K.mean(x * y, axis=-1, keepdims=True)`\r\nwhile it shall be\r\n`return K.dot(x,y)`", "there is also a mistake in the EndToEndLoss, you should do the 1- switch before applying the -log( )\r\nTo avoid using if conditions in the loss, use this trick that usually is learned from implementing logistic regression.\r\nAssuming y_true is 0 or 1 (reject or accept) and score is the cosine similarity in the range [-1,1]\r\n`-( y_true*log(sigmoid(w*score-b)) + (1-y_true)*log(1-sigmoid(w*score-b)) ) `\r\n\r\nyou can use this directly to get the sigmoid\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid\r\ninstead of using backend.exp and computing it yourself", "Hi @kareemamrr, I made a couple of changes to your colab and was able to get the training working. Please [see the gist here.](https://colab.research.google.com/gist/nikitamaia/ae3cb10a3b8944031d4ea00c036b7ff3/untitled12.ipynb)\r\n\r\nIn `model.compile`, you have `loss=EndToEndLoss`, I changed that to `loss=EndToEndLoss()`\r\nI also replaced the `np.log` with `tf.math.log` in` EndToEndLoss` because using TF ops is preferred over using numpy ops. ", "Thank you both @steve3nto @nikitamaia ! Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45079\">No</a>\n"]}, {"number": 45078, "title": "Model weights using the HDF5 format are not saved", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yes\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Not on mobile device\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): Not compiling from source\r\n- GCC/Compiler version (if compiling from source): Not compiling from source\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: No GPU\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the HDF5 format to save the weights of a model (directly added in the model class), the value of the weights I get when loading the model are not the same as the ones of the saved model.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe value of the weights of the loaded model should be the same as that of the saved model.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis is [the colab](https://colab.research.google.com/drive/1TAitBc7wM_5jj0vi3V00xoH7paFxy2YP?usp=sharing) I used to illustrate this issue.\r\n\r\n**Other info / logs**\r\n\r\nWhen using layers in the model, instead of weights added via (`self.add_weight`), the values of the layers' weights are correctly saved. So it's really a problem of the weights of a `Model` not being saved (or loaded correctly).", "comments": ["Small note, when not using the HDF5 format, i.e. `model.save_weights('model')`, I get the following error: `AttributeError: 'NoneType' object has no attribute 'replace'`.\r\n\r\nThe full stacktrace is:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-dcf87c4ca34b> in <module>()\r\n----> 1 model.save_weights('model')\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in save_weights(self, filepath, overwrite, save_format, options)\r\n   2099              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\r\n   2100             % (optimizer,))\r\n-> 2101       self._trackable_saver.save(filepath, session=session, options=options)\r\n   2102       # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\r\n   2103       checkpoint_management.update_checkpoint_state_internal(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py in save(self, file_prefix, checkpoint_number, session, options)\r\n   1198     file_io.recursive_create_dir(os.path.dirname(file_prefix))\r\n   1199     save_path, new_feed_additions = self._save_cached_when_graph_building(\r\n-> 1200         file_prefix_tensor, object_graph_tensor, options)\r\n   1201     if new_feed_additions:\r\n   1202       feed_dict.update(new_feed_additions)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py in _save_cached_when_graph_building(self, file_prefix, object_graph_tensor, options)\r\n   1135     (named_saveable_objects, graph_proto,\r\n   1136      feed_additions) = self._gather_saveables(\r\n-> 1137          object_graph_tensor=object_graph_tensor)\r\n   1138     if (self._last_save_object_graph != graph_proto\r\n   1139         # When executing eagerly, we need to re-create SaveableObjects each time\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py in _gather_saveables(self, object_graph_tensor)\r\n   1101     \"\"\"Wraps _serialize_object_graph to include the object graph proto.\"\"\"\r\n   1102     (named_saveable_objects, graph_proto,\r\n-> 1103      feed_additions) = self._graph_view.serialize_object_graph()\r\n   1104     if object_graph_tensor is None:\r\n   1105       with ops.device(\"/cpu:0\"):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/graph_view.py in serialize_object_graph(self)\r\n    385     trackable_objects, path_to_root = self._breadth_first_traversal()\r\n    386     return self._serialize_gathered_objects(\r\n--> 387         trackable_objects, path_to_root)\r\n    388 \r\n    389   def frozen_saveable_objects(self, object_map=None, to_graph=None,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/graph_view.py in _serialize_gathered_objects(self, trackable_objects, path_to_root, object_map, call_with_mapped_captures)\r\n    340     object_names = object_identity.ObjectIdentityDictionary()\r\n    341     for obj, path in path_to_root.items():\r\n--> 342       object_names[obj] = _object_prefix_from_path(path)\r\n    343     node_ids = object_identity.ObjectIdentityDictionary()\r\n    344     for node_id, node in enumerate(trackable_objects):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/graph_view.py in _object_prefix_from_path(path_to_root)\r\n     62   return \"/\".join(\r\n     63       (_escape_local_name(trackable.name)\r\n---> 64        for trackable in path_to_root))\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/graph_view.py in <genexpr>(.0)\r\n     62   return \"/\".join(\r\n     63       (_escape_local_name(trackable.name)\r\n---> 64        for trackable in path_to_root))\r\n     65 \r\n     66 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/graph_view.py in _escape_local_name(name)\r\n     55   # edges traversed to reach the variable, so we escape forward slashes in\r\n     56   # names.\r\n---> 57   return (name.replace(_ESCAPE_CHAR, _ESCAPE_CHAR + _ESCAPE_CHAR)\r\n     58           .replace(r\"/\", _ESCAPE_CHAR + \"S\"))\r\n     59 \r\n\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n```", "@zaccharieramzi,\r\nPlease take a look at issue [#26811](https://github.com/tensorflow/tensorflow/issues/26811#issuecomment-474255444) with a similar error and let us know if it helps. Thanks!", "Naming the weight didn't help unfortunately. You can see this in the same [colab](https://colab.research.google.com/drive/1TAitBc7wM_5jj0vi3V00xoH7paFxy2YP?usp=sharing).", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/12ef06e0238ccd4160f1470621dbca73/45078-2-3.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/33d6b2e906acbab5fd97213a9502d33e/45078-2-5.ipynb). Please find the attached gist. Thanks!", "@zaccharieramzi,\r\nI think this is an intended behavior (not very sure though) as per [this comment](https://github.com/tensorflow/tensorflow/issues/41116#issuecomment-657768371). Can you please check it and let me know? Thanks!", "Hmmm I don't think so because it's not related the optimizer but really to actual model weights. The only difference is that they are instantiated inside a model object instead of inside a layer.", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/dc72e669d69e3dfdba9e305a4173fefa/untitled267.ipynb) ..Thanks!", "@zaccharieramzi Looks like this was resolved in `TF2.7`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/0607382ea383bad03880633fc8118bbe/model_direct_weights_not_saved.ipynb) is a gist for reference.\r\n\r\n`model.b`\r\n<tf.Variable 'bias:0' shape=() dtype=float32, numpy=1.9999998>\r\n\r\n`other_model.b`\r\n<tf.Variable 'bias:0' shape=() dtype=float32, numpy=1.9999998>\r\n\r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45078\">No</a>\n"]}]