[{"number": 37001, "title": "[Intel MKL] Fix memory leak in dnnl 0.21.2", "body": "", "comments": ["@penpornk Thanks for the comments, I have update the format by using clang-format 8.0.1.\r\nBTW, is this version the minimal requirement in the format check? Thank you!", "@penpornk Thank you, will use the clang-format 8.0.1 to do the format check, thanks."]}, {"number": 37000, "title": "Proposal: Modify tf.math.reduce_variance so that it is compatible with ragged tensors", "body": "Sorry for filing this under \"bug\", but the \"feature request\" submission option didn't show up for me. Please correct this if possible (I copied the template by hand from the markdown file).\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1, but applies to master branch as of opening time\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently the variance in the mentioned op is computed as E[(X - E[X])^2]:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c71db95e184447c0008dc007a27d692cd5496d0e/tensorflow/python/ops/math_ops.py#L2112-L2114\r\n\r\nBecause that requires keepdims == True for the first mean reduction, this is not compatible with ragged tensors. An alternative formulation for the variance could however be E[X**2] - E[X]^2\r\n\r\n```\r\n mean_of_square = reduce_mean(gen_math_ops.square(input_tensor), axis=axis, keepdims=keepdims) \r\n square_of_mean = gen_math_ops.square(reduce_mean(input_tensor, axis=axis, keepdims=keepdims)) \r\n return mean_of_square - square_of_mean\r\n```\r\n\r\n**Will this change the current api? How?**\r\nThis might have a reduced numerical stability for Var << Mean, but it could be used at least for ragged tensors in the keepdims == False case. Checking if the tensor is ragged and choosing the implementation based on that would guarantee API stability, but potentially increase maintenance burden later on.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who want to aggregate statistics over variable-length sequences encoded in ragged tensors.\r\n\r\n**Any Other info.**\r\n\r\n\r\n\r\n", "comments": ["I think it should be possible to add support for keepdims == True to the ragged versions of reduce_xyz.", "Encountered the same problem with `reduce_std`, would be very useful if this was fixed."]}, {"number": 36999, "title": "recompute_grad computes gradient incorrectly when the same tensor is passed in multiple argument positions", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux 5.5.3-arch1-1\r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: 10.2/7\r\n- GPU model and memory: 1070 Ti, 8GB\r\n\r\n**Describe the current behavior**\r\nPassing the same tensor as two arguments to a function decorated with `recompute_grad` leads to incorrect gradient computation.\r\n\r\n**Describe the expected behavior**\r\nThe gradient should be computed properly. The `tf.custom_gradient` code uses `experimental_ref`s to deduplicate variables, and doing the same for input tensors will resolve this issue.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.recompute_grad\r\ndef broken_add(a, b):\u00b7\r\n    return a + b\u00b7\r\n\r\nx = tf.ones(3, dtype=tf.float32)\r\nwith tf.GradientTape() as g:\r\n    g.watch(x)\r\n    z = tf.reduce_sum(broken_add(x, x)) \r\nprint(g.gradient(z, x).numpy())\r\n```\r\nThis outputs `[4. 4. 4.]` instead of the correct value `[2., 2., 2.]`.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe following is used to deduplicate variables in `tf.custom_gradient`, and adding the same for input tensors should resolve the issue.\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/custom_gradient.py#L410-L414", "comments": ["@davisyoshida,\r\nWas able to reproduce the issue with [TF 2.1](https://colab.sandbox.google.com/gist/amahendrakar/6565a6eda1a453d494c974a51dd07142/36999.ipynb), but it seems to be fixed in [TF-nightly](https://colab.sandbox.google.com/gist/amahendrakar/9295e2162c43a37ca288e1faa8923d3d/copy-of-36999.ipynb). Please check the attached gists. Thanks!", "Great! Sorry to waste your time.", "No worries @davisyoshida, happy to help.\r\n\r\nClosing this issue as it resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36999\">No</a>\n"]}, {"number": 36998, "title": "Keras models train correctly with or without tf.function decorator but this is not correct for custom models", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Compare the following two codes. If include @tf.function then both works well. If not include @tf.fucntion then the custom low-level model does not train.\r\n\r\n\r\n[tf_function_problem.zip](https://github.com/tensorflow/tensorflow/files/4242370/tf_function_problem.zip)\r\n", "@miladtoutounchian \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I did pip install tensorflow==2.1, OS is Mac  ", "@miladtoutounchian \r\nI have run the code shared by you on tf 2.1 with and with out @tf.function and did not face any issues,please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/cf754f626032f37f7edc73d2ea079e39/36998.ipynb) for the [same](https://colab.sandbox.google.com/gist/Saduf2019/809a5453c7b7dbbcc1e231a486865744/36998.ipynb). the same code runs without any issues on nightly as well.\r\nin case your still facing issue please share a gist where the error is seen along with error logs if any.", "I am not reporting that we get error, I am saying the model is not trained which means the accuracy performance is very low and is not getting better through epochs if do not use  @tf.function decorator. After 5 epoch, here is the model very poor performance:\r\nEpoch: 5\r\nAvg loss: 0\r\naccuracy: 0.2763499915599823\r\n\r\n**Add tf.function decorator then the model is being trained.** ", "@miladtoutounchian \r\nI believe the difference in performance accuracy is because, \"@tf.function decorator.\"  will be compiled into a graph, which means you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.\r\nThis [link](https://www.tensorflow.org/guide/function) confirms the same.please le tme now if that answers your query.", "@Saduf2019 Decorating with `@tf.function` may indeed have benefits as to execution runtime, however it should not have any effect on the accuracy reached, unless there is either a tensorflow bug, or some error-inducing side effects within @miladtoutounchian's code.", "I have ran both the codes(with and without @tf.function) on colab with tf-nightly but didn't find much of a difference in accuracy. Please find my gist([with tf.function](https://colab.sandbox.google.com/gist/gowthamkpr/7f6d5051e5b1cbfebeab3ab962ec6017/with_tf_function.ipynb) and [without tf.function](https://colab.sandbox.google.com/gist/gowthamkpr/1337b792dedc5305fa9339dcda8b1a5c/without_tf_function.ipynb))\r\n\r\nPlease take a look at these gists and if I am wrong please correct me. Thanks!", "@gowthamkpr Thank you for running the tests and sharing the gists :)\r\nInterestingly, the accuracy seems slightly (but significantly) lower with `tf.function`, but I think this is actually due to the custom `Model` rather than to the decorator; more precisely, in the custom model, weights initialization does not follow the default scheme used in the `tf.keras.layers.Dense` layers, which may cause this difference (alternatively, it may be sheer randomness due to the dataset shuffling, or an interaction between both aspects).\r\n\r\n@miladtoutounchian Could you please check that these tests correspond to what you were running in the first place? If so, I honestly have no idea why you encountered the reported convergence issues...", "Please carefully read the title of this issue that I reported. **Keras models train correctly with or without tf.function decorator but this is not correct for custom models.** \r\n\r\nWe have two Model cases and each case has two propagation methods, therefore:\r\n\r\n_Case 1: Model is Keras:_\r\n1-1 : propagate function with `@tf.function` decorator -> Works well\r\n1-2:  propagate function without `@tf.function` decorator -> Works well\r\n\r\n_Case 2: Model is custom (not Keras):_\r\n2-1: propagate function with `@tf.function` decorator -> Works well\r\n2-2: propagate function without `@tf.function` decorator -> **the model does not train at all** \r\n\r\nCan any of you answer why we encounter of model training problem with Case 2-2?\r\nHowever there is no problem with Case 1-2.\r\n\r\nI believe this is a Tensorflow Bug ", "Hi,\r\nI was able to replicate the issue, thanks for clarifying.\r\nI have two distinct answers, and I will start with the obvious but unsatisfactory one, before moving to what appears like an actual issue.\r\n\r\n1. Your custom Model does not abide by the current API.\r\nIf you replace it with a custom keras Model subclass, such as the one implemented below, then it trains perfectly, with or without tf.function decorating your custom training step.\r\n```python\r\nclass ModelKeras(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n      super().__init__()\r\n      kwargs = {'kernel_initializer': 'normal', 'bias_initializer': 'normal'}\r\n      self.layer_1 = tf.keras.layers.Dense(512, 'relu', **kwargs)\r\n      self.layer_2 = tf.keras.layers.Dense(512, 'relu', **kwargs)\r\n      self.out_layer = tf.keras.layers.Dense(10, **kwargs)\r\n\r\n    @property\r\n    def trainable_vars(self):  # merely to leave the rest of the code unchanged\r\n        return self.trainable_variables\r\n\r\n    def call(self, inputs):\r\n      output = self.layer_1(inputs)\r\n      output = self.layer_2(output)\r\n      return self.out_layer(output)\r\n```\r\n\r\n2. Gradient computation differs when `tf.function` decorates `propagate`.\r\nI do not get why, but here is the test I ran:\r\n```python\r\n# Define a function to compute gradients of a network's weights w.r.t. a given batch.\r\ndef compute_gradients(model, x_batch, y_batch):\r\n    with tf.GradientTape() as tape:\r\n        pred = tf.nn.softmax(model(x_batch))\r\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, pred)\r\n    return tape.gradient(loss, model.trainable_vars)\r\n# Make a tf.function-decorated copy of the previous.\r\ndecorated_gradients = tf.function(compute_gradients)\r\n\r\n# Gather a training batch.\r\nx_batch, y_batch = next(iter(mnist_dataset()))\r\n# Instantiate two models and build them.\r\nmodel_custom = Model()  # weights are built at instantiation\r\nmodel_keras = ModelKeras()\r\n_ = model_keras(x_batch)  # build weights through sample processing\r\n# Set the second model's weights equal to those of the first one.\r\nweights = [\r\n    w.numpy() for pair in zip(model_custom.trainable_vars[:3], model_custom.trainable_vars[3:])\r\n    for w in pair\r\n]\r\nmodel_keras.set_weights(weights)\r\n\r\n# Compute gradients for both models without tf.function.\r\n# Save for ordering, the results are the same for both, as should be.\r\ncompute_gradients(model_custom, x_batch, y_batch)\r\ncompute_gradients(model_keras, x_batch, y_batch)\r\n\r\n# Compute gradients for both models with tf.function.\r\n# Save for ordering, the results are the same for both, as should be.\r\n# However, they differ from the outputs of the the non-decorated function, which is weird.\r\ndecorated_gradients(model_custom, x_batch, y_batch)\r\ndecorated_gradients(model_keras, x_batch, y_batch)\r\n```\r\n\r\nSo, for some reason, it appears that `tf.function` decoration changes the way gradients are computed, which might be the cause of the model's lack of convergence. As a matter of fact, when not decorated, the computed gradients tend to be very sparse, i.e. there are a lot of zero values resulting in most weights not being updated during the training step. I do not know why this is the case; it would seem that part of the computation is not properly tracked?\r\n\r\nNow, the reason why the keras Model trains better is also that in spite of my forcing the use of random normal weights initializers, the initial weights (when not forcefully replaced as in the previous test) are smaller than that generated in the custom model, which seems to result in smoother initial predictions and may explain why it is easier to train.", "What is the next action folks?", "@alextp for more thoughts\r\n\r\nGradientTape should be expected to produce different results inside and outside of tf.function due to the sensitivity of floating-point calculation on ordering. That said, the differences should be small, excluding any source of numerical instability like calculating the loss through the softmax function.\r\n\r\nAs far as I know, Keras runs in graph mode with or without tf.function. So what we're seeing is consistent, in the sense that models train in graph mode, but not in eager mode. I'd expect the eager version to still train, but with a different learning rate. The differences seem indeed significant.", "@mdanatg I look at @pandrey-fr 's example code and I see the very problematic pattern of doing softmax and then cross entropy, instead of using softmax_cross_entropy_with_logits or the equivalent.\r\n\r\nsoftmax-then-cross-entropy is _really_ numerically unstable (you throw away most of the bits of your logits when doing softmax) and should _never_ be used. Because of this the keras cross-entropy function has logic to \"undo\" the softmax in graph mode: https://github.com/tensorflow/tensorflow/blob/ead7a372a82edab37e9103999905cc0d3f7ab74e/tensorflow/python/keras/backend.py#L4628 this code however doesn't work in eager mode because you cannot walk the graph.\r\n\r\nSo this is a known issue, and the fix is to never use softmax followed by cross entropy. @MarkDaoust recently removed this from all of our official examples, so hopefully this will stop happening.\r\n\r\nSadly, then, I'll have to close this issue as working-as-intended.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36998\">No</a>\n", "Thanks Alex,\r\n\r\nWhat are the loss values you're seeing with and without `tf.function`? **Huge** with `tf-function` and ~15 without?  \r\nIn my answer on #34221 I explain a little how this happens and why (unless you have a large number of classes) it's probably also an indicator that  (in addition to the softmax issue) either:\r\n\r\n* Your inputs are not normalized.\r\n* Or your initialization is bad.", "I have changed the code based on @alextp suggestion, now used softmax_cross_entropy_with_logits: \r\n[keras_vs_custom_TF2.zip](https://github.com/tensorflow/tensorflow/files/4340550/keras_vs_custom_TF2.zip)\r\n\r\nNow, the custom model loss is really large compared to Keras model. Also the accuracy of custom model is not as good as Keras model. \r\n\r\n\r\n\r\n", "Please do not do softmax and then cross-entropy; use\nsoftmax_cross_entropy_with_logits instead\n\nOn Mon, Mar 16, 2020 at 2:47 PM Milad Toutounchian <notifications@github.com>\nwrote:\n\n> I have changed the code based on @alextp <https://github.com/alextp>\n> which do softmax first then followed by cross entropy:\n> keras_vs_custom_TF2.zip\n> <https://github.com/tensorflow/tensorflow/files/4340550/keras_vs_custom_TF2.zip>\n>\n> Now, the custom model loss is really large compared to Keras model. Also\n> the accuracy of custom model is not as good as Keras model.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36998#issuecomment-599772546>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNDZBEURS2H226DOQTRH2M6PANCNFSM4KZ5RKAA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp , exactly did what you said. But please compare the loss for Keras vs custom also compare the lower accuracy of custom model.\r\n ", "That means having no softmax activation. If you add the softmax you won't\nget a good loss.\n\nOn Mon, Mar 16, 2020 at 2:58 PM Milad Toutounchian <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> , exactly did what you said. But\n> please compare the loss for Keras vs custom also compare the lower accuracy\n> of custom model.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36998#issuecomment-599776313>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLQP7QBQBBMP56ERMTRH2OG3ANCNFSM4KZ5RKAA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp , I do not get what you are saying. Please offer the solution by writing/modifying my attached code.\r\n", "@miladtoutounchian, again, please see my response on tensorflow/tensorflow#34221: Check your layer initialization. \r\n\r\nYou're using:\r\n\r\n`tf.random.normal([n_hidden_2, n_classes])` (mean=0, var=1)\r\n\r\nKeras is using [GlorotUniform](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform?hl=en).\r\n\r\nAlso, for issues like this:\r\n\r\n* You'll have a much easier time if you go one function at a time, instead of just comparing final results.\r\n* Stack overflow is probably a better place for further discussion, since this is not an error in tensorflow.\r\n\r\nGood luck."]}, {"number": 36997, "title": "cleared unwanted else clauses and minor readability improvement", "body": "", "comments": ["@jaingaurav waiting for approval "]}, {"number": 36996, "title": "[Colab][TF2.x] Socket closed - Error received from peer", "body": "I was trying to train an GAN. Here is the code:\r\n(Full code on Colab: https://colab.research.google.com/drive/1TUKZnOEPT8C8mVJ1iK90n32qT2SiJ6Xo)\r\n\r\n```python\r\nwith strategy.scope():\r\n    # generator_model\r\n    generator_model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Dense(512, activation='relu', input_shape=noise_shape),\r\n        tf.keras.layers.Dropout(0.1),\r\n        tf.keras.layers.Dense(1024, activation='relu'),\r\n        tf.keras.layers.Dense(784, activation='tanh'),\r\n        tf.keras.layers.Reshape(img_shape)\r\n    ])\r\n    generator_model.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\r\n                            loss='binary_crossentropy',\r\n                            metrics=['accuracy'])\r\n    generator_model.summary()\r\n\r\n    # discriminator_model\r\n    discriminator_model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=img_shape),\r\n        tf.keras.layers.Dense(1024, activation='relu'),\r\n        tf.keras.layers.Dropout(0.1),\r\n        tf.keras.layers.Dense(512, activation='relu'),\r\n        tf.keras.layers.Dense(1, activation='sigmoid'),\r\n    ])\r\n    discriminator_model.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\r\n                                loss='binary_crossentropy')\r\n    discriminator_model.summary()\r\n\r\n    # combined_model\r\n    z = tf.keras.layers.Input(shape=noise_shape)\r\n    discriminator_model.trainable = False \r\n    valid = discriminator_model(generator_model(z)) \r\n    combined_model = tf.keras.models.Model(z, valid) \r\n    combined_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\r\n    combined_model.summary()\r\n\r\n# train\r\nfor epoch in range(10000):\r\n    idx = np.random.randint(0, x_train.shape[0], batch_size)\r\n    imgs = x_train[idx]\r\n    noise = np.random.normal(0, 1, (batch_size, 100))\r\n    gen_imgs = generator_model.predict(noise.astype(np.float32))\r\n\r\n    # train discriminator_model\r\n    d_loss_real = discriminator_model.fit(imgs.astype(np.float32), np.ones((batch_size, 1)))\r\n    d_loss_fake = discriminator_model.fit(gen_imgs.astype(np.float32), np.zeros((batch_size, 1)))\r\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n\r\n    # train combined_model\r\n    noise = np.random.normal(0, 1, (batch_size * 2, 100))\r\n    valid_y = np.array([1] * batch_size * 2)  \r\n    g_loss = combined_model.fit(noise.astype(np.float32), valid_y)\r\n```\r\n\r\nWhen I run, it throws out `Socket closed`.\r\n\r\n```python\r\nTrain on 128 samples\r\n 32/128 [======>.......................] - ETA: 7s\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-1-5ba8c95500e1> in <module>()\r\n     82 \r\n     83     # train discriminator_model\r\n---> 84     d_loss_real = discriminator_model.fit(imgs.astype(np.float32), np.ones((batch_size, 1)))\r\n     85     d_loss_fake = discriminator_model.fit(gen_imgs.astype(np.float32), np.zeros((batch_size, 1)))\r\n     86     d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n\r\n10 frames\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    566 \r\n    567   return pack_sequence_as(\r\n--> 568       structure[0], [func(*x) for x in entries],\r\n    569       expand_composites=expand_composites)\r\n    570 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py in <listcomp>(.0)\r\n    566 \r\n    567   return pack_sequence_as(\r\n--> 568       structure[0], [func(*x) for x in entries],\r\n    569       expand_composites=expand_composites)\r\n    570 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py in _non_none_constant_value(v)\r\n    128 \r\n    129 def _non_none_constant_value(v):\r\n--> 130   constant_value = tensor_util.constant_value(v)\r\n    131   return constant_value if constant_value is not None else v\r\n    132 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_util.py in constant_value(tensor, partial)\r\n    820   \"\"\"\r\n    821   if isinstance(tensor, ops.EagerTensor):\r\n--> 822     return tensor.numpy()\r\n    823   if not is_tensor(tensor):\r\n    824     return tensor\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    940     \"\"\"\r\n    941     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n--> 942     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    943     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    944 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n    908       return self._numpy_internal()\r\n    909     except core._NotOkStatusException as e:\r\n--> 910       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    911 \r\n    912   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: Socket closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1582482234.677879877\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\n```\r\n", "comments": ["After some debug, I simplify the code. It may easier to be analyzed.\r\n\r\nColab link: https://colab.research.google.com/drive/1ZhbfF0jqtOU2Gz5k1-M_-T-frQQ10v-D\r\n\r\n```python\r\n%tensorflow_version 2.x\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(1, activation='sigmoid'),\r\n    ])\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy')\r\n\r\nd_loss_real = model.fit(x_train.astype(np.float32), np.ones((x_train.shape[0],1)))\r\n```\r\n\r\nIt will throw the same error.\r\n\r\n```\r\nWARNING:tensorflow:TPU system 10.77.68.2:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\r\nWARNING:tensorflow:TPU system 10.77.68.2:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\r\nINFO:tensorflow:Initializing the TPU system: 10.77.68.2:8470\r\nINFO:tensorflow:Initializing the TPU system: 10.77.68.2:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nTrain on 60000 samples\r\n   32/60000 [..............................] - ETA: 57:25\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-4-75afcd6f6fcd> in <module>()\r\n     19     model.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='binary_crossentropy')\r\n     20 \r\n---> 21 d_loss_real = model.fit(x_train.astype(np.float32), np.ones((x_train.shape[0],1)))\r\n\r\n10 frames\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    566 \r\n    567   return pack_sequence_as(\r\n--> 568       structure[0], [func(*x) for x in entries],\r\n    569       expand_composites=expand_composites)\r\n    570 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py in <listcomp>(.0)\r\n    566 \r\n    567   return pack_sequence_as(\r\n--> 568       structure[0], [func(*x) for x in entries],\r\n    569       expand_composites=expand_composites)\r\n    570 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py in _non_none_constant_value(v)\r\n    128 \r\n    129 def _non_none_constant_value(v):\r\n--> 130   constant_value = tensor_util.constant_value(v)\r\n    131   return constant_value if constant_value is not None else v\r\n    132 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_util.py in constant_value(tensor, partial)\r\n    820   \"\"\"\r\n    821   if isinstance(tensor, ops.EagerTensor):\r\n--> 822     return tensor.numpy()\r\n    823   if not is_tensor(tensor):\r\n    824     return tensor\r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    940     \"\"\"\r\n    941     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n--> 942     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    943     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    944 \r\n\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n    908       return self._numpy_internal()\r\n    909     except core._NotOkStatusException as e:\r\n--> 910       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    911 \r\n    912   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: Socket closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1582484923.512283972\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\nSEARCH STACK OVERFLOW\r\n```", "I solve the problem.\r\n\r\nReplace\r\n\r\n```python\r\nd_loss_real = model.fit(x_train.astype(np.float32), np.ones((x_train.shape[0],1)))\r\n```\r\n\r\nwith\r\n\r\n```python\r\nd_loss_real = model.fit(x_train.astype(np.float32), np.ones((x_train.shape[0],1)).astype(np.float32))\r\n```\r\n\r\nIt seems is a bug. I think it should throw out `TypeError`.", "@chn-lee-yumi Yes agreed. The error doesn't look to be intuitive. ", "@chn-lee-yumi,\r\nI was able to run the code without any issues with TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f838683868b5e02729ba910248474a8d/36996.ipynb). Thanks!", "@amahendrakar\r\nHello, I have checked the code, it can run on TF v2.2 without any issues. The colab default TF 2.x version had changed to TF v2.2 too.", "@chn-lee-yumi,\r\nThank you for the update. Marking the issue as closed, as it is resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36996\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36996\">No</a>\n"]}, {"number": 36995, "title": "Added doc for all application models for decode_prediction() and preprocess_input()", "body": "Fixes #37021.\r\nAdded doc for the two functions since there are Issues being created for it.", "comments": ["@ashutosh1919 can you please resolve conflicts? Thanks!", "> @ashutosh1919 can you please resolve conflicts? Thanks!\r\n\r\n@gbaned , I have resolved conflicts.", "@gbaned , Please merge this one.", "It is going to be merged automatically once internal review and testing passes."]}, {"number": 36994, "title": "Added doc in inception_resnet_v2 for decode_prediction() and preprocess_input()", "body": "Fixes #37021.\r\nSince there is no doc till now.", "comments": ["Same comment about merging all of these in a single PR in the future", "> Same comment about merging all of these in a single PR in the future\r\n\r\nSure @mihaimaruseac."]}, {"number": 36993, "title": "Added doc in DenseNet for decode_prediction() and preprocess_input()", "body": "Fixes #37021.\r\nSince, there was no doc on how to use the functions.", "comments": ["PS: In the future, since these comments are the same across multiple PRs, please merge them all together in just one PR."]}, {"number": 36992, "title": "Added doc in MobileNet for decode_predictions() and preprocess_input()", "body": "Fixes #37021.\r\nAdded doc. Similar PR for mobilenet_v2 #36951. @tanzhenyu , @mihaimaruseac , Please review this one as well.", "comments": []}, {"number": 36991, "title": "How to get integer batch size in Keras model.fit()", "body": "I'm trying to use `model.fit()` on a `Sequential` model consisting of custom layers subclassing `tf.keras.layers.Layer`. Using `GradientTape` where I feed every batch in explicitly works fine (including in graph mode with `tf.function`). Trying to use the high-level Keras API for training,\r\n\r\n```py\r\nmodel.compile(loss=loss_fn, optimizer=\"adam\")\r\nmodel.fit(X_train, y_train)\r\n```\r\n\r\nI get a bunch of `ValueError: None values not supported.` for things like\r\n\r\n```py\r\ndef call(self, x):\r\n    ...\r\n    epsilon = tf.random.normal(x.shape)  # reparametrization trick\r\n    ...\r\n```\r\n\r\nsince `x.shape[0]` is `None`. So the question is, how do I get an integer batch size when using `model.fit()`? I tried\r\n\r\n```py\r\nmodel.compile(loss=loss_fn, optimizer=\"adam\")\r\nmodel.fit(\r\n    X_train, y_train, batch_size=64, steps_per_epoch=X_train.shape[0] // 64,\r\n)\r\n```\r\n\r\nbut that makes no difference. `x.shape[0]` remains `None` during graph creation.", "comments": ["Can you please try numpy.shape(X_train)[0]//64", "Please let me know if it works.", "@ghosalsattam Makes no difference.", "Can you provide me the full code?", "@ghosalsattam Sorry, the code should remain private for now. But the following stripped down example already exhibits the problem:\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nclass Foo(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, x):\r\n        return tf.random.normal(x.shape)\r\n\r\nmodel = tf.keras.Sequential([Foo()])\r\n\r\nmodel.compile()\r\n\r\nmodel.fit(tf.random.normal([10,5]), tf.random.normal([10]))\r\n```", "It is difficult to say something if I don't have any information of x. But from the error it seems that in the function, the value of x passed does not match the x you intend to pass.  May be it is an array of None.", "@janosh,\r\nI tried to run the above code snippet and am facing an error stating `    ValueError: Cannot convert a partially known TensorShape to a Tensor: (None, 5)`. You can find the gist of it [here](https://colab.sandbox.google.com/gist/saikumarchalla/641b725d245017106586ab46c03acdea/36991.ipynb).\r\n\r\nCould you please confirm if you are facing the same error? Thanks!", "@amahendrakar Yes, that's the error I'm getting as well.", "@janosh \r\nThen you can try using batch size =1\r\nI used the approach for a slightly different case and it worked there.\r\nYou can use the format of batch size used in predict().\r\nfor i in range(30,70):\r\n        print(i)\r\n        Pixels,img=detectCorner(path,i)\r\n        if(len(Pixels)==0):\r\n            continue\r\n        Roi=findClusters(Pixels,img)\r\n        print(Roi)\r\n        crop = np.empty((40,32,32,1),dtype=int)\r\n        for roi in range(len(Roi)):\r\n            for j in range(30,70):\r\n                 ll=Roi[roi][0]\r\n                 ur=Roi[roi][1]\r\n                 print(ll,ur)\r\n                 img1=pronounce(data[:,:,j].T)\r\n                 #print(ll[0],ur[0])\r\n                 crop1=img1[ur[0]:ll[0],ll[1]:ur[1]]\r\n                 plt.imshow(crop1,'gray')\r\n                #plt.show()\r\n                crop1=cv2.resize(crop1,interpolation=cv2.INTER_CUBIC,dsize=(32,32))\r\n                crop[j-30,]=np.expand_dims(crop1,axis=2)\r\n               #print(np.shape(crop[roi]))\r\n              #crop[roi]=np.expand_dims(crop[roi],axis=2)\r\n           a=classifier.predict_classes(crop)#batch size=1\r\n\r\n\r\nFor details:\r\nhttps://stackoverflow.com/questions/35289773/cannot-convert-a-partially-converted-tensor-in-tensorflow", "Hope this helps.", "Use tf.shape(x) instead of x.shape. \r\n\r\nx.shape is the static shape of x and evaluates to (None,5).\r\n\r\ntf.shape(x) on the other hand is the dynamic shape of x and is evaluated as the actual shape of x when training/predicting.", "@sixChar Thanks, I tried that as well but it's not really a solution. The errors in my case are thrown by some nested functions that really only need to know the size of the first dimension of `x`. If I pass in `tf.shape(x)[0]` instead of `x.shape[0]`, nothing changes. I get the same errors saying `None values not supported.` for things like `tf.zeros(batch_size)` where `batch_size` was passed in as `tf.shape(x)[0]`.  I suspect that it might work if I pass in the whole tensor and only call tf.shape in the nested functions themselves. But I'd prefer not to do that since those functions don't need to know anything except `tf.shape(x)[0]`.", "Could you give an example of the kind of code that still causes the same error when using tf.shape(x)[0]?", "@sixChar Oops, I take it all back. I tried producing a minimal example that errors with `tf.shape(x)`. Didn't manage though. So I went back to my actual code and tried to find out why I was still getting the error there. Turns out there was one place I had overlooked converting from `x.shape` to `tf.shape(x)`. To be fair, the graph code you get when debugging `model.fit(...)` is pretty near unreadable which is why I'd missed it. Sorry about the noise.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36991\">No</a>\n", "One final suggestion though: I think it would help if the [docs on `tf.shape`](https://www.tensorflow.org/api_docs/python/tf/shape?version=nightly) as well as the [Writing custom layers and models with Keras](https://www.tensorflow.org/guide/keras/custom_layers_and_models) guide had a note on this, i.e. that `tf.shape` should be used instead of `x.shape` when defining custom layers and models to avoid `None` errors.", "@janosh Would you like to update docs through PR? Thanks!", "@jvishnuvardhan PR submitted. Suggestions for improvement welcome.", "@Al-Badri179 Can you please open a new issue with a simple standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan ok, sorry for inconvenience"]}, {"number": 36990, "title": "Fix label_smoothing in multidimensional CategoricalCrossentropy.", "body": "When label smoothing in CategoricalCrossentropy is non-zero, it takes `tf.shape(y_true)[1]` as the number of classes. However, if the true values and predictions are multidimensional (for example when training a POS tagger where batch elements are sentences composed of words), a wrong value is taken and the training does not work.\r\n\r\nThis fix takes the _last_ dimension as the one containing classes.", "comments": ["@omalleyt12 Gentle bump on this one-line PR after 30 days", "@omalleyt12 @pavithrasv I hope you are doing fine even in these turbulent times!\r\n\r\nGentle bump on this one-line PR after another 30 days."]}, {"number": 36989, "title": "Added example of from_tensors and from_tensor_slices", "body": "Added very important examples which differentiates `from_tensors` and `from_tensor_slices` in Dataset. The example describes the property of `from_tensor_slices` to merge the input tensor. In case of `from_tensors`, it doesn't merge input tensor.", "comments": ["@ashutosh1919 Thank you for your interest in improving the docs!\r\n\r\nI'm confused about what you mean by \"merge the input tensor\". Both methods convert their input argument to a `Tensor`. The only difference is whether they produce the whole tensor, or slices of the tensor. In particular, `dataset.from_tensor_slices(x)` is functionally equivalent to `dataset.from_tensors(x).unbatch()`. Does that clarify how things work?", "> @ashutosh1919 Thank you for your interest in improving the docs!\r\n> \r\n> I'm confused about what you mean by \"merge the input tensor\". Both methods convert their input argument to a `Tensor`. The only difference is whether they produce the whole tensor, or slices of the tensor. In particular, `dataset.from_tensor_slices(x)` is functionally equivalent to `dataset.from_tensors(x).unbatch()`. Does that clarify how things work?\r\n\r\n@aaudiber , Thanks for the comment. Yes I understand that. Sorry for the language, I understand what it means but failed to describe it properly.", "@ashutosh1919 Can you share what behavior you originally expected, and how that was different from the actual behavior? Then we can figure out how to modify the doc to address the misunderstanding better", "> @ashutosh1919 Can you share what behavior you originally expected, and how that was different from the actual behavior? Then we can figure out how to modify the doc to address the misunderstanding better\r\n\r\nSure. Please take a look at [this answer](https://stackoverflow.com/a/54051300/8289194)'s second part. I observed this behavior that `from_tensor_slices` slices the tensor along it's first dimension. But  `from_tensors` doesn't do that. Since, there was no example given in doc to explain that behavior difference in both, I have raised this PR.", "@aaudiber , Thank you so much for clearing my doubts on this. I have fixed the comments and also changed the example as per your suggestions. I think now it makes more sense. Please review this one. Thanks again.", "@aaudiber , Changed examples and comments so that readers can differentiate both methods by looking at the results we get in two examples.", "@aaudiber , Removed section for `from_tensor_slices` and keeping other things as it is.", "@ashutosh1919 thank you, it is failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/27f15da0-a852-415d-9d83-a312ac980112/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests).\r\n\r\nPlease run the doctest locally as mentioned here in the [contributor guidelines](https://www.tensorflow.org/community/contribute/docs_ref).", "@gbaned , @mihaimaruseac and @aaudiber , Sorry for that. You need to approve it again. There was slight problem in the doc which was failing tests."]}, {"number": 36988, "title": "Rising a memory on every step", "body": "Hello. I used a MobileNet for Object detection and my model is training by following code:\r\n\r\n```py\r\n@tf.function\r\ndef train(model, dataset, epochs=50, lr=1e-5, checkpoints=None):\r\n  optimizer = tf.keras.optimizers.Adam(lr=lr)\r\n  for i in range(epochs):\r\n    dataset = dataset.shuffle(1000)\r\n    # training\r\n    for images, boxes, labels in tqdm(dataset, desc='Epoch {} of {}'.format(i + 1, epochs)):\r\n      with tf.GradientTape() as tape:\r\n        localization, classification = model(images, training=True)\r\n        smooth = losses.smooth_l1(boxes, localization)\r\n        focal  = losses.focal(labels, classification)\r\n        loss = smooth * 0.6 + focal * 0.4\r\n      gradients = tape.gradient(loss, model.trainable_variables)\r\n      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    if checkpoints:\r\n      model.save_weights(checkpoints)\r\n```\r\nBut my PC memory is rising on every step.\r\nCan you help me to fix it?", "comments": ["Maybe you are saving the weights after each epoch, that is rising the PC memory. \r\nBring the  \"model.save_weights(checkpoints)\" outside the for loop. ", "When I not use tf.function. It is working okay.\r\n```py\r\n# @tf.function\r\ndef train(model, dataset, epochs=50, lr=1e-5, checkpoints=None):\r\n  ...\r\n```", "@open-v, Can you provide Tensorflow version and complete code to analyze the issue. Thanks!", "My TensorFlow version is: 2.1.0.\r\n[Link for wheel](https://drive.google.com/file/d/1Ur6kvaY96P3dLjo-gxqykO8CUh4gEyy8/view?usp=sharing)\r\n[Code](https://github.com/open-v/tf-retinanet)", "@open-v, Can you provide the instructions to execute the code. Thanks!", "@open-v, Can you update for the above comment.", "Yes I updated, and I am using another way", "@open-v, The given [github repo](https://github.com/open-v/tf-retinanet) don't have clear instructions to execute the code. \r\n`Yes I updated, and I am using another way`\r\nJust to verify, did you find solution. \r\n\r\nThanks!", "@open-v, Can you update for the above comment. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36987, "title": "multi head multi loss model with GradientTape", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/GradientTape\r\n\r\n## Description of issue (what needs changing):\r\nI have model with two head and two loss. I want to optimize my model in the way that each loss propagate separately in it's head branch. \r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 36986, "title": "Could not import tensorflow after installing it through anaconda navigator.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: installed using Environments in Anaconda Navigator\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\r\n    self.showtraceback(running_compiled_code=True)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2043, in showtraceback\r\n    value, tb, tb_offset=tb_offset)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\r\n    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\nTypeError: can only concatenate str (not \"list\") to str\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\r\n    self.showtraceback(running_compiled_code=True)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2043, in showtraceback\r\n    value, tb, tb_offset=tb_offset)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1385, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in structured_traceback\r\n    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\nTypeError: can only concatenate str (not \"list\") to str\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\abida\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\abida\\Anaconda3\\folder\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\folder\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\folder\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n~\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\r\n   2039                         # in the engines. This should return a list of strings.\r\n-> 2040                         stb = value._render_traceback_()\r\n   2041                     except Exception:\r\n\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n~\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self, code_obj, result, async_)\r\n   3341             if result is not None:\r\n   3342                 result.error_in_exec = sys.exc_info()[1]\r\n-> 3343             self.showtraceback(running_compiled_code=True)\r\n   3344         else:\r\n   3345             outflag = False\r\n\r\n~\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\r\n   2041                     except Exception:\r\n   2042                         stb = self.InteractiveTB.structured_traceback(etype,\r\n-> 2043                                             value, tb, tb_offset=tb_offset)\r\n   2044 \r\n   2045                     self._showtraceback(etype, value, stb)\r\n\r\n~\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n   1383         self.tb = tb\r\n   1384         return FormattedTB.structured_traceback(\r\n-> 1385             self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n   1386 \r\n   1387 \r\n\r\n~\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n   1286             # Verbose modes need a full traceback\r\n   1287             return VerboseTB.structured_traceback(\r\n-> 1288                 self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n   1289             )\r\n   1290         elif mode == 'Minimal':\r\n\r\n~\\Anaconda3\\folder\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\r\n   1148         exception = self.get_parts_of_chained_exception(evalue)\r\n   1149         if exception:\r\n-> 1150             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\n   1151             etype, evalue, etb = exception\r\n   1152         else:\r\n\r\nTypeError: can only concatenate str (not \"list\") to str\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@thazmach \r\nLooks like  Tensorflow not installed correctly.\r\nBelow are the instructions to install Tensorflow 2.0 using PIP\r\n```\r\n#Install tensorflow using pip virtual env \r\n$pip install virtualenv\r\n$virtualenv tf_2.0.0   # tf_2.0.0 is virtual env name\r\n$source tf_2.0.0/bin/activate\r\ntf_2.0.0 $ pip install tensorflow==2.0.0\r\ntf_2.0.0 $ python\r\n>>import tensorflow as tf\r\n>>tf.__version__\r\n2.0.0\r\n```\r\nPlease, refer #36167 and see if it helps you. Thanks!", "Hey, this is all working in the command prompt. But, not in my Jupiter notebook. I can see TensorFlow version in python in command prompt but not on jypyter notebook", "Please, check the jupyter notebook and tensorflow are installed in the same environment. Thanks!", "> Hey, this is all working in the command prompt. But, not in my Jupiter notebook. I can see TensorFlow version in python in command prompt but not on jypyter notebook\r\n\r\nHave you managed to get this issue resolved? What steps you took to make it work?", "Hello! I have the same problem. I've traid to uninstall/install tensorflow, but it didn't work... Any guess? Thanks!", "@thazmach \r\n\r\nAny update on this issue please. Thanks!", "@thazmach @ravikyram \r\nI solved it installing a previous version (2.0)\r\n", "In this case, you need to update MSVC redistributable (install the latest one) and then you can use 2.1.\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36986\">No</a>\n"]}, {"number": 36985, "title": "Shape issues in keras.metrics.sparse_top_k_categorical_accuracy with multiple dimensions", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-24394-gc24d2f9 2.2.0-dev20200210\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n**Describe the expected behavior**\r\n**Standalone code to reproduce the issue** \r\nWhen there are multiple dimensions (e.g., for image data), the behavior of tf.metrics.sparse_top_k_categorical_accuracy and tf.metrics.top_k_categorical_accuracy differ from both each other and that of their categorical_crossentropy equivalents.\r\n\r\nIn particular, as of https://github.com/tensorflow/tensorflow/commit/ddca4b92b4eb4e187716b03f443362c904bdad7e (which was used to fix https://github.com/tensorflow/tensorflow/issues/33825), tf.metrics.sparse_top_k_categorical_accuracy flattens the extra dims (resulting in a different output shape compared to sparse_categorical_crossentropy), while tf.metrics.top_k_categorical_accuracy raises an error saying there must be only 2 dimensions. The docs don't say much about what should be expected.\r\n\r\nThe flattened shape also causes an error when passing weights to tf.keras.metrics.SparseTopKCategoricalAccuracy.\r\n\r\nSee minimal repro code here:\r\nhttps://colab.research.google.com/drive/1DR7SSoj8cpo0Y35swz-dyOX4DCazidPe", "comments": ["Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7a14269ffd5bc8b10e0284f817533809/35650.ipynb). Thanks!", "The issue is still reproducible on tf 2.7, [nightly](https://colab.research.google.com/gist/Saduf2019/441a7724c4586d488007fff3f2be86e8/untitled638.ipynb) version", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36985\">No</a>\n"]}, {"number": 36984, "title": "TensorFlow 2.x prevents me using two tf.data.Dataset in multiprocess", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n\ud83d\udc49\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n\ud83d\udc49Macos Catalina version 10.15.3 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n\ud83d\udc49No\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n\ud83d\udc49From binary, 2.0.0 and 2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n\ud83d\udc49Python 3.7.4\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n\ud83d\udc49No\r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\ud83d\udc49No\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\ud83d\udc49 In my multiprocess program, I create a Queue for process_communication, but when I use two different tf.data.Dataset in different Process my program gets stuck.\r\n\r\n**Describe the expected behavior**\r\n\ud83d\udc49 When I replace one of these tf.data.Dataset to a list, everything work ideally.  And this piece of code transfer from TF1.x which also works well.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```py\r\nimport os\r\nfrom multiprocessing import Process, Queue\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass Trainable(object):\r\n    def __init__(self):\r\n        self.queue = Queue()\r\n        self.valid_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])  # [1, 2, 3, 4, 5]\r\n        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n\r\n    def eval(self, q):\r\n        print('Process to write: %s' % os.getpid())\r\n        tmp = -1\r\n        for parsed_record in self.valid_handle:\r\n            print(parsed_record)\r\n            tmp = parsed_record\r\n        self.queue.put((q + 1, tmp))\r\n\r\n    def train(self):\r\n        process = None\r\n        print('Process to read: %s' % os.getpid())\r\n        for context in self.train_handle:\r\n            if process:\r\n                valid_detail = self.queue.get()\r\n                process = None\r\n                print(valid_detail)\r\n                if 8 in valid_detail:\r\n                    print('Early stopping')\r\n                    break\r\n\r\n            process = Process(target=self.eval, args=(context.numpy(),))\r\n            process.start()\r\n            if not self.queue.empty():\r\n                valid_detail = self.queue.get()\r\n                process = None\r\n                print(valid_detail)\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Trainable()\r\n    model.train()\r\n\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nHere is my question posted on StackOverflow : https://stackoverflow.com/questions/60354870/tensorflow-2-x-prevent-me-using-two-tf-data-dataset-in-multiprocess-why", "comments": ["@fuhailin \r\nI have tried on colab with TF version 2.1.0 beta and was able to reproduce the issue.However I think this was resolved recently in tf-nightly(`!pip install tf-nightly`). I ran it with tf-nightly without any issue. Please check the gist [here](https://colab.sandbox.google.com/gist/ravikyram/880272580be3b640c8cc599cbc771def/untitled666.ipynb). Thanks!", "Thanks, it seems this bug will be fixed in next stable version.", "I have tried to replace one of the data handle to a TFRecordDataset, the problem seems still existent in tf-nightly`(!pip install tf-nightly)` with multiprocess. Here is my another standalone code to reproduce the issue:\r\n```py\r\nimport os\r\nfrom multiprocessing import Process, Queue\r\n\r\nimport tensorflow as tf\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n# v1.12.1-25210-gcafd3318ed 2.2.0-dev20200219\r\n\r\nfsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\",\r\n                                         \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\r\n\r\nclass Trainable(object):\r\n\r\n    def __init__(self):\r\n        self.queue = Queue()\r\n        self.valid_handle = tf.data.TFRecordDataset(filenames=[fsns_test_file])\r\n        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n\r\n    def eval(self, q):\r\n        print('Process to write: %s' % os.getpid())\r\n        tmp = -1\r\n        for parsed_record in self.valid_handle:\r\n            print(parsed_record)\r\n            tmp = parsed_record\r\n        self.queue.put((q + 1, tmp))\r\n\r\n    def train(self):\r\n        process = None\r\n        print('Process to read: %s' % os.getpid())\r\n        for context in self.train_handle:\r\n            if process:\r\n                valid_detail = self.queue.get()\r\n                process = None\r\n                print(valid_detail)\r\n                if 1 in valid_detail:\r\n                    print('Early stopping')\r\n                    break\r\n\r\n            process = Process(target=self.eval, args=(context.numpy(),))\r\n            process.start()\r\n            if not self.queue.empty():\r\n                valid_detail = self.queue.get()\r\n                process = None\r\n                print(valid_detail)\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Trainable()\r\n    model.train()\r\n\r\n```", "I have tried on colab with TF nightly version with the recent code and was able to reproduce the issue.Please,find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d42bbf325dad48866e49ae0ddc4feb03/untitled669.ipynb). Thanks!", "TLDR: Using multiprocessing with TF is not guaranteed to work.\r\n\r\nTensorFlow runtime internally uses multi-threading and forking a multi-threaded program is generally not safe.  As per POSIX standard:\r\n\r\n> A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called. [THR] [Option Start]  Fork handlers may be established by means of the pthread_atfork() function in order to maintain application invariants across fork() calls.\r\n\r\nThe reason your program is hanging is because the single thread replicated in the child process attempts to (inside of TensorFlow runtime) coordinate with others threads.\r\n\r\nYou could \"fix\" your program as follows:\r\n\r\n```\r\n    def __init__(self):\r\n        self.queue = Queue()\r\n        options = tf.data.Options()\r\n        options.experimental_optimization.autotune = False\r\n        self.valid_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]).with_options(options)\r\n        self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n```\r\n\r\nbut there is no guarantee that as you start relying on other parts of TensorFlow runtime, this issue won't resurface.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36984\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36984\">No</a>\n", "> TLDR: Using multiprocessing with TF is not guaranteed to work.\r\n> \r\n> TensorFlow runtime internally uses multi-threading and forking a multi-threaded program is generally not safe. As per POSIX standard:\r\n> \r\n> > A process shall be created with a single thread. If a multi-threaded process calls fork(), the new process shall contain a replica of the calling thread and its entire address space, possibly including the states of mutexes and other resources. Consequently, to avoid errors, the child process may only execute async-signal-safe operations until such time as one of the exec functions is called. [THR] [Option Start]  Fork handlers may be established by means of the pthread_atfork() function in order to maintain application invariants across fork() calls.\r\n> \r\n> The reason your program is hanging is because the single thread replicated in the child process attempts to (inside of TensorFlow runtime) coordinate with others threads.\r\n> \r\n> You could \"fix\" your program as follows:\r\n> \r\n> ```\r\n>     def __init__(self):\r\n>         self.queue = Queue()\r\n>         options = tf.data.Options()\r\n>         options.experimental_optimization.autotune = False\r\n>         self.valid_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]).with_options(options)\r\n>         self.train_handle = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n> ```\r\n> \r\n> but there is no guarantee that as you start relying on other parts of TensorFlow runtime, this issue won't resurface.\r\n\r\nForking may not work, but importing `from multiprocessing import set_start_method, get_context` and starting with\r\n```\r\nif __name__ == '__main__':\r\n    set_start_method(\"spawn\", force=True)\r\n    with get_context(\"spawn\").Pool(1) as pool: \r\n        pool.starmap \r\n```\r\nworked for me. Sure, this may limit the design of the software a bit, but at least I'm not facing any memory issues after three cross-validation folds and can happily train a six-headed DenseNet beast until eternity."]}, {"number": 36982, "title": "Update code example to Python 3", "body": "Now that it is past January 1, 2020, the code example in the README should be in Python 3", "comments": []}, {"number": 36980, "title": "InternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg", "body": "when run bert tpu on colab get  error info  :  \r\nInternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg\r\n\r\nhere is [my code on colab ](https://colab.research.google.com/drive/1i9ZhGGv8KDv5-VISbwTdCwDqrhh-67pu#scrollTo=xBggqjKaHU9w) You can reproduce this bug through my code\r\n", "comments": ["Was able to reproduce the reported issue with Tf2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/4e0b537e9fb6e58c85f46c71b88e2258/untitled403.ipynb). Thanks!", "Same problem for me. Did anyone succeed in solving this issue? Shutting off eager mode? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36980\">No</a>\n", "Hi @bestpredicts and @simonsays1980,\r\n\r\nYou need to use \r\n`tf.config.experimental_connect_to_cluster(resolver)` API instead of\r\n`tf.config.experimental_connect_to_host(resolver.master())`\r\nSee https://www.tensorflow.org/guide/tpu"]}, {"number": 36979, "title": "AttributeError: 'Tensor' object has no attribute 'numpy'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Fedora 31\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: no\r\n- TensorFlow installed from (source or\r\nbinary): binary (anaconda)\r\n- TensorFlow version (use command below): 2.1 \r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: nvidia 1050 ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI can't call `numpy()` of `Tensor` arguments passed to the mapping function given to `tf.data.Dataset` even with `tf.executing_eagerly()` returning `True`:\r\n```\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n**Describe the expected behavior**\r\nI'd like to be able to access the `numpy()` property of arguments passed to a mapping function passed to `tf.data.Dataset.map`\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef transformer(x):\r\n    x.numpy()\r\n    return x\r\n\r\ndataset = tf.data.Dataset.from_tensors([0,1,2])\r\ndataset.map(transformer)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@nscotto , You can convert tensor into numpy array using `tensor.numpy()`, But you can't do the same in case of `MapDataset`. In the example code you have given,  the `x` in the transformer function is `MapDataset` and not `Tensor`.\r\nI have seen similar issue [#30035](https://github.com/tensorflow/tensorflow/issues/30035) and also you can refer to [this function](https://www.tensorflow.org/api_docs/python/tf/numpy_function?version=nightly) which I think will solve your problem. \r\nAlso, if you have any doubts regarding `from_tensors` and `from_tensor_slices`, then please review the official docs [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#from_tensor_slices). Also, take a look at these example which I recently Added through my PR #36989.\r\n", "@nscotto \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram \r\nYes, ashutosh1919 is right, it's because `Dataset.map` passes `Tensor` instead of `EagerTensor`, thus I need to wrap my code inside a `tf.py_function` if I need to convert to numpy (and obviously reducing the performances).\r\nAs a suggestion I think the documentation could make this information more visible.", "@ashutosh1919  @ravikyram @nscotto \r\nI am trying to address the problem by making a numpy() function in the Tensor class. But I am facing a problem. Can anyone tell me from which attribute of Tensor class I can find the data.\r\n", "@ashutosh1919 I went through all the codes. From the codes it seems that internally the map() function iterates through all the tensors(for each function call). So what x sees from inside the function in map() is the single tensor passed to it at that very time. So using .numpy() inside the function called through map() is pointless. ", "@nscotto , @ravikyram and @ghosalsattam , I have raised PR #37853 to add an example on `tf.numpy_function` and how it is different from `tf.py_function`. Now, 2nd and 3rd point [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#map) about `py_function` and `numpy_function` clearly explains the difference. Note that you won't be able to see 3rd point until my PR is merged. See PR changes to see example. \r\nPlease give your suggestions if anything else is required."]}, {"number": 36978, "title": "Converting unsupported operation: IdentityN", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\nGoogle Colab\r\ntensorflow version:'1.15.0'\r\n\r\n**Provide the text output from tflite_convert**\r\nFile not created\r\n```\r\n# Copy and paste here\r\n```\r\nConverterError: See console for info.\r\n2020-02-22 12:33:05.427036: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN\r\n2020-02-22 12:33:05.427232: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 15 operators, 18 arrays (0 quantized)\r\n2020-02-22 12:33:05.427378: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 15 operators, 18 arrays (0 quantized)\r\n2020-02-22 12:33:05.427577: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)\r\n2020-02-22 12:33:05.427619: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 2 operators, 5 arrays (0 quantized)\r\n2020-02-22 12:33:05.427646: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 2 operators, 5 arrays (0 quantized)\r\n2020-02-22 12:33:05.427684: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 64 bytes, theoretical optimal value: 64 bytes.\r\n2020-02-22 12:33:05.427712: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 3 ops, equivalently 1 MACs\r\n2020-02-22 12:33:05.427726: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 2\r\n2020-02-22 12:33:05.427933: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED. Here is a list of operators for which you will need custom implementations: IdentityN.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1clPmDDCmcO7my2spTm2H_cj1PRW6YpYk#scrollTo=7x-p-H9A2EB7\r\n\r\nx = [-1,0,1,2,3,4]\r\ny = [-3,-1,1,3,5,7]\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    [tf.keras.layers.Dense(units=1,input_shape=[1])]\r\n)\r\nmodel.compile(optimizer='sgd',loss = 'mean_squared_error')\r\nmodel.fit(x,y,epochs=100)\r\n\r\nimport pathlib\r\n#export the Saved Model\r\n\r\nexport_dir = '/content/model'\r\ntf.saved_model.save(model,export_dir)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\r\ntflite_model = converter.convert()\r\n\r\n#Save the model\r\n\r\ntflite_model_file = pathlib.Path('foo.tflite')\r\ntflite_model_file.write_bytes(tflite_model)\r\n\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["The code worked with  tensorflow 2.0 on colab"]}, {"number": 36977, "title": "improve API doc page: tf.data.Dataset", "body": "-  show list style for `tf.data.Dataset.list_files` example \r\n-  change `NOTE:` to `Note:` to keep a consistent style in API doc", "comments": ["Thanks for your PR, I think this improvement is not necessary, the `NOTE` word style existed in https://github.com/tensorflow/tensorflow/blob/5ad917782a5bc3895aa92292fd4447e57eda2560/tensorflow/python/data/ops/iterator_ops.py#L42\r\nand \r\nhttps://github.com/tensorflow/tensorflow/blob/5ad917782a5bc3895aa92292fd4447e57eda2560/tensorflow/python/data/ops/optional_ops.py#L111\r\nare already in a consistent style ", "@dothinking thank you for attempting to improve tf.data API documentation. Having said that, I agree with @fuhailin that this change is not necessary.\r\n\r\nI quickly searched through all of tensorflow/python/data and found 9 occurrences of \"Note\" and 16 occurrences of \"NOTE\". So there does not seem to be a precedent and I am also not aware of official style guide for TensorFlow that would state to use \"Note\". If we wanted to be consistent, I'd prefer \"NOTE\".", "> TensorFlow that would state to use \"Note\"\r\n\r\nDevsite highlights `Note:` with a special format. Sadly it does not recognize `NOTE:`.  See https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly for example and the highlighting that `Note:` causes.", "Thanks @yashk2810, if that's the case I agree that this PR makes sense.", "Thank @fuhailin, @jsimsa  for the comments, it's OK to use `NOTE` through the doc string. But as explained by @yashk2810 , the `NOTE` style could not be rendered properly in the API documentation page.\r\n\r\nOf course, it'll be better if both `Note` and `NOTE` are able to be accommodated.", "Thank @yashk2810 for your explanation, that's exactly what I meant."]}, {"number": 36976, "title": "Failed to load the native Tensorflow runtime.", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 (x64), i7 - 7th Gen\r\n- TensorFlow installed from (source or binary): installed via pip\r\n- TensorFlow version: 2.1.0 - tensorflow-gpu\r\n- Python version: 3.7.x\r\n- CUDA/cuDNN version: tried with CUDA 10.1, 9.0, 9.2 with respective cuDNNs\r\n- GPU model and memory: GTX 1050ti - 4gb\r\n\r\n\r\n\r\n**Unable to import the tensorflow library after succesfully installing it.**\r\n\r\n1. Installed tensorflow-gpu==2.1.0\r\n2. Installed CUDA and cuDNN \r\n3. Fails to import tensorflow.\r\n\r\n\r\n**Error Stack Trace:**\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["I think this may be a duplicate of https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nWhat Visual Studio are you running?", "@leoAshu Make sure to download the latest microsoft visual c++ redistributable f[rom here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website](https://www.tensorflow.org/install/gpu#windows_setup).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #[36167](https://github.com/tensorflow/tensorflow/issues/36167) and see if it helps you. Thanks!", "Thanks @Saduf2019. Updating the VIsual Studio helped.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36976\">No</a>\n"]}, {"number": 36975, "title": "Hi, I don't know what the problem is... plz help me to find out the bugs", "body": "python: 3.7.3\r\nwindows 10\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-946f26bfac6d>\", line 4, in <module>\r\n    print(tf.__version__)\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \uc9c0\uc815\ub41c \ubaa8\ub4c8\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 8, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-946f26bfac6d>\", line 4, in <module>\r\n    print(tf.__version__)\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \uc9c0\uc815\ub41c \ubaa8\ub4c8\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n", "comments": ["I think this may be a duplicate of https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nWhat Visual Studio are you running?", "@aeddung,\r\nAny updates regarding this issue? Thanks!", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36975\">No</a>\n"]}, {"number": 36974, "title": "CUDA broken on docker image tensorflow/tensorflow:devel-gpu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker Hub\r\n- TensorFlow version: [tensorflow/tensorflow:latest-devel-gpu](https://hub.docker.com/layers/tensorflow/tensorflow/latest-devel-gpu/images/sha256-607098dc1fe31c990b2c54c37bd5c81099cad165232cb0489c35e51689de27c7?context=explore)\r\n- Python version: python3\r\n- Installed using virtualenv? pip? conda?: docker \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCUDA is broken on [tensorflow/tensorflow:devel-gpu](https://hub.docker.com/layers/tensorflow/tensorflow/latest-devel-gpu/images/sha256-607098dc1fe31c990b2c54c37bd5c81099cad165232cb0489c35e51689de27c7?context=explore) image. TensorFlow could not recognize the GPUs.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n### Host Server (Debian GNU/Linux 9)\r\n\r\nOn the host server, CUDA is on 10.1. Tensorflow can detect 8 GPUs.\r\n\r\nRun `nvidia-smi` and it shows \r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nVerify that TensorFlow can see the 8 GPUs.\r\n```\r\n$ pip install tensorflow \r\n$ python -c 'import tensorflow as tf; print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))'\r\n\r\n('Num GPUs Available: ', 8)\r\n```\r\n\r\n\r\n\r\n### Inside Container (tensorflow/tensorflow:devel-gpu)\r\n\r\nThen, start the container.\r\n```\r\n$ docker run --gpus=all -it tensorflow/tensorflow:latest-devel-gpu-py3\r\n```\r\n\r\nIn the container, run\r\n```\r\nroot@5f3ceeb2248b:/# nvidia-smi\r\nSat Feb 22 02:06:17 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: ERR!     |\r\n|-------------------------------+----------------------+----------------------+\r\n```\r\nNotice that the CUDA version was **ERR!**. It may indicate that the CUDA version is broken. \r\nRun \r\n```\r\n$ pip install tensorflow \r\n$ python -c 'import tensorflow as tf; print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))'\r\n\r\n2020-02-22 02:11:29.788170: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2020-02-22 02:11:29.788245: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.87.1\r\nNum GPUs Available:  0\r\n```\r\n\r\nTensorFlow could not see the GPUs.\r\n\r\n\r\n**Any other info / logs**\r\n\r\n\r\n", "comments": ["Thanks for your report. I investigated this and replicated part of it. You should be able to use CUDA correctly by removing the \"stubs\" entry from LD_LIBRARY_PATH. See [here for details](https://github.com/Kaggle/docker-python/issues/361#issuecomment-448093930) (follow the link chain starting from \"should not\" to see why our dockerfiles have this. I'm not sure if this is still necessary).\r\n\r\nHowever, on my machine, `nvidia-smi` still finds my graphics card, even though CUDA reports ERR. Can you try nvidia-smi with the `tensorflow/tensorflow:latest` and `nvidia/cuda:10.1-runtime-ubuntu16.04` images?", "@angerson Thanks for the investigation! \r\n\r\nYes. Once I take away the `stubs` from LD_LIBRARY_PATH, the problem is gone. \r\n```\r\nroot@f1a39ae5bce1:/# echo $LD_LIBRARY_PATH\r\n/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nroot@f1a39ae5bce1:/# export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nroot@f1a39ae5bce1:/# nvidia-smi\r\nTue Feb 25 02:07:07 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nBoth nvidia-smi and TF work properly on images you mentioned:\r\n- `tensorflow/tensorflow:latest-gpu`\r\n- `nvidia/cuda:10.1-runtime-ubuntu16.04`\r\n\r\nI tried to find the exact command that produces this ERR result. It turns out to be [this command](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile#L95) in the [devel-gpu.Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile). I don't understand how it works. But, if it's no longer necessary, removing it should fix our problem. \r\n", "I was about to open a new issue, and then GitHub suggested this current issue (36974). Unfortunately, I didn't find this current issue (36974) when I was struggling to resolve the problem for myself.\r\n\r\nThe source of the problem is that `LD_LIBRARY_PATH` includes `/usr/local/cuda/lib64/stubs`. That is set on [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile#L82) in `devel-gpu.Dockerfile`. Apparently, the problem is **not** actually caused by [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile#L95), as has been suggested earlier in this thread.\r\n\r\nAs already mentioned, a simple solution is to remove the stubs path, which can be achieved like this: `LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH//\\/usr\\/local\\/cuda\\/lib64\\/stubs:}\"`\r\n\r\nI understand that `/usr/local/cuda/lib64/stubs` is used to enable the TensorFlow wheel to be built in the absence of a real CUDA driver (and then spat out for use elsewhere), and I assume that the `devel-gpu` container is used for this purpose. However, this makes it harder and more confusing to use the `devel-gpu` container for development, in which case we want to actually use TensorFlow inside the container with real underlying CUDA driver functionality.\r\n\r\nApparently, there are preferable ways to allow the build to successfully complete in the absence of a real CUDA driver, and these ways do not require the presence of `/usr/local/cuda/lib64/stubs` in `LD_LIBRARY_PATH`. Four different approaches are discussed in nvidia-docker [issue 775](https://github.com/NVIDIA/nvidia-docker/issues/775#issuecomment-400178344).\r\n\r\nHere are another couple of relevant links:\r\n  * nvidia-container-runtime [issue 37](https://github.com/NVIDIA/nvidia-container-runtime/issues/37)\r\n  * rapidsai [issue 328](https://github.com/rapidsai/cudf/issues/328)", "I hit that problem today. I made a PR that move the stub to the end. This fix this problem: https://github.com/tensorflow/tensorflow/pull/44732", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36974\">No</a>\n", "This has been fixed here https://github.com/tensorflow/tensorflow/pull/46190 , will be closing this request. Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36974\">No</a>\n"]}, {"number": 36973, "title": "Can't find docs on SSL support for distributed training", "body": "**Describe the current behavior**\r\n\r\nI haven't be able to find documentation on if SSL is used during distributed training with `tf.distribute.Strategy` with gRPC.\r\n\r\n**Describe the expected behavior**\r\n\r\nI should be able to easily find this information in the documentation, and if it's supported, then I should easily be able to turn SSL on/off in distributed training for when I prefer security vs performance.", "comments": ["@haoyuz @dubey can one you comment on this? both for the collective ops used in distributed training, but also other types of communication in the runtime in TF2. ", "We [do not support](https://github.com/tensorflow/tensorflow/blob/1381fc8e15e22402417b98e3881dfd409998daea/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L486) SSL credentials."]}, {"number": 36972, "title": "Tensorflow Lite Hexagon delegate support for QCOM XR1 - soc_id: 371", "body": "QCOM XR1 (with soc_id:371) is in the same family as Snapdragon 710, Snapdragon 710 is in the list of the supported Socs:\r\n\r\nCurrently most Qualcomm SoCs are supported, including:\r\n\r\nSnapdragon 835 (682 DSP)\r\nSnapdragon 660/820/821 (680 DSP)\r\nSnapdragon 710/845 (685 DSP)\r\nSnapdragon 8150/855 (690 DSP)\r\n\r\nwe were wondering if XR1 could be added to this list as well, based on my understanding, the only change is to add the soc_id:371 to the supported list. We would be happy to help test/verify it if necessary. DSP acceleration is very important to our use cases, your help is greatly appreciated.\r\n\r\nthanks for your support,\r\n-richard", "comments": ["Hi Richard,\r\nCan you share more details on the SoC, i can't find reference on the details about the hexagon version used on this SoC.\r\nSadly, i don't have one of them to check and verify.\r\nThanks", "Hi Karim,\r\n\r\nQCOM XR1 (SXR1130) has the same DSP as Snapdragon 710/845, which is 685 DSP.\r\nI can help test and verify.\r\n\r\nthanks,\r\n-richard\r\n\r\n", "Hi Richard,\r\nThat should mean it is a v65. I will enable soc_id:371 to v65/CDSP.\r\nWhen the libhexagon_interface version is updated it should include this change.\r\nWill update the issue here too when it is pushed \r\n\r\nThanks", "Hi Karim,\r\n\r\nYes, it's v65. thank you so much for the quick support.\r\n\r\n-richard", "Hi Richard,\r\n\r\nIf you're building from bazel then you can try master branch now, it should be available.\r\n\r\nPlease try it and let me know if it works or not or if you have any issues.\r\n\r\nThanks", "Hi Karim, \r\n\r\njust verified it's working now, will do more testing next. \r\nthanks again.\r\n\r\n-richard"]}, {"number": 36971, "title": "Dockerfiles to python 3", "body": "This PR removes the Python 2 support for docker images. \r\n\r\nBuild output can be seen here\r\n- nightly: https://gist.github.com/liamwazherealso/6c5f4368120168c461d3ad3eef380731", "comments": ["Awesome. https://gist.github.com/liamwazherealso/01932edd692d15fb1bd538046eb70d0b added the tags back in. ", "Thanks for the fix. Once my local verification finishes, I'll approve and start the merge."]}, {"number": 36970, "title": "TensorFlow supports multiple threads/streams on one GPU for training?", "body": "UPDATE:\r\n\r\nI found the source code of GPUDevice, it hard-coded max streams to 1, may I know the know reason? \r\n\r\nGPUDevice(const SessionOptions& options, const string& name,\r\n            Bytes memory_limit, const DeviceLocality& locality,\r\n            TfGpuId tf_gpu_id, const string& physical_device_desc,\r\n            Allocator* gpu_allocator, Allocator* cpu_allocator)\r\n      : BaseGPUDevice(options, name, memory_limit, locality, tf_gpu_id,\r\n                      physical_device_desc, gpu_allocator, cpu_allocator,\r\n                      false /* sync every op */, **1 /* max_streams */**) {\r\n    if (options.config.has_gpu_options()) {\r\n      force_gpu_compatible_ =\r\n          options.config.gpu_options().force_gpu_compatible();\r\n    }\r\n\r\n\r\n=================================================\r\nI am wondering whether TensorFlow(1.x version) supports multi-thread or multi-stream on a single GPU. If not, I am curious the underlying reasons, TF did this on some purposes or some libs like CUDA prevents TF from providing or some other reasons?\r\n\r\nI tried to run multiple training ops in TF, i.e. sees.run([train_op1, train_op2],feed_dict={...}), I used the TF timeline to profile each iteration. However, TF timeline always showed that two train ops run sequentially (although timeline is not accurate[1], the wall time of each op suggests sequential running). I also looked at some source code of TF, it looks like the each op are computed by in device->ComputeAsync() or device->Compute(), and the GPU is blocked when computing an op. If I am correct, one GPU can only run a single op each time, which may lower GPU utilization.\r\n\r\nAny comments are welcome.\r\n\r\n1.https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-244251867", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks!"]}]