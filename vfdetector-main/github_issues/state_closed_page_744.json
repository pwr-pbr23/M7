[{"number": 31249, "title": "tensorflow + numpy compatibility?", "body": "\r\n```console\r\n\u279c  ~ pip show tensorflow\r\npipName: tensorflow\r\nVersion: 1.14.0\r\n\u279c  ~ pip show numpy\r\nName: numpy\r\nVersion: 1.17.0\r\n```\r\n\r\n\r\n```console\r\n>>> import tensorflow as tf\r\n~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n~/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n\r\n```", "comments": ["Do you have multiple versions of numpy installed in your system?\r\n```pip show numpy```\r\nCan you uninstall all numpy versions and later install numpy==1.16.4?\r\n```pip uninstall numpy``` (till you uninstall all versions)\r\n``` pip install numpy==1.16.4```\r\n", "@ymodak \r\n\r\nI manually upgraded **numpy**:\r\n\r\n```console\r\npip install -U numpy --user\r\n```\r\nto **~/.local/lib/python3.7/site-packages**\r\n\r\nAnd, yes, I happened to notice I've got **python3-numpy** installed as well, from Ubuntu repository. \r\n\r\nDid tensorflow find **python3-nump** from repository and had it installed automatically? I specified PYTHONPATH to **~/.lcoal/lib/python3/site-packages** already.\r\n\r\nCheers\r\n\r\n", "@ymodak :\r\nSame issue for me after fresh python and tensorflow installation (tensorflow 1.14.0 and numpy 1.17.0) from today morning.\r\nhad only one numpy installation. but after uninstallation of 1.17.0 and installation of 1.16.4 the Future Warnings are gone. Thank you!", "@jiapei100  Can you please let us know if you are happy to close if no issue persists. Thanks!", "For me the warning got resolved after doing the below 2:\r\n1) >pip install scikit-learn\r\n2) >pip install tensorflow-datasets", "@ymodak \r\nSame issue happened to me with (tensorflow  1.9.0 and numpy 1.17.0) \r\n after uninstall of 1.17.0 and installation of 1.16.4 the Future Warnings are gone. Thank you so much!\r\n\r\nCan you help me for solve another issue?\r\nafter  i load model ,when i predicted this error is occur (Tensor Tensor(\"dense_2/Softmax:0\", shape=(?, 2), dtype=float32) is not an element of this graph) .\r\nWhat do you think I should do...", "@fatemehtorki\n\nYou should open a new issue for that error since it is unrelated to this error ", "Please can you advise, it it the intention to update Tensorflow 1.x so that these warnings won't occur in numpy 1.17 and later? Or will we always have to stick at 1.16.4?", "See #30559 Next release will have the issue solved", "@fatemehtorki \r\nYou may ask in stackoverflow for that", "hello i have the same issue as jiapi i have the same versions of numpy and tensor flow please tell its solution", "@ymodak It's helpful, thank you!", "I'm a bit new @ymodak @jiapei100.\r\nI have \r\n`numpy 1.16.4\r\ntensorflow 1.14.0\r\ntensorflow-gpu 1.14.0\r\nkeras 2.2.4\r\nkeras -Applications 1.0.8\r\nKeras-Preprocessing 1.1.0`\r\nI am trying CNN and my jupyter is crashing on executing.\r\n`bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train)`\r\n\r\nPlease help I think its a compatibility issue. \r\n\r\nFull code [link](https://github.com/imamun93/animal-image-classifications/blob/master/final_notebook.ipynb).\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/29543170/63399741-1b0dcc80-c3ef-11e9-909b-898e456d4e13.png)\r\n", "> I'm a bit new @ymodak @jiapei100.\r\n> I have\r\n> `numpy 1.16.4 tensorflow 1.14.0 tensorflow-gpu 1.14.0 keras 2.2.4 keras -Applications 1.0.8 Keras-Preprocessing 1.1.0`\r\n> I am trying CNN and my jupyter is crashing on executing.\r\n> `bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train)`\r\n> \r\n> Please help I think its a compatibility issue.\r\n> \r\n> Full code [link](https://github.com/imamun93/animal-image-classifications/blob/master/final_notebook.ipynb).\r\n> \r\n> ![image](https://user-images.githubusercontent.com/29543170/63399741-1b0dcc80-c3ef-11e9-909b-898e456d4e13.png)\r\n\r\nYour Tensorflow is not properly installed. Try installing it again.", "I had the same problem in my linux laptop with tensorflow in python3 v3.6\r\nActually, you just need to change some lines in 2 files :\r\n- 1\r\n\r\n`~/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py\r\n\r\nnow change this code : (line 516)\r\n```\r\n\r\n_np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n_np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n_np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n_np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n_np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n\r\n\r\n# _np_bfloat16 is defined by a module import.\r\n\r\n# Custom struct dtype for directly-fed ResourceHandles of supported type(s).\r\nnp_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n```\r\nby this code :\r\n```\r\n\r\n_np_qint8 = np.dtype([(\"qint8\", np.int8, (1,))])\r\n_np_quint8 = np.dtype([(\"quint8\", np.uint8, (1,))])\r\n_np_qint16 = np.dtype([(\"qint16\", np.int16, (1,))])\r\n_np_quint16 = np.dtype([(\"quint16\", np.uint16, (1,))])\r\n_np_qint32 = np.dtype([(\"qint32\", np.int32, (1,))])\r\n\r\n# _np_bfloat16 is defined by a module import.\r\n\r\n# Custom struct dtype for directly-fed ResourceHandles of supported type(s).\r\nnp_resource = np.dtype([(\"resource\", np.ubyte, (1,))])\r\n```\r\nyou have to do the same on this file :\r\n\r\n`~/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py`\r\n\r\n\r\nwubbalubadubdub", "@SamiEzz \r\nIs this dependent on a particular version of numpy? I am currently using 1.16.4, but 1.17.0 should be fine with TF2.0.0 right?", "> @SamiEzz\r\n> Is this dependent on a particular version of numpy? I am currently using 1.16.4, but 1.17.0 should be fine with TF2.0.0 right?\r\n\r\nI had this issue with the latest version of numpy ('1.17.3')", "@mattc-eostar and @SamiEzz, Please use numpy 1.16.5 version. Thanks!", "Thought to mention a related issue #30120: \r\n\r\nThe use of the numpy (1.17.3) together and latest tf-nightly,  causes the error of discussion in #30120  \r\n\r\n- **SOLUTION** :  install previous version of numpy    1.16.4\r\n\r\n- tensorlfow version : 2.1.0-dev20191023 \r\n- numpy:  1.17.3 (install the previous version 1.16.4)\r\n-  python: 3.7\r\n", "I was facing the same issue. Downgrading numpy to version 1.16.4 solved the problem. Thanks @ymodak.", "Got the same issue with numpy==1.18.0. Downgrading to numpy 1.16.4 solved the issue for me:\r\n\r\n`pip3 uninstall numpy`\r\n`pip3 install numpy==1.16.4 `", "For reference, the numpy change which causes this warning in tensorflow is https://github.com/numpy/numpy/pull/13326\r\n\r\nThe warning is harmless to end users, and there is no need to downgrade. Aside from emitting a warning, the behavior has not changed.\r\n\r\nTensorflow fixed this in 3402d7118460857cf484f57338d14d9113597d15 (v1.15.0)", "> Got the same issue with numpy==1.18.0. Downgrading to numpy 1.16.4 solved the issue for me:\r\n> \r\n> `pip3 uninstall numpy`\r\n> `pip3 install numpy==1.16.4 `\r\n\r\nthis solved for me :)", "Locking as issue has been already solved. If there are new instances of issue and no solution here helps, please open a new issue, filling in issue template and documenting why these steps don't work."]}, {"number": 31248, "title": "WALSMatrixFactorization: Input matrix is not invertible. node cond/MatrixSolve", "body": "**System information**\r\n- Custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device: Noe\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.13.0-rc2-0-gc865ec5621 / 1.13.0-rc2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nFactorizing a small 4x4 (dense) matrix with WALSMatrixFactorizaion results in the following error:\r\n\r\n> InvalidArgumentError (see above for traceback): Input matrix is not invertible.\r\n> \t [[node cond/MatrixSolve (defined at /home/milad/PycharmProjects/wals_rec/temp.py:55) ]] \r\n\r\n**Describe the expected behavior**\r\nFactorize the matrix and evaluate without the error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n\timport numpy as np\r\n\timport tensorflow as tf\r\n\tfrom tensorflow.contrib.factorization import WALSMatrixFactorization\r\n\timport shutil\r\n\r\n\r\n\tdef make_input_fn(args):\r\n\r\n\t\tdef to_sparse(values):\r\n\t\t\tindices = np.array([[0], [1], [2], [3]], dtype=np.int64)\r\n\t\t\tsp_tensor = tf.SparseTensor(indices=indices, values=values, dense_shape=values.shape)\r\n\t\t\treturn sp_tensor\r\n\r\n\t\tdef _input_fn():\r\n\t\t\trows_dataset = tf.data.Dataset.from_tensor_slices(args['matrix_values']). \\\r\n\t\t\t\tmap(map_func=to_sparse). \\\r\n\t\t\t\trepeat(count=None). \\\r\n\t\t\t\tbatch(batch_size=args['batch_size'])\r\n\r\n\t\t\tcols_dataset = tf.data.Dataset.from_tensor_slices(np.transpose(args['matrix_values'])). \\\r\n\t\t\t\tmap(map_func=to_sparse). \\\r\n\t\t\t\trepeat(count=None). \\\r\n\t\t\t\tbatch(batch_size=4)\r\n\r\n\t\t\tfeatures = {\r\n\t\t\t\tWALSMatrixFactorization.INPUT_ROWS: rows_dataset.make_one_shot_iterator().get_next(),\r\n\t\t\t\tWALSMatrixFactorization.INPUT_COLS: cols_dataset.make_one_shot_iterator().get_next(),\r\n\t\t\t\tWALSMatrixFactorization.PROJECT_ROW: tf.constant(True, name=\"PROJECT_ROW\")\r\n\t\t\t}\r\n\t\t\treturn features, None\r\n\r\n\t\treturn _input_fn\r\n\r\n\r\n\tdef train_and_evaluate(args):\r\n\r\n\t\tdef experiment_fn(output_dir):\r\n\t\t\treturn tf.contrib.learn.Experiment(\r\n\t\t\t\ttf.contrib.factorization.WALSMatrixFactorization(\r\n\t\t\t\t\tnum_rows=args[\"nrows\"],\r\n\t\t\t\t\tnum_cols=args[\"ncols\"],\r\n\t\t\t\t\trow_weights=None,\r\n\t\t\t\t\tcol_weights=None,\r\n\t\t\t\t\tregularization_coeff=0,\r\n\t\t\t\t\tembedding_dimension=args[\"n_embeds\"],\r\n\t\t\t\t\tmodel_dir=output_dir),\r\n\t\t\t\ttrain_input_fn=make_input_fn(args),\r\n\t\t\t\teval_input_fn=make_input_fn(args),\r\n\t\t\t\ttrain_steps=10,\r\n\t\t\t\teval_steps=1,\r\n\t\t\t\t# Error doesn't happen if min_eval_frequency > train_steps\r\n\t\t\t\tmin_eval_frequency=5)\r\n\r\n\t\tfrom tensorflow.contrib.learn.python.learn import learn_runner\r\n\t\tlearn_runner.run(experiment_fn=experiment_fn, output_dir=args[\"output_dir\"])\r\n\r\n\r\n\tif __name__ == '__main__':\r\n\r\n\t\tK = 4\r\n\t\targs = {\r\n\t\t\t'ncols': K,\r\n\t\t\t'nrows': K,\r\n\t\t\t'output_dir': 'wals_temp',\r\n\t\t\t'n_embeds': 2,\r\n\t\t\t'n_steps': 5,\r\n\t\t\t'batch_size': K,\r\n\t\t\t'matrix_values': np.array([[1, 2, 3, 4],\r\n\t\t\t\t\t\t\t\t\t   [2, 4, 8, 12],\r\n\t\t\t\t\t\t\t\t\t   [3, 6, 7, 13],\r\n\t\t\t\t\t\t\t\t\t   [3, 5, 7, 9]], dtype=np.float32)\r\n\r\n\t\t}\r\n\r\n\t\tshutil.rmtree(args['output_dir'], ignore_errors=True)\r\n\t\ttrain_and_evaluate(args)\r\n\r\n**Other info / logs**\r\nThe error happens on [this line](https://github.com/tensorflow/tensorflow/blob/27d5e277dd6b9a6c3a331ae0dfd51361607d6b9b/tensorflow/contrib/factorization/python/ops/factorization_ops.py#L919). The cause seems to be that the (column) Gramian suddenly becomes zero during evaluation and therefore the lhs of the normal equation is not invertible (training steps run fine). I confirmed this by running it step by step and using tfdbg.\r\n", "comments": ["Issue replicating with TF version-1.13,please find the gist of [collab](https://colab.research.google.com/drive/12h3Ud8_NI7i2KrSq7fhIre7memfsoriP) .Thanks!", "Thanks for getting back. Do you mean you were not able to replicate the problem with TF 1.13?\r\n\r\nHere is (part of) what I get when I run the colab notebook you created.\r\n\r\n> InvalidArgumentError: Input matrix is not invertible.\r\n> \t [[{{node cond/MatrixSolve}}]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n>    1368           pass\r\n>    1369       message = error_interpolation.interpolate(message, self._graph)\r\n> -> 1370       raise type(e)(node_def, op, message)\r\n>    1371 \r\n>    1372   def _extend_graph(self):\r\n> \r\n> InvalidArgumentError: Input matrix is not invertible.\r\n> \t [[node cond/MatrixSolve (defined at <ipython-input-1-81cc9fbfe66d>:49) ]]", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31248\">No</a>\n"]}, {"number": 31247, "title": "Branch r2.0 Failed Source Build on macOS Mojave", "body": "**System information**\r\n- OS Platform and Distribution: macOS Mojave, 10.14.5 (18F203), (15-inch 2018)\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r2.0\r\n- Python version: Here's the result for `python` in terminal\r\n```\r\n  Python 3.7.3 (default, Mar 27 2019, 16:54:48) \r\n  [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\n  Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): The result for `gcc -v`\r\n```\r\nConfigured with: --prefix=/Applications/Xcode-beta.app/Contents/Developer/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1\r\nApple clang version 11.0.0 (clang-1100.0.31.5)\r\nTarget: x86_64-apple-darwin18.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nBuild failed with `bazel run //tensorflow/python/autograph/operators:py_builtins_test` command. I have tried both answering yes and no for `Do you wish to download a fresh release of clang?` question on `./configure`, everything else is the default.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Clone tensorflow from official github repo\r\n2. Checkout to branch r2.0\r\n3. run `./configure` and press enter (all default) except for `Do you wish to download a fresh release of clang?` where I tried both yes & no\r\n4. `bazel run //tensorflow/python/autograph/operators:py_builtins_test`\r\n\r\n\r\n**Any other info / logs**\r\nHere's one of the `./configure` log: https://pastebin.com/yqsPgKeS\r\nHere's the terminal log for `yes` on bazel run command: https://pastebin.com/cvsdbcHY\r\nHere's the terminal log for `no` on bazel run command: https://pastebin.com/VrEVn8ig\r\n", "comments": ["What version of C++ are you using? This is important when compiling C++ code.", "Aha! I have traced your issue to this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/git/gen_git_source.py", "@aaronhma Hi! Here you go:\r\n```\r\n$ gcc --version\r\nConfigured with: --prefix=/Applications/Xcode-beta.app/Contents/Developer/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1\r\nApple clang version 11.0.0 (clang-1100.0.31.5)\r\nTarget: x86_64-apple-darwin18.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```", "@ilhamfp Please use Python 3 instead of Python 2.", "@aaronhma I think I do use Python 3 though.\r\n\r\nHere's the result for `python` in terminal\r\n```\r\n$ python\r\nPython 3.7.3 (default, Mar 27 2019, 16:54:48) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```", "# ** HOW TO FIX THE ISSUE  **\r\n## FROM:\r\n```bash\r\n#!/usr/bin/env bash\r\nset -e\r\nset -o pipefail\r\nif [ -z \"$PYTHON_BIN_PATH\" ]; then\r\n  PYTHON_BIN_PATH=$(which python || which python3 || true)\r\nfi\r\n# Set all env variables\r\nCONFIGURE_DIR=$(dirname \"$0\")\r\n\"$PYTHON_BIN_PATH\" \"${CONFIGURE_DIR}/configure.py\" \"$@\"\r\necho \"Configuration finished\"\r\n```\r\n\r\n## TO\r\n```bash\r\n#!/usr/bin/env bash\r\nset -e\r\nset -o pipefail\r\nif [ -z \"$PYTHON_BIN_PATH\" ]; then\r\n  PYTHON_BIN_PATH=$(which python3 || true)\r\nfi\r\n# Set all env variables\r\nCONFIGURE_DIR=$(dirname \"$0\")\r\n\"$PYTHON_BIN_PATH\" \"${CONFIGURE_DIR}/configure.py\" \"$@\"\r\necho \"Configuration finished\"\r\n```\r\n\r\nThis configuration worked for me. How about you?", "@ilhamfp Change it in the ./configure file. I will submit a PR regarding this.", "@ilhamfp If this worked for you, please close this issue. Thanks!", "@aaronhma It still failed :(\r\nI follow the exact same sequence of command as my first post. Here's the result:\r\n\r\n```\r\n>>>$ bazel run //tensorflow/python/autograph/operators:py_builtins_test\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=223\r\nINFO: Reading rc options for 'run' from /Users/ilhamfirdausiputra/Desktop/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'run' from /Users/ilhamfirdausiputra/Desktop/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/Users/ilhamfirdausiputra/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/Users/ilhamfirdausiputra/anaconda3/lib/python3.7/site-packages --python_path=/Users/ilhamfirdausiputra/anaconda3/bin/python3 --action_env TF_CONFIGURE_IOS=0\r\nINFO: Analyzed target //tensorflow/python/autograph/operators:py_builtins_test (196 packages loaded, 13521 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /private/var/tmp/_bazel_ilhamfirdausiputra/dcdfe0a4c6b40c8776a8245d2d35846a/sandbox\r\nERROR: /Users/ilhamfirdausiputra/Desktop/tensorflow/tensorflow/cc/BUILD:491:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/user_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command \r\n  (cd /private/var/tmp/_bazel_ilhamfirdausiputra/dcdfe0a4c6b40c8776a8245d2d35846a/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/ilhamfirdausiputra/google-cloud-sdk/bin:/Users/ilhamfirdausiputra/anaconda3/bin:/Users/ilhamfirdausiputra/anaconda3/condabin:/Library/Frameworks/Python.framework/Versions/3.7/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Users/ilhamfirdausiputra/bin \\\r\n  /var/tmp/_bazel_ilhamfirdausiputra/install/37d3a264d720ec9ae63603a543c8ff7c/_embedded_binaries/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/user_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/user_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15\r\nTarget //tensorflow/python/autograph/operators:py_builtins_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: no such package '@local_config_download_clang//': cc_download_clang_toolchain rule //external:local_config_download_clang must create a directory\r\nINFO: Elapsed time: 26.316s, Critical Path: 0.25s\r\nINFO: 7 processes: 7 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "@ilhamfp This is a problem related with your computer. This is not TebsorFlow's problem.", "@gunan Anything you can do to help @ilhamfp ?", "I think the problem is that you tried to download a new clang and use that to build TF.\r\nI do not think we have tested that feature on macos.\r\n@r4nt I think Ilya added this feature, but I do not know if he tested this on macos.\r\nAnd I do not remember his github handle :(", "Hi @ilhamfp , Did you try with instructions in this[ link ](https://developer.apple.com/metal/tensorflow-plugin/).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31247\">No</a>\n"]}, {"number": 31246, "title": "Trying to quantize resnet50 tf model", "body": "I am trying to quantize [resnet50 tf model](https://github.com/tensorflow/models/tree/master/official/resnet) (NHWC) using imagenet images. \r\n\r\nI am using [this](https://www.tensorflow.org/lite/performance/post_training_quantization) tutorial for reference.\r\n\r\nThe code is like that - \r\n![quantize](https://user-images.githubusercontent.com/39498824/62318443-ba731b80-b450-11e9-9dc4-32ec55ac3c0a.png)\r\n\r\n\r\nI am confused about how to take image inputs as a numpy array inside representative_dataset_gen(). I have tried like this -\r\n\r\n![dataset_gen](https://user-images.githubusercontent.com/39498824/62318466-c3fc8380-b450-11e9-9c85-691395f86e38.png)\r\n\r\nIt is not working.\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 31245, "title": "gRPC: terminate called after throwing an instance of 'std::bad_alloc'", "body": "System information\r\n    \u2022 OS Platform and Distribution: CentOS Linux release 7.4.1708 (Core)\r\n    \u2022 TensorFlow version : Tensorflow-1.12.0 built from source\r\n\r\nDescribe the problem:\r\nRunning a model parallel implementation results in the following error:\r\n``` terminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n  Aborted\r\n```\r\nThe code runs fine on a single node i.e. with a single worker, but distributing across 2 nodes/2 workers results in the above error. Suspect it is related to grpc.\r\n\r\nSource code / logs\r\nOn the chief worker:\r\n```\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\ndef conv_op(inputs, kernel_, name):\r\n    with tf.variable_scope(name) as scope:\r\n        conv = tf.nn.conv3d(inputs, kernel_, [1, 1, 1, 1, 1], dilations=[1, 1, 1, 1, 1], padding='SAME')\r\n    return conv\r\n\r\ndef inference_withconv(inputs, kernel_):\r\n    with tf.device('/job:worker/task:{}'.format(0)):\r\n        conv1_woc = conv_op(inputs, kernel_, 'conv1_woc')\r\n    with tf.device('/job:worker/task:{}'.format(1)):\r\n        conv2_woc = conv_op(conv1_woc, kernel_, 'conv2_woc')\r\n    with tf.device('/job:worker/task:{}'.format(0)):\r\n        convadd_results = tf.math.add(conv1_woc, conv2_woc)\r\n        return convadd_results\r\n\r\ndef run_benchmark():\r\n    image_shape = (1,1024,1024,1024,1)\r\n    kernel_1_shape = (5,5,5,1,1)\r\n    bias_shape = (1)\r\n\r\n    dummy_image = tf.truncated_normal(\r\n       image_shape,\r\n       dtype=tf.float32,\r\n       mean=0,\r\n       stddev=1,\r\n       name='3Dconv_image_1')\r\n\r\n    dummy_kernel_1 = tf.truncated_normal(\r\n       kernel_1_shape,\r\n       dtype=tf.float32,\r\n       mean=0,\r\n       stddev=1,\r\n       name='3Dconv_kernel_1')\r\n\r\n    image_shape = (1, 1024, 1024, 1024, 1)\r\n    image_init = tf.placeholder(tf.float32, shape=image_shape, name='input_1')\r\n\r\n    res_ = inference_withconv(image_init, dummy_kernel_1)\r\n\r\n    # Define the cluster spec\r\n    cluster_spec = tf.train.ClusterSpec({'worker' : [('<ip_address_1>' + \":\" + '2222'), ('<ip_address_2>' + \":\" + '2222')]})\r\n\r\n    task_id=0 # Chief worker\r\n    server_config = tf.ConfigProto(inter_op_parallelism_threads=2, intra_op_parallelism_threads=20)\r\n    server = tf.train.Server(cluster_spec, job_name='worker', task_index=task_id, config=server_config)\r\n\r\n    session_config = tf.ConfigProto(\r\n      inter_op_parallelism_threads=2,\r\n      intra_op_parallelism_threads=20)\r\n\r\n    with tf.Session(server.target, config=session_config) as sess:\r\n        sess.run(tf.initialize_all_variables())\r\n        image_, kernel_1 = sess.run([dummy_image, dummy_kernel_1])\r\n        infer_results_ = sess.run(res_, feed_dict={'input_1:0': image_, '3Dconv_kernel_1:0': kernel_1})\r\n\r\nif __name__ == '__main__':\r\n    run_benchmark()\r\n```\r\n\r\nOn the non-chief worker (on another node with a different IP address):\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Define the cluster spec\r\ncluster_spec = tf.train.ClusterSpec({'worker' : [('<ip_address_1>' + \":\" + '2222'), ('<ip_address_2>' + \":\" + '2222')]})\r\n\r\ntask_id=1 # Non-chief worker\r\nserver_config = tf.ConfigProto(inter_op_parallelism_threads=2, intra_op_parallelism_threads=20)\r\nserver = tf.train.Server(cluster_spec, job_name='worker', task_index=task_id, config=server_config)\r\n\r\nserver.join()\r\n\r\n\r\n```\r\n\r\n", "comments": ["@karkadad ,\r\nCan you please go through the link of similar [issue](https://github.com/tensorflow/tensorflow/issues/9487).Thanks!", "This is not a memory allocation issue and therefore not related to this issue: https://github.com/tensorflow/tensorflow/issues/9487. Consider the memory profile for a single node run:\r\n![MemProfile](https://user-images.githubusercontent.com/26637358/62395028-e9a68d00-b523-11e9-83a4-1cf32262e9ba.PNG)\r\nThere are no issues running on a single node/single worker and as you can see, there is plenty of memory available so it doesn't run out of memory (The total memory available is ~384 GB and the max utilization is ~208GB) .\r\nThe issue exists only for the model parallel version and scaling on 2 nodes using gRPC.\r\nAlso, if it helps, there are no issues when running the code for smaller image sizes (1, 512, 512, 512, 1). It's only when the image size is increased, for ex: (1,1024,1024,1024,1) , that this error is encountered:\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted", "Since you are able to execute code on parallel version by using lower image size and encounter error for higher image sizes looks like its a failure to allocate memory.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@ymodak Can we reopen this issue? I took over this work from @karkadad and I'm facing this issue. \r\n\r\nWith the same size of the image (1,900,900,900,1), a single node can complete the run, but the parallel version with 2 nodes couldn't because of std::bad_alloc. I think this means that there's enough memory in 1 node and the error happens somewhere in the communication.", "Can you run the code under `gdb` and let us know what the stack trace is for the `std::bad_alloc` exception? It's likely that the communication layers are creating additional backing buffers, but seeing the actual source of the error will help us work out what in TensorFlow we can change to fix the problem.", "#0  0x00007ffff71281f7 in raise () from /lib64/libc.so.6\r\n#1  0x00007ffff71298e8 in abort () from /lib64/libc.so.6\r\n#2  0x00007fffed24a3df in __gnu_cxx::__verbose_terminate_handler () at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/vterminate.cc:95\r\n#3  0x00007fffed248b16 in __cxxabiv1::__terminate (handler=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:47\r\n#4  0x00007fffed248b4c in std::terminate () at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:57\r\n#5  0x00007fffed248d28 in __cxxabiv1::__cxa_throw (obj=0x7ffcc40c9e60, tinfo=0x7fffed2ddab0 <typeinfo for std::bad_alloc>, dest=0x7fffed2477c0 <std::bad_alloc::~bad_alloc()>)\r\n    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:95\r\n#6  0x00007fffed2490c1 in operator new (sz=18446744069414584364)\r\n    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/bits/exception.h:63\r\n#7  0x00007ffec6848b18 in tensorflow::grpc::EncodeTensorToByteBuffer(bool, tensorflow::Tensor const&, grpc::ByteBuffer*) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007ffec684367f in tensorflow::GrpcWorker::GrpcRecvTensorAsync(tensorflow::CallOptions*, tensorflow::RecvTensorRequest const*, grpc::ByteBuffer*, std::function<void (tensorflow::Status const&)>)::{lambda(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#2}::operator()(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) const [clone .constprop.369] () from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007ffec6854742 in std::_Function_handler<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool), std::_Bind<tensorflow::BaseRendezvousMgr::RecvLocalAsync(long long, tensorflow::Rendezvous::ParsedKey const&, std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::{lambda(std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1} (std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>, std::_Placeholder<1>, tensorflow::BaseRendezvousMgr::RecvLocalAsync(long long, tensorflow::Rendezvous::ParsedKey const&, std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::{lambda(std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<2>, tensorflow::BaseRendezvousMgr::RecvLocalAsync(long long, tensorflow::Rendezvous::ParsedKey const&, std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::{lambda(std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<3>, tensorflow::BaseRendezvousMgr::RecvLocalAsync(long long, tensorflow::Rendezvous::ParsedKey const&, std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::{lambda(std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<4>, tensorflow::BaseRendezvousMgr::RecvLocalAsync(long long, tensorflow::Rendezvous::ParsedKey const&, std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::{lambda(std::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)#1}<5>)> >::_M_invoke(std::_Any_data const&, tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007ffec2594eb0 in tensorflow::LocalRendezvousImpl::Send(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#11 0x00007ffec6858f1e in tensorflow::BaseRemoteRendezvous::Send(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007ffec6e9999a in tensorflow::SendOp::Compute(tensorflow::OpKernelContext*) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007ffec2764d31 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#14 0x00007ffec2758d65 in std::_Function_handler<void (), std::_Bind<void (tensorflow::(anonymous namespace)::ExecutorState::*(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long))(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#15 0x00007ffec2ba31d2 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#16 0x00007ffec2ba0fb7 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /miniconda3/envs/tf2.0_2.7/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#17 0x00007fffed264408 in std::execute_native_thread_routine (__p=0x55555afb59d0) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:80\r\n#18 0x00007ffff7bc6e25 in start_thread () from /lib64/libpthread.so.0\r\n#19 0x00007ffff71eb34d in clone () from /lib64/libc.so.6\r\n\r\n@mrry Thanks for jumping on this issue. Do you think this back trace is helpful? Please do let me know if you need any further info. ", "Thank you! This back trace is perfect, and I think it gives us enough information to find a fix. I'll ping this thread when we have more information.", "yes, this is still an issue. @mrry Any updates?", "Still waiting. Any help would be appreciated.", "I saw the same error on CPU, and reducing batch_size addressed the problem.", "@asterisk37n Thanks for your comment. Unfortunately, we are facing this issue with batch_size 1 because the input size is really big.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I am facing the same issue. When I tried to train my input embedding with shape [20K, 128] the training phase looked nice, but when the embedding shape was changed into [20M, 128] the std::bad_alloc error occured in the parameter server. At that time the memory used was less than 50% of total. \r\nAre there any updates about this issue?", "#6 0x00007fffed2490c1 in operator new (sz=18446744069414584364)\r\nThe call stack showed it was requesting to alloc a memory buffer with 18446744069414584364 bytes (1844674406G). of cause it failed.\r\n\r\nI think there is a bug in the method\r\ntensorflow::grpc::EncodeTensorToByteBuffer(bool, tensorflow::Tensor const&, grpc::ByteBuffer*) () when calculate the required buffer size."]}, {"number": 31243, "title": "Model loaded via tf.keras.load_model is very slow", "body": "OS: Ubuntu 18.04\r\nPython 3.6.8\r\nTensorflow version 2.0.0b1 (GPU)\r\nGPU: Titan RTX\r\nCUDA version 10.0\r\n\r\nIssue: current script is working fast\r\n```\r\ndata=np.random.rand(30,16000)\r\ndata = np.expand_dims(data, axis=2)\r\n#model = tf.keras.models.load_model('newmodel.h5')\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.LSTM(15, input_shape=(16000, 1), return_sequences=True))\r\nfor i in range(8):\r\n    model.add(keras.layers.LSTM(15, return_sequences=True))\r\nmodel.add(keras.layers.Dense(1))\r\nmodel.compile(loss='mae', optimizer='adam')\r\nest=model.predict(data)\r\nmodel.save(\"newmodel.h5\")\r\n```\r\n\r\nAfter finishing this script I launch next one, which work very slow (30-40 sec).\r\n\r\n```\r\ndata=np.random.rand(30,16000)\r\ndata = np.expand_dims(data, axis=2)\r\nmodel = tf.keras.models.load_model('newmodel.h5')\r\nest=model.predict(data)\r\n```\r\nthe last two lines of first script output\r\n```\r\n2019-08-01 20:09:24.245721: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-01 20:09:24.631869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n```\r\nSecond script for some reason doesn't output the loading of libcudnn.so.7\r\n", "comments": ["I am able to reproduce the issue on Colab with Tensorflow 2.0.0.beta1. Please see the [gist here](https://colab.research.google.com/drive/1sM__voGXB2NDvCBr4fMl1NJXy3llCZEA). Thanks!", "> I am able to reproduce the issue on Colab with Tensorflow 2.0.0.beta1. Please see the [gist here](https://colab.research.google.com/drive/1sM__voGXB2NDvCBr4fMl1NJXy3llCZEA). Thanks!\r\n\r\nCould you please open access to gist so I can access code and test myself? ", "@yarik1988 I cannot reproduce the issue with `tf-nightly-gpu-2.0-preview`. For me first block of code was taking 11 sec and second block was taking 19 sec. However, when you run the ops on `cpu` with `with tf.device('/cpu:0')` then second block takes only 9 sec. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/30b5952afbfa1ba6e6c19d7595e78c85/tf_31243_load_model.ipynb#scrollTo=lS2n6-L9ulp8). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31243\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31243\">No</a>\n", "I ran into a very similar problem on Jetson's TensorFlow distribution on version 1.14.0 and Keras version 2.2.4-tf (CUDA version 10.0).\r\n\r\nAs there currently does not seem to be a way of installing `tf-nightly-gpu-2.0-preview`, what do you think would be the best way of remedying this issue?\r\n\r\nThanks!", "The solution that works is creating model from the function each time, and then simply loading the weights. So I have the function create_model() that creates and returns the model and instead of loading the model I run it and then load weights from the same model.h5 file "]}, {"number": 31242, "title": "events.out.tfevents file is getting too large as training continues", "body": "\r\nTensorboard event file is getting too large (> 10Gig) as the training continues. I use custom estimator and input_fn reads data from a generator. Everything works fine except this huge tensorboard file that I think it also reduces the speed. When I increase save_summary_steps parameter, this problem doesn't go away. I have seen other blogs complaining about it and none of them provided a viable solution for it. \r\n\r\n\r\n\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): tensorflow-gpu==1.12.0 \r\ntensorflow==1.13.1\r\ntensorflow-estimator==1.13.0\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9\r\n- GPU model and memory: Tesla M40 24GB or Nvidia Geforece GTX 1080Ti \r\n\r\n\r\n", "comments": ["@mhajiaghayi ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "\r\n\r\n`  def inputFn(self,tasks, mode, params= None):\r\n\r\n    types = ((tf.string,tf.string,tf.float32),tf.float32)\r\n    shapes = ((None,None,None),None)\r\n    datasets = []\r\n    for task in tasks:\r\n        dataset = tf.data.Dataset.from_generator(\r\n                            functools.partial(task.relevanceDb.batchGen,self.args,mode),\r\n                            output_shapes=shapes, output_types=types)\r\n        datasets.append(dataset)\r\n        choice = tf.data.Dataset.range(len(datasets)).flat_map(self.getBatch).repeat(self.args.maxSteps)\r\n        multiDataset = tf.data.experimental.choose_from_datasets(datasets,choice)\r\n    multiDataset = multiDataset.batch(self.args.batchSize).prefetch(1) `", "@mhajiaghayi Can you please provide the github gist of the code that you are working with. Thanks!", "@mhajiaghayi Can you please post this issue in [Tensorflow/Tensorboard](https://github.com/tensorflow/tensorboard/issues/) as this issue is related to Tensorboard and you can  get a fast reply. Also Please take a look at this [issue](https://github.com/tensorflow/tensorboard/issues/1252) which is quite similar to yours. Thanks!"]}, {"number": 31241, "title": "Error when changing transformer hyperparameters", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters\r\n\r\n## Description of issue (what needs changing):\r\nI've been working with the 'Transformer model for language understanding' notebook on my own dataset. I got it to work with the default hyperparameters. The tutorial explains that I can create a Transformer XL by adjusting the hyperparameters to those that are used in the paper. I changed them to the suggested values, and I am now getting a `ValueError` when I try to train.\r\n```\r\nValueError: Tensor's shape (8220, 128) is not compatible with supplied shape (8220, 512)\r\n```\r\nI think that this means that some object is not configured properly (not using the hyper parameter variables), but I can't figure out where it's happening. I tried restarting the runtime and running everything again, but it didn't help.\r\n\r\n## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have made small adjustments to the provided code in the transformer notebook to accommodate my own data. I also changed the values of the hyper parameters of the model.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  I am running the notebook in a Colab GPU runtime; Linux Ubuntu 18.04.2\r\n- TensorFlow installed from (source or binary): I'm not exactly sure, I install it using this command, provided with the notebook `pip install -q tensorflow-gpu==2.0.0-beta1`\r\n- TensorFlow version (use command below): `tensorflow-gpu==2.0.0-beta1`\r\n- Python version: 3.6.8\r\n- CUDA version: CUDA: 10.0.130\r\n- GPU model and memory: I'm not sure how to get this info, but it's a Colab GPU runtime.\r\n\r\n## Code snippets\r\nI changed the output encoder from a `SubwordTextEncoder` to a `TokenTextEncoder`:\r\n```\r\ntokenizer_out = tfds.features.text.TokenTextEncoder(\r\n    unique_concepts\r\n)\r\n```\r\nI changed the `tf_encode` function to use a single argument instead of two:\r\n```\r\ndef tf_encode(element):\r\n    return tf.py_function(encode, [element[0], element[1]], [tf.int64, tf.int64])\r\n```\r\nAnd I changed the hyperparameter values:\r\n```\r\nnum_layers = 6\r\nd_model = 512\r\ndff = 2048\r\nnum_heads = 8\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@ravikyram Thanks for the response, and sorry for the delay. I updated the issue body with this info. Sorry for not including it originally - I used the documentation issue submission template, which does not say anything about system information.", "@dawsoneliasen \r\nI tried to reproduce the issue in the doc link https://www.tensorflow.org/beta/tutorials/text/transformer#set_hyperparameters\r\n by changing SubwordTextEncoder to a TokenTextEncoder:I am getting the below error `AttributeError: type object 'TokenTextEncoder' has no attribute 'build_from_corpus'`.Please, help me with the reproducible code. Thanks!", "@ravikyram \r\nThe `TokenTextEncoder` takes a list of tokens to build the vocabulary, instead of building from a corpus. As you can see in the snippet above, I pass a list of tokens to the constructor. You can replicate this by passing any list of unique strings.", "@ravikyram\r\nI just restarted and ran again to try to take another stab at identifying the problem, and it worked. I'm not sure what changed but I'm going to close the issue.", "@ravikyram \r\nIn case you're interested, or someone else runs into this problem:\r\nThe issue is caused by the checkpoints that are created when training the model. If you train a model with the default parameters, and then change to the Transformer XL parameters and try to train again, you'll get an error because TF wants to load the checkpoint of the old model.\r\n\r\nTo resolve, you have to remove the checkpoints before training the Transformer XL. Disconnecting from the runtime doesn't help.\r\n```\r\n!rm -r ./checkpoints\r\n```"]}, {"number": 31240, "title": "SequenceFeatures gets overly complex when used with a sequence_numeric_column", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nThe behaviour of `SequenceFeatures` layer is really different between when it's used with a feature column produced with `sequence_numeric_column` and when it's used with a feature column produced with `sequence_categorical_column_with_identity`and `embedding_column`.\r\n\r\nI've [already reported an issue](https://github.com/tensorflow/tensorflow/issues/29879) where the `call` method of `SequenceFeatures` used with `sequence_numeric_column` unnecessarily requires its input to be a `SparseTensor` instead of a regular `Tensor`.\r\n\r\nToday I was experimenting with Keras Funcional API by building a model using two `SequenceFeatures` layers along with both `sequence_numeric_column` and `embedding_column`. I got graphic depictions of the model using `tf.keras.Model.summary` and `tf.keras.utils.plot_model` and discovered that `SequenceFeatures`  with `sequence_numeric_column` gets decomposed in a million of sub-layers of type `TensorFlowOpLayer`, as opposite to the clean and to the point depiction of `SequenceFeatures` with `embedding_column`. This has the only effect of overloading the results of `summary`and `plot_model`with not so relevant information instead of having a concise insight of the graph.\r\n\r\nHere is the obtained plot:\r\n![Large and complex plot of the model](https://github.com/durandg12/nn_pictures/blob/master/total_mess.png)\r\n\r\nThe ![upper part](https://github.com/durandg12/nn_pictures/blob/master/zoom.png) of the plot is due to the conversion of the input into sparse tensor, so this is related to [the previous issue](https://github.com/tensorflow/tensorflow/issues/29879). The focus of the issue I'm opening today is ![this part](https://github.com/durandg12/nn_pictures/blob/master/zoom2.png) which should not exist in my opinion.\r\n\r\n**Describe the expected behavior**\r\n\r\nAssuming my previous issue is solved, I think that the result of `plot_model` should simply ![look like this](https://github.com/durandg12/nn_pictures/blob/master/expected_model.png).\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.feature_column import embedding_column, sequence_categorical_column_with_identity, \\\r\n    sequence_numeric_column\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.experimental import SequenceFeatures\r\nfrom tensorflow.keras.layers import Input, Dense\r\n\r\n\r\n#on my computer I have used a\r\n#custom plot_model instead of tf.keras.utils.plot_model \r\n#because of a bug, see nota bene below\r\n#from deep.modeltodot import plot_model\r\nfrom tensorflow.keras.utils import plot_model\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\n#Model preparation\r\nseq_fc_dense = sequence_numeric_column('denseFeat')\r\nseq_layer_dense = SequenceFeatures(seq_fc_dense, name='denseFeatLayer')\r\n\r\nnb_cat = 5\r\nseq_fc_cat = sequence_categorical_column_with_identity('catFeat', nb_cat)\r\nseq_fc_cat = embedding_column(seq_fc_cat, 2)\r\nseq_layer_cat = SequenceFeatures(seq_fc_cat, name='catFeatLayer')\r\n\r\ninput_dense = Input(shape=(None,), name='denseFeat')\r\ninput_cat = Input(shape=(None,), name='catFeat', dtype=tf.int32)\r\n# we need to convert input_dense to a sparse tensor, see https://github.com/tensorflow/tensorflow/issues/29879\r\nzero = tf.constant(0, dtype=tf.float32)\r\nindices = tf.where(tf.not_equal(input_dense, zero))\r\nvalues = tf.gather_nd(input_dense, indices)\r\nsparse = tf.SparseTensor(indices, values, tf.cast(tf.shape(input_dense), dtype=tf.int64))\r\n\r\nx_dense = seq_layer_dense({'denseFeat': sparse})[0]\r\nx_cat = seq_layer_cat({'catFeat': input_cat})[0]\r\nx = tf.concat([x_dense, x_cat], -1)\r\noutput = Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = Model(inputs={'denseFeat': input_dense, 'catFeat': input_cat}, outputs=output)\r\n\r\n#model.summary()\r\nplot_model(model)\r\n```\r\n\r\n**Nota bene**\r\nThe custom `plot_model` that I used to make my plots is the same as `tf.keras.utils.plot_model` except that I replaced the following import in keras/utils/vis_utils.py:\r\n```\r\ntry:\r\n  # pydot-ng is a fork of pydot that is better maintained.\r\n  import pydot_ng as pydot\r\nexcept ImportError:\r\n  # pydotplus is an improved version of pydot\r\n  try:\r\n    import pydotplus as pydot\r\n  except ImportError:\r\n    # Fall back on pydot if necessary.\r\n    try:\r\n      import pydot\r\n    except ImportError:\r\n      pydot = None\r\n```\r\nby `import pydot_ng as pydot`and I removed the call to `_check_pydot` in `model_to_dot` because it was raising an exception.\r\n", "comments": ["Was able to replicate the issue with Tensorflow 2.0.0.beta1 on Colab. Please take a look at colab [gist](https://colab.research.google.com/drive/1B9UwKkZRr2WweASkcF0iWJukS0i6pVPY). Thanks!", "The graph is still messy and overly complex in tf2.0.0. On a side note, the bug in `tf.keras.utils.plot_model` is gone and I don't need a custom `plot_model` anymore.", "@durandg12,\r\nCould you please update TensorFlow to v2.3 and check if you are still facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar the issue is solved, with the caveat that we still have to convert dense tensors to sparse tensors just for the code to convert them back to dense tensors which seems inefficient. Also some behind the scene changes caused an error with the code above so I have to put the transformation to a sparse tensor into a function and wrap it into a `Lambda` layer. So here is the final code I used to test:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.feature_column import embedding_column, sequence_categorical_column_with_identity, \\\r\n    sequence_numeric_column\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.experimental import SequenceFeatures\r\nfrom tensorflow.keras.layers import Input, Dense\r\n\r\nfrom tensorflow.keras.utils import plot_model\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\n#Model preparation\r\nseq_fc_dense = sequence_numeric_column('denseFeat')\r\nseq_layer_dense = SequenceFeatures(seq_fc_dense, name='denseFeatLayer')\r\n\r\nnb_cat = 5\r\nseq_fc_cat = sequence_categorical_column_with_identity('catFeat', nb_cat)\r\nseq_fc_cat = embedding_column(seq_fc_cat, 2)\r\nseq_layer_cat = SequenceFeatures(seq_fc_cat, name='catFeatLayer')\r\n\r\ninput_dense = Input(shape=(None,), name='denseFeat')\r\ninput_cat = Input(shape=(None,), name='catFeat', dtype=tf.int32)\r\n# we need to convert input_dense to a sparse tensor, see https://github.com/tensorflow/tensorflow/issues/29879\r\ndef sparse_f(input_dense):\r\n    zero = tf.constant(0, dtype=tf.float32)\r\n    indices = tf.where(tf.not_equal(input_dense, zero))\r\n    values = tf.gather_nd(input_dense, indices)\r\n    sparse = tf.SparseTensor(indices, values, tf.cast(tf.shape(input_dense), dtype=tf.int64))\r\n    return sparse\r\nsparse = tf.keras.layers.Lambda(sparse_f)(input_dense)\r\n\r\nx_dense = seq_layer_dense({'denseFeat': sparse})[0]\r\nx_cat = seq_layer_cat({'catFeat': input_cat})[0]\r\nx = tf.concat([x_dense, x_cat], -1)\r\noutput = Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = Model(inputs={'denseFeat': input_dense, 'catFeat': input_cat}, outputs=output)\r\n\r\nmodel.summary()\r\nplot_model(model)\r\n```\r\n\r\nHere is the resulting output from `plot_model` : [link](https://github.com/durandg12/nn_pictures/blob/master/tf2.3.0-model.png).", "@durang12 closing this one out, as looks like with latest TF version, the issues are resolved .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31240\">No</a>\n", "> @amahendrakar the issue is solved, with the caveat that we still have to convert dense tensors to sparse tensors just for the code to convert them back to dense tensors which seems inefficient. Also some behind the scene changes caused an error with the code above so I have to put the transformation to a sparse tensor into a function and wrap it into a `Lambda` layer. So here is the final code I used to test:\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> from tensorflow.feature_column import embedding_column, sequence_categorical_column_with_identity, \\\r\n>     sequence_numeric_column\r\n> from tensorflow.keras import Input, Model\r\n> from tensorflow.keras.experimental import SequenceFeatures\r\n> from tensorflow.keras.layers import Input, Dense\r\n> \r\n> from tensorflow.keras.utils import plot_model\r\n> \r\n> print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n> \r\n> #Model preparation\r\n> seq_fc_dense = sequence_numeric_column('denseFeat')\r\n> seq_layer_dense = SequenceFeatures(seq_fc_dense, name='denseFeatLayer')\r\n> \r\n> nb_cat = 5\r\n> seq_fc_cat = sequence_categorical_column_with_identity('catFeat', nb_cat)\r\n> seq_fc_cat = embedding_column(seq_fc_cat, 2)\r\n> seq_layer_cat = SequenceFeatures(seq_fc_cat, name='catFeatLayer')\r\n> \r\n> input_dense = Input(shape=(None,), name='denseFeat')\r\n> input_cat = Input(shape=(None,), name='catFeat', dtype=tf.int32)\r\n> # we need to convert input_dense to a sparse tensor, see https://github.com/tensorflow/tensorflow/issues/29879\r\n> def sparse_f(input_dense):\r\n>     zero = tf.constant(0, dtype=tf.float32)\r\n>     indices = tf.where(tf.not_equal(input_dense, zero))\r\n>     values = tf.gather_nd(input_dense, indices)\r\n>     sparse = tf.SparseTensor(indices, values, tf.cast(tf.shape(input_dense), dtype=tf.int64))\r\n>     return sparse\r\n> sparse = tf.keras.layers.Lambda(sparse_f)(input_dense)\r\n> \r\n> x_dense = seq_layer_dense({'denseFeat': sparse})[0]\r\n> x_cat = seq_layer_cat({'catFeat': input_cat})[0]\r\n> x = tf.concat([x_dense, x_cat], -1)\r\n> output = Dense(1, activation='sigmoid')(x)\r\n> \r\n> model = Model(inputs={'denseFeat': input_dense, 'catFeat': input_cat}, outputs=output)\r\n> \r\n> model.summary()\r\n> plot_model(model)\r\n> ```\r\n> \r\n> Here is the resulting output from `plot_model` : [link](https://github.com/durandg12/nn_pictures/blob/master/tf2.3.0-model.png).\r\n\r\nHow do we create an input for Sliding Window Time series data ?"]}, {"number": 31239, "title": "Tflite model show a completely different result than frozen inference model", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, I use a Github repo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung A3 2016\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.4\r\n- GPU model and memory: Nvidia GeForce 840m\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nHello, \r\nI have trained my custom dataset using the Github project [link](https://github.com/yinguobing/cnn-facial-landmark) to create a model that detects eye region with landmarks.\r\nI have converted the ckpt files model to pb and from pb to tflite model.\r\nEverything is OK here when testing with ckpt files and pb frozen graph in prediction it gives 75 %, 80 % predictions :\r\nThat a result on ibug dataset\r\n![resulttaa](https://user-images.githubusercontent.com/19480228/62306855-e7d8bd00-b482-11e9-92e1-05756165c8c3.PNG)\r\n\r\nI have used ML Kit firebase SDK to integrate tflite model and I have followed instruction for how to use custom model.\r\nFor the first time, I tried to print just values (x,y) couples of points ( landmarks ) that can be drawn on the eye region contours\r\n\r\n**Describe the expected behavior**\r\nI got values but are not the same with the same image tested with ckpt files.\r\nI have \r\n\r\n**Code to reproduce the issue**\r\nI have add a new issue on firebase Ml kit , you can find the code and logs [here](https://github.com/firebase/quickstart-android/issues/923)\r\n\r\n\r\nUpdate:\r\nThis is the difference showing between tflite model prediction and frozen inference model prediction:\r\n\r\n+ Tflite model prediction:\r\n\r\n![2019-08-07](https://user-images.githubusercontent.com/19480228/62698776-426f9d00-b9de-11e9-9e7a-0927c467ff78.png) \r\n\r\n+ frozen inference graph model prediction:\r\n\r\n![image](https://user-images.githubusercontent.com/19480228/62699230-55cf3800-b9df-11e9-98e9-8a2fb5f9563a.png)\r\n\r\n\r\n\r\nHow can I solve this?\r\nI really need help to fix this problem.\r\nThanks\r\n", "comments": ["Hi,\r\nIs this still an issue ?\r\nIf yes, can you share the TF pb file \r\n\r\nThanks", "No activity.\r\nClosing.\r\nPlease reopen if you're still having issues.\r\n\r\nThanks"]}, {"number": 31238, "title": "Use where_v2 in lookup_ops to silence warnings", "body": "Without this change, the following deprecation warning is raised:\r\n\r\nW0801 19:32:15.921911 140481946597120 deprecation.py:323] From [...]/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py:1159: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where", "comments": ["@guillaumekln can you please update description little bit more context of warnings or a issue related to this PR , thank you.", "Sorry. Without this change, the following deprecation warning is raised:\r\n\r\n`\r\nW0801 19:32:15.921911 140481946597120 deprecation.py:323] From [...]/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py:1159: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n`"]}, {"number": 31237, "title": "Incomplete description of LSTM.__call__() params", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/48af54a586790cc29150be3d8665d7e8a1770257/tensorflow/python/keras/layers/recurrent.py#L2461\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe current documentation reads:\r\n\r\n```\r\n  Call arguments:\r\n    inputs: A 3D tensor.\r\n    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\r\n      a given timestep should be masked.\r\n    training: Python boolean indicating whether the layer should behave in\r\n      training mode or in inference mode. This argument is passed to the cell\r\n      when calling it. This is only relevant if `dropout` or\r\n      `recurrent_dropout` is used.\r\n    initial_state: List of initial state tensors to be passed to the first\r\n      call of the cell.\r\n```\r\n\r\nIt would be worth mentioning that\r\n\r\n- mask, training, and initial_state are optional\r\n- if initial_state is not provided, default zeros are imputed (by calling `cell.get_initial_state()` or `cell.zero_state()`), see tensorflow_core/python/ops/rnn.py lines 677 or 1382.\r\n\r\nDue to iheritance of RNN layers (incl. mixins) and missing documentation at some places (haven't found any comments on `get_initial_state`), it's quite hard to figure it out.\r\n\r\n### Submit a pull request?\r\n\r\nI'd like to keep it to someone who really knows the Keras internals; to me it's still a bit cryptic (i.e., where exactly `get_initial_state_fn` comes from, etc.)", "comments": ["Thanks. That file [is here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py), can you send a pull request?", "@lamberta I'm not sure what you mean", "Fixed in this PR: https://github.com/tensorflow/tensorflow/pull/31646"]}, {"number": 31236, "title": "PyCharm debugger automatically evaluates Tensor._shape and spams console", "body": "**System information**\r\n- OS Platform and Distribution: macOS 10.14.6\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen analyzing a tensor in eager-execution mode in the \"Watches\" of the PyCharm Debugger, the deprecated object Tensor._shape is automatically evaluated. This results in following warning in the console (two times for one evaluation). When evaluating many tensor, this substantially spams the console.\r\n\r\n```console\r\nW0801 15:29:42.581780 4505736640 ops.py:465] Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\n```\r\n\r\n**Describe the expected behavior**\r\nSignificant reduce in the amount of warnings, ideally to zero.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.enable_eager_execution()\r\n\r\nimport numpy as np\r\n\r\ntensor_1 = tf.convert_to_tensor(np.ones((20, 20)))\r\n\r\nprint()  # set breakpoint here\r\n```", "comments": ["@jannik-w ,\r\nCan you please try using different Python IDE and let us know your feedback.Thanks!", "Hi @anush-o,\r\n\r\nI think the problem emerges when a debugger of any IDE automatically evaluates all attributes of a tensor, including the private ones. So I am not sure what reproducing the problem in another IDE would accomplish.\r\n\r\nThere might be two ways to move forward:\r\n(1) Remove either the warning or the deprecated attribute as the disadvantages outweigh the advantages for some users of one of the most popular IDEs.\r\n(2) Somehow manage that the warning is not printed excessively.", "@jannik-w Did you try this\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\nimport tensorflow as tf\r\n\r\n#0 = all messages are logged (default behavior)\r\n#1 = INFO messages are not printed\r\n#2 = INFO and WARNING messages are not printed\r\n#3 = INFO, WARNING, and ERROR messages are not printed\r\n```", "This issue will also get fixed as we move to TF2.1 release, when we can remove the old deprecation messages", "@jvishnuvardhan\r\nYes, but I do not want to turn off warnings during debugging, because those can be helpful.", "@jannik-w Could you please confirm if the issue persist, i have tried this on anaconda and colab there is no issue.", "@mihaimaruseac I am running 2.1 and I am still getting the warning with logs off.", "@jannik-w Can you check with `TF2.2` or `tf-nightly` and let us know if you notice the issue? \r\n\r\nPlease close the issue if this was already resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31236\">No</a>\n", "Running TF2.3.1 and still getting this message.  Using pycharm.\r\n\r\n```\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\nWARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\r\n\r\n```", "@kevinashaw Can you please open a new issue with a simple standalone code to reproduce the issue? Thanks!", "I am also getting this issue even with TensorFlow 2.4. It always happens when unfolding the a `tf.Tensor` instance in the _Variables_ view."]}, {"number": 31235, "title": "tf.numpy_function throws error when encounter np.transpose", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): kinda\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04LTS Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): conda forge\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory: Quadrao 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to move some of Keras's image [transformation functions](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/affine_transformations.py) into my dataset constructor function, e.g. \r\n\r\n```python\r\n# inside my function that makes a dataset\r\nfilenames = [...]\r\n\r\npreprocess_fn = lambda p: preprocess_image(p, some, default, arguments)\r\n\r\n# workaround as shown by @mrry in https://github.com/tensorflow/tensorflow/issues/12396\r\ntf_preproc_fn = lambda p: tf.numpy_function(preprocess_fn, [p], tf.float32)\r\n\r\npath_ds = tf.data.Dataset.from_tensor_slices(fullpaths)\r\nimgs_ds = path_ds.map(tf_preproc_fn, num_parallel_calls=AUTOTUNE)\r\n\r\n\r\n```\r\nwhere  at somepoint in `preprocess_fn` `np.rollaxis` is called, which in turns calls `np.transpose`. this throws:\r\n\r\n```\r\n\r\n  File \"/home/user/anaconda3/envs/tf/lib/python3.6/site-packages/numpy/core/numeric.py\", line 1554, in rollaxis\r\n    return a.transpose(axes)\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'transpose'\r\n\r\n\r\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThat `tf.numpy_function` just \"works\". Already had to use `tf.cast(tensor.shape[0], tf.float32)` to prevent some errors.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n\r\n```python\r\n# from keras\r\ndef apply_channel_shift(x:list, intensity:float, channel_axis:int=0)->list:\r\n    '''\r\n    Performs a channel shift.\r\n\r\n    Arguments\r\n        x (np.ndarray): Input tensor. Must be 3D.\r\n        intensity (float): Transformation intensity.\r\n        channel_axis (int): Index of axis for channels in the input tensor.\r\n\r\n    Returns\r\n        Numpy image tensor.\r\n    '''\r\n    x = np.rollaxis(x, channel_axis, 0)\r\n    min_x, max_x = np.min(x), np.max(x)\r\n    channel_images = [\r\n        np.clip(x_channel + intensity,\r\n                min_x,\r\n                max_x)\r\n        for x_channel in x]\r\n    x = np.stack(channel_images, axis=0)\r\n    x = np.rollaxis(x, 0, channel_axis + 1)\r\n    return x\r\n\r\n_fn = lambda img_tnsr : apply_channel_shift(img_tnsr, 1, 3)\r\ntf_fn = tf.numpy_function(lambda i: _fn(i), [i], [tf.float32])\r\n\r\npath_ds.map(tf_fn)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@SumNeuron \r\n\r\nIn order to expedite the trouble-shooting process, please provide full code snippet to reproduce it on our environment.Thanks\r\n\r\n", "@ravikyram ask and you shall receive, link to colab where you can recreate the error:\r\n\r\nhttps://colab.research.google.com/drive/1TDYeqKE16jcAdnAAeFT4KYyESisizibq\r\n\r\n\r\n", "I have tried on colab with TF version 1.14 ,TF-nightly and 2.0.0.dev20190731 was able to reproduce the issue. Please, find the [gist](https://colab.research.google.com/drive/1a_GykZMBHIn7wE9O4lKOvPmzOUIL0My_) here.Thanks!", "Is there any update/solution on this?", "@SumNeuron Sorry for missing this issue. I ran it with `TF1.15.3` and I cannot reproduce the issue. I think it was resolved in recently updated `TF1.15.3`. PTAL at this [gist](https://colab.research.google.com/gist/jvishnuvardhan/b920183b618e9f91e3866fdeb51ae7d0/tf-issue-31235.ipynb). Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31235\">No</a>\n"]}, {"number": 31234, "title": "model pruning", "body": "in tensorflow/tensorflow/contrib/model_pruning/, can it speed up the inference of the pruned model?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 31233, "title": "tensorflow r1.10  gpu slow when createDevices", "body": "when i run tensorflowr1.10 in gpu , i found createDevice is slow . and Pause 4 minutes .\r\n\r\nproblem:\r\n```\r\n2019-07-31 23:15:38.412176: I server.cpp:152] Starting the server\r\n2019-07-31 23:19:23.888051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966  Device interconnect StreamExecutor with stiength 1 edge matrix\r\n```\r\n\r\ncode\r\n```\r\nbool CreateSessionFromGraph(\r\n        const string& graph,\r\n        std::unique_ptr<tensorflow::Session>& session) {\r\n      tensorflow::GraphDef graph_def;\r\n      Status status = ReadBinaryProto(tensorflow::Env::Default(),\r\n          graph, &graph_def);\r\n      LOG(INFO) << \"load \" << graph;\r\n      if (!status.ok()) {\r\n        LOG(FATAL) << \"Failed to load: \" << graph;\r\n        return false;\r\n      }\r\n      tensorflow::SessionOptions options;\r\n      options.config.set_allow_soft_placement(true);\r\n      options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);\r\n      options.config.mutable_gpu_options()->set_allow_growth(true);\r\n      options.config.set_intra_op_parallelism_threads(0);\r\n      options.config.set_inter_op_parallelism_threads(0);\r\n      session.reset(tensorflow::NewSession(options));\r\n      status = session->Create(graph_def);\r\n      if (!status.ok()) {\r\n        LOG(FATAL) << \"Failed to create session.\";\r\n        return false;\r\n      }\r\n      return true;\r\n    }\r\n\r\n    TFModel::TFModel(\r\n        const Device& device,\r\n        const string& graph): device_(device), graph_(graph){\r\n      CreateSessionFromGraph(graph_, session_);\r\n    }\r\n\r\n    TFModel::~TFModel() {\r\n      session_->Close();\r\n    }\r\n```\r\n\r\nenv:\r\n```\r\n\r\n$ nvidia-smi \r\nThu Aug  1 19:03:57 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  Off  | 00000000:0D:00.0 Off |                    0 |\r\n| N/A   34C    P0    39W / 250W |   5136MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n$ gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) \r\n\r\n$ g++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) \r\n\r\n$ cat /etc/redhat-release \r\nCentOS Linux release 7.2.1511 (Core) \r\n```", "comments": ["@AnberLu ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31232, "title": "Transfer learning with frozen model ( Resnet )", "body": "Hello,\r\nI want to use transfer learning to detect 80 eye region landmarks instead of 68 face landmarks.\r\nI have a pre-trained frozen graph model pb .\r\nI want to know what are the steps that I can follow to perform the transfer learning to my model?\r\nI see this script, but I don't know how can I use it.\r\nhttps://github.com/tensorflow/models/blob/master/official/resnet/resnet_run_loop.py", "comments": ["@abdou31 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nI am not able to open the link you have provided.Request you to share again.\r\n\r\nPlease, see the below link if it helps you.Thanks!\r\nhttps://www.tensorflow.org/tutorials/images/transfer_learning", "+ Operating system: Windows 10 x64\r\n+ Tensorflow version: binary 1.13.1\r\n+ Model: Resnet\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31232\">No</a>\n"]}, {"number": 31231, "title": "DistributionStrategy is not supported by tf.keras.models.Model.fit_generator ", "body": "Hi! Recently I've encountered a `NonImplemetedError` while trying to apply a fit_generator method of a `tf.keras.models.Model` with a `MultiWorkerDistributionStrategy`. It is almost a year since this handlers were added to the code ( https://github.com/tensorflow/tensorflow/commit/9541ce3475ea70fd8eb9552f60de462127f15440#diff-de9b96ac2d81503324cbbbe21732031f ) and I'm wondering whether to expect an implementation to be added any time soon? (with the release of TF2.0 for example)\r\n\r\nMaking efforts to find a workaround I've tried to transform a generator to TF Dataset by `tf.data.Dataset.from_generator` to replace the `fit_generator` by `fit` but encountered similar problem. The obtained object has type `DatasetV1Adapter`which is also incompatible with distribution strategies\r\n\r\nI dare to assume that for a wide society of TF users and for me in particular this functionality would be of a great interest. Dealing with large, domain specific data sets that doesn't fit into memory, one  often has no choice other than writing a custom data generator. When big data is involved the distributed training might be crucial. \r\n\r\nI would highly appreciate any information on the current state of the problem or possible workarounds from the TensorFlow developers team. Thanks in advance!\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0.dev20190729\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n", "comments": ["@etsygankov Will it be possible to provide the minimal code snippet which replicates the reported issue. Thanks!", "Getting `NotImplementedError` is rather trivial\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n# strategy = tf.distribute.MirroredStrategy()\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\ndef generator(batch_size=100):\r\n    while True:\r\n        data = np.random.random((100, 1))\r\n        yield {'inp': data}, 3 * data\r\n\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(1))\r\n    model.compile('Adam', 'mae')\r\n    model.fit_generator(generator(), steps_per_epoch=1000, epochs=10)\r\n\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/snap/pycharm-professional/147/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/snap/pycharm-professional/147/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/tsygankov/Development/next/bin/tmp1.py\", line 17, in <module>\r\n    model.fit_generator(generator(), steps_per_epoch=1000, epochs=10)\r\n  File \"/home/tsygankov/Development/next/.vtf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1277, in fit_generator\r\n    raise NotImplementedError('`fit_generator` is not supported for '\r\nNotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy.\r\n```\r\n\r\nExperimenting further with TF Data API and a `fit` method I found the bug in my own code, so finally  `tf.data.Dataset.from_generator` works as expected, at least with `MirroredStrategy`.", "@etsygankov Thanks for sharing the code. \r\nI could able to reproduce the issue on Colab with Tensorflow 2.0.0.dev20190729. Please take a look at gist [here](https://colab.research.google.com/drive/1V6n4HIVDPJNZNQjmthrk6s1Xm2R0PS-0). Thanks!", "Model.fit() was recently made work with generators and distribution strategies.  Could you try the latest version of TF2?  Does it provide a work-around for you?\r\n\r\nThis works:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#strategy = tf.distribute.MirroredStrategy()\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\ndef generator():\r\n      while True:\r\n        yield np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)\r\n\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.models.Sequential()\r\n  model.add(tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\"))\r\n  model.compile('Adam', 'mae')\r\n  model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n```", "Please reopen if it doesn't provide a workaround.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31231\">No</a>\n", "Hi, \r\nI have found that the workaround those not work if the model has multiple inputs. The following code fails:\r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#strategy = tf.distribute.MirroredStrategy()\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\ndef generator():\r\n    while True:\r\n        yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)\r\n\r\n\r\nwith strategy.scope():\r\n    inputA = tf.keras.layers.Input(shape=(10,))\r\n    inputB = tf.keras.layers.Input(shape=(10,))\r\n\r\n    output = tf.keras.layers.Concatenate()([inputA, inputB])\r\n    output = tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\")(output)\r\n    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n    model.compile('Adam', 'mae')\r\n    model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts/distributed_strategy_test/fit_test_multiple_inputs.py\", line 20, in <module>\r\n    model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 734, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 224, in fit\r\n    distribution_strategy=strategy)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 547, in _process_training_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 605, in _process_inputs\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 550, in __init__\r\n    reassemble, nested_dtypes, output_shapes=nested_shape)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 540, in from_generator\r\n    output_types, tensor_shape.as_shape, output_shapes)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\", line 471, in map_structure_up_to\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\", line 471, in <listcomp>\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1216, in as_shape\r\n    return TensorShape(shape)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 718, in as_dimension\r\n    return Dimension(value)\r\n  File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 193, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'\r\n```\r\n\r\nShould I open a new thread for this issue?", "@etsygankov hi I meet the same problem with 1.14 tf version, can u share ur solution with tf.data.Dataset.from_generator?", "I have the same error:\r\n\r\n`TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'`\r\n \r\nwith tensorflow 2.0.0-rc0 and a generator based on `keras.utils.Sequence`", "@AHEADer @kielnino According to my experience the generator must return a tuple of length two while objects inside it may be python dicts. Say, your model has two inputs  'x1', 'x2' and a single output. Respective generator may be defined as follows:\r\n\r\n```\r\ndef generator():\r\n    while True:\r\n        data = np.random.random((100, 1))\r\n        yield {'x1': data, 'x2': 2 * data}, 3 * data\r\n\r\ngn = tf.data.Dataset.from_generator(\r\n    generator, ({'x1': tf.float32, 'x2': tf.float32}, tf.float32),\r\n    ({'x1': tf.TensorShape([100, 1]), 'x2': tf.TensorShape([100, 1])}, tf.TensorShape([100, 1]))),\r\n\r\n```\r\nPlease note that shape and type of the data produced by generator must be specified while calling `from_generator` though the shape might be not fully defined", "I tried every workaround proposed here with Colab using TF 1.14 and it didn't work, even though it works on a GPU without any issue, using both `tf.keras.utils.Sequence` or a python generator.\r\n\r\nIs there any workaround for people using a `tf.keras` with a generator, TPU and TF 1.14? \r\n", "Hi isaprykin, thanks again for this clean workaround using tf.distribute.experimental.MultiWorkerMirroredStrategy, but it does not perfectly replace tf.distribute.MirroredStrategy since we cannot set a devices list as param.\r\n", "I have the same error,  \r\n\r\n> NotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy.\r\n\r\nwith `TPUStrategy`.\r\n\r\nI am trying to run a keras model on TPU with significant CPU preprocessing of data that I want to be done in parallel with running batches on TPU.\r\nThe strategy, `TPUStrategy` is a simple one that distribute everything to one TPU core.\r\nThere is no reason why it cannot run preprocessing on CPU in parallel.\r\n\r\nI shall have to switch to fit from fit_generator and run everything sequentially.\r\n\r\nI suggest to reopen this issue.", "any optimal solution for this issue?", "> Hi,\r\n> I have found that the workaround those not work if the model has multiple inputs. The following code fails:\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> #strategy = tf.distribute.MirroredStrategy()\r\n> strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n> \r\n> \r\n> def generator():\r\n>     while True:\r\n>         yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)\r\n> \r\n> \r\n> with strategy.scope():\r\n>     inputA = tf.keras.layers.Input(shape=(10,))\r\n>     inputB = tf.keras.layers.Input(shape=(10,))\r\n> \r\n>     output = tf.keras.layers.Concatenate()([inputA, inputB])\r\n>     output = tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\")(output)\r\n>     model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n>     model.compile('Adam', 'mae')\r\n>     model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n> ```\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"scripts/distributed_strategy_test/fit_test_multiple_inputs.py\", line 20, in <module>\r\n>     model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 734, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 224, in fit\r\n>     distribution_strategy=strategy)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 547, in _process_training_inputs\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 605, in _process_inputs\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 550, in __init__\r\n>     reassemble, nested_dtypes, output_shapes=nested_shape)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 540, in from_generator\r\n>     output_types, tensor_shape.as_shape, output_shapes)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\", line 471, in map_structure_up_to\r\n>     results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\", line 471, in <listcomp>\r\n>     results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1216, in as_shape\r\n>     return TensorShape(shape)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in __init__\r\n>     self._dims = [as_dimension(d) for d in dims_iter]\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in <listcomp>\r\n>     self._dims = [as_dimension(d) for d in dims_iter]\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 718, in as_dimension\r\n>     return Dimension(value)\r\n>   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 193, in __init__\r\n>     self._value = int(value)\r\n> TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'\r\n> ```\r\n> \r\n> Should I open a new thread for this issue?\r\n\r\n did you solve this?", "Any update to this? Particularly curious about tf.keras.utils.Sequence", "> > Hi,\r\n> > I have found that the workaround those not work if the model has multiple inputs. The following code fails:\r\n> > ```python\r\n> > import numpy as np\r\n> > import tensorflow as tf\r\n> > #strategy = tf.distribute.MirroredStrategy()\r\n> > strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n> > \r\n> > \r\n> > def generator():\r\n> >     while True:\r\n> >         yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)\r\n> > \r\n> > \r\n> > with strategy.scope():\r\n> >     inputA = tf.keras.layers.Input(shape=(10,))\r\n> >     inputB = tf.keras.layers.Input(shape=(10,))\r\n> > \r\n> >     output = tf.keras.layers.Concatenate()([inputA, inputB])\r\n> >     output = tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\")(output)\r\n> >     model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n> >     model.compile('Adam', 'mae')\r\n> >     model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n> > ```\r\n> > \r\n> > \r\n> > ```\r\n> > Traceback (most recent call last):\r\n> >   File \"scripts/distributed_strategy_test/fit_test_multiple_inputs.py\", line 20, in <module>\r\n> >     model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 734, in fit\r\n> >     use_multiprocessing=use_multiprocessing)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 224, in fit\r\n> >     distribution_strategy=strategy)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 547, in _process_training_inputs\r\n> >     use_multiprocessing=use_multiprocessing)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 605, in _process_inputs\r\n> >     use_multiprocessing=use_multiprocessing)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\", line 550, in __init__\r\n> >     reassemble, nested_dtypes, output_shapes=nested_shape)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 540, in from_generator\r\n> >     output_types, tensor_shape.as_shape, output_shapes)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\", line 471, in map_structure_up_to\r\n> >     results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py\", line 471, in <listcomp>\r\n> >     results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1216, in as_shape\r\n> >     return TensorShape(shape)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in __init__\r\n> >     self._dims = [as_dimension(d) for d in dims_iter]\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in <listcomp>\r\n> >     self._dims = [as_dimension(d) for d in dims_iter]\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 718, in as_dimension\r\n> >     return Dimension(value)\r\n> >   File \"/home/gbarbadillo/miniconda3/envs/tino/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 193, in __init__\r\n> >     self._value = int(value)\r\n> > TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'\r\n> > ```\r\n> > \r\n> > \r\n> > Should I open a new thread for this issue?\r\n> \r\n> did you solve this?\r\n\r\nTry returning tuples instead of lists.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#strategy = tf.distribute.MirroredStrategy()\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\ndef generator():\r\n    while True:\r\n        yield (np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)), np.ones([10, 1], np.float32)\r\n\r\n\r\nwith strategy.scope():\r\n    inputA = tf.keras.layers.Input(shape=(10,))\r\n    inputB = tf.keras.layers.Input(shape=(10,))\r\n\r\n    output = tf.keras.layers.Concatenate()([inputA, inputB])\r\n    output = tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\")(output)\r\n    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n    model.compile('Adam', 'mae')\r\n    model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n```", "Had the same problem. Then I referred to the [tf2.0.0 documentation for fit()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), where it says:\r\n\r\n> x: Input data. It could be:\r\n> - **generator or keras.utils.Sequence returning (inputs, targets)** or (inputs, targets, sample weights).  more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below.\r\n> y: Target data. Like the input data x, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, **y should not be specified (since targets will be obtained from x).**\r\n> \r\n\r\nThis leads me to the following solution: \r\n\r\n```\r\n# define your DataGenerator() like\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    ...\r\n    return X, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)\r\n\r\n\r\n# when fitting the model\r\nmodel.fit(x = training_generator, \r\n    validation_data = validation_generator, \r\n    use_multiprocessing = True, \r\n    workers = num_cores,\r\n    epochs= epochs, \r\n    verbose = 2)\r\n```\r\n \r\n\r\n", "This issue has been resolved with TF v2.1.0 by replacing `model.fit_generator()` with `model.fit()`", "Neither the model.fit() or model.fit_generator() or disabling the eager computation tf.compat.v1.disable_eager_execution() have solved the problem for me. \r\nHere is my code:\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nwith strategy.scope():\r\n    parallel_model = Sequential([\r\n      Dense(n, input_dim=n, kernel_initializer='normal', activation='relu'),\r\n      Dense(17, kernel_initializer='normal', activation='relu'),\r\n      Dense(4, kernel_initializer='normal', activation='relu'),\r\n      Dense(1, kernel_initializer='normal')])\r\n    parallel_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae', 'mse'])\r\n    \r\nparallel_model.fit(X, y, epochs=20, batch_size=128)\r\n\r\nand here is the error I am getting:\r\n\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3')\r\nINFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CollectiveCommunication.AUTO\r\nModel: \"sequential_3\"\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-54-2f563f7695eb> in <module>()\r\n     11 \r\n     12 parallel_model.summary()\r\n---> 13 parallel_model.fit(X, y, epochs=20, batch_size=128)\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n   1211         else:\r\n   1212             fit_inputs = x + y + sample_weights\r\n-> 1213         self._make_train_function()\r\n   1214         fit_function = self.train_function\r\n   1215 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py in _make_train_function(self)\r\n    314                     training_updates = self.optimizer.get_updates(\r\n    315                         params=self._collected_trainable_weights,\r\n--> 316                         loss=self.total_loss)\r\n    317                 updates = self.updates + training_updates\r\n    318 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)\r\n     89                 warnings.warn('Update your `' + object_name + '` call to the ' +\r\n     90                               'Keras 2 API: ' + signature, stacklevel=2)\r\n---> 91             return func(*args, **kwargs)\r\n     92         wrapper._original_function = func\r\n     93         return wrapper\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)\r\n     73         if _SYMBOLIC_SCOPE.value:\r\n     74             with get_graph().as_default():\r\n---> 75                 return func(*args, **kwargs)\r\n     76         else:\r\n     77             return func(*args, **kwargs)\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/optimizers.py in get_updates(self, loss, params)\r\n    548 \r\n    549             # Apply constraints.\r\n--> 550             if getattr(p, 'constraint', None) is not None:\r\n    551                 new_p = p.constraint(new_p)\r\n    552 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py in constraint(self)\r\n    569       Can be `None` if no constraint was passed.\r\n    570     \"\"\"\r\n--> 571     raise NotImplementedError\r\n    572 \r\n    573   def assign(self, value, use_locking=False, name=None, read_value=True):\r\n\r\nNotImplementedError: \r\n\r\nWhen I try your generator example given above: \r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ndef generator():\r\n    while True:\r\n        yield (np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)), np.ones([10, 1], np.float32)\r\n\r\nwith strategy.scope():\r\n    inputA = tf.keras.layers.Input(shape=(10,))\r\n    inputB = tf.keras.layers.Input(shape=(10,))\r\n\r\n    output = tf.keras.layers.Concatenate()([inputA, inputB])\r\n    output = tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\")(output)\r\n    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n    model.compile('Adam', 'mae')\r\n    model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n\r\nI am getting this Assertion error. \r\n\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3')\r\nINFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3'), communication = CollectiveCommunication.AUTO\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-52-e7b1966dc150> in <module>()\r\n     18     model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n     19     model.compile('Adam', 'mae')\r\n---> 20     model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    617         validation_split=validation_split,\r\n    618         shuffle=shuffle,\r\n--> 619         epochs=epochs)\r\n    620     if not dist_utils.is_distributing_by_cloning(model):\r\n    621       with model._distribution_strategy.scope():\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2240         x = ds.batch(batch_size, drop_remainder=drop_remainder)\r\n   2241       else:\r\n-> 2242         assert isinstance(x, dataset_ops.DatasetV2)\r\n   2243         training_utils.validate_dataset_input(x, y, sample_weight,\r\n   2244                                               validation_split)\r\n\r\nAssertionError: \r\n\r\nCan we please reopen this issue, I do not think it is solved, especially for Keras Sequential Model users? ", "> Model.fit() was recently made work with generators and distribution strategies. Could you try the latest version of TF2? Does it provide a work-around for you?\r\n> \r\n> This works:\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> #strategy = tf.distribute.MirroredStrategy()\r\n> strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n> \r\n> \r\n> def generator():\r\n>       while True:\r\n>         yield np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)\r\n> \r\n> \r\n> with strategy.scope():\r\n>   model = tf.keras.models.Sequential()\r\n>   model.add(tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\"))\r\n>   model.compile('Adam', 'mae')\r\n>   model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n> ```\r\n\r\nIt does not, can we please reopen this issue?", "I have just run a colab notebook and it still works. \r\n\r\nhttps://colab.research.google.com/drive/1lmr3vS6q8OkAtpgogIGBvmBK-VV53GAY", "@etsygankov \r\n> According to my experience the generator must return a tuple of length two while objects inside it may be python dicts. Say, your model has two inputs 'x1', 'x2' and a single output.......\r\n\r\nThank you for this comment. It helped me very much when I had a similar issue."]}, {"number": 31230, "title": "Porting micro_vision example to other other boards.", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n\r\nIs it possible to run this [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_vision) on an Arduino UNO following the the same workflow as in the example, or at least with some minor hacks? TIA", "comments": ["Please refer to the following [doc](https://www.tensorflow.org/lite/microcontrollers#get_started) where tflite with micro controllers is given. Thanks!"]}, {"number": 31229, "title": "Incorrect predictions of Mobilenet_V2", "body": "Hi,\r\nI built the tensorflow lite on iMX8(4xA53) platform. I can get correct results when using models of mobilenet_v1_1.0_224, mobilenet_v1_1.0_224_quant and mobilenet_v2_1.0_224 as input model for  [label_image](https://github.com/tensorflow/tensorflow/tree/v2.0.0-alpha0/tensorflow/lite/examples/label_image). But I got unreasonable predictons when using [mobilenet_v2_1.0_quant](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgzl).\r\n\r\nHere are the results:\r\n_./label_image -i dog.bmp -m **mobilenet_v1_1.0_224.tflite** -l labels.txt                          \r\nLoaded model mobilenet_v1_1.0_224.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 200.31 ms \r\n0.987781: 209 Labrador retriever\r\n0.00432148: 208 golden retriever\r\n0.00298653: 163 beagle\r\n0.00134322: 160 Rhodesian ridgeback\r\n./label_image -i dog.bmp -m **mobilenet_v1_1.0_224_quant.tflite** -l labels.txt \r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 85.934 ms \r\n0.984314: 209 Labrador retriever\r\n0.00392157: 435 bath towel\r\n0.00392157: 208 golden retriever\r\n0.00392157: 169 redbone\r\n0.00392157: 163 beagle\r\n./label_image -i dog.bmp -m **mobilenet_v2_1.0_224.tflite** -l labels.txt \r\nLoaded model mobilenet_v2_1.0_224.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 180.721 ms \r\n0.946971: 209 Labrador retriever\r\n0.00624463: 160 Rhodesian ridgeback\r\n0.0035156: 208 golden retriever\r\n0.00168908: 244 bull mastiff\r\n0.0015452: 163 beagle\r\n./label_image -i dog.bmp -m **mobilenet_v2_1.0_224_quant.tflite** -l labels.txt \r\nLoaded model mobilenet_v2_1.0_224_quant.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 87.001 ms \r\n**0.780392: 209 Labrador retriever\r\n0.529412: 853 tennis ball\r\n0.529412: 160 Rhodesian ridgeback\r\n0.509804: 208 golden retriever\r\n0.498039: 244 bull mastiff**_\r\n\r\nMy questionn is that is this a bug about the Mobilenet_V2 quantized models in tensorflow lite 2.0 alpha?  Would you please help? Thanks in advance.", "comments": ["Hi @ravikyram,\r\nAny update? Thanks.\r\n", "@JionBin What do you mean by \"unreasonable predictions\"? The results look fine to me.", "@freedomtan For quantized model of Mobilenet_v2, the sum of predictions are not equal to '1'. Is that reasonable?", "@JionBin I figured out what the problem is. The output of the `mobilenet_v2_1.0_224_quant.tflite` is not softmax output! It seems it's something before softmax. If you freeze the graph with softmax as the output node and convert it to tflite, then you can get expected results. E.g.,\r\n```\r\nfreeze_graph --input_graph mobilenet_v2_1.0_224_quant_eval.pbtxt \\\r\n--input_checkpoint mobilenet_v2_1.0_224_quant.ckpt \\\r\n--output_graph foo_frozen.pb \\\r\n--output_node_names=MobilenetV2/Predictions/Reshape_1\r\n```\r\nand\r\n```\r\ntflite_convert --graph_def foo_frozen.pb \\\r\n--output_file foo.tflite \\\r\n--input_arrays=input \\\r\n--output_arrays=MobilenetV2/Predictions/Softmax  \\\r\n--input_shape=1,224,224,3 \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--std_dev_values=127 --mean_values=128\r\n```\r\nThen use the quantized MobilenetV2 `foo.tflite` should provide what you expected.", "@JionBin ,\r\n\r\nCan you check @freedomtan 's [answer](https://github.com/tensorflow/tensorflow/issues/31229#issuecomment-527296093) and let us know if it helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31229\">No</a>\n"]}, {"number": 31228, "title": "contrib.receptive_field: ValueError: Weight layer's name input to conv layer does not end with '/read'", "body": "I want to compute the receptive field of some convolutional model defined using tf.keras.\r\nI get the graph directly from the session in which the model is built and run `compute_receptive_field_from_graph_def`:\r\n\r\n```\r\nfrom tensorflow.contrib import receptive_field\r\nfrom tensorflow.keras.layers import Input, Conv2D, AvgPool2D, Dense, Flatten\r\nfrom tensorflow.keras.models import Model                                                                                                                                           \r\nfrom tensorflow.keras.backend import get_session\r\n\r\ndef build_model(input_shape):\r\n    inp = Input(shape=input_shape, name='my_input')\r\n    x = Conv2D(32, (3, 3))(inp)\r\n    x = AvgPool2D((2, 2))(x)\r\n    x = Conv2D(32, (3, 3))(x)\r\n    x = AvgPool2D((2, 2))(x)\r\n    x = Conv2D(32, (3, 3), name='my_output')(x)\r\n    x = Flatten()(x)\r\n    x = Dense(100)(x)\r\n    x = Dense(10, activation='softmax')(x)\r\n    return x\r\n\r\nmodel = build_model([64,64,3])\r\ng = get_session().graph\r\nreceptive_field.compute_receptive_field_from_graph_def(g, 'my_input', 'my_output/BiasAdd')\r\n```\r\n\r\nI set the name of the output as the last name of the operation in my_output layer, obtained by checking `g.get_operations()`, i.e. 'my_output/BiasAdd'\r\n\r\n```\r\n...\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform/shape' type=Const>,\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform/min' type=Const>,\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform/max' type=Const>,\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>,\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform/sub' type=Sub>,\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform/mul' type=Mul>,\r\n <tf.Operation 'my_output/kernel/Initializer/random_uniform' type=Add>,\r\n <tf.Operation 'my_output/kernel' type=VarHandleOp>,\r\n <tf.Operation 'my_output/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>,\r\n <tf.Operation 'my_output/kernel/Assign' type=AssignVariableOp>,\r\n <tf.Operation 'my_output/kernel/Read/ReadVariableOp' type=ReadVariableOp>,\r\n <tf.Operation 'my_output/bias/Initializer/zeros' type=Const>,\r\n <tf.Operation 'my_output/bias' type=VarHandleOp>,\r\n <tf.Operation 'my_output/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>,\r\n <tf.Operation 'my_output/bias/Assign' type=AssignVariableOp>,\r\n <tf.Operation 'my_output/bias/Read/ReadVariableOp' type=ReadVariableOp>,\r\n <tf.Operation 'my_output/dilation_rate' type=Const>,\r\n <tf.Operation 'my_output/Conv2D/ReadVariableOp' type=ReadVariableOp>,\r\n <tf.Operation 'my_output/Conv2D' type=Conv2D>,\r\n <tf.Operation 'my_output/BiasAdd/ReadVariableOp' type=ReadVariableOp>,\r\n <tf.Operation 'my_output/BiasAdd' type=BiasAdd>,\r\n...\r\n```\r\nbut I get\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-aad80edbd6db> in <module>\r\n----> 1 receptive_field.compute_receptive_field_from_graph_def(g, 'my_input', 'my_output/BiasAdd')\r\n\r\n~/.miniconda3/envs/phaunos_ml/lib/python3.6/site-packages/tensorflow/contrib/receptive_field/python/util/receptive_field.py in compute_receptive_field_from_graph_def(graph_def, input_node, output_node, stop_propagation, input_resolution)\r\n    272       (kernel_size_x, kernel_size_y, stride_x, stride_y, padding_x, padding_y,\r\n    273        _, _) = parse_layer_parameters.get_layer_params(\r\n--> 274            node, name_to_node, node_info[node.name].input_size)\r\n    275       logging.vlog(\r\n    276           3, \"kernel_size_x = %s, kernel_size_y = %s, \"\r\n\r\n~/.miniconda3/envs/phaunos_ml/lib/python3.6/site-packages/tensorflow/contrib/receptive_field/python/util/parse_layer_parameters.py in get_layer_params(node, name_to_node, input_resolution, force)\r\n    275   if node.op == \"Conv2D\" or node.op == \"DepthwiseConv2dNative\":\r\n    276     stride_x, stride_y = _stride_size(node, name_to_node)\r\n--> 277     kernel_size_x, kernel_size_y = _conv_kernel_size(node, name_to_node)\r\n    278     # Compute the padding for this node separately for each direction.\r\n    279     total_padding_x, padding_x = _padding_size_conv_pool(\r\n\r\n~/.miniconda3/envs/phaunos_ml/lib/python3.6/site-packages/tensorflow/contrib/receptive_field/python/util/parse_layer_parameters.py in _conv_kernel_size(node, name_to_node)\r\n     86   if not weights_layer_read_name.endswith(\"/read\"):\r\n     87     raise ValueError(\r\n---> 88         \"Weight layer's name input to conv layer does not end with '/read'\")\r\n     89   weights_layer_param_name = weights_layer_read_name[:-5]\r\n     90   weights_node = name_to_node[weights_layer_param_name]\r\n\r\nValueError: Weight layer's name input to conv layer does not end with '/read'\r\n```\r\n\r\nI could not find any layer ending with '/read', as suggested. I also tried just 'my_output', but got `ValueError: Output node was not found`. I also tried to set `g.as_graph_def()` instead of `g` as the function's first argument and failed, so here I am.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Could reproduce the issue with tf-nightly and tensorflow 1.14.0 on Colab. Please take a look Colab [gist](https://colab.research.google.com/drive/1gV0AgZsYTOLK1bDsQxG1TZU5OGwbxYhM) here. Thanks!", "@julj  \r\nIs this still an issue", "@Saduf2019 I haven't tried since I posted the issue, and I am now using TF2.1, so I guess from tensorflow.contrib import receptive_field does not exist anymore or has been updated...", "@julj \r\nIn that case please confirm if can we may move this issue to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31228\">No</a>\n"]}, {"number": 31227, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, TANH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jichangsheng ,\r\nCan you please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!", "As the error suggests you have to build custom operation for ```DEPTH_TO_SPACE```\r\nSee https://www.tensorflow.org/lite/guide/ops_custom", "@jichangsheng ,\r\nCan you please try @ymodak solution and let us know your feedback?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "\nIt worked and successfully produced the .tflite file.\nBut when I want to use this file with tf.lite.Interpreter(.tflite), An error raised as\n\u2018ValueError:Didn\u2019t find custom op for name \u2018DEPTH_TO_SPACE\u2019 with version 1\uff0cRegistration failed \u2018\n\n\n\n\n--\n\u53d1\u81ea\u6211\u7684\u7f51\u6613\u90ae\u7bb1\u624b\u673a\u667a\u80fd\u7248\n\n\n\n\u5728 2019-08-13 18:16:07\uff0coanush <notifications@github.com> \u5199\u9053\uff1a\n\n\nClosed #31227.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread."]}, {"number": 31226, "title": "bad juju in docker vm url", "body": "goal as user: \r\nwould like to click the url produced referring to localhost tokens. using docker as recceomended by tensorflow project  is the fastest and most repeatable way to get gpu+tensorflow+ jupyter\r\n  \r\nconsole output:\r\n\r\n```\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nWARNING: You are running this container as root, which can cause new files in\r\nmounted volumes to be created as the root user on your host machine.\r\n\r\nTo avoid this, run the container by specifying your user's userid:\r\n\r\n$ docker run -u $(id -u):$(id -g) args...\r\n\r\n[I 07:28:24.895 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\njupyter_http_over_ws extension initialized. Listening on /http_over_websocket\r\n[I 07:28:25.795 NotebookApp] Serving notebooks from local directory: /tf\r\n[I 07:28:25.795 NotebookApp] The Jupyter Notebook is running at:\r\n[I 07:28:25.795 NotebookApp] http://(2950c268081b or 127.0.0.1):8888/?token=9c954171d97b7cf33d46c63611fe4c4f04558871aaf5eb06\r\n[I 07:28:25.795 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 07:28:25.798 NotebookApp] \r\n    \r\n    To access the notebook, open this file in a browser:\r\n        file:///root/.local/share/jupyter/runtime/nbserver-1-open.html\r\n    Or copy and paste one of these URLs:\r\n        http://(2950c268081b or 127.0.0.1):8888/?token=9c954171d97b7cf33d46c63611fe4c4f04558871aaf5eb06\r\n```  \r\n\r\nproblem:\r\n\r\n` http://(2950c268081b or 127.0.0.1):8888 ` is not a legal url in most browsers.  in even fewer browsers, this will result in useful copy and paste \r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@ravikyram this is bone stock ubuntu 18.04 following the tensorflow web page to pursue the docker prescription with the above mentioned instance tags\r\n\r\n", "I'm afraid I don't have the time to work on this, but please feel free to submit a PR!", "well, what produces the string \"(2950c268081b or 127.0.0.1)\" ?\r\n\r\nand what code uses that string?\r\n\r\ncoming from the 5 minute docker instructions, my familiarity with the internals is quite low.\r\n", "i think the best PR to solve the bug would be to comment out the docker instructions from the website and leave the jupyter code alone. ", "Those error messages seem to come from Jupyter, not Tensorflow code. See for example https://github.com/jupyter/notebook/issues/3947\r\n\r\nI think we can close this issue in this case."]}, {"number": 31225, "title": "tensotflow website", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/guide/keras\r\n\r\n## Description of issue (what needs changing):\r\n\r\ncan't open URL\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["This might help:\r\n[https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras)\r\n***\r\n**Explanation:**\r\ntensorflow.org/~~tutorials~~/guide/keras  =>  tensorflow.org/guide/keras", "Tnak you. But in the www.tensorflow.org page thr URL is https://www.tensorflow.org/tutorials/guide/keras. Maybe it can be updated.", "> Tnak you. But in the [www.tensorflow.org](http://www.tensorflow.org) page thr URL is https://www.tensorflow.org/tutorials/guide/keras. Maybe it can be updated.\r\n\r\n@HymEric Can you please tell us, where exactly the link is present? Thanks!", "In Chinses language.\r\n![image](https://user-images.githubusercontent.com/26487821/62363970-f8586880-b552-11e9-8828-7241ab7d6877.png)\r\n", "@HymEric Thanks for finding the issue.", "The issue is [here](https://www.tensorflow.org/tutorials/keras). Thanks!\r\n![Screenshot from 2019-08-08 11-15-56](https://user-images.githubusercontent.com/42785357/62797119-b8800c80-ba8f-11e9-8624-ce140c7ca654.png)\r\n", "Thanks. This is fixed with https://github.com/tensorflow/docs/pull/914\r\nThe site will be updated on the next Chinese translations push.\r\nIf you're interested in contributing to the community translations, see https://www.tensorflow.org/community/contribute/docs#community_translations and join the docs-zh-cn@tensorflow.org list :)\r\n"]}, {"number": 31224, "title": "Can not load tflite model", "body": "# TF information\r\n```\r\ntf-nightly           1.15.0.dev20190730\r\n```\r\n# Model Information\r\n\r\nI use the bert's `run_classifier.py` to export the classifier model\r\n```\r\n1141         name_to_features = {\r\n1142             \"input_ids\": tf.VarLenFeature(tf.int64),\r\n1143             \"input_mask\": tf.VarLenFeature(tf.int64),\r\n1144             \"segment_ids\": tf.VarLenFeature(tf.int64),\r\n1145             \"label_ids\": tf.FixedLenFeature([], tf.int64)\r\n1146         }\r\n1147         serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(name_to_features)\r\n1148         estimator.export_savedmodel(FLAGS.output_dir, serving_input_receiver_fn)\r\n```\r\nThen the estimator to export model.\r\n\r\n# Code to generate tflite model\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nsaved_model_dir = sys.argv[1]\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_quant_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\n# Code to load tflite model\r\n```import numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\nprint(input_details)\r\noutput_details = interpreter.get_output_details()\r\nprint(output_details)\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\nWhen load tflite load model path I get error message\r\n```\r\n$python test_quant.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0801 15:01:28.343195 140634983794496 __init__.py:328] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nTraceback (most recent call last):\r\n  File \"test_quant.py\", line 5, in <module>\r\n    interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\n  File \"/home/guohuawu.wgh/miniconda3/envs/quant/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py\", line 206, in __init__\r\n    model_path))\r\nValueError: Input array not provided for operation 'reshape'.\r\n```\r\n\r\n", "comments": ["You can remove \"the reshape layer\", because the operation of \"reshape\" is  unsupported in tensorflow-1.15.0. ", "@wugh  I'm having this same issues, where you able to resolve it", "@raziel Can we expect anything to be done about this\r\nnote for me only have this issue with the quantized version of the model", "Hi.\r\n\r\nDoes the model convert if you don't set the quantization flag?\r\nMeaning removing: converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n", "@raziel and @suharshs Yes conversions completes successfully when the quantization flag is not set", "Hi,\r\ntf version-1.13\r\ni created one tf lite model, to load this model i am using tensorflow code i.e\r\n\r\nfrom __future__ import print_function\r\nfrom absl import flags\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nflags.DEFINE_string('model_path', None, 'Path to model.')\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main(_):\r\n\r\n  flags.mark_flag_as_required('model_path')\r\n\r\n  # Load TFLite model and allocate tensors.\r\n  interpreter = tf.lite.Interpreter(model_path=FLAGS.model_path)\r\n  interpreter.allocate_tensors()\r\n\r\n  # Get input and output tensors.\r\n  input_details = interpreter.get_input_details()\r\n  # print('input_details:', input_details)\r\n  output_details = interpreter.get_output_details()\r\n  # print('output_details:', output_details)\r\n\r\n  # Test model on random input data.\r\n  input_shape = input_details[0]['shape']\r\n  # change the following line to feed into your own data.\r\n  input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n  interpreter.invoke()\r\n  output_data = interpreter.get_tensor(output_details[0]['index'])\r\n  print(output_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n\r\n\r\nBut i am getting error while running this code i.e:\r\n\r\n   interpreter = tf.lite.Interpreter(model_path=FLAGS.model_path)\r\n  File \"/lib/python3.5/site-packages/tensorflow/lite/python/interpreter.py\", line 55, in __init__\r\n    model_path))\r\nValueError: Model provided has model identifier 'inpu', should be 'TFL3'\r\n\r\ndoes anybody know about this error?\r\nthanks in advance.\r\n", "@devarajnadiger It seems your models is malformed. \"TFL3\" is the versioning info we include in the flatbuffer file. How did you created the model?\r\nYou should probably file another issue and include the model.", "sorry @rachellj218. there was a mistake from my side i.e we should convert our checkpoints to pb twice to get tflite model as mentioned here: https://github.com/tensorflow/models/blob/master/research/lstm_object_detection/g3doc/exporting_models.md .\r\nafter following this, I was able to test my tflite model.", "I have the same problem. Does tf2.0 support the reshape operation in tflite?", "Hi all, I tried to debug the tensorflow source code to find the reason.\r\nIt seems that while quantizing the model, toco(tflite_convert)  misses putting the **new_shape** attribute in the **ReshapeOptions** of flatbuffers.\r\nThe only difference in **ReshapeOptions** between the unquantized and the quantized model is whether to have the **new_shape** attribute.\r\nMore interesting, the unquantized model barely has an empty list for new_shape.\r\nBy inserting an empty list for new_shape, I can successfully run my quantized model.\r\nI also made a script  [fix_reshape.py](https://github.com/zldrobit/onnx_tflite_yolov3/blob/master/fix_reshape.py) to temporarily fix the issue, plz refer to **Quantization** section in [onnx_tflite_yolov3](https://github.com/zldrobit/onnx_tflite_yolov3)\r\n", "@zldrobit  - Thanks; your instructions worked to fix this issue for me.\r\n\r\n@suharshs I can readily reproduce this issue on TF2. Feel free to contact me if you need any info. LDAP is jbetker.", "> p\r\nthis code might help you.\r\n\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n## Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"/content/drive/My Drive/Colab Notebooks/YOLO/optimized_pydnet++.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n##Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n## Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n## The function `get_tensor()` returns a copy of the tensor data.\r\n## Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\nprint(output_data)", "@wugh Hello, have you solved the problem? I met the same issue.", "Hi @wugh ! We are checking to see if you still need help in this issue or not .Have you tried latest versions yet ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please refer to this solution and let us know if it helps: [link](https://github.com/tensorflow/tensorflow/issues/32204#issuecomment-703506041)", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31224\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31224\">No</a>\n", "tf information: tf.2.4.0  \r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\n#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\ntflite_model = converter.convert()\r\nopen(output_model_filepath, \"wb\").write(tflite_model)\r\n\r\n\r\nand \r\n\r\ninterpreter = tf.lite.Interpreter(model_path=model_path)\r\n\r\n _interpreter_wrapper.CreateWrapperFromFile(\r\nValueError: Could not open(\"xx.tflite\")\r\n\r\n\r\ndoes anybody know about this error?\r\nthanks in advance.\r\n\r\n"]}, {"number": 31223, "title": "added `__restrict__` tag to all `__global__`kernels", "body": "@chsigg \r\n\r\nI have added `__restrict__` tag to all `__global__` kernels.\r\n\r\nHowever, I have left `GpuDeviceArrayStruct<T*>` alone, such as:\r\n\r\n```cpp\r\n__global__ void SplitVOpKernel_fixed(const T* __restrict__ input, int32 prefix_dim_size,\r\n                                     int32 suffix_dim_size,\r\n                                     GpuDeviceArrayStruct<T*> output_ptr_data) {\r\n```", "comments": ["I have also prepared a PR for adding `__restrict__` to `__device__` kernels. I will send it once we are done with this one.", "> I have also prepared a PR for adding __restrict__ to __device__ kernels. I will send it once we are done with this one.\r\n\r\nNever mind this comment. I committed changed to both `__device__` and `__global__`.", "@chsigg \r\n\r\nSince there are so many kernels, I think it would be better to profile them by calling the high-level python API instead of timing the kernels directly like we have been doing previously.\r\n\r\nI am going to:\r\n1. turn eager mode: `tf.enable_eager_execution()`\r\n2. and call functions that call the CUDA kernels that have been modified using `%timeit`\r\n\r\nDoes this sound alright? I am not sure if there exists any additional overhead by turning on eager execution. Also, calling Python API would have overhead, but it seems negligible according to [this](https://gist.github.com/colesbury/41fc51225f25e72d7dd0049ec3966e04) conversation I had with a Pytorch dev.", "I think you know more than me ;-)\r\nAnd yes, your plan sounds good.\r\nYou also don't need to benchmark every last kernel. A sampling should be sufficient.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31223) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31223) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31223) for more info**.\n\n<!-- ok -->", "@ThisIsIsaac Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "@gbaned planning on doing so as soon as I profile some meaningful results :)", "@ThisIsIsaac, Still, conflicts are appearing. Can you please resolve? Thanks!", "@gbaned are you planning on merging once I resolve the conflict? I was going to resolve the conflict after I get approval so that there aren't more conflicts created while waiting for approval. If you are, then I will resolve the conflict ASAP. ", "I will resolve the conflict during merge.", "Oops, sorry. That doesn't work (tool limitation). Would you please fix the conflicts, then I will perform the merge. Thanks!", "@chsigg @gbaned conflict resolved.", "@chsigg @ThisIsIsaac @gbaned  Are you sure that this blanket __restrict is a good idea? For example for some kernels, specifically non_max_suppression_op.cu.cc, it is pointless since compiler can see whether aliasing is possible or not, or in some cases aliasing restriction might not be guaranteed. I believe __restrict__ keyword should be added case by case when its benefit is obvious, otherwise it is just complicating code. I did some tests for the some of the kernels modified by this PR, and compiler generated exactly the same assembly for both cases. Also these changes may introduce heisen-bugs that are data dependent and notoriously hard to debug. ", "In my tests for NMS op, I see a small slowdown due to this change. @ThisIsIsaac do you have some table from your profiling showing that these changes are actually beneficial?", "@samikama Thanks so much for taking a close look at the modifications and profiling some kernels!\r\n\r\n> Also these changes may introduce heisen-bugs that are data dependent and notoriously hard to debug.\r\n\r\nI am not aware of any bugs that `__restrict__` can introduce, unless there are aliases to the pointers within the kernel.\r\n\r\n---\r\n\r\n> In my tests for NMS op, I see a small slowdown due to this change.\r\n\r\nCould you share details of how and what versions you profiled against? I don't think `__restrict__` can slow down the speed, but please let me know if I am mistaken.\r\n\r\nWhen I profiled with below configuration:\r\n**TF version**: 1.14\r\n**GPU:** GTX1660 & V100\r\n**code:**\r\n```python\r\nimport tensorflow as tf\r\nfrom IPython import get_ipython\r\n\r\nipython = get_ipython()\r\n\r\n# TF configuration\r\ntf.enable_eager_execution()\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)\r\n\r\nt = tf.Variable(tf.random.uniform([50, 50, 50, 50]))\r\nf = tf.Variable(tf.random.uniform([50, 50, 50, 50]))\r\n\r\n# start profiling\r\nipython.magic('timeit out = tf.nn.conv2d(t, filter=f, padding=\"SAME\")')\r\n```\r\n\r\nI saw **10% slowdown** but it turned out to be due to some other code that changed, not because of adding `__restrict__`. **So I would isolate the CUDA kernels rather than using the Python API to profile, if you haven't done so already.**\r\n\r\nWhen I pulled from the commit right before mine and profiled CUDA kernels directly instead of using the Python API we didn't see any slowdown.\r\n\r\n---\r\n\r\n>  I believe restrict keyword should be added case by case when its benefit is obvious, otherwise it is just complicating code.\r\n\r\nYes, I agree. I may have over-done it as I have thought there couldn't be any slowdown or bugs due to `__restrict__` (and again, please correct me if I am wrong). But not by too much. Many Tensorflow kernels access global memeory like the below kernel:\r\n\r\n```cpp\r\ntemplate <typename T>\r\n__global__ void UnaryClipCustomKernel(const int size_in, const T* __restrict__ in0,\r\n                                      const T* __restrict__ in1, const T* __restrict__ in2, T* __restrict__ out) {\r\n  for(int i = blockIdx.x * blockDim.x + threadIdx.x; i <= size_in; i += gridDim.x * blockDim.x) {\r\n    T value = in2[0] < in0[i] ? in2[0] : in0[i];\r\n    out[i] = value < in1[0] ? in1[0] : value;\r\n  }\r\n}\r\n```\r\n\r\nEach thread reads a single consecutive item of type `T` at a time. Since each global read & write granularity is 128bytes / warp, this is fine if  `sizeof(T) >= 4` (since `4 * 32 = 128`). But for `half` and `int8` types, there is wasted bandwidth, 61 and 96 bytes respectively. Adding `__restrict__` allows the compiler to cache these wasted bandwidth which should make memory-bottleknecked kernels significantly faster. And most of Tensorflow kernels only reads a single item at a time, which means most kernels can benefit from using `__restrict__` for `half` and `int8`.\r\n\r\nAfter having profiled 3 kernels, there was about 3~5% increase in speed due to adding `__restrict__` using `float` type since it is the most popular one among `float`, `half`, `int` and `int8`. I'll share some more details as @minty and I profile the code a little more.", "I'm having some second thoughts as well that this might be on the risky side. I do like getting speedups almost for free, and I know that __restrict__ does help. Let's do some benchmarks and then decide. For example, remove the restricts where it doesn't help and look at the code more closely where it does.", "From CUDA programming guide about using `__restrict__` (confirming what @samikama said about performance):\r\n\r\n> The effects here are a reduced number of memory accesses and reduced number of computations. This is balanced by an increase in register pressure due to \"cached\" loads and common sub-expressions.\r\n> Since register pressure is a critical issue in many CUDA codes, use of restricted pointers can have negative performance impact on CUDA code, due to reduced occupancy.\r\n\r\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#restrict", "@chsigg I was just pointing to the doc but @pooyadavoodi beat me to it. Our feelings on getting speedups almost for free is mutual, but I believe there are very few silver bullets left if any. __restrict will help in some cases but not necessarily all and even might worsen the performance as described in the manual. So I believe best would be doing it case-by-case basis, possibly by investigating investigating individual kernels by [nsight-compute](https://developer.nvidia.com/nsight-compute-2019_4) and by profiling them. ", "@ThisIsIsaac \r\n\r\n> I am not aware of any bugs that __restrict__ can introduce, unless there are aliases to the pointers \r\n> within the kernel.\r\n\r\nDid you verify that pointer aliasing is never a possibility in the cases you modified? If pointer aliasing can happen, there will be hard to trace bugs in the results.\r\n\r\n> Could you share details of how and what versions you profiled against? I don't think __restrict__\r\n> can slow down the speed, but please let me know if I am mistaken.\r\n\r\nPlease see comment above.\r\nAlso, correct me if I am wrong but in your example above, having in and out to point the same array is perfectly fine right? It would be in-place clipping. However you are promising the compiler that they are never going to be same or alias each other. \r\nAs I mentioned above, I suggest using nsight-compute to verify your changes actually improve the the results, if possible significantly.\r\n\r\n", "@chsigg @samikama \r\n\r\nAfter having read the doc, I also think it is best to profile per-kernel instead of applying a sweeping change to all kernels. \r\n\r\nI will work with @minty99 and break this PR into smaller PRs per kernel.\r\n\r\nThanks again for your insights :)"]}, {"number": 31222, "title": "Specifying shapehandle's shape when creating new op in tensorflow (rank problem of the output tensor)", "body": "System: Ubuntu 18.04\r\nTensorflow: 1.13.1\r\nCuDNN: 7.4.2\r\nCUDA: 10.2\r\nNVIDIA: 430.26\r\nPython: 3.6\r\nGPU: 2080TI\r\n\r\nI have successfully compiled the op registration file and tested when only using this file. But during training process, I tried to call the function defined in the op, these errors were encountered, which vary every time:\r\n\r\n`Segmentation fault (core dumped)`\r\n\r\nor\r\n\r\n```\r\ndouble free or corruption (!prev)\r\nAborted (core dumped)\r\n```\r\n\r\nor\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1659, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 99648624 for '...myop' (op: 'myop') with input shapes: [50,2048,3].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 327, in <module>\r\n    network.build_graph(training=True)\r\n  File \"main.py\", line 36, in build_graph\r\n    reuse=None, bn_decay=self.bn_decay, up_ratio=opt.u)\r\n  File \"/home/.../graph.py\", line 87, in graph\r\n    p_out = myfunc(x)\r\n  File \"/home/.../myop.py\", line 19, in myfunc\r\n    return myop_module.myfunc(x)\r\n  File \"<string>\", line 68, in myfunc\r\n  File \"/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1823, in __init__\r\n    control_input_ops)\r\n  File \"/home/tifo-kj/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1662, in _create_c_op\r\n    raise ValueError(str(e))\r\n\r\nValueError: Shape must be rank 1 but is rank 99648624 for '...myop' (op: 'myop') with input shapes: [50, 1000, 3].\r\n```\r\n\r\nAnd please note the number `99648624` above is uncertain, sometimes it could be 0 or any weird number.\r\n\r\nBelow is the code for registering the op in tensorflow, where I specify the output's dimension as `(b,200,200,1)`:\r\n \r\n````\r\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        ::tensorflow::shape_inference::ShapeHandle input\r\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 3, &input));   \r\n        ::tensorflow::shape_inference::ShapeHandle dim2;\r\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(200, &dim2));    \r\n        ::tensorflow::shape_inference::ShapeHandle dim3;\r\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(200, &dim3));    \r\n        ::tensorflow::shape_inference::ShapeHandle dim4;\r\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &dim4));    \r\n        ::tensorflow::shape_inference::ShapeHandle output = c->MakeShape({c->Dim(input, 0), c->Dim(dim2, 0), c->Dim(dim3, 0), c->Dim(dim4, 0)});\r\n        c->set_output(0, output);\r\n        return Status::OK();\r\n    });\r\n````\r\n\r\nI believe that in my code of op registration, the output shape has been already successfully determined. There is no similar question that I can find online. Please help!\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31222\">No</a>\n"]}, {"number": 31221, "title": "[INTEL MKL] Work around tf.data related performance regression on Int\u2026", "body": "\u2026el MKL backend.\r\n\r\nRN50 real data performance regression was about 40%. This PR brings the\r\nperformance back by adding the MKL specific #ifdef macro.", "comments": ["Furthermore, while your fix might address the resnet performance regression, it would break any programs that require the use of multi-device function backend.", "Thanks @jsimsa for the review. We now roughly know why is_multi_device_function==true would cause performance slowdown, and we are internally working on a real solution, rather than a workaround. It will take time to finish validation to ensure the general solution does not incur other regressions. \r\n\r\nFor \"programs that require the use of multi-device function backend\",  good point. This PR will generate an inconsistency from the python level. Could you please point me to some design principals (documents) of this is_multi_device_function? E.g. I'd like to understand is_multi_device_function transparent to the user and how can I tell that a particular program is using/benefiting from multi-device function backend, especially on why multi_device_function would benefit the Eigen on CPU and MKL on CPU, where the sharing of inter-op thread pool is already causing some headache sometimes. Thanks!", "Here is an example of a program that requires multi-device function backend: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/kernel_tests/map_test.py#L1227\r\n\r\nOr any program that runs with a GPU and access a variable from input pipeline. In the presence of GPU, variables will be placed on GPU by default and without multi-device function such programs would not work. Here is a trivial example::\r\n\r\n```\r\n    v = tf.Variable([1.])  # implicitly on GPU\r\n    def _map_fn(features, labels):\r\n      return tf.add(features, v), labels\r\n\r\n    features = tf.constant([[0., 1.], [2., 3.]])\r\n    labels = tf.constant([1, 0])\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\n    dataset = dataset.map(_map_fn)\r\n```", "multi-device function backend is considered an implementation detail and there are no design principals / documents (as far as I know). You can study its code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/process_function_library_runtime.h) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/process_function_library_runtime.cc).", "Thanks @jsimsa for your time and this example! I understand is_multi_device_function more now. \r\n\"Or any program that runs with a GPU and access a variable from input pipeline. In the presence of GPU, variables will be placed on GPU by default and without multi-device function such programs would not work. \" It seems to me that this design is benefitting heterogeneous (CPU w/ GPU, CPU w/TPU) where the accelerators might need CPU's help for input pipelining. But different from  GPU, Intel MKL also runs on CPU, so Intel MKL does not need multi_device capability to read variables from the input pipelining, right? \r\nAlso, my PR has added \"#ifdef\", so when we find it is GPU, multi_device will still be true.  \r\n\r\nI think programs that require multi_device support are mostly written for non-CPUs (as you said,  to transfer data from input to compute), this PR maybe does not affect such programs I think. Please correct me if I am wrong. I think for pure CPUs, Intel MKL does not need to read variable from the input pipeline, therefore does not need multi_device_function?  \r\n\r\nOr, for Intel MKL, are you thinking of the support that \"one socket of a two socket system does input processing, and the compute would be on the other socket feature\"?  Without multi_device_function feature, this will not be possible in TensorFlow, correct?  If that is the case, I think it makes sense. \r\n", "As you pointed out, cross-device computation does not have to be just between CPU and GPU/TPU. There can be multiple CPU devices (e.g. one for each NUMA zone or if explicitly created by the user program).\r\n\r\nIndependently of the above, I think your change is making the tf.data code brittle (and harder to test) and I would prefer to first understand the problem (and convince myself that there is no better solution) before accepting such a change.", "Ok, thanks @jsimsa It now makes sense for me. Actually a better solution (not affecting tf.data at all) is coming up soon by our team. I am going to close my PR in light of our discussion above. Thank you! ", "Sounds good.", "@jsimsa  FYI, this is the real fix: https://github.com/tensorflow/tensorflow/pull/31254 \r\nRestricting graph rewrite rule so that we don't have a standalone mkl version of slice op while others are non MKL. "]}, {"number": 31220, "title": "\"Build from source\" fails during \"bazel build\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.14\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: CUDA Version 10.0.130 / cuDNN 7.4.2\r\n- GPU model and memory: GeForce GTX 1080 Ti (11178MiB)\r\n\r\n\r\n**Describe the problem**\r\nI am trying to install TensorFlow (TF) from source, since I found that I have to revise some codes in order to use the TensorFlow's MFCC function with TensorFlow Lite (https://github.com/tensorflow/tensorflow/issues/26174).\r\n\r\nCurrently, my TF 1.14 installed with pip works fine in the virtual environment, but when I tried to install it from source, some errors occur during the \"bazel build\", and I could not find how to fix it.\r\n\r\nWhen I run \r\nbazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures //tensorflow/tools/pip_package:build_pip_package, \r\nit runs into the problem.\r\n\r\nWhen I repeatedly run the same command (without touching anything), some different error messages come out (maybe they look different to me since I'm very ignorant to what's happening during the build process).\r\n\r\nThe error messages (from the terminal) are copied and pasted below, and the full logs are attached as files.\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI followed the manual provided by https://www.tensorflow.org/install/source\r\n\r\n(1) Install Bazel\r\nDownloaded the binary installer \"bazel-0.26.1-installer-linux-x86_64.sh\" (from https://github.com/bazelbuild/bazel/releases).\r\n./bazel-0.26.1-installer-linux-x86_64.sh --user\r\n\r\n(2) Cloned tensorflow\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n\r\n(3) Manually revised mfcc.cc\r\nRevised \"tensorflow/lite/kernels/mfcc.cc\" according to \"https://github.com/tensorflow/tensorflow/issues/26174\"\r\n\r\n(4) Configure the build\r\ndreadbird@dreadbird:~/tensorflow$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1]: 6.1\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n(5) Build the pip package (Bazel build)\r\nbazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nBelow are the error messages that I got by running the same command of \"bazel build\" three times (The entire log messages are attached as files).\r\nbazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n[error_log1.txt](https://github.com/tensorflow/tensorflow/files/3454957/error_log1.txt)\r\n[error_log2.txt](https://github.com/tensorflow/tensorflow/files/3454958/error_log2.txt)\r\n[error_log3.txt](https://github.com/tensorflow/tensorflow/files/3454959/error_log3.txt)\r\n\r\n(1)\r\nERROR: missing input file '@nccl_archive//:src/collectives.h'\r\nERROR: /home/dreadbird/tensorflow/tensorflow/cc/BUILD:506:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command \r\n  (cd /home/dreadbird/.cache/bazel/_bazel_dreadbird/275612784e938fcd472711fffeaee420/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64: \\\r\n    PATH=/usr/local/cuda-10.0/bin:/home/dreadbird/bin:/home/dreadbird/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/dreadbird/bin:/home/dreadbird/bin \\\r\n  /home/dreadbird/.cache/bazel/_bazel_dreadbird/install/8212a63544dc9a497979f8d7c698aee5/_embedded_binaries/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15\r\nERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1: //tensorflow/python:pywrap_tensorflow_internal_py_wrap: missing input file '@nccl_archive//:src/collectives.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 163.877s, Critical Path: 40.98s\r\nINFO: 1714 processes: 1714 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n(2)\r\nERROR: missing input file '@nccl_archive//:src/nccl.h'\r\nERROR: /home/dreadbird/tensorflow/tensorflow/cc/BUILD:610:1: Creating runfiles tree bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc.runfiles [for host] failed: build-runfiles failed: error executing command \r\n  (cd /home/dreadbird/.cache/bazel/_bazel_dreadbird/275612784e938fcd472711fffeaee420/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64: \\\r\n    PATH=/usr/local/cuda-10.0/bin:/home/dreadbird/bin:/home/dreadbird/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/dreadbird/bin:/home/dreadbird/bin \\\r\n  /home/dreadbird/.cache/bazel/_bazel_dreadbird/install/8212a63544dc9a497979f8d7c698aee5/_embedded_binaries/build-runfiles bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc.runfiles_manifest bazel-out/host/bin/tensorflow/cc/ops/remote_fused_graph_ops_gen_cc.runfiles): Process terminated by signal 15: Process terminated by signal 15\r\nERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1: //tensorflow/python:pywrap_tensorflow_internal_py_wrap: missing input file '@nccl_archive//:src/nccl.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/dreadbird/tensorflow/tensorflow/python/BUILD:4790:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 0.861s, Critical Path: 0.19s\r\nINFO: 10 processes: 10 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n(3)\r\nERROR: /home/dreadbird/tensorflow/tensorflow/BUILD:537:1: Linking of rule '//tensorflow:libtensorflow_framework.so.1.14.0' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/dreadbird/.cache/bazel/_bazel_dreadbird/275612784e938fcd472711fffeaee420/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64: \\\r\n    PATH=/usr/local/cuda-10.0/bin:/home/dreadbird/bin:/home/dreadbird/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/dreadbird/bin:/home/dreadbird/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/host/bin/tensorflow/libtensorflow_framework.so.1.14.0 -Wl,--version-script,tensorflow/tf_framework_version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so.1 -pthread -pthread -pthread -pthread -Wl,-S -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/host/bin/tensorflow/libtensorflow_framework.so.1.14.0-2.params)\r\n\r\n", "comments": []}, {"number": 31219, "title": "'tensorflow/lite/c/c_api_internal.h' file not found", "body": "pod create the library MyLibray.framework    s.dependency 'TensorFlowLite'.\r\n\r\nbuild error in tensorlow_lite.framework.  #include \"tensorflow/lite/c/c_api_internal.h\"\r\n\r\nprompt:  'tensorflow/lite/c/c_api_internal.h' file not found\r\n\r\nIn my framework. just #include <tensorflow_lite/tensorflow/lite/kernels/register.h>\r\n\r\nwhy?\r\n\r\ni need to set up what? \r\n", "comments": ["@meiwuzhao Please provide the information asked in [Template](https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md). Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]