[{"number": 26255, "title": "Failed to get device properties, error code: 30", "body": "Failed to initialize GPU device #0: unknown error\r\n\r\nOs: windows 10\r\nTensorflow version: 1.13.1\r\nKeras Version: 2.1.6\r\nCUDA version: 10(major)\r\nCudnn version: 7.5\r\npython 3.6\r\n\r\n\r\nI got following error:\r\n\r\nWARNING:tensorflow:From C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nCreated base network...\r\n(1, 1)\r\nTensor(\"Shape:0\", shape=(2,), dtype=int32)\r\nTraining started...\r\nWARNING:tensorflow:From C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nEpoch 1/20\r\n2019-03-01 16:54:42.970057: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: CUB reduce errorinvalid configuration argument\r\n\t [[{{node dense_1/Sum}}]]\r\n\t [[{{node loss/add_1}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py\", line 370, in <module>\r\n    main(args)\r\n  File \"F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py\", line 333, in main\r\n    callbacks=[checkpointer, tbpointer])  # KERAS 2\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\", line 2230, in fit_generator\r\n    class_weight=class_weight)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\", line 1883, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2482, in __call__\r\n    **self.session_kwargs)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: CUB reduce errorinvalid configuration argument\r\n\t [[node dense_1/Sum (defined at C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1269) ]]\r\n\t [[node loss/add_1 (defined at C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py:848) ]]\r\n\r\nCaused by op 'dense_1/Sum', defined at:\r\n  File \"F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py\", line 370, in <module>\r\n    main(args)\r\n  File \"F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py\", line 274, in main\r\n    base_network = create_base_network_signet(input_shape)\r\n  File \"F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py\", line 139, in create_base_network_signet\r\n    seq.add(Dense(1024, kernel_regularizer=l2(0.0005), activation=\"relu\", kernel_initializer=\"glorot_uniform\"))\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\models.py\", line 522, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\topology.py\", line 592, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\layers\\core.py\", line 864, in build\r\n    constraint=self.kernel_constraint)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\topology.py\", line 418, in add_weight\r\n    self.add_loss(regularizer(weight))\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\regularizers.py\", line 42, in __call__\r\n    regularization += K.sum(self.l2 * K.square(x))\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1269, in sum\r\n    return tf.reduce_sum(x, axis, keepdims)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1286, in reduce_sum_v1\r\n    return reduce_sum(input_tensor, axis, keepdims, name)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1334, in reduce_sum\r\n    name=name))\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 10209, in _sum\r\n    name=name)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): CUB reduce errorinvalid configuration argument\r\n\t [[node dense_1/Sum (defined at C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1269) ]]\r\n\t [[node loss/add_1 (defined at C:\\Users\\SUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py:848) ]]\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\nI have tried updating tensorflow to 2.0-preview but that leads to other errors like module ConfigProto not found.\r\n\r\nPlease help to resolve this issue.\r\n", "comments": ["@Sawatdatta Could you provide more details on the issue and its context. If possible, provide a code to reproduce the bug. Thanks!", "@jvishnuvardhan  I have updated the TensorFlow to 2.0-preview which resolved this issue but Keras backend is using updated TensorFlow and causing no module errors.  Is there any way to force Keras to use compat.V1 modules?", "@Sawatdatta why do you want to force Keras to use compat.V1 modules? Thanks!", "> \r\n> \r\n> @Sawatdatta why do you want to force Keras to use compat.V1 modules? Thanks!\r\n\r\nSince Keras cannot find the modules(used in my code) in the 2.0-preview, and I don't want to change my existing code because not much documentation is available for tf-2.0-preview.", "I'm also seeing this unknown error.\r\n```\r\n2019-04-19 13:16:22.838705: E tensorflow/core/grappler/clusters/utils.cc:83] Failed to get device properties, error code: 30\r\nFailed to initialize GPU device #0: unknown error\r\n```\r\nMy configuration:\r\nWindows 10 Home\r\nTensorflow 1.13.1\r\nPython 3.5\r\nGTX 1060 Mobile Max-Q\r\n\r\nIt doesn't happen every time I run my program. I have localized it to running load_model from keras, before reaching that point I have imported tensorflow and verified that gpu is available.\r\n```python\r\n  if tf.test.is_gpu_available():\r\n        logger.debug(\"GPU is available\")\r\n```\r\nSwitching to tensorflow 2 alpha is not an option for me at this time. \r\n\r\nIs there a way to catch this error or check for it and recover/attempt to reinitialize?\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26255\">No</a>\n", "i have the same problem,and at the end i found that because i use double GPU ,one is intel GPU,another one is nvidia GPU,so i opened nvidia control panel,changed my prefer graphic processor to nvidia GPU instead of auto choose(auto choose intel GPU or nvidia GPU),that works for me ."]}, {"number": 26254, "title": "cuda 9 or 10", "body": "TF installation via pip is pointing to 1.13.1, requiring CUDA 10, while in the documentation it's mentioned CUDA 9\r\n\r\nIn order to install via pip for cuda 9 one should\r\n> pip3 install tensorflow-gpu==1.12.0\r\n\r\nsee\r\nhttps://github.com/tensorflow/tensorflow/issues/26209", "comments": ["@helonayala Could you point us where you are seeing CUDA 9 for TF 1.13.1? Please check release notes [here](https://github.com/tensorflow/tensorflow/releases/tag/v1.13.1) that mentions \"TensorFlow GPU binaries are now built against CUDA 10\". As the TF.1.13.1 was built against CUDA10, it will look for CUDA10 path. If you have CUDA9.0, please upgrade it to CUDA 10 and then install tensorflow. Thanks!", "On https://www.tensorflow.org/install/gpu it suggests CUDA 9.0 for Tensorflow generally, but doesn't specifically mention 1.13.1.\r\n\r\nI had issues using CUDA 10.1 with Tensorflow 1.13.1, but resolved them by switching to CUDA 10.0", "yep, in the link @blemke  provided", "Thanks @helonayala @blemke. Those instructions are for TF 1.12. Team is working on updating them for latest version. Stay tuned. Thanks!\r\n\r\nClosing this issue. ", "Same issue for me.\r\nThe cluster I use only contains Cuda 9.0, and when I reinstalled tensorflow it automatically installed 1.13.1, which didn't support Cuda 9.\r\nSo I downgraded the version to 1.12.2 and it worked.", "Hello together, \r\n\r\ndo I have to build tensorflow from source to get it running on a compute capability of 3.0?\r\nOr can i just install it with: \"pip3 install tensorflow-gpu==1.12.2\" ?", "> Hello together,\r\n> \r\n> do I have to build tensorflow from source to get it running on a compute capability of 3.0?\r\n> Or can i just install it with: \"pip3 install tensorflow-gpu==1.12.2\" ?\r\n\r\n@SteffenEpp use python3.5, then pip install tensorflow-gpu==1.12.2"]}, {"number": 26253, "title": "MPI call makes `//tensorflow:tf_python_api_gen_v1` fail when building tf-1.12.0 or 1.13.1 on Cray system", "body": "**System information**\r\n- Cray XC 50 (both login node and compute nodes)\r\n- Bazel-0.18.1 and 0.21.0\r\n- TensorFlow-1.12.0 and 1.13.1\r\n- Python-3.6.5\r\n- GCC-6.2.0\r\n- CUDA-9.1\r\n- cuDNN-7.1.4\r\n\r\n\r\n\r\n**Describe the problem**\r\nWe are encountering the following error when building TensorFlow-1.12.0 or 1.13.1:\r\n```\r\nERROR: /path/to/my/installation/tensorflow-1.13.1/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 255): bash failed: error executing command \r\n  (echo of the command that failed)\r\n```\r\nThis continues with the echo of the command that failed \r\n```\r\n/bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/python_api_1_tf_python_api_gen_v1 ...\r\n```\r\nan then at the end there is this\r\n```\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n[Fri Mar  1 09:31:29 2019] [unknown] Fatal error in PMPI_Init_thread: Other MPI error, error stack:\r\nMPIR_Init_thread(537): \r\nMPID_Init(246).......: channel initialization failed\r\nMPID_Init(638).......:  PMI2 init failed: 1 \r\naborting job:\r\nFatal error in PMPI_Init_thread: Other MPI error, error stack:\r\nMPIR_Init_thread(537): \r\nMPID_Init(246).......: channel initialization failed\r\nMPID_Init(638).......:  PMI2 init failed: 1 \r\n```\r\nOne gets this when trying to run an MPI program on the login node (which is the desired behavior in our system). The message appears twice as if something is trying to execute an MPI program with 2 ranks during the installation. I don't manage to find where this is coming from. The error also occurs if building on a compute node. Is there any option in Bazel to avoid this?\r\n\r\nThis doesn't happen for TensorFlow-1.11.0 or other older versions that we have build.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nOur choices on the `configure` phase for tf-1.13.1 (the same we use for other versions we have compiled):\r\n```\r\nDo you wish to build TensorFlow with XLA JIT support? No\r\nDo you wish to build TensorFlow with OpenCL SYCL support? No\r\nDo you wish to build TensorFlow with ROCm support? No\r\nDo you wish to build TensorFlow with CUDA support? Yes\r\nPlease specify the CUDA SDK version you want to use: 9.1\r\nPlease specify the cuDNN version you want to use: 7.1.4\r\nDo you wish to build TensorFlow with TensorRT support? No\r\nPlease specify the locally installed NCCL version you want to use: 2.2.13\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with: 6.0\r\nDo you want to use clang as CUDA compiler? No\r\nPlease specify which gcc should be used by nvcc as the host compiler: /path/to/gcc-6.2.0\r\nDo you wish to build TensorFlow with MPI support? No\r\nPlease specify optimization flags \"--config=opt\" : -march=native\r\nAndroid builds? No\r\n```\r\nThe bazel command is:\r\n```\r\nbazel build --verbose_failures --distinct_host_configuration=false --action_env=PYTHONPATH=$PYTHONPATH --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --cxxopt='-D_GLIBCXX_USE_CXX11_ABI=0' -c opt //tensorflow/tools/pip_package:build_pip_package && bazel-bin/tensorflow/tools/pip_package/build_pip_package $BUILDDIR\r\n```\r\n\r\nWe also edit `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` to replace the paths to the compilers, but this is always the same for every version that we build.", "comments": ["Found the problem. My installation of `h5py`, which is a dependency of `keras_applications`, seemed to have been corrupted. Reinstalling it solved the issue. Sorry for the misunderstanding.", "Thanks! I am closing the issue as it was resolved. Please open a new ticket if you see similar issue again. Thanks!\r\n"]}, {"number": 26252, "title": "Fix eigen_spatial_convolutions_test benchmarks", "body": "This commit fixes a crash in PackRhsHelper caused by a memory corruption\r\nerror.  The function contains a loop that populates two vectors, one\r\ncontaining input Tensors and the other containing InputMappers that point to\r\nthose input Tensors.  The problem is that the emplace_back call on the\r\nvector of input Tensors can cause that vector to grow which can\r\ninvalidate the pointers to the previously allocated input Tensors.\r\nUnfortunately, these invalidated pointers are still used by the InputMappers\r\nin the second vector and so when we use the InputMappers we get a crash.\r\nThe commit fixes the issue by reserving sufficient space in the input vector\r\nthereby preventing reallocations and invalidation of the pointers to the\r\nInput Tensors.\r\n\r\nAlthough the PackLhsHelper function does not crash on my machine it suffers\r\nfrom the same error and so this commit also contains a fix for that function.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/26251", "comments": ["Can one of the admins verify this patch?"]}, {"number": 26251, "title": "The eigen_spatial_convolutions_test benchmark crashes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): b'v1.8.0-14450-g9c38906' 1.13.1\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): g++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\n$ bazel test --config opt -- //tensorflow/core/kernels:eigen_spatial_convolutions_test\r\n\r\nStarting local Bazel server and connecting to it...\r\nDEBUG: /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nINFO: Analysed target //tensorflow/core/kernels:eigen_spatial_convolutions_test (78 packages loaded, 3781 targets configured).\r\nINFO: Found 1 test target...\r\nINFO: From Compiling tensorflow/core/kernels/eigen_spatial_convolutions_test.cc:\r\ntensorflow/core/kernels/eigen_spatial_convolutions_test.cc: In function 'void Eigen::PackRhsHelper(int, int, int, int, int, int, int, int, int, int, Eigen::Index, Eigen::Index)':\r\ntensorflow/core/kernels/eigen_spatial_convolutions_test.cc:1400:69: warning: typedef 'using Traits = struct Eigen::internal::gebp_traits<float, float>' locally defined but not used [-Wunused-local-typedefs]\r\n   using Traits = typename Eigen::internal::gebp_traits<float, float>;\r\n                                                                     ^\r\ntensorflow/core/kernels/eigen_spatial_convolutions_test.cc: In function 'void Eigen::PackLhsHelper(int, int, int, int, int, Eigen::Index, Eigen::Index)':\r\ntensorflow/core/kernels/eigen_spatial_convolutions_test.cc:1631:16: warning: variable 'nocontract_dim' set but not used [-Wunused-but-set-variable]\r\n   nocontract_t nocontract_dim = {0};\r\n                ^\r\ntensorflow/core/kernels/eigen_spatial_convolutions_test.cc:1632:14: warning: variable 'contract_dim' set but not used [-Wunused-but-set-variable]\r\n   contract_t contract_dim = {1};\r\n              ^\r\nTarget //tensorflow/core/kernels:eigen_spatial_convolutions_test up-to-date:\r\n  bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test\r\nINFO: Elapsed time: 36.190s, Critical Path: 31.21s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 25 processes: 25 local.\r\nINFO: Build completed successfully, 27 total actions\r\n//tensorflow/core/kernels:eigen_spatial_convolutions_test                PASSED in 0.2s\r\n\r\nINFO: Build completed successfully, 27 total actions\r\n\r\n$ bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test --benchmarks=all\r\nRunning main() from test_main.cc\r\nBenchmark                                                        Time(ns) Iterations\r\n------------------------------------------------------------------------------------\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/_U_S_Stensorflow_Score_Skernels_Ceigen_Uspatial_Uconvolutions_Utest___Utensorflow/libtensorflow_framework.so(+0x83085e)[0x7f470a1d785e]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f4708fdf390]\r\nbazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x40c6cc]\r\nbazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x412c58]\r\nbazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x41316f]\r\n/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/libtensorflow_Score_Slibtest.so(_ZN10tensorflow7testing9Benchmark3RunEiiPiPd+0x44)[0x7f470ab601e4]\r\n/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/libtensorflow_Score_Slibtest.so(_ZN10tensorflow7testing9Benchmark3RunEPKc+0x409)[0x7f470ab612d9]\r\n/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/libtensorflow_Score_Slibtest_Umain.so(main+0xb3)[0x7f470ab65b83]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f4708395830]\r\nbazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x409649]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace[abi:cxx11]()\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\ttensorflow::testing::Benchmark::Run(int, int, int*, double*)\r\n\ttensorflow::testing::Benchmark::Run(char const*)\r\n\tmain\r\n\t__libc_start_main\r\n\t\r\n*** End stack trace ***\r\nAborted (core dumped)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe benchmark should run correctly and should not crash.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n$ bazel test --config opt -- //tensorflow/core/kernels:eigen_spatial_convolutions_test\r\n$ bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test --benchmarks=all\r\n```\r\n\r\nThe issue is 100% reproducible on my machine\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26251\">No</a>\n"]}, {"number": 26250, "title": "Tf.Keras metrics issue ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): Source ( Pip ) \r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6.7\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n> 1.13.1\r\n\r\n**Describe the current behavior**\r\nI need to use Keras metric while compiling an LSTM model. it is getting compiled. But when I started to train I am getting error.\r\n\r\nmy code looks as follows : \r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM (120,activation = \"tanh\", input_shape=(timesteps,dim), return_sequences=True))\r\nmodel.add(LSTM(120, activation = \"tanh\", return_sequences=True))\r\nmodel.add(LSTM(120, activation = \"tanh\", return_sequences=True))\r\nmodel.add(LSTM(120, activation = \"tanh\", return_sequences=True))\r\nmodel.add(LSTM(120, activation = \"tanh\", return_sequences=True))\r\nmodel.add(LSTM(120, activation = \"tanh\", return_sequences=True))\r\nmodel.add(Dense(dim))\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\",  metrics=[tf.keras.metrics.Precision()])\r\n\r\nhistory = model.fit(data,data, \r\n                    epochs=100,\r\n                    batch_size=10,\r\n                    validation_split=0.2,\r\n                    shuffle=True,\r\n                    callbacks=[ch]).history\r\n```\r\n\r\nThere error I am getting as follows\r\n\r\n\r\n> InvalidArgumentError: assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:x (dense_3/BiasAdd:0) = ] [[[2.72658144e-06 1.17555362e-06 1.96436554e-06...]]...] [y (metrics_3/precision_1/Cast/x:0) = ] [0] [[{{node metrics_3/precision_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\r\n\r\n", "comments": ["Assigning to Pavithra who is the owner of metric/loss.", "@AkbarAlam Precision metric takes predictions as probabilities, hence the error `predictions must be >= 0`. For this you will need to add `sigmoid` (if dim == 1) or `softmax` (for dim > 1) activation function to the last dense layer. ", "@pavithrasv Thanks for the input. Let me check if that  works ", "@pavithrasv I dont know if this is the right config. But as you suggested I have done it\r\n \r\nit looks as follows\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM (120,activation = \"tanh\", input_shape=(timesteps,dim), return_sequences=True))\r\nmodel.add(LSTM(120, return_sequences=True))\r\nmodel.add(LSTM(120, return_sequences=True))\r\nmodel.add(LSTM(120, return_sequences=True))\r\nmodel.add(LSTM(120, return_sequences=True))\r\nmodel.add(LSTM(120, return_sequences=True))\r\nmodel.add(Dense(dim, activation=\"softmax\"))\r\n```\r\n\r\n", "Output of the LSTM layer will be 3D (samples, timesteps, features) because of return_sequences being enabled. Are the labels/targets in your data 3D ?", "@AkbarAlam Is this resolved? If it was not resolved, please provide a  simple standalone code to reproduce the issue. Thanks!", "@AkbarAlam Did you try @pavithrasv suggestion? If this was not resolved, can you please share a standalone code to reproduce the issue? Thanks!", "I am so sorry guys for late response. The project was over long ago and the use case was changed. Right now I can't reproduce it anymore ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26250\">No</a>\n"]}, {"number": 26249, "title": "Compiling TF 1.12 failes under CentOS 6: name 'http_archive' is not defined", "body": "**System information**\r\n- _CentOS release 6.9 (Final), Kernel: 4.15.0-45-generic, x86_64_\r\n- TensorFlow installed from (source or binary): _source_\r\n- TensorFlow version: _1.12.0_\r\n- Python version: _3.6.8, 64bit_\r\n- Bazel version (if compiling from source): _Build label: 0.23.0- (@non-git)_ \r\n   actually Bazel, is build from source too.\r\n- GCC/Compiler version (if compiling from source): _gcc (GCC) 6.3.1 20170216 (Red Hat 6.3.1-3)_\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to use TensorFlow under the system (actually inside a Docker container) specified above (the Linux/GCC/Python versions are fixed).  I get the following error while executing ` bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n    ERROR: /usr/src/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined\r\n    ERROR: Error evaluating WORKSPACE file\r\n    ERROR: error loading package '': Encountered error while reading extension \r\n        file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error\r\n        loading package 'external': Could not load //external package\r\n    ERROR: error loading package '': Encountered error while reading extension \r\n        file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error \r\n        loading package 'external': Could not load //external package\r\n    INFO: Elapsed time: 1.276s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n    FROM amd64/centos:6.9\r\n    \r\n    RUN yum -y install gcc openssl-devel bzip2-devel wget centos-release-scl\r\n    \r\n    # Install bazel\r\n    WORKDIR /usr/src\r\n    RUN yum -y install java-1.8.0-openjdk-devel devtoolset-6\r\n    \r\n    RUN wget https://github.com/bazelbuild/bazel/releases/download/0.23.0/bazel-0.23.0-dist.zip \\\r\n        && unzip bazel-*.zip -d /bazel\r\n    \r\n    # Set PATH, because \"scl enable\" does not have any effects to \"docker build\"\r\n    ENV PATH /opt/rh/devtoolset-6/root/usr/bin:$PATH\r\n    \r\n    RUN cd /bazel \\\r\n        && scl enable devtoolset-6 bash \\\r\n        && ./compile.sh \\\r\n        && mkdir -p /my/bin/ \\\r\n        && cp output/bazel /my/bin \\\r\n        && exit\r\n    \r\n    # Install Python new build\r\n    RUN wget https://www.python.org/ftp/python/3.6.8/Python-3.6.8.tgz\r\n    RUN tar xzf Python-3.6.8.tgz\r\n    \r\n    WORKDIR /usr/src/Python-3.6.8\r\n    RUN ./configure --enable-optimizations\r\n    RUN make altinstall\r\n    \r\n     # Install pip\r\n    RUN wget https://bootstrap.pypa.io/get-pip.py\r\n    RUN python3.6 get-pip.py\r\n    \r\n    RUN pip3 install -U pip six numpy wheel mock \\\r\n        && pip3 install -U keras_applications==1.0.6 --no-deps \\\r\n        && pip3 install -U keras_preprocessing==1.0.5 --no-deps\r\n    \r\n    # Build Tensorflow\r\n    WORKDIR /usr/src/\r\n    RUN yum -y install git\r\n\r\n    RUN git clone https://github.com/tensorflow/tensorflow.git \\\r\n        && cd /usr/src/tensorflow \\\r\n        && git checkout v1.12.0 \r\n    \r\n    WORKDIR /usr/src/tensorflow\r\n    RUN scl enable devtoolset-6 bash \\\r\n       && export \\\r\n          PATH=$PATH:/my/bin \\\r\n          PYTHON_BIN_PATH=/usr/local/bin/python3.6 \\\r\n          PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/ \\\r\n          TF_NEED_JEMALLOC=0 \\\r\n          TF_NEED_GCP=0 \\\r\n          TF_NEED_HDFS=0 \\\r\n          TF_NEED_S3=0 \\\r\n          TF_ENABLE_XLA=0 \\\r\n          TF_NEED_GDR=0 \\\r\n          TF_NEED_VERBS=0 \\\r\n          TF_NEED_OPENCL=0 \\\r\n          TF_NEED_CUDA=0 \\\r\n          TF_NEED_MPI=0 \\\r\n       && ./configure \\\r\n       && echo \"import /usr/src/tensorflow/tools/bazel.rc\" > /usr/src/tensorflow/.bazelrc \\\r\n       && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \\\r\n       && mkdir -p /tmp/tensorflow_pkg \\\r\n       && bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\r\n       && exit\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n    [Non relevant Docker cache logs above]\r\n    \r\n    Step 21/21 : RUN scl enable devtoolset-6 bash    && export       PATH=$PATH:/my/bin       PYTHON_BIN_PATH=/usr/local/bin/python3.6       PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/       TF_NEED_JEMALLOC=0       TF_NEED_GCP=0       TF_NEED_HDFS=0       TF_NEED_S3=0       TF_ENABLE_XLA=0       TF_NEED_GDR=0       TF_NEED_VERBS=0       TF_NEED_OPENCL=0       TF_NEED_CUDA=0       TF_NEED_MPI=0    && ./configure    && echo \"import /usr/src/tensorflow/tools/bazel.rc\" > /usr/src/tensorflow/.bazelrc    && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package    && exit\r\n     ---> Running in e006e5ffc788\r\n    WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n    /usr/src/tensorflow/tools/bazel.rc\r\n    Extracting Bazel installation...\r\n    WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n    You have bazel 0.23.0- (@non-git) installed.\r\n    Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: Apache Ignite support will be enabled for TensorFlow.\r\n    \r\n    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.\r\n    \r\n    Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n    \r\n    Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n    \r\n    Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n    \r\n    Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n    \r\n    Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n    \t--config=mkl         \t# Build with MKL support.\r\n    \t--config=monolithic  \t# Config for mostly static monolithic build.\r\n    \t--config=gdr         \t# Build with GDR support.\r\n    \t--config=verbs       \t# Build with libverbs support.\r\n    \t--config=ngraph      \t# Build with Intel nGraph support.\r\n    Configuration finished\r\n    Starting local Bazel server and connecting to it...\r\n    Loading: \r\n    Loading: 0 packages loaded\r\n    ERROR: /usr/src/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined\r\n    ERROR: Error evaluating WORKSPACE file\r\n    ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\n    ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\n    INFO: Elapsed time: 1.276s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n    ERROR: Service 'mysys_replica' failed to build: The command '/bin/sh -c scl enable devtoolset-6 bash    && export       PATH=$PATH:/my/bin       PYTHON_BIN_PATH=/usr/local/bin/python3.6       PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/       TF_NEED_JEMALLOC=0       TF_NEED_GCP=0       TF_NEED_HDFS=0       TF_NEED_S3=0       TF_ENABLE_XLA=0       TF_NEED_GDR=0       TF_NEED_VERBS=0       TF_NEED_OPENCL=0       TF_NEED_CUDA=0       TF_NEED_MPI=0    && ./configure    && echo \"import /usr/src/tensorflow/tools/bazel.rc\" > /usr/src/tensorflow/.bazelrc    && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package    && exit' returned a non-zero code: 1\r\n", "comments": ["I have the exact same problem on Ubuntu 18.04. I hope my steps help as well.\r\n```\r\n# Python Dependencies\r\nsudo apt install python3-dev python3-pip\r\npip3 install -U --user six numpy wheel mock\r\npip3 install -U --user keras_applications==1.0.6 --no-deps\r\npip3 install -U --user keras_preprocessing==1.0.5 --no-deps\r\n\r\n# Installing Bazel\r\nsudo apt-get install pkg-config zip g++ zlib1g-dev unzip python\r\nwget https://github.com/bazelbuild/bazel/releases/download/0.23.0/bazel-0.23.0-installer-linux-x86_64.sh\r\nchmod +x bazel-0.23.0-installer-linux-x86_64.sh\r\n./bazel-0.23.0-installer-linux-x86_64.sh --user\r\n\r\n# Downloading TensorFlow source code\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.12\r\n\r\n# Configure the build\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```", "Same issue for me, on CentOS 7.", "I was able to get past this by adding the following to the `WORKSPACE` file, prior to the `http_archive` call:\r\n\r\n```\r\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\")\r\n```", "I found a workaround by using an older version of bazel(0.17.2)\r\nHope this helps.", "Thank you,\r\n\r\nfor both workarounds more errors appear. I think there is something not configured correctly in my case...\r\n\r\n@joshpencheon \r\n> I was able to get past this by adding the following to the `WORKSPACE` file, prior to the `http_archive` call:\r\n> \r\n> ```\r\n> load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\")\r\n> ```\r\n\r\nAfterwards I get the following\r\n\r\n    starting local Bazel server and connecting to it...\r\n    Loading: \r\n    Loading: 0 packages loaded\r\n    ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/io_bazel_rules_closure/closure/compiler/closure_js_deps.bzl:82:17: Traceback (most recent call last):\r\n    \tFile \"/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/io_bazel_rules_closure/closure/compiler/closure_js_deps.bzl\", line 78\r\n    \t\trule(implementation = _impl, attrs = {\"...\")}, ...\"})\r\n    \tFile \"/root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/io_bazel_rules_closure/closure/compiler/closure_js_deps.bzl\", line 82, in rule\r\n    \t\tattr.label_list(cfg = \"data\", allow_files = True)\r\n    Using cfg = \"data\" on an attribute is a noop and no longer supported. Please remove it. You can use --incompatible_disallow_data_transition=false to temporarily disable this check.\r\n    ERROR: error loading package '': in /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/io_bazel_rules_closure/closure/defs.bzl: Extension file 'closure/compiler/closure_js_deps.bzl' has errors\r\n    ERROR: error loading package '': in /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/io_bazel_rules_closure/closure/defs.bzl: Extension file 'closure/compiler/closure_js_deps.bzl' has errors\r\n    INFO: Elapsed time: 1.884s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n    FAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\n\r\n@KasparPeterson For the second workaround\r\n> I found a workaround by using an older version of bazel(0.17.2)\r\n> Hope this helps.\r\n\r\n\r\n Now I get the following error (with my original bazel version 0.23.0)\r\n\r\n     ERROR: /usr/src/tensorflow/tensorflow/contrib/kafka/BUILD:36:1: no such package '@kafka//': Traceback (most recent call last):\r\n    \tFile \"/usr/src/tensorflow/third_party/repo.bzl\", line 106\r\n    \t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n    \tFile \"/usr/src/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n    \t\tfail(\"patch command is not found, ple...\")\r\n    patch command is not found, please install it and referenced by '//tensorflow/contrib/kafka:dataset_kernels'\r\n    ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@kafka//': Traceback (most recent call last):\r\n    \tFile \"/usr/src/tensorflow/third_party/repo.bzl\", line 106\r\n    \t\t_apply_patch(ctx, ctx.attr.patch_file)\r\n    \tFile \"/usr/src/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n    \t\tfail(\"patch command is not found, ple...\")\r\n    patch command is not found, please install it\r\n    INFO: Elapsed time: 8.806s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully (261 packages loaded)\r\n    FAILED: Build did NOT complete successfully (261 packages loaded)\r\n\r\n", "@eugenmk yes I got stuck going down that route, and followed @KasparPeterson 's approach of downgrading `bazel` instead. \r\n\r\n```\r\npatch command is not found, please install it\r\n```\r\n\r\nShould be resolveable with `yum install patch`.\r\n\r\nI'm now having problems along the lines of tensorflow/tensorflow#15129\r\n\r\n```\r\nundefined reference to `clock_gettime'\r\n```\r\n\r\n...and have as yet been unable to resolve; I think possibly because of anaconda including a different version of `gcc`.", "Using the dowgraded bazel with installed `patch` helped me forward. \r\nI didn't get the error about undefined reference yet. but after some other fixes I'm at the the following now\r\n\r\n    ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external\r\n        /lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed (Exit 1)\r\n    cc1: error: unrecognized command line option \"-fcolor-diagnostics\"\r\n    cc1: error: unrecognized command line option \"-fno-canonical-system-headers\"\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n    Use --verbose_failures to see the command lines of failed build steps.\r\n    INFO: Elapsed time: 58.638s, Critical Path: 0.12s\r\n    INFO: 0 processes.\r\n    FAILED: Build did NOT complete successfully\r\n    FAILED: Build did NOT complete successfully\r\n\r\nThere are hints to upgrade GCC, but this is not possible for my case. I'm relying on the specified version.", "I've finally got the build to succeed, and didn't come across that issue @eugenmk - but perhaps `\r\nbazel clean -expunge`, as described on tensorflow/tensorflow#15849, might help?", "Had the same exact problem while building graph_transforms tool from r1.11 with latest bazel 0.23.0, solved by using bazel 0.17.2", "@eugenmk Could you follow the suggestion listed by the users to resolve the issue. Some time it is better to build it freshly by uninstalling and reinstalling. If it was resolved, then please close the issue. Thanks!", "I achieved the same error as @joshpencheon \r\n\r\n    undefined reference to `clock_gettime'\r\n\r\nFor solving it, I needed to add ['-lrt'] at different locations of `tensorflow/tensorflow.bzl` file (not sure if all locations are required). Now it compiles, but the tests fail to compile with same link error. At least I can install the python package and import it. Still I have to check if Tensorflow works properly.\r\n\r\nThe changes I did to the `tensorflow/tensorflow.bzl`\r\n\r\nCurrently I do them manually inside the docker container. Actually I change all the `linkopts` at lowest level in the file.\r\n\r\ntensorflow/tensorflow.bzl:\r\n\r\n\tdef tf_cc_shared_object(\r\n\t\tname,\r\n\t\tsrcs = [],\r\n\t\tdeps = [],\r\n\t\tdata = [],\r\n\t\tlinkopts = [],\r\n\t\tframework_so = tf_binary_additional_srcs(),\r\n\t\tkernels = [],\r\n\t\t**kwargs):\r\n\t    native.cc_binary(\r\n\t\tname = name,\r\n\t\tsrcs = srcs + framework_so,\r\n\t\tdeps = deps + tf_binary_dynamic_kernel_deps(kernels),\r\n\t\tlinkshared = 1,\r\n\t\tdata = data + tf_binary_dynamic_kernel_dsos(kernels),\r\n\t\tlinkopts = linkopts + _rpath_linkopts(name) + select({\r\n\t\t    clean_dep(\"//tensorflow:darwin\"): [\r\n\t\t        \"-Wl,-install_name,@rpath/\" + name.split(\"/\")[-1],\r\n\t\t    ],\r\n\t\t    clean_dep(\"//tensorflow:windows\"): [],\r\n\t\t    \"//conditions:default\": [\r\n\t\t        \"-Wl,-soname,\" + name.split(\"/\")[-1],\r\n\t\t    ],\r\n\t-        }),\r\n\t+        }) + ['-lrt'],\r\n\t\t**kwargs\r\n\t    )\r\n\r\n\t--------------------------\r\n\tef tf_cc_binary(\r\n\t\tname,\r\n\t\tsrcs = [],\r\n\t\tdeps = [],\r\n\t\tdata = [],\r\n\t\tlinkopts = [],\r\n\t\tcopts = tf_copts(),\r\n\t\tkernels = [],\r\n\t\t**kwargs):\r\n\t    native.cc_binary(\r\n\t\tname = name,\r\n\t\tcopts = copts,\r\n\t\tsrcs = srcs + tf_binary_additional_srcs(),\r\n\t\tdeps = deps + tf_binary_dynamic_kernel_deps(kernels) + if_mkl_ml(\r\n\t\t    [\r\n\t\t        clean_dep(\"//third_party/mkl:intel_binary_blob\"),\r\n\t\t    ],\r\n\t\t),\r\n\t\tdata = data + tf_binary_dynamic_kernel_dsos(kernels),\r\n\t-        linkopts = linkopts + _rpath_linkopts(name),\r\n\t+        linkopts = linkopts + _rpath_linkopts(name) + ['-lrt'],\r\n\t\t**kwargs\r\n\t    )\r\n\r\n\t--------------------------\r\n\r\n\tdef tf_cc_test(\r\n\t\t[...]\r\n\t   native.cc_test(\r\n\t\tname = \"%s%s\" % (name, suffix),\r\n\t\tsrcs = srcs + tf_binary_additional_srcs(),\r\n\t\tcopts = tf_copts() + extra_copts,\r\n\t\tlinkopts = select({\r\n\t\t    clean_dep(\"//tensorflow:android\"): [\r\n\t\t        \"-pie\",\r\n\t\t    ],\r\n\t\t    clean_dep(\"//tensorflow:windows\"): [],\r\n\t\t    clean_dep(\"//tensorflow:darwin\"): [\r\n\t\t        \"-lm\",\r\n\t\t    ],\r\n\t\t    \"//conditions:default\": [\r\n\t\t        \"-lpthread\",\r\n\t\t        \"-lm\",\r\n\t\t    ],\r\n\t-        }) + linkopts + _rpath_linkopts(name),\r\n\t+        }) + linkopts + _rpath_linkopts(name) + ['-lrt'],\r\n\r\n\r\n\t--------------------------\r\n\tdef tf_cuda_only_cc_test(\r\n\t\t...\r\n\t\tdeps = deps + tf_binary_dynamic_kernel_deps(kernels) +\r\n\t\t       if_cuda_is_configured([\r\n\t\t           clean_dep(\"//tensorflow/core:cuda\"),\r\n\t\t           clean_dep(\"//tensorflow/core:gpu_lib\"),\r\n\t\t       ]) +\r\n\t\t       if_rocm_is_configured([\r\n\t\t           clean_dep(\"//tensorflow/core:gpu_lib\"),\r\n\t\t       ]),\r\n\t-        linkopts = if_not_windows([\"-lpthread\", \"-lm\"]) + linkopts + _rpath_linkopts(name) + ['-lrt'],\r\n\t+        linkopts = if_not_windows([\"-lpthread\", \"-lm\"]) + linkopts + _rpath_linkopts(name),\r\n\t\tlinkstatic = linkstatic or select({\r\n\t\t    # cc_tests with \".so\"s in srcs incorrectly link on Darwin\r\n\t\t    # unless linkstatic=1.\r\n\t\t    # TODO(allenl): Remove Mac static linking when Bazel 0.6 is out.\r\n\t\t    clean_dep(\"//tensorflow:darwin\"): 1,\r\n\t\t    \"//conditions:default\": 0,\r\n\t\t}),\r\n\t\ttags = tags + tf_cuda_tests_tags(),\r\n\t    )\r\n\r\n\t--------------------------\r\n\r\n\tdef tf_cc_test_mkl(\r\n\t\t...\r\n\t native.cc_test(\r\n\t\t    name = src_to_test_name(src),\r\n\t\t    srcs = if_mkl([src]) + tf_binary_additional_srcs(),\r\n\t\t    copts = tf_copts(),\r\n\t\t    linkopts = select({\r\n\t\t        clean_dep(\"//tensorflow:android\"): [\r\n\t\t            \"-pie\",\r\n\t\t        ],\r\n\t\t        clean_dep(\"//tensorflow:windows\"): [],\r\n\t\t        \"//conditions:default\": [\r\n\t\t            \"-lpthread\",\r\n\t\t            \"-lm\",\r\n\t\t        ],\r\n\t-            }) + _rpath_linkopts(src_to_test_name(src)),\r\n\t+            }) + _rpath_linkopts(src_to_test_name(src)) + ['-lrt'],\r\n\t\t    deps = deps + tf_binary_dynamic_kernel_deps(kernels) + mkl_deps(),\r\n\t\t    data = data + tf_binary_dynamic_kernel_dsos(kernels),\r\n\t\t    linkstatic = linkstatic,\r\n\r\nMy current Dockerfile:\r\n\r\n\tFROM amd64/centos:6.9\r\n\r\n\tRUN yum -y install gcc openssl-devel bzip2-devel wget centos-release-scl\r\n\r\n\t# Install bazel\r\n\t# source: https://blog.abysm.org/2016/06/building-tensorflow-centos-6/\r\n\tWORKDIR /usr/src\r\n\tRUN yum -y install java-1.8.0-openjdk-devel devtoolset-6\r\n\r\n\tRUN wget https://github.com/bazelbuild/bazel/releases/download/0.17.2/bazel-0.17.2-dist.zip \\\r\n\t#RUN wget https://github.com/bazelbuild/bazel/releases/download/0.23.0/bazel-0.23.0-dist.zip \\\r\n\t    && unzip bazel-*.zip -d /bazel\r\n\r\n\tRUN gcc --version -v\r\n\t# Set PATH, because \"scl enable\" does not have any effects to \"docker build\"\r\n\tENV PATH /opt/rh/devtoolset-6/root/usr/bin:$PATH\r\n\r\n\tRUN cd /bazel \\\r\n\t    && scl enable devtoolset-6 bash \\\r\n\t    && ./compile.sh \\\r\n\t    && mkdir -p /my/bin/ \\\r\n\t    && cp output/bazel /my/bin \\\r\n\t    && exit\r\n\r\n\t# Install Python new build\r\n\tRUN wget https://www.python.org/ftp/python/3.6.8/Python-3.6.8.tgz\r\n\tRUN tar xzf Python-3.6.8.tgz\r\n\r\n\tWORKDIR /usr/src/Python-3.6.8\r\n\tRUN ./configure --enable-optimizations\r\n\tRUN make altinstall\r\n\r\n\t # Install pip\r\n\tRUN wget https://bootstrap.pypa.io/get-pip.py\r\n\tRUN python3.6 get-pip.py\r\n\r\n\tRUN pip3 install -U pip six numpy wheel mock \\\r\n\t    && pip3 install -U keras_applications==1.0.6 --no-deps \\\r\n\t    && pip3 install -U keras_preprocessing==1.0.5 --no-deps\r\n\r\n\t# ... Build Tensorflow\r\n\tWORKDIR /usr/src/\r\n\tRUN yum -y install git\r\n\tRUN git clone https://github.com/tensorflow/tensorflow.git \\\r\n\t    && cd /usr/src/tensorflow \\\r\n\t    && git checkout v1.12.0 \r\n\r\n\tRUN yum -y install patch g++\r\n\r\n\tRUN ln -s /usr/local/bin/python3.6 /usr/local/bin/python\r\n\r\n\tWORKDIR /usr/src/tensorflow\r\n        ###### NOTE\r\n       # AFTER editing of the tensorflow.bzl\r\n      # NEED to execute manually the following steps inside the Docker container. \r\n\tRUN scl enable devtoolset-6 bash \\\r\n\t   && export \\\r\n\t#      PATH=/usr/lib/gcc/x86_64-redhat-linux/4.4.7:/usr/local/bin:$PATH:/my/bin \\\r\n\t      PATH=/usr/local/bin:$PATH:/my/bin \\\r\n\t      PYTHON_BIN_PATH=/usr/local/bin/python3.6 \\\r\n\t      PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/ \\\r\n\t      TF_NEED_JEMALLOC=0 \\\r\n\t      TF_NEED_GCP=0 \\\r\n\t      TF_NEED_HDFS=0 \\\r\n\t      TF_NEED_S3=0 \\\r\n\t      TF_ENABLE_XLA=0 \\\r\n\t      TF_NEED_GDR=0 \\\r\n\t      TF_NEED_VERBS=0 \\\r\n\t      TF_NEED_OPENCL=0 \\\r\n\t      TF_NEED_CUDA=0 \\\r\n\t      TF_NEED_MPI=0 \\\r\n\t   && ./configure \\\r\n\t   && sed -i '1i import /usr/src/tensorflow/tools/bazel.rc' /usr/src/tensorflow/.bazelrc \\\r\n\t   && sed -i '2i load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\")' /usr/src/tensorflow/WORKSPACE \\\r\n           # TODO: Provide the appropriate target architecture.\r\n\t   # && bazel build --march=XXX -c opt //tensorflow/tools/pip_package:build_pip_package \\\r\n\t   && bazel clean --expunge \\\r\n\t#   && bazel build --linkopt='-lrt' -c opt //tensorflow/tools/pip_package:build_pip_package \\\r\n\t#   && mkdir -p /tmp/tensorflow_pkg \\\r\n\t#   && bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \\\r\n\t#   && exit\r\n\t      \r\n I don't want to close the issue, since it not really tested. And I want to provide the Dockerfile for others.\r\n\r\nHope can do it in couple of days.", "I believe you need an older version of bazel for compiling 1.12 branch.\r\nPlease see here for the configurations we tested for building from sources:\r\nhttps://www.tensorflow.org/install/source#linux", "After 1.12, our configure script reports what is the minimum and maximum version of bazel you need to use to build TF.\r\nSorry for the inconvenience, and hopefully we do not run into such issues moving forward.", "For everyone reading this from Google, in case you're not familiar with/are using HomeBrew, I fixed it with:\r\n\r\n```\r\nbrew uninstall bazel\r\nbrew install https://raw.githubusercontent.com/Homebrew/homebrew-core/b729cc3bcf6feeb1f49a5f24a509db39926bf1c9/Formula/bazel.rb\r\n```"]}, {"number": 26248, "title": "Just want to confirm if this failure is intended", "body": "I have a model acts like the test case attached. If tested with tf-v1.8 (which I previously worked on), incorrect dims will be output. If tested with *master*, a recently added assertion in `tensorflow/lite/simple_memory_arena.cc:100` will be triggered.\r\n\r\nTo reproduce the error, posting test case is simpler. So even if this PR is more like an issue report, please kindly allow me to post here and ask if model like this is a misuse? Or it is not supported by TFLite? Thanks.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26248) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26248) for more info**.\n\n<!-- ok -->", "Is there any chance you can attach your model as well? There can be subtle differences between a loaded model and trying to manually construct it in a test.", "Sure. I just realized there is Python front-end for TFLite now :) So it is possible to reproduce it in single Python script:\r\n\r\nOS: Ubuntu Linux 18.04 LTS\r\nPython & TF version:\r\n\r\n```\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.3.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow                                                                                                                                                                                   \r\n\r\nIn [2]: tensorflow.__version__                                                                                                                                                                              \r\nOut[2]: '1.13.1'\r\n```\r\n\r\nReproduce steps:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ni = tf.placeholder(dtype=tf.float32, shape=[2, 2, 1, 1])\r\np = tf.placeholder(dtype=tf.int32, shape=[4, 2])\r\na = tf.placeholder(dtype=tf.float32, shape=[1])\r\n\r\nn = tf.negative(a)\r\no = tf.add(tf.pad(i, p), n)\r\n\r\ntest_input = np.array([[[[0]], [[0]]],[[[0]], [[0]]]], dtype=np.float32)\r\ntest_pad = np.array([[2,2],[2,2],[0,0],[0,0]], dtype=np.int32)\r\ntest_pad_2 = np.array([[4,4],[4,4],[0,0],[0,0]], dtype=np.int32)\r\ntest_a = np.array([1], dtype=np.float32)\r\n\r\n\r\ndef test_tf():\r\n    with tf.Session() as sess:\r\n        out = sess.run(o, {i: test_input, p: test_pad, a: test_a})\r\n        assert out.shape == (6, 6, 1, 1)\r\n\r\n        out = sess.run(o, {i: test_input, p: test_pad_2, a: test_a})\r\n        assert out.shape == (10, 10, 1, 1)\r\n\r\ntest_tf()\r\n\r\n\r\n# convert\r\nwith tf.Session() as sess:\r\n    conv = tf.lite.TFLiteConverter.from_session(sess, [i, p, a], [o])\r\n    lite_model_bytes = conv.convert()\r\n\r\n\r\ndef test_tflite():\r\n    interp = tf.lite.Interpreter(model_content=lite_model_bytes)\r\n\r\n    #### I just hard-coded them:\r\n    # print(interp.get_input_details())\r\n    # print(interp.get_output_details())\r\n\r\n    interp.allocate_tensors()\r\n    interp.set_tensor(3, test_input)\r\n    interp.set_tensor(4, test_pad)\r\n    interp.set_tensor(5, test_a)\r\n\r\n    interp.invoke()\r\n    assert interp.get_tensor(0).shape == (6, 6, 1, 1)\r\n\r\n    interp.set_tensor(4, test_pad_2)\r\n\r\n    interp.invoke()\r\n    assert interp.get_tensor(0).shape == (10, 10, 1, 1)\r\n\r\ntest_tflite()\r\n```\r\n", "Awesome, thanks for the minimal repro! Just to confirm, is [this the line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/simple_memory_arena.cc#L100) that is failing in your local repro?\r\n\r\nIf this graph works properly in TF, then it should also behave properly in TFLite, so it's most likely a bug or an issue with conversion. Feel free to file a bug in parallel while we investigate.", "> Awesome, thanks for the minimal repro! Just to confirm, is [this the line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/simple_memory_arena.cc#L100) that is failing in your local repro?\r\n\r\nYes. I got following console output:\r\n\r\n```\r\nRuntimeError: tensorflow/lite/simple_memory_arena.cc:100 erased_allocs_count != 1 (0 != 1)\r\n```\r\n\r\nIf changing input and paddings to other reasonable values, we can get similar error like this one:\r\n\r\n```\r\nRuntimeError: tensorflow/lite/simple_memory_arena.cc:93 it->size != alloc.size (36 != 4)\r\n```\r\n\r\n> If this graph works properly in TF, then it should also behave properly in TFLite, so it's most likely a bug or an issue with conversion. Feel free to file a bug in parallel while we investigate.\r\n\r\nIMHO arena is not properly reset in some cases. If we force arena to re-compute before every invocation, this issue won't happen. But it is kind ugly...\r\n", "So, I'm not able to repro the error from your Python test case, but I do see it with the native unit test case. Looking into it now.", "I'm going to close the PR, but we're tracking this internally. If you want to file a separate issue that would be helpful, as it very likely could be a legit bug.", "Ops have to be in specific order to trigger this, so we probably still need the model file to figure out. I posted the issue and model file here: #26549."]}, {"number": 26247, "title": "wav_io needs to accept wav files which has 'JUNK' chunk before 'fmt ' chunk", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Not yet decided\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSometimes we encounter wav files which has `JUNK` chunk before `fmt `chunk. CPython's wave module handles this kind of wav files [by skipping those unnecessary chunk](https://github.com/python/cpython/blob/3.6/Lib/wave.py#L125). But tensorflow's wav_io module [expects `fmt ` chunk comes immediately after `WAVE` tag](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/wav/wav_io.cc#L220). As a consequence, those wav files with `JUNK` chunk, which is playable by media file players, generates *Header mismatch* InvalidArgument error.\r\n\r\nAs a conclusion, we need to implement, I may call it as, unnecessary chunk skipping functionality on wav_io.\r\n\r\n**Will this change the current api? How?**\r\nI expect that api won't be affected at all.\r\n\r\n**Who will benefit with this feature?**\r\nThose who are handling wide variety of wav files would be benefit with this feature, as they do not have to do some preprocessing(removing `JUNK` chunk manually all the time).\r\n\r\n**Any Other info.**\r\n[Kaldi toolkit's handling of wav input](https://github.com/kaldi-asr/kaldi/blob/master/src/feat/wave-reader.cc#L135) would be a good reference.", "comments": ["@combacsa Do you have a small sample file with `JUNK` chunk? I can help create a fix if there is a sample file I could verify.", "@combacsa Could you share a sample file as requested by @yongtang . If it was resolved with latest TF version, please close the issue. Thanks!", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "@jvishnuvardhan this issue should be reopened. This is still an issue in `tfio.IOTensor.from_audio(path)` and `tf.audio.decode_wav(tf.io.read_file(path))` on the latest stable release of TensorFlow and TensorFlow I/O.\r\n\r\n**tf.audio.decode_wav**\r\n>InvalidArgumentError: Header mismatch: Expected fmt  but found JUNK [Op:DecodeWav]\r\n\r\n**tfio.IOTensor.from_audio**\r\n>InvalidArgumentError: WAV file must contains `fmt ` mark [Op:IO>WAVReadableInit]", "And @yongtang, here's an example with a JUNK header:\r\n\r\n[example.zip](https://github.com/tensorflow/tensorflow/files/3784317/example.zip)\r\n\r\nSeems that it might be about being compatible with CD players and so on:\r\n- https://www.daubnet.com/en/file-format-riff\r\n- https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf\r\n- https://tech.ebu.ch/docs/tech/tech3306v1_0.pdf", "@carlthome added a PR in tensorflow/io (https://github.com/tensorflow/io/pull/594)", "\r\n@yongtang\r\nHi, this problem still exists in Tensorflow2.0\uff0c could you solve it \uff1f  \r\n\r\nAudio WAV files from iOS are not read by Tensorflow audio_ops.decode_wav: Header mismatch: Expected fmt but found JUNK #32382", "@SmileTM In https://github.com/tensorflow/io, the latest nightly `pip3 install tensorflow-io-nightly` you can use `tfio.audio.decode_wav`  which should fix this issue. ", "> @SmileTM In https://github.com/tensorflow/io, the latest nightly `pip3 install tensorflow-io-nightly` you can use `tfio.audio.decode_wav` which should fix this issue.\r\n\r\nIt works well. Thank you. \r\nI think this new `tfio.audio` code should be merged to `tf.audio` .", "Hello @yongtang,\r\n\r\nI am using transfer learning for audio using this [tutorial](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio) and I have a few wav files with 'BEXT' chunk and it throws an error. \r\n\r\n\r\n> 2021-08-17 16:13:21.912804: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at audio_video_wav_kernels.cc:315 : Out of range: EOF reached\r\n> 2021-08-17 16:13:21.916281: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Data too short when trying to read string\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\contextlib.py\", line 135, in __exit__\r\n>     self.gen.throw(type, value, traceback)\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2833, in variable_creator_scope\r\n>     yield\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\keras\\engine\\training.py\", line 1184, in fit\r\n>     tmp_logs = self.train_function(iterator)\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 885, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 917, in _call\r\n>     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3039, in __call__\r\n>     return graph_function._call_flat(\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1963, in _call_flat\r\n>     return self._build_call_outputs(self._inference_function.call(\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 591, in call\r\n>     outputs = execute.execute(\r\n>   File \"C:\\Users\\Mona\\anaconda3\\envs\\lisnen_work\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n>     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n>   (0) Invalid argument:  Function invocation produced OutOfRangeError: EOF reached\r\n> \t [[{{node PartitionedCall/IO>AudioDecodeWAV}}]]\r\n> \t [[IteratorGetNext]]\r\n>   (1) Invalid argument:  Function invocation produced OutOfRangeError: EOF reached\r\n> \t [[{{node PartitionedCall/IO>AudioDecodeWAV}}]]\r\n> \t [[IteratorGetNext]]\r\n> \t [[IteratorGetNext/_2]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored. [Op:__inference_train_function_31415]\r\n> \r\n> Function call stack:\r\n> train_function -> train_function\r\n> \r\n> 2021-08-17 16:13:29.065415: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Header mismatch: Expected fmt  but found bext\r\n\r\n(I am using tensorflow-gpu 2.6 and I tried first with just tfio 0.20.0,\r\nand then after installing tensorflow-io-nightly  0.20.0.dev20210815170710)\r\n\r\n\r\nI have recreated the function load_16k_mono() as discussed here on this thread with `tfio.audio.decode_wav` instead of `tf.audio.decode_wav`\r\n\r\nBoth functions load_16k_mono() and load_16k_mono_modified are showing the exact same output in the debugger. However, when I use audio files processed through this function for training, I still get the same error.\r\n\r\nHere is the full code:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\n\r\n\r\n@tf.function\r\ndef load_wav_16k_mono(filename):\r\n    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\r\n    file_contents = tf.io.read_file(filename)\r\n    wav, sample_rate = tf.audio.decode_wav(\r\n          file_contents,\r\n          desired_channels=1)\r\n    wav = tf.squeeze(wav, axis=-1)\r\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\r\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\r\n    return wav\r\n\r\n\r\n@tf.function\r\ndef load_wav_16k_mono_modified(filename):\r\n    file_contents = tf.io.read_file(filename)\r\n\r\n    wav = tfio.audio.decode_wav(file_contents, dtype=tf.int16)\r\n    wav = wav[:, 0]\r\n    wav = tf.cast(wav, tf.float32)\r\n\r\n    _, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\r\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\r\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\r\n\r\n    return wav\r\n\r\ntesting_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',\r\n                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',\r\n                                                cache_dir='./',\r\n                                                cache_subdir='test_data')\r\n\r\nload_wav_16k_mono (testing_wav_file_name)\r\nload_wav_16k_mono_modified (testing_wav_file_name)\r\n\r\n```\r\nI also saw this [code](https://github.com/tensorflow/io/pull/594/files) in one of the issues, and it is handling bext chunks, but probably not tfio.audio.wave_decode?  \r\n\r\n\r\n", "@MemoonaTahira I created an issue in https://github.com/tensorflow/io/issues/1503, let's follow up there.\r\n"]}, {"number": 26246, "title": "Implement unidirectional sequence GRU kernel.", "body": "Implement unidirectional sequence GRU kernel supporting float eval, and add basic unit tests.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26246) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "> I signed it!\r\n\r\nwe could not see CLA , please use same username and email associated while you sign CLA .", "Corporate CLA has been signed and processed. Please check again.", "I signed it!", "looks good! the clas check is not passing, can you take a look? thanks!", "I don't know how to verify author consent, pls help me.", "It says unable to verify author consent, can you try again?\r\n\r\n@rthadur can you help them? thanks a lot!", "adding Jian to take a second pass, but please sign the cla", "> I signed it!\r\n\r\nboth @shenli-mobvoi  @sixuewang  needs to sign CLA before we proceed to merge, thank you ", "> > I signed it!\r\n> \r\n> both @shenli-mobvoi @sixuewang needs to sign CLA before we proceed to merge, thank you\r\n\r\nWe are covered by the same corporate CLA, which can be found under https://cla.developers.google.com/clas page for both accounts. Are there any other actions we can try?", "> shenli-mobvoi\r\n\r\nI can't see CLA for \"shenli-mobvoi\" this username , please resubmit a new PR with @sixuewang username and submit.", "> > shenli-mobvoi\r\n> \r\n> I can't see CLA for \"shenli-mobvoi\" this username , please resubmit a new PR with @sixuewang username and submit.\r\n\r\nSure, @sixuewang has created a new PR [27731](https://github.com/tensorflow/tensorflow/pull/27731). I'll close this PR. Sorry for the inconvenience. @renjie-liu @jianlijianli "]}, {"number": 26245, "title": "InvalidArgumentError Incompatible shapes when multi_gpu_model used on LSTM model in TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190228\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10/7.4\r\n- GPU model and memory:  Tesla M60 on AWS g3.8xlarge\r\n\r\n**Describe the current behavior**\r\n`multi_gpu_model` fails to parallelize LSTM model. Things I've tried without success:\r\n- setting  `cpu_relocation=True` parameter\r\n- disabling eager mode\r\n\r\n**Describe the expected behavior**\r\nthe code below should run instead of giving an error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding\r\nfrom tensorflow.keras.utils import multi_gpu_model\r\n\r\nvocab_size= 20000\r\nmaxlen=80\r\n\r\n(X_train, y_train), (X_test, y_test) = \\\r\n    imdb.load_data(num_words=vocab_size)\r\n\r\nX_train_pad = pad_sequences(X_train, maxlen=maxlen)\r\nX_test_pad = pad_sequences(X_test, maxlen=maxlen)\r\n\r\nwith tf.device('/cpu:0'):\r\n    model = Sequential([\r\n        Embedding(vocab_size, 100, input_length=maxlen),\r\n        LSTM(64, dropout=0.2, recurrent_dropout=0.2),\r\n        Dense(1, activation='sigmoid')\r\n    ])\r\n\r\nmodel = multi_gpu_model(model, 2)\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(X_train_pad, y_train,\r\n          batch_size=2048,\r\n          epochs=2,\r\n          shuffle=True)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```python\r\nW0301 07:00:25.055628 139938579490560 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f45f84f5470>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\r\n\r\nEpoch 1/2\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-6-827e285c76cb> in <module>\r\n     33           batch_size=2048,\r\n     34           epochs=2,\r\n---> 35           shuffle=True)\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    349 \r\n    350         # Get outputs.\r\n--> 351         batch_outs = f(ins_batch)\r\n    352         if not isinstance(batch_outs, list):\r\n    353           batch_outs = [batch_outs]\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    523       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    524           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 525     return self._call_flat(args)\r\n    526 \r\n    527   def _filtered_call(self, args, kwargs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    592     # Only need to override the gradient in graph mode and when we have outputs.\r\n    593     if context.executing_eagerly() or not self.outputs:\r\n--> 594       outputs = self._inference_function.call(ctx, args)\r\n    595     else:\r\n    596       self._register_gradient()\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    380             attrs=(\"executor_type\", executor_type,\r\n    381                    \"config_proto\", config),\r\n--> 382             ctx=ctx)\r\n    383       # Replace empty list with None\r\n    384       outputs = outputs or None\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Incompatible shapes: [1024,64] vs. [2048,64]\r\n\t [[{{node sequential_5_1/unified_lstm_5/while/body/_1008/mul_6}}]]\r\n\t [[training_3/RMSprop/gradients/loss_5/dense_5_loss/binary_crossentropy/Mean_grad/Prod/_842]] [Op:__inference_keras_scratch_graph_32546]\r\n```", "comments": ["Thanks for reporting the issue and sorry for the late reply. Let me take some time to troubleshoot the issue.", "Thanks @qlzh727, let me know if I can help you with anything.", "Is `multi_gpu_model` not recommended any longer with TF 2.0 ? \r\n", "In 2.0  we will recommend user to use distribution strategy. Having said that, I don't see the multi_gpu_model gets deprecated, @fchollet, should we actually deprecate multi_gpu_model in v2 since istribution strategy is the new recommended way for model sharding?", "Update on this bug:\r\nreplacing `LSTM(64, dropout=0.2, recurrent_dropout=0.2)` with `LSTM(64, dropout=0.2)` in the model definition runs fine, albeit the warnings:\r\n\r\n```python\r\nW0321 18:52:50.309778 140426690877184 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fb7748c2748>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\r\nW0321 18:52:50.793717 140426690877184 deprecation.py:506] From /home/ubuntu/miniconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n```\r\n\r\nI suspect the problem is somewhere in the new bundled LSTM layer that automatically chooses where to allocate the weights and whether to use CuDNN or not. In fact CuDNN layers need to be on the GPU and do not support `recurrent_dropout`.\r\n\r\nAlso, notice that when the LSTM layer is defined without the recurrent dropout, it's allocated on the GPU regardless of the context setter. This seems similar to #26244 ", "Interesting, thanks for sharing the finding, which will narrow down the cause of the issue. \r\n\r\nRegarding to the device context, it has been recently updated with https://github.com/tensorflow/tensorflow/commit/584ad2f575ffc644accb8f0d8c4e1038edd42412. You can try updating your installed TF with nightly-2.0-preview and it should fixed issue.", "Looks like nightly builds are not available for mac py3.7? I get: \r\n\r\n```bash\r\n$ pip install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: )\r\nNo matching distribution found for tf-nightly-2.0-preview\r\n```\r\n\r\nI'll try on linux\r\n", "Still getting this error in `2.1.0`. Am I doing something wrong? I also get the `No matching distribution` error trying to install the nightly build on Ubuntu ....", "Switching from `multi_gpu_model` to `MirroredStrategy` seemed to fix it. Although I still can't install the nightly build ....", "@ghego,\r\nIt is recommended to use [Distribution Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute) over **`multi_gpu_model`**. Even the documentation of **`multi_gpu_model`** is not available in the [Tensorflow Site](https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model?language=zh_cn). \r\nPlease refer the documentation for [Distributed Training using Keras](https://www.tensorflow.org/tutorials/distribute/keras) and [Distributed Training Using Tensorflow](https://www.tensorflow.org/guide/distributed_training) for more information\r\nThanks!", "Yes, this issue is 3 years old. I'm aware that Distribution Strategy is the way to go. I'll close it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26245\">No</a>\n"]}, {"number": 26244, "title": "FailedPreconditionError when running Convolutional Keras model on CPU in TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190228\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10/7.4\r\n- GPU model and memory:  Tesla M60 on AWS g3.8xlarge\r\n\r\n\r\n**Describe the current behavior**\r\nTraining a model defined on the CPU raises a `FailedPreconditionError` when using a machine with a GPU in TF 2.0 nightly.\r\n\r\n**Describe the expected behavior**\r\nNo error is raised if I use one of the following fixes:\r\n1) use `tensorflow.compat.v1.disable_eager_execution()`\r\n2) remove the `Conv2D` layer\r\n3) remove the `batch_size` and `epochs` arguments from the `.fit` call\r\n\r\nHowever, the context setter seems to have no effect and training is happening on the GPU anyways (I can tell by how fast it's training)\r\n\r\nthe behavior seems weird, can anyone explain what's going on?\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import cifar10\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Flatten, Dense\r\nfrom tensorflow.keras.layers import Conv2D\r\n\r\n(X_train, y_train), _ = cifar10.load_data()\r\nX_train = X_train.astype('float32') / 255.0\r\n\r\nwith tf.device('cpu:0'):\r\n    model = Sequential([\r\n        Conv2D(32, (3, 3), input_shape=(32, 32, 3)),\r\n        Flatten(),\r\n        Dense(10, activation='softmax')\r\n    ])\r\n    \r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                  optimizer='rmsprop',\r\n                  metrics=['accuracy'])\r\n\r\n    \r\nmodel.fit(X_train, y_train,\r\n          batch_size=1024,\r\n          epochs=2)\r\n```\r\n\r\n**Other info / logs**\r\ntraceback:\r\n```python\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-1-d00bb2a707a4> in <module>\r\n     24 model.fit(X_train, y_train,\r\n     25           batch_size=1024,\r\n---> 26           epochs=2)\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    349 \r\n    350         # Get outputs.\r\n--> 351         batch_outs = f(ins_batch)\r\n    352         if not isinstance(batch_outs, list):\r\n    353           batch_outs = [batch_outs]\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    523       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    524           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 525     return self._call_flat(args)\r\n    526 \r\n    527   def _filtered_call(self, args, kwargs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    592     # Only need to override the gradient in graph mode and when we have outputs.\r\n    593     if context.executing_eagerly() or not self.outputs:\r\n--> 594       outputs = self._inference_function.call(ctx, args)\r\n    595     else:\r\n    596       self._register_gradient()\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    380             attrs=(\"executor_type\", executor_type,\r\n    381                    \"config_proto\", config),\r\n--> 382             ctx=ctx)\r\n    383       # Replace empty list with None\r\n    384       outputs = outputs or None\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError: Error while reading resource variable _AnonymousVar15 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar15 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\r\n\t [[{{node training/RMSprop/RMSprop/update_3/mul/ReadVariableOp}}]] [Op:__inference_keras_scratch_graph_646]\r\n```\r\n", "comments": ["@ezhulenev The presence of AnonymousVar here makes me think this is related to the arithmetic optimizer.", "It's just the name auto-generated in https://github.com/tensorflow/tensorflow/blob/abdbe6ee3d81d22d70e0181cea2a3bb261f7c09f/tensorflow/core/framework/resource_mgr.cc#L48, I don't see any artifact of arithmetic optimizer in the variable name. I think it's just the problem with Keras graph.", "@robieta can you take a look at this?", "Sure, let me take a look.", "It looks like what's happening is that the model weights are initialized inside of the `tf.device` context, but the RMSProp variables aren't. So when we try to call fit, the resultant variables are then (lazily) created and assigned to the default device: `gpu:0`. I still need to dig into why this only happens in 2.0 though.", "Thanks @robieta for looking into this. Keep in mind that:\r\n1) this only happens in eager mode, if I use `tensorflow.compat.v1.disable_eager_execution()` no error is shown\r\n2) in this case, the code executes but I suspect all variables are created on the GPU regardless of the context setter because training runs really fast", "@robieta I think optimizer colocate its variable with model.trainable_weights, so it's likely that the model variables are lazily constructed, which does not respect tf.device(\"cpu:0\") anymore, ergo optimizer follows the lead and does the same thing.", "I'm too facing this. I use tensorflow low level apis. Inside my while loop, the message of '_AnonymousVar60 variable was uninitialized' pops once the counter goes to [1]\r\n@tf.function\r\ndef func():\r\n     with tf.device('/device:GPU:0'):\r\n           for i in tf.range(10):\r\n                  {runs smoothly for i=0}\r\n\r\nI use cuda 10.0 and built tensorflow 2.0.0 from source around 20th March.\r\nUbuntu 18.04, NVIDIA GTX 1060", "import tensorflow as tf\r\nfrom tensorflow.keras.applications import Xception\r\nfrom tensorflow.keras.utils import multi_gpu_model\r\nimport numpy as np\r\n\r\nnum_samples = 32\r\nheight = 224\r\nwidth = 224\r\nnum_classes = 32\r\n\r\n\"Instantiate the base model (or \"template\" model).\r\n\" We recommend doing this with under a CPU device scope,\r\n\" so that the model's weights are hosted on CPU memory.\r\n\" Otherwise they may end up hosted on a GPU, which would\r\n\" complicate weight sharing.\r\nwith tf.device('/cpu:0'):\r\n    model = Xception(weights=None,\r\n                     input_shape=(height, width, 3),\r\n                     classes=num_classes)\r\n\r\n\"Replicates the model on `2` GPUs.\r\n\" This assumes that your machine has 2 available GPUs.\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.compile(loss='categorical_crossentropy',\r\n                       optimizer='rmsprop')\r\n\r\n\" Generate dummy data.\r\nx = np.random.random((num_samples, height, width, 3))\r\ny = np.random.random((num_samples, num_classes))\r\n\r\n\" This `fit` call will be distributed on 8 GPUs.\r\n\" Since the batch size is 256, each GPU will process 32 samples.\r\nparallel_model.fit(x, y, epochs=20, batch_size=256)\r\n\r\n\" Save model via the template model (which shares the same weights):\r\n\r\n\" Save model via the template model (which shares the same weights):\r\nmodel.save('my_model.h5')\r\n\r\nand the error is \r\nEpoch 1/20\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-1-21adf99b6266> in <module>\r\n     31 # This `fit` call will be distributed on 8 GPUs.\r\n     32 # Since the batch size is 256, each GPU will process 32 samples.\r\n---> 33 parallel_model.fit(x, y, epochs=20, batch_size=256)\r\n     34 \r\n     35 # Save model via the template model (which shares the same weights):\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    350 \r\n    351         # Get outputs.\r\n--> 352         batch_outs = f(ins_batch)\r\n    353         if not isinstance(batch_outs, list):\r\n    354           batch_outs = [batch_outs]\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3215         value = math_ops.cast(value, tensor.dtype)\r\n   3216       converted_inputs.append(value)\r\n-> 3217     outputs = self._graph_fn(*converted_inputs)\r\n   3218     return nest.pack_sequence_as(self._outputs_structure,\r\n   3219                                  [x.numpy() for x in outputs])\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n    556       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n    557           list(kwargs.keys()), list(self._arg_keywords)))\r\n--> 558     return self._call_flat(args)\r\n    559 \r\n    560   def _filtered_call(self, args, kwargs):\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args)\r\n    625     # Only need to override the gradient in graph mode and when we have outputs.\r\n    626     if context.executing_eagerly() or not self.outputs:\r\n--> 627       outputs = self._inference_function.call(ctx, args)\r\n    628     else:\r\n    629       self._register_gradient()\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args)\r\n    413             attrs=(\"executor_type\", executor_type,\r\n    414                    \"config_proto\", config),\r\n--> 415             ctx=ctx)\r\n    416       # Replace empty list with None\r\n    417       outputs = outputs or None\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~\\Anaconda3\\envs\\py36\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError: Error while reading resource variable _AnonymousVar341 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar341 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0\r\n\t [[{{node training/RMSprop/RMSprop/update_99/mul/ReadVariableOp}}]]\r\n\t [[training_rmsprop_rmsprop_update_98_mul_readvariableop_resource/_3]] [Op:__inference_keras_scratch_graph_37943]\r\n", "> I'm too facing this. I use tensorflow low level apis. Inside my while loop, the message of '_AnonymousVar60 variable was uninitialized' pops once the counter goes to [1]\r\n> @tf.function\r\n> def func():\r\n> with tf.device('/device:GPU:0'):\r\n> for i in tf.range(10):\r\n> {runs smoothly for i=0}\r\n> \r\n> I use cuda 10.0 and built tensorflow 2.0.0 from source around 20th March.\r\n> Ubuntu 18.04, NVIDIA GTX 1060\r\n\r\nThis is basically the exact error I am facing, the first batch executes fine, then one the second batch, this error appears.\r\n\r\nHow is this even possible? Did you ever find a solution?", "@ghego, This issue is fixed in Tf-nightly version.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/ee6d90cb4fb5c3c9ea7c377afa9e327c/untitled442.ipynb) and let us know if you are happy with fix and close this issue. Thanks!", "@ghego, Did you review the above attached gist. \r\nPlease close the issue if it was already resolved for you. Thanks!", "It works. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26244\">No</a>\n"]}, {"number": 26243, "title": "TF Lite updated arena_planner.cc", "body": "Fixed warning in the file.", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 26242, "title": "Build failure on s390x on hwloc ", "body": "**System information**\r\n\r\n    * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n\r\n    * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n\r\n    * TensorFlow installed from (source or binary): source\r\n\r\n    * TensorFlow version: master as on today\r\n\r\n    * Python version:  2.7.x\r\n\r\n    * Bazel version (if compiling from source): 0.19.0 \r\n\r\n    * GCC/Compiler version (if compiling from source): gcc 7.3.0, glibc 2.28\r\n\r\n    * CUDA/cuDNN version: NA\r\n\r\n    * GPU model and memory: NA\r\n\r\n\r\n**Describe the problem**\r\nTensorflow build fails with an error:\r\n\r\n```\r\nERROR: /home/jenkins/.cache/bazel/_bazel_jenkins/14d9bef57f8e4d2a0eef0de174c4144b/external/hwloc/BUILD.bazel:212:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)\r\nIn file included from external/hwloc/hwloc/topology-x86.c:23:0:\r\nexternal/hwloc/hwloc/topology-x86.c: In function 'cpuid_or_from_dump':\r\nexternal/hwloc/include/private/cpuid-x86.h:67:3: error: inconsistent operand constraints in an 'asm'\r\n   __asm__(\r\n   ^~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "comments": ["Tensorflow CI is failing on dependency hwloc. This is a new dependency added to TensorFlow recently.  \r\nhwloc contains arch specific assembly code which Z don't have support. \r\n\r\nCI link: https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/2147/console", "hwloc contains Intel specific assembly code which fails on s390x.  Can you please advise if we can disable this feature? What impact it can make? ", "ppc64le had the same error, I submitted https://github.com/tensorflow/tensorflow/pull/26220 to fix.\r\n\r\nIt seems like hwloc was added for numa support, which is optional and not included by default in the build. So you don't need it. ", "#26220 is merged now. @Nayana-ibm , I see a successful build here:\r\nhttps://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/2154/\r\n\r\nShould we close this one out?", "Yes. Build is successful after #26220  commit. \r\nClosing the issue. \r\n\r\nThanks @wdirons "]}, {"number": 26241, "title": "F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. Aborted", "body": "Error :\r\n\r\ntensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.\r\nAborted\r\n\r\n getting this error after running\r\n\r\n**python -m rasa_core.train -d domain.yml -s data/stories.md -o models/current/dialogue -c policies.yml**\r\n\r\nVersion:\r\n\r\n \r\nMy System Config\r\n\r\n\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                1\r\nOn-line CPU(s) list:   0\r\nThread(s) per core:    1\r\nCore(s) per socket:    1\r\nSocket(s):             1\r\nNUMA node(s):          1\r\nVendor ID:             AuthenticAMD\r\nCPU family:            16\r\nModel:                 9\r\nModel name:            AMD Opteron(tm) Processor 6128 HE\r\nStepping:              1\r\nCPU MHz:               2000.000\r\nBogoMIPS:              4000.00\r\nHypervisor vendor:     VMware\r\nVirtualization type:   full\r\nL1d cache:             64K\r\nL1i cache:             64K\r\nL2 cache:              512K\r\nL3 cache:              10236K\r\nNUMA node0 CPU(s):     0\r\n\r\n\r\n\r\n  \r\n", "comments": ["@saurabhaec hank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "I have the same error", "The error: \r\nF tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. Aborted (core dumped) \r\nmy machine: Intel Celeron CPU J1900 1.99GHz\r\n\r\n", "@MaxKKuznetsov Please open a new issue as this was an old issue that was closed more than two years ago. Thanks!"]}, {"number": 26240, "title": "cannot import tensorflow", "body": "**System information**\r\n- Win10 X64 :\r\n- pip install tensorflow-gpu==1.12.0\r\n- TensorFlow version:\r\n- Python version: 3.6\r\n- Installed using  pip\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA: 9 \r\n- cuDNN version: 7\r\n- GPU model and memory: NVIDIA GeForce MX150\r\n\r\n**Describe the problem**\r\ncannot import tensorflow,  always this error: tensorflow.python.pywrap_tensorflow_internal import *,\r\nI have checked therre is no related DLL existed, and i know it is an install issue. I have searched many same issue and tried to fix, but it does not work . \r\n\r\nPS: This issue only for tensorflow-gpu install, it's ok for tensorflow install on the same machine. But i want to use tensorflow-gpu.\r\n\r\n\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@liuxinni0406 Could try the instruction [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) that worked for installing TF1.12_gpu successfully. Please uninstall python and tensorflow then follow the instruction to install the TF1.12. Please let me know how it progresses. Thanks!", "> @liuxinni0406 Could try the instruction [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12) that worked for installing TF1.12_gpu successfully. Please uninstall python and tensorflow then follow the instruction to install the TF1.12. Please let me know how it progresses. Thanks!\r\n\r\nHave tried as your step, but the same issue as before. I think there might be some environment issue, but i cannot find the problem. Thank you.", "@liuxinni0406 You can install tensorflow-gpu for windows easily on Anaconda. Here's the link https://anaconda.org/anaconda/tensorflow-gpu", "There are solutions and workarounds presented in a similar issue #26182. Please go through them if your issue was not resolved. I am closing this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26240\">No</a>\n"]}, {"number": 26239, "title": "Why is op:BatchMatMul listed in file(.pbtxt), when there is no call to such op?", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6 \r\n\r\n**Describe the current behavior**\r\nThere is no usage of op named _**BatchMatMul**_ in my code. But the op is listed in the file(.pbtxt) which gives me other issues while converting the graph to tflite.\r\n\r\n**Describe the expected behavior**\r\nIf there is no explicit usage of the particular op, it should not be listed in file(.pbtxt)", "comments": ["@vinayonchip Can you provide a code to reproduce the bug? Also, mention more details about the bug and its context. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 26238, "title": "Support lowercase for precision mode", "body": "TF-TRT used to work with lowercase letters for the precision mode in the API. This was broken recently. I am not sure if the breakage was in purpose. But we would need this for backward compatibility.", "comments": ["@aaroey I think we need this in the release branch."]}, {"number": 26237, "title": "TF Lite updated quantization_utils.cc", "body": "Fixed warning in the file.", "comments": ["This changes combine with #26236 ."]}, {"number": 26236, "title": "TF Lite updated subgraph_quantizer.cc", "body": "Fixed warning in the file.", "comments": ["thanks for your contribution , could you please combine related PR here #26237 #26243 ", "@rthadur\r\n\r\n#26237 #26243 merged here.", "This changes combine with #26236 ."]}, {"number": 26235, "title": "typos in tensorflow/core fixed", "body": "", "comments": ["@siju-samuel gentle ping to resolve conflicts", "@rthadur thank you for notifying, conflicts are resolved. @hgadig gentle ping to approve again.", "@rthadur could you please help to merge this pr. thank you.", "@rthadur i could see the changes are there in master, but the PR is still open. This PR should go to merged state, right?", "@siju-samuel as the changes are merged , can i close this PR"]}, {"number": 26234, "title": "DLL Load failed, WIn10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary - pip install tensorflow-gpu\r\n- TensorFlow version: tensorflow-gpu-1.13.1\r\n- Python version: Python 3.6.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9 - cudnn-9.0-windows10-x64-v7.5.0.56\r\n- GPU model and memory: 2080 TI SLI - 64 GB DDR4\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nReports missing modules:\r\n\r\n```\r\n(venv) C:\\Users\\Ryan\\Developer\\TF>python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Ryan\\Developer\\TF\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n.\\venv\\Scripts\\activate\r\npython -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n\r\n**Any other info / logs**\r\n\r\nHere is a photo of the cudnn DLL's placed in the appropriate locations, and also my environment variables:\r\nhttps://i.imgur.com/T0yuuQa.png\r\n\r\nAlso, here is the output from deviceQuery.exe, I could build that example just fine:\r\n```\r\nC:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v9.0\\bin\\win64\\Debug>deviceQuery.exe\r\ndeviceQuery.exe Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 2 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce RTX 2080 Ti\"\r\n  CUDA Driver Version / Runtime Version          10.1 / 9.0\r\n  CUDA Capability Major/Minor version number:    7.5\r\n  Total amount of global memory:                 11264 MBytes (11811160064 bytes)\r\nMapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM\r\nMapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM\r\n  (68) Multiprocessors, ( 64) CUDA Cores/MP:     4352 CUDA Cores\r\n  GPU Max Clock rate:                            1650 MHz (1.65 GHz)\r\n  Memory Clock rate:                             7000 Mhz\r\n  Memory Bus Width:                              352-bit\r\n  L2 Cache Size:                                 5767168 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  1024\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 6 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Supports Cooperative Kernel Launch:            No\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\nDevice 1: \"GeForce RTX 2080 Ti\"\r\n  CUDA Driver Version / Runtime Version          10.1 / 9.0\r\n  CUDA Capability Major/Minor version number:    7.5\r\n  Total amount of global memory:                 11264 MBytes (11811160064 bytes)\r\nMapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM\r\nMapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM\r\n  (68) Multiprocessors, ( 64) CUDA Cores/MP:     4352 CUDA Cores\r\n  GPU Max Clock rate:                            1650 MHz (1.65 GHz)\r\n  Memory Clock rate:                             7000 Mhz\r\n  Memory Bus Width:                              352-bit\r\n  L2 Cache Size:                                 5767168 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  1024\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 6 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Supports Cooperative Kernel Launch:            No\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 9.0, NumDevs = 2\r\nResult = PASS\r\n```\r\n\r\nAny help appreciated I am quite stuck here. ", "comments": ["I tried installing cudnn version: cudnn-9.0-windows10-x64-v7.3.0.29\r\n\r\nSame issue.", "I got the same problem when I upgraded to 1.13.1. I found the release notes for 1.13.1.[https://github.com/tensorflow/tensorflow/releases](url)\r\n\r\n> TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0.\r\n\r\nIt seems that the requirements on the official website are out of date.\r\n\r\nInstalling CUDA 10 may fix your problem.", "I tried adding CUDA_HOME system environment variable set to;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\r\n\r\nAlso changed path variable:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\r\nto\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64\r\n\r\nNeither of that worked.\r\n\r\nBut as per suggestion of fishersosoo, I did pip uninstall tensorflow-gpu to get rid of 1.13, then I installed 1.12 from the link:\r\npip install https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl\r\n\r\nIt works now. Thanks.\r\n\r\nI will try CUDA 10 and 1.13 again as well."]}, {"number": 26233, "title": "Any plan to support control flow in TF lite?", "body": "Seems TF lite doesn't support object detection models like rcnn (https://github.com/tensorflow/tensorflow/issues/19293), I guess it was due to the control flow stuff, is there any plan/timeline to add the support? ", "comments": ["Hi, We support single shot detectors such MobileNet SSD and FPN via the [custom op implementation for NMS ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc). We will support control flow in TF Lite soon.", "I think it was resolved. I am closing the issue. If you think I made a mistake, please open another issue. Thanks!"]}, {"number": 26232, "title": "Fetch real CPU cache values for Conv2DCustomBackpropInputOp kernel", "body": "This brings about from 4%/5%/7% speedups on cifar10 runs on an\r\nIntel(R) Core(TM) i9-7900X CPU @ 3.30GHz for\r\nMKL-DNN/Eigen-AVX512/Eigen-AVX2 builds, respectively.", "comments": ["Seems fine to me, but Andy added the TODO, so he should review.", "It seems that @penpornk has tried this change and seen performance regressions due to it.", "I did similar changes last May https://github.com/tensorflow/tensorflow/commit/398a62037eb5f0aa049d3243818d16f2b3a10dec. I saw 10% latency regressions in several benchmarks and reverted the changes in June  https://github.com/tensorflow/tensorflow/commit/261ab05537885556f92d7322017ddf73ea5a7357.", "Oh wow, curious. It would help if those were listed on the revert commit, though. Thanks, back to the drawing board here, then."]}, {"number": 26231, "title": "Remove unused variable", "body": "This PR removes a unused variable which leads to a warning during build time.", "comments": []}, {"number": 26230, "title": "Cuda 10.1 support", "body": "Enables building TensorFlow with CUDA 10.1.\r\nAddresses #26150", "comments": ["@chsigg, do you mind taking a look at this? Thanks!", "@chsigg Starting with CUDA 10.1 cublas is installed into the system /usr/lib and /usr/include directories rather than being located with the toolkit. Also, a build number is being added to the cublas libs, so they won't necessarily match the toolkit version. See https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-new-features\r\n\r\nReducing the coverage of the stub library should improve future maintainability as well. I'll update the PR shortly.", "@chsigg anything else needed on this?", "Hi Nathan. Thanks for updating the PR, and sorry for the delay.", "Would this still be compatible for those who are packaging CUDA 9.X + 10.X to place them in their own path? For Pop!_OS, I have each CUDA SDK to store their files in `/usr/lib/cuda-${version}`, with `update-alternatives` making it possible to switch active toolchains from a symlink at `/usr/lib/cuda`. This enables multiple toolchains to be installed at the same time.\r\n\r\nWe've packaged CUDA 10.1 in this manner as well:\r\n\r\n```sh\r\n# ls /usr/lib/cuda-10.1/lib64\r\nlibaccinj64.so\t\t      libcufftw.so.10.1.105    libnppc_static.a        libnppig.so.10.1.105   libnvblas.so.10\r\nlibaccinj64.so.10.1\t      libcufftw_static.a       libnppial.so\t       libnppig_static.a      libnvblas.so.10.1.0.105\r\nlibaccinj64.so.10.1.105       libcuinj64.so\t       libnppial.so.10\t       libnppim.so\t      libnvgraph.so\r\nlibcublasLt.so\t\t      libcuinj64.so.10.1       libnppial.so.10.1.105   libnppim.so.10\t      libnvgraph.so.10\r\nlibcublasLt.so.10\t      libcuinj64.so.10.1.105   libnppial_static.a      libnppim.so.10.1.105   libnvgraph.so.10.1.105\r\nlibcublasLt.so.10.1.0.105     libculibos.a\t       libnppicc.so\t       libnppim_static.a      libnvgraph_static.a\r\nlibcublasLt_static.a\t      libcurand.so\t       libnppicc.so.10\t       libnppist.so\t      libnvjpeg.so\r\nlibcublas.so\t\t      libcurand.so.10\t       libnppicc.so.10.1.105   libnppist.so.10\t      libnvjpeg.so.10\r\nlibcublas.so.10\t\t      libcurand.so.10.1.105    libnppicc_static.a      libnppist.so.10.1.105  libnvjpeg.so.10.1.105\r\nlibcublas.so.10.1.0.105       libcurand_static.a       libnppicom.so\t       libnppist_static.a     libnvjpeg_static.a\r\nlibcublas_static.a\t      libcusolver.so\t       libnppicom.so.10        libnppisu.so\t      libnvrtc-builtins.so\r\nlibcudadevrt.a\t\t      libcusolver.so.10        libnppicom.so.10.1.105  libnppisu.so.10\t      libnvrtc-builtins.so.10.1\r\nlibcudart.so\t\t      libcusolver.so.10.1.105  libnppicom_static.a     libnppisu.so.10.1.105  libnvrtc-builtins.so.10.1.105\r\nlibcudart.so.10.1\t      libcusolver_static.a     libnppidei.so\t       libnppisu_static.a     libnvrtc.so\r\nlibcudart.so.10.1.105\t      libcusparse.so\t       libnppidei.so.10        libnppitc.so\t      libnvrtc.so.10.1\r\nlibcudart_static.a\t      libcusparse.so.10        libnppidei.so.10.1.105  libnppitc.so.10\t      libnvrtc.so.10.1.105\r\nlibcufft.so\t\t      libcusparse.so.10.1.105  libnppidei_static.a     libnppitc.so.10.1.105  libnvToolsExt.so\r\nlibcufft.so.10\t\t      libcusparse_static.a     libnppif.so\t       libnppitc_static.a     libnvToolsExt.so.1\r\nlibcufft.so.10.1.105\t      liblapack_static.a       libnppif.so.10\t       libnpps.so\t      libnvToolsExt.so.1.0.0\r\nlibcufft_static.a\t      libmetis_static.a        libnppif.so.10.1.105    libnpps.so.10\t      libOpenCL.so\r\nlibcufft_static_nocallback.a  libnppc.so\t       libnppif_static.a       libnpps.so.10.1.105    libOpenCL.so.1\r\nlibcufftw.so\t\t      libnppc.so.10\t       libnppig.so\t       libnpps_static.a       libOpenCL.so.1.1\r\nlibcufftw.so.10\t\t      libnppc.so.10.1.105      libnppig.so.10\t       libnvblas.so\t      stubs\r\n# ls /usr/lib/cuda-10.1/include\r\nbuiltin_types.h\t\t      curand_discrete2.h\t     host_defines.h\t\t\t       nvToolsExtCudaRt.h\r\nchannel_descriptor.h\t      curand_discrete.h\t\t     library_types.h\t\t\t       nvToolsExt.h\r\nCL\t\t\t      curand_globals.h\t\t     math_constants.h\t\t\t       nvToolsExtMeta.h\r\ncommon_functions.h\t      curand.h\t\t\t     math_functions.h\t\t\t       nvToolsExtSync.h\r\ncooperative_groups.h\t      curand_kernel.h\t\t     mma.h\t\t\t\t       nvtx3\r\ncooperative_groups_helpers.h  curand_lognormal.h\t     nppcore.h\t\t\t\t       sm_20_atomic_functions.h\r\ncrt\t\t\t      curand_mrg32k3a.h\t\t     nppdefs.h\t\t\t\t       sm_20_atomic_functions.hpp\r\ncublas_api.h\t\t      curand_mtgp32dc_p_11213.h      npp.h\t\t\t\t       sm_20_intrinsics.h\r\ncublas.h\t\t      curand_mtgp32.h\t\t     nppi_arithmetic_and_logical_operations.h  sm_20_intrinsics.hpp\r\ncublasLt.h\t\t      curand_mtgp32_host.h\t     nppi_color_conversion.h\t\t       sm_30_intrinsics.h\r\ncublas_v2.h\t\t      curand_mtgp32_kernel.h\t     nppi_compression_functions.h\t       sm_30_intrinsics.hpp\r\ncublas_version.h\t      curand_normal.h\t\t     nppi_computer_vision.h\t\t       sm_32_atomic_functions.h\r\ncublasXt.h\t\t      curand_normal_static.h\t     nppi_data_exchange_and_initialization.h   sm_32_atomic_functions.hpp\r\ncuComplex.h\t\t      curand_philox4x32_x.h\t     nppi_filtering_functions.h\t\t       sm_32_intrinsics.h\r\ncuda_device_runtime_api.h     curand_poisson.h\t\t     nppi_geometry_transforms.h\t\t       sm_32_intrinsics.hpp\r\ncudaEGL.h\t\t      curand_precalc.h\t\t     nppi.h\t\t\t\t       sm_35_atomic_functions.h\r\ncuda_egl_interop.h\t      curand_uniform.h\t\t     nppi_linear_transforms.h\t\t       sm_35_intrinsics.h\r\ncuda_fp16.h\t\t      cusolver_common.h\t\t     nppi_morphological_operations.h\t       sm_60_atomic_functions.h\r\ncuda_fp16.hpp\t\t      cusolverDn.h\t\t     nppi_statistics_functions.h\t       sm_60_atomic_functions.hpp\r\ncudaGL.h\t\t      cusolverRf.h\t\t     nppi_support_functions.h\t\t       sm_61_intrinsics.h\r\ncuda_gl_interop.h\t      cusolverSp.h\t\t     nppi_threshold_and_compare_operations.h   sm_61_intrinsics.hpp\r\ncuda.h\t\t\t      cusolverSp_LOWLEVEL_PREVIEW.h  npps_arithmetic_and_logical_operations.h  sobol_direction_vectors.h\r\ncudalibxt.h\t\t      cusparse.h\t\t     npps_conversion_functions.h\t       surface_functions.h\r\ncuda_occupancy.h\t      cusparse_v2.h\t\t     npps_filtering_functions.h\t\t       surface_functions.hpp\r\ncuda_profiler_api.h\t      device_atomic_functions.h      npps.h\t\t\t\t       surface_indirect_functions.h\r\ncudaProfiler.h\t\t      device_atomic_functions.hpp    npps_initialization.h\t\t       surface_indirect_functions.hpp\r\ncudart_platform.h\t      device_double_functions.h      npps_statistics_functions.h\t       surface_types.h\r\ncuda_runtime_api.h\t      device_functions.h\t     npps_support_functions.h\t\t       texture_fetch_functions.h\r\ncuda_runtime.h\t\t      device_launch_parameters.h     nppversion.h\t\t\t       texture_fetch_functions.hpp\r\ncuda_surface_types.h\t      device_types.h\t\t     nvblas.h\t\t\t\t       texture_indirect_functions.h\r\ncuda_texture_types.h\t      driver_functions.h\t     nvfunctional\t\t\t       texture_indirect_functions.hpp\r\ncudaVDPAU.h\t\t      driver_types.h\t\t     nvgraph.h\t\t\t\t       texture_types.h\r\ncuda_vdpau_interop.h\t      fatBinaryCtl.h\t\t     nvjpeg.h\t\t\t\t       thrust\r\ncufft.h\t\t\t      fatbinary.h\t\t     nvml.h\t\t\t\t       vector_functions.h\r\ncufftw.h\t\t      fatbinary_section.h\t     nvrtc.h\t\t\t\t       vector_functions.hpp\r\ncufftXt.h\t\t      host_config.h\t\t     nvToolsExtCuda.h\t\t\t       vector_types.h\r\n```\r\n\r\nEssentially, I'd expect for the \"so\"'s to be detected from lib / include paths automatically, rather than being hardcoded to a specific path.", "It's not clear to me whether you want to build or run TensorFlow with\ndifferent CUDA versions, but I think this PR should not affect either (for\nbuilding, assuming we allow specifying the cuBLAS include directory during\n./configure)\n\nOn Fri, Mar 15, 2019, 18:25 Michael Murphy <notifications@github.com> wrote:\n\n> Would this still be compatible for those who are packaging CUDA 9.X + 10.X\n> to place them in their own path? For Pop!_OS, I have each CUDA SDK to store\n> their files in /usr/lib/cuda-${version}, with update-alternatives making\n> it possible to switch active toolchains from a symlink at /usr/lib/cuda.\n> This enables multiple toolchains to be installed at the same time.\n>\n> We've packaged CUDA 10.1 in this manner as well:\n>\n> # ls /usr/lib/cuda-10.1/lib64\n> libaccinj64.so\t\t      libcufftw.so.10.1.105    libnppc_static.a        libnppig.so.10.1.105   libnvblas.so.10\n> libaccinj64.so.10.1\t      libcufftw_static.a       libnppial.so\t       libnppig_static.a      libnvblas.so.10.1.0.105\n> libaccinj64.so.10.1.105       libcuinj64.so\t       libnppial.so.10\t       libnppim.so\t      libnvgraph.so\n> libcublasLt.so\t\t      libcuinj64.so.10.1       libnppial.so.10.1.105   libnppim.so.10\t      libnvgraph.so.10\n> libcublasLt.so.10\t      libcuinj64.so.10.1.105   libnppial_static.a      libnppim.so.10.1.105   libnvgraph.so.10.1.105\n> libcublasLt.so.10.1.0.105     libculibos.a\t       libnppicc.so\t       libnppim_static.a      libnvgraph_static.a\n> libcublasLt_static.a\t      libcurand.so\t       libnppicc.so.10\t       libnppist.so\t      libnvjpeg.so\n> libcublas.so\t\t      libcurand.so.10\t       libnppicc.so.10.1.105   libnppist.so.10\t      libnvjpeg.so.10\n> libcublas.so.10\t\t      libcurand.so.10.1.105    libnppicc_static.a      libnppist.so.10.1.105  libnvjpeg.so.10.1.105\n> libcublas.so.10.1.0.105       libcurand_static.a       libnppicom.so\t       libnppist_static.a     libnvjpeg_static.a\n> libcublas_static.a\t      libcusolver.so\t       libnppicom.so.10        libnppisu.so\t      libnvrtc-builtins.so\n> libcudadevrt.a\t\t      libcusolver.so.10        libnppicom.so.10.1.105  libnppisu.so.10\t      libnvrtc-builtins.so.10.1\n> libcudart.so\t\t      libcusolver.so.10.1.105  libnppicom_static.a     libnppisu.so.10.1.105  libnvrtc-builtins.so.10.1.105\n> libcudart.so.10.1\t      libcusolver_static.a     libnppidei.so\t       libnppisu_static.a     libnvrtc.so\n> libcudart.so.10.1.105\t      libcusparse.so\t       libnppidei.so.10        libnppitc.so\t      libnvrtc.so.10.1\n> libcudart_static.a\t      libcusparse.so.10        libnppidei.so.10.1.105  libnppitc.so.10\t      libnvrtc.so.10.1.105\n> libcufft.so\t\t      libcusparse.so.10.1.105  libnppidei_static.a     libnppitc.so.10.1.105  libnvToolsExt.so\n> libcufft.so.10\t\t      libcusparse_static.a     libnppif.so\t       libnppitc_static.a     libnvToolsExt.so.1\n> libcufft.so.10.1.105\t      liblapack_static.a       libnppif.so.10\t       libnpps.so\t      libnvToolsExt.so.1.0.0\n> libcufft_static.a\t      libmetis_static.a        libnppif.so.10.1.105    libnpps.so.10\t      libOpenCL.so\n> libcufft_static_nocallback.a  libnppc.so\t       libnppif_static.a       libnpps.so.10.1.105    libOpenCL.so.1\n> libcufftw.so\t\t      libnppc.so.10\t       libnppig.so\t       libnpps_static.a       libOpenCL.so.1.1\n> libcufftw.so.10\t\t      libnppc.so.10.1.105      libnppig.so.10\t       libnvblas.so\t      stubs# ls /usr/lib/cuda-10.1/include\n> builtin_types.h\t\t      curand_discrete2.h\t     host_defines.h\t\t\t       nvToolsExtCudaRt.h\n> channel_descriptor.h\t      curand_discrete.h\t\t     library_types.h\t\t\t       nvToolsExt.h\n> CL\t\t\t      curand_globals.h\t\t     math_constants.h\t\t\t       nvToolsExtMeta.h\n> common_functions.h\t      curand.h\t\t\t     math_functions.h\t\t\t       nvToolsExtSync.h\n> cooperative_groups.h\t      curand_kernel.h\t\t     mma.h\t\t\t\t       nvtx3\n> cooperative_groups_helpers.h  curand_lognormal.h\t     nppcore.h\t\t\t\t       sm_20_atomic_functions.h\n> crt\t\t\t      curand_mrg32k3a.h\t\t     nppdefs.h\t\t\t\t       sm_20_atomic_functions.hpp\n> cublas_api.h\t\t      curand_mtgp32dc_p_11213.h      npp.h\t\t\t\t       sm_20_intrinsics.h\n> cublas.h\t\t      curand_mtgp32.h\t\t     nppi_arithmetic_and_logical_operations.h  sm_20_intrinsics.hpp\n> cublasLt.h\t\t      curand_mtgp32_host.h\t     nppi_color_conversion.h\t\t       sm_30_intrinsics.h\n> cublas_v2.h\t\t      curand_mtgp32_kernel.h\t     nppi_compression_functions.h\t       sm_30_intrinsics.hpp\n> cublas_version.h\t      curand_normal.h\t\t     nppi_computer_vision.h\t\t       sm_32_atomic_functions.h\n> cublasXt.h\t\t      curand_normal_static.h\t     nppi_data_exchange_and_initialization.h   sm_32_atomic_functions.hpp\n> cuComplex.h\t\t      curand_philox4x32_x.h\t     nppi_filtering_functions.h\t\t       sm_32_intrinsics.h\n> cuda_device_runtime_api.h     curand_poisson.h\t\t     nppi_geometry_transforms.h\t\t       sm_32_intrinsics.hpp\n> cudaEGL.h\t\t      curand_precalc.h\t\t     nppi.h\t\t\t\t       sm_35_atomic_functions.h\n> cuda_egl_interop.h\t      curand_uniform.h\t\t     nppi_linear_transforms.h\t\t       sm_35_intrinsics.h\n> cuda_fp16.h\t\t      cusolver_common.h\t\t     nppi_morphological_operations.h\t       sm_60_atomic_functions.h\n> cuda_fp16.hpp\t\t      cusolverDn.h\t\t     nppi_statistics_functions.h\t       sm_60_atomic_functions.hpp\n> cudaGL.h\t\t      cusolverRf.h\t\t     nppi_support_functions.h\t\t       sm_61_intrinsics.h\n> cuda_gl_interop.h\t      cusolverSp.h\t\t     nppi_threshold_and_compare_operations.h   sm_61_intrinsics.hpp\n> cuda.h\t\t\t      cusolverSp_LOWLEVEL_PREVIEW.h  npps_arithmetic_and_logical_operations.h  sobol_direction_vectors.h\n> cudalibxt.h\t\t      cusparse.h\t\t     npps_conversion_functions.h\t       surface_functions.h\n> cuda_occupancy.h\t      cusparse_v2.h\t\t     npps_filtering_functions.h\t\t       surface_functions.hpp\n> cuda_profiler_api.h\t      device_atomic_functions.h      npps.h\t\t\t\t       surface_indirect_functions.h\n> cudaProfiler.h\t\t      device_atomic_functions.hpp    npps_initialization.h\t\t       surface_indirect_functions.hpp\n> cudart_platform.h\t      device_double_functions.h      npps_statistics_functions.h\t       surface_types.h\n> cuda_runtime_api.h\t      device_functions.h\t     npps_support_functions.h\t\t       texture_fetch_functions.h\n> cuda_runtime.h\t\t      device_launch_parameters.h     nppversion.h\t\t\t       texture_fetch_functions.hpp\n> cuda_surface_types.h\t      device_types.h\t\t     nvblas.h\t\t\t\t       texture_indirect_functions.h\n> cuda_texture_types.h\t      driver_functions.h\t     nvfunctional\t\t\t       texture_indirect_functions.hpp\n> cudaVDPAU.h\t\t      driver_types.h\t\t     nvgraph.h\t\t\t\t       texture_types.h\n> cuda_vdpau_interop.h\t      fatBinaryCtl.h\t\t     nvjpeg.h\t\t\t\t       thrust\n> cufft.h\t\t\t      fatbinary.h\t\t     nvml.h\t\t\t\t       vector_functions.h\n> cufftw.h\t\t      fatbinary_section.h\t     nvrtc.h\t\t\t\t       vector_functions.hpp\n> cufftXt.h\t\t      host_config.h\t\t     nvToolsExtCuda.h\t\t\t       vector_types.h\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26230#issuecomment-473371091>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHLOjkCGP8WIGc5kivx-1x1YI2QaYZ5aks5vW9eEgaJpZM4bX6E2>\n> .\n>\n", "That would be both building and running TF with different CUDA versions. We compile a new TF package for each CUDA version + a CPU package, so that `tensorflow-1.13-cuda-10.1` would be a TF package built with `system76-cuda-10.1` & co. as dependencies. This also allows installing multiple versions of TF and toggling between the active version symlinked at `/usr/lib/tensorflow`.", "Hi Nathan. Sorry that this hasn't made any progress. I will try to split this up a little and get it submitted as multiple CLs.", "FYI, the version for cusparse needs to be updated as well: https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L907", "> @chsigg Starting with CUDA 10.1 cublas is installed into the system /usr/lib and /usr/include directories rather than being located with the toolkit. Also, a build number is being added to the cublas libs, so they won't necessarily match the toolkit version. See https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-new-features\r\n> \r\n> Reducing the coverage of the stub library should improve future maintainability as well. I'll update the PR shortly.\r\n\r\nOn my systems I consolidate everything in /opt/nvidia/cuda(/lib64) because I don't like how the Nvidia installer spews a bunch of stuff into various system directories.\r\n\r\nHopefully whatever build changes are made to TF won't be making assumptions that things are located in the \"standard\" places the latest flavor of the Nvidia installer decides to use.\r\n\r\nMy hope is the approach below I use will continue to work.\r\n\r\n```\r\n$ cat /etc/ld.so.conf.d/cuda.conf \r\n\r\n/opt/nvidia/cuda/lib64\r\n/opt/nvidia/cuda/extras/CUPTI/lib64\r\n/opt/nvidia/nccl/lib\r\n\r\n$ ldconfig\r\n```", "Some further info in case it's useful.\r\n\r\n```\r\n$ strace python3 classify_image.py 2>&1 | grep openat | grep -i cublas | grep opt\r\n\r\nopenat(AT_FDCWD, \"/opt/nvidia/cuda/lib64/libcublas.so.10\", O_RDONLY|O_CLOEXEC) = 4\r\nopenat(AT_FDCWD, \"/opt/nvidia/cuda/lib64/libcublasLt.so.10\", O_RDONLY|O_CLOEXEC) = 4\r\n```\r\n\r\nIt is looking in /opt as it should - great. But then I don't understand why it's not using libcublas from there. In that directory there are `libcublas.so`, `libcublas.so.10`, `libcublas.so.10.1`, `libcublas.so.10.1.0.105`, `libcublasLt.so`,  `libcublasLt.so.10`, `libcublasLt.so.10.1.0.105`\r\n\r\nThen I check if it is looking in /usr:\r\n```\r\n$ strace python3 classify_image.py 2>&1 | grep openat | grep -i cublas | grep '/usr/lib64/libcublas'\r\n\r\nopenat(AT_FDCWD, \"/usr/lib64/libcublas.so.10.1\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)\r\n```\r\n\r\nIt is looking there. So for now I worked around the issue as follows:\r\n```\r\nln -s /opt/nvidia/cuda/lib64/libcublas.so.10.1 /usr/lib64/libcublas.so.10.1\r\n```", "I cherry-picked the 2 commits into Tensorflow master, however, run into following errors:\r\n```ERROR: /home/dongm/workspace/GITHUB/tensorflow/third_party/gpus/cuda_configure.bzl:131:1: Variable _GCC_HOST_COMPILER_PATH is read only\r\nNeed help? See https://bazel.build/versions/master/docs/skylark/errors/read-only-variable.html\r\nERROR: error loading package '': in /home/dongm/workspace/GITHUB/tensorflow/tensorflow/workspace.bzl: Extension 'third_party/gpus/cuda_configure.bzl' has errors\r\nERROR: error loading package '': in /home/dongm/workspace/GITHUB/tensorflow/tensorflow/workspace.bzl: Extension 'third_party/gpus/cuda_configure.bzl' has errors\r\nINFO: Elapsed time: 0.193s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)```", "currently this seems to suffer the same deficiency as described in #28093 ", "Hopefully PR #28021 will fix the issue you are seeing.", "Any updates here?", "Closing as 10.1 support has been implemented separately by @chsigg."]}, {"number": 26229, "title": "[INTEL MKL]  RequantizeOp to support int8 data type", "body": "Add registration for RequantizeOp for to support \"int8\" date type.\r\n\r\nCurrently RequantizeOp only supports \"qint8\". MklQuantizedConvOp etc ops already support both. ", "comments": ["Penpornk, \r\nThank you so much for the review. \r\nGZ", "@rthadur Gentle ping for pulling. Thank you!", "Penporn: thank you very much!   -GZ"]}, {"number": 26228, "title": "[Intel MKL]: Dequantize Op", "body": "This PR adds support for Dequantize Op using MKL-DNN.", "comments": ["Hi @penpornk , could you please review this PR? also if you can review #26585, it will be great. Thanks!", "@penpornk Thank you so much for reviewing this. I have addressed all the comments. ", "@penpornk I have addressed the missed comment. Thanks!"]}, {"number": 26227, "title": "[SOLVED] Failed to build 1.13.0-rc2 with CUDA in Windows 7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WINDOWS 7\r\n- TensorFlow installed from (source or binary): SOURCE\r\n- TensorFlow version: 1.13.0-rc2\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: building\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): MSVS 2015 14.0.24215.1\r\n- CUDA/cuDNN version: 10 / 7.4.2.24\r\n- GPU model and memory: NVIDIA QUADRO k5000 4GB (3.0)\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilding from source in Windows 7 with CUDA 10 support for 3.0 computing capability, with a clean Python 3.7 installation, fails with the message: `ImportError: DLL load failed: The specified module could not be found.`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n* Installed CUDA 10 and cuDnn 7.4.2.24\r\n* Installed mysys64 in C:/mysys64\r\n* Installed bazel 0.21\r\n* Installed a fresh 3.7 Python in %APPDATA%/Programs/Python\r\n* pip3 install six numpy wheel\r\n* pip3 install keras_applications --no-deps\r\n* pip3 install keras_preprocessing --no-deps\r\n* configured all enviroment variables according to the tutorial\r\n* downloaded and extracted tensorflow 1.13.0-rc2 release\r\n* opened VS 2015 x64 Native Tools Command Prompt as Administrator\r\n* entered the tensorflow source dir\r\n* python ./configure.py\r\n* bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n```cmd\r\nINFO: From Linking tensorflow/contrib/boosted_trees/python/ops/_boosted_trees_ops.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/boosted_trees/python/ops/python/ops/lib_boosted_trees_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/boosted_trees/python/ops/\r\nINFO: From Linking tensorflow/contrib/tpu/python/ops/_tpu_ops.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/python/ops/python/ops/lib_tpu_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/python/ops/python/ops/lib_tpu_ops.so.exp\r\nERROR: C:/users/reinert/downloads/tensorflow-1.13.0-rc2/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/reinert/_bazel_reinert/kbgdyy6s/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/reinert/AppData/Local/Programs/Python/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/reinert/AppData/Local/Programs/Python/Python37/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\reinert\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\reinert\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\reinert\\AppData\\Local\\Temp\\Bazel.runfiles_t5_pd1qh\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\reinert\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\reinert\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2166,410s, Critical Path: 1238,93s\r\nINFO: 4651 processes: 4651 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["@reinert Could you follow the instructions [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12/blob/master/README.md) to install TF. The instructions are mainly for installing TF1.12 with CUDA9.0 but you can follow the same approach but use updated files/drivers etc. Please let me know how it progresses. Thanks!", "Hi, I followed the instructions and now the error is different (it seems to be in a earlier step):\r\n\r\n```cmd\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/nn_ops_gen_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/nn_ops_gen_cc.exp\r\nINFO: From Linking tensorflow/cc/ops/sparse_ops_gen_cc:\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/sparse_ops_gen_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/sparse_ops_gen_cc.exp\r\nINFO: From Linking tensorflow/cc/ops/logging_ops_gen_cc:\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/logging_ops_gen_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/cc/ops/logging_ops_gen_cc.exp\r\nERROR: C:/tf2/tensorflow/contrib/tpu/BUILD:137:1: Executing genrule //tensorflow/contrib/tpu:tpu_ops_pygenrule failed (Exit 3): bash.exe failed: error executing command\r\n  cd C:/users/reinert/_bazel_reinert/6gxwwe2m/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/reinert/AppData/Local/Programs/Python/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/reinert/AppData/Local/Programs/Python/Python37/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/gen_tpu_ops_py_wrappers_cc , SendTPUEmbeddingGradients,EnqueueTPUEmbeddingIntegerBatch,En\r\nPUEmbeddingSparseBatch,EnqueueTPUEmbeddingSparseTensorBatch 0 0 > bazel-out/x64_windows-opt/genfiles/tensorflow/contrib/tpu/ops/gen_tpu_ops.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:318] Invalid file descriptor data passed to EncodedDescriptorDatabase::Add().\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1358] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2064,588s, Critical Path: 591,08s\r\nINFO: 4424 processes: 4424 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "After running again, I got the same initial error. It keeps crashing while creating this `lib_tpu_ops`. Is there anyway to disable TPU in the building process?", "Maybe related to https://github.com/tensorflow/tensorflow/commit/cbe8172cba4f2ed05a0980585ba48ef257922c18 ?", "Can I disable building the `contrib` directory?", "@reinert Did you uninstall everything before installing TF with the instructions I provided? Thanks! ", "No, I didn't. But I've found the problem is not related to TPU contrib. Instead it's related to this `swig` lib. I found many issues regarding it. I'll dig deeper and report here what I can.", "When building on windows ` --define=no_tensorflow_py_deps=true` should not be used.\r\nThat is an option we created for testing only.", "My \"ImportError: DLL load failed\" issue was solved after I downgraded to bazel 0.20.\r\n\r\nTo use cuda 10.1, a symbolic link is required.\r\n\r\n`mklink cuda\\bin\\cublas64_101.dll cuda\\bin\\cublas64_10.dll`\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: building\r\n- Bazel version (if compiling from source): 0.20\r\n- GCC/Compiler version (if compiling from source): MSVC 19.00.24215.1 for x64\r\n- CUDA/cuDNN version: 10.1 / 7.5.0\r\n- GPU model and memory: NVIDIA GeForce GTX 1060 6 GB", "@HarryHHung did you build tensorflow_cc.dll for windows?", "No, I only have built a whl for python.", "I finally got it! After 3 weeks trying :)\r\n\r\nThank you @gunan and @HarryHHung for the valuable tips.\r\n\r\nI'm now having an issue to build the whl. Here's the error:\r\n```cmd\r\nunzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or\r\n        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.\r\n```", "I have seen the 0-byte issue before.\r\n\r\nhttps://stackoverflow.com/questions/52394305/creating-pip-package-for-tensorflow-with-gpu-support-results-in-0-byte-simple-co/", "And you were right again! Thank you very much @HarryHHung! Tensorflow with GPU support is now installed in my machine. My build environment is as follows:\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.2\r\n- Bazel version: 0.20.0\r\n- GCC/Compiler version: MSVC 19.00.24215.1 for x64 (MSVC 2015 Update 3)\r\n- CUDA/cuDNN version: 10.0 / 7.4\r\n- GPU model and memory: NVIDIA Quadro K5000 4GB\r\n\r\nMy two cents: After setting everything up and building for the first time, the build can fail, but you must try building again (`bazel build ...`). Sometimes the build passes on the second attempt.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!\r\n"]}, {"number": 26226, "title": "Tensorboard 1.12 not working after upgrade from 1.8 to 1.12", "body": "I upgraded tensorflow to 1.12 which also upgraded tensorboard to 1.12 from 1.8 version but I get the error:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 7, in <module>\r\n    from tensorboard.main import run_main\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorboard/main.py\", line 44, in <module>\r\n    from tensorboard import default\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorboard/default.py\", line 44, in <module>\r\n    from tensorboard.plugins.interactive_inference import interactive_inference_plugin\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorboard/plugins/interactive_inference/interactive_inference_plugin.py\", line 27, in <module>\r\n    from grpc.framework.interfaces.face.face import AbortionError\r\nModuleNotFoundError: No module named 'grpc.framework'\r\n\r\nI unsintalled and reinstalled grpcio, I completely removed tensorflow and tensorboard and then re-installed 1.12 version but same error keeps coming back.", "comments": ["This issue is more appropriate on **tensorflow/tensorboard repo**. Please post it on tensorboard from [here](https://github.com/tensorflow/tensorboard/issues/new) and provide all the information asked by the template. Thanks! "]}]